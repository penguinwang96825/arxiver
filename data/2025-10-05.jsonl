{"title": "SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations", "abstract": "Large Language Models (LLMs) are increasingly deployed in high-risk domains.\nHowever, state-of-the-art LLMs often produce hallucinations, raising serious\nconcerns about their reliability. Prior work has explored adversarial attacks\nfor hallucination elicitation in LLMs, but it often produces unrealistic\nprompts, either by inserting gibberish tokens or by altering the original\nmeaning. As a result, these approaches offer limited insight into how\nhallucinations may occur in practice. While adversarial attacks in computer\nvision often involve realistic modifications to input images, the problem of\nfinding realistic adversarial prompts for eliciting LLM hallucinations has\nremained largely underexplored. To address this gap, we propose Semantically\nEquivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic\nmodifications to the prompt that preserve its meaning while maintaining\nsemantic coherence. Our contributions are threefold: (i) we formulate finding\nrealistic attacks for hallucination elicitation as a constrained optimization\nproblem over the input prompt space under semantic equivalence and coherence\nconstraints; (ii) we introduce a constraint-preserving zeroth-order method to\neffectively search for adversarial yet feasible prompts; and (iii) we\ndemonstrate through experiments on open-ended multiple-choice question\nanswering tasks that SECA achieves higher attack success rates while incurring\nalmost no constraint violations compared to existing methods. SECA highlights\nthe sensitivity of both open-source and commercial gradient-inaccessible LLMs\nto realistic and plausible prompt variations. Code is available at\nhttps://github.com/Buyun-Liang/SECA.", "published": "2025-10-05 23:44:54", "link": "http://arxiv.org/abs/2510.04398v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Time Is Effort: Estimating Human Post-Editing Time for Grammar Error Correction Tool Evaluation", "abstract": "Text editing can involve several iterations of revision. Incorporating an\nefficient Grammar Error Correction (GEC) tool in the initial correction round\ncan significantly impact further human editing effort and final text quality.\nThis raises an interesting question to quantify GEC Tool usability: How much\neffort can the GEC Tool save users? We present the first large-scale dataset of\npost-editing (PE) time annotations and corrections for two English GEC test\ndatasets (BEA19 and CoNLL14). We introduce Post-Editing Effort in Time (PEET)\nfor GEC Tools as a human-focused evaluation scorer to rank any GEC Tool by\nestimating PE time-to-correct. Using our dataset, we quantify the amount of\ntime saved by GEC Tools in text editing. Analyzing the edit type indicated that\ndetermining whether a sentence needs correction and edits like paraphrasing and\npunctuation changes had the greatest impact on PE time. Finally, comparison\nwith human rankings shows that PEET correlates well with technical effort\njudgment, providing a new human-centric direction for evaluating GEC tool\nusability. We release our dataset and code at:\nhttps://github.com/ankitvad/PEET_Scorer.", "published": "2025-10-05 23:24:24", "link": "http://arxiv.org/abs/2510.04394v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards", "abstract": "RAG systems are increasingly deployed in high-stakes domains where users\nexpect outputs to be consistent across semantically equivalent queries.\nHowever, existing systems often exhibit significant inconsistencies due to\nvariability in both the retriever and generator (LLM), undermining trust and\nreliability. In this work, we focus on information consistency, i.e., the\nrequirement that outputs convey the same core content across semantically\nequivalent inputs. We introduce a principled evaluation framework that\ndecomposes RAG consistency into retriever-level, generator-level, and\nend-to-end components, helping identify inconsistency sources. To improve\nconsistency, we propose Paraphrased Set Group Relative Policy Optimization\n(PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased\nset to assign group similarity rewards. We leverage PS-GRPO to achieve\nInformation Consistent RAG (Con-RAG), training the generator to produce\nconsistent outputs across paraphrased queries and remain robust to\nretrieval-induced variability. Because exact reward computation over paraphrase\nsets is computationally expensive, we also introduce a scalable approximation\nmethod that retains effectiveness while enabling efficient, large-scale\ntraining. Empirical evaluations across short-form, multi-hop, and long-form QA\nbenchmarks demonstrate that Con-RAG significantly improves both consistency and\naccuracy over strong baselines, even in the absence of explicit ground-truth\nsupervision. Our work provides practical solutions for evaluating and building\nreliable RAG systems for safety-critical deployments.", "published": "2025-10-05 23:14:13", "link": "http://arxiv.org/abs/2510.04392v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Internal World Models as Imagination Networks in Cognitive Agents", "abstract": "What is the computational objective of imagination? While classical\ninterpretations suggest imagination is useful for maximizing rewards, recent\nfindings challenge this view. In this study, we propose that imagination serves\nto access an internal world model (IWM) and use psychological network analysis\nto explore IWMs in humans and large language models (LLMs). Specifically, we\nassessed imagination vividness ratings using two questionnaires and constructed\nimagination networks from these reports. Imagination networks from human groups\nshowed correlations between different centrality measures, including expected\ninfluence, strength, and closeness. However, imagination networks from LLMs\nshowed a lack of clustering and lower correlations between centrality measures\nunder different prompts and conversational memory conditions. Together, these\nresults indicate a lack of similarity between IWMs in human and LLM agents.\nOverall, our study offers a novel method for comparing internally-generated\nrepresentations in humans and AI, providing insights for developing human-like\nimagination in artificial intelligence.", "published": "2025-10-05 23:01:10", "link": "http://arxiv.org/abs/2510.04391v1", "categories": ["cs.AI", "cs.CL", "cs.SI", "q-bio.NC"], "primary_category": "cs.AI"}
{"title": "MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator", "abstract": "World models that support controllable\n  and editable spatiotemporal environments are valuable\n  for robotics, enabling scalable training data, repro ducible evaluation, and\nflexible task design. While\n  recent text-to-video models generate realistic dynam ics, they are\nconstrained to 2D views and offer limited\n  interaction. We introduce MorphoSim, a language guided framework that\ngenerates 4D scenes with\n  multi-view consistency and object-level controls. From\n  natural language instructions, MorphoSim produces\n  dynamic environments where objects can be directed,\n  recolored, or removed, and scenes can be observed\n  from arbitrary viewpoints. The framework integrates\n  trajectory-guided generation with feature field dis tillation, allowing edits\nto be applied interactively\n  without full re-generation. Experiments show that Mor phoSim maintains high\nscene fidelity while enabling\n  controllability and editability. The code is available\n  at https://github.com/eric-ai-lab/Morph4D.", "published": "2025-10-05 22:55:17", "link": "http://arxiv.org/abs/2510.04390v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models", "abstract": "We introduce MacroBench, a code-first benchmark that evaluates whether LLMs\ncan synthesize reusable browser automation programs from natural language goals\nby reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates\nseven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,\nFacebook-like, Discord-like, and Threads-like, covering 681 tasks across\ninteraction complexity and targeting difficulty. Our end-to-end protocol\nvalidates generated code via static checks, sandboxed execution, and outcome\nverification including DOM assertions and database snapshots, and includes a\nsafety suite for scraping, spam/abuse, and credential/privacy prompts. Across\n2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8\npercent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,\nand DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at\n91.7 percent but fail on complex workflows at 0.0 percent, and none meet\nproduction-quality coding practices despite functional completion. We release\nour complete benchmark pipeline, evaluation framework, and experimental results\nto enable reproducible assessment of macro synthesis for web automation.", "published": "2025-10-05 21:15:11", "link": "http://arxiv.org/abs/2510.04363v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models", "abstract": "Pre-trained language models have achieved remarkable success across a wide\nrange of natural language processing (NLP) tasks, particularly when fine-tuned\non large, domain-relevant datasets. However, they remain vulnerable to backdoor\nattacks, where adversaries embed malicious behaviors using trigger patterns in\nthe training data. These triggers remain dormant during normal usage, but, when\nactivated, can cause targeted misclassifications. In this work, we investigate\nthe internal behavior of backdoored pre-trained encoder-based language models,\nfocusing on the consistent shift in attention and gradient attribution when\nprocessing poisoned inputs; where the trigger token dominates both attention\nand gradient signals, overriding the surrounding context. We propose an\ninference-time defense that constructs anomaly scores by combining token-level\nattention and gradient information. Extensive experiments on text\nclassification tasks across diverse backdoor attack scenarios demonstrate that\nour method significantly reduces attack success rates compared to existing\nbaselines. Furthermore, we provide an interpretability-driven analysis of the\nscoring mechanism, shedding light on trigger localization and the robustness of\nthe proposed defense.", "published": "2025-10-05 20:15:56", "link": "http://arxiv.org/abs/2510.04347v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time", "abstract": "Language model finetuning often results in learning undesirable traits in\ncombination with desired ones. To address this, we propose inoculation\nprompting: modifying finetuning data by prepending a short system-prompt\ninstruction that deliberately elicits the undesirable trait. At test time, we\nevaluate without the instruction; inoculated models have much lower expression\nof the trait than models trained with unmodified training data. Inoculation is\nselective: in a toy setting where assistant responses are always in Spanish and\nALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'')\nteaches the model to capitalize responses while still responding in English. We\nfind that inoculation is also effective across several additional settings:\nreducing emergent misalignment (EM) from task-specific finetuning, defending\nagainst backdoor injections, and mitigating the transmission of traits via\nsubliminal learning. Follow-up analysis suggests a mechanism: making a trait\nless surprising via inoculation reduces optimization pressure to globally\nupdate the model, thereby reducing the degree of generalization. Our analysis\nrelates to prior work on EM: inoculation explains prior findings that\neducational contexts mitigate EM from insecure code. Beyond demonstrating a\nsimple and effective technique for selective learning, our results contribute\nto a better conceptual understanding of how and why language models generalize.", "published": "2025-10-05 20:04:22", "link": "http://arxiv.org/abs/2510.04340v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluation of Clinical Trials Reporting Quality using Large Language Models", "abstract": "Reporting quality is an important topic in clinical trial research articles,\nas it can impact clinical decisions. In this article, we test the ability of\nlarge language models to assess the reporting quality of this type of article\nusing the Consolidated Standards of Reporting Trials (CONSORT). We create\nCONSORT-QA, an evaluation corpus from two studies on abstract reporting quality\nwith CONSORT-abstract standards. We then evaluate the ability of different\nlarge generative language models (from the general domain or adapted to the\nbiomedical domain) to correctly assess CONSORT criteria with different known\nprompting methods, including Chain-of-thought. Our best combination of model\nand prompting method achieves 85% accuracy. Using Chain-of-thought adds\nvaluable information on the model's reasoning for completing the task.", "published": "2025-10-05 20:01:28", "link": "http://arxiv.org/abs/2510.04338v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Read the Scene, Not the Script: Outcome-Aware Safety for LLMs", "abstract": "Safety-aligned Large Language Models (LLMs) still show two dominant failure\nmodes: they are easily jailbroken, or they over-refuse harmless inputs that\ncontain sensitive surface signals. We trace both to a common cause: current\nmodels reason weakly about links between actions and outcomes and over-rely on\nsurface-form signals, lexical or stylistic cues that do not encode\nconsequences. We define this failure mode as Consequence-blindness. To study\nconsequence-blindness, we build a benchmark named CB-Bench covering four risk\nscenarios that vary whether semantic risk aligns with outcome risk, enabling\nevaluation under both matched and mismatched conditions which are often ignored\nby existing safety benchmarks. Mainstream models consistently fail to separate\nthese risks and exhibit consequence-blindness, indicating that\nconsequence-blindness is widespread and systematic. To mitigate\nconsequence-blindness, we introduce CS-Chain-4k, a consequence-reasoning\ndataset for safety alignment. Models fine-tuned on CS-Chain-4k show clear gains\nagainst semantic-camouflage jailbreaks and reduce over-refusal on harmless\ninputs, while maintaining utility and generalization on other benchmarks. These\nresults clarify the limits of current alignment, establish consequence-aware\nreasoning as a core alignment goal and provide a more practical and\nreproducible evaluation path.", "published": "2025-10-05 18:46:49", "link": "http://arxiv.org/abs/2510.04320v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Wave-PDE Nets: Trainable Wave-Equation Layers as an Alternative to Attention", "abstract": "We introduce Wave-PDE Nets, a neural architecture whose elementary operation\nis a differentiable simulation of the second-order wave equation. Each layer\npropagates its hidden state as a continuous field through a medium with\ntrainable spatial velocity c(x) and damping {\\gamma}(x). A symplectic spectral\nsolver based on FFTs realises this propagation in O(nlog n) time. This\noscillatory, global mechanism provides a powerful alternative to attention and\nfirst-order state-space models. We prove that a single Wave-PDE layer is a\nuniversal approximator. On language and vision benchmarks, Wave-PDE Nets match\nor exceed Transformer performance while demonstrating superior practical\nefficiency, reducing wall-clock time by up to 30% and peak memory by 25%.\nAblation studies confirm the critical role of symplectic integration and a\nspectral Laplacian for stability and performance. Visualizations of the learned\nphysical parameters reveal that the model learns intuitive strategies for\ninformation propagation. These results position Wave-PDE Nets as a\ncomputationally efficient and robust architecture with a strong physical\ninductive bias.", "published": "2025-10-05 17:52:52", "link": "http://arxiv.org/abs/2510.04304v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Measuring Language Model Hallucinations Through Distributional Correctness", "abstract": "Common evaluation paradigms for language models focus on scoring single\nresponses through accuracy metrics or proper scoring rules, failing to capture\nthe full richness of a model's belief state. Recent work illustrates that\nlanguage models hallucinate in-part because they are optimised to be good\ntest-takers under binary scoring schemes that reward any answer over\nabstention. While this insight naturally leads to penalty-based approaches,\nthey ignore crucial distinctions in how models distribute uncertainty, for\nexample between hedging toward incorrect answers versus hedging toward \"I don't\nknow\" responses. A novel evaluation metric, the Distributional Correctness\nScore (DCS), is introduced to solve this problem, i.e., of not considering a\nmodel's entire probability distribution over answer choices. DCS naturally\ndistinguishes between harmful overconfidence in wrong answers and uncertainty\nexpressed through abstention, providing scores in an interpretable default\nrange. Through theoretical analysis and illustrative examples, DCS is\ndemonstrated to offer a more nuanced and aligned evaluation paradigm that\nincentivises models to express genuine uncertainty rather than guessing.\nAdapting 12 existing evaluation benchmarks to DCS's variants and measuring\nperformance on six language models reveals that for half of the tested\nbenchmarks scores are negative across all tested models, indicating significant\ntendencies towards hallucination.", "published": "2025-10-05 17:50:42", "link": "http://arxiv.org/abs/2510.04302v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Equipping Retrieval-Augmented Large Language Models with Document Structure Awareness", "abstract": "While large language models (LLMs) demonstrate impressive capabilities, their\nreliance on parametric knowledge often leads to factual inaccuracies.\nRetrieval-Augmented Generation (RAG) mitigates this by leveraging external\ndocuments, yet existing approaches treat retrieved passages as isolated chunks,\nignoring valuable structure that is crucial for document organization.\nMotivated by this gap, we propose Retrieve-DocumentRoute-Read (RDR2), a novel\nframework that explicitly incorporates structural information throughout the\nRAG process. RDR2 employs an LLM-based router to dynamically navigate document\nstructure trees, jointly evaluating content relevance and hierarchical\nrelationships to assemble optimal evidence. Our key innovation lies in\nformulating document routing as a trainable task, with automatic action\ncuration and structure-aware passage selection inspired by human reading\nstrategies. Through comprehensive evaluation on five challenging datasets, RDR2\nachieves state-of-the-art performance, demonstrating that explicit structural\nawareness significantly enhances RAG systems' ability to acquire and utilize\nknowledge, particularly in complex scenarios requiring multi-document\nsynthesis.", "published": "2025-10-05 17:04:24", "link": "http://arxiv.org/abs/2510.04293v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PABSA: Hybrid Framework for Persian Aspect-Based Sentiment Analysis", "abstract": "Sentiment analysis is a key task in Natural Language Processing (NLP),\nenabling the extraction of meaningful insights from user opinions across\nvarious domains. However, performing sentiment analysis in Persian remains\nchallenging due to the scarcity of labeled datasets, limited preprocessing\ntools, and the lack of high-quality embeddings and feature extraction methods.\nTo address these limitations, we propose a hybrid approach that integrates\nmachine learning (ML) and deep learning (DL) techniques for Persian\naspect-based sentiment analysis (ABSA). In particular, we utilize polarity\nscores from multilingual BERT as additional features and incorporate them into\na decision tree classifier, achieving an accuracy of 93.34%-surpassing existing\nbenchmarks on the Pars-ABSA dataset. Additionally, we introduce a Persian\nsynonym and entity dictionary, a novel linguistic resource that supports text\naugmentation through synonym and named entity replacement. Our results\ndemonstrate the effectiveness of hybrid modeling and feature augmentation in\nadvancing sentiment analysis for low-resource languages such as Persian.", "published": "2025-10-05 17:02:31", "link": "http://arxiv.org/abs/2510.04291v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling", "abstract": "Mixture-of-Experts (MoE) layers scale transformers by routing tokens to a\nsparse subset of feed-forward experts. Token-level routing, however, assigns an\nentire semantic spectrum to each expert, creating capacity bottlenecks,\nload-balancing pathologies, and limited specialization. We introduce SliceMoE,\nan architecture that routes contiguous slices of a token's hidden vector. A\nd-dimensional embedding is partitioned into S slices, and for each slice, a\nlightweight shared router predicts the top-k experts. Experts operate on their\nassigned slices independently, and outputs are reassembled, maintaining\nper-token FLOP efficiency. Because slices from different tokens interleave\nwithin an expert, utilization is naturally smoother. We propose a slice-level\ncapacity loss, cross-slice dropout, and efficient fused batched GEMM kernels.\nExperiments on WikiText-103 language modeling, WMT En-De translation, and three\ntext-classification datasets show SliceMoE attains up to 1.7x faster inference\nthan dense baselines, 12 to 18 percent lower perplexity than parameter-matched\ntoken-MoE, and improved expert balance, with interpretable expertise over\nsyntactic versus semantic subspaces.", "published": "2025-10-05 16:57:32", "link": "http://arxiv.org/abs/2510.04286v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy", "abstract": "We introduce a cumulant-expansion framework for quantifying how large\nlanguage models (LLMs) internalize higher-order statistical structure during\nnext-token prediction. By treating the softmax entropy of each layer's logit\ndistribution as a perturbation around its \"center\" distribution, we derive\nclosed-form cumulant observables that isolate successively higher-order\ncorrelations. Empirically, we track these cumulants in GPT-2 and Pythia models\non Pile-10K prompts. (i) Structured prompts exhibit a characteristic\nrise-and-plateau profile across layers, whereas token-shuffled prompts remain\nflat, revealing the dependence of the cumulant profile on meaningful context.\n(ii) During training, all cumulants increase monotonically before saturating,\ndirectly visualizing the model's progression from capturing variance to\nlearning skew, kurtosis, and higher-order statistical structures. (iii)\nMathematical prompts show distinct cumulant signatures compared to general\ntext, quantifying how models employ fundamentally different processing\nmechanisms for mathematical versus linguistic content. Together, these results\nestablish cumulant analysis as a lightweight, mathematically grounded probe of\nfeature-learning dynamics in high-dimensional neural networks.", "published": "2025-10-05 16:55:58", "link": "http://arxiv.org/abs/2510.04285v1", "categories": ["cs.CL", "cond-mat.stat-mech", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "LongTail-Swap: benchmarking language models' abilities on rare words", "abstract": "Children learn to speak with a low amount of data and can be taught new words\non a few-shot basis, making them particularly data-efficient learners. The\nBabyLM challenge aims at exploring language model (LM) training in the low-data\nregime but uses metrics that concentrate on the head of the word distribution.\nHere, we introduce LongTail-Swap (LT-Swap), a benchmark that focuses on the\ntail of the distribution, i.e., measures the ability of LMs to learn new words\nwith very little exposure, like infants do. LT-Swap is a pretraining\ncorpus-specific test set of acceptable versus unacceptable sentence pairs that\nisolate semantic and syntactic usage of rare words. Models are evaluated in a\nzero-shot fashion by computing the average log probabilities over the two\nmembers of each pair. We built two such test sets associated with the 10M words\nand 100M words BabyLM training sets, respectively, and evaluated 16 models from\nthe BabyLM leaderboard. Our results not only highlight the poor performance of\nlanguage models on rare words but also reveal that performance differences\nacross LM architectures are much more pronounced in the long tail than in the\nhead. This offers new insights into which architectures are better at handling\nrare word generalization. We've also made the code publicly avail", "published": "2025-10-05 16:17:33", "link": "http://arxiv.org/abs/2510.04268v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Don't Pass$\\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation", "abstract": "Pass$@k$ is widely used to report performance for LLM reasoning, but it often\nyields unstable, misleading rankings, especially when the number of trials\n(samples) is limited and compute is constrained. We present a principled\nBayesian evaluation framework that replaces Pass$@k$ and average accuracy over\n$N$ trials (avg$@N$) with posterior estimates of a model's underlying success\nprobability and credible intervals, yielding stable rankings and a transparent\ndecision rule for differences. Evaluation outcomes are modeled as categorical\n(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the\nposterior mean and uncertainty of any weighted rubric and enabling the use of\nprior evidence when appropriate. Theoretically, under a uniform prior, the\nBayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),\nexplaining its empirical robustness while adding principled uncertainty.\nEmpirically, in simulations with known ground-truth success rates and on\nAIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster\nconvergence and greater rank stability than Pass$@k$ and recent variants,\nenabling reliable comparisons at far smaller sample counts. The framework\nclarifies when observed gaps are statistically meaningful (non-overlapping\ncredible intervals) versus noise, and it naturally extends to graded,\nrubric-based evaluations. Together, these results recommend replacing Pass$@k$\nfor LLM evaluation and ranking with a posterior-based, compute-efficient\nprotocol that unifies binary and non-binary evaluation while making uncertainty\nexplicit. Code is available at https://mohsenhariri.github.io/bayes-kit", "published": "2025-10-05 16:14:03", "link": "http://arxiv.org/abs/2510.04265v1", "categories": ["cs.AI", "cs.CL", "math.ST", "stat.ML", "stat.TH"], "primary_category": "cs.AI"}
{"title": "Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought", "abstract": "Recent frontier models employ long chain-of-thought reasoning to explore\nsolution spaces in context and achieve stonger performance. While many works\nstudy distillation to build smaller yet capable models, most focus on English\nand little is known about language-specific reasoning. To bridge this gap, we\nfirst introduct **Language-Mixed CoT**, a reasoning schema that switches\nbetween English and a target language, using English as an anchor to excel in\nreasoning while minimizing translation artificats. As a Korean case study, we\ncurate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and\ncode; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k\nhigh-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5,\nLlama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves\nstate-of-the-art performance, with the highest overall average score (64.0 \\pm\n25), ranking first on 5/9 benchmarks and second on the remainder. Samller and\nmid-sized models also benefit substantially, with an average improvement of\n+18.6 points across teh evaluated nine benchmarks. Ablations show\n**Language-Mixed CoT** is more effective than monolingual CoT, also resulting\nin cross-lingual and mult-modal performance gains. We release our data-curation\npipeline, evaluation system, datasets, and models to advance research on\nlanguage-specific reasoning. Data and model collection:\nhttps://huggingface.co/KOREAson.", "published": "2025-10-05 14:39:41", "link": "http://arxiv.org/abs/2510.04230v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Epistemic Diversity and Knowledge Collapse in Large Language Models", "abstract": "Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation", "published": "2025-10-05 14:29:15", "link": "http://arxiv.org/abs/2510.04226v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zoom-In to Sort AI-Generated Images Out", "abstract": "The rapid growth of AI-generated imagery has blurred the boundary between\nreal and synthetic content, raising critical concerns for digital integrity.\nVision-language models (VLMs) offer interpretability through explanations but\noften fail to detect subtle artifacts in high-quality synthetic images. We\npropose ZoomIn, a two-stage forensic framework that improves both accuracy and\ninterpretability. Mimicking human visual inspection, ZoomIn first scans an\nimage to locate suspicious regions and then performs a focused analysis on\nthese zoomed-in areas to deliver a grounded verdict. To support training, we\nintroduce MagniFake, a dataset of 20,000 real and high-quality synthetic images\nannotated with bounding boxes and forensic explanations, generated through an\nautomated VLM-based pipeline. Our method achieves 96.39% accuracy with robust\ngeneralization, while providing human-understandable explanations grounded in\nvisual evidence.", "published": "2025-10-05 14:29:01", "link": "http://arxiv.org/abs/2510.04225v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "68T45", "I.2.10; I.2.7"], "primary_category": "cs.CV"}
{"title": "Teaching LLM to be Persuasive: Reward-Enhanced Policy Optimization for Alignment frm Heterogeneous Rewards", "abstract": "We study deploying large language models (LLMs) as business development (BD)\nagents for persuasive price negotiation in online travel agencies (OTAs), where\naligning traveler affordability and hotel profitability directly affects\nbookings, partner relationships, and access to travel. The agent must follow a\nStandard Operating Procedure (SOP) while conducting multi-turn persuasion,\ninterpreting colloquial inputs, and adhering to guardrails (no over-promising,\nno hallucinations). Conventional post-training -- supervised fine-tuning (SFT)\nor single-source reward optimization -- overfits scripts, misses nuanced\npersuasive style, and fails to enforce verifiable business constraints.\n  We propose Reward-Enhanced Policy Optimization (REPO), a reinforcement\nlearning post-training framework that aligns an LLM with heterogeneous rewards:\na preference-trained reward model (RM) for dense human alignment, a reward\njudge (RJ) for high-level persuasive behavior and SOP compliance, and\nprogrammatic reward functions (RF) for deterministic checks on numerics,\nformatting, and guardrails. A straightforward enhancement mechanism is proposed\nto combine the RM with RJ and RF signals to curb reward hacking and improve\nnegotiation quality. In production-style evaluations -- approximately 150 turns\nfrom real dialogues and 225 turns from curated bad-case dialogues -- REPO lifts\naverage dialogue rating to 4.63: +1.20 over base, +0.83 over Direct Preference\nOptimization (DPO); +0.33 over Group Relative Policy Optimization (GRPO),\nincreases the share of conversations with at least one excellent response to\n66.67% (+23.34 percentage points over GRPO), and achieves a 93.33% bad-case fix\nrate with 75.56% clean fixes, outperforming SFT, DPO, PPO, and GRPO. We also\nobserve emergent capabilities -- proactive empathy, localized reasoning,\ncalibrated tactics -- that surpass gold annotations.", "published": "2025-10-05 14:08:01", "link": "http://arxiv.org/abs/2510.04214v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling", "abstract": "Large Reasoning Models (LRMs) have demonstrated strong capabilities in\ncomplex multi-step reasoning, opening new opportunities for automating\noptimization modeling. However, existing domain adaptation methods, originally\ndesigned for earlier instruction-tuned models, often fail to exploit the\nadvanced reasoning patterns of modern LRMs -- In particular, we show that\ndirect fine-tuning on traditional \\textit{non-reflective} datasets leads to\nlimited gains. To fully leverage LRMs' inherent reasoning abilities, we propose\n\\textbf{CALM} (\\textit{Corrective Adaptation with Lightweight Modification}), a\nframework that progressively refines LRMs within their native reasoning modes\nfor optimization modeling tasks. In CALM, an expert intervener identifies\nreasoning flaws and provides concise corrective hints, which the LRM\nincorporates to produce improved reasoning trajectories. These interventions\nmodify fewer than 2.6\\% of generated tokens, but generate high-quality data for\nsoft adaptation through supervised fine-tuning. The adapted model is then\nfurther improved through reinforcement learning. Building on CALM, we develop\n\\textbf{STORM} (\\textit{Smart Thinking Optimization Reasoning Model}), a\n4B-parameter LRM that achieves a new state-of-the-art average accuracy of\n68.9\\% across five popular optimization modeling benchmarks, matching the\nperformance of a 671B LRM. These results demonstrate that dynamic, hint-based\ndata synthesis both preserves and amplifies the native reasoning patterns of\nmodern LRMs, offering a more effective and scalable path towards expert-level\nperformance on challenging optimization modeling tasks.", "published": "2025-10-05 13:38:31", "link": "http://arxiv.org/abs/2510.04204v1", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization", "abstract": "Recent advancements in Large Language Models (LLMs) have shifted from\nexplicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning,\nwhere intermediate thoughts are represented as vectors rather than text.\nHowever, latent reasoning can be brittle on challenging, out-of-distribution\ntasks where robust reasoning is most critical. To overcome these limitations,\nwe introduce Latent Thought Policy Optimization (LTPO), a parameter-free\nframework that enhances LLM reasoning entirely at test time, without requiring\nmodel parameter updates. LTPO treats intermediate latent \"thought\" vectors as\ndynamic parameters that are actively optimized for each problem instance. It\nemploys an online policy gradient method guided by an intrinsic,\nconfidence-based reward signal computed directly from the frozen LLM's own\noutput distributions, eliminating the need for external supervision or\nexpensive text generation during optimization. Extensive experiments on five\nreasoning benchmarks show that LTPO not only matches or surpasses strong\nbaselines on standard tasks but also demonstrates remarkable robustness where\nothers fail. Most notably, on highly challenging AIME benchmarks where existing\nlatent reasoning baselines collapse to near-zero accuracy, LTPO delivers\nsubstantial improvements, showcasing a unique capability for complex reasoning.", "published": "2025-10-05 12:50:39", "link": "http://arxiv.org/abs/2510.04182v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self Speculative Decoding for Diffusion Large Language Models", "abstract": "Diffusion-based Large Language Models (dLLMs) have emerged as a competitive\nalternative to autoregressive models, offering unique advantages through\nbidirectional attention and parallel generation paradigms. However, the\ngeneration results of current parallel decoding methods deviate from stepwise\ndecoding, introducing potential performance degradation, which limits their\npractical deployment. To address this problem, we propose \\textbf{S}elf\n\\textbf{S}peculative \\textbf{D}ecoding (SSD), a lossless inference acceleration\nmethod that leverages the dLLM itself as both speculative decoding drafter and\nverifier without auxiliary modules. SSD introduces a self-drafting mechanism\nwhere the model generates predictions for multiple positions, then verifies\nthem through hierarchical verification trees in a single forward pass. Unlike\ntraditional speculative decoding that requires separate draft models, SSD\neliminates model redundancy and memory overhead by exploiting the dLLM's\ninherent parallel prediction capability for multiple positions. This\nself-speculative approach allows the model to progressively verify and accept\nmultiple tokens in a single forward pass. Our experiments demonstrate that SSD\nachieves up to 3.46$\\times$ speedup while keeping the output identical to\nstepwise decoding on open source models such as LLaDA and Dream. Code will be\nmade publicly available on GitHub.", "published": "2025-10-05 10:52:28", "link": "http://arxiv.org/abs/2510.04147v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models", "abstract": "Large Language Models (LLMs) have achieved state-of-the-art performance on a\nbroad range of Natural Language Processing (NLP) tasks, including document\nprocessing and coding. Autoregressive Language Models (ARMs), which generate\ntokens sequentially conditioned on all previous tokens, have been the\npredominant paradigm for LLMs. However, while these networks have achieved high\naccuracy across a range of downstream tasks, they exhibit low arithmetic\nintensity due to the inherent sequential dependency with next-token prediction.\nRecently, Diffusion Language Models (DLMs) have emerged as a promising\nalternative architecture. DLMs generate output text in parallel, breaking the\nlimitations of sequential dependency. However, the performance implications of\nDLMs relative to commonly deployed ARMs are not fully understood. In this work,\nwe present a comprehensive performance study analyzing the performance\ncharacteristics of ARMs and DLMs, using both theoretical analysis and profiling\ndata to characterize the trade-offs between these approaches. We illustrate\nthat although DLMs exhibit higher arithmetic intensity compared to ARMs because\nof their capability to utilize parallelism across sequence lengths, they fail\nto scale effectively to longer contexts. We then explore DLMs with block-wise\ndecoding, outlining how this approach allows for increased arithmetic\nintensity, while still scaling well to long contexts (similar to ARMs). We also\nshow interesting trade-offs for batched inference, where we find that ARMs\nexhibit superior throughput, as they benefit more from parallelism across\nsequences in the batch. Finally, we highlight opportunities for accelerating\nDLM inference, and, in particular, highlight the importance of reducing the\nnumber of sampling steps for allowing open-source DLMs to provide improved\nlatency relative to ARMs.", "published": "2025-10-05 10:50:52", "link": "http://arxiv.org/abs/2510.04146v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Automating construction safety inspections using a multi-modal vision-language RAG framework", "abstract": "Conventional construction safety inspection methods are often inefficient as\nthey require navigating through large volume of information. Recent advances in\nlarge vision-language models (LVLMs) provide opportunities to automate safety\ninspections through enhanced visual and linguistic understanding. However,\nexisting applications face limitations including irrelevant or unspecific\nresponses, restricted modal inputs and hallucinations. Utilisation of Large\nLanguage Models (LLMs) for this purpose is constrained by availability of\ntraining data and frequently lack real-time adaptability. This study introduces\nSiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)\nframework for automating construction safety inspection reports by integrating\nvisual and audio inputs. Using real-world data, SiteShield outperformed\nunimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,\nprecision of 0.76, and recall of 0.96. The findings indicate that SiteShield\noffers a novel pathway to enhance information retrieval and efficiency in\ngenerating safety reports.", "published": "2025-10-05 10:48:54", "link": "http://arxiv.org/abs/2510.04145v1", "categories": ["cs.CV", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely\nadopted technique for enhancing the reasoning ability of Large Language Models\n(LLMs). However, the effectiveness of RLVR strongly depends on the capability\nof base models. This issue arises because it requires the model to have\nsufficient capability to perform high-quality exploration, which involves both\neffectiveness and diversity. Unfortunately, existing methods address this issue\nby imitating expert trajectories, which improve effectiveness but neglect\ndiversity. To address this, we argue that the expert only needs to provide\nguidance only at critical decision points rather than the entire reasoning\npath. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation\nfor Token-level Optimization of Reasoning, a framework that provides expert\nguidance only at critical decision points to perform effective and diverse\nexploration in RLVR. Extensive experiments show that MENTOR enables models\ncapture the essence of expert strategies rather than surface imitation, thereby\nperforming high-quality exploration and achieving superior overall performance.\nOur code is available online.", "published": "2025-10-05 10:38:55", "link": "http://arxiv.org/abs/2510.04140v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Fine Tuning Methods for Low-resource Languages", "abstract": "The rise of Large Language Models has not been inclusive of all cultures. The\nmodels are mostly trained on English texts and culture which makes them\nunderperform in other languages and cultural contexts. By developing a\ngeneralizable method for preparing culturally relevant datasets and\npost-training the Gemma 2 model, this project aimed to increase the performance\nof Gemma 2 for an underrepresented language and showcase how others can do the\nsame to unlock the power of Generative AI in their country and preserve their\ncultural heritage.", "published": "2025-10-05 10:36:36", "link": "http://arxiv.org/abs/2510.04139v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Internal states before wait modulate reasoning patterns", "abstract": "Prior work has shown that a significant driver of performance in reasoning\nmodels is their ability to reason and self-correct. A distinctive marker in\nthese reasoning traces is the token wait, which often signals reasoning\nbehavior such as backtracking. Despite being such a complex behavior, little is\nunderstood of exactly why models do or do not decide to reason in this\nparticular manner, which limits our understanding of what makes a reasoning\nmodel so effective. In this work, we address the question whether model's\nlatents preceding wait tokens contain relevant information for modulating the\nsubsequent reasoning process. We train crosscoders at multiple layers of\nDeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent\nattribution technique in the crosscoder setting. We locate a small set of\nfeatures relevant for promoting/suppressing wait tokens' probabilities.\nFinally, through a targeted series of experiments analyzing max activating\nexamples and causal interventions, we show that many of our identified features\nindeed are relevant for the reasoning process and give rise to different types\nof reasoning patterns such as restarting from the beginning, recalling prior\nknowledge, expressing uncertainty, and double-checking.", "published": "2025-10-05 10:03:42", "link": "http://arxiv.org/abs/2510.04128v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Sri Lanka Document Datasets: A Large-Scale, Multilingual Resource for Law, News, and Policy (v20251005)", "abstract": "We present a collection of open, machine-readable document datasets covering\nparliamentary proceedings, legal judgments, government publications, news, and\ntourism statistics from Sri Lanka. As of v20251005, the collection currently\ncomprises 215,670 documents (60.3 GB) across 13 datasets in Sinhala, Tamil, and\nEnglish. The datasets are updated daily and mirrored on GitHub and Hugging\nFace. These resources aim to support research in computational linguistics,\nlegal analytics, socio-political studies, and multilingual natural language\nprocessing. We describe the data sources, collection pipeline, formats, and\npotential use cases, while discussing licensing and ethical considerations.", "published": "2025-10-05 09:57:40", "link": "http://arxiv.org/abs/2510.04124v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling LLMs' Metaphorical Understanding: Exploring Conceptual Irrelevance, Context Leveraging and Syntactic Influence", "abstract": "Metaphor analysis is a complex linguistic phenomenon shaped by context and\nexternal factors. While Large Language Models (LLMs) demonstrate advanced\ncapabilities in knowledge integration, contextual reasoning, and creative\ngeneration, their mechanisms for metaphor comprehension remain insufficiently\nexplored. This study examines LLMs' metaphor-processing abilities from three\nperspectives: (1) Concept Mapping: using embedding space projections to\nevaluate how LLMs map concepts in target domains (e.g., misinterpreting \"fall\nin love\" as \"drop down from love\"); (2) Metaphor-Literal Repository: analyzing\nmetaphorical words and their literal counterparts to identify inherent\nmetaphorical knowledge; and (3) Syntactic Sensitivity: assessing how\nmetaphorical syntactic structures influence LLMs' performance. Our findings\nreveal that LLMs generate 15\\%-25\\% conceptually irrelevant interpretations,\ndepend on metaphorical indicators in training data rather than contextual cues,\nand are more sensitive to syntactic irregularities than to structural\ncomprehension. These insights underline the limitations of LLMs in metaphor\nanalysis and call for more robust computational approaches.", "published": "2025-10-05 09:45:51", "link": "http://arxiv.org/abs/2510.04120v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning", "abstract": "Reasoning capability is pivotal for Large Language Models (LLMs) to solve\ncomplex tasks, yet achieving reliable and scalable reasoning remains\nchallenging. While Chain-of-Thought (CoT) prompting has become a mainstream\napproach, existing methods often suffer from uncontrolled generation,\ninsufficient quality, and limited diversity in reasoning paths. Recent efforts\nleverage code to enhance CoT by grounding reasoning in executable steps, but\nsuch methods are typically constrained to predefined mathematical problems,\nhindering scalability and generalizability. In this work, we propose Caco\n(Code-Assisted Chain-of-ThOught), a novel framework that automates the\nsynthesis of high-quality, verifiable, and diverse instruction-CoT reasoning\ndata through code-driven augmentation. Unlike prior work, Caco first fine-tunes\na code-based CoT generator on existing math and programming solutions in a\nunified code format, then scales the data generation to a large amount of\ndiverse reasoning traces. Crucially, we introduce automated validation via code\nexecution and rule-based filtering to ensure logical correctness and structural\ndiversity, followed by reverse-engineering filtered outputs into natural\nlanguage instructions and language CoTs to enrich task adaptability. This\nclosed-loop process enables fully automated, scalable synthesis of reasoning\ndata with guaranteed executability. Experiments on our created Caco-1.3M\ndataset demonstrate that Caco-trained models achieve strong competitive\nperformance on mathematical reasoning benchmarks, outperforming existing strong\nbaselines. Further analysis reveals that Caco's code-anchored verification and\ninstruction diversity contribute to superior generalization across unseen\ntasks. Our work establishes a paradigm for building self-sustaining,\ntrustworthy reasoning systems without human intervention.", "published": "2025-10-05 07:59:24", "link": "http://arxiv.org/abs/2510.04081v1", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity", "abstract": "Conditional Semantic Textual Similarity (C-STS) measures the semantic\nproximity between text segments under a specific condition, thereby overcoming\nthe ambiguity inherent in traditional STS. However, existing methods are\nlargely confined to discriminative models, failing to fully integrate recent\nbreakthroughs in the NLP community concerning Large Language Models (LLMs) and\nReinforcement Learning (RL). RL is a particularly well-suited paradigm for this\ntask, as it can directly optimize the non-differentiable Spearman ranking\nmetric and guide the reasoning process required by C-STS. However, we find that\nnaively applying listwise RL fails to produce meaningful improvements, as the\nmodel is overwhelmed by complex, coarse-grained reward signals. To address this\nchallenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning\nframework. PoLi-RL employs a two-stage curriculum: it first trains the model\nwith simple pointwise rewards to establish fundamental scoring capabilities,\nthen transitions to a hybrid reward that combines pointwise, pairwise, and\nlistwise objectives to refine the model's ability to discern subtle semantic\ndistinctions. Crucially, we propose an innovative Parallel Slice Ranking Reward\n(PSRR) mechanism that computes ranking rewards in parallel slices, where each\nslice comprises same-indexed completions from different samples. This provides\na precise, differentiated learning signal for each individual completion,\nenabling granular credit assignment and effective optimization. On the official\nC-STS benchmark, PoLi-RL achieves a Spearman correlation coefficient of 48.18,\nestablishing a new SOTA for the cross-encoder architecture. As the first work\nto successfully apply RL to C-STS, our study introduces a powerful and precise\nparadigm for training LLMs on complex, ranking-based conditional judgment\ntasks.", "published": "2025-10-05 07:57:26", "link": "http://arxiv.org/abs/2510.04080v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive double-phase Rudin--Osher--Fatemi denoising model", "abstract": "We propose a new image denoising model based on a variable-growth total\nvariation regularization of double-phase type with adaptive weight. It is\ndesigned to reduce staircasing with respect to the classical\nRudin--Osher--Fatemi model, while preserving the edges of the image in a\nsimilar fashion. We implement the model and test its performance on synthetic\nand natural images in 1D and 2D over a range of noise levels.", "published": "2025-10-05 22:26:06", "link": "http://arxiv.org/abs/2510.04382v1", "categories": ["eess.IV", "cs.CV", "cs.NA", "math.NA"], "primary_category": "eess.IV"}
{"title": "The method of the approximate inverse for limited-angle CT", "abstract": "Limited-angle computerized tomography stands for one of the most difficult\nchallenges in imaging. Although it opens the way to faster data acquisition in\nindustry and less dangerous scans in medicine, standard approaches, such as the\nfiltered backprojection (FBP) algorithm or the widely used total-variation\nfunctional, often produce various artefacts that hinder the diagnosis. With the\nrise of deep learning, many modern techniques have proven themselves successful\nin removing such artefacts but at the cost of large datasets. In this paper, we\npropose a new model-driven approach based on the method of the approximate\ninverse, which could serve as new starting point for learning strategies in the\nfuture. In contrast to FBP-type approaches, our reconstruction step consists in\nevaluating linear functionals on the measured data using reconstruction kernels\nthat are precomputed as solution of an auxiliary problem. With this problem\nbeing uniquely solvable, the derived limited-angle reconstruction kernel (LARK)\nis able to fully reconstruct the object without the well-known streak\nartefacts, even for large limited angles. However, it inherits severe\nill-conditioning which leads to a different kind of artefacts arising from the\nsingular functions of the limited-angle Radon transform. The problem becomes\nparticularly challenging when working on semi-discrete (real or analytical)\nmeasurements. We develop a general regularization strategy, named constrained\nlimited-angle reconstruction kernel (CLARK), by combining spectral filter, the\nmethod of the approximate inverse and custom edge-preserving denoising in order\nto stabilize the whole process. We further derive and interpret error estimates\nfor the application on real, i.e. semi-discrete, data and we validate our\napproach on synthetic and real data.", "published": "2025-10-05 21:24:44", "link": "http://arxiv.org/abs/2510.04369v1", "categories": ["eess.IV", "cs.CV", "cs.NA", "math.NA"], "primary_category": "eess.IV"}
{"title": "Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction", "abstract": "Accurate pedestrian trajectory prediction is crucial for ensuring safety and\nefficiency in autonomous driving and human-robot interaction scenarios. Earlier\nstudies primarily utilized sufficient observational data to predict future\ntrajectories. However, in real-world scenarios, such as pedestrians suddenly\nemerging from blind spots, sufficient observational data is often unavailable\n(i.e. momentary trajectory), making accurate prediction challenging and\nincreasing the risk of traffic accidents. Therefore, advancing research on\npedestrian trajectory prediction under extreme scenarios is critical for\nenhancing traffic safety. In this work, we propose a novel framework termed\nDiffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists\nof two sequentially connected diffusion models: one for backward prediction,\nwhich generates unobserved historical trajectories, and the other for forward\nprediction, which forecasts future trajectories. Given that the generated\nunobserved historical trajectories may introduce additional noise, we propose a\ndual-head parameterization mechanism to estimate their aleatoric uncertainty\nand design a temporally adaptive noise module that dynamically modulates the\nnoise scale in the forward diffusion process. Empirically, Diffusion^2 sets a\nnew state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford\nDrone datasets.", "published": "2025-10-05 21:19:33", "link": "http://arxiv.org/abs/2510.04365v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "RAP: 3D Rasterization Augmented End-to-End Planning", "abstract": "Imitation learning for end-to-end driving trains policies only on expert\ndemonstrations. Once deployed in a closed loop, such policies lack recovery\ndata: small mistakes cannot be corrected and quickly compound into failures. A\npromising direction is to generate alternative viewpoints and trajectories\nbeyond the logged path. Prior work explores photorealistic digital twins via\nneural rendering or game engines, but these methods are prohibitively slow and\ncostly, and thus mainly used for evaluation. In this work, we argue that\nphotorealism is unnecessary for training end-to-end planners. What matters is\nsemantic fidelity and scalability: driving depends on geometry and dynamics,\nnot textures or lighting. Motivated by this, we propose 3D Rasterization, which\nreplaces costly rendering with lightweight rasterization of annotated\nprimitives, enabling augmentations such as counterfactual recovery maneuvers\nand cross-agent view synthesis. To transfer these synthetic views effectively\nto real-world deployment, we introduce a Raster-to-Real feature-space alignment\nthat bridges the sim-to-real gap. Together, these components form Rasterization\nAugmented Planning (RAP), a scalable data augmentation pipeline for planning.\nRAP achieves state-of-the-art closed-loop robustness and long-tail\ngeneralization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo\nOpen Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that\nlightweight rasterization with feature alignment suffices to scale E2E\ntraining, offering a practical alternative to photorealistic rendering. Project\npage: https://alan-lanfeng.github.io/RAP/.", "published": "2025-10-05 19:31:24", "link": "http://arxiv.org/abs/2510.04333v1", "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV"}
{"title": "DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks", "abstract": "Parameter-efficient fine-tuning (PEFT) methods have become the standard\nparadigm for adapting large-scale models. Among these techniques,\nWeight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the\nlearning capacity and training stability of the vanilla Low-Rank Adaptation\n(LoRA) method by explicitly decomposing pre-trained weights into magnitude and\ndirectional components. In this work, we propose DoRAN, a new variant of DoRA\ndesigned to further stabilize training and boost the sample efficiency of DoRA.\nOur approach includes two key stages: (i) injecting noise into the denominator\nof DoRA's weight decomposition, which serves as an adaptive regularizer to\nmitigate instabilities; and (ii) replacing static low-rank matrices with\nauxiliary networks that generate them dynamically, enabling parameter coupling\nacross layers and yielding better sample efficiency in both theory and\npractice. Comprehensive experiments on vision and language benchmarks show that\nDoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These\nresults underscore the effectiveness of combining stabilization through\nnoise-based regularization with network-based parameter generation, offering a\npromising direction for robust and efficient fine-tuning of foundation models.", "published": "2025-10-05 19:27:48", "link": "http://arxiv.org/abs/2510.04331v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "GenAR: Next-Scale Autoregressive Generation for Spatial Gene Expression Prediction", "abstract": "Spatial Transcriptomics (ST) offers spatially resolved gene expression but\nremains costly. Predicting expression directly from widely available\nHematoxylin and Eosin (H&E) stained images presents a cost-effective\nalternative. However, most computational approaches (i) predict each gene\nindependently, overlooking co-expression structure, and (ii) cast the task as\ncontinuous regression despite expression being discrete counts. This mismatch\ncan yield biologically implausible outputs and complicate downstream analyses.\nWe introduce GenAR, a multi-scale autoregressive framework that refines\npredictions from coarse to fine. GenAR clusters genes into hierarchical groups\nto expose cross-gene dependencies, models expression as codebook-free discrete\ntoken generation to directly predict raw counts, and conditions decoding on\nfused histological and spatial embeddings. From an information-theoretic\nperspective, the discrete formulation avoids log-induced biases and the\ncoarse-to-fine factorization aligns with a principled conditional\ndecomposition. Extensive experimental results on four Spatial Transcriptomics\ndatasets across different tissue types demonstrate that GenAR achieves\nstate-of-the-art performance, offering potential implications for precision\nmedicine and cost-effective molecular profiling. Code is publicly available at\nhttps://github.com/oyjr/genar.", "published": "2025-10-05 18:28:21", "link": "http://arxiv.org/abs/2510.04315v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "CARE-PD: A Multi-Site Anonymized Clinical Dataset for Parkinson's Disease Gait Assessment", "abstract": "Objective gait assessment in Parkinson's Disease (PD) is limited by the\nabsence of large, diverse, and clinically annotated motion datasets. We\nintroduce CARE-PD, the largest publicly available archive of 3D mesh gait data\nfor PD, and the first multi-site collection spanning 9 cohorts from 8 clinical\ncenters. All recordings (RGB video or motion capture) are converted into\nanonymized SMPL meshes via a harmonized preprocessing pipeline. CARE-PD\nsupports two key benchmarks: supervised clinical score prediction (estimating\nUnified Parkinson's Disease Rating Scale, UPDRS, gait scores) and unsupervised\nmotion pretext tasks (2D-to-3D keypoint lifting and full-body 3D\nreconstruction). Clinical prediction is evaluated under four generalization\nprotocols: within-dataset, cross-dataset, leave-one-dataset-out, and\nmulti-dataset in-domain adaptation. To assess clinical relevance, we compare\nstate-of-the-art motion encoders with a traditional gait-feature baseline,\nfinding that encoders consistently outperform handcrafted features. Pretraining\non CARE-PD reduces MPJPE (from 60.8mm to 7.5mm) and boosts PD severity macro-F1\nby 17 percentage points, underscoring the value of clinically curated, diverse\ntraining data. CARE-PD and all benchmark code are released for non-commercial\nresearch at https://neurips2025.care-pd.ca/.", "published": "2025-10-05 18:14:50", "link": "http://arxiv.org/abs/2510.04312v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation", "abstract": "Recent advances in large generative models have significantly advanced image\nediting and in-context image generation, yet a critical gap remains in ensuring\nphysical consistency, where edited objects must remain coherent. This\ncapability is especially vital for world simulation related tasks. In this\npaper, we present ChronoEdit, a framework that reframes image editing as a\nvideo generation problem. First, ChronoEdit treats the input and edited images\nas the first and last frames of a video, allowing it to leverage large\npretrained video generative models that capture not only object appearance but\nalso the implicit physics of motion and interaction through learned temporal\nconsistency. Second, ChronoEdit introduces a temporal reasoning stage that\nexplicitly performs editing at inference time. Under this setting, the target\nframe is jointly denoised with reasoning tokens to imagine a plausible editing\ntrajectory that constrains the solution space to physically viable\ntransformations. The reasoning tokens are then dropped after a few steps to\navoid the high computational cost of rendering a full video. To validate\nChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for\ncontexts that require physical consistency, and demonstrate that ChronoEdit\nsurpasses state-of-the-art baselines in both visual fidelity and physical\nplausibility. Code and models for both the 14B and 2B variants of ChronoEdit\nwill be released on the project page:\nhttps://research.nvidia.com/labs/toronto-ai/chronoedit", "published": "2025-10-05 17:02:01", "link": "http://arxiv.org/abs/2510.04290v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Flexible and Efficient Spatio-Temporal Transformer for Sequential Visual Place Recognition", "abstract": "Sequential Visual Place Recognition (Seq-VPR) leverages transformers to\ncapture spatio-temporal features effectively; however, existing approaches\nprioritize performance at the expense of flexibility and efficiency. In\npractice, a transformer-based Seq-VPR model should be flexible to the number of\nframes per sequence (seq-length), deliver fast inference, and have low memory\nusage to meet real-time constraints. To our knowledge, no existing\ntransformer-based Seq-VPR method achieves both flexibility and efficiency. To\naddress this gap, we propose Adapt-STformer, a Seq-VPR method built around our\nnovel Recurrent Deformable Transformer Encoder (Recurrent-DTE), which uses an\niterative recurrent mechanism to fuse information from multiple sequential\nframes. This design naturally supports variable seq-lengths, fast inference,\nand low memory usage. Experiments on the Nordland, Oxford, and NuScenes\ndatasets show that Adapt-STformer boosts recall by up to 17% while reducing\nsequence extraction time by 36% and lowering memory usage by 35% compared to\nthe second-best baseline.", "published": "2025-10-05 16:52:12", "link": "http://arxiv.org/abs/2510.04282v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks", "abstract": "Adversarial patch attacks pose a practical threat to deep learning models by\nforcing targeted misclassifications through localized perturbations, often\nrealized in the physical world. Existing defenses typically assume prior\nknowledge of patch size or location, limiting their applicability. In this\nwork, we propose a patch-agnostic defense that leverages concept-based\nexplanations to identify and suppress the most influential concept activation\nvectors, thereby neutralizing patch effects without explicit detection.\nEvaluated on Imagenette with a ResNet-50, our method achieves higher robust and\nclean accuracy than the state-of-the-art PatchCleanser, while maintaining\nstrong performance across varying patch sizes and locations. Our results\nhighlight the promise of combining interpretability with robustness and suggest\nconcept-driven defenses as a scalable strategy for securing machine learning\nmodels against adversarial patch attacks.", "published": "2025-10-05 15:26:03", "link": "http://arxiv.org/abs/2510.04245v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast): Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and Test-Time Adaptation", "abstract": "Accurate liver segmentation from contrast-enhanced MRI is essential for\ndiagnosis, treatment planning, and disease monitoring. However, it remains\nchallenging due to limited annotated data, heterogeneous enhancement protocols,\nand significant domain shifts across scanners and institutions. Traditional\nimage-to-image translation frameworks have made great progress in domain\ngeneralization, but their application is not straightforward. For example,\nPix2Pix requires image registration, and cycle-GAN cannot be integrated\nseamlessly into segmentation pipelines. Meanwhile, these methods are originally\nused to deal with cross-modality scenarios, and often introduce structural\ndistortions and suffer from unstable training, which may pose drawbacks in our\nsingle-modality scenario. To address these challenges, we propose CoSSeg-TTA, a\ncompact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary\nphase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised\nmean teacher scheme to exploit large amounts of unlabeled volumes. A domain\nadaptation module, incorporating a randomized histogram-based style appearance\ntransfer function and a trainable contrast-aware network, enriches domain\ndiversity and mitigates cross-center variability. Furthermore, a continual\ntest-time adaptation strategy is employed to improve robustness during\ninference. Extensive experiments demonstrate that our framework consistently\noutperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff\nDistance while exhibiting strong generalization to unseen domains under\nlow-annotation conditions.", "published": "2025-10-05 15:18:53", "link": "http://arxiv.org/abs/2510.04243v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Scaling Sequence-to-Sequence Generative Neural Rendering", "abstract": "We present Kaleido, a family of generative models designed for\nphotorealistic, unified object- and scene-level neural rendering. Kaleido\noperates on the principle that 3D can be regarded as a specialised sub-domain\nof video, expressed purely as a sequence-to-sequence image synthesis task.\nThrough a systemic study of scaling sequence-to-sequence generative neural\nrendering, we introduce key architectural innovations that enable our model to:\ni) perform generative view synthesis without explicit 3D representations; ii)\ngenerate any number of 6-DoF target views conditioned on any number of\nreference views via a masked autoregressive framework; and iii) seamlessly\nunify 3D and video modelling within a single decoder-only rectified flow\ntransformer. Within this unified framework, Kaleido leverages large-scale video\ndata for pre-training, which significantly improves spatial consistency and\nreduces reliance on scarce, camera-labelled 3D datasets -- all without any\narchitectural modifications. Kaleido sets a new state-of-the-art on a range of\nview synthesis benchmarks. Its zero-shot performance substantially outperforms\nother generative methods in few-view settings, and, for the first time, matches\nthe quality of per-scene optimisation methods in many-view settings.", "published": "2025-10-05 15:03:31", "link": "http://arxiv.org/abs/2510.04236v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Detection of retinal diseases using an accelerated reused convolutional network", "abstract": "Convolutional neural networks are continually evolving, with some efforts\naimed at improving accuracy, others at increasing speed, and some at enhancing\naccessibility. Improving accessibility broadens the application of neural\nnetworks across a wider range of tasks, including the detection of eye\ndiseases. Early diagnosis of eye diseases and consulting an ophthalmologist can\nprevent many vision disorders. Given the importance of this issue, various\ndatasets have been collected from the cornea to facilitate the process of\nmaking neural network models. However, most of the methods introduced in the\npast are computationally complex. In this study, we tried to increase the\naccessibility of deep neural network models. We did this at the most\nfundamental level, specifically by redesigning and optimizing the convolutional\nlayers. By doing so, we created a new general model that incorporates our novel\nconvolutional layer named ArConv layers. Thanks to the efficient performance of\nthis new layer, the model has suitable complexity for use in mobile phones and\ncan perform the task of diagnosing the presence of disease with high accuracy.\nThe final model we present contains only 1.3 million parameters. In comparison\nto the MobileNetV2 model, which has 2.2 million parameters, our model\ndemonstrated better accuracy when trained and evaluated on the RfMiD dataset\nunder identical conditions, achieving an accuracy of 0.9328 versus 0.9266 on\nthe RfMiD test set.", "published": "2025-10-05 14:44:09", "link": "http://arxiv.org/abs/2510.04232v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Recursive Pyramidal Algorithm for Solving the Image Registration Problem", "abstract": "The problem of image registration is finding a transformation that aligns two\nimages, such that the corresponding points are in the same location. This paper\nintroduces a simple, end-to-end trainable algorithm that is implementable in a\nfew lines of Python code. The approach is shown to work with very little\ntraining data and training time, while achieving accurate results in some\nsettings. An example application to stereo vision was trained from 74 images on\na 19x15 input window. With just a dozen lines of Python code this algorithm\nexcels in brevity and may serve as a good start in related scenarios with\nlimitations to training data, training time or code complexity.", "published": "2025-10-05 14:44:04", "link": "http://arxiv.org/abs/2510.04231v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering", "abstract": "Autoregressive (AR) models have shown great promise in image generation, yet\nthey face a fundamental inefficiency stemming from their core component: a\nvast, unstructured vocabulary of visual tokens. This conventional approach\ntreats tokens as a flat vocabulary, disregarding the intrinsic structure of the\ntoken embedding space where proximity often correlates with semantic\nsimilarity. This oversight results in a highly complex prediction task, which\nhinders training efficiency and limits final generation quality. To resolve\nthis, we propose Manifold-Aligned Semantic Clustering (MASC), a principled\nframework that constructs a hierarchical semantic tree directly from the\ncodebook's intrinsic structure. MASC employs a novel geometry-aware distance\nmetric and a density-driven agglomerative construction to model the underlying\nmanifold of the token embeddings. By transforming the flat, high-dimensional\nprediction task into a structured, hierarchical one, MASC introduces a\nbeneficial inductive bias that significantly simplifies the learning problem\nfor the AR model. MASC is designed as a plug-and-play module, and our extensive\nexperiments validate its effectiveness: it accelerates training by up to 57%\nand significantly improves generation quality, reducing the FID of LlamaGen-XL\nfrom 2.87 to 2.58. MASC elevates existing AR frameworks to be highly\ncompetitive with state-of-the-art methods, establishing that structuring the\nprediction space is as crucial as architectural innovation for scalable\ngenerative modeling.", "published": "2025-10-05 14:23:51", "link": "http://arxiv.org/abs/2510.04220v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge", "abstract": "While text-to-image (T2I) models can synthesize high-quality images, their\nperformance degrades significantly when prompted with novel or\nout-of-distribution (OOD) entities due to inherent knowledge cutoffs. We\nintroduce World-To-Image, a novel framework that bridges this gap by empowering\nT2I generation with agent-driven world knowledge. We design an agent that\ndynamically searches the web to retrieve images for concepts unknown to the\nbase model. This information is then used to perform multimodal prompt\noptimization, steering powerful generative backbones toward an accurate\nsynthesis. Critically, our evaluation goes beyond traditional metrics,\nutilizing modern assessments like LLMGrader and ImageReward to measure true\nsemantic fidelity. Our experiments show that World-To-Image substantially\noutperforms state-of-the-art methods in both semantic alignment and visual\naesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated\nNICE benchmark. Our framework achieves these results with high efficiency in\nless than three iterations, paving the way for T2I systems that can better\nreflect the ever-changing real world. Our demo code is available\nhere\\footnote{https://github.com/mhson-kyle/World-To-Image}.", "published": "2025-10-05 13:35:30", "link": "http://arxiv.org/abs/2510.04201v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Let Features Decide Their Own Solvers: Hybrid Feature Caching for Diffusion Transformers", "abstract": "Diffusion Transformers offer state-of-the-art fidelity in image and video\nsynthesis, but their iterative sampling process remains a major bottleneck due\nto the high cost of transformer forward passes at each timestep. To mitigate\nthis, feature caching has emerged as a training-free acceleration technique\nthat reuses or forecasts hidden representations. However, existing methods\noften apply a uniform caching strategy across all feature dimensions, ignoring\ntheir heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by\nmodeling hidden feature evolution as a mixture of ODEs across dimensions, and\nintroduce HyCa, a Hybrid ODE solver inspired caching framework that applies\ndimension-wise caching strategies. HyCa achieves near-lossless acceleration\nacross diverse domains and models, including 5.55 times speedup on FLUX, 5.56\ntimes speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and\nQwen-Image-Edit without retraining.", "published": "2025-10-05 13:01:08", "link": "http://arxiv.org/abs/2510.04188v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "From Segments to Concepts: Interpretable Image Classification via Concept-Guided Segmentation", "abstract": "Deep neural networks have achieved remarkable success in computer vision;\nhowever, their black-box nature in decision-making limits interpretability and\ntrust, particularly in safety-critical applications. Interpretability is\ncrucial in domains where errors have severe consequences. Existing models not\nonly lack transparency but also risk exploiting unreliable or misleading\nfeatures, which undermines both robustness and the validity of their\nexplanations. Concept Bottleneck Models (CBMs) aim to improve transparency by\nreasoning through human-interpretable concepts. Still, they require costly\nconcept annotations and lack spatial grounding, often failing to identify which\nregions support each concept. We propose SEG-MIL-CBM, a novel framework that\nintegrates concept-guided image segmentation into an attention-based multiple\ninstance learning (MIL) framework, where each segmented region is treated as an\ninstance and the model learns to aggregate evidence across them. By reasoning\nover semantically meaningful regions aligned with high-level concepts, our\nmodel highlights task-relevant evidence, down-weights irrelevant cues, and\nproduces spatially grounded, concept-level explanations without requiring\nannotations of concepts or groups. SEG-MIL-CBM achieves robust performance\nacross settings involving spurious correlations (unintended dependencies\nbetween background and label), input corruptions (perturbations that degrade\nvisual quality), and large-scale benchmarks, while providing transparent,\nconcept-level explanations.", "published": "2025-10-05 12:48:43", "link": "http://arxiv.org/abs/2510.04180v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "BLADE: Bias-Linked Adaptive DEbiasing", "abstract": "Neural networks have revolutionized numerous fields, yet they remain\nvulnerable to a critical flaw: the tendency to learn implicit biases, spurious\ncorrelations between certain attributes and target labels in training data.\nThese biases are often more prevalent and easier to learn, causing models to\nrely on superficial patterns rather than task-relevant features necessary for\ngeneralization. Existing methods typically rely on strong assumptions, such as\nprior knowledge of these biases or access to bias-conflicting samples, i.e.,\nsamples that contradict spurious correlations and counterbalance bias-aligned\nsamples, samples that conform to these spurious correlations. However, such\nassumptions are often impractical in real-world settings. We propose BLADE\n({B}ias-{L}inked {A}daptive {DE}biasing), a generative debiasing framework that\nrequires no prior knowledge of bias or bias-conflicting samples. BLADE first\ntrains a generative model to translate images across bias domains while\npreserving task-relevant features. Then, it adaptively refines each image with\nits synthetic counterpart based on the image's susceptibility to bias. To\nencourage robust representations, BLADE aligns an image with its\nbias-translated synthetic counterpart that shares task-relevant features but\ndiffers in bias, while misaligning it with samples sharing the same bias. We\nevaluate BLADE on multiple benchmark datasets and show that it significantly\noutperforms state-of-the-art methods. Notably, it exceeds the closest baseline\nby an absolute margin of around 18% on the corrupted CIFAR-10 dataset under the\nworst group setting, establishing a new benchmark in bias mitigation and\ndemonstrating its potential for developing more robust deep learning models\nwithout explicit supervision.", "published": "2025-10-05 12:28:54", "link": "http://arxiv.org/abs/2510.04174v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs", "abstract": "This paper identifies a critical yet underexplored challenge in distilling\nfrom multimodal large language models (MLLMs): the reasoning trajectories\ngenerated by multiple drifting teachers exhibit concept drift, whereby their\nreasoning distributions evolve unpredictably and transmit biases to the student\nmodel, ultimately compromising its performance. To tackle this issue, we\npioneer a theoretical connection between concept drift and knowledge\ndistillation, casting the non-stationary reasoning dynamics from multiple MLLM\nteachers as next-token prediction of multi-stream reasoning trajectories.Guided\nby concept drift, we introduce the \"learn, compare, critique\" paradigm,\nculminating in autonomous preference optimization (APO). Under the active\nguidance of the teachers, the student model first learns and self-distils\npreferred thinking by comparing multiple teachers. It then engages in critical\nreflection over the drifting inference from teachers, performing concept\nalignment through APO, ultimately yielding a robust, consistent, and\ngeneralizable model.Extensive experiments demonstrate our superior performance\nof consistency, robustness and generalization within knowledge distillation.\nBesides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers\nAlignment X-rays), comprising 170,982 distilled reasoning trajectories derived\nfrom publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public\nat: https://anonymous.4open.science/r/Autonomous-Distillation/.", "published": "2025-10-05 10:42:21", "link": "http://arxiv.org/abs/2510.04142v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition", "abstract": "Large language models (LLMs) have recently shown strong potential in\naudio-visual speech recognition (AVSR), but their high computational demands\nand sensitivity to token granularity limit their practicality in\nresource-constrained settings. Token compression methods can reduce inference\ncost, but they require fixing a compression rate in advance and produce a\nsingle fixed-length output, offering no flexibility to balance information\ndensity and efficiency at inference time. Matryoshka representation learning\n(MRL) addresses this by enabling a single model to operate across multiple\ntoken granularities, allowing compression rates to be adjusted dynamically.\nHowever, current MRL-based methods treat each scale independently during\ntraining, limiting cross-scale generalization, robustness at high compression,\nand interpretability. To overcome these limitations, we propose MoME (Mixture\nof Matryoshka Experts), a novel framework that integrates sparse\nMixture-of-Experts (MoE) into MRL-based LLMs for AVSR. MoME augments a frozen\nLLM with top-k routed and shared experts, allowing dynamic capacity allocation\nacross scales and modalities. A shared router promotes consistent expert\nactivation across granularities, enabling compressed sequences to benefit from\nrepresentations learned at lower compression. Experiments on LRS2 and LRS3\ndemonstrate that MoME achieves state-of-the-art performance across AVSR, ASR,\nand VSR tasks, while requiring significantly fewer parameters and maintaining\nrobustness under noise. MoME unifies the adaptability of MRL with the\nefficiency of MoE, offering a scalable and interpretable solution for\nresource-aware speech recognition.", "published": "2025-10-05 10:34:34", "link": "http://arxiv.org/abs/2510.04136v1", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Distinct Distances on Pfaffian Curves", "abstract": "We generalize Pach and de Zeeuw's bound for distinct distances between points\non two curves, from algebraic curves to Pfaffian curves. Pfaffian curves\ninclude those that can be defined by any combination of elementary functions,\nincluding exponential and logarithmic functions, rational and irrational\npowers, trigonometric functions and their inverses, integration, and more. The\nbound remains $\\Omega(\\min\\{m^{3/4}n^{3/4},m^2,n^2\\})$, as obtained from the\nproximity technique of Solymosi and Zahl.", "published": "2025-10-05 20:01:00", "link": "http://arxiv.org/abs/2510.04337v1", "categories": ["math.MG", "cs.DM", "math.CO", "math.LO"], "primary_category": "math.MG"}
{"title": "On the Modular Chromatic Index of Random Hypergraphs", "abstract": "Let $k,r \\geq 2$ be two integers. We consider the problem of partitioning the\nhyperedge set of an $r$-uniform hypergraph $H$ into the minimum number\n$\\chi_k'(H)$ of edge-disjoint subhypergraphs in which every vertex has either\ndegree $0$ or degree congruent to $1$ modulo $k$. For a random hypergraph $H$\ndrawn from the binomial model $\\mathbf{H}(n,p,r)$, with edge probability $p \\in\n(C\\log(n)/n,1)$ for a large enough constant $C>0$ independent of $n$ and\nsatisfying $n^{r-1}p(1-p)\\to\\infty$ as $n\\to\\infty$, we show that\nasymptotically almost surely $\\chi_k'(H) = k$ if $n$ is divisible by\n$\\gcd(k,r)$, and $\\max(k,r) \\le \\chi_k'(H) \\le k+r+1$ otherwise. A key\ningredient in our approach is a sufficient condition ensuring the existence of\na $k$-factor, a $k$-regular spanning subhypergraph, within subhypergraphs of a\nrandom hypergraph from $\\mathbf{H}(n,p,r)$, a result that may be of independent\ninterest. Our main result extends a theorem of Botler, Colucci, and Kohayakawa\n(2023), who proved an analogous statement for graphs, and provides a partial\nanswer to a question posed by Goetze, Klute, Knauer, Parada, Pe\\~na, and\nUeckerdt (2025) regarding whether $\\chi_2'(H)$ can be bounded by a constant for\nevery hypergraph $H$.", "published": "2025-10-05 19:36:56", "link": "http://arxiv.org/abs/2510.04334v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Vector Trifference", "abstract": "We investigate a geometric generalization of trifference, a concept\nintroduced by Elias in 1988 in the study of zero-error channel capacity. In the\ndiscrete setting, a code C \\subseteq {0,1,2}^n is trifferent if for any three\ndistinct codewords x, y, z in C, there exists a coordinate i in [n] where x_i,\ny_i, z_i are all distinct. Determining the maximum size of such codes remains a\ncentral open problem; the classical upper bound |C| \\leq 2 * (3/2)^n, proved\nvia a simple pruning argument, has resisted significant improvement.\n  Motivated by the search for new techniques, and in line with vectorial\nextensions of other classical combinatorial notions, we introduce the concept\nof vector trifferent codes. Consider C \\subseteq (S^2)^n, where the alphabet is\nthe unit sphere S^2 = { v in R^3 : ||v|| = 1 }. We say C is vector trifferent\nif for any three distinct x, y, z in C, there is an index i where the vectors\nx_i, y_i, z_i are mutually orthogonal. A direct reduction of the vectorial\nproblem to the discrete setting appears infeasible, making it difficult to\nreplicate Elias's pruning argument. Nevertheless, we develop a new method to\nestablish the upper bound |C| \\leq (sqrt(2) + o(1)) * (3/2)^n.\n  Interestingly, our approach, when adapted back to the discrete setting,\nyields a polynomial improvement to Elias's bound: |C| \\lesssim n^(-1/4) *\n(3/2)^n. This improvement arises from a technique that parallels, but is not\nidentical to, a recent method of the authors, though it still falls short of\nthe sharper n^(-2/5) factor obtained there. We also generalize the concept of\nvector trifferent codes to richer alphabets and prove a vectorial version of\nthe Fredman-Komlos theorem (1984) for general k-separating codes.", "published": "2025-10-05 07:52:41", "link": "http://arxiv.org/abs/2510.04079v1", "categories": ["cs.DM", "math.CO"], "primary_category": "cs.DM"}
{"title": "Evaluating Keyframe Layouts for Visual Known-Item Search in Homogeneous Collections", "abstract": "Multimodal deep-learning models power interactive video retrieval by ranking\nkeyframes in response to textual queries. Despite these advances, users must\nstill browse ranked candidates manually to locate a target. Keyframe\narrangement within the search grid highly affects browsing effectiveness and\nuser efficiency, yet remains underexplored. We report a study with 49\nparticipants evaluating seven keyframe layouts for the Visual Known-Item Search\ntask. Beyond efficiency and accuracy, we relate browsing phenomena, such as\noverlooks, to layout characteristics. Our results show that a video-grouped\nlayout is the most efficient, while a four-column, rank-preserving grid\nachieves the highest accuracy. Sorted grids reveal potentials and trade-offs,\nenabling rapid scanning of uninteresting regions but down-ranking relevant\ntargets to less prominent positions, delaying first arrival times and\nincreasing overlooks.\n  These findings motivate hybrid designs that preserve positions of top-ranked\nitems while sorting or grouping the remainder, and offer guidance for searching\nin grids beyond video retrieval.", "published": "2025-10-05 23:30:33", "link": "http://arxiv.org/abs/2510.04396v1", "categories": ["cs.MM", "cs.IR", "H.3.3; H.5.2; H.5.1"], "primary_category": "cs.MM"}
{"title": "Empowering Denoising Sequential Recommendation with Large Language Model Embeddings", "abstract": "Sequential recommendation aims to capture user preferences by modeling\nsequential patterns in user-item interactions. However, these models are often\ninfluenced by noise such as accidental interactions, leading to suboptimal\nperformance. Therefore, to reduce the effect of noise, some works propose\nexplicitly identifying and removing noisy items. However, we find that simply\nrelying on collaborative information may result in an over-denoising problem,\nespecially for cold items. To overcome these limitations, we propose a novel\nframework: Interest Alignment for Denoising Sequential Recommendation (IADSR)\nwhich integrates both collaborative and semantic information. Specifically,\nIADSR is comprised of two stages: in the first stage, we obtain the\ncollaborative and semantic embeddings of each item from a traditional\nsequential recommendation model and an LLM, respectively. In the second stage,\nwe align the collaborative and semantic embeddings and then identify noise in\nthe interaction sequence based on long-term and short-term interests captured\nin the collaborative and semantic modalities. Our extensive experiments on four\npublic datasets validate the effectiveness of the proposed framework and its\ncompatibility with different sequential recommendation systems.", "published": "2025-10-05 15:10:51", "link": "http://arxiv.org/abs/2510.04239v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "Learning-Based Hashing for ANN Search: Foundations and Early Advances", "abstract": "Approximate Nearest Neighbour (ANN) search is a fundamental problem in\ninformation retrieval, underpinning large-scale applications in computer\nvision, natural language processing, and cross-modal search. Hashing-based\nmethods provide an efficient solution by mapping high-dimensional data into\ncompact binary codes that enable fast similarity computations in Hamming space.\nOver the past two decades, a substantial body of work has explored learning to\nhash, where projection and quantisation functions are optimised from data\nrather than chosen at random.\n  This article offers a foundational survey of early learning-based hashing\nmethods, with an emphasis on the core ideas that shaped the field. We review\nsupervised, unsupervised, and semi-supervised approaches, highlighting how\nprojection functions are designed to generate meaningful embeddings and how\nquantisation strategies convert these embeddings into binary codes. We also\nexamine extensions to multi-bit and multi-threshold models, as well as early\nadvances in cross-modal retrieval.\n  Rather than providing an exhaustive account of the most recent methods, our\ngoal is to introduce the conceptual foundations of learning-based hashing for\nANN search. By situating these early models in their historical context, we aim\nto equip readers with a structured understanding of the principles, trade-offs,\nand open challenges that continue to inform current research in this area.", "published": "2025-10-05 09:59:56", "link": "http://arxiv.org/abs/2510.04127v1", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.IR"}
{"title": "RLRF: Competitive Search Agent Design via Reinforcement Learning from Ranker Feedback", "abstract": "Competitive search is a setting where document publishers modify them to\nimprove their ranking in response to a query. Recently, publishers have\nincreasingly leveraged LLMs to generate and modify competitive content. We\nintroduce Reinforcement Learning from Ranker Feedback (RLRF), a framework that\ntrains LLMs using preference datasets derived from ranking competitions. The\ngoal of a publisher (LLM-based) agent is to optimize content for improved\nranking while accounting for the strategies of competing agents. We generate\nthe datasets using approaches that do not rely on human-authored data. We show\nthat our proposed agents consistently and substantially outperform previously\nsuggested approaches for LLM-based competitive document modification. We\nfurther show that our agents are effective with ranking functions they were not\ntrained for (i.e., out of distribution) and they adapt to strategic opponents.\nThese findings provide support to the significant potential of using\nreinforcement learning in competitive search.", "published": "2025-10-05 08:46:23", "link": "http://arxiv.org/abs/2510.04096v1", "categories": ["cs.IR", "cs.GT", "cs.LG"], "primary_category": "cs.IR"}
{"title": "The LCLStream Ecosystem for Multi-Institutional Dataset Exploration", "abstract": "We describe a new end-to-end experimental data streaming framework designed\nfrom the ground up to support new types of applications -- AI training,\nextremely high-rate X-ray time-of-flight analysis, crystal structure\ndetermination with distributed processing, and custom data science applications\nand visualizers yet to be created. Throughout, we use design choices merging\ncloud microservices with traditional HPC batch execution models for security\nand flexibility. This project makes a unique contribution to the DOE Integrated\nResearch Infrastructure (IRI) landscape. By creating a flexible, API-driven\ndata request service, we address a significant need for high-speed data\nstreaming sources for the X-ray science data analysis community. With the\ncombination of data request API, mutual authentication web security framework,\njob queue system, high-rate data buffer, and complementary nature to facility\ninfrastructure, the LCLStreamer framework has prototyped and implemented\nseveral new paradigms critical for future generation experiments.", "published": "2025-10-05 03:13:38", "link": "http://arxiv.org/abs/2510.04012v1", "categories": ["cs.IR", "physics.ins-det"], "primary_category": "cs.IR"}
{"title": "Visual Lifelog Retrieval through Captioning-Enhanced Interpretation", "abstract": "People often struggle to remember specific details of past experiences, which\ncan lead to the need to revisit these memories. Consequently, lifelog retrieval\nhas emerged as a crucial application. Various studies have explored methods to\nfacilitate rapid access to personal lifelogs for memory recall assistance. In\nthis paper, we propose a Captioning-Integrated Visual Lifelog (CIVIL) Retrieval\nSystem for extracting specific images from a user's visual lifelog based on\ntextual queries. Unlike traditional embedding-based methods, our system first\ngenerates captions for visual lifelogs and then utilizes a text embedding model\nto project both the captions and user queries into a shared vector space.\nVisual lifelogs, captured through wearable cameras, provide a first-person\nviewpoint, necessitating the interpretation of the activities of the individual\nbehind the camera rather than merely describing the scene. To address this, we\nintroduce three distinct approaches: the single caption method, the collective\ncaption method, and the merged caption method, each designed to interpret the\nlife experiences of lifeloggers. Experimental results show that our method\neffectively describes first-person visual images, enhancing the outcomes of\nlifelog retrieval. Furthermore, we construct a textual dataset that converts\nvisual lifelogs into captions, thereby reconstructing personal life\nexperiences.", "published": "2025-10-05 03:00:58", "link": "http://arxiv.org/abs/2510.04010v1", "categories": ["cs.IR", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.IR"}
{"title": "Beyond Static Evaluation: Rethinking the Assessment of Personalized Agent Adaptability in Information Retrieval", "abstract": "Personalized AI agents are becoming central to modern information retrieval,\nyet most evaluation methodologies remain static, relying on fixed benchmarks\nand one-off metrics that fail to reflect how users' needs evolve over time.\nThese limitations hinder our ability to assess whether agents can meaningfully\nadapt to individuals across dynamic, longitudinal interactions. In this\nperspective paper, we propose a conceptual lens for rethinking evaluation in\nadaptive personalization, shifting the focus from static performance snapshots\nto interaction-aware, evolving assessments. We organize this lens around three\ncore components: (1) persona-based user simulation with temporally evolving\npreference models; (2) structured elicitation protocols inspired by reference\ninterviews to extract preferences in context; and (3) adaptation-aware\nevaluation mechanisms that measure how agent behavior improves across sessions\nand tasks. While recent works have embraced LLM-driven user simulation, we\nsituate this practice within a broader paradigm for evaluating agents over\ntime. To illustrate our ideas, we conduct a case study in e-commerce search\nusing the PersonalWAB dataset. Beyond presenting a framework, our work lays a\nconceptual foundation for understanding and evaluating personalization as a\ncontinuous, user-centric endeavor.", "published": "2025-10-05 00:35:37", "link": "http://arxiv.org/abs/2510.03984v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Geometry of Distance Protection", "abstract": "Distance relays detect faults on transmission lines. They face uncertainty\nfrom the fault's location and resistance, as well as the current from the\nline's remote terminal. In this paper, we aggregate this uncertainty with the\nMinkowski sum. This allows us to explicitly model the power grid surrounding\nthe relay's line, and in turn accommodate any mix of synchronous machines and\ninverter-based resources. To make the relay's task easier, inverters can inject\nperturbations, or auxiliary signals, such as negative-sequence current. We use\nFarkas' lemma to construct an optimization for designing inverter auxiliary\nsignals.", "published": "2025-10-05 21:55:37", "link": "http://arxiv.org/abs/2510.04379v1", "categories": ["math.OC", "cs.IT", "cs.SY", "eess.SY", "math.IT"], "primary_category": "math.OC"}
{"title": "Relative Divergence and Maximum Relative Divergence Principle for Grading Functions on Partially Ordered Sets", "abstract": "Relative Divergence (RD) and Maximum Relative Divergence Principle (MRDP) for\ngrading (order-comonotonic) functions (GF) on posets are used as an expression\nof Insufficient Reason Principle under the given prior information (IRP+).\nClassic Probability Theory formulas are presented as IRP+ solutions of MRDP\nproblems on conjoined posets. RD definition principles are analyzed in relation\nto the poset structure. MRDP techniques are presented for standard posets:\npower sets, direct products of chains, etc. \"Population group-testing\" and\n\"Single server of multiple queues\" applications are stated and analyzed as\n\"IRP+ by MRDP\" problems on conjoined base posets.", "published": "2025-10-05 18:24:13", "link": "http://arxiv.org/abs/2510.04314v1", "categories": ["cs.IT", "math.IT", "Primary 94, Secondary 90"], "primary_category": "cs.IT"}
{"title": "Ambidextrous Degree Sequence Bounds for Pessimistic Cardinality Estimation", "abstract": "In a large database system, upper-bounding the cardinality of a join query is\na crucial task called $\\textit{pessimistic cardinality estimation}$. Recently,\nAbo Khamis, Nakos, Olteanu, and Suciu unified related works into the following\ndexterous framework. Step 1: Let $(X_1, \\dotsc, X_n)$ be a random row of the\njoin, equating $H(X_1, \\dotsc, X_n)$ to the log of the join cardinality. Step\n2: Upper-bound $H(X_1, \\dotsc, X_n)$ using Shannon-type inequalities such as\n$H(X, Y, Z) \\le H(X) + H(Y|X) + H(Z|Y)$. Step 3: Upper-bound $H(X_i) + p H(X_j\n| X_i)$ using the $p$-norm of the degree sequence of the underlying graph of a\nrelation.\n  While old bound in step 3 count \"claws $\\in$\" in the underlying graph, we\nproposed $\\textit{ambidextrous}$ bounds that count \"claw pairs\n${\\ni}\\!{-}\\!{\\in}$\". The new bounds are provably not looser and empirically\ntighter: they overestimate by $x^{3/4}$ times when the old bounds overestimate\nby $x$ times. An example is counting friend triples in the\n$\\texttt{com-Youtube}$ dataset, the best dexterous bound is $1.2 \\cdot 10^9$,\nthe best ambidextrous bound is $5.1 \\cdot 10^8$, and the actual cardinality is\n$1.8 \\cdot 10^7$.", "published": "2025-10-05 15:34:18", "link": "http://arxiv.org/abs/2510.04249v1", "categories": ["cs.DB", "cs.IT", "math.IT"], "primary_category": "cs.DB"}
{"title": "Multiplicative Turing Ensembles, Pareto's Law, and Creativity", "abstract": "We study integer-valued multiplicative dynamics driven by i.i.d. prime\nmultipliers and connect their macroscopic statistics to universal codelengths.\nWe introduce the Multiplicative Turing Ensemble (MTE) and show how it arises\nnaturally - though not uniquely - from ensembles of probabilistic Turing\nmachines. Our modeling principle is variational: taking Elias' Omega codelength\nas an energy and imposing maximum entropy constraints yields a canonical Gibbs\nprior on integers and, by restriction, on primes. Under mild tail assumptions,\nthis prior induces exponential tails for log-multipliers (up to slowly varying\ncorrections), which in turn generate Pareto tails for additive gaps. We also\nprove time-average laws for the Omega codelength along MTE trajectories.\nEmpirically, on Debian and PyPI package size datasets, a scaled Omega prior\nachieves the lowest KL divergence against codelength histograms. Taken\ntogether, the theory-data comparison suggests a qualitative split:\nmachine-adapted regimes (Gibbs-aligned, finite first moment) exhibit clean\naveraging behavior, whereas human-generated complexity appears to sit beyond\nthis regime, with tails heavy enough to produce an unbounded first moment, and\ntherefore no averaging of the same kind.", "published": "2025-10-05 12:04:50", "link": "http://arxiv.org/abs/2510.04167v1", "categories": ["cs.IT", "cs.CC", "math-ph", "math.IT", "math.MP", "H.1.1"], "primary_category": "cs.IT"}
{"title": "Optimal frames for Phase Retrieval from Edge Vectors of Optimal Polygons", "abstract": "This paper aims to characterize the optimal frame for phase retrieval,\ndefined as the frame whose condition number for phase retrieval attains its\nminimal value. In the context of the two-dimensional real case, we reveal the\nconnection between optimal frames for phase retrieval and the\nperimeter-maximizing isodiametric problem, originally proposed by Reinhardt in\n1922. Our work establishes that every optimal solution to the\nperimeter-maximizing isodiametric problem inherently leads to an optimal frame\nin ${\\mathbb R}^2$. By recasting the optimal polygons problem as one concerning\nthe discrepancy of roots of unity, we characterize all optimal polygons.\nBuilding upon this connection, we then characterize all optimal frames with $m$\nvectors in ${\\mathbb R}^2$ for phase retrieval when $m \\geq 3$ has an odd\nfactor. As a key corollary, we show that the harmonic frame $E_m$ is {\\em not}\noptimal for any even integer $m \\geq 4$. This finding disproves a conjecture\nproposed by Xia, Xu, and Xu (Math. Comp., 90(356): 2931-2960). Previous work\nhas established that the harmonic frame $E_m \\subset {\\mathbb R}^2$ is indeed\noptimal when $m$ is an odd integer.\n  Exploring the connection between phase retrieval and discrete geometry, this\npaper aims to illuminate advancements in phase retrieval and offer new\nperspectives on the perimeter-maximizing isodiametric problem.", "published": "2025-10-05 08:56:00", "link": "http://arxiv.org/abs/2510.04099v1", "categories": ["cs.IT", "cs.NA", "math.FA", "math.IT", "math.MG", "math.NA"], "primary_category": "cs.IT"}
{"title": "Volume-Based Lower Bounds to the Capacity of the Gaussian Channel Under Pointwise Additive Input Constraints", "abstract": "We present a family of relatively simple and unified lower bounds on the\ncapacity of the Gaussian channel under a set of pointwise additive input\nconstraints. Specifically, the admissible channel input vectors $\\bx = (x_1,\n\\ldots, x_n)$ must satisfy $k$ additive cost constraints of the form\n$\\sum_{i=1}^n \\phi_j(x_i) \\le n \\Gamma_j$, $j = 1,2,\\ldots,k$, which are\nenforced pointwise for every $\\bx$, rather than merely in expectation. More\ngenerally, we also consider cost functions that depend on a sliding window of\nfixed length $m$, namely, $\\sum_{i=m}^n \\phi_j(x_i, x_{i-1}, \\ldots, x_{i-m+1})\n\\le n \\Gamma_j$, $j = 1,2,\\ldots,k$, a formulation that naturally accommodates\ncorrelation constraints as well as a broad range of other constraints of\npractical relevance. We propose two classes of lower bounds, derived by two\nmethodologies that both rely on the exact evaluation of the volume exponent\nassociated with the set of input vectors satisfying the given constraints. This\nevaluation exploits extensions of the method of types to continuous alphabets,\nthe saddle-point method of integration, and basic tools from large deviations\ntheory. The first class of bounds is obtained via the entropy power inequality\n(EPI), and therefore applies exclusively to continuous-valued inputs. The\nsecond class, by contrast, is more general, and it applies to discrete input\nalphabets as well. It is based on a direct manipulation of mutual information,\nand it yields stronger and tighter bounds, though at the cost of greater\ntechnical complexity. Numerical examples illustrating both types of bounds are\nprovided, and several extensions and refinements are also discussed.", "published": "2025-10-05 08:34:08", "link": "http://arxiv.org/abs/2510.04095v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Large Deviations Principle for Isoperimetry and Its Equivalence to Nonlinear Log-Sobolev Inequalities", "abstract": "We investigate the large deviations principle (which concerns sequences of\nexponentially small sets) for the isoperimetric problem on product Riemannian\nmanifolds $M^{n}$ equipped with product probability measures $\\nu^{\\otimes n}$,\nwhere $M$ is a Riemannian manifold satisfying curvature-dimension bound\n$\\mathrm{CD}(0,\\infty)$. When the probability measure ${\\nu}$ satisfies a\nspecific light-tail condition, we establish an exact characterization of the\nlarge deviations asymptotics for the isoperimetric profile, which shows a\nprecise equivalence between these asymptotic isoperimetric inequalities and\nnonlinear log-Sobolev inequalities. It is observed that the product of two\nrelative entropy typical sets or their one-sided versions (or the product of\ntwo empirically typical sets) forms an asymptotically optimal solution to the\nisoperimetric problem. The proofs in this paper rely on tools from information\ntheory, optimal transport, and geometric measure theory.", "published": "2025-10-05 04:33:02", "link": "http://arxiv.org/abs/2510.04030v1", "categories": ["math.MG", "cs.IT", "math.FA", "math.IT", "math.PR", "52B60, 39B62, 60F10"], "primary_category": "math.MG"}
{"title": "Multi-Modal Multi-Task Semantic Communication: A Distributed Information Bottleneck Perspective", "abstract": "Semantic communication (SemCom) shifts the focus from data transmission to\nmeaning delivery, enabling efficient and intelligent communication.\n  Existing AI-based coding schemes for multi-modal multi-task SemCom often\nrequire transmitters with full-modal data to participate in all receivers'\ntasks, which leads to redundant transmissions and conflicts with the physical\nlimits of channel capacity and computational capability.\n  In this paper, we propose PoM$^2$-DIB, a novel framework that extends the\ndistributed information bottleneck (DIB) theory to address this problem.\n  Unlike the typical DIB, this framework introduces modality selection as an\nadditional key design variable, enabling a more flexible tradeoff between\ncommunication rate and inference quality.\n  This extension selects only the most relevant modalities for task\nparticipation, adhering to the physical constraints, while following efficient\nDIB-based coding.\n  To optimize selection and coding end-to-end, we relax modality selection into\na probabilistic form, allowing the use of score function estimation with common\nrandomness to enable optimizable coordinated decisions across distributed\ndevices.\n  Experimental results on public datasets verify that PoM$^2$-DIB achieves high\ninference quality compared to full-participation baselines in various tasks\nunder physical limits.", "published": "2025-10-05 02:18:56", "link": "http://arxiv.org/abs/2510.04000v1", "categories": ["cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.IT"}
{"title": "Speculative Actions: A Lossless Framework for Faster Agentic Systems", "abstract": "Despite growing interest in AI agents across industry and academia, their\nexecution in an environment is often slow, hampering training, evaluation, and\ndeployment. For example, a game of chess between two state-of-the-art agents\nmay take hours. A critical bottleneck is that agent behavior unfolds\nsequentially: each action requires an API call, and these calls can be\ntime-consuming. Inspired by speculative execution in microprocessors and\nspeculative decoding in LLM inference, we propose speculative actions, a\nlossless framework for general agentic systems that predicts likely actions\nusing faster models, enabling multiple steps to be executed in parallel. We\nevaluate this framework across three agentic environments: gaming, e-commerce,\nweb search, and a \"lossy\" extension for an operating systems environment. In\nall cases, speculative actions achieve substantial accuracy in next-action\nprediction (up to 55%), translating into significant reductions in end-to-end\nlatency. Moreover, performance can be further improved through stronger\nguessing models, top-K action prediction, multi-step speculation, and\nuncertainty-aware optimization, opening a promising path toward deploying\nlow-latency agentic systems in the real world.", "published": "2025-10-05 21:28:11", "link": "http://arxiv.org/abs/2510.04371v1", "categories": ["cs.AI", "cs.DC", "cs.MA"], "primary_category": "cs.AI"}
{"title": "NegotiationGym: Self-Optimizing Agents in a Multi-Agent Social Simulation Environment", "abstract": "We design and implement NegotiationGym, an API and user interface for\nconfiguring and running multi-agent social simulations focused upon negotiation\nand cooperation. The NegotiationGym codebase offers a user-friendly,\nconfiguration-driven API that enables easy design and customization of\nsimulation scenarios. Agent-level utility functions encode optimization\ncriteria for each agent, and agents can self-optimize by conducting multiple\ninteraction rounds with other agents, observing outcomes, and modifying their\nstrategies for future rounds.", "published": "2025-10-05 21:23:21", "link": "http://arxiv.org/abs/2510.04368v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "Audit the Whisper: Detecting Steganographic Collusion in Multi-Agent LLMs", "abstract": "Multi-agent deployments of large language models (LLMs) are increasingly\nembedded in market, allocation, and governance workflows, yet covert\ncoordination among agents can silently erode trust and social welfare. Existing\naudits are dominated by heuristics that lack theoretical guarantees, struggle\nto transfer across tasks, and seldom ship with the infrastructure needed for\nindependent replication. We introduce \\emph{Audit the Whisper}, a\nconference-grade research artifact that spans theory, benchmark design,\ndetection, and reproducibility. Our contributions are: (i) a channel-capacity\nanalysis showing how interventions such as paraphrase, rate limiting, and role\npermutation impose quantifiable capacity penalties -- operationalized via\npaired-run Kullback--Leibler diagnostics -- that tighten mutual-information\nthresholds with finite-sample guarantees; (ii) \\textsc{ColludeBench}-v0,\ncovering pricing, first-price auctions, and peer review with configurable\ncovert schemes, deterministic manifests, and reward instrumentation; and (iii)\na calibrated auditing pipeline that fuses cross-run mutual information,\npermutation invariance, watermark variance, and fairness-aware acceptance bias,\neach tuned to a \\(10^{-3}\\) false-positive budget. Across 600 audited runs\nspanning 12 intervention conditions, the union meta-test attains TPR~$=1$ with\nzero observed false alarms, while ablations surface the price-of-auditing\ntrade-off and highlight fairness-driven colluders invisible to MI alone. We\nrelease regeneration scripts, seed-stamped manifests, and documentation so that\nexternal auditors can reproduce every figure and extend the framework with\nminimal effort.", "published": "2025-10-05 17:51:52", "link": "http://arxiv.org/abs/2510.04303v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "Small Fleet, Big Impact: Enhancing Shared Micromobility Efficiency through Minimal Autonomous Vehicle Deployment", "abstract": "Shared micromobility systems, such as electric scooters and bikes, have\ngained widespread popularity as sustainable alternatives to traditional\ntransportation modes. However, these systems face persistent challenges due to\nspatio-temporal demand fluctuations, often resulting in a mismatch between\nvehicle supply and user demand. Existing shared micromobility vehicle\nscheduling methods typically redistribute vehicles once or twice per day, which\nmakes them vulnerable to performance degradation under atypical conditions. In\nthis work, we design to augment existing micromobility scheduling methods by\nintegrating a small number of autonomous shared micromobility vehicles (ASMVs),\nwhich possess self-rebalancing capabilities to dynamically adapt to real-time\ndemand. Specifically, we introduce SMART, a hierarchical reinforcement learning\nframework that jointly optimizes high-level initial deployment and low-level\nreal-time rebalancing for ASMVs. We evaluate our framework based on real-world\ne-scooter usage data from Chicago. Our experiment results show that our\nframework is highly effective and possesses strong generalization capability,\nallowing it to seamlessly integrate with existing vehicle scheduling methods\nand significantly enhance overall micromobility service performance.", "published": "2025-10-05 16:26:42", "link": "http://arxiv.org/abs/2510.04271v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "Cooperative Flexibility Exchange: Fair and Comfort-Aware Decentralized Resource Allocation", "abstract": "The growing electricity demand and increased use of smart appliances are\nplacing new pressures on power grids, making efficient energy management more\nimportant than ever. The existing energy management systems often prioritize\nsystem efficiency (balanced energy demand and supply) at the expense of user\ncomfort. This paper addresses this gap by proposing a novel decentralized\nmulti-agent coordination-based demand-side management system. The proposed\nsystem enables individual agents to coordinate for demand-side energy\noptimization while improving the user comfort and maintaining the system\nefficiency. A key innovation of this work is the introduction of a slot\nexchange mechanism, where agents first receive optimized appliance-level energy\nconsumption schedules and then coordinate with each other to adjust these\nschedules through slot exchanges. This approach improves user comfort even when\nagents show non-altruistic behaviour, and it scales well with large\npopulations. The system also promotes fairness by balancing satisfaction levels\nacross users. For performance evaluation, a real-world dataset is used, and the\nresults demonstrate that the proposed slot exchange mechanism increases user\ncomfort and fairness without raising system inefficiency cost, making it a\npractical and scalable solution for future smart grids.", "published": "2025-10-05 13:17:12", "link": "http://arxiv.org/abs/2510.04192v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "Structure-Preserving MHD-Driftkinetic Discretization for Wave-Particle Interactions", "abstract": "We present a structure-preserving discretization of the hybrid\nmagnetohydrodynamics (MHD)-driftkinetic system for simulations of low-frequency\nwave-particle interactions. The model equations are derived from a variational\nprinciple, assuring energetically consistent couplings between MHD fluids and\ndriftkinetic particles. The spatial discretization is based on a\nfinite-element-exterior-calculus (FEEC) framework for the MHD and a\nparticle-in-cell (PIC) method for the driftkinetic. A key feature of the scheme\nis the inclusion of the non-quadratic particle magnetic moment energy term in\nthe Hamiltonian, which is introduced by the guiding-center approximation. The\nresulting discrete Hamiltonian structure naturally organizes the dynamics into\nskew-symmetric subsystems, enabling balanced energy exchange. To handle the\nnon-quadratic energy term, we develop energy-preserving time integrators based\non discrete gradient methods. The algorithm is implemented in the open-source\nPython package \\texttt{STRUPHY}. Numerical experiments confirm the\nenergy-conserving property of the scheme and demonstrate the capability to\nsimulate energetic particles (EP) induced excitation of toroidal Alfv\\'en\neigenmodes (TAE) without artificial dissipation or mode filtering. This\ncapability highlights the potential of structure-preserving schemes for\nhigh-fidelity simulations of hybrid systems.", "published": "2025-10-05 22:34:28", "link": "http://arxiv.org/abs/2510.04385v1", "categories": ["physics.comp-ph", "cs.NA", "math.NA", "physics.plasm-ph"], "primary_category": "physics.comp-ph"}
{"title": "Environment-Aware Indoor LoRaWAN Path Loss: Parametric Regression Comparisons, Shadow Fading, and Calibrated Fade Margins", "abstract": "Indoor LoRaWAN propagation is shaped by structural and time-varying context\nfactors, which challenge log-distance models and the assumption of log-normal\nshadowing. We present an environment-aware, statistically disciplined path loss\nframework evaluated using leakage-safe cross-validation on a 12-month campaign\nin an eighth-floor office measuring 240 m^2. A log-distance multi-wall mean is\naugmented with environmental covariates (relative humidity, temperature, carbon\ndioxide, particulate matter, and barometric pressure), as well as the\nsignal-to-noise ratio. We compare multiple linear regression with regularized\nvariants, Bayesian linear regression, and a selective second-order polynomial\napplied to continuous drivers. Predictor relevance is established using\nheteroscedasticity-robust Type II and III analysis of variance and nested\npartial F tests. Shadow fading is profiled with kernel density estimation and\nnon-parametric families, including Normal, Skew-Normal, Student's t, and\nGaussian mixtures. The polynomial mean reduces cross-validated RMSE from 8.07\nto 7.09 dB and raises R^2 from 0.81 to 0.86. Out-of-fold residuals are\nnon-Gaussian; a 3-component mixture captures a sharp core with a light, broad\ntail. We convert accuracy into reliability by prescribing the fade margin as\nthe upper-tail quantile of cross-validated residuals, quantifying uncertainty\nvia a moving-block bootstrap, and validating on a held-out set. At 99% packet\ndelivery ratio, the environment-aware polynomial requires 25.7 dB versus 27.7\nto 27.9 dB for linear baselines. This result presents a deployment-ready,\ninterpretable workflow with calibrated reliability control for indoor Internet\nof Things planning, aligned with 6G targets.", "published": "2025-10-05 20:14:48", "link": "http://arxiv.org/abs/2510.04346v1", "categories": ["cs.NI", "cs.LG", "cs.NA", "eess.SP", "math.NA"], "primary_category": "cs.NI"}
{"title": "Reducibility and rational torsion in modular abelian varieties", "abstract": "Let N be a square-free positive integer and let f be a newform of weight 2 on\n\\Gamma_0(N). Let A denote the abelian subvariety of J_0(N) associated to f and\nlet m be a maximal ideal of the Hecke algebra T that contains Ann_T(f) and has\nresidue characteristic r such that r does not divide 6N. We show that if either\nA[m] or the canonical representation \\rho_m over T/m associated to m is\nreducible, then r divides the order of the cuspidal subgroup of J_0(N) and A[m]\nhas a nontrivial rational point. We mention some applications of this result,\nincluding an application to the second part of the Birch and Swinnerton-Dyer\nconjecture for A.", "published": "2025-10-05 19:01:36", "link": "http://arxiv.org/abs/2510.04323v1", "categories": ["math.NT", "cs.NA", "math.NA"], "primary_category": "math.NT"}
{"title": "Towards Fast Option Pricing PDE Solvers Powered by PIELM", "abstract": "Partial differential equation (PDE) solvers underpin modern quantitative\nfinance, governing option pricing and risk evaluation. Physics-Informed Neural\nNetworks (PINNs) have emerged as a promising approach for solving the forward\nand inverse problems of partial differential equations (PDEs) using deep\nlearning. However they remain computationally expensive due to their iterative\ngradient descent based optimization and scale poorly with increasing model\nsize. This paper introduces Physics-Informed Extreme Learning Machines (PIELMs)\nas fast alternative to PINNs for solving both forward and inverse problems in\nfinancial PDEs. PIELMs replace iterative optimization with a single\nleast-squares solve, enabling deterministic and efficient training. We\nbenchmark PIELM on the Black-Scholes and Heston-Hull-White models for forward\npricing and demonstrate its capability in inverse model calibration to recover\nvolatility and interest rate parameters from noisy data. From experiments we\nobserve that PIELM achieve accuracy comparable to PINNs while being up to\n$30\\times$ faster, highlighting their potential for real-time financial\nmodeling.", "published": "2025-10-05 18:50:49", "link": "http://arxiv.org/abs/2510.04322v1", "categories": ["cs.CE", "cs.LG", "cs.NA", "math.NA", "J.2; I.6.3; G.1.7; G.1.8"], "primary_category": "cs.CE"}
{"title": "PolyKAN: A Polyhedral Analysis Framework for Provable and Minimal KAN Compression", "abstract": "Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to\ntraditional Multi-Layer Perceptrons (MLPs), offering enhanced interpretability\nand a strong mathematical foundation. However, their parameter efficiency\nremains a significant challenge for practical deployment. This paper introduces\nPolyKAN, a novel theoretical framework for KAN compression that provides formal\nguarantees on both model size reduction and approximation error. By leveraging\nthe inherent piecewise polynomial structure of KANs, we formulate the\ncompression problem as one of optimal polyhedral region merging. We establish a\nrigorous polyhedral characterization of KANs, develop a complete theory of\n$\\epsilon$-equivalent compression, and design an optimal dynamic programming\nalgorithm that guarantees minimal compression under specified error bounds. Our\ntheoretical analysis demonstrates that PolyKAN achieves provably minimal\ncompression while maintaining strict error control, with polynomial-time\ncomplexity in all network parameters. The framework provides the first formal\nfoundation for KAN compression with mathematical guarantees, opening new\ndirections for efficient deployment of interpretable neural architectures.", "published": "2025-10-05 13:39:18", "link": "http://arxiv.org/abs/2510.04205v1", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "math.OC", "68T07, 41A15, 52B11", "F.2.2; G.1.2; I.2.6"], "primary_category": "cs.LG"}
{"title": "Robust and efficient solvers for nonlinear partial differential equations based on random feature method", "abstract": "The random feature method (RFM), a mesh-free machine learning-based\nframework, has emerged as a promising alternative for solving PDEs on complex\ndomains. However, for large three-dimensional nonlinear problems, attaining\nhigh accuracy typically requires domain partitioning with many collocation\npoints and random features per subdomain, which leads to extremely large and\nill-conditioned nonlinear least-squares systems. To overcome these challenges,\nwe propose two randomized Newton-type solvers. The first is an inexact Newton\nmethod with right preconditioning (IPN), in which randomized Jacobian\ncompression and QR factorization are used to construct an efficient\npreconditioner that substantially reduces the condition number. Each Newton\nstep is then approximately solved by LSQR, and a derivative-free line search is\nincorporated to ensure residual reduction and stable convergence. Building upon\nthis framework, we further develop an adaptive multi-step inexact\npreconditioned Newton method (AMIPN). In this approach, the preconditioned\nJacobian is reused across multiple inner iterations, while a prescribed maximum\nnumber of inner iterations together with an adaptive early-stopping criterion\ndetermines whether the current preconditioner can be retained in subsequent\nouter iterations. These mechanisms effectively avoid redundant computations and\nenhance robustness. Extensive numerical experiments on both three-dimensional\nsteady-state and two-dimensional time-dependent PDEs with complex geometries\nconfirm the remarkable effectiveness of the proposed solvers. Compared with\nclassical discretization techniques and recent machine-learning-based\napproaches, the methods consistently deliver substantial accuracy improvements\nand robust convergence, thereby establishing the RFM combined with IPN/AMIPN as\nan efficient framework for large-scale nonlinear PDEs. .", "published": "2025-10-05 12:14:07", "link": "http://arxiv.org/abs/2510.04170v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Bound-Preserving WENO Schemes for Temple-class systems", "abstract": "This paper explores numerical schemes for Temple-class systems, which are\nintegral to various applications including one-dimensional two-phase flow,\nelasticity, traffic flow, and sedimentation. Temple-class systems are\ncharacterized by conservative equations, with different pressure function\nexpressions leading to specific models such as the Aw-Rascle-Zhang (ARZ)\ntraffic model and the sedimentation model. Our work extends existing studies by\nintroducing a moving mesh approach to address the challenges of preserving\nnon-convex invariant domains, a common issue in the numerical simulation of\nsuch systems. Our study outlines a novel bound-preserving (BP) and conservative\nnumerical scheme, designed specifically for non-convex sets in Temple-class\nsystems, which is critical for avoiding non-physical solutions and ensuring\nrobustness in simulations. We develop both local and global BP methods based on\nfinite difference schemes, with numerical experiments demonstrating the\neffectiveness and reliability of our methods. Furthermore, a parameterized flux\nlimiter is introduced to restrict high-order fluxes and maintain bound\npreservation. This innovation marks the first time such a parameterized\napproach has been applied to non-convex sets, offering significant improvements\nover traditional methods. The findings presented extend beyond theoretical\nimplications, as they are applicable to general Temple-class systems and can be\ntailored to ARZ traffic flow networks, highlighting the versatility and broad\napplicability of our approach. The paper contributes significantly to the field\nby providing a comprehensive method that maintains the physical and\nmathematical constrains of Temple-class systems.", "published": "2025-10-05 09:57:32", "link": "http://arxiv.org/abs/2510.04123v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "High order well-balanced and total-energy-conserving local discontinuous Galerkin methods for compressible self-gravitating Euler equations", "abstract": "In this paper, we develop a high order structure-preserving local\ndiscontinuous Galerkin (DG) scheme for the compressible self-gravitating Euler\nequations, which pose great challenges due to the presence of time-dependent\ngravitational potential. The designed scheme is well-balanced for general\npolytropic equilibrium state and total energy conserving for multiple spatial\ndimensions without an assumption of spherical symmetry. The well-balanced\nproperty is achieved by decomposing the gravitational potential into\nequilibrium and perturbation parts, employing a modified Harten-Lax-van\nLeer-contact flux and a modification of the discretization for the source term.\nConservation of total energy is particularly challenging in the presence of\nself-gravity, especially when aiming for high order accuracy. To address this,\nwe rewrite the energy equation into a conservative form, and carefully design\nan energy flux with the aid of weak formulation from the DG method to maintain\nconservation as well as high order accuracy. The resulting scheme can be\nextended to high order in time discretizations. Numerical examples for two and\nthree dimensional problems are provided to verify the desired properties of our\nproposed scheme, including shock-capturing, high order accuracy, well balance,\nand total energy conservation.", "published": "2025-10-05 09:32:27", "link": "http://arxiv.org/abs/2510.04112v1", "categories": ["math.NA", "cs.NA", "35L65, 65M60, 76L05"], "primary_category": "math.NA"}
{"title": "Can Linear Probes Measure LLM Uncertainty?", "abstract": "Effective Uncertainty Quantification (UQ) represents a key aspect for\nreliable deployment of Large Language Models (LLMs) in automated\ndecision-making and beyond. Yet, for LLM generation with multiple choice\nstructure, the state-of-the-art in UQ is still dominated by the naive baseline\ngiven by the maximum softmax score. To address this shortcoming, we demonstrate\nthat taking a principled approach via Bayesian statistics leads to improved\nperformance despite leveraging the simplest possible model, namely linear\nregression. More precisely, we propose to train multiple Bayesian linear\nmodels, each predicting the output of a layer given the output of the previous\none. Based on the obtained layer-level posterior distributions, we infer the\nglobal uncertainty level of the LLM by identifying a sparse combination of\ndistributional features, leading to an efficient UQ scheme. Numerical\nexperiments on various LLMs show consistent improvement over state-of-the-art\nbaselines.", "published": "2025-10-05 09:14:57", "link": "http://arxiv.org/abs/2510.04108v1", "categories": ["cs.LG", "cs.NA", "math.NA", "math.ST", "stat.TH"], "primary_category": "cs.LG"}
{"title": "Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws", "abstract": "Motivated by the remarkable success of Foundation Models (FMs) in language\nmodeling, there has been growing interest in developing FMs for time series\nprediction, given the transformative power such models hold for science and\nengineering. This culminated in significant success of FMs in short-range\nforecasting settings. However, extrapolation or long-range forecasting remains\nelusive for FMs, which struggle to outperform even simple baselines. This\ncontrasts with physical laws which have strong extrapolation properties, and\nraises the question of the fundamental difference between the structure of\nneural networks and physical laws. In this work, we identify and formalize a\nfundamental property characterizing the ability of statistical learning models\nto predict more accurately outside of their training domain, hence explaining\nperformance deterioration for deep learning models in extrapolation settings.\nIn addition to a theoretical analysis, we present empirical results showcasing\nthe implications of this property on current deep learning architectures. Our\nresults not only clarify the root causes of the extrapolation gap but also\nsuggest directions for designing next-generation forecasting models capable of\nmastering extrapolation.", "published": "2025-10-05 09:07:25", "link": "http://arxiv.org/abs/2510.04102v1", "categories": ["cs.LG", "cs.NA", "math.NA", "math.PR"], "primary_category": "cs.LG"}
{"title": "Sharp Lower Bounds for Linearized ReLU^k Approximation on the Sphere", "abstract": "We prove a saturation theorem for linearized shallow ReLU$^k$ neural networks\non the unit sphere $\\mathbb S^d$. For any antipodally quasi-uniform set of\ncenters, if the target function has smoothness $r>\\tfrac{d+2k+1}{2}$, then the\nbest $\\mathcal{L}^2(\\mathbb S^d)$ approximation cannot converge faster than\norder $n^{-\\frac{d+2k+1}{2d}}$. This lower bound matches existing upper bounds,\nthereby establishing the exact saturation order $\\tfrac{d+2k+1}{2d}$ for such\nnetworks. Our results place linearized neural-network approximation firmly\nwithin the classical saturation framework and show that, although ReLU$^k$\nnetworks outperform finite elements under equal degrees $k$, this advantage is\nintrinsically limited.", "published": "2025-10-05 06:47:24", "link": "http://arxiv.org/abs/2510.04060v1", "categories": ["math.NA", "cs.LG", "cs.NA"], "primary_category": "math.NA"}
{"title": "A Mathematical Explanation of Transformers for Large Language Models and GPTs", "abstract": "The Transformer architecture has revolutionized the field of sequence\nmodeling and underpins the recent breakthroughs in large language models\n(LLMs). However, a comprehensive mathematical theory that explains its\nstructure and operations remains elusive. In this work, we propose a novel\ncontinuous framework that rigorously interprets the Transformer as a\ndiscretization of a structured integro-differential equation. Within this\nformulation, the self-attention mechanism emerges naturally as a non-local\nintegral operator, and layer normalization is characterized as a projection to\na time-dependent constraint. This operator-theoretic and variational\nperspective offers a unified and interpretable foundation for understanding the\narchitecture's core components, including attention, feedforward layers, and\nnormalization. Our approach extends beyond previous theoretical analyses by\nembedding the entire Transformer operation in continuous domains for both token\nindices and feature dimensions. This leads to a principled and flexible\nframework that not only deepens theoretical insight but also offers new\ndirections for architecture design, analysis, and control-based\ninterpretations. This new interpretation provides a step toward bridging the\ngap between deep learning architectures and continuous mathematical modeling,\nand contributes a foundational perspective to the ongoing development of\ninterpretable and theoretically grounded neural network models.", "published": "2025-10-05 01:16:08", "link": "http://arxiv.org/abs/2510.03989v1", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "primary_category": "cs.LG"}
{"title": "From News to Returns: A Granger-Causal Hypergraph Transformer on the Sphere", "abstract": "We propose the Causal Sphere Hypergraph Transformer (CSHT), a novel\narchitecture for interpretable financial time-series forecasting that unifies\n\\emph{Granger-causal hypergraph structure}, \\emph{Riemannian geometry}, and\n\\emph{causally masked Transformer attention}. CSHT models the directional\ninfluence of financial news and sentiment on asset returns by extracting\nmultivariate Granger-causal dependencies, which are encoded as directional\nhyperedges on the surface of a hypersphere. Attention is constrained via\nangular masks that preserve both temporal directionality and geometric\nconsistency. Evaluated on S\\&P 500 data from 2018 to 2023, including the 2020\nCOVID-19 shock, CSHT consistently outperforms baselines across return\nprediction, regime classification, and top-asset ranking tasks. By enforcing\npredictive causal structure and embedding variables in a Riemannian manifold,\nCSHT delivers both \\emph{robust generalisation across market regimes} and\n\\emph{transparent attribution pathways} from macroeconomic events to\nstock-level responses. These results suggest that CSHT is a principled and\npractical solution for trustworthy financial forecasting under uncertainty.", "published": "2025-10-05 20:51:59", "link": "http://arxiv.org/abs/2510.04357v1", "categories": ["cs.LG", "q-fin.CP"], "primary_category": "cs.LG"}
{"title": "Convergence in probability of numerical solutions of a highly non-linear delayed stochastic interest rate model", "abstract": "We examine a delayed stochastic interest rate model with super-linearly\ngrowing coefficients and develop several new mathematical tools to establish\nthe properties of its true and truncated EM solutions. Moreover, we show that\nthe true solution converges to the truncated EM solutions in probability as the\nstep size tends to zero. Further, we support the convergence result with some\nillustrative numerical examples and justify the convergence result for the\nMonte Carlo evaluation of some financial quantities.", "published": "2025-10-05 08:30:35", "link": "http://arxiv.org/abs/2510.04092v1", "categories": ["math.PR", "q-fin.CP", "65C05, 65C30, 91G30, 91G60"], "primary_category": "math.PR"}
{"title": "Short-rate models with stochastic discontinuities: a PDE approach", "abstract": "With the reform of interest rate benchmarks, interbank offered rates (IBORs)\nlike LIBOR have been replaced by risk-free rates (RFRs), such as the Secured\nOvernight Financing Rate (SOFR) in the U.S. and the Euro Short-Term Rate (\\euro\nSTR) in Europe. These rates exhibit characteristics like jumps and spikes that\ncorrespond to specific market events, driven by regulatory and liquidity\nconstraints. To capture these characteristics, this paper considers a general\nshort-rate model that incorporates discontinuities at fixed times with random\nsizes. Within this framework, we introduce a PDE-based approach for pricing\ninterest rate derivatives and establish, under suitable assumptions, a\nFeynman-Ka\\v{c} representation for the solution. For affine models, we derive\n(quasi) closed-form solutions, while for the general case, we develop numerical\nmethods to solve the resulting PDEs.", "published": "2025-10-05 17:01:35", "link": "http://arxiv.org/abs/2510.04289v1", "categories": ["q-fin.MF", "math.PR", "35Q91, 60H30, 91G20, 91G30, 91G60"], "primary_category": "q-fin.MF"}
{"title": "Panel regression for the GDP of the Central and Eastern European countries using time-varying coefficients", "abstract": "The integration of Central and Eastern European (CEE) countries into the\nEuropean Economic Area serves as a valuable experiment for the regional\neconomic development theory. The long-lasting convergence of these economies\nwith more advanced Western Europe exhibits a few standard features and varying\npolicies implemented. Even the Baltic countries, which started from very\nsimilar starting positions, demonstrate their unique trajectories of\ndevelopment. We employ a panel data regression model that allows coefficients\nto vary over time to compare the contributions of a few macroeconomic factors\nto the GDP growth of CEE countries. In particular, we regress the annual change\nof GDP per capita in PPP terms as a function of achieved GDP, price, trade,\ninvestment, and debt levels. Time-varying common slope coefficients in this\napproach describe the external economic environment in which countries\nimplement their own policies. The panel consists of 11 Central and Eastern\nEuropean countries (Bulgaria, Czechia, Estonia, Croatia, Latvia, Lithuania,\nHungary, Poland, Romania, Slovenia, and Slovakia), which have been observed\nannually from 1995 to 2024. While the main selected factors of this\ninvestigation contribute to economic growth, in agreement with previous\nfindings, the role of private debt appears vital in determining the pace of\neconomic growth.", "published": "2025-10-05 14:00:44", "link": "http://arxiv.org/abs/2510.04211v1", "categories": ["q-fin.ST", "econ.GN", "physics.soc-ph", "q-fin.EC"], "primary_category": "q-fin.ST"}
{"title": "Score-based generative emulation of impact-relevant Earth system model outputs", "abstract": "Policy targets evolve faster than the Couple Model Intercomparison Project\ncycles, complicating adaptation and mitigation planning that must often contend\nwith outdated projections. Climate model output emulators address this gap by\noffering inexpensive surrogates that can rapidly explore alternative futures\nwhile staying close to Earth System Model (ESM) behavior. We focus on emulators\ndesigned to provide inputs to impact models. Using monthly ESM fields of\nnear-surface temperature, precipitation, relative humidity, and wind speed, we\nshow that deep generative models have the potential to model jointly the\ndistribution of variables relevant for impacts. The specific model we propose\nuses score-based diffusion on a spherical mesh and runs on a single mid-range\ngraphical processing unit. We introduce a thorough suite of diagnostics to\ncompare emulator outputs with their parent ESMs, including their probability\ndensities, cross-variable correlations, time of emergence, or tail behavior. We\nevaluate performance across three distinct ESMs in both pre-industrial and\nforced regimes. The results show that the emulator produces distributions that\nclosely match the ESM outputs and captures key forced responses. They also\nreveal important failure cases, notably for variables with a strong regime\nshift in the seasonal cycle. Although not a perfect match to the ESM, the\ninaccuracies of the emulator are small relative to the scale of internal\nvariability in ESM projections. We therefore argue that it shows potential to\nbe useful in supporting impact assessment. We discuss priorities for future\ndevelopment toward daily resolution, finer spatial scales, and bias-aware\ntraining. Code is made available at https://github.com/shahineb/climemu.", "published": "2025-10-05 20:54:19", "link": "http://arxiv.org/abs/2510.04358v1", "categories": ["physics.ao-ph", "stat.AP", "stat.ML"], "primary_category": "physics.ao-ph"}
{"title": "Arithmetic-Mean $\u03bc$P for Modern Architectures: A Unified Learning-Rate Scale for CNNs and ResNets", "abstract": "Choosing an appropriate learning rate remains a key challenge in scaling\ndepth of modern deep networks. The classical maximal update parameterization\n($\\mu$P) enforces a fixed per-layer update magnitude, which is well suited to\nhomogeneous multilayer perceptrons (MLPs) but becomes ill-posed in\nheterogeneous architectures where residual accumulation and convolutions\nintroduce imbalance across layers. We introduce Arithmetic-Mean $\\mu$P\n(AM-$\\mu$P), which constrains not each individual layer but the network-wide\naverage one-step pre-activation second moment to a constant scale. Combined\nwith a residual-aware He fan-in initialization - scaling residual-branch\nweights by the number of blocks ($\\mathrm{Var}[W]=c/(K\\cdot\n\\mathrm{fan\\text{-}in})$) - AM-$\\mu$P yields width-robust depth laws that\ntransfer consistently across depths. We prove that, for one- and\ntwo-dimensional convolutional networks, the maximal-update learning rate\nsatisfies $\\eta^\\star(L)\\propto L^{-3/2}$; with zero padding, boundary effects\nare constant-level as $N\\gg k$. For standard residual networks with general\nconv+MLP blocks, we establish $\\eta^\\star(L)=\\Theta(L^{-3/2})$, with $L$ the\nminimal depth. Empirical results across a range of depths confirm the $-3/2$\nscaling law and enable zero-shot learning-rate transfer, providing a unified\nand practical LR principle for convolutional and deep residual networks without\nadditional tuning overhead.", "published": "2025-10-05 19:22:50", "link": "http://arxiv.org/abs/2510.04327v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Adaptive Coverage Policies in Conformal Prediction", "abstract": "Traditional conformal prediction methods construct prediction sets such that\nthe true label falls within the set with a user-specified coverage level.\nHowever, poorly chosen coverage levels can result in uninformative predictions,\neither producing overly conservative sets when the coverage level is too high,\nor empty sets when it is too low. Moreover, the fixed coverage level cannot\nadapt to the specific characteristics of each individual example, limiting the\nflexibility and efficiency of these methods. In this work, we leverage recent\nadvances in e-values and post-hoc conformal inference, which allow the use of\ndata-dependent coverage levels while maintaining valid statistical guarantees.\nWe propose to optimize an adaptive coverage policy by training a neural network\nusing a leave-one-out procedure on the calibration set, allowing the coverage\nlevel and the resulting prediction set size to vary with the difficulty of each\nindividual example. We support our approach with theoretical coverage\nguarantees and demonstrate its practical benefits through a series of\nexperiments.", "published": "2025-10-05 18:37:22", "link": "http://arxiv.org/abs/2510.04318v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Relative Information Gain and Gaussian Process Regression", "abstract": "The sample complexity of estimating or maximising an unknown function in a\nreproducing kernel Hilbert space is known to be linked to both the effective\ndimension and the information gain associated with the kernel. While the\ninformation gain has an attractive information-theoretic interpretation, the\neffective dimension typically results in better rates. We introduce a new\nquantity called the relative information gain, which measures the sensitivity\nof the information gain with respect to the observation noise. We show that the\nrelative information gain smoothly interpolates between the effective dimension\nand the information gain, and that the relative information gain has the same\ngrowth rate as the effective dimension. In the second half of the paper, we\nprove a new PAC-Bayesian excess risk bound for Gaussian process regression. The\nrelative information gain arises naturally from the complexity term in this\nPAC-Bayesian bound. We prove bounds on the relative information gain that\ndepend on the spectral properties of the kernel. When these upper bounds are\ncombined with our excess risk bound, we obtain minimax-optimal rates of\nconvergence.", "published": "2025-10-05 16:35:51", "link": "http://arxiv.org/abs/2510.04277v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Scalable Causal Discovery from Recursive Nonlinear Data via Truncated Basis Function Scores and Tests", "abstract": "Learning graphical conditional independence structures from nonlinear,\ncontinuous or mixed data is a central challenge in machine learning and the\nsciences, and many existing methods struggle to scale to thousands of samples\nor hundreds of variables. We introduce two basis-expansion tools for scalable\ncausal discovery. First, the Basis Function BIC (BF-BIC) score uses truncated\nadditive expansions to approximate nonlinear dependencies. BF-BIC is\ntheoretically consistent under additive models and extends to post-nonlinear\n(PNL) models via an invertible reparameterization. It remains robust under\nmoderate interactions and supports mixed data through a degenerate-Gaussian\nembedding for discrete variables. In simulations with fully nonlinear neural\ncausal models (NCMs), BF-BIC outperforms kernel- and constraint-based methods\n(e.g., KCI, RFCI) in both accuracy and runtime. Second, the Basis Function\nLikelihood Ratio Test (BF-LRT) provides an approximate conditional independence\ntest that is substantially faster than kernel tests while retaining competitive\naccuracy. Extensive simulations and a real-data application to Canadian\nwildfire risk show that, when integrated into hybrid searches, BF-based methods\nenable interpretable and scalable causal discovery. Implementations are\navailable in Python, R, and Java.", "published": "2025-10-05 16:34:54", "link": "http://arxiv.org/abs/2510.04276v1", "categories": ["stat.ML", "cs.AI"], "primary_category": "stat.ML"}
{"title": "Offline Reinforcement Learning in Large State Spaces: Algorithms and Guarantees", "abstract": "This article introduces the theory of offline reinforcement learning in large\nstate spaces, where good policies are learned from historical data without\nonline interactions with the environment. Key concepts introduced include\nexpressivity assumptions on function approximation (e.g., Bellman completeness\nvs. realizability) and data coverage (e.g., all-policy vs. single-policy\ncoverage). A rich landscape of algorithms and results is described, depending\non the assumptions one is willing to make and the sample and computational\ncomplexity guarantees one wishes to achieve. We also discuss open questions and\nconnections to adjacent areas.", "published": "2025-10-05 08:23:40", "link": "http://arxiv.org/abs/2510.04088v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning", "abstract": "Reinforcement learning (RL) has become central to enhancing reasoning in\nlarge language models (LLMs). Yet on-policy algorithms such as Group Relative\nPolicy Optimization (GRPO) often suffer in early training: noisy gradients from\nlow-quality rollouts lead to unstable updates and inefficient exploration. We\nintroduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient\nframework to address these limitations via decomposing each step into three\nstages: a short fast trajectory of inner steps on the same batch, a reposition\nmechanism to control off-policy drift, and a final slow correction. This\nreposition-before-update design preserves the objective and rollout process\nunchanged, making SFPO plug-compatible with existing policy-gradient pipelines.\nExtensive experiments demonstrate that SFPO consistently improves stability,\nreduces rollouts, and accelerates convergence of reasoning RL training.\nSpecifically, it outperforms GRPO by up to 2.80 points in average on math\nreasoning benchmarks. It also achieves up to 4.93\\texttimes{} fewer rollouts\nand a 4.19\\texttimes{} reduction in wall-clock time to match GRPO's best\naccuracy.", "published": "2025-10-05 07:22:54", "link": "http://arxiv.org/abs/2510.04072v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Simulation-based inference via telescoping ratio estimation for trawl processes", "abstract": "The growing availability of large and complex datasets has increased interest\nin temporal stochastic processes that can capture stylized facts such as\nmarginal skewness, non-Gaussian tails, long memory, and even non-Markovian\ndynamics. While such models are often easy to simulate from, parameter\nestimation remains challenging. Simulation-based inference (SBI) offers a\npromising way forward, but existing methods typically require large training\ndatasets or complex architectures and frequently yield confidence (credible)\nregions that fail to attain their nominal values, raising doubts on the\nreliability of estimates for the very features that motivate the use of these\nmodels. To address these challenges, we propose a fast and accurate,\nsample-efficient SBI framework for amortized posterior inference applicable to\nintractable stochastic processes. The proposed approach relies on two main\nsteps: first, we learn the posterior density by decomposing it sequentially\nacross parameter dimensions. Then, we use Chebyshev polynomial approximations\nto efficiently generate independent posterior samples, enabling accurate\ninference even when Markov chain Monte Carlo methods mix poorly. We further\ndevelop novel diagnostic tools for SBI in this context, as well as post-hoc\ncalibration techniques; the latter not only lead to performance improvements of\nthe learned inferential tool, but also to the ability to reuse it directly with\nnew time series of varying lengths, thus amortizing the training cost. We\ndemonstrate the method's effectiveness on trawl processes, a class of flexible\ninfinitely divisible models that generalize univariate Gaussian processes,\napplied to energy demand data.", "published": "2025-10-05 05:26:46", "link": "http://arxiv.org/abs/2510.04042v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Pitch-Conditioned Instrument Sound Synthesis From an Interactive Timbre Latent Space", "abstract": "This paper presents a novel approach to neural instrument sound synthesis\nusing a two-stage semi-supervised learning framework capable of generating\npitch-accurate, high-quality music samples from an expressive timbre latent\nspace. Existing approaches that achieve sufficient quality for music production\noften rely on high-dimensional latent representations that are difficult to\nnavigate and provide unintuitive user experiences. We address this limitation\nthrough a two-stage training paradigm: first, we train a pitch-timbre\ndisentangled 2D representation of audio samples using a Variational\nAutoencoder; second, we use this representation as conditioning input for a\nTransformer-based generative model. The learned 2D latent space serves as an\nintuitive interface for navigating and exploring the sound landscape. We\ndemonstrate that the proposed method effectively learns a disentangled timbre\nspace, enabling expressive and controllable audio generation with reliable\npitch conditioning. Experimental results show the model's ability to capture\nsubtle variations in timbre while maintaining a high degree of pitch accuracy.\nThe usability of our method is demonstrated in an interactive web application,\nhighlighting its potential as a step towards future music production\nenvironments that are both intuitive and creatively empowering:\nhttps://pgesam.faresschulz.com", "published": "2025-10-05 20:03:30", "link": "http://arxiv.org/abs/2510.04339v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Machine Unlearning in Speech Emotion Recognition via Forget Set Alone", "abstract": "Speech emotion recognition aims to identify emotional states from speech\nsignals and has been widely applied in human-computer interaction, education,\nhealthcare, and many other fields. However, since speech data contain rich\nsensitive information, partial data can be required to be deleted by speakers\ndue to privacy concerns. Current machine unlearning approaches largely depend\non data beyond the samples to be forgotten. However, this reliance poses\nchallenges when data redistribution is restricted and demands substantial\ncomputational resources in the context of big data. We propose a novel\nadversarial-attack-based approach that fine-tunes a pre-trained speech emotion\nrecognition model using only the data to be forgotten. The experimental results\ndemonstrate that the proposed approach can effectively remove the knowledge of\nthe data to be forgotten from the model, while preserving high model\nperformance on the test set for emotion recognition.", "published": "2025-10-05 15:44:15", "link": "http://arxiv.org/abs/2510.04251v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Probing Whisper for Dysarthric Speech in Detection and Assessment", "abstract": "Large-scale end-to-end models such as Whisper have shown strong performance\non diverse speech tasks, but their internal behavior on pathological speech\nremains poorly understood. Understanding how dysarthric speech is represented\nacross layers is critical for building reliable and explainable clinical\nassessment tools. This study probes the Whisper-Medium model encoder for\ndysarthric speech for detection and assessment (i.e., severity classification).\nWe evaluate layer-wise embeddings with a linear classifier under both\nsingle-task and multi-task settings, and complement these results with\nSilhouette scores and mutual information to provide perspectives on layer\ninformativeness. To examine adaptability, we repeat the analysis after\nfine-tuning Whisper on a dysarthric speech recognition task. Across metrics,\nthe mid-level encoder layers (13-15) emerge as most informative, while\nfine-tuning induces only modest changes. The findings improve the\ninterpretability of Whisper's embeddings and highlight the potential of probing\nanalyses to guide the use of large-scale pretrained models for pathological\nspeech.", "published": "2025-10-05 14:21:39", "link": "http://arxiv.org/abs/2510.04219v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhancing Speaker Verification with w2v-BERT 2.0 and Knowledge Distillation guided Structured Pruning", "abstract": "Large-scale self-supervised Pre-Trained Models (PTMs) have shown significant\nimprovements in the speaker verification (SV) task by providing rich feature\nrepresentations. In this paper, we utilize w2v-BERT 2.0, a model with\napproximately 600 million parameters trained on 450 million hours of unlabeled\ndata across 143 languages, for the SV task. The MFA structure with Layer\nAdapter is employed to process the multi-layer feature outputs from the PTM and\nextract speaker embeddings. Additionally, we incorporate LoRA for efficient\nfine-tuning. Our model achieves state-of-the-art results with 0.12% and 0.55%\nEER on the Vox1-O and Vox1-H test sets, respectively. Furthermore, we apply\nknowledge distillation guided structured pruning, reducing the model size by\n80% while achieving only a 0.04% EER degradation. Source code and models are\nreleased at https://github.com/ZXHY-82/w2v-BERT-2.0_SV.", "published": "2025-10-05 14:03:09", "link": "http://arxiv.org/abs/2510.04213v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Drax: Speech Recognition with Discrete Flow Matching", "abstract": "Diffusion and flow-based non-autoregressive (NAR) models have shown strong\npromise in large language modeling, however, their potential for automatic\nspeech recognition (ASR) remains largely unexplored. We propose Drax, a\ndiscrete flow matching framework for ASR that enables efficient parallel\ndecoding. To better align training with inference, we construct an\naudio-conditioned probability path that guides the model through trajectories\nresembling likely intermediate inference errors, rather than direct random\nnoise to target transitions. Our theoretical analysis links the generalization\ngap to divergences between training and inference occupancies, controlled by\ncumulative velocity errors, thereby motivating our design choice. Empirical\nevaluation demonstrates that our approach attains recognition accuracy on par\nwith state-of-the-art speech models while offering improved accuracy-efficiency\ntrade-offs, highlighting discrete flow matching as a promising direction for\nadvancing NAR ASR.", "published": "2025-10-05 11:38:01", "link": "http://arxiv.org/abs/2510.04162v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "GDiffuSE: Diffusion-based speech enhancement with noise model guidance", "abstract": "This paper introduces a novel speech enhancement (SE) approach based on a\ndenoising diffusion probabilistic model (DDPM), termed Guided diffusion for\nspeech enhancement (GDiffuSE). In contrast to conventional methods that\ndirectly map noisy speech to clean speech, our method employs a lightweight\nhelper model to estimate the noise distribution, which is then incorporated\ninto the diffusion denoising process via a guidance mechanism. This design\nimproves robustness by enabling seamless adaptation to unseen noise types and\nby leveraging large-scale DDPMs originally trained for speech generation in the\ncontext of SE. We evaluate our approach on noisy signals obtained by adding\nnoise samples from the BBC sound effects database to LibriSpeech utterances,\nshowing consistent improvements over state-of-the-art baselines under\nmismatched noise conditions. Examples are available at our project webpage.", "published": "2025-10-05 11:22:52", "link": "http://arxiv.org/abs/2510.04157v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Multilingual Framework for Dysarthria: Detection, Severity Classification, Speech-to-Text, and Clean Speech Generation", "abstract": "Dysarthria is a motor speech disorder that results in slow and often\nincomprehensible speech. Speech intelligibility significantly impacts\ncommunication, leading to barriers in social interactions. Dysarthria is often\na characteristic of neurological diseases including Parkinson's and ALS, yet\ncurrent tools lack generalizability across languages and levels of severity. In\nthis study, we present a unified AI-based multilingual framework that addresses\nsix key components: (1) binary dysarthria detection, (2) severity\nclassification, (3) clean speech generation, (4) speech-to-text conversion, (5)\nemotion detection, and (6) voice cloning. We analyze datasets in English,\nRussian, and German, using spectrogram-based visualizations and acoustic\nfeature extraction to inform model training. Our binary detection model\nachieved 97% accuracy across all three languages, demonstrating strong\ngeneralization across languages. The severity classification model also reached\n97% test accuracy, with interpretable results showing model attention focused\non lower harmonics. Our translation pipeline, trained on paired Russian\ndysarthric and clean speech, reconstructed intelligible outputs with low\ntraining (0.03) and test (0.06) L1 losses. Given the limited availability of\nEnglish dysarthric-clean pairs, we fine-tuned the Russian model on English data\nand achieved improved losses of 0.02 (train) and 0.03 (test), highlighting the\npromise of cross-lingual transfer learning for low-resource settings. Our\nspeech-to-text pipeline achieved a Word Error Rate of 0.1367 after three\nepochs, indicating accurate transcription on dysarthric speech and enabling\ndownstream emotion recognition and voice cloning from transcribed speech.\nOverall, the results and products of this study can be used to diagnose\ndysarthria and improve communication and understanding for patients across\ndifferent languages.", "published": "2025-10-05 00:52:04", "link": "http://arxiv.org/abs/2510.03986v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient Domain Generalization in Wireless Networks with Scarce Multi-Modal Data", "abstract": "In 6G wireless networks, multi-modal ML models can be leveraged to enable\nsituation-aware network decisions in dynamic environments. However, trained ML\nmodels often fail to generalize under domain shifts when training and test data\ndistributions are different because they often focus on modality-specific\nspurious features. In practical wireless systems, domain shifts occur\nfrequently due to dynamic channel statistics, moving obstacles, or hardware\nconfiguration. Thus, there is a need for learning frameworks that can achieve\nrobust generalization under scarce multi-modal data in wireless networks. In\nthis paper, a novel and data-efficient two-phase learning framework is proposed\nto improve generalization performance in unseen and unfamiliar wireless\nenvironments with minimal amount of multi-modal data. In the first stage, a\nphysics-based loss function is employed to enable each BS to learn the physics\nunderlying its wireless environment captured by multi-modal data. The\ndata-efficiency of the physics-based loss function is analytically\ninvestigated. In the second stage, collaborative domain adaptation is proposed\nto leverage the wireless environment knowledge of multiple BSs to guide\nunder-performing BSs under domain shift. Specifically, domain-similarity-aware\nmodel aggregation is proposed to utilize the knowledge of BSs that experienced\nsimilar domains. To validate the proposed framework, a new dataset generation\nframework is developed by integrating CARLA and MATLAB-based mmWave channel\nmodeling to predict mmWave RSS. Simulation results show that the proposed\nphysics-based training requires only 13% of data samples to achieve the same\nperformance as a state-of-the-art baseline that does not use physics-based\ntraining. Moreover, the proposed collaborative domain adaptation needs only 25%\nof data samples and 20% of FLOPs to achieve the convergence compared to\nbaselines.", "published": "2025-10-05 20:54:48", "link": "http://arxiv.org/abs/2510.04359v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Terahertz Channel Measurement and Modeling for Short-Range Indoor Environments", "abstract": "Accurate channel modeling is essential for realizing the potential of\nterahertz (THz) communications in 6G indoor networks, where existing models\nstruggle with severe frequency selectivity and multipath effects. We propose a\nphysically grounded Rician fading channel model that jointly incorporates\ndeterministic line-of-sight (LOS) and stochastic non-line-of-sight (NLOS)\ncomponents, enhanced by frequency-dependent attenuation characterized by\noptimized exponents alpha and beta. Unlike conventional approaches, our model\nintegrates a two-ray reflection framework to capture standing wave phenomena\nand employs wideband spectral averaging to mitigate frequency selectivity over\nbandwidths up to 15 GHz. Empirical measurements at a 208 GHz carrier, spanning\n0.1-0.9 m, demonstrate that our model achieves root mean square errors (RMSE)\nas low as 2.54 dB, outperforming free-space path loss (FSPL) by up to 14.2% and\nreducing RMSE by 73.3% as bandwidth increases. These findings underscore the\nimportance of bandwidth in suppressing oscillatory artifacts and improving\nmodeling accuracy. Our approach provides a robust foundation for THz system\ndesign, supporting reliable indoor wireless personal area networks (WPANs),\ndevice-to-device (D2D) communications, and precise localization in future 6G\napplications.", "published": "2025-10-05 15:48:17", "link": "http://arxiv.org/abs/2510.04258v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Integrating Phase-Coherent Multistatic Imaging in Downlink D-MIMO Networks", "abstract": "This paper addresses the challenge of integrating multistatic coherent\nimaging functionalities in the downlink (DL) of a phase-coherent distributed\nmultiple input multiple output (D-MIMO) communication network. During DL, the\nD-MIMO access points (APs) jointly precode the transmitted signals to maximize\nthe spectral efficiency (SE) at the users (UEs) locations. However, imaging\nrequires that \\textit{(i)} a fraction of the APs work as receivers for sensing\nand \\textit{(ii)} the transmitting APs emit AP-specific and orthogonal signals\nto illuminate the area to be imaged and allow multistatic operation. In these\nsettings, our contribution is twofold. We propose a novel distributed\nintegrated sensing and communication (D-ISAC) system that superposes a\npurposely designed AP-specific signal for imaging to the legacy UE-specific\ncommunication one, with a tunable trade-off factor. We detail both the imaging\nwaveform design according to the \\textit{extended orthogonality condition} and\nthe space-frequency precoder design. Then, we propose an optimized selection\nstrategy for the receiving APs, in order to maximize imaging performance under\nhalf-duplex constraints. Extensive numerical results prove the feasibility and\nbenefits of our proposal, materializing the potential of joint multistatic\nimaging and communications in practical D-MIMO deployments.", "published": "2025-10-05 15:11:44", "link": "http://arxiv.org/abs/2510.04240v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "CLEAR: A Closed-Form Minimal-Sensor TDOA/FDOA Estimator for Moving-Source IoT Localization", "abstract": "This paper presents CLEAR -- a closed-form localization estimator with a\nreduced sensor network. The proposed method is a computationally efficient,\ntwo-stage estimator that fuses time-difference-of-arrival (TDOA) and\nfrequency-difference-of-arrival (FDOA) measurements with a minimal number of\nsensors. CLEAR localizes a moving source in N-dimensional space using only N+1\nsensors, achieving the theoretical minimum sensor count. The first stage\nintroduces auxiliary range and range-rate parameters to construct a set of\npseudo-linear equations, solved via weighted least squares. An algebraic\nelimination using Sylvester's resultant then reduces the problem to a quartic\nequation, yielding closed-form estimates for the nuisance variables. A second,\nlightweight linear refinement stage is applied to mitigate residual bias. Under\nmild Gaussian noise assumptions, the estimator's position and velocity\nestimates are statistically efficient, closely approaching the Cramer-Rao lower\nbound (CRLB). Extensive Monte Carlo simulations in 2-D and 3-D scenarios\ndemonstrate CRLB-level accuracy and consistent performance gains over\nrepresentative two-stage and iterative baselines, confirming the method's high\nsuitability for power-constrained, distributed Internet of Things (IoT)\napplications such as UAV tracking and smart transportation.", "published": "2025-10-05 11:28:02", "link": "http://arxiv.org/abs/2510.04160v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Closed-form Solutions for Velocity and Acceleration of a Moving Vehicle Using Range, Range Rate, and Derivative of Range Rate", "abstract": "This letter presents a novel method for estimating the position, velocity,\nand acceleration of a moving target using range-based measurements. Although\nmost existing studies focus on position and velocity estimation, the framework\nof this letter is extended to include acceleration. To achieve this, we propose\nusing the derivative of the range rate, in addition to the range and range rate\nmeasurements. The proposed method estimates the position at first using\nTime-of-Arrival (TOA)-based techniques; then, develops a reformulated least\nsquares (LS) and weighted least squares (WLS) approaches for velocity\nestimation; and finally, employs the derivative of the range rate to estimate\nthe acceleration using previous position and velocity estimates. On the other\nhand, closed-form LS and WLS solutions are derived for both velocity and\nacceleration. The simulation results show that the proposed approach provides\nimproved performance in estimating moving target kinematics compared to\nexisting methods.", "published": "2025-10-05 05:08:29", "link": "http://arxiv.org/abs/2510.04037v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
