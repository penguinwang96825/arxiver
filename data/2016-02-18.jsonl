{"title": "Corpus analysis without prior linguistic knowledge - unsupervised mining\n  of phrases and subphrase structure", "abstract": "When looking at the structure of natural language, \"phrases\" and \"words\" are\ncentral notions. We consider the problem of identifying such \"meaningful\nsubparts\" of language of any length and underlying composition principles in a\ncompletely corpus-based and language-independent way without using any kind of\nprior linguistic knowledge. Unsupervised methods for identifying \"phrases\",\nmining subphrase structure and finding words in a fully automated way are\ndescribed. This can be considered as a step towards automatically computing a\n\"general dictionary and grammar of the corpus\". We hope that in the long run\nvariants of our approach turn out to be useful for other kind of sequence data\nas well, such as, e.g., speech, genom sequences, or music annotation. Even if\nwe are not primarily interested in immediate applications, results obtained for\na variety of languages show that our methods are interesting for many practical\ntasks in text mining, terminology extraction and lexicography, search engine\ntechnology, and related fields.", "published": "2016-02-18 12:08:05", "link": "http://arxiv.org/abs/1602.05772v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Interaction of Memory and Attention in Novel Word Generalization: A\n  Computational Investigation", "abstract": "People exhibit a tendency to generalize a novel noun to the basic-level in a\nhierarchical taxonomy -- a cognitively salient category such as \"dog\" -- with\nthe degree of generalization depending on the number and type of exemplars.\nRecently, a change in the presentation timing of exemplars has also been shown\nto have an effect, surprisingly reversing the prior observed pattern of\nbasic-level generalization. We explore the precise mechanisms that could lead\nto such behavior by extending a computational model of word learning and word\ngeneralization to integrate cognitive processes of memory and attention. Our\nresults show that the interaction of forgetting and attention to novelty, as\nwell as sensitivity to both type and token frequencies of exemplars, enables\nthe model to replicate the empirical results from different presentation\ntimings. Our results reinforce the need to incorporate general cognitive\nprocesses within word learning models to better understand the range of\nobserved behaviors in vocabulary acquisition.", "published": "2016-02-18 20:53:26", "link": "http://arxiv.org/abs/1602.05944v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overview of Annotation Creation: Processes & Tools", "abstract": "Creating linguistic annotations requires more than just a reliable annotation\nscheme. Annotation can be a complex endeavour potentially involving many\npeople, stages, and tools. This chapter outlines the process of creating\nend-to-end linguistic annotations, identifying specific tasks that researchers\noften perform. Because tool support is so central to achieving high quality,\nreusable annotations with low cost, the focus is on identifying capabilities\nthat are necessary or useful for annotation tools, as well as common problems\nthese tools present that reduce their utility. Although examples of specific\ntools are provided in many cases, this chapter concentrates more on abstract\ncapabilities and problems because new tools appear continuously, while old\ntools disappear into disuse or disrepair. The two core capabilities tools must\nhave are support for the chosen annotation scheme and the ability to work on\nthe language under study. Additional capabilities are organized into three\ncategories: those that are widely provided; those that often useful but found\nin only a few tools; and those that have as yet little or no available tool\nsupport.", "published": "2016-02-18 10:56:46", "link": "http://arxiv.org/abs/1602.05753v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Entity Embeddings with Conceptual Subspaces as a Basis for Plausible\n  Reasoning", "abstract": "Conceptual spaces are geometric representations of conceptual knowledge, in\nwhich entities correspond to points, natural properties correspond to convex\nregions, and the dimensions of the space correspond to salient features. While\nconceptual spaces enable elegant models of various cognitive phenomena, the\nlack of automated methods for constructing such representations have so far\nlimited their application in artificial intelligence. To address this issue, we\npropose a method which learns a vector-space embedding of entities from\nWikipedia and constrains this embedding such that entities of the same semantic\ntype are located in some lower-dimensional subspace. We experimentally\ndemonstrate the usefulness of these subspaces as (approximate) conceptual space\nrepresentations by showing, among others, that important features can be\nmodelled as directions and that natural properties tend to correspond to convex\nregions.", "published": "2016-02-18 11:37:50", "link": "http://arxiv.org/abs/1602.05765v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Convolutional RNN: an Enhanced Model for Extracting Features from\n  Sequential Data", "abstract": "Traditional convolutional layers extract features from patches of data by\napplying a non-linearity on an affine function of the input. We propose a model\nthat enhances this feature extraction process for the case of sequential data,\nby feeding patches of the data into a recurrent neural network and using the\noutputs or hidden states of the recurrent units to compute the extracted\nfeatures. By doing so, we exploit the fact that a window containing a few\nframes of the sequential data is a sequence itself and this additional\nstructure might encapsulate valuable information. In addition, we allow for\nmore steps of computation in the feature extraction process, which is\npotentially beneficial as an affine function followed by a non-linearity can\nresult in too simple features. Using our convolutional recurrent layers we\nobtain an improvement in performance in two audio classification tasks,\ncompared to traditional convolutional layers. Tensorflow code for the\nconvolutional recurrent layers is publicly available in\nhttps://github.com/cruvadom/Convolutional-RNN.", "published": "2016-02-18 16:55:30", "link": "http://arxiv.org/abs/1602.05875v3", "categories": ["stat.ML", "cs.CL"], "primary_category": "stat.ML"}
