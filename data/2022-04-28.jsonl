{"title": "HybriDialogue: An Information-Seeking Dialogue Dataset Grounded on\n  Tabular and Textual Data", "abstract": "A pressing challenge in current dialogue systems is to successfully converse\nwith users on topics with information distributed across different modalities.\nPrevious work in multiturn dialogue systems has primarily focused on either\ntext or table information. In more realistic scenarios, having a joint\nunderstanding of both is critical as knowledge is typically distributed over\nboth unstructured and structured forms. We present a new dialogue dataset,\nHybriDialogue, which consists of crowdsourced natural conversations grounded on\nboth Wikipedia text and tables. The conversations are created through the\ndecomposition of complex multihop questions into simple, realistic multiturn\ndialogue interactions. We propose retrieval, system state tracking, and\ndialogue response generation tasks for our dataset and conduct baseline\nexperiments for each. Our results show that there is still ample opportunity\nfor improvement, demonstrating the importance of building stronger dialogue\nsystems that can reason over the complex setting of information-seeking\ndialogue grounded on tables and text.", "published": "2022-04-28 00:52:16", "link": "http://arxiv.org/abs/2204.13243v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-modal Memory Networks for Radiology Report Generation", "abstract": "Medical imaging plays a significant role in clinical practice of medical\ndiagnosis, where the text reports of the images are essential in understanding\nthem and facilitating later treatments. By generating the reports\nautomatically, it is beneficial to help lighten the burden of radiologists and\nsignificantly promote clinical automation, which already attracts much\nattention in applying artificial intelligence to medical domain. Previous\nstudies mainly follow the encoder-decoder paradigm and focus on the aspect of\ntext generation, with few studies considering the importance of cross-modal\nmappings and explicitly exploit such mappings to facilitate radiology report\ngeneration. In this paper, we propose a cross-modal memory networks (CMN) to\nenhance the encoder-decoder framework for radiology report generation, where a\nshared memory is designed to record the alignment between images and texts so\nas to facilitate the interaction and generation across modalities. Experimental\nresults illustrate the effectiveness of our proposed model, where\nstate-of-the-art performance is achieved on two widely used benchmark datasets,\ni.e., IU X-Ray and MIMIC-CXR. Further analyses also prove that our model is\nable to better align information from radiology images and texts so as to help\ngenerating more accurate reports in terms of clinical indicators.", "published": "2022-04-28 02:32:53", "link": "http://arxiv.org/abs/2204.13258v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving robustness of language models from a geometry-aware\n  perspective", "abstract": "Recent studies have found that removing the norm-bounded projection and\nincreasing search steps in adversarial training can significantly improve\nrobustness. However, we observe that a too large number of search steps can\nhurt accuracy. We aim to obtain strong robustness efficiently using fewer\nsteps. Through a toy experiment, we find that perturbing the clean data to the\ndecision boundary but not crossing it does not degrade the test accuracy.\nInspired by this, we propose friendly adversarial data augmentation (FADA) to\ngenerate friendly adversarial data. On top of FADA, we propose geometry-aware\nadversarial training (GAT) to perform adversarial training on friendly\nadversarial data so that we can save a large number of search steps.\nComprehensive experiments across two widely used datasets and three pre-trained\nlanguage models demonstrate that GAT can obtain stronger robustness via fewer\nsteps. In addition, we provide extensive empirical results and in-depth\nanalyses on robustness to facilitate future studies.", "published": "2022-04-28 07:07:47", "link": "http://arxiv.org/abs/2204.13309v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Copenhagen Corpus of Eye Tracking Recordings from Natural Reading of\n  Danish Texts", "abstract": "Eye movement recordings from reading are one of the richest signals of human\nlanguage processing. Corpora of eye movements during reading of contextualized\nrunning text is a way of making such records available for natural language\nprocessing purposes. Such corpora already exist in some languages. We present\nCopCo, the Copenhagen Corpus of eye tracking recordings from natural reading of\nDanish texts. It is the first eye tracking corpus of its kind for the Danish\nlanguage. CopCo includes 1,832 sentences with 34,897 tokens of Danish text\nextracted from a collection of speech manuscripts. This first release of the\ncorpus contains eye tracking data from 22 participants. It will be extended\ncontinuously with more participants and texts from other genres. We assess the\ndata quality of the recorded eye movements and find that the extracted features\nare in line with related research. The dataset available here:\nhttps://osf.io/ud8s5/.", "published": "2022-04-28 07:13:00", "link": "http://arxiv.org/abs/2204.13311v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniTE: Unified Translation Evaluation", "abstract": "Translation quality evaluation plays a crucial role in machine translation.\nAccording to the input format, it is mainly separated into three tasks, i.e.,\nreference-only, source-only and source-reference-combined. Recent methods,\ndespite their promising results, are specifically designed and optimized on one\nof them. This limits the convenience of these methods, and overlooks the\ncommonalities among tasks. In this paper, we propose UniTE, which is the first\nunified framework engaged with abilities to handle all three evaluation tasks.\nConcretely, we propose monotonic regional attention to control the interaction\namong input segments, and unified pretraining to better adapt multi-task\nlearning. We testify our framework on WMT 2019 Metrics and WMT 2020 Quality\nEstimation benchmarks. Extensive analyses show that our \\textit{single model}\ncan universally surpass various state-of-the-art or winner methods across\ntasks. Both source code and associated models are available at\nhttps://github.com/NLP2CT/UniTE.", "published": "2022-04-28 08:35:26", "link": "http://arxiv.org/abs/2204.13346v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RoBLEURT Submission for the WMT2021 Metrics Task", "abstract": "In this paper, we present our submission to Shared Metrics Task: RoBLEURT\n(Robustly Optimizing the training of BLEURT). After investigating the recent\nadvances of trainable metrics, we conclude several aspects of vital importance\nto obtain a well-performed metric model by: 1) jointly leveraging the\nadvantages of source-included model and reference-only model, 2) continuously\npre-training the model with massive synthetic data pairs, and 3) fine-tuning\nthe model with data denoising strategy. Experimental results show that our\nmodel reaching state-of-the-art correlations with the WMT2020 human annotations\nupon 8 out of 10 to-English language pairs.", "published": "2022-04-28 08:49:40", "link": "http://arxiv.org/abs/2204.13352v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention Mechanism with Energy-Friendly Operations", "abstract": "Attention mechanism has become the dominant module in natural language\nprocessing models. It is computationally intensive and depends on massive\npower-hungry multiplications. In this paper, we rethink variants of attention\nmechanism from the energy consumption aspects. After reaching the conclusion\nthat the energy costs of several energy-friendly operations are far less than\ntheir multiplication counterparts, we build a novel attention model by\nreplacing multiplications with either selective operations or additions.\nEmpirical results on three machine translation tasks demonstrate that the\nproposed model, against the vanilla one, achieves competitable accuracy while\nsaving 99\\% and 66\\% energy during alignment calculation and the whole\nattention procedure. Code is available at: https://github.com/NLP2CT/E-Att.", "published": "2022-04-28 08:50:09", "link": "http://arxiv.org/abs/2204.13353v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neighbors Are Not Strangers: Improving Non-Autoregressive Translation\n  under Low-Frequency Lexical Constraints", "abstract": "However, current autoregressive approaches suffer from high latency. In this\npaper, we focus on non-autoregressive translation (NAT) for this problem for\nits efficiency advantage. We identify that current constrained NAT models,\nwhich are based on iterative editing, do not handle low-frequency constraints\nwell. To this end, we propose a plug-in algorithm for this line of work, i.e.,\nAligned Constrained Training (ACT), which alleviates this problem by\nfamiliarizing the model with the source-side context of the constraints.\nExperiments on the general and domain datasets show that our model improves\nover the backbone constrained NAT model in constraint preservation and\ntranslation quality, especially for rare constraints.", "published": "2022-04-28 08:57:47", "link": "http://arxiv.org/abs/2204.13355v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tailor: A Prompt-Based Approach to Attribute-Based Controlled Text\n  Generation", "abstract": "Attribute-based Controlled Text Generation (CTG) refers to generating\nsentences that satisfy desirable attributes (e.g., emotions and topics).\nExisting works often utilize fine-tuning or resort to extra attribute\nclassifiers, yet suffer from storage and inference time increases. To address\nthese concerns, we explore attribute-based CTG in a prompt-based manner. In\nshort, the proposed Tailor represents each attribute as a pre-trained\ncontinuous vector (i.e., single-attribute prompt) and guides the generation of\na fixed PLM switch to a pre-specified attribute. We experimentally find that\nthese prompts can be simply concatenated as a whole to multi-attribute CTG\nwithout any re-training, yet raises problems of fluency decrease and position\nsensitivity. To this end, Tailor provides a multi-attribute prompt mask and a\nre-indexing position-ids sequence to bridge the gap between the training (one\nprompt for each task) and testing stage (concatenating more than one prompt).\nTo further enhance such single-attribute prompt combinations, Tailor also\nintroduces a trainable prompt connector, which can be concatenated with any two\nsingle-attribute prompts to multi-attribute text generation. Experiments on 11\nattribute-specific generation tasks demonstrate strong performances of Tailor\non both single-attribute and multi-attribute CTG, with 0.08\\% training\nparameters of a GPT-2.", "published": "2022-04-28 09:09:45", "link": "http://arxiv.org/abs/2204.13362v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Placing M-Phasis on the Plurality of Hate: A Feature-Based Corpus of\n  Hate Online", "abstract": "Even though hate speech (HS) online has been an important object of research\nin the last decade, most HS-related corpora over-simplify the phenomenon of\nhate by attempting to label user comments as \"hate\" or \"neutral\". This ignores\nthe complex and subjective nature of HS, which limits the real-life\napplicability of classifiers trained on these corpora. In this study, we\npresent the M-Phasis corpus, a corpus of ~9k German and French user comments\ncollected from migration-related news articles. It goes beyond the\n\"hate\"-\"neutral\" dichotomy and is instead annotated with 23 features, which in\ncombination become descriptors of various types of speech, ranging from\ncritical comments to implicit and explicit expressions of hate. The annotations\nare performed by 4 native speakers per language and achieve high (0.77 <= k <=\n1) inter-annotator agreements. Besides describing the corpus creation and\npresenting insights from a content, error and domain analysis, we explore its\ndata characteristics by training several classification baselines.", "published": "2022-04-28 10:36:49", "link": "http://arxiv.org/abs/2204.13400v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification", "abstract": "Hierarchical text classification (HTC) is a challenging subtask of\nmulti-label classification due to its complex label hierarchy. Recently, the\npretrained language models (PLM)have been widely adopted in HTC through a\nfine-tuning paradigm. However, in this paradigm, there exists a huge gap\nbetween the classification tasks with sophisticated label hierarchy and the\nmasked language model (MLM) pretraining tasks of PLMs and thus the potentials\nof PLMs can not be fully tapped. To bridge the gap, in this paper, we propose\nHPT, a Hierarchy-aware Prompt Tuning method to handle HTC from a multi-label\nMLM perspective. Specifically, we construct a dynamic virtual template and\nlabel words that take the form of soft prompts to fuse the label hierarchy\nknowledge and introduce a zero-bounded multi-label cross entropy loss to\nharmonize the objectives of HTC and MLM. Extensive experiments show HPT\nachieves state-of-the-art performances on 3 popular HTC datasets and is adept\nat handling the imbalance and low resource situations. Our code is available at\nhttps://github.com/wzh9969/HPT.", "published": "2022-04-28 11:22:49", "link": "http://arxiv.org/abs/2204.13413v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Post-Training Dialogue Summarization using Pseudo-Paraphrasing", "abstract": "Previous dialogue summarization techniques adapt large language models\npretrained on the narrative text by injecting dialogue-specific features into\nthe models. These features either require additional knowledge to recognize or\nmake the resulting models harder to tune. To bridge the format gap between\ndialogues and narrative summaries in dialogue summarization tasks, we propose\nto post-train pretrained language models (PLMs) to rephrase from dialogue to\nnarratives. After that, the model is fine-tuned for dialogue summarization as\nusual. Comprehensive experiments show that our approach significantly improves\nvanilla PLMs on dialogue summarization and outperforms other SOTA models by the\nsummary quality and implementation costs.", "published": "2022-04-28 13:42:19", "link": "http://arxiv.org/abs/2204.13498v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Effect of Pretraining Corpora on In-context Learning by a\n  Large-scale Language Model", "abstract": "Many recent studies on large-scale language models have reported successful\nin-context zero- and few-shot learning ability. However, the in-depth analysis\nof when in-context learning occurs is still lacking. For example, it is unknown\nhow in-context learning performance changes as the training corpus varies.\nHere, we investigate the effects of the source and size of the pretraining\ncorpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From\nour in-depth investigation, we introduce the following observations: (1)\nin-context learning performance heavily depends on the corpus domain source,\nand the size of the pretraining corpus does not necessarily determine the\nemergence of in-context learning, (2) in-context learning ability can emerge\nwhen a language model is trained on a combination of multiple corpora, even\nwhen each corpus does not result in in-context learning on its own, (3)\npretraining with a corpus related to a downstream task does not always\nguarantee the competitive in-context learning performance of the downstream\ntask, especially in the few-shot setting, and (4) the relationship between\nlanguage modeling (measured in perplexity) and in-context learning does not\nalways correlate: e.g., low perplexity does not always imply high in-context\nfew-shot learning performance.", "published": "2022-04-28 13:59:54", "link": "http://arxiv.org/abs/2204.13509v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RobBERTje: a Distilled Dutch BERT Model", "abstract": "Pre-trained large-scale language models such as BERT have gained a lot of\nattention thanks to their outstanding performance on a wide range of natural\nlanguage tasks. However, due to their large number of parameters, they are\nresource-intensive both to deploy and to fine-tune. Researchers have created\nseveral methods for distilling language models into smaller ones to increase\nefficiency, with a small performance trade-off. In this paper, we create\nseveral different distilled versions of the state-of-the-art Dutch RobBERT\nmodel and call them RobBERTje. The distillations differ in their distillation\ncorpus, namely whether or not they are shuffled and whether they are merged\nwith subsequent sentences. We found that the performance of the models using\nthe shuffled versus non-shuffled datasets is similar for most tasks and that\nrandomly merging subsequent sentences in a corpus creates models that train\nfaster and perform better on tasks with long sequences. Upon comparing\ndistillation architectures, we found that the larger DistilBERT architecture\nworked significantly better than the Bort hyperparametrization. Interestingly,\nwe also found that the distilled models exhibit less gender-stereotypical bias\nthan its teacher model. Since smaller architectures decrease the time to\nfine-tune, these models allow for more efficient training and more lightweight\ndeployment of many Dutch downstream language tasks.", "published": "2022-04-28 14:02:13", "link": "http://arxiv.org/abs/2204.13511v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UM6P-CS at SemEval-2022 Task 11: Enhancing Multilingual and Code-Mixed\n  Complex Named Entity Recognition via Pseudo Labels using Multilingual\n  Transformer", "abstract": "Building real-world complex Named Entity Recognition (NER) systems is a\nchallenging task. This is due to the complexity and ambiguity of named entities\nthat appear in various contexts such as short input sentences, emerging\nentities, and complex entities. Besides, real-world queries are mostly\nmalformed, as they can be code-mixed or multilingual, among other scenarios. In\nthis paper, we introduce our submitted system to the Multilingual Complex Named\nEntity Recognition (MultiCoNER) shared task. We approach the complex NER for\nmultilingual and code-mixed queries, by relying on the contextualized\nrepresentation provided by the multilingual Transformer XLM-RoBERTa. In\naddition to the CRF-based token classification layer, we incorporate a span\nclassification loss to recognize named entities spans. Furthermore, we use a\nself-training mechanism to generate weakly-annotated data from a large\nunlabeled dataset. Our proposed system is ranked 6th and 8th in the\nmultilingual and code-mixed MultiCoNER's tracks respectively.", "published": "2022-04-28 14:07:06", "link": "http://arxiv.org/abs/2204.13515v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What do You Mean by Relation Extraction? A Survey on Datasets and Study\n  on Scientific Relation Classification", "abstract": "Over the last five years, research on Relation Extraction (RE) witnessed\nextensive progress with many new dataset releases. At the same time, setup\nclarity has decreased, contributing to increased difficulty of reliable\nempirical evaluation (Taill\\'e et al., 2020). In this paper, we provide a\ncomprehensive survey of RE datasets, and revisit the task definition and its\nadoption by the community. We find that cross-dataset and cross-domain setups\nare particularly lacking. We present an empirical study on scientific Relation\nClassification across two datasets. Despite large data overlap, our analysis\nreveals substantial discrepancies in annotation. Annotation discrepancies\nstrongly impact Relation Classification performance, explaining large drops in\ncross-dataset evaluations. Variation within further sub-domains exists but\nimpacts Relation Classification only to limited degrees. Overall, our study\ncalls for more rigour in reporting setups in RE and evaluation across multiple\ntest sets.", "published": "2022-04-28 14:07:25", "link": "http://arxiv.org/abs/2204.13516v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Life is not Always Depressing: Exploring the Happy Moments of People\n  Diagnosed with Depression", "abstract": "In this work, we explore the relationship between depression and\nmanifestations of happiness in social media. While the majority of works\nsurrounding depression focus on symptoms, psychological research shows that\nthere is a strong link between seeking happiness and being diagnosed with\ndepression. We make use of Positive-Unlabeled learning paradigm to\nautomatically extract happy moments from social media posts of both controls\nand users diagnosed with depression, and qualitatively analyze them with\nlinguistic tools such as LIWC and keyness information. We show that the life of\ndepressed individuals is not always bleak, with positive events related to\nfriends and family being more noteworthy to their lives compared to the more\nmundane happy events reported by control users.", "published": "2022-04-28 15:32:04", "link": "http://arxiv.org/abs/2204.13569v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NMTScore: A Multilingual Analysis of Translation-based Text Similarity\n  Measures", "abstract": "Being able to rank the similarity of short text segments is an interesting\nbonus feature of neural machine translation. Translation-based similarity\nmeasures include direct and pivot translation probability, as well as\ntranslation cross-likelihood, which has not been studied so far. We analyze\nthese measures in the common framework of multilingual NMT, releasing the\nNMTScore library (available at https://github.com/ZurichNLP/nmtscore). Compared\nto baselines such as sentence embeddings, translation-based measures prove\ncompetitive in paraphrase identification and are more robust against\nadversarial or multilingual input, especially if proper normalization is\napplied. When used for reference-based evaluation of data-to-text generation in\n2 tasks and 17 languages, translation-based measures show a relatively high\ncorrelation to human judgments.", "published": "2022-04-28 17:57:17", "link": "http://arxiv.org/abs/2204.13692v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HiNER: A Large Hindi Named Entity Recognition Dataset", "abstract": "Named Entity Recognition (NER) is a foundational NLP task that aims to\nprovide class labels like Person, Location, Organisation, Time, and Number to\nwords in free text. Named Entities can also be multi-word expressions where the\nadditional I-O-B annotation information helps label them during the NER\nannotation process. While English and European languages have considerable\nannotated data for the NER task, Indian languages lack on that front -- both in\nterms of quantity and following annotation standards. This paper releases a\nsignificantly sized standard-abiding Hindi NER dataset containing 109,146\nsentences and 2,220,856 tokens, annotated with 11 tags. We discuss the dataset\nstatistics in all their essential detail and provide an in-depth analysis of\nthe NER tag-set used with our data. The statistics of tag-set in our dataset\nshow a healthy per-tag distribution, especially for prominent classes like\nPerson, Location and Organisation. Since the proof of resource-effectiveness is\nin building models with the resource and testing the model on benchmark data\nand against the leader-board entries in shared tasks, we do the same with the\naforesaid data. We use different language models to perform the sequence\nlabelling task for NER and show the efficacy of our data by performing a\ncomparative evaluation with models trained on another dataset available for the\nHindi NER task. Our dataset helps achieve a weighted F1 score of 88.78 with all\nthe tags and 92.22 when we collapse the tag-set, as discussed in the paper. To\nthe best of our knowledge, no available dataset meets the standards of volume\n(amount) and variability (diversity), as far as Hindi NER is concerned. We fill\nthis gap through this work, which we hope will significantly help NLP for\nHindi. We release this dataset with our code and models at\nhttps://github.com/cfiltnlp/HiNER", "published": "2022-04-28 19:14:21", "link": "http://arxiv.org/abs/2204.13743v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Faithful to the Document or to the World? Mitigating Hallucinations via\n  Entity-linked Knowledge in Abstractive Summarization", "abstract": "Despite recent advances in abstractive summarization, current summarization\nsystems still suffer from content hallucinations where models generate text\nthat is either irrelevant or contradictory to the source document. However,\nprior work has been predicated on the assumption that any generated facts not\nappearing explicitly in the source are undesired hallucinations. Methods have\nbeen proposed to address this scenario by ultimately improving `faithfulness'\nto the source document, but in reality, there is a large portion of entities in\nthe gold reference targets that are not directly in the source. In this work,\nwe show that these entities are not aberrations, but they instead require\nutilizing external world knowledge to infer reasoning paths from entities in\nthe source. We show that by utilizing an external knowledge base, we can\nimprove the faithfulness of summaries without simply making them more\nextractive, and additionally, we show that external knowledge bases linked from\nthe source can benefit the factuality of generated summaries.", "published": "2022-04-28 20:28:45", "link": "http://arxiv.org/abs/2204.13761v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inferring Implicit Relations in Complex Questions with Language Models", "abstract": "A prominent challenge for modern language understanding systems is the\nability to answer implicit reasoning questions, where the required reasoning\nsteps for answering the question are not mentioned in the text explicitly. In\nthis work, we investigate why current models struggle with implicit reasoning\nquestion answering (QA) tasks, by decoupling inference of reasoning steps from\ntheir execution. We define a new task of implicit relation inference and\nconstruct a benchmark, IMPLICITRELATIONS, where given a question, a model\nshould output a list of concept-relation pairs, where the relations describe\nthe implicit reasoning steps required for answering the question. Using\nIMPLICITRELATIONS, we evaluate models from the GPT-3 family and find that,\nwhile these models struggle on the implicit reasoning QA task, they often\nsucceed at inferring implicit relations. This suggests that the challenge in\nimplicit reasoning questions does not stem from the need to plan a reasoning\nstrategy alone, but to do it while also retrieving and reasoning over relevant\ninformation.", "published": "2022-04-28 21:00:54", "link": "http://arxiv.org/abs/2204.13778v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Sentence Embedding Models Performance for Patent Analysis", "abstract": "Patent data is an important source of knowledge for innovation research,\nwhile the technological similarity between pairs of patents is a key enabling\nindicator for patent analysis. Recently researchers have been using patent\nvector space models based on different NLP embeddings models to calculate the\ntechnological similarity between pairs of patents to help better understand\ninnovations, patent landscaping, technology mapping, and patent quality\nevaluation. More often than not, Text Embedding is a vital precursor to patent\nanalysis tasks. A pertinent question then arises: How should we measure and\nevaluate the accuracy of these embeddings? To the best of our knowledge, there\nis no comprehensive survey that builds a clear delineation of embedding models'\nperformance for calculating patent similarity indicators. Therefore, in this\nstudy, we provide an overview of the accuracy of these algorithms based on\npatent classification performance and propose a standard library and dataset\nfor assessing the accuracy of embeddings models based on PatentSBERTa approach.\nIn a detailed discussion, we report the performance of the top 3 algorithms at\nsection, class, and subclass levels. The results based on the first claim of\npatents show that PatentSBERTa, Bert-for-patents, and TF-IDF Weighted Word\nEmbeddings have the best accuracy for computing sentence embeddings at the\nsubclass level. According to the first results, the performance of the models\nin different classes varies, which shows researchers in patent analysis can\nutilize the results of this study to choose the best proper model based on the\nspecific section of patent data they used.", "published": "2022-04-28 12:04:42", "link": "http://arxiv.org/abs/2206.02690v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "D3: A Massive Dataset of Scholarly Metadata for Analyzing the State of\n  Computer Science Research", "abstract": "DBLP is the largest open-access repository of scientific articles on computer\nscience and provides metadata associated with publications, authors, and\nvenues. We retrieved more than 6 million publications from DBLP and extracted\npertinent metadata (e.g., abstracts, author affiliations, citations) from the\npublication texts to create the DBLP Discovery Dataset (D3). D3 can be used to\nidentify trends in research activity, productivity, focus, bias, accessibility,\nand impact of computer science research. We present an initial analysis focused\non the volume of computer science research (e.g., number of papers, authors,\nresearch activity), trends in topics of interest, and citation patterns. Our\nfindings show that computer science is a growing research field (approx. 15%\nannually), with an active and collaborative researcher community. While papers\nin recent years present more bibliographical entries in comparison to previous\ndecades, the average number of citations has been declining. Investigating\npapers' abstracts reveals that recent topic trends are clearly reflected in D3.\nFinally, we list further applications of D3 and pose supplemental research\nquestions. The D3 dataset, our findings, and source code are publicly available\nfor research purposes.", "published": "2022-04-28 09:59:52", "link": "http://arxiv.org/abs/2204.13384v4", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "WeaNF: Weak Supervision with Normalizing Flows", "abstract": "A popular approach to decrease the need for costly manual annotation of large\ndata sets is weak supervision, which introduces problems of noisy labels,\ncoverage and bias. Methods for overcoming these problems have either relied on\ndiscriminative models, trained with cost functions specific to weak\nsupervision, and more recently, generative models, trying to model the output\nof the automatic annotation process. In this work, we explore a novel direction\nof generative modeling for weak supervision: Instead of modeling the output of\nthe annotation process (the labeling function matches), we generatively model\nthe input-side data distributions (the feature space) covered by labeling\nfunctions. Specifically, we estimate a density for each weak labeling source,\nor labeling function, by using normalizing flows. An integral part of our\nmethod is the flow-based modeling of multiple simultaneously matching labeling\nfunctions, and therefore phenomena such as labeling function overlap and\ncorrelations are captured. We analyze the effectiveness and modeling\ncapabilities on various commonly used weak supervision data sets, and show that\nweakly supervised normalizing flows compare favorably to standard weak\nsupervision baselines.", "published": "2022-04-28 10:59:54", "link": "http://arxiv.org/abs/2204.13409v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Simplifying Multilingual News Clustering Through Projection From a\n  Shared Space", "abstract": "The task of organizing and clustering multilingual news articles for media\nmonitoring is essential to follow news stories in real time. Most approaches to\nthis task focus on high-resource languages (mostly English), with low-resource\nlanguages being disregarded. With that in mind, we present a much simpler\nonline system that is able to cluster an incoming stream of documents without\ndepending on language-specific features. We empirically demonstrate that the\nuse of multilingual contextual embeddings as the document representation\nsignificantly improves clustering quality. We challenge previous crosslingual\napproaches by removing the precondition of building monolingual clusters. We\nmodel the clustering process as a set of linear classifiers to aggregate\nsimilar documents, and correct closely-related multilingual clusters through\nmerging in an online fashion. Our system achieves state-of-the-art results on a\nmultilingual news stream clustering dataset, and we introduce a new evaluation\nfor zero-shot news clustering in multiple languages. We make our code available\nas open-source.", "published": "2022-04-28 11:32:49", "link": "http://arxiv.org/abs/2204.13418v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "EVI: Multilingual Spoken Dialogue Tasks and Dataset for Knowledge-Based\n  Enrolment, Verification, and Identification", "abstract": "Knowledge-based authentication is crucial for task-oriented spoken dialogue\nsystems that offer personalised and privacy-focused services. Such systems\nshould be able to enrol (E), verify (V), and identify (I) new and recurring\nusers based on their personal information, e.g. postcode, name, and date of\nbirth. In this work, we formalise the three authentication tasks and their\nevaluation protocols, and we present EVI, a challenging spoken multilingual\ndataset with 5,506 dialogues in English, Polish, and French. Our proposed\nmodels set the first competitive benchmarks, explore the challenges of\nmultilingual natural language processing of spoken dialogue, and set directions\nfor future research.", "published": "2022-04-28 13:39:24", "link": "http://arxiv.org/abs/2204.13496v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Label Search for Zero-Shot Multi-Lingual Extractive Summarization", "abstract": "In zero-shot multilingual extractive text summarization, a model is typically\ntrained on English summarization dataset and then applied on summarization\ndatasets of other languages. Given English gold summaries and documents,\nsentence-level labels for extractive summarization are usually generated using\nheuristics. However, these monolingual labels created on English datasets may\nnot be optimal on datasets of other languages, for that there is the syntactic\nor semantic discrepancy between different languages. In this way, it is\npossible to translate the English dataset to other languages and obtain\ndifferent sets of labels again using heuristics. To fully leverage the\ninformation of these different sets of labels, we propose NLSSum (Neural Label\nSearch for Summarization), which jointly learns hierarchical weights for these\ndifferent sets of labels together with our summarization model. We conduct\nmultilingual zero-shot summarization experiments on MLSUM and WikiLingua\ndatasets, and we achieve state-of-the-art results using both human and\nautomatic evaluations across these two datasets.", "published": "2022-04-28 14:02:16", "link": "http://arxiv.org/abs/2204.13512v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MeSHup: A Corpus for Full Text Biomedical Document Indexing", "abstract": "Medical Subject Heading (MeSH) indexing refers to the problem of assigning a\ngiven biomedical document with the most relevant labels from an extremely large\nset of MeSH terms. Currently, the vast number of biomedical articles in the\nPubMed database are manually annotated by human curators, which is time\nconsuming and costly; therefore, a computational system that can assist the\nindexing is highly valuable. When developing supervised MeSH indexing systems,\nthe availability of a large-scale annotated text corpus is desirable. A\npublicly available, large corpus that permits robust evaluation and comparison\nof various systems is important to the research community. We release a large\nscale annotated MeSH indexing corpus, MeSHup, which contains 1,342,667 full\ntext articles in English, together with the associated MeSH labels and\nmetadata, authors, and publication venues that are collected from the MEDLINE\ndatabase. We train an end-to-end model that combines features from documents\nand their associated labels on our corpus and report the new baseline.", "published": "2022-04-28 16:04:20", "link": "http://arxiv.org/abs/2204.13604v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Russian Texts Detoxification with Levenshtein Editing", "abstract": "Text detoxification is a style transfer task of creating neutral versions of\ntoxic texts. In this paper, we use the concept of text editing to build a\ntwo-step tagging-based detoxification model using a parallel corpus of Russian\ntexts. With this model, we achieved the best style transfer accuracy among all\nmodels in the RUSSE Detox shared task, surpassing larger sequence-to-sequence\nmodels.", "published": "2022-04-28 16:58:17", "link": "http://arxiv.org/abs/2204.13638v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Instilling Type Knowledge in Language Models via Multi-Task QA", "abstract": "Understanding human language often necessitates understanding entities and\ntheir place in a taxonomy of knowledge -- their types. Previous methods to\nlearn entity types rely on training classifiers on datasets with coarse, noisy,\nand incomplete labels. We introduce a method to instill fine-grained type\nknowledge in language models with text-to-text pre-training on type-centric\nquestions leveraging knowledge base documents and knowledge graphs. We create\nthe WikiWiki dataset: entities and passages from 10M Wikipedia articles linked\nto the Wikidata knowledge graph with 41K types. Models trained on WikiWiki\nachieve state-of-the-art performance in zero-shot dialog state tracking\nbenchmarks, accurately infer entity types in Wikipedia articles, and can\ndiscover new types deemed useful by human judges.", "published": "2022-04-28 22:06:32", "link": "http://arxiv.org/abs/2204.13796v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Investigating writing style as a contributor to gender gaps in science\n  and technology", "abstract": "A growing stream of research finds that scientific contributions are\nevaluated differently depending on the gender of the author. In this article,\nwe consider whether gender differences in writing styles - how men and women\ncommunicate their work - may contribute to these observed gender gaps. We\nground our investigation in a framework for characterizing the linguistic style\nof written text, with two sets of features - informational (i.e., features that\nemphasize facts) and involved (i.e., features that emphasize relationships).\nUsing a large sample of academic papers and patents, we find significant\ndifferences in writing style by gender, with women using more involved features\nin their writing. Papers and patents with more involved features also tend to\nbe cited more by women. Our findings suggest that scientific text is not devoid\nof personal character, which could contribute to bias in evaluation, thereby\ncompromising the norm of universalism as a foundational principle of science.", "published": "2022-04-28 22:33:36", "link": "http://arxiv.org/abs/2204.13805v3", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Local dynamic mode of Cognitive Behavioral Therapy", "abstract": "In order to increase mental health equity among the most vulnerable and\nmarginalized communities, it is important to increase access to high-quality\ntherapists. One facet of addressing these needs, is to provide timely feedback\nto clinicians as they interact with their clients, in a way that is also\ncontextualized to specific clients and interactions they have had. Dynamical\nsystems provide a framework through which to analyze interactions. The present\nwork applies these methods to the domain of automated psychotherapist\nevaluation for Cognitive Behavioral Therapy (CBT). Our methods extract local\ndynamic modes from short windows of conversation and learns to correlate the\nobserved dynamics to CBT competence. The results demonstrate the value of this\nparadigm and outlines the way in which these methods can be used to study and\nimprove therapeutic strategies.", "published": "2022-04-28 15:03:35", "link": "http://arxiv.org/abs/2205.09752v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Machine Learning for Violence Risk Assessment Using Dutch Clinical Notes", "abstract": "Violence risk assessment in psychiatric institutions enables interventions to\navoid violence incidents. Clinical notes written by practitioners and available\nin electronic health records are valuable resources capturing unique\ninformation, but are seldom used to their full potential. We explore\nconventional and deep machine learning methods to assess violence risk in\npsychiatric patients using practitioner notes. The performance of our best\nmodels is comparable to the currently used questionnaire-based method, with an\narea under the Receiver Operating Characteristic curve of approximately 0.8. We\nfind that the deep-learning model BERTje performs worse than conventional\nmachine learning methods. We also evaluate our data and our classifiers to\nunderstand the performance of our models better. This is particularly important\nfor the applicability of evaluated classifiers to new data, and is also of\ngreat interest to practitioners, due to the increased availability of new data\nin electronic format.", "published": "2022-04-28 14:36:06", "link": "http://arxiv.org/abs/2204.13535v1", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "CAVES: A Dataset to facilitate Explainable Classification and\n  Summarization of Concerns towards COVID Vaccines", "abstract": "Convincing people to get vaccinated against COVID-19 is a key societal\nchallenge in the present times. As a first step towards this goal, many prior\nworks have relied on social media analysis to understand the specific concerns\nthat people have towards these vaccines, such as potential side-effects,\nineffectiveness, political factors, and so on. Though there are datasets that\nbroadly classify social media posts into Anti-vax and Pro-Vax labels, there is\nno dataset (to our knowledge) that labels social media posts according to the\nspecific anti-vaccine concerns mentioned in the posts. In this paper, we have\ncurated CAVES, the first large-scale dataset containing about 10k COVID-19\nanti-vaccine tweets labelled into various specific anti-vaccine concerns in a\nmulti-label setting. This is also the first multi-label classification dataset\nthat provides explanations for each of the labels. Additionally, the dataset\nalso provides class-wise summaries of all the tweets. We also perform\npreliminary experiments on the dataset and show that this is a very challenging\ndataset for multi-label explainable classification and tweet summarization, as\nis evident by the moderate scores achieved by some state-of-the-art models. Our\ndataset and codes are available at: https://github.com/sohampoddar26/caves-data", "published": "2022-04-28 19:26:54", "link": "http://arxiv.org/abs/2204.13746v2", "categories": ["cs.CL", "cs.CY", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Split for Automatic Bias Detection", "abstract": "Classifiers are biased when trained on biased datasets. As a remedy, we\npropose Learning to Split (ls), an algorithm for automatic bias detection.\nGiven a dataset with input-label pairs, ls learns to split this dataset so that\npredictors trained on the training split cannot generalize to the testing\nsplit. This performance gap suggests that the testing split is\nunder-represented in the dataset, which is a signal of potential bias.\nIdentifying non-generalizable splits is challenging since we have no\nannotations about the bias. In this work, we show that the prediction\ncorrectness of each example in the testing split can be used as a source of\nweak supervision: generalization performance will drop if we move examples that\nare predicted correctly away from the testing split, leaving only those that\nare mis-predicted. ls is task-agnostic and can be applied to any supervised\nlearning problem, ranging from natural language understanding and image\nclassification to molecular property prediction. Empirical results show that ls\nis able to generate astonishingly challenging splits that correlate with\nhuman-identified biases. Moreover, we demonstrate that combining robust\nlearning algorithms (such as group DRO) with splits identified by ls enables\nautomatic de-biasing. Compared to previous state-of-the-art, we substantially\nimprove the worst-group performance (23.4% on average) when the source of\nbiases is unknown during training and validation.", "published": "2022-04-28 19:41:08", "link": "http://arxiv.org/abs/2204.13749v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Pseudo strong labels for large scale weakly supervised audio tagging", "abstract": "Large-scale audio tagging datasets inevitably contain imperfect labels, such\nas clip-wise annotated (temporally weak) tags with no exact on- and offsets,\ndue to a high manual labeling cost. This work proposes pseudo strong labels\n(PSL), a simple label augmentation framework that enhances the supervision\nquality for large-scale weakly supervised audio tagging. A machine annotator is\nfirst trained on a large weakly supervised dataset, which then provides finer\nsupervision for a student model. Using PSL we achieve an mAP of 35.95 balanced\ntrain subset of Audioset using a MobileNetV2 back-end, significantly\noutperforming approaches without PSL. An analysis is provided which reveals\nthat PSL mitigates missing labels. Lastly, we show that models trained with PSL\nare also superior at generalizing to the Freesound datasets (FSD) than their\nweakly trained counterparts.", "published": "2022-04-28 12:01:16", "link": "http://arxiv.org/abs/2204.13430v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music Enhancement via Image Translation and Vocoding", "abstract": "Consumer-grade music recordings such as those captured by mobile devices\ntypically contain distortions in the form of background noise, reverb, and\nmicrophone-induced EQ. This paper presents a deep learning approach to enhance\nlow-quality music recordings by combining (i) an image-to-image translation\nmodel for manipulating audio in its mel-spectrogram representation and (ii) a\nmusic vocoding model for mapping synthetically generated mel-spectrograms to\nperceptually realistic waveforms. We find that this approach to music\nenhancement outperforms baselines which use classical methods for\nmel-spectrogram inversion and an end-to-end approach directly mapping noisy\nwaveforms to clean waveforms. Additionally, in evaluating the proposed method\nwith a listening test, we analyze the reliability of common audio enhancement\nevaluation metrics when used in the music domain.", "published": "2022-04-28 05:00:07", "link": "http://arxiv.org/abs/2204.13289v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Regotron: Regularizing the Tacotron2 architecture via monotonic\n  alignment loss", "abstract": "Recent deep learning Text-to-Speech (TTS) systems have achieved impressive\nperformance by generating speech close to human parity. However, they suffer\nfrom training stability issues as well as incorrect alignment of the\nintermediate acoustic representation with the input text sequence. In this\nwork, we introduce Regotron, a regularized version of Tacotron2 which aims to\nalleviate the training issues and at the same time produce monotonic\nalignments. Our method augments the vanilla Tacotron2 objective function with\nan additional term, which penalizes non-monotonic alignments in the\nlocation-sensitive attention mechanism. By properly adjusting this\nregularization term we show that the loss curves become smoother, and at the\nsame time Regotron consistently produces monotonic alignments in unseen\nexamples even at an early stage (13\\% of the total number of epochs) of its\ntraining process, whereas the fully converged Tacotron2 fails to do so.\nMoreover, our proposed regularization method has no additional computational\noverhead, while reducing common TTS mistakes and achieving slighlty improved\nspeech naturalness according to subjective mean opinion scores (MOS) collected\nfrom 50 evaluators.", "published": "2022-04-28 12:08:53", "link": "http://arxiv.org/abs/2204.13437v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Emotion Recognition In Persian Speech Using Deep Neural Networks", "abstract": "Speech Emotion Recognition (SER) is of great importance in Human-Computer\nInteraction (HCI), as it provides a deeper understanding of the situation and\nresults in better interaction. In recent years, various machine learning and\nDeep Learning (DL) algorithms have been developed to improve SER techniques.\nRecognition of the spoken emotions depends on the type of expression that\nvaries between different languages. In this paper, to further study important\nfactors in the Farsi language, we examine various DL techniques on a\nFarsi/Persian dataset, Sharif Emotional Speech Database (ShEMO), which was\nreleased in 2018. Using signal features in low- and high-level descriptions and\ndifferent deep neural networks and machine learning techniques, Unweighted\nAccuracy (UA) of 65.20% and Weighted Accuracy (WA) of 78.29% are achieved.", "published": "2022-04-28 16:02:05", "link": "http://arxiv.org/abs/2204.13601v2", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T10 (primary) 68T07 (secondary)", "I.2"], "primary_category": "cs.SD"}
{"title": "Fast Cross-Correlation for TDoA Estimation on Small Aperture Microphone\n  Arrays", "abstract": "This paper introduces the Fast Cross-Correlation (FCC) method for Time\nDifference of Arrival (TDoA) Estimation for pairs of microphones on a small\naperture microphone array. FCC relies on low-rank decomposition and exploits\nsymmetry in even and odd bases to speed up computation while preserving TDoA\naccuracy. FCC reduces the number of flops by a factor of 4.5 and the execution\nspeed by factors between 3.5 and 8.3 on embedded hardware, compared to the\nstate-of-the-art Generalized Cross-Correlation (GCC) method that relies on the\nFast Fourier Transform (FFT). This improvement can provide portable microphone\narrays with extended battery life and allow real-time processing on low-cost\nhardware.", "published": "2022-04-28 16:38:22", "link": "http://arxiv.org/abs/2204.13622v3", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype\n  Contrast", "abstract": "We present an approach to learn voice-face representations from the talking\nface videos, without any identity labels. Previous works employ cross-modal\ninstance discrimination tasks to establish the correlation of voice and face.\nThese methods neglect the semantic content of different videos, introducing\nfalse-negative pairs as training noise. Furthermore, the positive pairs are\nconstructed based on the natural correlation between audio clips and visual\nframes. However, this correlation might be weak or inaccurate in a large amount\nof real-world data, which leads to deviating positives into the contrastive\nparadigm. To address these issues, we propose the cross-modal prototype\ncontrastive learning (CMPC), which takes advantage of contrastive methods and\nresists adverse effects of false negatives and deviate positives. On one hand,\nCMPC could learn the intra-class invariance by constructing semantic-wise\npositives via unsupervised clustering in different modalities. On the other\nhand, by comparing the similarities of cross-modal instances from that of\ncross-modal prototypes, we dynamically recalibrate the unlearnable instances'\ncontribution to overall loss. Experiments show that the proposed approach\noutperforms state-of-the-art unsupervised methods on various voice-face\nassociation evaluation protocols. Additionally, in the low-shot supervision\nsetting, our method also has a significant improvement compared to previous\ninstance-wise contrastive learning.", "published": "2022-04-28 07:28:56", "link": "http://arxiv.org/abs/2204.14057v3", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Vehicle Noise: Comparison of Loudness Ratings in the Field and the\n  Laboratory", "abstract": "Objective: Distorted loudness perception is one of the main complaints of\nhearing aid users. Being able to measure loudness perception correctly in the\nclinic is essential for fitting hearing aids. For this, experiments in the\nclinic should be able to reflect and capture loudness perception as in\neveryday-life situations. Little research has been done comparing loudness\nperception in the field and in the laboratory. Design: Participants rated the\nloudness in the field and in the laboratory of 36 driving actions done by four\ndifferent vehicles. The field measurements were done in a restricted street and\nrecorded with a 360deg camera and a tetrahedral microphone. The recorded\nstimuli, which are openly accessible, were presented in three different\nconditions in the laboratory: 360deg video recordings with a head-mounted\ndisplay, video recordings with a desktop monitor, and audio-only. Sample:\nThirteen normal-hearing participants and 18 hearing-impaired participants\nparticipated in the study. Results: The driving actions were rated\nsignificantly louder in the laboratory than in the field for the audio-only\ncondition. These loudness rating differences were bigger for louder sounds in\ntwo laboratory conditions, i.e., the higher the sound level of a driving action\nwas the more likely it was to be rated louder in the laboratory. There were no\nsignificant differences in the loudness ratings between the three laboratory\nconditions and between groups. Conclusions: The results of this experiment\nfurther remark the importance of increasing the realism and immersion when\nmeasuring loudness in the clinic.", "published": "2022-04-28 22:06:35", "link": "http://arxiv.org/abs/2205.02110v1", "categories": ["q-bio.NC", "cs.SD", "eess.AS"], "primary_category": "q-bio.NC"}
{"title": "Unaligned Supervision For Automatic Music Transcription in The Wild", "abstract": "Multi-instrument Automatic Music Transcription (AMT), or the decoding of a\nmusical recording into semantic musical content, is one of the holy grails of\nMusic Information Retrieval. Current AMT approaches are restricted to piano and\n(some) guitar recordings, due to difficult data collection. In order to\novercome data collection barriers, previous AMT approaches attempt to employ\nmusical scores in the form of a digitized version of the same song or piece.\nThe scores are typically aligned using audio features and strenuous human\nintervention to generate training labels. We introduce NoteEM, a method for\nsimultaneously training a transcriber and aligning the scores to their\ncorresponding performances, in a fully-automated process. Using this unaligned\nsupervision scheme, complemented by pseudo-labels and pitch-shift augmentation,\nour method enables training on in-the-wild recordings with unprecedented\naccuracy and instrumental variety. Using only synthetic data and unaligned\nsupervision, we report SOTA note-level accuracy of the MAPS dataset, and large\nfavorable margins on cross-dataset evaluations. We also demonstrate robustness\nand ease of use; we report comparable results when training on a small, easily\nobtainable, self-collected dataset, and we propose alternative labeling to the\nMusicNet dataset, which we show to be more accurate. Our project page is\navailable at https://benadar293.github.io", "published": "2022-04-28 17:31:43", "link": "http://arxiv.org/abs/2204.13668v1", "categories": ["cs.SD", "cs.AI", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
