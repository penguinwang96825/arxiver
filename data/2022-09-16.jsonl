{"title": "NELLIE: A Neuro-Symbolic Inference Engine for Grounded, Compositional,\n  and Explainable Reasoning", "abstract": "Our goal is a modern approach to answering questions via systematic reasoning\nwhere answers are supported by human interpretable proof trees grounded in an\nNL corpus of authoritative facts. Such a system would help alleviate the\nchallenges of interpretability and hallucination with modern LMs, and the lack\nof grounding of current explanation methods (e.g., Chain-of-Thought). This\npaper proposes a new take on Prolog-based inference engines, where we replace\nhandcrafted rules with a combination of neural language modeling, guided\ngeneration, and semiparametric dense retrieval. Our implementation, NELLIE, is\nthe first system to demonstrate fully interpretable, end-to-end grounded QA as\nentailment tree proof search, going beyond earlier work explaining\nknown-to-be-true facts from text. In experiments, NELLIE outperforms a\nsimilar-sized state-of-the-art reasoner [Tafjord et al., 2022] while producing\nknowledge-grounded explanations. We also find NELLIE can exploit both\nsemi-structured and NL text corpora to guide reasoning. Together these suggest\na new way to jointly reap the benefits of both modern neural methods and\ntraditional symbolic reasoning.", "published": "2022-09-16 00:54:44", "link": "http://arxiv.org/abs/2209.07662v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Answering Numerical Reasoning Questions in Table-Text Hybrid Contents\n  with Graph-based Encoder and Tree-based Decoder", "abstract": "In the real-world question answering scenarios, hybrid form combining both\ntabular and textual contents has attracted more and more attention, among which\nnumerical reasoning problem is one of the most typical and challenging\nproblems. Existing methods usually adopt encoder-decoder framework to represent\nhybrid contents and generate answers. However, it can not capture the rich\nrelationship among numerical value, table schema, and text information on the\nencoder side. The decoder uses a simple predefined operator classifier which is\nnot flexible enough to handle numerical reasoning processes with diverse\nexpressions. To address these problems, this paper proposes a\n\\textbf{Re}lational \\textbf{G}raph enhanced \\textbf{H}ybrid table-text\n\\textbf{N}umerical reasoning model with \\textbf{T}ree decoder\n(\\textbf{RegHNT}). It models the numerical question answering over table-text\nhybrid contents as an expression tree generation task. Moreover, we propose a\nnovel relational graph modeling method, which models alignment between\nquestions, tables, and paragraphs. We validated our model on the publicly\navailable table-text hybrid QA benchmark (TAT-QA). The proposed RegHNT\nsignificantly outperform the baseline model and achieve state-of-the-art\nresults. We openly released the source code and data at\nhttps://github.com/lfy79001/RegHNT (2022-05-05).", "published": "2022-09-16 03:15:12", "link": "http://arxiv.org/abs/2209.07692v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SF-DST: Few-Shot Self-Feeding Reading Comprehension Dialogue State\n  Tracking with Auxiliary Task", "abstract": "Few-shot dialogue state tracking (DST) model tracks user requests in dialogue\nwith reliable accuracy even with a small amount of data. In this paper, we\nintroduce an ontology-free few-shot DST with self-feeding belief state input.\nThe self-feeding belief state input increases the accuracy in multi-turn\ndialogue by summarizing previous dialogue. Also, we newly developed a slot-gate\nauxiliary task. This new auxiliary task helps classify whether a slot is\nmentioned in the dialogue. Our model achieved the best score in a few-shot\nsetting for four domains on multiWOZ 2.0.", "published": "2022-09-16 06:54:25", "link": "http://arxiv.org/abs/2209.07742v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Automatic Post-Editing", "abstract": "Automatic post-editing (APE) aims to reduce manual post-editing efforts by\nautomatically correcting errors in machine-translated output. Due to the\nlimited amount of human-annotated training data, data scarcity is one of the\nmain challenges faced by all APE systems. To alleviate the lack of genuine\ntraining data, most of the current APE systems employ data augmentation methods\nto generate large-scale artificial corpora. In view of the importance of data\naugmentation in APE, we separately study the impact of the construction method\nof artificial corpora and artificial data domain on the performance of APE\nmodels. Moreover, the difficulty of APE varies between different machine\ntranslation (MT) systems. We study the outputs of the state-of-art APE model on\na difficult APE dataset to analyze the problems in existing APE systems.\nPrimarily, we find that 1) Artificial corpora with high-quality source text and\nmachine-translated text more effectively improve the performance of APE models;\n2) In-domain artificial training data can better improve the performance of APE\nmodels, while irrelevant out-of-domain data actually interfere with the model;\n3) Existing APE model struggles with cases containing long source text or\nhigh-quality machine-translated text; 4) The state-of-art APE model works well\non grammatical and semantic addition problems, but the output is prone to\nentity and semantic omission errors.", "published": "2022-09-16 07:38:27", "link": "http://arxiv.org/abs/2209.07759v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Whole Truth and Nothing But the Truth: Faithful and Controllable\n  Dialogue Response Generation with Dataflow Transduction and Constrained\n  Decoding", "abstract": "In a real-world dialogue system, generated text must be truthful and\ninformative while remaining fluent and adhering to a prescribed style.\nSatisfying these constraints simultaneously is difficult for the two\npredominant paradigms in language generation: neural language modeling and\nrule-based generation. We describe a hybrid architecture for dialogue response\ngeneration that combines the strengths of both paradigms. The first component\nof this architecture is a rule-based content selection model defined using a\nnew formal framework called dataflow transduction, which uses declarative rules\nto transduce a dialogue agent's actions and their results (represented as\ndataflow graphs) into context-free grammars representing the space of\ncontextually acceptable responses. The second component is a constrained\ndecoding procedure that uses these grammars to constrain the output of a neural\nlanguage model, which selects fluent utterances. Our experiments show that this\nsystem outperforms both rule-based and learned approaches in human evaluations\nof fluency, relevance, and truthfulness.", "published": "2022-09-16 09:00:49", "link": "http://arxiv.org/abs/2209.07800v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity-based Claim Representation Improves Fact-Checking of Medical\n  Content in Tweets", "abstract": "False medical information on social media poses harm to people's health.\nWhile the need for biomedical fact-checking has been recognized in recent\nyears, user-generated medical content has received comparably little attention.\nAt the same time, models for other text genres might not be reusable, because\nthe claims they have been trained with are substantially different. For\ninstance, claims in the SciFact dataset are short and focused: \"Side effects\nassociated with antidepressants increases risk of stroke\". In contrast, social\nmedia holds naturally-occurring claims, often embedded in additional context:\n\"`If you take antidepressants like SSRIs, you could be at risk of a condition\ncalled serotonin syndrome' Serotonin syndrome nearly killed me in 2010. Had\nsymptoms of stroke and seizure.\" This showcases the mismatch between real-world\nmedical claims and the input that existing fact-checking systems expect. To\nmake user-generated content checkable by existing models, we propose to\nreformulate the social-media input in such a way that the resulting claim\nmimics the claim characteristics in established datasets. To accomplish this,\nour method condenses the claim with the help of relational entity information\nand either compiles the claim out of an entity-relation-entity triple or\nextracts the shortest phrase that contains these elements. We show that the\nreformulated input improves the performance of various fact-checking models as\nopposed to checking the tweet text in its entirety.", "published": "2022-09-16 09:59:22", "link": "http://arxiv.org/abs/2209.07834v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Findings of the Shared Task on Multilingual Coreference Resolution", "abstract": "This paper presents an overview of the shared task on multilingual\ncoreference resolution associated with the CRAC 2022 workshop. Shared task\nparticipants were supposed to develop trainable systems capable of identifying\nmentions and clustering them according to identity coreference. The public\nedition of CorefUD 1.0, which contains 13 datasets for 10 languages, was used\nas the source of training and evaluation data. The CoNLL score used in previous\ncoreference-oriented shared tasks was used as the main evaluation metric. There\nwere 8 coreference prediction systems submitted by 5 participating teams; in\naddition, there was a competitive Transformer-based baseline system provided by\nthe organizers at the beginning of the shared task. The winner system\noutperformed the baseline by 12 percentage points (in terms of the CoNLL scores\naveraged across all datasets for individual languages).", "published": "2022-09-16 10:17:06", "link": "http://arxiv.org/abs/2209.07841v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantifying Discourse Support for Omitted Pronouns", "abstract": "Pro-drop is commonly seen in many languages, but its discourse motivations\nhave not been well characterized. Inspired by the topic chain theory in\nChinese, this study shows how character-verb usage continuity distinguishes\ndropped pronouns from overt references to story characters. We model the choice\nto drop vs. not drop as a function of character-verb continuity. The results\nshow that omitted subjects have higher character history-current verb\ncontinuity salience than non-omitted subjects. This is consistent with the idea\nthat discourse coherence with a particular topic, such as a story character,\nindeed facilitates the omission of pronouns in languages and contexts where\nthey are optional.", "published": "2022-09-16 14:21:13", "link": "http://arxiv.org/abs/2209.07961v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer-based Detection of Multiword Expressions in Flower and Plant\n  Names", "abstract": "Multiword expression (MWE) is a sequence of words which collectively present\na meaning which is not derived from its individual words. The task of\nprocessing MWEs is crucial in many natural language processing (NLP)\napplications, including machine translation and terminology extraction.\nTherefore, detecting MWEs in different domains is an important research topic.\nIn this paper, we explore state-of-the-art neural transformers in the task of\ndetecting MWEs in flower and plant names. We evaluate different transformer\nmodels on a dataset created from Encyclopedia of Plants and Flower. We\nempirically show that transformer models outperform the previous neural models\nbased on long short-term memory (LSTM).", "published": "2022-09-16 15:59:55", "link": "http://arxiv.org/abs/2209.08016v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Skill Extraction from Job Postings using Weak Supervision", "abstract": "Aggregated data obtained from job postings provide powerful insights into\nlabor market demands, and emerging skills, and aid job matching. However, most\nextraction approaches are supervised and thus need costly and time-consuming\nannotation. To overcome this, we propose Skill Extraction with Weak\nSupervision. We leverage the European Skills, Competences, Qualifications and\nOccupations taxonomy to find similar skills in job ads via latent\nrepresentations. The method shows a strong positive signal, outperforming\nbaselines based on token-level and syntactic patterns.", "published": "2022-09-16 17:35:19", "link": "http://arxiv.org/abs/2209.08071v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConFiguRe: Exploring Discourse-level Chinese Figures of Speech", "abstract": "Figures of speech, such as metaphor and irony, are ubiquitous in literature\nworks and colloquial conversations. This poses great challenge for natural\nlanguage understanding since figures of speech usually deviate from their\nostensible meanings to express deeper semantic implications. Previous research\nlays emphasis on the literary aspect of figures and seldom provide a\ncomprehensive exploration from a view of computational linguistics. In this\npaper, we first propose the concept of figurative unit, which is the carrier of\na figure. Then we select 12 types of figures commonly used in Chinese, and\nbuild a Chinese corpus for Contextualized Figure Recognition (ConFiguRe).\nDifferent from previous token-level or sentence-level counterparts, ConFiguRe\naims at extracting a figurative unit from discourse-level context, and\nclassifying the figurative unit into the right figure type. On ConFiguRe, three\ntasks, i.e., figure extraction, figure type classification and figure\nrecognition, are designed and the state-of-the-art techniques are utilized to\nimplement the benchmarks. We conduct thorough experiments and show that all\nthree tasks are challenging for existing models, thus requiring further\nresearch. Our dataset and code are publicly available at\nhttps://github.com/pku-tangent/ConFiguRe.", "published": "2022-09-16 02:31:48", "link": "http://arxiv.org/abs/2209.07678v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Selecting Stickers in Open-Domain Dialogue through Multitask Learning", "abstract": "With the increasing popularity of online chatting, stickers are becoming\nimportant in our online communication. Selecting appropriate stickers in\nopen-domain dialogue requires a comprehensive understanding of both dialogues\nand stickers, as well as the relationship between the two types of modalities.\nTo tackle these challenges, we propose a multitask learning method comprised of\nthree auxiliary tasks to enhance the understanding of dialogue history, emotion\nand semantic meaning of stickers. Extensive experiments conducted on a recent\nchallenging dataset show that our model can better combine the multimodal\ninformation and achieve significantly higher accuracy over strong baselines.\nAblation study further verifies the effectiveness of each auxiliary task. Our\ncode is available at \\url{https://github.com/nonstopfor/Sticker-Selection}", "published": "2022-09-16 03:45:22", "link": "http://arxiv.org/abs/2209.07697v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Possible Stories: Evaluating Situated Commonsense Reasoning under\n  Multiple Possible Scenarios", "abstract": "The possible consequences for the same context may vary depending on the\nsituation we refer to. However, current studies in natural language processing\ndo not focus on situated commonsense reasoning under multiple possible\nscenarios. This study frames this task by asking multiple questions with the\nsame set of possible endings as candidate answers, given a short story text.\nOur resulting dataset, Possible Stories, consists of more than 4.5K questions\nover 1.3K story texts in English. We discover that even current strong\npretrained language models struggle to answer the questions consistently,\nhighlighting that the highest accuracy in an unsupervised setting (60.2%) is\nfar behind human accuracy (92.5%). Through a comparison with existing datasets,\nwe observe that the questions in our dataset contain minimal annotation\nartifacts in the answer options. In addition, our dataset includes examples\nthat require counterfactual reasoning, as well as those requiring readers'\nreactions and fictional information, suggesting that our dataset can serve as a\nchallenging testbed for future studies on situated commonsense reasoning.", "published": "2022-09-16 07:38:51", "link": "http://arxiv.org/abs/2209.07760v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Negation, Coordination, and Quantifiers in Contextualized Language\n  Models", "abstract": "With the success of contextualized language models, much research explores\nwhat these models really learn and in which cases they still fail. Most of this\nwork focuses on specific NLP tasks and on the learning outcome. Little research\nhas attempted to decouple the models' weaknesses from specific tasks and focus\non the embeddings per se and their mode of learning. In this paper, we take up\nthis research opportunity: based on theoretical linguistic insights, we explore\nwhether the semantic constraints of function words are learned and how the\nsurrounding context impacts their embeddings. We create suitable datasets,\nprovide new insights into the inner workings of LMs vis-a-vis function words\nand implement an assisting visual web interface for qualitative analysis.", "published": "2022-09-16 10:01:11", "link": "http://arxiv.org/abs/2209.07836v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adaptive Natural Language Generation for Task-oriented Dialogue via\n  Reinforcement Learning", "abstract": "When a natural language generation (NLG) component is implemented in a\nreal-world task-oriented dialogue system, it is necessary to generate not only\nnatural utterances as learned on training data but also utterances adapted to\nthe dialogue environment (e.g., noise from environmental sounds) and the user\n(e.g., users with low levels of understanding ability). Inspired by recent\nadvances in reinforcement learning (RL) for language generation tasks, we\npropose ANTOR, a method for Adaptive Natural language generation for\nTask-Oriented dialogue via Reinforcement learning. In ANTOR, a natural language\nunderstanding (NLU) module, which corresponds to the user's understanding of\nsystem utterances, is incorporated into the objective function of RL. If the\nNLG's intentions are correctly conveyed to the NLU, which understands a\nsystem's utterances, the NLG is given a positive reward. We conducted\nexperiments on the MultiWOZ dataset, and we confirmed that ANTOR could generate\nadaptive utterances against speech recognition errors and the different\nvocabulary levels of users.", "published": "2022-09-16 12:08:57", "link": "http://arxiv.org/abs/2209.07873v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Multi-turn Machine Reading Comprehension Framework with Rethink\n  Mechanism for Emotion-Cause Pair Extraction", "abstract": "Emotion-cause pair extraction (ECPE) is an emerging task in emotion cause\nanalysis, which extracts potential emotion-cause pairs from an emotional\ndocument. Most recent studies use end-to-end methods to tackle the ECPE task.\nHowever, these methods either suffer from a label sparsity problem or fail to\nmodel complicated relations between emotions and causes. Furthermore, they all\ndo not consider explicit semantic information of clauses. To this end, we\ntransform the ECPE task into a document-level machine reading comprehension\n(MRC) task and propose a Multi-turn MRC framework with Rethink mechanism\n(MM-R). Our framework can model complicated relations between emotions and\ncauses while avoiding generating the pairing matrix (the leading cause of the\nlabel sparsity problem). Besides, the multi-turn structure can fuse explicit\nsemantic information flow between emotions and causes. Extensive experiments on\nthe benchmark emotion cause corpus demonstrate the effectiveness of our\nproposed framework, which outperforms existing state-of-the-art methods.", "published": "2022-09-16 14:38:58", "link": "http://arxiv.org/abs/2209.07972v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Belief Revision based Caption Re-ranker with Visual Semantic Information", "abstract": "In this work, we focus on improving the captions generated by image-caption\ngeneration systems. We propose a novel re-ranking approach that leverages\nvisual-semantic measures to identify the ideal caption that maximally captures\nthe visual information in the image. Our re-ranker utilizes the Belief Revision\nframework (Blok et al., 2003) to calibrate the original likelihood of the top-n\ncaptions by explicitly exploiting the semantic relatedness between the depicted\ncaption and the visual context. Our experiments demonstrate the utility of our\napproach, where we observe that our re-ranker can enhance the performance of a\ntypical image-captioning system without the necessity of any additional\ntraining or fine-tuning.", "published": "2022-09-16 20:36:41", "link": "http://arxiv.org/abs/2209.08163v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Changing the Representation: Examining Language Representation for\n  Neural Sign Language Production", "abstract": "Neural Sign Language Production (SLP) aims to automatically translate from\nspoken language sentences to sign language videos. Historically the SLP task\nhas been broken into two steps; Firstly, translating from a spoken language\nsentence to a gloss sequence and secondly, producing a sign language video\ngiven a sequence of glosses. In this paper we apply Natural Language Processing\ntechniques to the first step of the SLP pipeline. We use language models such\nas BERT and Word2Vec to create better sentence level embeddings, and apply\nseveral tokenization techniques, demonstrating how these improve performance on\nthe low resource translation task of Text to Gloss. We introduce Text to\nHamNoSys (T2H) translation, and show the advantages of using a phonetic\nrepresentation for sign language translation rather than a sign level gloss\nrepresentation. Furthermore, we use HamNoSys to extract the hand shape of a\nsign and use this as additional supervision during training, further increasing\nthe performance on T2H. Assembling best practise, we achieve a BLEU-4 score of\n26.99 on the MineDGS dataset and 25.09 on PHOENIX14T, two new state-of-the-art\nbaselines.", "published": "2022-09-16 12:45:29", "link": "http://arxiv.org/abs/2210.06312v1", "categories": ["cs.CL", "cs.AI", "68T50 (Primary)"], "primary_category": "cs.CL"}
{"title": "On the Relation between Sensitivity and Accuracy in In-context Learning", "abstract": "In-context learning (ICL) suffers from oversensitivity to the prompt, making\nit unreliable in real-world scenarios. We study the sensitivity of ICL with\nrespect to multiple perturbation types. First, we find that label bias obscures\nthe true sensitivity, and therefore prior work may have significantly\nunderestimated ICL sensitivity. Second, we observe a strong negative\ncorrelation between ICL sensitivity and accuracy: predictions sensitive to\nperturbations are less likely to be correct. Motivated by these findings, we\npropose \\textsc{SenSel}, a few-shot selective prediction method that abstains\nfrom sensitive predictions. Experiments on ten classification datasets show\nthat \\textsc{SenSel} consistently outperforms two commonly used\nconfidence-based and entropy-based baselines on abstention decisions.", "published": "2022-09-16 00:52:34", "link": "http://arxiv.org/abs/2209.07661v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango", "abstract": "The past decade has witnessed dramatic gains in natural language processing\nand an unprecedented scaling of large language models. These developments have\nbeen accelerated by the advent of few-shot techniques such as chain of thought\n(CoT) prompting. Specifically, CoT pushes the performance of large language\nmodels in a few-shot setup by augmenting the prompts with intermediate steps.\nDespite impressive results across various tasks, the reasons behind their\nsuccess have not been explored. This work uses counterfactual prompting to\ndevelop a deeper understanding of CoT-based few-shot prompting mechanisms in\nlarge language models. We first systematically identify and define the key\ncomponents of a prompt: symbols, patterns, and text. Then, we devise and\nconduct an exhaustive set of experiments across four different tasks, by\nquerying the model with counterfactual prompts where only one of these\ncomponents is altered. Our experiments across three models (PaLM, GPT-3, and\nCODEX) reveal several surprising findings and brings into question the\nconventional wisdom around few-shot prompting. First, the presence of factual\npatterns in a prompt is practically immaterial to the success of CoT. Second,\nour results conclude that the primary role of intermediate steps may not be to\nfacilitate learning how to solve a task. The intermediate steps are rather a\nbeacon for the model to realize what symbols to replicate in the output to form\na factual answer. Further, text imbues patterns with commonsense knowledge and\nmeaning. Our empirical and qualitative analysis reveals that a symbiotic\nrelationship between text and patterns explains the success of few-shot\nprompting: text helps extract commonsense from the question to help patterns,\nand patterns enforce task understanding and direct text generation.", "published": "2022-09-16 02:54:00", "link": "http://arxiv.org/abs/2209.07686v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PINEAPPLE: Personifying INanimate Entities by Acquiring Parallel\n  Personification data for Learning Enhanced generation", "abstract": "A personification is a figure of speech that endows inanimate entities with\nproperties and actions typically seen as requiring animacy. In this paper, we\nexplore the task of personification generation. To this end, we propose\nPINEAPPLE: Personifying INanimate Entities by Acquiring Parallel\nPersonification data for Learning Enhanced generation. We curate a corpus of\npersonifications called PersonifCorp, together with automatically generated\nde-personified literalizations of these personifications. We demonstrate the\nusefulness of this parallel corpus by training a seq2seq model to personify a\ngiven literal input. Both automatic and human evaluations show that fine-tuning\nwith PersonifCorp leads to significant gains in personification-related\nqualities such as animacy and interestingness. A detailed qualitative analysis\nalso highlights key strengths and imperfections of PINEAPPLE over baselines,\ndemonstrating a strong ability to generate diverse and creative\npersonifications that enhance the overall appeal of a sentence.", "published": "2022-09-16 07:16:05", "link": "http://arxiv.org/abs/2209.07752v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Jaco: An Offline Running Privacy-aware Voice Assistant", "abstract": "With the recent advance in speech technology, smart voice assistants have\nbeen improved and are now used by many people. But often these assistants are\nrunning online as a cloud service and are not always known for a good\nprotection of users' privacy. This paper presents the architecture of a novel\nvoice assistant, called Jaco, with the following features: (a) It can run\ncompletely offline, even on low resource devices like a RaspberryPi. (b)\nThrough a skill concept it can be easily extended. (c) The architectural focus\nis on protecting users' privacy, but without restricting capabilities for\ndevelopers. (d) It supports multiple languages. (e) It is competitive with\nother voice assistant solutions. In this respect the assistant combines and\nextends the advantages of other approaches.", "published": "2022-09-16 08:03:46", "link": "http://arxiv.org/abs/2209.07775v1", "categories": ["cs.CR", "cs.CL", "cs.HC"], "primary_category": "cs.CR"}
{"title": "Less is Better: Recovering Intended-Feature Subspace to Robustify NLU\n  Models", "abstract": "Datasets with significant proportions of bias present threats for training a\ntrustworthy model on NLU tasks. Despite yielding great progress, current\ndebiasing methods impose excessive reliance on the knowledge of bias\nattributes. Definition of the attributes, however, is elusive and varies across\ndifferent datasets. Furthermore, leveraging these attributes at input level to\nbias mitigation may leave a gap between intrinsic properties and the underlying\ndecision rule. To narrow down this gap and liberate the supervision on bias, we\nsuggest extending bias mitigation into feature space. Therefore, a novel model,\nRecovering Intended-Feature Subspace with Knowledge-Free (RISK) is developed.\nAssuming that shortcut features caused by various biases are unintended for\nprediction, RISK views them as redundant features. When delving into a lower\nmanifold to remove redundancies, RISK reveals that an extremely low-dimensional\nsubspace with intended features can robustly represent the highly biased\ndataset. Empirical results demonstrate our model can consistently improve model\ngeneralization to out-of-distribution set, and achieves a new state-of-the-art\nperformance.", "published": "2022-09-16 12:14:56", "link": "http://arxiv.org/abs/2209.07879v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Automatic Speech Recognition System for Bengali Language based on\n  Wav2Vec2 and Transfer Learning", "abstract": "An independent, automated method of decoding and transcribing oral speech is\nknown as automatic speech recognition (ASR). A typical ASR system extracts\nfeature from audio recordings or streams and run one or more algorithms to map\nthe features to corresponding texts. Numerous of research has been done in the\nfield of speech signal processing in recent years. When given adequate\nresources, both conventional ASR and emerging end-to-end (E2E) speech\nrecognition have produced promising results. However, for low-resource\nlanguages like Bengali, the current state of ASR lags behind, although the low\nresource state does not reflect upon the fact that this language is spoken by\nover 500 million people all over the world. Despite its popularity, there\naren't many diverse open-source datasets available, which makes it difficult to\nconduct research on Bengali speech recognition systems. This paper is a part of\nthe competition named `BUET CSE Fest DL Sprint'. The purpose of this paper is\nto improve the speech recognition performance of the Bengali language by\nadopting speech recognition technology on the E2E structure based on the\ntransfer learning framework. The proposed method effectively models the Bengali\nlanguage and achieves 3.819 score in `Levenshtein Mean Distance' on the test\ndataset of 7747 samples, when only 1000 samples of train dataset were used to\ntrain.", "published": "2022-09-16 18:20:16", "link": "http://arxiv.org/abs/2209.08119v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Evons: A Dataset for Fake and Real News Virality Analysis and Prediction", "abstract": "We present a novel collection of news articles originating from fake and real\nnews media sources for the analysis and prediction of news virality. Unlike\nexisting fake news datasets which either contain claims or news article\nheadline and body, in this collection each article is supported with a Facebook\nengagement count which we consider as an indicator of the article virality. In\naddition we also provide the article description and thumbnail image with which\nthe article was shared on Facebook. These images were automatically annotated\nwith object tags and color attributes. Using cloud based vision analysis tools,\nthumbnail images were also analyzed for faces and detected faces were annotated\nwith facial attributes. We empirically investigate the use of this collection\non an example task of article virality prediction.", "published": "2022-09-16 18:52:44", "link": "http://arxiv.org/abs/2209.08129v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Psychologically-informed chain-of-thought prompts for metaphor\n  understanding in large language models", "abstract": "Probabilistic models of language understanding are valuable tools for\ninvestigating human language use. However, they need to be hand-designed for a\nparticular domain. In contrast, large language models (LLMs) are trained on\ntext that spans a wide array of domains, but they lack the structure and\ninterpretability of probabilistic models. In this paper, we use\nchain-of-thought prompts to introduce structures from probabilistic models into\nLLMs. We explore this approach in the case of metaphor understanding. Our\nchain-of-thought prompts lead language models to infer latent variables and\nreason about their relationships in order to choose appropriate paraphrases for\nmetaphors. The latent variables and relationships chosen are informed by\ntheories of metaphor understanding from cognitive psychology. We apply these\nprompts to the two largest versions of GPT-3 and show that they can improve\nperformance in a paraphrase selection task.", "published": "2022-09-16 19:23:13", "link": "http://arxiv.org/abs/2209.08141v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots", "abstract": "We introduce ScreenQA, a novel benchmarking dataset designed to advance\nscreen content understanding through question answering. The existing screen\ndatasets are focused either on low-level structural and component\nunderstanding, or on a much higher-level composite task such as navigation and\ntask completion for autonomous agents. ScreenQA attempts to bridge this gap. By\nannotating 86k question-answer pairs over the RICO dataset, we aim to benchmark\nthe screen reading comprehension capacity, thereby laying the foundation for\nvision-based automation over screenshots. Our annotations encompass full\nanswers, short answer phrases, and corresponding UI contents with bounding\nboxes, enabling four subtasks to address various application scenarios. We\nevaluate the dataset's efficacy using both open-weight and proprietary models\nin zero-shot, fine-tuned, and transfer learning settings. We further\ndemonstrate positive transfer to web applications, highlighting its potential\nbeyond mobile applications.", "published": "2022-09-16 23:49:00", "link": "http://arxiv.org/abs/2209.08199v4", "categories": ["cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Hierarchical Interdisciplinary Topic Detection Model for Research\n  Proposal Classification", "abstract": "The peer merit review of research proposals has been the major mechanism for\ndeciding grant awards. However, research proposals have become increasingly\ninterdisciplinary. It has been a longstanding challenge to assign\ninterdisciplinary proposals to appropriate reviewers, so proposals are fairly\nevaluated. One of the critical steps in reviewer assignment is to generate\naccurate interdisciplinary topic labels for proposal-reviewer matching.\nExisting systems mainly collect topic labels manually generated by principal\ninvestigators. However, such human-reported labels can be non-accurate,\nincomplete, labor intensive, and time costly. What role can AI play in\ndeveloping a fair and precise proposal reviewer assignment system? In this\nstudy, we collaborate with the National Science Foundation of China to address\nthe task of automated interdisciplinary topic path detection. For this purpose,\nwe develop a deep Hierarchical Interdisciplinary Research Proposal\nClassification Network (HIRPCN). Specifically, we first propose a hierarchical\ntransformer to extract the textual semantic information of proposals. We then\ndesign an interdisciplinary graph and leverage GNNs for learning\nrepresentations of each discipline in order to extract interdisciplinary\nknowledge. After extracting the semantic and interdisciplinary knowledge, we\ndesign a level-wise prediction component to fuse the two types of knowledge\nrepresentations and detect interdisciplinary topic paths for each proposal. We\nconduct extensive experiments and expert evaluations on three real-world\ndatasets to demonstrate the effectiveness of our proposed model.", "published": "2022-09-16 16:59:25", "link": "http://arxiv.org/abs/2209.13519v3", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "musicaiz: A Python Library for Symbolic Music Generation, Analysis and\n  Visualization", "abstract": "In this article, we present musicaiz, an object-oriented library for\nanalyzing, generating and evaluating symbolic music. The submodules of the\npackage allow the user to create symbolic music data from scratch, build\nalgorithms to analyze symbolic music, encode MIDI data as tokens to train deep\nlearning sequence models, modify existing music data and evaluate music\ngeneration systems. The evaluation submodule builds on previous work to\nobjectively measure music generation systems and to be able to reproduce the\nresults of music generation models. The library is publicly available online.\nWe encourage the community to contribute and provide feedback.", "published": "2022-09-16 14:42:47", "link": "http://arxiv.org/abs/2209.07974v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
