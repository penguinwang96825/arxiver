{"title": "Submodularity-Inspired Data Selection for Goal-Oriented Chatbot Training\n  Based on Sentence Embeddings", "abstract": "Spoken language understanding (SLU) systems, such as goal-oriented chatbots\nor personal assistants, rely on an initial natural language understanding (NLU)\nmodule to determine the intent and to extract the relevant information from the\nuser queries they take as input. SLU systems usually help users to solve\nproblems in relatively narrow domains and require a large amount of in-domain\ntraining data. This leads to significant data availability issues that inhibit\nthe development of successful systems. To alleviate this problem, we propose a\ntechnique of data selection in the low-data regime that enables us to train\nwith fewer labeled sentences, thus smaller labelling costs.\n  We propose a submodularity-inspired data ranking function, the ratio-penalty\nmarginal gain, for selecting data points to label based only on the information\nextracted from the textual embedding space. We show that the distances in the\nembedding space are a viable source of information that can be used for data\nselection. Our method outperforms two known active learning techniques and\nenables cost-efficient training of the NLU unit. Moreover, our proposed\nselection technique does not need the model to be retrained in between the\nselection steps, making it time efficient as well.", "published": "2018-02-02 16:26:39", "link": "http://arxiv.org/abs/1802.00757v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Order matters: Distributional properties of speech to young children\n  bootstraps learning of semantic representations", "abstract": "Some researchers claim that language acquisition is critically dependent on\nexperiencing linguistic input in order of increasing complexity. We set out to\ntest this hypothesis using a simple recurrent neural network (SRN) trained to\npredict word sequences in CHILDES, a 5-million-word corpus of speech directed\nto children. First, we demonstrated that age-ordered CHILDES exhibits a gradual\nincrease in linguistic complexity. Next, we compared the performance of two\ngroups of SRNs trained on CHILDES which had either been age-ordered or not.\nSpecifically, we assessed learning of grammatical and semantic structure and\nshowed that training on age-ordered input facilitates learning of semantic, but\nnot of sequential structure. We found that this advantage is eliminated when\nthe models were trained on input with utterance boundary information removed.", "published": "2018-02-02 16:57:18", "link": "http://arxiv.org/abs/1802.00768v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Preserved Structure Across Vector Space Representations", "abstract": "Certain concepts, words, and images are intuitively more similar than others\n(dog vs. cat, dog vs. spoon), though quantifying such similarity is notoriously\ndifficult. Indeed, this kind of computation is likely a critical part of\nlearning the category boundaries for words within a given language. Here, we\nuse a set of 27 items (e.g. 'dog') that are highly common in infants' input,\nand use both image- and word-based algorithms to independently compute\nsimilarity among them. We find three key results. First, the pairwise item\nsimilarities derived within image-space and word-space are correlated,\nsuggesting preserved structure among these extremely different representational\nformats. Second, the closest 'neighbors' for each item, within each space,\nshowed significant overlap (e.g. both found 'egg' as a neighbor of 'apple').\nThird, items with the most overlapping neighbors are later-learned by infants\nand toddlers. We conclude that this approach, which does not rely on human\nratings of similarity, may nevertheless reflect stable within-class structure\nacross these two spaces. We speculate that such invariance might aid lexical\nacquisition, by serving as an informative marker of category boundaries.", "published": "2018-02-02 20:35:36", "link": "http://arxiv.org/abs/1802.00840v2", "categories": ["q-bio.NC", "cs.CL"], "primary_category": "q-bio.NC"}
{"title": "Quantitative Fine-Grained Human Evaluation of Machine Translation\n  Systems: a Case Study on English to Croatian", "abstract": "This paper presents a quantitative fine-grained manual evaluation approach to\ncomparing the performance of different machine translation (MT) systems. We\nbuild upon the well-established Multidimensional Quality Metrics (MQM) error\ntaxonomy and implement a novel method that assesses whether the differences in\nperformance for MQM error types between different MT systems are statistically\nsignificant. We conduct a case study for English-to-Croatian, a language\ndirection that involves translating into a morphologically rich language, for\nwhich we compare three MT systems belonging to different paradigms: pure\nphrase-based, factored phrase-based and neural. First, we design an\nMQM-compliant error taxonomy tailored to the relevant linguistic phenomena of\nSlavic languages, which made the annotation process feasible and accurate.\nErrors in MT outputs were then annotated by two annotators following this\ntaxonomy. Subsequently, we carried out a statistical analysis which showed that\nthe best-performing system (neural) reduces the errors produced by the worst\nsystem (pure phrase-based) by more than half (54\\%). Moreover, we conducted an\nadditional analysis of agreement errors in which we distinguished between short\n(phrase-level) and long distance (sentence-level) errors. We discovered that\nphrase-based MT approaches are of limited use for long distance agreement\nphenomena, for which neural MT was found to be especially effective.", "published": "2018-02-02 14:41:08", "link": "http://arxiv.org/abs/1802.01451v1", "categories": ["cs.CL", "cs.AI", "68T50"], "primary_category": "cs.CL"}
{"title": "Monaural Speech Enhancement using Deep Neural Networks by Maximizing a\n  Short-Time Objective Intelligibility Measure", "abstract": "In this paper we propose a Deep Neural Network (DNN) based Speech Enhancement\n(SE) system that is designed to maximize an approximation of the Short-Time\nObjective Intelligibility (STOI) measure. We formalize an approximate-STOI cost\nfunction and derive analytical expressions for the gradients required for DNN\ntraining and show that these gradients have desirable properties when used\ntogether with gradient based optimization techniques. We show through\nsimulation experiments that the proposed SE system achieves large improvements\nin estimated speech intelligibility, when tested on matched and unmatched\nnatural noise types, at multiple signal-to-noise ratios. Furthermore, we show\nthat the SE system, when trained using an approximate-STOI cost function\nperforms on par with a system trained with a mean square error cost applied to\nshort-time temporal envelopes. Finally, we show that the proposed SE system\nperforms on par with a traditional DNN based Short-Time Spectral Amplitude\n(STSA) SE system in terms of estimated speech intelligibility. These results\nare important because they suggest that traditional DNN based STSA SE systems\nmight be optimal in terms of estimated speech intelligibility.", "published": "2018-02-02 09:00:39", "link": "http://arxiv.org/abs/1802.00604v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Scalable Preprocessing of High Volume Bird Acoustic Data", "abstract": "In this work, we examine the problem of efficiently preprocessing high volume\nbird acoustic data. We combine several existing preprocessing steps including\nnoise reduction approaches into a single efficient pipeline by examining each\nprocess individually. We then utilise a distributed computing architecture to\nimprove execution time. Using a master-slave model with data parallelisation,\nwe developed a near-linear automated scalable system, capable of preprocessing\nbird acoustic recordings 21.76 times faster with 32 cores over 8 virtual\nmachines, compared to a serial process. This work contributes to the research\narea of bioacoustic analysis, which is currently very active because of its\npotential to monitor animals quickly at low cost. Overcoming noise interference\nis a significant challenge in many bioacoustic studies, and the volume of data\nin these studies is increasing. Our work makes large scale bird acoustic\nanalyses more feasible by parallelising important bird acoustic processing\ntasks to significantly reduce execution times.", "published": "2018-02-02 01:52:36", "link": "http://arxiv.org/abs/1802.00535v1", "categories": ["cs.DC", "cs.SD", "eess.AS"], "primary_category": "cs.DC"}
{"title": "A Generative Model for Natural Sounds Based on Latent Force Modelling", "abstract": "Recent advances in analysis of subband amplitude envelopes of natural sounds\nhave resulted in convincing synthesis, showing subband amplitudes to be a\ncrucial component of perception. Probabilistic latent variable analysis is\nparticularly revealing, but existing approaches don't incorporate prior\nknowledge about the physical behaviour of amplitude envelopes, such as\nexponential decay and feedback. We use latent force modelling, a probabilistic\nlearning paradigm that incorporates physical knowledge into Gaussian process\nregression, to model correlation across spectral subband envelopes. We augment\nthe standard latent force model approach by explicitly modelling correlations\nover multiple time steps. Incorporating this prior knowledge strengthens the\ninterpretation of the latent functions as the source that generated the signal.\nWe examine this interpretation via an experiment which shows that sounds\ngenerated by sampling from our probabilistic model are perceived to be more\nrealistic than those generated by similar models based on nonnegative matrix\nfactorisation, even in cases where our model is outperformed from a\nreconstruction error perspective.", "published": "2018-02-02 13:34:46", "link": "http://arxiv.org/abs/1802.00680v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
