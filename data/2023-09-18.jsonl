{"title": "Investigating Zero- and Few-shot Generalization in Fact Verification", "abstract": "In this paper, we explore zero- and few-shot generalization for fact\nverification (FV), which aims to generalize the FV model trained on\nwell-resourced domains (e.g., Wikipedia) to low-resourced domains that lack\nhuman annotations. To this end, we first construct a benchmark dataset\ncollection which contains 11 FV datasets representing 6 domains. We conduct an\nempirical analysis of generalization across these FV datasets, finding that\ncurrent models generalize poorly. Our analysis reveals that several factors\naffect generalization, including dataset size, length of evidence, and the type\nof claims. Finally, we show that two directions of work improve generalization:\n1) incorporating domain knowledge via pretraining on specialized domains, and\n2) automatically generating training data via claim generation.", "published": "2023-09-18 02:53:12", "link": "http://arxiv.org/abs/2309.09444v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Factorized Neural Transducer Model For text-only Domain\n  Adaptation", "abstract": "Adapting End-to-End ASR models to out-of-domain datasets with text data is\nchallenging. Factorized neural Transducer (FNT) aims to address this issue by\nintroducing a separate vocabulary decoder to predict the vocabulary.\nNonetheless, this approach has limitations in fusing acoustic and language\ninformation seamlessly. Moreover, a degradation in word error rate (WER) on the\ngeneral test sets was also observed, leading to doubts about its overall\nperformance. In response to this challenge, we present the improved factorized\nneural Transducer (IFNT) model structure designed to comprehensively integrate\nacoustic and language information while enabling effective text adaptation. We\nassess the performance of our proposed method on English and Mandarin datasets.\nThe results indicate that IFNT not only surpasses the neural Transducer and FNT\nin baseline performance in both scenarios but also exhibits superior adaptation\nability compared to FNT. On source domains, IFNT demonstrated statistically\nsignificant accuracy improvements, achieving a relative enhancement of 1.2% to\n2.8% in baseline accuracy compared to the neural Transducer. On out-of-domain\ndatasets, IFNT shows relative WER(CER) improvements of up to 30.2% over the\nstandard neural Transducer with shallow fusion, and relative WER(CER)\nreductions ranging from 1.1% to 2.8% on test sets compared to the FNT model.", "published": "2023-09-18 07:02:04", "link": "http://arxiv.org/abs/2309.09524v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Large Language Models to Domains via Reading Comprehension", "abstract": "We explore how continued pre-training on domain-specific corpora influences\nlarge language models, revealing that training on the raw corpora endows the\nmodel with domain knowledge, but drastically hurts its prompting ability for\nquestion answering. Taken inspiration from human learning via reading\ncomprehension--practice after reading improves the ability to answer questions\nbased on the learned knowledge--we propose a simple method for transforming raw\ncorpora into reading comprehension texts. Each raw text is enriched with a\nseries of tasks related to its content. Our method, highly scalable and\napplicable to any pre-training corpora, consistently enhances performance\nacross various tasks in three different domains: biomedicine, finance, and law.\nNotably, our 7B language model achieves competitive performance with\ndomain-specific models of much larger scales, such as BloombergGPT-50B.\nFurthermore, we demonstrate that domain-specific reading comprehension texts\ncan improve the model's performance even on general benchmarks, showing the\npotential to develop a general model across even more domains. Our model, code,\nand data are available at https://github.com/microsoft/LMOps.", "published": "2023-09-18 07:17:52", "link": "http://arxiv.org/abs/2309.09530v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Summarization is (Almost) Dead", "abstract": "How well can large language models (LLMs) generate summaries? We develop new\ndatasets and conduct human evaluation experiments to evaluate the zero-shot\ngeneration capability of LLMs across five distinct summarization tasks. Our\nfindings indicate a clear preference among human evaluators for LLM-generated\nsummaries over human-written summaries and summaries generated by fine-tuned\nmodels. Specifically, LLM-generated summaries exhibit better factual\nconsistency and fewer instances of extrinsic hallucinations. Due to the\nsatisfactory performance of LLMs in summarization tasks (even surpassing the\nbenchmark of reference summaries), we believe that most conventional works in\nthe field of text summarization are no longer necessary in the era of LLMs.\nHowever, we recognize that there are still some directions worth exploring,\nsuch as the creation of novel datasets with higher quality and more reliable\nevaluation methods.", "published": "2023-09-18 08:13:01", "link": "http://arxiv.org/abs/2309.09558v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Method of Fuzzy Topic Modeling based on Transformer Processing", "abstract": "Topic modeling is admittedly a convenient way to monitor markets trend.\nConventionally, Latent Dirichlet Allocation, LDA, is considered a must-do model\nto gain this type of information. By given the merit of deducing keyword with\ntoken conditional probability in LDA, we can know the most possible or\nessential topic. However, the results are not intuitive because the given\ntopics cannot wholly fit human knowledge. LDA offers the first possible\nrelevant keywords, which also brings out another problem of whether the\nconnection is reliable based on the statistic possibility. It is also hard to\ndecide the topic number manually in advance. As the booming trend of using\nfuzzy membership to cluster and using transformers to embed words, this work\npresents the fuzzy topic modeling based on soft clustering and document\nembedding from state-of-the-art transformer-based model. In our practical\napplication in a press release monitoring, the fuzzy topic modeling gives a\nmore natural result than the traditional output from LDA.", "published": "2023-09-18 10:52:54", "link": "http://arxiv.org/abs/2309.09658v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-turn Dialogue Comprehension from a Topic-aware Perspective", "abstract": "Dialogue related Machine Reading Comprehension requires language models to\neffectively decouple and model multi-turn dialogue passages. As a dialogue\ndevelopment goes after the intentions of participants, its topic may not keep\nconstant through the whole passage. Hence, it is non-trivial to detect and\nleverage the topic shift in dialogue modeling. Topic modeling, although has\nbeen widely studied in plain text, deserves far more utilization in dialogue\nreading comprehension. This paper proposes to model multi-turn dialogues from a\ntopic-aware perspective. We start with a dialogue segmentation algorithm to\nsplit a dialogue passage into topic-concentrated fragments in an unsupervised\nway. Then we use these fragments as topic-aware language processing units in\nfurther dialogue comprehension. On one hand, the split segments indict specific\ntopics rather than mixed intentions, thus showing convenient on in-domain topic\ndetection and location. For this task, we design a clustering system with a\nself-training auto-encoder, and we build two constructed datasets for\nevaluation. On the other hand, the split segments are an appropriate element of\nmulti-turn dialogue response selection. For this purpose, we further present a\nnovel model, Topic-Aware Dual-Attention Matching (TADAM) Network, which takes\ntopic segments as processing elements and matches response candidates with a\ndual cross-attention. Empirical studies on three public benchmarks show great\nimprovements over baselines. Our work continues the previous studies on\ndocument topic, and brings the dialogue modeling to a novel topic-aware\nperspective with exhaustive experiments and analyses.", "published": "2023-09-18 11:03:55", "link": "http://arxiv.org/abs/2309.09666v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Gender Bias of Pre-trained Language Models in Natural\n  Language Inference by Considering All Labels", "abstract": "Discriminatory gender biases have been found in Pre-trained Language Models\n(PLMs) for multiple languages. In Natural Language Inference (NLI), existing\nbias evaluation methods have focused on the prediction results of one specific\nlabel out of three labels, such as neutral. However, such evaluation methods\ncan be inaccurate since unique biased inferences are associated with unique\nprediction labels. Addressing this limitation, we propose a bias evaluation\nmethod for PLMs, called NLI-CoAL, which considers all the three labels of NLI\ntask. First, we create three evaluation data groups that represent different\ntypes of biases. Then, we define a bias measure based on the corresponding\nlabel output of each data group. In the experiments, we introduce a\nmeta-evaluation technique for NLI bias measures and use it to confirm that our\nbias measure can distinguish biased, incorrect inferences from non-biased\nincorrect inferences better than the baseline, resulting in a more accurate\nbias evaluation. We create the datasets in English, Japanese, and Chinese, and\nsuccessfully validate the compatibility of our bias measure across multiple\nlanguages. Lastly, we observe the bias tendencies in PLMs of different\nlanguages. To our knowledge, we are the first to construct evaluation datasets\nand measure PLMs' bias from NLI in Japanese and Chinese.", "published": "2023-09-18 12:02:21", "link": "http://arxiv.org/abs/2309.09697v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Facilitating NSFW Text Detection in Open-Domain Dialogue Systems via\n  Knowledge Distillation", "abstract": "NSFW (Not Safe for Work) content, in the context of a dialogue, can have\nsevere side effects on users in open-domain dialogue systems. However, research\non detecting NSFW language, especially sexually explicit content, within a\ndialogue context has significantly lagged behind. To address this issue, we\nintroduce CensorChat, a dialogue monitoring dataset aimed at NSFW dialogue\ndetection. Leveraging knowledge distillation techniques involving GPT-4 and\nChatGPT, this dataset offers a cost-effective means of constructing NSFW\ncontent detectors. The process entails collecting real-life human-machine\ninteraction data and breaking it down into single utterances and single-turn\ndialogues, with the chatbot delivering the final utterance. ChatGPT is employed\nto annotate unlabeled data, serving as a training set. Rationale validation and\ntest sets are constructed using ChatGPT and GPT-4 as annotators, with a\nself-criticism strategy for resolving discrepancies in labeling. A BERT model\nis fine-tuned as a text classifier on pseudo-labeled data, and its performance\nis assessed. The study emphasizes the importance of AI systems prioritizing\nuser safety and well-being in digital conversations while respecting freedom of\nexpression. The proposed approach not only advances NSFW content detection but\nalso aligns with evolving user protection needs in AI-driven dialogues.", "published": "2023-09-18 13:24:44", "link": "http://arxiv.org/abs/2309.09749v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The ParlaSent Multilingual Training Dataset for Sentiment Identification\n  in Parliamentary Proceedings", "abstract": "The paper presents a new training dataset of sentences in 7 languages,\nmanually annotated for sentiment, which are used in a series of experiments\nfocused on training a robust sentiment identifier for parliamentary\nproceedings. The paper additionally introduces the first domain-specific\nmultilingual transformer language model for political science applications,\nwhich was additionally pre-trained on 1.72 billion words from parliamentary\nproceedings of 27 European parliaments. We present experiments demonstrating\nhow the additional pre-training on parliamentary data can significantly improve\nthe model downstream performance, in our case, sentiment identification in\nparliamentary proceedings. We further show that our multilingual model performs\nvery well on languages not seen during fine-tuning, and that additional\nfine-tuning data from other languages significantly improves the target\nparliament's results. The paper makes an important contribution to multiple\ndisciplines inside the social sciences, and bridges them with computer science\nand computational linguistics. Lastly, the resulting fine-tuned language model\nsets up a more robust approach to sentiment analysis of political texts across\nlanguages, which allows scholars to study political sentiment from a\ncomparative perspective using standardized tools and techniques.", "published": "2023-09-18 14:01:06", "link": "http://arxiv.org/abs/2309.09783v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AMuRD: Annotated Arabic-English Receipt Dataset for Key Information\n  Extraction and Classification", "abstract": "The extraction of key information from receipts is a complex task that\ninvolves the recognition and extraction of text from scanned receipts. This\nprocess is crucial as it enables the retrieval of essential content and\norganizing it into structured documents for easy access and analysis. In this\npaper, we present AMuRD, a novel multilingual human-annotated dataset\nspecifically designed for information extraction from receipts. This dataset\ncomprises $47,720$ samples and addresses the key challenges in information\nextraction and item classification - the two critical aspects of data analysis\nin the retail industry. Each sample includes annotations for item names and\nattributes such as price, brand, and more. This detailed annotation facilitates\na comprehensive understanding of each item on the receipt. Furthermore, the\ndataset provides classification into $44$ distinct product categories. This\nclassification feature allows for a more organized and efficient analysis of\nthe items, enhancing the usability of the dataset for various applications. In\nour study, we evaluated various language model architectures, e.g., by\nfine-tuning LLaMA models on the AMuRD dataset. Our approach yielded exceptional\nresults, with an F1 score of 97.43\\% and accuracy of 94.99\\% in information\nextraction and classification, and an even higher F1 score of 98.51\\% and\naccuracy of 97.06\\% observed in specific tasks. The dataset and code are\npublicly accessible for further\nresearchhttps://github.com/Update-For-Integrated-Business-AI/AMuRD.", "published": "2023-09-18 14:18:19", "link": "http://arxiv.org/abs/2309.09800v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Not Enough Labeled Data? Just Add Semantics: A Data-Efficient Method for\n  Inferring Online Health Texts", "abstract": "User-generated texts available on the web and social platforms are often long\nand semantically challenging, making them difficult to annotate. Obtaining\nhuman annotation becomes increasingly difficult as problem domains become more\nspecialized. For example, many health NLP problems require domain experts to be\na part of the annotation pipeline. Thus, it is crucial that we develop\nlow-resource NLP solutions able to work with this set of limited-data problems.\nIn this study, we employ Abstract Meaning Representation (AMR) graphs as a\nmeans to model low-resource Health NLP tasks sourced from various online health\nresources and communities. AMRs are well suited to model online health texts as\nthey can represent multi-sentence inputs, abstract away from complex\nterminology, and model long-distance relationships between co-referring tokens.\nAMRs thus improve the ability of pre-trained language models to reason about\nhigh-complexity texts. Our experiments show that we can improve performance on\n6 low-resource health NLP tasks by augmenting text embeddings with semantic\ngraph embeddings. Our approach is task agnostic and easy to merge into any\nstandard text classification pipeline. We experimentally validate that AMRs are\nuseful in the modeling of complex texts by analyzing performance through the\nlens of two textual complexity measures: the Flesch Kincaid Reading Level and\nSyntactic Complexity. Our error analysis shows that AMR-infused language models\nperform better on complex texts and generally show less predictive variance in\nthe presence of changing complexity.", "published": "2023-09-18 15:37:30", "link": "http://arxiv.org/abs/2309.09877v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speaker attribution in German parliamentary debates with QLoRA-adapted\n  large language models", "abstract": "The growing body of political texts opens up new opportunities for rich\ninsights into political dynamics and ideologies but also increases the workload\nfor manual analysis. Automated speaker attribution, which detects who said what\nto whom in a speech event and is closely related to semantic role labeling, is\nan important processing step for computational text analysis. We study the\npotential of the large language model family Llama 2 to automate speaker\nattribution in German parliamentary debates from 2017-2021. We fine-tune Llama\n2 with QLoRA, an efficient training strategy, and observe our approach to\nachieve competitive performance in the GermEval 2023 Shared Task On Speaker\nAttribution in German News Articles and Parliamentary Debates. Our results shed\nlight on the capabilities of large language models in automating speaker\nattribution, revealing a promising avenue for computational analysis of\npolitical discourse and the development of semantic role labeling systems.", "published": "2023-09-18 16:06:16", "link": "http://arxiv.org/abs/2309.09902v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchy Builder: Organizing Textual Spans into a Hierarchy to\n  Facilitate Navigation", "abstract": "Information extraction systems often produce hundreds to thousands of strings\non a specific topic. We present a method that facilitates better consumption of\nthese strings, in an exploratory setting in which a user wants to both get a\nbroad overview of what's available, and a chance to dive deeper on some\naspects. The system works by grouping similar items together and arranging the\nremaining items into a hierarchical navigable DAG structure. We apply the\nmethod to medical information extraction.", "published": "2023-09-18 18:11:24", "link": "http://arxiv.org/abs/2309.10057v1", "categories": ["cs.CL", "H.3.1; H.3.3; H.5.3; I.2.7; E.1; I.2.4"], "primary_category": "cs.CL"}
{"title": "Few-Shot Adaptation for Parsing Contextual Utterances with LLMs", "abstract": "We evaluate the ability of semantic parsers based on large language models\n(LLMs) to handle contextual utterances. In real-world settings, there typically\nexists only a limited number of annotated contextual utterances due to\nannotation cost, resulting in an imbalance compared to non-contextual\nutterances. Therefore, parsers must adapt to contextual utterances with a few\ntraining examples. We examine four major paradigms for doing so in\nconversational semantic parsing i.e., Parse-with-Utterance-History,\nParse-with-Reference-Program, Parse-then-Resolve, and Rewrite-then-Parse. To\nfacilitate such cross-paradigm comparisons, we construct\nSMCalFlow-EventQueries, a subset of contextual examples from SMCalFlow with\nadditional annotations. Experiments with in-context learning and fine-tuning\nsuggest that Rewrite-then-Parse is the most promising paradigm when\nholistically considering parsing accuracy, annotation cost, and error types.", "published": "2023-09-18 21:35:19", "link": "http://arxiv.org/abs/2309.10168v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Search and Learning for Unsupervised Text Generation", "abstract": "With the advances of deep learning techniques, text generation is attracting\nincreasing interest in the artificial intelligence (AI) community, because of\nits wide applications and because it is an essential component of AI.\nTraditional text generation systems are trained in a supervised way, requiring\nmassive labeled parallel corpora. In this paper, I will introduce our recent\nwork on search and learning approaches to unsupervised text generation, where a\nheuristic objective function estimates the quality of a candidate sentence, and\ndiscrete search algorithms generate a sentence by maximizing the search\nobjective. A machine learning model further learns from the search results to\nsmooth out noise and improve efficiency. Our approach is important to the\nindustry for building minimal viable products for a new task; it also has high\nsocial impacts for saving human annotation labor and for processing\nlow-resource languages.", "published": "2023-09-18 05:44:11", "link": "http://arxiv.org/abs/2309.09497v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language\n  Models", "abstract": "Graphic layout generation, a growing research field, plays a significant role\nin user engagement and information perception. Existing methods primarily treat\nlayout generation as a numerical optimization task, focusing on quantitative\naspects while overlooking the semantic information of layout, such as the\nrelationship between each layout element. In this paper, we propose LayoutNUWA,\nthe first model that treats layout generation as a code generation task to\nenhance semantic information and harness the hidden layout expertise of large\nlanguage models~(LLMs). More concretely, we develop a Code Instruct Tuning\n(CIT) approach comprising three interconnected modules: 1) the Code\nInitialization (CI) module quantifies the numerical conditions and initializes\nthem as HTML code with strategically placed masks; 2) the Code Completion (CC)\nmodule employs the formatting knowledge of LLMs to fill in the masked portions\nwithin the HTML code; 3) the Code Rendering (CR) module transforms the\ncompleted code into the final layout output, ensuring a highly interpretable\nand transparent layout generation procedure that directly maps code to a\nvisualized layout. We attain significant state-of-the-art performance (even\nover 50\\% improvements) on multiple datasets, showcasing the strong\ncapabilities of LayoutNUWA. Our code is available at\nhttps://github.com/ProjectNUWA/LayoutNUWA.", "published": "2023-09-18 06:35:10", "link": "http://arxiv.org/abs/2309.09506v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Pruning Large Language Models via Accuracy Predictor", "abstract": "Large language models(LLMs) containing tens of billions of parameters (or\neven more) have demonstrated impressive capabilities in various NLP tasks.\nHowever, substantial model size poses challenges to training, inference, and\ndeployment so that it is necessary to compress the model. At present, most\nmodel compression for LLMs requires manual design of pruning features, which\nhas problems such as complex optimization pipeline and difficulty in retaining\nthe capabilities of certain parts of the model.Therefore, we propose a novel\npruning approach: firstly, a training set of a certain number of\narchitecture-accuracy pairs is established, and then a non-neural model is\ntrained as an accuracy predictor. Using the accuracy predictor to further\noptimize the search space and search, the optimal model can be automatically\nselected. Experiments show that our proposed approach is effective and\nefficient. Compared with the baseline, the perplexity(PPL) on Wikitext2 and PTB\ndropped by 9.48% and 5,76% respectively, and the average accuracy of MMLU\nincreased by 6.28%.", "published": "2023-09-18 06:38:24", "link": "http://arxiv.org/abs/2309.09507v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Understanding Divergent Framing of the Supreme Court Controversies:\n  Social Media vs. News Outlets", "abstract": "Understanding the framing of political issues is of paramount importance as\nit significantly shapes how individuals perceive, interpret, and engage with\nthese matters. While prior research has independently explored framing within\nnews media and by social media users, there remains a notable gap in our\ncomprehension of the disparities in framing political issues between these two\ndistinct groups. To address this gap, we conduct a comprehensive investigation,\nfocusing on the nuanced distinctions both qualitatively and quantitatively in\nthe framing of social media and traditional media outlets concerning a series\nof American Supreme Court rulings on affirmative action, student loans, and\nabortion rights. Our findings reveal that, while some overlap in framing exists\nbetween social media and traditional media outlets, substantial differences\nemerge both across various topics and within specific framing categories.\nCompared to traditional news media, social media platforms tend to present more\npolarized stances across all framing categories. Further, we observe\nsignificant polarization in the news media's treatment (i.e., Left vs. Right\nleaning media) of affirmative action and abortion rights, whereas the topic of\nstudent loans tends to exhibit a greater degree of consensus. The disparities\nin framing between traditional and social media platforms carry significant\nimplications for the formation of public opinion, policy decision-making, and\nthe broader political landscape.", "published": "2023-09-18 06:40:21", "link": "http://arxiv.org/abs/2309.09508v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "A Multitask Training Approach to Enhance Whisper with Contextual Biasing\n  and Open-Vocabulary Keyword Spotting", "abstract": "The recognition of rare named entities, such as personal names and\nterminologies, is challenging for automatic speech recognition (ASR) systems,\nespecially when they are not frequently observed in the training data. In this\npaper, we introduce keyword spotting enhanced Whisper (KWS-Whisper), a novel\nASR system that leverages the Whisper model and performs open-vocabulary\nkeyword spotting (OV-KWS) on the hidden states of the Whisper encoder to\nrecognize user-defined named entities. These entities serve as prompts for the\nWhisper decoder. To optimize the model, we propose a multitask training\napproach that learns OV-KWS and contextual-ASR tasks. We evaluate our approach\non Chinese Aishell hot word subsets and two internal code-switching test sets\nand show that it significantly improves the entity recall compared to the\noriginal Whisper model. Moreover, we demonstrate that the OV-KWS can be a\nplug-and-play module to enhance the ASR error correction methods and frozen\nWhisper models.", "published": "2023-09-18 08:03:54", "link": "http://arxiv.org/abs/2309.09552v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Fabricator: An Open Source Toolkit for Generating Labeled Training Data\n  with Teacher LLMs", "abstract": "Most NLP tasks are modeled as supervised learning and thus require labeled\ntraining data to train effective models. However, manually producing such data\nat sufficient quality and quantity is known to be costly and time-intensive.\nCurrent research addresses this bottleneck by exploring a novel paradigm called\nzero-shot learning via dataset generation. Here, a powerful LLM is prompted\nwith a task description to generate labeled data that can be used to train a\ndownstream NLP model. For instance, an LLM might be prompted to \"generate 500\nmovie reviews with positive overall sentiment, and another 500 with negative\nsentiment.\" The generated data could then be used to train a binary sentiment\nclassifier, effectively leveraging an LLM as a teacher to a smaller student\nmodel. With this demo, we introduce Fabricator, an open-source Python toolkit\nfor dataset generation. Fabricator implements common dataset generation\nworkflows, supports a wide range of downstream NLP tasks (such as text\nclassification, question answering, and entity recognition), and is integrated\nwith well-known libraries to facilitate quick experimentation. With Fabricator,\nwe aim to support researchers in conducting reproducible dataset generation\nexperiments using LLMs and help practitioners apply this approach to train\nmodels for downstream tasks.", "published": "2023-09-18 08:45:47", "link": "http://arxiv.org/abs/2309.09582v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM4Jobs: Unsupervised occupation extraction and standardization\n  leveraging Large Language Models", "abstract": "Automated occupation extraction and standardization from free-text job\npostings and resumes are crucial for applications like job recommendation and\nlabor market policy formation. This paper introduces LLM4Jobs, a novel\nunsupervised methodology that taps into the capabilities of large language\nmodels (LLMs) for occupation coding. LLM4Jobs uniquely harnesses both the\nnatural language understanding and generation capacities of LLMs. Evaluated on\nrigorous experimentation on synthetic and real-world datasets, we demonstrate\nthat LLM4Jobs consistently surpasses unsupervised state-of-the-art benchmarks,\ndemonstrating its versatility across diverse datasets and granularities. As a\nside result of our work, we present both synthetic and real-world datasets,\nwhich may be instrumental for subsequent research in this domain. Overall, this\ninvestigation highlights the promise of contemporary LLMs for the intricate\ntask of occupation extraction and standardization, laying the foundation for a\nrobust and adaptable framework relevant to both research and industrial\ncontexts.", "published": "2023-09-18 12:22:00", "link": "http://arxiv.org/abs/2309.09708v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dealing with negative samples with multi-task learning on span-based\n  joint entity-relation extraction", "abstract": "Recent span-based joint extraction models have demonstrated significant\nadvantages in both entity recognition and relation extraction. These models\ntreat text spans as candidate entities, and span pairs as candidate\nrelationship tuples, achieving state-of-the-art results on datasets like ADE.\nHowever, these models encounter a significant number of non-entity spans or\nirrelevant span pairs during the tasks, impairing model performance\nsignificantly. To address this issue, this paper introduces a span-based\nmultitask entity-relation joint extraction model. This approach employs the\nmultitask learning to alleviate the impact of negative samples on entity and\nrelation classifiers. Additionally, we leverage the Intersection over\nUnion(IoU) concept to introduce the positional information into the entity\nclassifier, achieving a span boundary detection. Furthermore, by incorporating\nthe entity Logits predicted by the entity classifier into the embedded\nrepresentation of entity pairs, the semantic input for the relation classifier\nis enriched. Experimental results demonstrate that our proposed SpERT.MT model\ncan effectively mitigate the adverse effects of excessive negative samples on\nthe model performance. Furthermore, the model demonstrated commendable F1\nscores of 73.61\\%, 53.72\\%, and 83.72\\% on three widely employed public\ndatasets, namely CoNLL04, SciERC, and ADE, respectively.", "published": "2023-09-18 12:28:46", "link": "http://arxiv.org/abs/2309.09713v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "When Large Language Models Meet Citation: A Survey", "abstract": "Citations in scholarly work serve the essential purpose of acknowledging and\ncrediting the original sources of knowledge that have been incorporated or\nreferenced. Depending on their surrounding textual context, these citations are\nused for different motivations and purposes. Large Language Models (LLMs) could\nbe helpful in capturing these fine-grained citation information via the\ncorresponding textual context, thereby enabling a better understanding towards\nthe literature. Furthermore, these citations also establish connections among\nscientific papers, providing high-quality inter-document relationships and\nhuman-constructed knowledge. Such information could be incorporated into LLMs\npre-training and improve the text representation in LLMs. Therefore, in this\npaper, we offer a preliminary review of the mutually beneficial relationship\nbetween LLMs and citation analysis. Specifically, we review the application of\nLLMs for in-text citation analysis tasks, including citation classification,\ncitation-based summarization, and citation recommendation. We then summarize\nthe research pertinent to leveraging citation linkage knowledge to improve text\nrepresentations of LLMs via citation prediction, network structure information,\nand inter-document relationship. We finally provide an overview of these\ncontemporary methods and put forth potential promising avenues in combining\nLLMs and citation analysis for further investigation.", "published": "2023-09-18 12:48:48", "link": "http://arxiv.org/abs/2309.09727v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Task Selection and Assignment for Multi-modal Multi-task Dialogue Act\n  Classification with Non-stationary Multi-armed Bandits", "abstract": "Multi-task learning (MTL) aims to improve the performance of a primary task\nby jointly learning with related auxiliary tasks. Traditional MTL methods\nselect tasks randomly during training. However, both previous studies and our\nresults suggest that such a random selection of tasks may not be helpful, and\ncan even be harmful to performance. Therefore, new strategies for task\nselection and assignment in MTL need to be explored. This paper studies the\nmulti-modal, multi-task dialogue act classification task, and proposes a method\nfor selecting and assigning tasks based on non-stationary multi-armed bandits\n(MAB) with discounted Thompson Sampling (TS) using Gaussian priors. Our\nexperimental results show that in different training stages, different tasks\nhave different utility. Our proposed method can effectively identify the task\nutility, actively avoid useless or harmful tasks, and realise the task\nassignment during training. Our proposed method is significantly superior in\nterms of UAR and F1 to the single-task and multi-task baselines with p-values <\n0.05. Further analysis of experiments indicates that for the dataset with the\ndata imbalance problem, our proposed method has significantly higher stability\nand can obtain consistent and decent performance for minority classes. Our\nproposed method is superior to the current state-of-the-art model.", "published": "2023-09-18 14:51:51", "link": "http://arxiv.org/abs/2309.09832v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How to Generate Popular Post Headlines on Social Media?", "abstract": "Posts, as important containers of user-generated-content pieces on social\nmedia, are of tremendous social influence and commercial value. As an integral\ncomponents of a post, the headline has a decisive contribution to the post's\npopularity. However, current mainstream method for headline generation is still\nmanually writing, which is unstable and requires extensive human effort. This\ndrives us to explore a novel research question: Can we automate the generation\nof popular headlines on social media? We collect more than 1 million posts of\n42,447 celebrities from public data of Xiaohongshu, which is a well-known\nsocial media platform in China. We then conduct careful observations on the\nheadlines of these posts. Observation results demonstrate that trends and\npersonal styles are widespread in headlines on social medias and have\nsignificant contribution to posts's popularity. Motivated by these insights, we\npresent MEBART, which combines Multiple preference-Extractors with\nBidirectional and Auto-Regressive Transformers (BART), capturing trends and\npersonal styles to generate popular headlines on social medias. We perform\nextensive experiments on real-world datasets and achieve state-of-the-art\nperformance compared with several advanced baselines. In addition, ablation and\ncase studies demonstrate that MEBART advances in capturing trends and personal\nstyles.", "published": "2023-09-18 17:12:58", "link": "http://arxiv.org/abs/2309.09949v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models", "abstract": "Visual instruction tuning has recently shown encouraging progress with\nopen-source large multimodal models (LMM) such as LLaVA and MiniGPT-4. However,\nmost existing studies of open-source LMM are performed using models with 13B\nparameters or smaller. In this paper we present an empirical study of scaling\nLLaVA up to 33B and 65B/70B, and share our findings from our explorations in\nimage resolution, data mixing and parameter-efficient training methods such as\nLoRA/QLoRA. These are evaluated by their impact on the multi-modal and language\ncapabilities when completing real-world tasks in the wild.\n  We find that scaling LMM consistently enhances model performance and improves\nlanguage capabilities, and performance of LoRA/QLoRA tuning of LMM are\ncomparable to the performance of full-model fine-tuning. Additionally, the\nstudy highlights the importance of higher image resolutions and mixing\nmultimodal-language data to improve LMM performance, and visual instruction\ntuning can sometimes improve LMM's pure language capability. We hope that this\nstudy makes state-of-the-art LMM research at a larger scale more accessible,\nthus helping establish stronger baselines for future research. Code and\ncheckpoints will be made public.", "published": "2023-09-18 17:30:46", "link": "http://arxiv.org/abs/2309.09958v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Multimodal Foundation Models: From Specialists to General-Purpose\n  Assistants", "abstract": "This paper presents a comprehensive survey of the taxonomy and evolution of\nmultimodal foundation models that demonstrate vision and vision-language\ncapabilities, focusing on the transition from specialist models to\ngeneral-purpose assistants. The research landscape encompasses five core\ntopics, categorized into two classes. (i) We start with a survey of\nwell-established research areas: multimodal foundation models pre-trained for\nspecific purposes, including two topics -- methods of learning vision backbones\nfor visual understanding and text-to-image generation. (ii) Then, we present\nrecent advances in exploratory, open research areas: multimodal foundation\nmodels that aim to play the role of general-purpose assistants, including three\ntopics -- unified vision models inspired by large language models (LLMs),\nend-to-end training of multimodal LLMs, and chaining multimodal tools with\nLLMs. The target audiences of the paper are researchers, graduate students, and\nprofessionals in computer vision and vision-language multimodal communities who\nare eager to learn the basics and recent advances in multimodal foundation\nmodels.", "published": "2023-09-18 17:56:28", "link": "http://arxiv.org/abs/2309.10020v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Understanding Catastrophic Forgetting in Language Models via Implicit\n  Inference", "abstract": "We lack a systematic understanding of the effects of fine-tuning (via methods\nsuch as instruction-tuning or reinforcement learning from human feedback),\nparticularly on tasks outside the narrow fine-tuning distribution. In a\nsimplified scenario, we demonstrate that improving performance on tasks within\nthe fine-tuning data distribution comes at the expense of capabilities on other\ntasks. We hypothesize that language models implicitly infer the task of the\nprompt and that fine-tuning skews this inference towards tasks in the\nfine-tuning distribution. To test this, we propose Conjugate Prompting, which\nartificially makes the task look farther from the fine-tuning distribution\nwhile requiring the same capability, and we find that this recovers some of the\npretraining capabilities in our synthetic setup. Since real-world fine-tuning\ndistributions are predominantly English, we apply conjugate prompting to\nrecover pretrained capabilities in LLMs by simply translating the prompts to\ndifferent languages. This allows us to recover in-context learning abilities\nlost via instruction tuning, natural reasoning capability lost during code\nfine-tuning, and, more concerningly, harmful content generation suppressed by\nsafety fine-tuning in chatbots like ChatGPT.", "published": "2023-09-18 19:28:48", "link": "http://arxiv.org/abs/2309.10105v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Positive and Risky Message Assessment for Music Products", "abstract": "In this work, we introduce a pioneering research challenge: evaluating\npositive and potentially harmful messages within music products. We initiate by\nsetting a multi-faceted, multi-task benchmark for music content assessment.\nSubsequently, we introduce an efficient multi-task predictive model fortified\nwith ordinality-enforcement to address this challenge. Our findings reveal that\nthe proposed method not only significantly outperforms robust task-specific\nalternatives but also possesses the capability to assess multiple aspects\nsimultaneously. Furthermore, through detailed case studies, where we employed\nLarge Language Models (LLMs) as surrogates for content assessment, we provide\nvaluable insights to inform and guide future research on this topic. The code\nfor dataset creation and model implementation is publicly available at\nhttps://github.com/RiTUAL-UH/music-message-assessment.", "published": "2023-09-18 22:20:13", "link": "http://arxiv.org/abs/2309.10182v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Stabilizing RLHF through Advantage Model and Selective Rehearsal", "abstract": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet aligning these models with human values and preferences using RLHF remains\na significant challenge. This challenge is characterized by various\ninstabilities, such as reward hacking and catastrophic forgetting. In this\ntechnical report, we propose two innovations to stabilize RLHF training: 1)\nAdvantage Model, which directly models advantage score i.e., extra reward\ncompared to the expected rewards and regulates score distributions across tasks\nto prevent reward hacking. 2) Selective Rehearsal, which mitigates catastrophic\nforgetting by strategically selecting data for PPO training and knowledge\nrehearsing. Our experimental analysis on public and proprietary datasets\nreveals that the proposed methods not only increase stability in RLHF training\nbut also achieve higher reward scores and win rates.", "published": "2023-09-18 23:06:32", "link": "http://arxiv.org/abs/2309.10202v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Machine Learning Technique Based Fake News Detection", "abstract": "False news has received attention from both the general public and the\nscholarly world. Such false information has the ability to affect public\nperception, giving nefarious groups the chance to influence the results of\npublic events like elections. Anyone can share fake news or facts about anyone\nor anything for their personal gain or to cause someone trouble. Also,\ninformation varies depending on the part of the world it is shared on. Thus, in\nthis paper, we have trained a model to classify fake and true news by utilizing\nthe 1876 news data from our collected dataset. We have preprocessed the data to\nget clean and filtered texts by following the Natural Language Processing\napproaches. Our research conducts 3 popular Machine Learning (Stochastic\ngradient descent, Na\\\"ive Bayes, Logistic Regression,) and 2 Deep Learning\n(Long-Short Term Memory, ASGD Weight-Dropped LSTM, or AWD-LSTM) algorithms.\nAfter we have found our best Naive Bayes classifier with 56% accuracy and an\nF1-macro score of an average of 32%.", "published": "2023-09-18 19:26:54", "link": "http://arxiv.org/abs/2309.13069v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Does Video Summarization Require Videos? Quantifying the Effectiveness\n  of Language in Video Summarization", "abstract": "Video summarization remains a huge challenge in computer vision due to the\nsize of the input videos to be summarized. We propose an efficient,\nlanguage-only video summarizer that achieves competitive accuracy with high\ndata efficiency. Using only textual captions obtained via a zero-shot approach,\nwe train a language transformer model and forego image representations. This\nmethod allows us to perform filtration amongst the representative text vectors\nand condense the sequence. With our approach, we gain explainability with\nnatural language that comes easily for human interpretation and textual\nsummaries of the videos. An ablation study that focuses on modality and data\ncompression shows that leveraging text modality only effectively reduces input\ndata processing while retaining comparable results.", "published": "2023-09-18 00:08:49", "link": "http://arxiv.org/abs/2309.09405v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Enhancing Multilingual Speech Recognition through Language Prompt Tuning\n  and Frame-Level Language Adapter", "abstract": "Multilingual intelligent assistants, such as ChatGPT, have recently gained\npopularity. To further expand the applications of multilingual artificial\nintelligence assistants and facilitate international communication, it is\nessential to enhance the performance of multilingual speech recognition, which\nis a crucial component of speech interaction. In this paper, we propose two\nsimple and parameter-efficient methods: language prompt tuning and frame-level\nlanguage adapter, to respectively enhance language-configurable and\nlanguage-agnostic multilingual speech recognition. Additionally, we explore the\nfeasibility of integrating these two approaches using parameter-efficient\nfine-tuning methods. Our experiments demonstrate significant performance\nimprovements across seven languages using our proposed methods.", "published": "2023-09-18 02:51:59", "link": "http://arxiv.org/abs/2309.09443v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Training dynamic models using early exits for automatic speech\n  recognition on resource-constrained devices", "abstract": "The ability to dynamically adjust the computational load of neural models\nduring inference is crucial for on-device processing scenarios characterised by\nlimited and time-varying computational resources. A promising solution is\npresented by early-exit architectures, in which additional exit branches are\nappended to intermediate layers of the encoder. In self-attention models for\nautomatic speech recognition (ASR), early-exit architectures enable the\ndevelopment of dynamic models capable of adapting their size and architecture\nto varying levels of computational resources and ASR performance demands.\nPrevious research on early-exiting ASR models has relied on pre-trained\nself-supervised models, fine-tuned with an early-exit loss. In this paper, we\nundertake an experimental comparison between fine-tuning pre-trained backbones\nand training models from scratch with the early-exiting objective. Experiments\nconducted on public datasets reveal that early-exit models trained from scratch\nnot only preserve performance when using fewer encoder layers but also exhibit\nenhanced task accuracy compared to single-exit or pre-trained models.\nFurthermore, we explore an exit selection strategy grounded in posterior\nprobabilities as an alternative to the conventional frame-based entropy\napproach. Results provide insights into the training dynamics of early-exit\narchitectures for ASR models, particularly the efficacy of training strategies\nand exit selection methods.", "published": "2023-09-18 07:45:16", "link": "http://arxiv.org/abs/2309.09546v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Proposition from the Perspective of Chinese Language: A Chinese\n  Proposition Classification Evaluation Benchmark", "abstract": "Existing propositions often rely on logical constants for classification.\nCompared with Western languages that lean towards hypotaxis such as English,\nChinese often relies on semantic or logical understanding rather than logical\nconnectives in daily expressions, exhibiting the characteristics of parataxis.\nHowever, existing research has rarely paid attention to this issue. And\naccurately classifying these propositions is crucial for natural language\nunderstanding and reasoning. In this paper, we put forward the concepts of\nexplicit and implicit propositions and propose a comprehensive multi-level\nproposition classification system based on linguistics and logic.\nCorrespondingly, we create a large-scale Chinese proposition dataset PEACE from\nmultiple domains, covering all categories related to propositions. To evaluate\nthe Chinese proposition classification ability of existing models and explore\ntheir limitations, We conduct evaluations on PEACE using several different\nmethods including the Rule-based method, SVM, BERT, RoBERTA, and ChatGPT.\nResults show the importance of properly modeling the semantic features of\npropositions. BERT has relatively good proposition classification capability,\nbut lacks cross-domain transferability. ChatGPT performs poorly, but its\nclassification ability can be improved by providing more proposition\ninformation. Many issues are still far from being resolved and require further\nstudy.", "published": "2023-09-18 09:18:39", "link": "http://arxiv.org/abs/2309.09602v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speeding Up Speech Synthesis In Diffusion Models By Reducing Data\n  Distribution Recovery Steps Via Content Transfer", "abstract": "Diffusion based vocoders have been criticised for being slow due to the many\nsteps required during sampling. Moreover, the model's loss function that is\npopularly implemented is designed such that the target is the original input\n$x_0$ or error $\\epsilon_0$. For early time steps of the reverse process, this\nresults in large prediction errors, which can lead to speech distortions and\nincrease the learning time. We propose a setup where the targets are the\ndifferent outputs of forward process time steps with a goal to reduce the\nmagnitude of prediction errors and reduce the training time. We use the\ndifferent layers of a neural network (NN) to perform denoising by training them\nto learn to generate representations similar to the noised outputs in the\nforward process of the diffusion. The NN layers learn to progressively denoise\nthe input in the reverse process until finally the final layer estimates the\nclean speech. To avoid 1:1 mapping between layers of the neural network and the\nforward process steps, we define a skip parameter $\\tau>1$ such that an NN\nlayer is trained to cumulatively remove the noise injected in the $\\tau$ steps\nin the forward process. This significantly reduces the number of data\ndistribution recovery steps and, consequently, the time to generate speech. We\nshow through extensive evaluation that the proposed technique generates\nhigh-fidelity speech in competitive time that outperforms current\nstate-of-the-art tools. The proposed technique is also able to generalize well\nto unseen speech.", "published": "2023-09-18 10:35:27", "link": "http://arxiv.org/abs/2309.09652v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Do learned speech symbols follow Zipf's law?", "abstract": "In this study, we investigate whether speech symbols, learned through deep\nlearning, follow Zipf's law, akin to natural language symbols. Zipf's law is an\nempirical law that delineates the frequency distribution of words, forming\nfundamentals for statistical analysis in natural language processing. Natural\nlanguage symbols, which are invented by humans to symbolize speech content, are\nrecognized to comply with this law. On the other hand, recent breakthroughs in\nspoken language processing have given rise to the development of learned speech\nsymbols; these are data-driven symbolizations of speech content. Our objective\nis to ascertain whether these data-driven speech symbols follow Zipf's law, as\nthe same as natural language symbols. Through our investigation, we aim to\nforge new ways for the statistical analysis of spoken language processing.", "published": "2023-09-18 11:56:10", "link": "http://arxiv.org/abs/2309.09690v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Watch the Speakers: A Hybrid Continuous Attribution Network for Emotion\n  Recognition in Conversation With Emotion Disentanglement", "abstract": "Emotion Recognition in Conversation (ERC) has attracted widespread attention\nin the natural language processing field due to its enormous potential for\npractical applications. Existing ERC methods face challenges in achieving\ngeneralization to diverse scenarios due to insufficient modeling of context,\nambiguous capture of dialogue relationships and overfitting in speaker\nmodeling. In this work, we present a Hybrid Continuous Attributive Network\n(HCAN) to address these issues in the perspective of emotional continuation and\nemotional attribution. Specifically, HCAN adopts a hybrid recurrent and\nattention-based module to model global emotion continuity. Then a novel\nEmotional Attribution Encoding (EAE) is proposed to model intra- and\ninter-emotional attribution for each utterance. Moreover, aiming to enhance the\nrobustness of the model in speaker modeling and improve its performance in\ndifferent scenarios, A comprehensive loss function emotional cognitive loss\n$\\mathcal{L}_{\\rm EC}$ is proposed to alleviate emotional drift and overcome\nthe overfitting of the model to speaker modeling. Our model achieves\nstate-of-the-art performance on three datasets, demonstrating the superiority\nof our work. Another extensive comparative experiments and ablation studies on\nthree benchmarks are conducted to provided evidence to support the efficacy of\neach module. Further exploration of generalization ability experiments shows\nthe plug-and-play nature of the EAE module in our method.", "published": "2023-09-18 14:18:16", "link": "http://arxiv.org/abs/2309.09799v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Efficient Avoidance of Vulnerabilities in Auto-completed Smart Contract\n  Code Using Vulnerability-constrained Decoding", "abstract": "Auto-completing code enables developers to speed up coding significantly.\nRecent advances in transformer-based large language model (LLM) technologies\nhave been applied to code synthesis. However, studies show that many of such\nsynthesized codes contain vulnerabilities. We propose a novel\nvulnerability-constrained decoding approach to reduce the amount of vulnerable\ncode generated by such models. Using a small dataset of labeled vulnerable\nlines of code, we fine-tune an LLM to include vulnerability labels when\ngenerating code, acting as an embedded classifier. Then, during decoding, we\ndeny the model to generate these labels to avoid generating vulnerable code. To\nevaluate the method, we chose to automatically complete Ethereum Blockchain\nsmart contracts (SCs) as the case study due to the strict requirements of SC\nsecurity. We first fine-tuned the 6-billion-parameter GPT-J model using 186,397\nEthereum SCs after removing the duplication from 2,217,692 SCs. The fine-tuning\ntook more than one week using ten GPUs. The results showed that our fine-tuned\nmodel could synthesize SCs with an average BLEU (BiLingual Evaluation\nUnderstudy) score of 0.557. However, many codes in the auto-completed SCs were\nvulnerable. Using the code before the vulnerable line of 176 SCs containing\ndifferent types of vulnerabilities to auto-complete the code, we found that\nmore than 70% of the auto-completed codes were insecure. Thus, we further\nfine-tuned the model on other 941 vulnerable SCs containing the same types of\nvulnerabilities and applied vulnerability-constrained decoding. The fine-tuning\ntook only one hour with four GPUs. We then auto-completed the 176 SCs again and\nfound that our approach could identify 62% of the code to be generated as\nvulnerable and avoid generating 67% of them, indicating the approach could\nefficiently and effectively avoid vulnerabilities in the auto-completed code.", "published": "2023-09-18 14:47:34", "link": "http://arxiv.org/abs/2309.09826v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "RECAP: Retrieval-Augmented Audio Captioning", "abstract": "We present RECAP (REtrieval-Augmented Audio CAPtioning), a novel and\neffective audio captioning system that generates captions conditioned on an\ninput audio and other captions similar to the audio retrieved from a datastore.\nAdditionally, our proposed method can transfer to any domain without the need\nfor any additional fine-tuning. To generate a caption for an audio sample, we\nleverage an audio-text model CLAP to retrieve captions similar to it from a\nreplaceable datastore, which are then used to construct a prompt. Next, we feed\nthis prompt to a GPT-2 decoder and introduce cross-attention layers between the\nCLAP encoder and GPT-2 to condition the audio for caption generation.\nExperiments on two benchmark datasets, Clotho and AudioCaps, show that RECAP\nachieves competitive performance in in-domain settings and significant\nimprovements in out-of-domain settings. Additionally, due to its capability to\nexploit a large text-captions-only datastore in a training-free fashion, RECAP\nshows unique capabilities of captioning novel audio events never seen during\ntraining and compositional audios with multiple events. To promote research in\nthis space, we also release 150,000+ new weakly labeled captions for AudioSet,\nAudioCaps, and Clotho.", "published": "2023-09-18 14:53:08", "link": "http://arxiv.org/abs/2309.09836v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "HypR: A comprehensive study for ASR hypothesis revising with a reference\n  corpus", "abstract": "With the development of deep learning, automatic speech recognition (ASR) has\nmade significant progress. To further enhance the performance of ASR, revising\nrecognition results is one of the lightweight but efficient manners. Various\nmethods can be roughly classified into N-best reranking modeling and error\ncorrection modeling. The former aims to select the hypothesis with the lowest\nerror rate from a set of candidates generated by ASR for a given input speech.\nThe latter focuses on detecting recognition errors in a given hypothesis and\ncorrecting these errors to obtain an enhanced result. However, we observe that\nthese studies are hardly comparable to each other, as they are usually\nevaluated on different corpora, paired with different ASR models, and even use\ndifferent datasets to train the models. Accordingly, we first concentrate on\nproviding an ASR hypothesis revising (HypR) dataset in this study. HypR\ncontains several commonly used corpora (AISHELL-1, TED-LIUM 2, and LibriSpeech)\nand provides 50 recognition hypotheses for each speech utterance. The\ncheckpoint models of ASR are also published. In addition, we implement and\ncompare several classic and representative methods, showing the recent research\nprogress in revising speech recognition results. We hope that the publicly\navailable HypR dataset can become a reference benchmark for subsequent research\nand promote this field of research to an advanced level.", "published": "2023-09-18 14:55:21", "link": "http://arxiv.org/abs/2309.09838v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Instruction-Following Speech Recognition", "abstract": "Conventional end-to-end Automatic Speech Recognition (ASR) models primarily\nfocus on exact transcription tasks, lacking flexibility for nuanced user\ninteractions. With the advent of Large Language Models (LLMs) in speech\nprocessing, more organic, text-prompt-based interactions have become possible.\nHowever, the mechanisms behind these models' speech understanding and\n\"reasoning\" capabilities remain underexplored. To study this question from the\ndata perspective, we introduce instruction-following speech recognition,\ntraining a Listen-Attend-Spell model to understand and execute a diverse set of\nfree-form text instructions. This enables a multitude of speech recognition\ntasks -- ranging from transcript manipulation to summarization -- without\nrelying on predefined command sets. Remarkably, our model, trained from scratch\non Librispeech, interprets and executes simple instructions without requiring\nLLMs or pre-trained speech modules. It also offers selective transcription\noptions based on instructions like \"transcribe first half and then turn off\nlistening,\" providing an additional layer of privacy and safety compared to\nexisting LLMs. Our findings highlight the significant potential of\ninstruction-following training to advance speech foundation models.", "published": "2023-09-18 14:59:10", "link": "http://arxiv.org/abs/2309.09843v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SYNDICOM: Improving Conversational Commonsense with Error-Injection and\n  Natural Language Feedback", "abstract": "Commonsense reasoning is a critical aspect of human communication. Despite\nrecent advances in conversational AI driven by large language models,\ncommonsense reasoning remains a challenging task. In this work, we introduce\nSYNDICOM - a method for improving commonsense in dialogue response generation.\nSYNDICOM consists of two components. The first component is a dataset composed\nof commonsense dialogues created from a knowledge graph and synthesized into\nnatural language. This dataset includes both valid and invalid responses to\ndialogue contexts, along with natural language feedback (NLF) for the invalid\nresponses. The second contribution is a two-step procedure: training a model to\npredict natural language feedback (NLF) for invalid responses, and then\ntraining a response generation model conditioned on the predicted NLF, the\ninvalid response, and the dialogue. SYNDICOM is scalable and does not require\nreinforcement learning. Empirical results on three tasks are evaluated using a\nbroad range of metrics. SYNDICOM achieves a relative improvement of 53% over\nChatGPT on ROUGE1, and human evaluators prefer SYNDICOM over ChatGPT 57% of the\ntime. We will publicly release the code and the full dataset.", "published": "2023-09-18 15:08:48", "link": "http://arxiv.org/abs/2309.10015v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Personalized Impression Generation for PET Reports Using Large\n  Language Models", "abstract": "In this study, we aimed to determine if fine-tuned large language models\n(LLMs) can generate accurate, personalized impressions for whole-body PET\nreports. Twelve language models were trained on a corpus of PET reports using\nthe teacher-forcing algorithm, with the report findings as input and the\nclinical impressions as reference. An extra input token encodes the reading\nphysician's identity, allowing models to learn physician-specific reporting\nstyles. Our corpus comprised 37,370 retrospective PET reports collected from\nour institution between 2010 and 2022. To identify the best LLM, 30 evaluation\nmetrics were benchmarked against quality scores from two nuclear medicine (NM)\nphysicians, with the most aligned metrics selecting the model for expert\nevaluation. In a subset of data, model-generated impressions and original\nclinical impressions were assessed by three NM physicians according to 6\nquality dimensions (3-point scale) and an overall utility score (5-point\nscale). Each physician reviewed 12 of their own reports and 12 reports from\nother physicians. Bootstrap resampling was used for statistical analysis. Of\nall evaluation metrics, domain-adapted BARTScore and PEGASUSScore showed the\nhighest Spearman's rank correlations (0.568 and 0.563) with physician\npreferences. Based on these metrics, the fine-tuned PEGASUS model was selected\nas the top LLM. When physicians reviewed PEGASUS-generated impressions in their\nown style, 89% were considered clinically acceptable, with a mean utility score\nof 4.08 out of 5. Physicians rated these personalized impressions as comparable\nin overall utility to the impressions dictated by other physicians (4.03,\nP=0.41). In conclusion, personalized impressions generated by PEGASUS were\nclinically useful, highlighting its potential to expedite PET reporting.", "published": "2023-09-18 18:33:40", "link": "http://arxiv.org/abs/2309.10066v2", "categories": ["cs.AI", "cs.CL", "physics.med-ph"], "primary_category": "cs.AI"}
{"title": "Unified Coarse-to-Fine Alignment for Video-Text Retrieval", "abstract": "The canonical approach to video-text retrieval leverages a coarse-grained or\nfine-grained alignment between visual and textual information. However,\nretrieving the correct video according to the text query is often challenging\nas it requires the ability to reason about both high-level (scene) and\nlow-level (object) visual clues and how they relate to the text query. To this\nend, we propose a Unified Coarse-to-fine Alignment model, dubbed UCoFiA.\nSpecifically, our model captures the cross-modal similarity information at\ndifferent granularity levels. To alleviate the effect of irrelevant visual\nclues, we also apply an Interactive Similarity Aggregation module (ISA) to\nconsider the importance of different visual features while aggregating the\ncross-modal similarity to obtain a similarity score for each granularity.\nFinally, we apply the Sinkhorn-Knopp algorithm to normalize the similarities of\neach level before summing them, alleviating over- and under-representation\nissues at different levels. By jointly considering the crossmodal similarity of\ndifferent granularity, UCoFiA allows the effective unification of multi-grained\nalignments. Empirically, UCoFiA outperforms previous state-of-the-art\nCLIP-based methods on multiple video-text retrieval benchmarks, achieving 2.4%,\n1.4% and 1.3% improvements in text-to-video retrieval R@1 on MSR-VTT,\nActivity-Net, and DiDeMo, respectively. Our code is publicly available at\nhttps://github.com/Ziyang412/UCoFiA.", "published": "2023-09-18 19:04:37", "link": "http://arxiv.org/abs/2309.10091v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Corpus Synthesis for Zero-shot ASR domain Adaptation using Large\n  Language Models", "abstract": "While Automatic Speech Recognition (ASR) systems are widely used in many\nreal-world applications, they often do not generalize well to new domains and\nneed to be finetuned on data from these domains. However, target-domain data\nusually are not readily available in many scenarios. In this paper, we propose\na new strategy for adapting ASR models to new target domains without any text\nor speech from those domains. To accomplish this, we propose a novel data\nsynthesis pipeline that uses a Large Language Model (LLM) to generate a target\ndomain text corpus, and a state-of-the-art controllable speech synthesis model\nto generate the corresponding speech. We propose a simple yet effective\nin-context instruction finetuning strategy to increase the effectiveness of LLM\nin generating text corpora for new domains. Experiments on the SLURP dataset\nshow that the proposed method achieves an average relative word error rate\nimprovement of $28\\%$ on unseen target domains without any performance drop in\nsource domains.", "published": "2023-09-18 15:43:08", "link": "http://arxiv.org/abs/2309.10707v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM", "abstract": "Recently, Large Language Models (LLMs) have made significant advancements and\nare now widely used across various domains. Unfortunately, there has been a\nrising concern that LLMs can be misused to generate harmful or malicious\ncontent. Though a line of research has focused on aligning LLMs with human\nvalues and preventing them from producing inappropriate content, such\nalignments are usually vulnerable and can be bypassed by alignment-breaking\nattacks via adversarially optimized or handcrafted jailbreaking prompts. In\nthis work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against\npotential alignment-breaking attacks. RA-LLM can be directly constructed upon\nan existing aligned LLM with a robust alignment checking function, without\nrequiring any expensive retraining or fine-tuning process of the original LLM.\nFurthermore, we also provide a theoretical analysis for RA-LLM to verify its\neffectiveness in defending against alignment-breaking attacks. Through\nreal-world experiments on open-source large language models, we demonstrate\nthat RA-LLM can successfully defend against both state-of-the-art adversarial\nprompts and popular handcrafted jailbreaking prompts by reducing their attack\nsuccess rates from nearly 100% to around 10% or less.", "published": "2023-09-18 02:07:22", "link": "http://arxiv.org/abs/2309.14348v3", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HTEC: Human Transcription Error Correction", "abstract": "High-quality human transcription is essential for training and improving\nAutomatic Speech Recognition (ASR) models. Recent study~\\cite{libricrowd} has\nfound that every 1% worse transcription Word Error Rate (WER) increases\napproximately 2% ASR WER by using the transcriptions to train ASR models.\nTranscription errors are inevitable for even highly-trained annotators.\nHowever, few studies have explored human transcription correction. Error\ncorrection methods for other problems, such as ASR error correction and\ngrammatical error correction, do not perform sufficiently for this problem.\nTherefore, we propose HTEC for Human Transcription Error Correction. HTEC\nconsists of two stages: Trans-Checker, an error detection model that predicts\nand masks erroneous words, and Trans-Filler, a sequence-to-sequence generative\nmodel that fills masked positions. We propose a holistic list of correction\noperations, including four novel operations handling deletion errors. We\nfurther propose a variant of embeddings that incorporates phoneme information\ninto the input of the transformer. HTEC outperforms other methods by a large\nmargin and surpasses human annotators by 2.2% to 4.5% in WER. Finally, we\ndeployed HTEC to assist human annotators and showed HTEC is particularly\neffective as a co-pilot, which improves transcription quality by 15.1% without\nsacrificing transcription velocity.", "published": "2023-09-18 19:03:21", "link": "http://arxiv.org/abs/2309.10089v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "cs.SD", "68T50", "I.2.7"], "primary_category": "eess.AS"}
{"title": "Are Soft Prompts Good Zero-shot Learners for Speech Recognition?", "abstract": "Large self-supervised pre-trained speech models require computationally\nexpensive fine-tuning for downstream tasks. Soft prompt tuning offers a simple\nparameter-efficient alternative by utilizing minimal soft prompt guidance,\nenhancing portability while also maintaining competitive performance. However,\nnot many people understand how and why this is so. In this study, we aim to\ndeepen our understanding of this emerging method by investigating the role of\nsoft prompts in automatic speech recognition (ASR). Our findings highlight\ntheir role as zero-shot learners in improving ASR performance but also make\nthem vulnerable to malicious modifications. Soft prompts aid generalization but\nare not obligatory for inference. We also identify two primary roles of soft\nprompts: content refinement and noise information enhancement, which enhances\nrobustness against background noise. Additionally, we propose an effective\nmodification on noise prompts to show that they are capable of zero-shot\nlearning on adapting to out-of-distribution noise environments.", "published": "2023-09-18 01:00:40", "link": "http://arxiv.org/abs/2309.09413v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "HumTrans: A Novel Open-Source Dataset for Humming Melody Transcription\n  and Beyond", "abstract": "This paper introduces the HumTrans dataset, which is publicly available and\nprimarily designed for humming melody transcription. The dataset can also serve\nas a foundation for downstream tasks such as humming melody based music\ngeneration. It consists of 500 musical compositions of different genres and\nlanguages, with each composition divided into multiple segments. In total, the\ndataset comprises 1000 music segments. To collect this humming dataset, we\nemployed 10 college students, all of whom are either music majors or proficient\nin playing at least one musical instrument. Each of them hummed every segment\ntwice using the web recording interface provided by our designed website. The\nhumming recordings were sampled at a frequency of 44,100 Hz. During the humming\nsession, the main interface provides a musical score for students to reference,\nwith the melody audio playing simultaneously to aid in capturing both melody\nand rhythm. The dataset encompasses approximately 56.22 hours of audio, making\nit the largest known humming dataset to date. The dataset will be released on\nHugging Face, and we will provide a GitHub repository containing baseline\nresults and evaluation codes.", "published": "2023-09-18 09:52:54", "link": "http://arxiv.org/abs/2309.09623v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Electrolaryngeal Speech Intelligibility Enhancement Through Robust\n  Linguistic Encoders", "abstract": "We propose a novel framework for electrolaryngeal speech intelligibility\nenhancement through the use of robust linguistic encoders. Pretraining and\nfine-tuning approaches have proven to work well in this task, but in most\ncases, various mismatches, such as the speech type mismatch (electrolaryngeal\nvs. typical) or a speaker mismatch between the datasets used in each stage, can\ndeteriorate the conversion performance of this framework. To resolve this\nissue, we propose a linguistic encoder robust enough to project both EL and\ntypical speech in the same latent space, while still being able to extract\naccurate linguistic information, creating a unified representation to reduce\nthe speech type mismatch. Furthermore, we introduce HuBERT output features to\nthe proposed framework for reducing the speaker mismatch, making it possible to\neffectively use a large-scale parallel dataset during pretraining. We show that\ncompared to the conventional framework using mel-spectrogram input and output\nfeatures, using the proposed framework enables the model to synthesize more\nintelligible and naturally sounding speech, as shown by a significant 16%\nimprovement in character error rate and 0.83 improvement in naturalness score.", "published": "2023-09-18 09:58:36", "link": "http://arxiv.org/abs/2309.09627v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Refining DNN-based Mask Estimation using CGMM-based EM Algorithm for\n  Multi-channel Noise Reduction", "abstract": "In this paper, we present a method that allows to further improve speech\nenhancement obtained with recently introduced Deep Neural Network (DNN) models.\nWe propose a multi-channel refinement method of time-frequency masks obtained\nwith single-channel DNNs, which consists of an iterative Complex Gaussian\nMixture Model (CGMM) based algorithm, followed by optimum spatial filtration.\nWe validate our approach on time-frequency masks estimated with three recent\ndeep learning models, namely DCUnet, DCCRN, and FullSubNet. We show that our\nmethod with the proposed mask refinement procedure allows to improve the\naccuracy of estimated masks, in terms of the Area Under the ROC Curve (AUC)\nmeasure, and as a consequence the overall speech quality of the enhanced speech\nsignal, as measured by PESQ improvement, and that the improvement is consistent\nacross all three DNN models.", "published": "2023-09-18 10:05:41", "link": "http://arxiv.org/abs/2309.09630v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Synth-AC: Enhancing Audio Captioning with Synthetic Supervision", "abstract": "Data-driven approaches hold promise for audio captioning. However, the\ndevelopment of audio captioning methods can be biased due to the limited\navailability and quality of text-audio data. This paper proposes a SynthAC\nframework, which leverages recent advances in audio generative models and\ncommonly available text corpus to create synthetic text-audio pairs, thereby\nenhancing text-audio representation. Specifically, the text-to-audio generation\nmodel, i.e., AudioLDM, is used to generate synthetic audio signals with\ncaptions from an image captioning dataset. Our SynthAC expands the availability\nof well-annotated captions from the text-vision domain to audio captioning,\nthus enhancing text-audio representation by learning relations within synthetic\ntext-audio pairs. Experiments demonstrate that our SynthAC framework can\nbenefit audio captioning models by incorporating well-annotated text corpus\nfrom the text-vision domain, offering a promising solution to the challenge\ncaused by data scarcity. Furthermore, SynthAC can be easily adapted to various\nstate-of-the-art methods, leading to substantial performance improvements.", "published": "2023-09-18 12:17:16", "link": "http://arxiv.org/abs/2309.09705v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Investigating End-to-End ASR Architectures for Long Form Audio\n  Transcription", "abstract": "This paper presents an overview and evaluation of some of the end-to-end ASR\nmodels on long-form audios. We study three categories of Automatic Speech\nRecognition(ASR) models based on their core architecture: (1) convolutional,\n(2) convolutional with squeeze-and-excitation and (3) convolutional models with\nattention. We selected one ASR model from each category and evaluated Word\nError Rate, maximum audio length and real-time factor for each model on a\nvariety of long audio benchmarks: Earnings-21 and 22, CORAAL, and TED-LIUM3.\nThe model from the category of self-attention with local attention and global\ntoken has the best accuracy comparing to other architectures. We also compared\nmodels with CTC and RNNT decoders and showed that CTC-based models are more\nrobust and efficient than RNNT on long form audio.", "published": "2023-09-18 17:13:50", "link": "http://arxiv.org/abs/2309.09950v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Harmony and Duality: An introduction to Music Theory", "abstract": "We develop aspects of music theory related to harmony, such as scales, chord\nformation and improvisation from a combinatorial perspective. The goal is to\nprovide a foundation for this subject by deriving the basic structure from a\nfew assumptions, rather than writing down long lists of chords/scales to\nmemorize without an underlying principle. Our approach involves introducing\nconstraints that limit the possible scales we can consider. For example, we may\nimpose the constraint that two voices cannot be only a semitone apart as this\nis too dissonant. We can then study scales that do not contain notes that are a\nsemitone apart. A more refined constraint avoids three voices colliding by\nstudying scales that do not have three notes separated only by semitones.\nAdditionally, we require that our scales are complete, which roughly means that\nthey are the maximal sets of tones that satisfy these constraints. As it turns\nout, completeness as applied to these simple two/three voice constraints\ncharacterizes the types of scales that are commonly used in music composition.\nSurprisingly, there is a correspondence between scales subject to the two-voice\nconstraint and those subject to the three-voice constraint. We formulate this\ncorrespondence as a duality statement that provides a way to understand scales\nsubject to one type of constraint in terms of scales subject to the other.\nFinally, we combine these constraint ideas to provide a classification of\nchords.", "published": "2023-09-18 16:11:48", "link": "http://arxiv.org/abs/2309.10719v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spiking-LEAF: A Learnable Auditory front-end for Spiking Neural Networks", "abstract": "Brain-inspired spiking neural networks (SNNs) have demonstrated great\npotential for temporal signal processing. However, their performance in speech\nprocessing remains limited due to the lack of an effective auditory front-end.\nTo address this limitation, we introduce Spiking-LEAF, a learnable auditory\nfront-end meticulously designed for SNN-based speech processing. Spiking-LEAF\ncombines a learnable filter bank with a novel two-compartment spiking neuron\nmodel called IHC-LIF. The IHC-LIF neurons draw inspiration from the structure\nof inner hair cells (IHC) and they leverage segregated dendritic and somatic\ncompartments to effectively capture multi-scale temporal dynamics of speech\nsignals. Additionally, the IHC-LIF neurons incorporate the lateral feedback\nmechanism along with spike regularization loss to enhance spike encoding\nefficiency. On keyword spotting and speaker identification tasks, the proposed\nSpiking-LEAF outperforms both SOTA spiking auditory front-ends and conventional\nreal-valued acoustic features in terms of classification accuracy, noise\nrobustness, and encoding efficiency.", "published": "2023-09-18 04:03:05", "link": "http://arxiv.org/abs/2309.09469v2", "categories": ["cs.SD", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Face-Driven Zero-Shot Voice Conversion with Memory-based Face-Voice\n  Alignment", "abstract": "This paper presents a novel task, zero-shot voice conversion based on face\nimages (zero-shot FaceVC), which aims at converting the voice characteristics\nof an utterance from any source speaker to a newly coming target speaker,\nsolely relying on a single face image of the target speaker. To address this\ntask, we propose a face-voice memory-based zero-shot FaceVC method. This method\nleverages a memory-based face-voice alignment module, in which slots act as the\nbridge to align these two modalities, allowing for the capture of voice\ncharacteristics from face images. A mixed supervision strategy is also\nintroduced to mitigate the long-standing issue of the inconsistency between\ntraining and inference phases for voice conversion tasks. To obtain\nspeaker-independent content-related representations, we transfer the knowledge\nfrom a pretrained zero-shot voice conversion model to our zero-shot FaceVC\nmodel. Considering the differences between FaceVC and traditional voice\nconversion tasks, systematic subjective and objective metrics are designed to\nthoroughly evaluate the homogeneity, diversity and consistency of voice\ncharacteristics controlled by face images. Through extensive experiments, we\ndemonstrate the superiority of our proposed method on the zero-shot FaceVC\ntask. Samples are presented on our demo website.", "published": "2023-09-18 04:08:02", "link": "http://arxiv.org/abs/2309.09470v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "HiFTNet: A Fast High-Quality Neural Vocoder with Harmonic-plus-Noise\n  Filter and Inverse Short Time Fourier Transform", "abstract": "Recent advancements in speech synthesis have leveraged GAN-based networks\nlike HiFi-GAN and BigVGAN to produce high-fidelity waveforms from\nmel-spectrograms. However, these networks are computationally expensive and\nparameter-heavy. iSTFTNet addresses these limitations by integrating inverse\nshort-time Fourier transform (iSTFT) into the network, achieving both speed and\nparameter efficiency. In this paper, we introduce an extension to iSTFTNet,\ntermed HiFTNet, which incorporates a harmonic-plus-noise source filter in the\ntime-frequency domain that uses a sinusoidal source from the fundamental\nfrequency (F0) inferred via a pre-trained F0 estimation network for fast\ninference speed. Subjective evaluations on LJSpeech show that our model\nsignificantly outperforms both iSTFTNet and HiFi-GAN, achieving\nground-truth-level performance. HiFTNet also outperforms BigVGAN-base on\nLibriTTS for unseen speakers and achieves comparable performance to BigVGAN\nwhile being four times faster with only $1/6$ of the parameters. Our work sets\na new benchmark for efficient, high-quality neural vocoding, paving the way for\nreal-time applications that demand high quality speech synthesis.", "published": "2023-09-18 05:30:15", "link": "http://arxiv.org/abs/2309.09493v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive\n  Instruction-Tuning Benchmark for Speech", "abstract": "Text language models have shown remarkable zero-shot capability in\ngeneralizing to unseen tasks when provided with well-formulated instructions.\nHowever, existing studies in speech processing primarily focus on limited or\nspecific tasks. Moreover, the lack of standardized benchmarks hinders a fair\ncomparison across different approaches. Thus, we present Dynamic-SUPERB, a\nbenchmark designed for building universal speech models capable of leveraging\ninstruction tuning to perform multiple tasks in a zero-shot fashion. To achieve\ncomprehensive coverage of diverse speech tasks and harness instruction tuning,\nwe invite the community to collaborate and contribute, facilitating the dynamic\ngrowth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation\ninstances by combining 33 tasks and 22 datasets. This spans a broad spectrum of\ndimensions, providing a comprehensive platform for evaluation. Additionally, we\npropose several approaches to establish benchmark baselines. These include the\nutilization of speech models, text language models, and the multimodal encoder.\nEvaluation results indicate that while these baselines perform reasonably on\nseen tasks, they struggle with unseen ones. We release all materials to the\npublic and welcome researchers to collaborate on the project, advancing\ntechnologies in the field together.", "published": "2023-09-18 06:43:30", "link": "http://arxiv.org/abs/2309.09510v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Non-Intrusive Speech Intelligibility Prediction for Hearing Aids using\n  Whisper and Metadata", "abstract": "Automated speech intelligibility assessment is pivotal for hearing aid (HA)\ndevelopment. In this paper, we present three novel methods to improve\nintelligibility prediction accuracy and introduce MBI-Net+, an enhanced version\nof MBI-Net, the top-performing system in the 1st Clarity Prediction Challenge.\nMBI-Net+ leverages Whisper's embeddings to create cross-domain acoustic\nfeatures and includes metadata from speech signals by using a classifier that\ndistinguishes different enhancement methods. Furthermore, MBI-Net+ integrates\nthe hearing-aid speech perception index (HASPI) as a supplementary metric into\nthe objective function to further boost prediction performance. Experimental\nresults demonstrate that MBI-Net+ surpasses several intrusive baseline systems\nand MBI-Net on the Clarity Prediction Challenge 2023 dataset, validating the\neffectiveness of incorporating Whisper embeddings, speech metadata, and related\ncomplementary metrics to improve prediction performance for HA.", "published": "2023-09-18 07:51:09", "link": "http://arxiv.org/abs/2309.09548v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Spoofing attack augmentation: can differently-trained attack models\n  improve generalisation?", "abstract": "A reliable deepfake detector or spoofing countermeasure (CM) should be robust\nin the face of unpredictable spoofing attacks. To encourage the learning of\nmore generaliseable artefacts, rather than those specific only to known\nattacks, CMs are usually exposed to a broad variety of different attacks during\ntraining. Even so, the performance of deep-learning-based CM solutions are\nknown to vary, sometimes substantially, when they are retrained with different\ninitialisations, hyper-parameters or training data partitions. We show in this\npaper that the potency of spoofing attacks, also deep-learning-based, can\nsimilarly vary according to training conditions, sometimes resulting in\nsubstantial degradations to detection performance. Nevertheless, while a\nRawNet2 CM model is vulnerable when only modest adjustments are made to the\nattack algorithm, those based upon graph attention networks and self-supervised\nlearning are reassuringly robust. The focus upon training data generated with\ndifferent attack algorithms might not be sufficient on its own to ensure\ngeneraliability; some form of spoofing attack augmentation at the algorithm\nlevel can be complementary.", "published": "2023-09-18 08:47:54", "link": "http://arxiv.org/abs/2309.09586v2", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Scaling the time and Fourier domains to align periodically and their\n  convolution", "abstract": "This note shows how to align a periodic signal with its the Fourier transform\nby means of frequency or time scaling. This may be useful in developing new\nalgorithms, e.g. for pitch estimation. This note also convolves the signals and\nthe frequency time convolution is denoted fxt.", "published": "2023-09-18 10:30:16", "link": "http://arxiv.org/abs/2309.09645v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Single and Few-step Diffusion for Generative Speech Enhancement", "abstract": "Diffusion models have shown promising results in speech enhancement, using a\ntask-adapted diffusion process for the conditional generation of clean speech\ngiven a noisy mixture. However, at test time, the neural network used for score\nestimation is called multiple times to solve the iterative reverse process.\nThis results in a slow inference process and causes discretization errors that\naccumulate over the sampling trajectory. In this paper, we address these\nlimitations through a two-stage training approach. In the first stage, we train\nthe diffusion model the usual way using the generative denoising score matching\nloss. In the second stage, we compute the enhanced signal by solving the\nreverse process and compare the resulting estimate to the clean speech target\nusing a predictive loss. We show that using this second training stage enables\nachieving the same performance as the baseline model using only 5 function\nevaluations instead of 60 function evaluations. While the performance of usual\ngenerative diffusion algorithms drops dramatically when lowering the number of\nfunction evaluations (NFEs) to obtain single-step diffusion, we show that our\nproposed method keeps a steady performance and therefore largely outperforms\nthe diffusion baseline in this setting and also generalizes better than its\npredictive counterpart.", "published": "2023-09-18 11:30:58", "link": "http://arxiv.org/abs/2309.09677v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Frame-to-Utterance Convergence: A Spectra-Temporal Approach for Unified\n  Spoofing Detection", "abstract": "Voice spoofing attacks pose a significant threat to automated speaker\nverification systems. Existing anti-spoofing methods often simulate specific\nattack types, such as synthetic or replay attacks. However, in real-world\nscenarios, the countermeasures are unaware of the generation schema of the\nattack, necessitating a unified solution. Current unified solutions struggle to\ndetect spoofing artifacts, especially with recent spoofing mechanisms. For\ninstance, the spoofing algorithms inject spectral or temporal anomalies, which\nare challenging to identify. To this end, we present a spectra-temporal fusion\nleveraging frame-level and utterance-level coefficients. We introduce a novel\nlocal spectral deviation coefficient (SDC) for frame-level inconsistencies and\nemploy a bi-LSTM-based network for sequential temporal coefficients (STC),\nwhich capture utterance-level artifacts. Our spectra-temporal fusion strategy\ncombines these coefficients, and an auto-encoder generates spectra-temporal\ndeviated coefficients (STDC) to enhance robustness. Our proposed approach\naddresses multiple spoofing categories, including synthetic, replay, and\npartial deepfake attacks. Extensive evaluation on diverse datasets\n(ASVspoof2019, ASVspoof2021, VSDC, partial spoofs, and in-the-wild deepfakes)\ndemonstrated its robustness for a wide range of voice applications.", "published": "2023-09-18 14:54:42", "link": "http://arxiv.org/abs/2309.09837v1", "categories": ["cs.SD", "cs.CY", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Distilling HuBERT with LSTMs via Decoupled Knowledge Distillation", "abstract": "Much research effort is being applied to the task of compressing the\nknowledge of self-supervised models, which are powerful, yet large and memory\nconsuming. In this work, we show that the original method of knowledge\ndistillation (and its more recently proposed extension, decoupled knowledge\ndistillation) can be applied to the task of distilling HuBERT. In contrast to\nmethods that focus on distilling internal features, this allows for more\nfreedom in the network architecture of the compressed model. We thus propose to\ndistill HuBERT's Transformer layers into an LSTM-based distilled model that\nreduces the number of parameters even below DistilHuBERT and at the same time\nshows improved performance in automatic speech recognition.", "published": "2023-09-18 16:34:40", "link": "http://arxiv.org/abs/2309.09920v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
