{"title": "Knowledge-augmented Graph Neural Networks with Concept-aware Attention\n  for Adverse Drug Event Detection", "abstract": "Adverse drug events (ADEs) are an important aspect of drug safety. Various\ntexts such as biomedical literature, drug reviews, and user posts on social\nmedia and medical forums contain a wealth of information about ADEs. Recent\nstudies have applied word embedding and deep learning -based natural language\nprocessing to automate ADE detection from text. However, they did not explore\nincorporating explicit medical knowledge about drugs and adverse reactions or\nthe corresponding feature learning. This paper adopts the heterogenous text\ngraph which describes relationships between documents, words and concepts,\naugments it with medical knowledge from the Unified Medical Language System,\nand proposes a concept-aware attention mechanism which learns features\ndifferently for the different types of nodes in the graph. We further utilize\ncontextualized embeddings from pretrained language models and convolutional\ngraph neural networks for effective feature representation and relational\nlearning. Experiments on four public datasets show that our model achieves\nperformance competitive to the recent advances and the concept-aware attention\nconsistently outperforms other attention mechanisms.", "published": "2023-01-25 08:01:45", "link": "http://arxiv.org/abs/2301.10451v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SWING: Balancing Coverage and Faithfulness for Dialogue Summarization", "abstract": "Missing information is a common issue of dialogue summarization where some\ninformation in the reference summaries is not covered in the generated\nsummaries. To address this issue, we propose to utilize natural language\ninference (NLI) models to improve coverage while avoiding introducing factual\ninconsistencies. Specifically, we use NLI to compute fine-grained training\nsignals to encourage the model to generate content in the reference summaries\nthat have not been covered, as well as to distinguish between factually\nconsistent and inconsistent generated sentences. Experiments on the DialogSum\nand SAMSum datasets confirm the effectiveness of the proposed approach in\nbalancing coverage and faithfulness, validated with automatic metrics and human\nevaluations. Additionally, we compute the correlation between commonly used\nautomatic metrics with human judgments in terms of three different dimensions\nregarding coverage and factual consistency to provide insight into the most\nsuitable metric for evaluating dialogue summaries.", "published": "2023-01-25 09:33:11", "link": "http://arxiv.org/abs/2301.10483v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Tenant Optimization For Few-Shot Task-Oriented FAQ Retrieval", "abstract": "Business-specific Frequently Asked Questions (FAQ) retrieval in task-oriented\ndialog systems poses unique challenges vis-\\`a-vis community based FAQs. Each\nFAQ question represents an intent which is usually an umbrella term for many\nrelated user queries. We evaluate performance for such Business FAQs both with\nstandard FAQ retrieval techniques using query-Question (q-Q) similarity and\nfew-shot intent detection techniques. Implementing a real world solution for\nFAQ retrieval in order to support multiple tenants (FAQ sets) entails\noptimizing speed, accuracy and cost. We propose a novel approach to scale\nmulti-tenant FAQ applications in real-world context by contrastive fine-tuning\nof the last layer in sentence Bi-Encoders along with tenant-specific weight\nswitching.", "published": "2023-01-25 10:55:45", "link": "http://arxiv.org/abs/2301.10517v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Argument Mining in the Medical Domain", "abstract": "Nowadays the medical domain is receiving more and more attention in\napplications involving Artificial Intelligence as clinicians decision-making is\nincreasingly dependent on dealing with enormous amounts of unstructured textual\ndata. In this context, Argument Mining (AM) helps to meaningfully structure\ntextual data by identifying the argumentative components in the text and\nclassifying the relations between them. However, as it is the case for man\ntasks in Natural Language Processing in general and in medical text processing\nin particular, the large majority of the work on computational argumentation\nhas been focusing only on the English language. In this paper, we investigate\nseveral strategies to perform AM in medical texts for a language such as\nSpanish, for which no annotated data is available. Our work shows that\nautomatically translating and projecting annotations (data-transfer) from\nEnglish to a given target language is an effective way to generate annotated\ndata without costly manual intervention. Furthermore, and contrary to\nconclusions from previous work for other sequence labelling tasks, our\nexperiments demonstrate that data-transfer outperforms methods based on the\ncrosslingual transfer capabilities of multilingual pre-trained language models\n(model-transfer). Finally, we show how the automatically generated data in\nSpanish can also be used to improve results in the original English monolingual\nsetting, providing thus a fully automatic data augmentation strategy.", "published": "2023-01-25 11:21:12", "link": "http://arxiv.org/abs/2301.10527v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Consistency is Key: Disentangling Label Variation in Natural Language\n  Processing with Intra-Annotator Agreement", "abstract": "We commonly use agreement measures to assess the utility of judgements made\nby human annotators in Natural Language Processing (NLP) tasks. While\ninter-annotator agreement is frequently used as an indication of label\nreliability by measuring consistency between annotators, we argue for the\nadditional use of intra-annotator agreement to measure label stability over\ntime. However, in a systematic review, we find that the latter is rarely\nreported in this field. Calculating these measures can act as important quality\ncontrol and provide insights into why annotators disagree. We propose\nexploratory annotation experiments to investigate the relationships between\nthese measures and perceptions of subjectivity and ambiguity in text items.", "published": "2023-01-25 16:38:11", "link": "http://arxiv.org/abs/2301.10684v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a Unified Model for Generating Answers and Explanations in\n  Visual Question Answering", "abstract": "The field of visual question answering (VQA) has recently seen a surge in\nresearch focused on providing explanations for predicted answers. However,\ncurrent systems mostly rely on separate models to predict answers and generate\nexplanations, leading to less grounded and frequently inconsistent results. To\naddress this, we propose a multitask learning approach towards a Unified Model\nfor Answer and Explanation generation (UMAE). Our approach involves the\naddition of artificial prompt tokens to training data and fine-tuning a\nmultimodal encoder-decoder model on a variety of VQA-related tasks. In our\nexperiments, UMAE models surpass the prior state-of-the-art answer accuracy on\nA-OKVQA by 10~15%, show competitive results on OK-VQA, achieve new\nstate-of-the-art explanation scores on A-OKVQA and VCR, and demonstrate\npromising out-of-domain performance on VQA-X.", "published": "2023-01-25 19:29:19", "link": "http://arxiv.org/abs/2301.10799v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using novel data and ensemble models to improve automated labeling of\n  Sustainable Development Goals", "abstract": "A number of labeling systems based on text have been proposed to help monitor\nwork on the United Nations (UN) Sustainable Development Goals (SDGs). Here, we\npresent a systematic comparison of systems using a variety of text sources and\nshow that systems differ considerably in their specificity (i.e., true-positive\nrate) and sensitivity (i.e., true-negative rate), have systematic biases (e.g.,\nare more sensitive to specific SDGs relative to others), and are susceptible to\nthe type and amount of text analyzed. We then show that an ensemble model that\npools labeling systems alleviates some of these limitations, exceeding the\nlabeling performance of all currently available systems. We conclude that\nresearchers and policymakers should care about the choice of labeling system\nand that ensemble methods should be favored when drawing conclusions about the\nabsolute and relative prevalence of work on the SDGs based on automated\nmethods.", "published": "2023-01-25 07:44:46", "link": "http://arxiv.org/abs/2301.11353v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Model Detoxification in Dialogue with Contextualized Stance\n  Control", "abstract": "To reduce the toxic degeneration in a pretrained Language Model (LM),\nprevious work on Language Model detoxification has focused on reducing the\ntoxicity of the generation itself (self-toxicity) without consideration of the\ncontext. As a result, a type of implicit offensive language where the\ngenerations support the offensive language in the context is ignored. Different\nfrom the LM controlling tasks in previous work, where the desired attributes\nare fixed for generation, the desired stance of the generation depends on the\noffensiveness of the context. Therefore, we propose a novel control method to\ndo context-dependent detoxification with the stance taken into consideration.\nWe introduce meta prefixes to learn the contextualized stance control strategy\nand to generate the stance control prefix according to the input context. The\ngenerated stance prefix is then combined with the toxicity control prefix to\nguide the response generation. Experimental results show that our proposed\nmethod can effectively learn the context-dependent stance control strategies\nwhile keeping a low self-toxicity of the underlying LM.", "published": "2023-01-25 00:47:28", "link": "http://arxiv.org/abs/2301.10368v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "XNLI: Explaining and Diagnosing NLI-based Visual Data Analysis", "abstract": "Natural language interfaces (NLIs) enable users to flexibly specify\nanalytical intentions in data visualization. However, diagnosing the\nvisualization results without understanding the underlying generation process\nis challenging. Our research explores how to provide explanations for NLIs to\nhelp users locate the problems and further revise the queries. We present XNLI,\nan explainable NLI system for visual data analysis. The system introduces a\nProvenance Generator to reveal the detailed process of visual transformations,\na suite of interactive widgets to support error adjustments, and a Hint\nGenerator to provide query revision hints based on the analysis of user queries\nand interactions. Two usage scenarios of XNLI and a user study verify the\neffectiveness and usability of the system. Results suggest that XNLI can\nsignificantly enhance task accuracy without interrupting the NLI-based analysis\nprocess.", "published": "2023-01-25 02:24:16", "link": "http://arxiv.org/abs/2301.10385v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "ViDeBERTa: A powerful pre-trained language model for Vietnamese", "abstract": "This paper presents ViDeBERTa, a new pre-trained monolingual language model\nfor Vietnamese, with three versions - ViDeBERTa_xsmall, ViDeBERTa_base, and\nViDeBERTa_large, which are pre-trained on a large-scale corpus of high-quality\nand diverse Vietnamese texts using DeBERTa architecture. Although many\nsuccessful pre-trained language models based on Transformer have been widely\nproposed for the English language, there are still few pre-trained models for\nVietnamese, a low-resource language, that perform good results on downstream\ntasks, especially Question answering. We fine-tune and evaluate our model on\nthree important natural language downstream tasks, Part-of-speech tagging,\nNamed-entity recognition, and Question answering. The empirical results\ndemonstrate that ViDeBERTa with far fewer parameters surpasses the previous\nstate-of-the-art models on multiple Vietnamese-specific natural language\nunderstanding tasks. Notably, ViDeBERTa_base with 86M parameters, which is only\nabout 23% of PhoBERT_large with 370M parameters, still performs the same or\nbetter results than the previous state-of-the-art model. Our ViDeBERTa models\nare available at: https://github.com/HySonLab/ViDeBERTa.", "published": "2023-01-25 07:26:54", "link": "http://arxiv.org/abs/2301.10439v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Experimental Study on Pretraining Transformers from Scratch for IR", "abstract": "Finetuning Pretrained Language Models (PLM) for IR has been de facto the\nstandard practice since their breakthrough effectiveness few years ago. But, is\nthis approach well understood? In this paper, we study the impact of the\npretraining collection on the final IR effectiveness. In particular, we\nchallenge the current hypothesis that PLM shall be trained on a large enough\ngeneric collection and we show that pretraining from scratch on the collection\nof interest is surprisingly competitive with the current approach. We benchmark\nfirst-stage ranking rankers and cross-encoders for reranking on the task of\ngeneral passage retrieval on MSMARCO, Mr-Tydi for Arabic, Japanese and Russian,\nand TripClick for specific domain. Contrary to popular belief, we show that,\nfor finetuning first-stage rankers, models pretrained solely on their\ncollection have equivalent or better effectiveness compared to more general\nmodels. However, there is a slight effectiveness drop for rerankers pretrained\nonly on the target collection. Overall, our study sheds a new light on the role\nof the pretraining collection and should make our community ponder on building\nspecialized models by pretraining from scratch. Last but not least, doing so\ncould enable better control of efficiency, data bias and replicability, which\nare key research questions for the IR community.", "published": "2023-01-25 07:43:05", "link": "http://arxiv.org/abs/2301.10444v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked\n  Language Models", "abstract": "Large multilingual language models typically rely on a single vocabulary\nshared across 100+ languages. As these models have increased in parameter count\nand depth, vocabulary size has remained largely unchanged. This\n\\textit{vocabulary bottleneck} limits the representational capabilities of\nmultilingual models like XLM-R. In this paper, we introduce a new approach for\nscaling to very large multilingual vocabularies by de-emphasizing token sharing\nbetween languages with little lexical overlap and assigning vocabulary capacity\nto achieve sufficient coverage for each individual language. Tokenizations\nusing our vocabulary are typically more semantically meaningful and shorter\ncompared to XLM-R. Leveraging this improved vocabulary, we train XLM-V, a\nmultilingual language model with a one million token vocabulary. XLM-V\noutperforms XLM-R on every task we tested on ranging from natural language\ninference (XNLI), question answering (MLQA, XQuAD, TyDiQA), to named entity\nrecognition (WikiAnn). XLM-V is particularly effective on low-resource language\ntasks and outperforms XLM-R by 11.2% and 5.8% absolute on MasakhaNER and\nAmericas NLI, respectively.", "published": "2023-01-25 09:15:17", "link": "http://arxiv.org/abs/2301.10472v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FewShotTextGCN: K-hop neighborhood regularization for few-shot learning\n  on graphs", "abstract": "We present FewShotTextGCN, a novel method designed to effectively utilize the\nproperties of word-document graphs for improved learning in low-resource\nsettings. We introduce K-hop Neighbourhood Regularization, a regularizer for\nheterogeneous graphs, and show that it stabilizes and improves learning when\nonly a few training samples are available. We furthermore propose a\nsimplification in the graph-construction method, which results in a graph that\nis $\\sim$7 times less dense and yields better performance in little-resource\nsettings while performing on par with the state of the art in high-resource\nsettings. Finally, we introduce a new variant of Adaptive Pseudo-Labeling\ntailored for word-document graphs. When using as little as 20 samples for\ntraining, we outperform a strong TextGCN baseline with 17% in absolute accuracy\non average over eight languages. We demonstrate that our method can be applied\nto document classification without any language model pretraining on a wide\nrange of typologically diverse languages while performing on par with large\npretrained language models.", "published": "2023-01-25 09:30:32", "link": "http://arxiv.org/abs/2301.10481v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Backward Compatibility During Data Updates by Weight Interpolation", "abstract": "Backward compatibility of model predictions is a desired property when\nupdating a machine learning driven application. It allows to seamlessly improve\nthe underlying model without introducing regression bugs. In classification\ntasks these bugs occur in the form of negative flips. This means an instance\nthat was correctly classified by the old model is now classified incorrectly by\nthe updated model. This has direct negative impact on the user experience of\nsuch systems e.g. a frequently used voice assistant query is suddenly\nmisclassified. A common reason to update the model is when new training data\nbecomes available and needs to be incorporated. Simply retraining the model\nwith the updated data introduces the unwanted negative flips. We study the\nproblem of regression during data updates and propose Backward Compatible\nWeight Interpolation (BCWI). This method interpolates between the weights of\nthe old and new model and we show in extensive experiments that it reduces\nnegative flips without sacrificing the improved accuracy of the new model. BCWI\nis straight forward to implement and does not increase inference cost. We also\nexplore the use of importance weighting during interpolation and averaging the\nweights of multiple new models in order to further reduce negative flips.", "published": "2023-01-25 12:23:10", "link": "http://arxiv.org/abs/2301.10546v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Study on FGSM Adversarial Training for Neural Retrieval", "abstract": "Neural retrieval models have acquired significant effectiveness gains over\nthe last few years compared to term-based methods. Nevertheless, those models\nmay be brittle when faced to typos, distribution shifts or vulnerable to\nmalicious attacks. For instance, several recent papers demonstrated that such\nvariations severely impacted models performances, and then tried to train more\nresilient models. Usual approaches include synonyms replacements or typos\ninjections -- as data-augmentation -- and the use of more robust tokenizers\n(characterBERT, BPE-dropout). To further complement the literature, we\ninvestigate in this paper adversarial training as another possible solution to\nthis robustness issue. Our comparison includes the two main families of\nBERT-based neural retrievers, i.e. dense and sparse, with and without\ndistillation techniques. We then demonstrate that one of the most simple\nadversarial training techniques -- the Fast Gradient Sign Method (FGSM) -- can\nimprove first stage rankers robustness and effectiveness. In particular, FGSM\nincreases models performances on both in-domain and out-of-domain\ndistributions, and also on queries with typos, for multiple neural retrievers.", "published": "2023-01-25 13:28:54", "link": "http://arxiv.org/abs/2301.10576v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Automated multilingual detection of Pro-Kremlin propaganda in newspapers\n  and Telegram posts", "abstract": "The full-scale conflict between the Russian Federation and Ukraine generated\nan unprecedented amount of news articles and social media data reflecting\nopposing ideologies and narratives. These polarized campaigns have led to\nmutual accusations of misinformation and fake news, shaping an atmosphere of\nconfusion and mistrust for readers worldwide. This study analyses how the media\naffected and mirrored public opinion during the first month of the war using\nnews articles and Telegram news channels in Ukrainian, Russian, Romanian and\nEnglish. We propose and compare two methods of multilingual automated\npro-Kremlin propaganda identification, based on Transformers and linguistic\nfeatures. We analyse the advantages and disadvantages of both methods, their\nadaptability to new genres and languages, and ethical considerations of their\nusage for content moderation. With this work, we aim to lay the foundation for\nfurther development of moderation tools tailored to the current conflict.", "published": "2023-01-25 14:25:37", "link": "http://arxiv.org/abs/2301.10604v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fillers in Spoken Language Understanding: Computational and\n  Psycholinguistic Perspectives", "abstract": "Disfluencies (i.e. interruptions in the regular flow of speech), are\nubiquitous to spoken discourse. Fillers (\"uh\", \"um\") are disfluencies that\noccur the most frequently compared to other kinds of disfluencies. Yet, to the\nbest of our knowledge, there isn't a resource that brings together the research\nperspectives influencing Spoken Language Understanding (SLU) on these speech\nevents. This aim of this article is to survey a breadth of perspectives in a\nholistic way; i.e. from considering underlying (psycho)linguistic theory, to\ntheir annotation and consideration in Automatic Speech Recognition (ASR) and\nSLU systems, to lastly, their study from a generation standpoint. This article\naims to present the perspectives in an approachable way to the SLU and\nConversational AI community, and discuss moving forward, what we believe are\nthe trends and challenges in each area.", "published": "2023-01-25 18:55:05", "link": "http://arxiv.org/abs/2301.10761v4", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Explaining Large Language Model-Based Neural Semantic Parsers (Student\n  Abstract)", "abstract": "While large language models (LLMs) have demonstrated strong capability in\nstructured prediction tasks such as semantic parsing, few amounts of research\nhave explored the underlying mechanisms of their success. Our work studies\ndifferent methods for explaining an LLM-based semantic parser and qualitatively\ndiscusses the explained model behaviors, hoping to inspire future research\ntoward better understanding them.", "published": "2023-01-25 16:12:43", "link": "http://arxiv.org/abs/2301.13820v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Weakly Supervised Headline Dependency Parsing", "abstract": "English news headlines form a register with unique syntactic properties that\nhave been documented in linguistics literature since the 1930s. However,\nheadlines have received surprisingly little attention from the NLP syntactic\nparsing community. We aim to bridge this gap by providing the first news\nheadline corpus of Universal Dependencies annotated syntactic dependency trees,\nwhich enables us to evaluate existing state-of-the-art dependency parsers on\nnews headlines. To improve English news headline parsing accuracies, we develop\na projection method to bootstrap silver training data from unlabeled news\nheadline-article lead sentence pairs. Models trained on silver headline parses\ndemonstrate significant improvements in performance over models trained solely\non gold-annotated long-form texts. Ultimately, we find that, although projected\nsilver training data improves parser performance across different news outlets,\nthe improvement is moderated by constructions idiosyncratic to outlet.", "published": "2023-01-25 01:00:16", "link": "http://arxiv.org/abs/2301.10371v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "BDMMT: Backdoor Sample Detection for Language Models through Model\n  Mutation Testing", "abstract": "Deep neural networks (DNNs) and natural language processing (NLP) systems\nhave developed rapidly and have been widely used in various real-world fields.\nHowever, they have been shown to be vulnerable to backdoor attacks.\nSpecifically, the adversary injects a backdoor into the model during the\ntraining phase, so that input samples with backdoor triggers are classified as\nthe target class. Some attacks have achieved high attack success rates on the\npre-trained language models (LMs), but there have yet to be effective defense\nmethods. In this work, we propose a defense method based on deep model mutation\ntesting. Our main justification is that backdoor samples are much more robust\nthan clean samples if we impose random mutations on the LMs and that backdoors\nare generalizable. We first confirm the effectiveness of model mutation testing\nin detecting backdoor samples and select the most appropriate mutation\noperators. We then systematically defend against three extensively studied\nbackdoor attack levels (i.e., char-level, word-level, and sentence-level) by\ndetecting backdoor samples. We also make the first attempt to defend against\nthe latest style-level backdoor attacks. We evaluate our approach on three\nbenchmark datasets (i.e., IMDB, Yelp, and AG news) and three style transfer\ndatasets (i.e., SST-2, Hate-speech, and AG news). The extensive experimental\nresults demonstrate that our approach can detect backdoor samples more\nefficiently and accurately than the three state-of-the-art defense approaches.", "published": "2023-01-25 05:24:46", "link": "http://arxiv.org/abs/2301.10412v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Pre-computed memory or on-the-fly encoding? A hybrid approach to\n  retrieval augmentation makes the most of your compute", "abstract": "Retrieval-augmented language models such as Fusion-in-Decoder are powerful,\nsetting the state of the art on a variety of knowledge-intensive tasks.\nHowever, they are also expensive, due to the need to encode a large number of\nretrieved passages. Some work avoids this cost by pre-encoding a text corpus\ninto a memory and retrieving dense representations directly. However,\npre-encoding memory incurs a severe quality penalty as the memory\nrepresentations are not conditioned on the current input. We propose LUMEN, a\nhybrid between these two extremes, pre-computing the majority of the retrieval\nrepresentation and completing the encoding on the fly using a live encoder that\nis conditioned on the question and fine-tuned for the task. We show that LUMEN\nsignificantly outperforms pure memory on multiple question-answering tasks\nwhile being much cheaper than FiD, and outperforms both for any given compute\nbudget. Moreover, the advantage of LUMEN over FiD increases with model size.", "published": "2023-01-25 07:55:45", "link": "http://arxiv.org/abs/2301.10448v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improved Stock Price Movement Classification Using News Articles Based\n  on Embeddings and Label Smoothing", "abstract": "Stock price movement prediction is a challenging and essential problem in\nfinance. While it is well established in modern behavioral finance that the\nshare prices of related stocks often move after the release of news via\nreactions and overreactions of investors, how to capture the relationships\nbetween price movements and news articles via quantitative models is an active\narea research; existing models have achieved success with variable degrees. In\nthis paper, we propose to improve stock price movement classification using\nnews articles by incorporating regularization and optimization techniques from\ndeep learning. More specifically, we capture the dependencies between news\narticles and stocks through embeddings and bidirectional recurrent neural\nnetworks as in recent models. We further incorporate weight decay, batch\nnormalization, dropout, and label smoothing to improve the generalization of\nthe trained models. To handle high fluctuations of validation accuracy of batch\nnormalization, we propose dual-phase training to realize the improvements\nreliably. Our experimental results on a commonly used dataset show significant\nimprovements, achieving average accuracy of 80.7% on the test set, which is\nmore than 10.0% absolute improvement over existing models. Our ablation studies\nshow batch normalization and label smoothing are most effective, leading to\n6.0% and 3.4% absolute improvement, respectively on average.", "published": "2023-01-25 08:33:45", "link": "http://arxiv.org/abs/2301.10458v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "ExaRanker: Explanation-Augmented Neural Ranker", "abstract": "Recent work has shown that inducing a large language model (LLM) to generate\nexplanations prior to outputting an answer is an effective strategy to improve\nperformance on a wide range of reasoning tasks. In this work, we show that\nneural rankers also benefit from explanations. We use LLMs such as GPT-3.5 to\naugment retrieval datasets with explanations and train a sequence-to-sequence\nranking model to output a relevance label and an explanation for a given\nquery-document pair. Our model, dubbed ExaRanker, finetuned on a few thousand\nexamples with synthetic explanations performs on par with models finetuned on\n3x more examples without explanations. Furthermore, the ExaRanker model incurs\nno additional computational cost during ranking and allows explanations to be\nrequested on demand.", "published": "2023-01-25 11:03:04", "link": "http://arxiv.org/abs/2301.10521v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ARDIAS: AI-Enhanced Research Management, Discovery, and Advisory System", "abstract": "In this work, we present ARDIAS, a web-based application that aims to provide\nresearchers with a full suite of discovery and collaboration tools. ARDIAS\ncurrently allows searching for authors and articles by name and gaining\ninsights into the research topics of a particular researcher. With the aid of\nAI-based tools, ARDIAS aims to recommend potential collaborators and topics to\nresearchers. In the near future, we aim to add tools that allow researchers to\ncommunicate with each other and start new projects.", "published": "2023-01-25 13:30:10", "link": "http://arxiv.org/abs/2301.10577v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Distilling Text into Circuits", "abstract": "This paper concerns the structure of meanings within natural language.\nEarlier, a framework named DisCoCirc was sketched that (1) is compositional and\ndistributional (a.k.a. vectorial); (2) applies to general text; (3) captures\nlinguistic `connections' between meanings (cf. grammar) (4) updates word\nmeanings as text progresses; (5) structures sentence types; (6) accommodates\nambiguity. Here, we realise DisCoCirc for a substantial fragment of English.\n  When passing to DisCoCirc's text circuits, some `grammatical bureaucracy' is\neliminated, that is, DisCoCirc displays a significant degree of (7) inter- and\nintra-language independence. That is, e.g., independence from word-order\nconventions that differ across languages, and independence from choices like\nmany short sentences vs. few long sentences. This inter-language independence\nmeans our text circuits should carry over to other languages, unlike the\nlanguage-specific typings of categorial grammars. Hence, text circuits are a\nlean structure for the `actual substance of text', that is, the inner-workings\nof meanings within text across several layers of expressiveness (cf. words,\nsentences, text), and may capture that what is truly universal beneath grammar.\nThe elimination of grammatical bureaucracy also explains why DisCoCirc: (8)\napplies beyond language, e.g. to spatial, visual and other cognitive modes.\nWhile humans could not verbally communicate in terms of text circuits, machines\ncan.\n  We first define a `hybrid grammar' for a fragment of English, i.e. a\npurpose-built, minimal grammatical formalism needed to obtain text circuits. We\nthen detail a translation process such that all text generated by this grammar\nyields a text circuit. Conversely, for any text circuit obtained by freely\ncomposing the generators, there exists a text (with hybrid grammar) that gives\nrise to it. Hence: (9) text circuits are generative for text.", "published": "2023-01-25 13:56:34", "link": "http://arxiv.org/abs/2301.10595v1", "categories": ["cs.CL", "cs.AI", "cs.LO", "math.CT"], "primary_category": "cs.CL"}
{"title": "A Holistic Cascade System, benchmark, and Human Evaluation Protocol for\n  Expressive Speech-to-Speech Translation", "abstract": "Expressive speech-to-speech translation (S2ST) aims to transfer prosodic\nattributes of source speech to target speech while maintaining translation\naccuracy. Existing research in expressive S2ST is limited, typically focusing\non a single expressivity aspect at a time. Likewise, this research area lacks\nstandard evaluation protocols and well-curated benchmark datasets. In this\nwork, we propose a holistic cascade system for expressive S2ST, combining\nmultiple prosody transfer techniques previously considered only in isolation.\nWe curate a benchmark expressivity test set in the TV series domain and\nexplored a second dataset in the audiobook domain. Finally, we present a human\nevaluation protocol to assess multiple expressive dimensions across speech\npairs. Experimental results indicate that bi-lingual annotators can assess the\nquality of expressive preservation in S2ST systems, and the holistic modeling\napproach outperforms single-aspect systems. Audio samples can be accessed\nthrough our demo webpage:\nhttps://facebookresearch.github.io/speech_translation/cascade_expressive_s2st.", "published": "2023-01-25 14:27:00", "link": "http://arxiv.org/abs/2301.10606v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Probing Taxonomic and Thematic Embeddings for Taxonomic Information", "abstract": "Modelling taxonomic and thematic relatedness is important for building AI\nwith comprehensive natural language understanding. The goal of this paper is to\nlearn more about how taxonomic information is structurally encoded in\nembeddings. To do this, we design a new hypernym-hyponym probing task and\nperform a comparative probing study of taxonomic and thematic SGNS and GloVe\nembeddings. Our experiments indicate that both types of embeddings encode some\ntaxonomic information, but the amount, as well as the geometric properties of\nthe encodings, are independently related to both the encoder architecture, as\nwell as the embedding training data. Specifically, we find that only taxonomic\nembeddings carry taxonomic information in their norm, which is determined by\nthe underlying distribution in the data.", "published": "2023-01-25 15:59:26", "link": "http://arxiv.org/abs/2301.10656v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T30"], "primary_category": "cs.CL"}
{"title": "On the inconsistency of separable losses for structured prediction", "abstract": "In this paper, we prove that separable negative log-likelihood losses for\nstructured prediction are not necessarily Bayes consistent, or, in other words,\nminimizing these losses may not result in a model that predicts the most\nprobable structure in the data distribution for a given input. This fact opens\nthe question of whether these losses are well-adapted for structured prediction\nand, if so, why.", "published": "2023-01-25 20:02:07", "link": "http://arxiv.org/abs/2301.10810v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Partial Mobilization: Tracking Multilingual Information Flows Amongst\n  Russian Media Outlets and Telegram", "abstract": "In response to disinformation and propaganda from Russian online media\nfollowing the invasion of Ukraine, Russian media outlets such as Russia Today\nand Sputnik News were banned throughout Europe. To maintain viewership, many of\nthese Russian outlets began to heavily promote their content on messaging\nservices like Telegram. In this work, we study how 16 Russian media outlets\ninteracted with and utilized 732 Telegram channels throughout 2022. Leveraging\nthe foundational model MPNet, DP-means clustering, and Hawkes processes, we\ntrace how narratives spread between news sites and Telegram channels. We show\nthat news outlets not only propagate existing narratives through Telegram but\nthat they source material from the messaging platform. For example, across the\nwebsites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of\narticles discussed content that originated/resulted from activity on Telegram.\nFinally, tracking the spread of individual topics, we measure the rate at which\nnews outlets and Telegram channels disseminate content within the Russian media\necosystem, finding that websites like ura.news and Telegram channels such as\n@genshab are the most effective at disseminating their content.", "published": "2023-01-25 22:27:40", "link": "http://arxiv.org/abs/2301.10856v6", "categories": ["cs.CY", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Qualitative Analysis of a Graph Transformer Approach to Addressing Hate\n  Speech: Adapting to Dynamically Changing Content", "abstract": "Our work advances an approach for predicting hate speech in social media,\ndrawing out the critical need to consider the discussions that follow a post to\nsuccessfully detect when hateful discourse may arise. Using graph transformer\nnetworks, coupled with modelling attention and BERT-level natural language\nprocessing, our approach can capture context and anticipate upcoming\nanti-social behaviour. In this paper, we offer a detailed qualitative analysis\nof this solution for hate speech detection in social networks, leading to\ninsights into where the method has the most impressive outcomes in comparison\nwith competitors and identifying scenarios where there are challenges to\nachieving ideal performance. Included is an exploration of the kinds of posts\nthat permeate social media today, including the use of hateful images. This\nsuggests avenues for extending our model to be more comprehensive. A key\ninsight is that the focus on reasoning about the concept of context positions\nus well to be able to support multi-modal analysis of online posts. We conclude\nwith a reflection on how the problem we are addressing relates especially well\nto the theme of dynamic change, a critical concern for all AI solutions for\nsocial impact. We also comment briefly on how mental health well-being can be\nadvanced with our work, through curated content attuned to the extent of hate\nin posts.", "published": "2023-01-25 23:32:32", "link": "http://arxiv.org/abs/2301.10871v3", "categories": ["cs.LG", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Knowing About Knowing: An Illusion of Human Competence Can Hinder\n  Appropriate Reliance on AI Systems", "abstract": "The dazzling promises of AI systems to augment humans in various tasks hinge\non whether humans can appropriately rely on them. Recent research has shown\nthat appropriate reliance is the key to achieving complementary team\nperformance in AI-assisted decision making. This paper addresses an\nunder-explored problem of whether the Dunning-Kruger Effect (DKE) among people\ncan hinder their appropriate reliance on AI systems. DKE is a metacognitive\nbias due to which less-competent individuals overestimate their own skill and\nperformance. Through an empirical study (N = 249), we explored the impact of\nDKE on human reliance on an AI system, and whether such effects can be\nmitigated using a tutorial intervention that reveals the fallibility of AI\nadvice, and exploiting logic units-based explanations to improve user\nunderstanding of AI advice. We found that participants who overestimate their\nperformance tend to exhibit under-reliance on AI systems, which hinders optimal\nteam performance. Logic units-based explanations did not help users in either\nimproving the calibration of their competence or facilitating appropriate\nreliance. While the tutorial intervention was highly effective in helping users\ncalibrate their self-assessment and facilitating appropriate reliance among\nparticipants with overestimated self-assessment, we found that it can\npotentially hurt the appropriate reliance of participants with underestimated\nself-assessment. Our work has broad implications on the design of methods to\ntackle user cognitive biases while facilitating appropriate reliance on AI\nsystems. Our findings advance the current understanding of the role of\nself-assessment in shaping trust and reliance in human-AI decision making. This\nlays out promising future directions for relevant HCI research in this\ncommunity.", "published": "2023-01-25 14:26:10", "link": "http://arxiv.org/abs/2301.11333v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Editing Language Model-based Knowledge Graph Embeddings", "abstract": "Recently decades have witnessed the empirical success of framing Knowledge\nGraph (KG) embeddings via language models. However, language model-based KG\nembeddings are usually deployed as static artifacts, making them difficult to\nmodify post-deployment without re-training after deployment. To address this\nissue, we propose a new task of editing language model-based KG embeddings in\nthis paper. This task is designed to facilitate rapid, data-efficient updates\nto KG embeddings without compromising the performance of other aspects. We\nbuild four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and\nevaluate several knowledge editing baselines demonstrating the limited ability\nof previous models to handle the proposed challenging task. We further propose\na simple yet strong baseline dubbed KGEditor, which utilizes additional\nparametric layers of the hypernetwork to edit/add facts. Our comprehensive\nexperimental results reveal that KGEditor excels in updating specific facts\nwithout impacting the overall performance, even when faced with limited\ntraining resources. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/deltaKG.", "published": "2023-01-25 04:45:06", "link": "http://arxiv.org/abs/2301.10405v8", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "One Model for All Domains: Collaborative Domain-Prefix Tuning for\n  Cross-Domain NER", "abstract": "Cross-domain NER is a challenging task to address the low-resource problem in\npractical scenarios. Previous typical solutions mainly obtain a NER model by\npre-trained language models (PLMs) with data from a rich-resource domain and\nadapt it to the target domain. Owing to the mismatch issue among entity types\nin different domains, previous approaches normally tune all parameters of PLMs,\nending up with an entirely new NER model for each domain. Moreover, current\nmodels only focus on leveraging knowledge in one general source domain while\nfailing to successfully transfer knowledge from multiple sources to the target.\nTo address these issues, we introduce Collaborative Domain-Prefix Tuning for\ncross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically,\nwe present text-to-text generation grounding domain-related instructors to\ntransfer knowledge to new domain NER tasks without structural modifications. We\nutilize frozen PLMs and conduct collaborative domain-prefix tuning to stimulate\nthe potential of PLMs to handle NER tasks across various domains. Experimental\nresults on the Cross-NER benchmark show that the proposed approach has flexible\ntransfer ability and performs better on both one-source and multiple-source\ncross-domain NER tasks. Codes are available in\nhttps://github.com/zjunlp/DeepKE/tree/main/example/ner/cross.", "published": "2023-01-25 05:16:43", "link": "http://arxiv.org/abs/2301.10410v5", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving the Inference of Topic Models via Infinite Latent State\n  Replications", "abstract": "In text mining, topic models are a type of probabilistic generative models\nfor inferring latent semantic topics from text corpus. One of the most popular\ninference approaches to topic models is perhaps collapsed Gibbs sampling (CGS),\nwhich typically samples one single topic label for each observed document-word\npair. In this paper, we aim at improving the inference of CGS for topic models.\nWe propose to leverage state augmentation technique by maximizing the number of\ntopic samples to infinity, and then develop a new inference approach, called\ninfinite latent state replication (ILR), to generate robust soft topic\nassignment for each given document-word pair. Experimental results on the\npublicly available datasets show that ILR outperforms CGS for inference of\nexisting established topic models.", "published": "2023-01-25 17:07:25", "link": "http://arxiv.org/abs/2301.12974v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "math.ST", "stat.TH", "G.3; I.2.6"], "primary_category": "cs.CL"}
{"title": "Separate And Diffuse: Using a Pretrained Diffusion Model for Improving\n  Source Separation", "abstract": "The problem of speech separation, also known as the cocktail party problem,\nrefers to the task of isolating a single speech signal from a mixture of speech\nsignals. Previous work on source separation derived an upper bound for the\nsource separation task in the domain of human speech. This bound is derived for\ndeterministic models. Recent advancements in generative models challenge this\nbound. We show how the upper bound can be generalized to the case of random\ngenerative models. Applying a diffusion model Vocoder that was pretrained to\nmodel single-speaker voices on the output of a deterministic separation model\nleads to state-of-the-art separation results. It is shown that this requires\none to combine the output of the separation model with that of the diffusion\nmodel. In our method, a linear combination is performed, in the frequency\ndomain, using weights that are inferred by a learned model. We show\nstate-of-the-art results on 2, 3, 5, 10, and 20 speakers on multiple\nbenchmarks. In particular, for two speakers, our method is able to surpass what\nwas previously considered the upper performance bound.", "published": "2023-01-25 18:21:51", "link": "http://arxiv.org/abs/2301.10752v2", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "HEAR4Health: A blueprint for making computer audition a staple of modern\n  healthcare", "abstract": "Recent years have seen a rapid increase in digital medicine research in an\nattempt to transform traditional healthcare systems to their modern,\nintelligent, and versatile equivalents that are adequately equipped to tackle\ncontemporary challenges. This has led to a wave of applications that utilise AI\ntechnologies; first and foremost in the fields of medical imaging, but also in\nthe use of wearables and other intelligent sensors. In comparison, computer\naudition can be seen to be lagging behind, at least in terms of commercial\ninterest. Yet, audition has long been a staple assistant for medical\npractitioners, with the stethoscope being the quintessential sign of doctors\naround the world. Transforming this traditional technology with the use of AI\nentails a set of unique challenges. We categorise the advances needed in four\nkey pillars: Hear, corresponding to the cornerstone technologies needed to\nanalyse auditory signals in real-life conditions; Earlier, for the advances\nneeded in computational and data efficiency; Attentively, for accounting to\nindividual differences and handling the longitudinal nature of medical data;\nand, finally, Responsibly, for ensuring compliance to the ethical standards\naccorded to the field of medicine.", "published": "2023-01-25 09:25:08", "link": "http://arxiv.org/abs/2301.10477v1", "categories": ["cs.SD", "cs.CY", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On Batching Variable Size Inputs for Training End-to-End Speech\n  Enhancement Systems", "abstract": "The performance of neural network-based speech enhancement systems is\nprimarily influenced by the model architecture, whereas training times and\ncomputational resource utilization are primarily affected by training\nparameters such as the batch size. Since noisy and reverberant speech mixtures\ncan have different duration, a batching strategy is required to handle variable\nsize inputs during training, in particular for state-of-the-art end-to-end\nsystems. Such strategies usually strive for a compromise between zero-padding\nand data randomization, and can be combined with a dynamic batch size for a\nmore consistent amount of data in each batch. However, the effect of these\nstrategies on resource utilization and more importantly network performance is\nnot well documented. This paper systematically investigates the effect of\ndifferent batching strategies and batch sizes on the training statistics and\nspeech enhancement performance of a Conv-TasNet, evaluated in both matched and\nmismatched conditions. We find that using a small batch size during training\nimproves performance in both conditions for all batching strategies. Moreover,\nusing sorted or bucket batching with a dynamic batch size allows for reduced\ntraining time and GPU memory usage while achieving similar performance compared\nto random batching with a fixed batch size.", "published": "2023-01-25 13:45:02", "link": "http://arxiv.org/abs/2301.10587v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
