{"title": "Topic Stability over Noisy Sources", "abstract": "Topic modelling techniques such as LDA have recently been applied to speech\ntranscripts and OCR output. These corpora may contain noisy or erroneous texts\nwhich may undermine topic stability. Therefore, it is important to know how\nwell a topic modelling algorithm will perform when applied to noisy data. In\nthis paper we show that different types of textual noise will have diverse\neffects on the stability of different topic models. From these observations, we\npropose guidelines for text corpus generation, with a focus on automatic speech\ntranscription. We also suggest topic model selection methods for noisy corpora.", "published": "2015-08-05 13:18:51", "link": "http://arxiv.org/abs/1508.01067v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Progressive EM for Latent Tree Models and Hierarchical Topic Detection", "abstract": "Hierarchical latent tree analysis (HLTA) is recently proposed as a new method\nfor topic detection. It differs fundamentally from the LDA-based methods in\nterms of topic definition, topic-document relationship, and learning method. It\nhas been shown to discover significantly more coherent topics and better topic\nhierarchies. However, HLTA relies on the Expectation-Maximization (EM)\nalgorithm for parameter estimation and hence is not efficient enough to deal\nwith large datasets. In this paper, we propose a method to drastically speed up\nHLTA using a technique inspired by recent advances in the moments method.\nEmpirical experiments show that our method greatly improves the efficiency of\nHLTA. It is as efficient as the state-of-the-art LDA-based method for\nhierarchical topic detection and finds substantially better topics and topic\nhierarchies.", "published": "2015-08-05 05:00:32", "link": "http://arxiv.org/abs/1508.00973v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Relation Classification via Recurrent Neural Network", "abstract": "Deep learning has gained much success in sentence-level relation\nclassification. For example, convolutional neural networks (CNN) have delivered\ncompetitive performance without much effort on feature engineering as the\nconventional pattern-based methods. Thus a lot of works have been produced\nbased on CNN structures. However, a key issue that has not been well addressed\nby the CNN-based method is the lack of capability to learn temporal features,\nespecially long-distance dependency between nominal pairs. In this paper, we\npropose a simple framework based on recurrent neural networks (RNN) and compare\nit with CNN-based model. To show the limitation of popular used SemEval-2010\nTask 8 dataset, we introduce another dataset refined from MIMLRE(Angeli et al.,\n2014). Experiments on two different datasets strongly indicates that the\nRNN-based model can deliver better performance on relation classification, and\nit is particularly capable of learning long-distance relation patterns. This\nmakes it suitable for real-world applications where complicated expressions are\noften involved.", "published": "2015-08-05 09:03:46", "link": "http://arxiv.org/abs/1508.01006v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Learning from LDA using Deep Neural Networks", "abstract": "Latent Dirichlet Allocation (LDA) is a three-level hierarchical Bayesian\nmodel for topic inference. In spite of its great success, inferring the latent\ntopic distribution with LDA is time-consuming. Motivated by the transfer\nlearning approach proposed by~\\newcite{hinton2015distilling}, we present a\nnovel method that uses LDA to supervise the training of a deep neural network\n(DNN), so that the DNN can approximate the costly LDA inference with less\ncomputation. Our experiments on a document classification task show that a\nsimple DNN can learn the LDA behavior pretty well, while the inference is\nspeeded up tens or hundreds of times.", "published": "2015-08-05 09:22:25", "link": "http://arxiv.org/abs/1508.01011v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Listen, Attend and Spell", "abstract": "We present Listen, Attend and Spell (LAS), a neural network that learns to\ntranscribe speech utterances to characters. Unlike traditional DNN-HMM models,\nthis model learns all the components of a speech recognizer jointly. Our system\nhas two components: a listener and a speller. The listener is a pyramidal\nrecurrent network encoder that accepts filter bank spectra as inputs. The\nspeller is an attention-based recurrent network decoder that emits characters\nas outputs. The network produces character sequences without making any\nindependence assumptions between the characters. This is the key improvement of\nLAS over previous end-to-end CTC models. On a subset of the Google voice search\ntask, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a\nlanguage model, and 10.3% with language model rescoring over the top 32 beams.\nBy comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%.", "published": "2015-08-05 20:17:58", "link": "http://arxiv.org/abs/1508.01211v2", "categories": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
