{"title": "Hierarchical Question Answering for Long Documents", "abstract": "We present a framework for question answering that can efficiently scale to\nlonger documents while maintaining or even improving performance of\nstate-of-the-art models. While most successful approaches for reading\ncomprehension rely on recurrent neural networks (RNNs), running them over long\ndocuments is prohibitively slow because it is difficult to parallelize over\nsequences. Inspired by how people first skim the document, identify relevant\nparts, and carefully read these parts to produce an answer, we combine a\ncoarse, fast model for selecting relevant sentences and a more expensive RNN\nfor producing the answer from those sentences. We treat sentence selection as a\nlatent variable trained jointly from the answer only using reinforcement\nlearning. Experiments demonstrate the state of the art performance on a\nchallenging subset of the Wikireading and on a new dataset, while speeding up\nthe model by 3.5x-6.7x.", "published": "2016-11-06 20:24:40", "link": "http://arxiv.org/abs/1611.01839v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Fine Tuning: A Modular Approach to Learning on Small Data", "abstract": "In this paper we present a technique to train neural network models on small\namounts of data. Current methods for training neural networks on small amounts\nof rich data typically rely on strategies such as fine-tuning a pre-trained\nneural network or the use of domain-specific hand-engineered features. Here we\ntake the approach of treating network layers, or entire networks, as modules\nand combine pre-trained modules with untrained modules, to learn the shift in\ndistributions between data sets. The central impact of using a modular approach\ncomes from adding new representations to a network, as opposed to replacing\nrepresentations via fine-tuning. Using this technique, we are able surpass\nresults using standard fine-tuning transfer learning approaches, and we are\nalso able to significantly increase performance over such approaches when using\nsmaller amounts of data.", "published": "2016-11-06 01:32:39", "link": "http://arxiv.org/abs/1611.01714v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Words or Characters? Fine-grained Gating for Reading Comprehension", "abstract": "Previous work combines word-level and character-level representations using\nconcatenation or scalar weighting, which is suboptimal for high-level tasks\nlike reading comprehension. We present a fine-grained gating mechanism to\ndynamically combine word-level and character-level representations based on\nproperties of the words. We also extend the idea of fine-grained gating to\nmodeling the interaction between questions and paragraphs for reading\ncomprehension. Experiments show that our approach can improve the performance\non reading comprehension tasks, achieving new state-of-the-art results on the\nChildren's Book Test dataset. To demonstrate the generality of our gating\nmechanism, we also show improved results on a social media tag prediction task.", "published": "2016-11-06 03:17:42", "link": "http://arxiv.org/abs/1611.01724v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Biaffine Attention for Neural Dependency Parsing", "abstract": "This paper builds off recent work from Kiperwasser & Goldberg (2016) using\nneural attention in a simple graph-based dependency parser. We use a larger but\nmore thoroughly regularized parser than other recent BiLSTM-based approaches,\nwith biaffine classifiers to predict arcs and labels. Our parser gets state of\nthe art or near state of the art performance on standard treebanks for six\ndifferent languages, achieving 95.7% UAS and 94.1% LAS on the most popular\nEnglish PTB dataset. This makes it the highest-performing graph-based parser on\nthis benchmark---outperforming Kiperwasser Goldberg (2016) by 1.8% and\n2.2%---and comparable to the highest performing transition-based parser\n(Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show\nwhich hyperparameter choices had a significant effect on parsing accuracy,\nallowing us to achieve large gains over other graph-based approaches.", "published": "2016-11-06 07:26:38", "link": "http://arxiv.org/abs/1611.01734v3", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "A Compare-Aggregate Model for Matching Text Sequences", "abstract": "Many NLP tasks including machine comprehension, answer selection and text\nentailment require the comparison between sequences. Matching the important\nunits between sequences is a key to solve these problems. In this paper, we\npresent a general \"compare-aggregate\" framework that performs word-level\nmatching followed by aggregation using Convolutional Neural Networks. We\nparticularly focus on the different comparison functions we can use to match\ntwo vectors. We use four different datasets to evaluate the model. We find that\nsome simple comparison functions based on element-wise operations can work\nbetter than standard neural network and neural tensor network.", "published": "2016-11-06 09:50:24", "link": "http://arxiv.org/abs/1611.01747v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Domain Adaptation For Formant Estimation Using Deep Learning", "abstract": "In this paper we present a domain adaptation technique for formant estimation\nusing a deep network. We first train a deep learning network on a small read\nspeech dataset. We then freeze the parameters of the trained network and use\nseveral different datasets to train an adaptation layer that makes the obtained\nnetwork universal in the sense that it works well for a variety of speakers and\nspeech domains with very different characteristics. We evaluated our adapted\nnetwork on three datasets, each of which has different speaker characteristics\nand speech styles. The performance of our method compares favorably with\nalternative methods for formant estimation.", "published": "2016-11-06 14:00:14", "link": "http://arxiv.org/abs/1611.01783v1", "categories": ["cs.CL", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Self-Wiring Question Answering Systems", "abstract": "Question answering (QA) has been the subject of a resurgence over the past\nyears. The said resurgence has led to a multitude of question answering (QA)\nsystems being developed both by companies and research facilities. While a few\ncomponents of QA systems get reused across implementations, most systems do not\nleverage the full potential of component reuse. Hence, the development of QA\nsystems is currently still a tedious and time-consuming process. We address the\nchallenge of accelerating the creation of novel or tailored QA systems by\npresenting a concept for a self-wiring approach to composing QA systems. Our\napproach will allow the reuse of existing, web-based QA systems or modules\nwhile developing new QA platforms. To this end, it will rely on QA modules\nbeing described using the Web Ontology Language. Based on these descriptions,\nour approach will be able to automatically compose QA systems using a\ndata-driven approach automatically.", "published": "2016-11-06 16:08:21", "link": "http://arxiv.org/abs/1611.01802v2", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
