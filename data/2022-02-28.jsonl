{"title": "KMIR: A Benchmark for Evaluating Knowledge Memorization, Identification\n  and Reasoning Abilities of Language Models", "abstract": "Previous works show the great potential of pre-trained language models (PLMs)\nfor storing a large amount of factual knowledge. However, to figure out whether\nPLMs can be reliable knowledge sources and used as alternative knowledge bases\n(KBs), we need to further explore some critical features of PLMs. Firstly,\nknowledge memorization and identification abilities: traditional KBs can store\nvarious types of entities and relationships; do PLMs have a high knowledge\ncapacity to store different types of knowledge? Secondly, reasoning ability: a\nqualified knowledge source should not only provide a collection of facts, but\nsupport a symbolic reasoner. Can PLMs derive new knowledge based on the\ncorrelations between facts? To evaluate these features of PLMs, we propose a\nbenchmark, named Knowledge Memorization, Identification, and Reasoning test\n(KMIR). KMIR covers 3 types of knowledge, including general knowledge,\ndomain-specific knowledge, and commonsense, and provides 184,348 well-designed\nquestions. Preliminary experiments with various representative pre-training\nlanguage models on KMIR reveal many interesting phenomenons: 1) The\nmemorization ability of PLMs depends more on the number of parameters than\ntraining schemes. 2) Current PLMs are struggling to robustly remember the\nfacts. 3) Model compression technology retains the amount of knowledge well,\nbut hurts the identification and reasoning abilities. We hope KMIR can\nfacilitate the design of PLMs as better knowledge sources.", "published": "2022-02-28 03:52:57", "link": "http://arxiv.org/abs/2202.13529v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CINO: A Chinese Minority Pre-trained Language Model", "abstract": "Multilingual pre-trained language models have shown impressive performance on\ncross-lingual tasks. It greatly facilitates the applications of natural\nlanguage processing on low-resource languages. However, there are still some\nlanguages that the current multilingual models do not perform well on. In this\npaper, we propose CINO (Chinese Minority Pre-trained Language Model), a\nmultilingual pre-trained language model for Chinese minority languages. It\ncovers Standard Chinese, Yue Chinese, and six other ethnic minority languages.\nTo evaluate the cross-lingual ability of the multilingual model on ethnic\nminority languages, we collect documents from Wikipedia and news websites, and\nconstruct two text classification datasets, WCM (Wiki-Chinese-Minority) and\nCMNews (Chinese-Minority-News). We show that CINO notably outperforms the\nbaselines on various classification tasks. The CINO model and the datasets are\npublicly available at http://cino.hfl-rc.com.", "published": "2022-02-28 06:02:06", "link": "http://arxiv.org/abs/2202.13558v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "'Tis but Thy Name: Semantic Question Answering Evaluation with 11M Names\n  for 1M Entities", "abstract": "Classic lexical-matching-based QA metrics are slowly being phased out because\nthey punish succinct or informative outputs just because those answers were not\nprovided as ground truth. Recently proposed neural metrics can evaluate\nsemantic similarity but were trained on small textual similarity datasets\ngrafted from foreign domains. We introduce the Wiki Entity Similarity (WES)\ndataset, an 11M example, domain targeted, semantic entity similarity dataset\nthat is generated from link texts in Wikipedia. WES is tailored to QA\nevaluation: the examples are entities and phrases and grouped into semantic\nclusters to simulate multiple ground-truth labels. Human annotators\nconsistently agree with WES labels, and a basic cross encoder metric is better\nthan four classic metrics at predicting human judgments of correctness.", "published": "2022-02-28 07:12:39", "link": "http://arxiv.org/abs/2202.13581v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supervised Machine Learning Algorithm for Detecting Consistency between\n  Reported Findings and the Conclusions of Mammography Reports", "abstract": "Objective. Mammography reports document the diagnosis of patients'\nconditions. However, many reports contain non-standard terms (non-BI-RADS\ndescriptors) and incomplete statements, which can lead to conclusions that are\nnot well-supported by the reported findings. Our aim was to develop a tool to\ndetect such discrepancies by comparing the reported conclusions to those that\nwould be expected based on the reported radiology findings. Materials and\nMethods. A deidentified data set from an academic hospital containing 258\nmammography reports supplemented by 120 reports found on the web was used for\ntraining and evaluation. Spell checking and term normalization was used to\nunambiguously determine the reported BI-RADS descriptors. The resulting data\nwere input into seven classifiers that classify mammography reports, based on\ntheir Findings sections, into seven BI-RADS final assessment categories.\nFinally, the semantic similarity score of a report to each BI-RADS category is\nreported. Results. Our term normalization algorithm correctly identified 97% of\nthe BI-RADS descriptors in mammography reports. Our system provided 76%\nprecision and 83% recall in correctly classifying the reports according to\nBI-RADS final assessment category. Discussion. The strength of our approach\nrelies on providing high importance to BI-RADS terms in the summarization\nphase, on the semantic similarity that considers the complex data\nrepresentation, and on the classification into all seven BI-RADs categories.\nConclusion. BI-RADS descriptors and expected final assessment categories could\nbe automatically detected by our approach with fairly good accuracy, which\ncould be used to make users aware that their reported findings do not match\nwell with their conclusion.", "published": "2022-02-28 08:59:04", "link": "http://arxiv.org/abs/2202.13618v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Lexical Embeddings for Robust Question Answering", "abstract": "Recent techniques in Question Answering (QA) have gained remarkable\nperformance improvement with some QA models even surpassed human performance.\nHowever, the ability of these models in truly understanding the language still\nremains dubious and the models are revealing limitations when facing\nadversarial examples. To strengthen the robustness of QA models and their\ngeneralization ability, we propose a representation Enhancement via Semantic\nand Context constraints (ESC) approach to improve the robustness of lexical\nembeddings. Specifically, we insert perturbations with semantic constraints and\ntrain enhanced contextual representations via a context-constraint loss to\nbetter distinguish the context clues for the correct answer. Experimental\nresults show that our approach gains significant robustness improvement on four\nadversarial test sets.", "published": "2022-02-28 09:28:34", "link": "http://arxiv.org/abs/2202.13636v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MSCTD: A Multimodal Sentiment Chat Translation Dataset", "abstract": "Multimodal machine translation and textual chat translation have received\nconsiderable attention in recent years. Although the conversation in its\nnatural form is usually multimodal, there still lacks work on multimodal\nmachine translation in conversations. In this work, we introduce a new task\nnamed Multimodal Chat Translation (MCT), aiming to generate more accurate\ntranslations with the help of the associated dialogue history and visual\ncontext. To this end, we firstly construct a Multimodal Sentiment Chat\nTranslation Dataset (MSCTD) containing 142,871 English-Chinese utterance pairs\nin 14,762 bilingual dialogues and 30,370 English-German utterance pairs in\n3,079 bilingual dialogues. Each utterance pair, corresponding to the visual\ncontext that reflects the current conversational scene, is annotated with a\nsentiment label. Then, we benchmark the task by establishing multiple baseline\nsystems that incorporate multimodal and sentiment features for MCT. Preliminary\nexperiments on four language directions (English-Chinese and English-German)\nverify the potential of contextual and multimodal information fusion and the\npositive impact of sentiment on the MCT task. Additionally, as a by-product of\nthe MSCTD, it also provides two new benchmarks on multimodal dialogue sentiment\nanalysis. Our work can facilitate research on both multimodal chat translation\nand multimodal dialogue sentiment analysis.", "published": "2022-02-28 09:40:46", "link": "http://arxiv.org/abs/2202.13645v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GausSetExpander: A Simple Approach for Entity Set Expansion", "abstract": "Entity Set Expansion is an important NLP task that aims at expanding a small\nset of entities into a larger one with items from a large pool of candidates.\nIn this paper, we propose GausSetExpander, an unsupervised approach based on\noptimal transport techniques. We propose to re-frame the problem as choosing\nthe entity that best completes the seed set. For this, we interpret a set as an\nelliptical distribution with a centroid which represents the mean and a spread\nthat is represented by the scale parameter. The best entity is the one that\nincreases the spread of the set the least. We demonstrate the validity of our\napproach by comparing to state-of-the art approaches.", "published": "2022-02-28 09:44:43", "link": "http://arxiv.org/abs/2202.13649v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Text Classification with Multilingual Distillation and\n  Zero-Shot-Aware Training", "abstract": "Multilingual pre-trained language models (MPLMs) not only can handle tasks in\ndifferent languages but also exhibit surprising zero-shot cross-lingual\ntransferability. However, MPLMs usually are not able to achieve comparable\nsupervised performance on rich-resource languages compared to the\nstate-of-the-art monolingual pre-trained models. In this paper, we aim to\nimprove the multilingual model's supervised and zero-shot performance\nsimultaneously only with the resources from supervised languages. Our approach\nis based on transferring knowledge from high-performance monolingual models\nwith a teacher-student framework. We let the multilingual model learn from\nmultiple monolingual models simultaneously. To exploit the model's\ncross-lingual transferability, we propose MBLM (multi-branch multilingual\nlanguage model), a model built on the MPLMs with multiple language branches.\nEach branch is a stack of transformers. MBLM is trained with the\nzero-shot-aware training strategy that encourages the model to learn from the\nmixture of zero-shot representations from all the branches. The results on two\ncross-lingual classification tasks show that, with only the task's supervised\ndata used, our method improves both the supervised and zero-shot performance of\nMPLMs.", "published": "2022-02-28 09:51:32", "link": "http://arxiv.org/abs/2202.13654v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LiLT: A Simple yet Effective Language-Independent Layout Transformer for\n  Structured Document Understanding", "abstract": "Structured document understanding has attracted considerable attention and\nmade significant progress recently, owing to its crucial role in intelligent\ndocument processing. However, most existing related models can only deal with\nthe document data of specific language(s) (typically English) included in the\npre-training collection, which is extremely limited. To address this issue, we\npropose a simple yet effective Language-independent Layout Transformer (LiLT)\nfor structured document understanding. LiLT can be pre-trained on the\nstructured documents of a single language and then directly fine-tuned on other\nlanguages with the corresponding off-the-shelf monolingual/multilingual\npre-trained textual models. Experimental results on eight languages have shown\nthat LiLT can achieve competitive or even superior performance on diverse\nwidely-used downstream benchmarks, which enables language-independent benefit\nfrom the pre-training of document layout structure. Code and model are publicly\navailable at https://github.com/jpWang/LiLT.", "published": "2022-02-28 10:33:01", "link": "http://arxiv.org/abs/2202.13669v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Recent Advances and Challenges in Reinforcement Learning\n  Methods for Task-Oriented Dialogue Policy Learning", "abstract": "Dialogue Policy Learning is a key component in a task-oriented dialogue\nsystem (TDS) that decides the next action of the system given the dialogue\nstate at each turn. Reinforcement Learning (RL) is commonly chosen to learn the\ndialogue policy, regarding the user as the environment and the system as the\nagent. Many benchmark datasets and algorithms have been created to facilitate\nthe development and evaluation of dialogue policy based on RL. In this paper,\nwe survey recent advances and challenges in dialogue policy from the\nprescriptive of RL. More specifically, we identify the major problems and\nsummarize corresponding solutions for RL-based dialogue policy learning.\nBesides, we provide a comprehensive survey of applying RL to dialogue policy\nlearning by categorizing recent methods into basic elements in RL. We believe\nthis survey can shed a light on future research in dialogue management.", "published": "2022-02-28 10:50:22", "link": "http://arxiv.org/abs/2202.13675v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-to-text Generation with Variational Sequential Planning", "abstract": "We consider the task of data-to-text generation, which aims to create textual\noutput from non-linguistic input. We focus on generating long-form text, i.e.,\ndocuments with multiple paragraphs, and propose a neural model enhanced with a\nplanning component responsible for organizing high-level information in a\ncoherent and meaningful way. We infer latent plans sequentially with a\nstructured variational model, while interleaving the steps of planning and\ngeneration. Text is generated by conditioning on previous variational decisions\nand previously generated text. Experiments on two data-to-text benchmarks\n(RotoWire and MLB) show that our model outperforms strong baselines and is\nsample efficient in the face of limited training data (e.g., a few hundred\ninstances).", "published": "2022-02-28 13:17:59", "link": "http://arxiv.org/abs/2202.13756v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Textual Embedding against Word-level Adversarial Attacks", "abstract": "We attribute the vulnerability of natural language processing models to the\nfact that similar inputs are converted to dissimilar representations in the\nembedding space, leading to inconsistent outputs, and we propose a novel robust\ntraining method, termed Fast Triplet Metric Learning (FTML). Specifically, we\nargue that the original sample should have similar representation with its\nadversarial counterparts and distinguish its representation from other samples\nfor better robustness. To this end, we adopt the triplet metric learning into\nthe standard training to pull words closer to their positive samples (i.e.,\nsynonyms) and push away their negative samples (i.e., non-synonyms) in the\nembedding space. Extensive experiments demonstrate that FTML can significantly\npromote the model robustness against various advanced adversarial attacks while\nkeeping competitive classification accuracy on original samples. Besides, our\nmethod is efficient as it only needs to adjust the embedding and introduces\nvery little overhead on the standard training. Our work shows great potential\nof improving the textual robustness through robust word embedding.", "published": "2022-02-28 14:25:00", "link": "http://arxiv.org/abs/2202.13817v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Smoothing: Enhance Various Data Augmentation Methods on Text\n  Classification Tasks", "abstract": "Before entering the neural network, a token is generally converted to the\ncorresponding one-hot representation, which is a discrete distribution of the\nvocabulary. Smoothed representation is the probability of candidate tokens\nobtained from a pre-trained masked language model, which can be seen as a more\ninformative substitution to the one-hot representation. We propose an efficient\ndata augmentation method, termed text smoothing, by converting a sentence from\nits one-hot representation to a controllable smoothed representation. We\nevaluate text smoothing on different benchmarks in a low-resource regime.\nExperimental results show that text smoothing outperforms various mainstream\ndata augmentation methods by a substantial margin. Moreover, text smoothing can\nbe combined with those data augmentation methods to achieve better performance.", "published": "2022-02-28 14:54:08", "link": "http://arxiv.org/abs/2202.13840v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The impact of lexical and grammatical processing on generating code from\n  natural language", "abstract": "Considering the seq2seq architecture of TranX for natural language to code\ntranslation, we identify four key components of importance: grammatical\nconstraints, lexical preprocessing, input representations, and copy mechanisms.\nTo study the impact of these components, we use a state-of-the-art architecture\nthat relies on BERT encoder and a grammar-based decoder for which a\nformalization is provided. The paper highlights the importance of the lexical\nsubstitution component in the current natural language to code systems.", "published": "2022-02-28 17:23:30", "link": "http://arxiv.org/abs/2202.13972v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study on Explanations in Out-of-Domain Settings", "abstract": "Recent work in Natural Language Processing has focused on developing\napproaches that extract faithful explanations, either via identifying the most\nimportant tokens in the input (i.e. post-hoc explanations) or by designing\ninherently faithful models that first select the most important tokens and then\nuse them to predict the correct label (i.e. select-then-predict models).\nCurrently, these approaches are largely evaluated on in-domain settings. Yet,\nlittle is known about how post-hoc explanations and inherently faithful models\nperform in out-of-domain settings. In this paper, we conduct an extensive\nempirical study that examines: (1) the out-of-domain faithfulness of post-hoc\nexplanations, generated by five feature attribution methods; and (2) the\nout-of-domain performance of two inherently faithful models over six datasets.\nContrary to our expectations, results show that in many cases out-of-domain\npost-hoc explanation faithfulness measured by sufficiency and comprehensiveness\nis higher compared to in-domain. We find this misleading and suggest using a\nrandom baseline as a yardstick for evaluating post-hoc explanation\nfaithfulness. Our findings also show that select-then predict models\ndemonstrate comparable predictive performance in out-of-domain settings to\nfull-text trained models.", "published": "2022-02-28 19:50:23", "link": "http://arxiv.org/abs/2203.00056v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking and Refining the Distinct Metric", "abstract": "Distinct-$n$ score\\cite{Li2016} is a widely used automatic metric for\nevaluating diversity in language generation tasks. However, we observed that\nthe original approach for calculating distinct scores has evident biases that\ntend to assign higher penalties to longer sequences. We refine the calculation\nof distinct scores by scaling the number of distinct tokens based on their\nexpectations. We provide both empirical and theoretical evidence to show that\nour method effectively removes the biases existing in the original distinct\nscore. Our experiments show that our proposed metric,\n\\textit{Expectation-Adjusted Distinct (EAD)}, correlates better with human\njudgment in evaluating response diversity. To foster future research, we\nprovide an example implementation at\n\\url{https://github.com/lsy641/Expectation-Adjusted-Distinct}.", "published": "2022-02-28 07:36:30", "link": "http://arxiv.org/abs/2202.13587v3", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "LCP-dropout: Compression-based Multiple Subword Segmentation for Neural\n  Machine Translation", "abstract": "In this study, we propose a simple and effective preprocessing method for\nsubword segmentation based on a data compression algorithm. Compression-based\nsubword segmentation has recently attracted significant attention as a\npreprocessing method for training data in Neural Machine Translation. Among\nthem, BPE/BPE-dropout is one of the fastest and most effective method compared\nto conventional approaches. However, compression-based approach has a drawback\nin that generating multiple segmentations is difficult due to the determinism.\nTo overcome this difficulty, we focus on a probabilistic string algorithm,\ncalled locally-consistent parsing (LCP), that has been applied to achieve\noptimum compression. Employing the probabilistic mechanism of LCP, we propose\nLCP-dropout for multiple subword segmentation that improves BPE/BPE-dropout,\nand show that it outperforms various baselines in learning from especially\nsmall training data.", "published": "2022-02-28 07:49:07", "link": "http://arxiv.org/abs/2202.13590v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quality-aware News Recommendation", "abstract": "News recommendation is a core technique used by many online news platforms.\nRecommending high-quality news to users is important for keeping good user\nexperiences and news platforms' reputations. However, existing news\nrecommendation methods mainly aim to optimize news clicks while ignoring the\nquality of news they recommended, which may lead to recommending news with\nuninformative content or even clickbaits. In this paper, we propose a\nquality-aware news recommendation method named QualityRec that can effectively\nimprove the quality of recommended news. In our approach, we first propose an\neffective news quality evaluation method based on the distributions of users'\nreading dwell time on news. Next, we propose to incorporate news quality\ninformation into user interest modeling by designing a content-quality\nattention network to select clicked news based on both news semantics and\nqualities. We further train the recommendation model with an auxiliary news\nquality prediction task to learn quality-aware recommendation model, and we add\na recommendation quality regularization loss to encourage the model to\nrecommend higher-quality news. Extensive experiments on two real-world datasets\nshow that QualityRec can effectively improve the overall quality of recommended\nnews and reduce the recommendation of low-quality news, with even slightly\nbetter recommendation accuracy.", "published": "2022-02-28 08:25:58", "link": "http://arxiv.org/abs/2202.13605v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Interactive Machine Learning for Image Captioning", "abstract": "We propose an approach for interactive learning for an image captioning\nmodel. As human feedback is expensive and modern neural network based\napproaches often require large amounts of supervised data to be trained, we\nenvision a system that exploits human feedback as good as possible by\nmultiplying the feedback using data augmentation methods, and integrating the\nresulting training examples into the model in a smart way. This approach has\nthree key components, for which we need to find suitable practical\nimplementations: feedback collection, data augmentation, and model update. We\noutline our idea and review different possibilities to address these tasks.", "published": "2022-02-28 09:02:32", "link": "http://arxiv.org/abs/2202.13623v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Confidence Based Bidirectional Global Context Aware Training Framework\n  for Neural Machine Translation", "abstract": "Most dominant neural machine translation (NMT) models are restricted to make\npredictions only according to the local context of preceding words in a\nleft-to-right manner. Although many previous studies try to incorporate global\ninformation into NMT models, there still exist limitations on how to\neffectively exploit bidirectional global context. In this paper, we propose a\nConfidence Based Bidirectional Global Context Aware (CBBGCA) training framework\nfor NMT, where the NMT model is jointly trained with an auxiliary conditional\nmasked language model (CMLM). The training consists of two stages: (1)\nmulti-task joint training; (2) confidence based knowledge distillation. At the\nfirst stage, by sharing encoder parameters, the NMT model is additionally\nsupervised by the signal from the CMLM decoder that contains bidirectional\nglobal contexts. Moreover, at the second stage, using the CMLM as teacher, we\nfurther pertinently incorporate bidirectional global context to the NMT model\non its unconfidently-predicted target words via knowledge distillation.\nExperimental results show that our proposed CBBGCA training framework\nsignificantly improves the NMT model by +1.02, +1.30 and +0.57 BLEU scores on\nthree large-scale translation datasets, namely WMT'14 English-to-German, WMT'19\nChinese-to-English and WMT'14 English-to-French, respectively.", "published": "2022-02-28 10:24:22", "link": "http://arxiv.org/abs/2202.13663v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Mutually Reinforced Framework for Pretrained Sentence Embeddings", "abstract": "The lack of labeled data is a major obstacle to learning high-quality\nsentence embeddings. Recently, self-supervised contrastive learning (SCL) is\nregarded as a promising way to address this problem. However, the existing\nworks mainly rely on hand-crafted data annotation heuristics to generate\npositive training samples, which not only call for domain expertise and\nlaborious tuning, but are also prone to the following unfavorable cases: 1)\ntrivial positives, 2) coarse-grained positives, and 3) false positives. As a\nresult, the self-supervision's quality can be severely limited in reality. In\nthis work, we propose a novel framework InfoCSE to address the above problems.\nInstead of relying on annotation heuristics defined by humans, it leverages the\nsentence representation model itself and realizes the following iterative\nself-supervision process: on one hand, the improvement of sentence\nrepresentation may contribute to the quality of data annotation; on the other\nhand, more effective data annotation helps to generate high-quality positive\nsamples, which will further improve the current sentence representation model.\nIn other words, the representation learning and data annotation become mutually\nreinforced, where a strong self-supervision effect can be derived. Extensive\nexperiments are performed based on three benchmark datasets, where notable\nimprovements can be achieved against the existing SCL-based methods.", "published": "2022-02-28 14:00:16", "link": "http://arxiv.org/abs/2202.13802v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "TraceNet: Tracing and Locating the Key Elements in Sentiment Analysis", "abstract": "In this paper, we study sentiment analysis task where the outcomes are mainly\ncontributed by a few key elements of the inputs. Motivated by the two-streams\nhypothesis, we propose a neural architecture, named TraceNet, to address this\ntype of task. It not only learns discriminative representations for the target\ntask via its encoders, but also traces key elements at the same time via its\nlocators. In TraceNet, both encoders and locators are organized in a layer-wise\nmanner, and a smoothness regularization is employed between adjacent\nencoder-locator combinations. Moreover, a sparsity constraints are enforced on\nlocators for tracing purposes and items are proactively masked according to the\nitem weights output by locators.A major advantage of TraceNet is that the\noutcomes are easier to understand, since the most responsible parts of inputs\nare identified. Also, under the guidance of locators, it is more robust to\nattacks due to its focus on key elements and the proactive masking training\nstrategy. Experimental results show its effectiveness for sentiment\nclassification. Moreover, we provide several case studies to demonstrate its\nrobustness and interpretability.", "published": "2022-02-28 14:20:34", "link": "http://arxiv.org/abs/2202.13812v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PMC-Patients: A Large-scale Dataset of Patient Summaries and Relations\n  for Benchmarking Retrieval-based Clinical Decision Support Systems", "abstract": "Objective: Retrieval-based Clinical Decision Support (ReCDS) can aid clinical\nworkflow by providing relevant literature and similar patients for a given\npatient. However, the development of ReCDS systems has been severely obstructed\nby the lack of diverse patient collections and publicly available large-scale\npatient-level annotation datasets. In this paper, we aim to define and\nbenchmark two ReCDS tasks: Patient-to-Article Retrieval (ReCDS-PAR) and\nPatient-to-Patient Retrieval (ReCDS-PPR) using a novel dataset called\nPMC-Patients. Methods: We extract patient summaries from PubMed Central\narticles using simple heuristics and utilize the PubMed citation graph to\ndefine patient-article relevance and patient-patient similarity. We also\nimplement and evaluate several ReCDS systems on the PMC-Patients benchmarks,\nincluding sparse retrievers, dense retrievers, and nearest neighbor retrievers.\nWe conduct several case studies to show the clinical utility of PMC-Patients.\nResults: PMC-Patients contains 167k patient summaries with 3.1M patient-article\nrelevance annotations and 293k patient-patient similarity annotations, which is\nthe largest-scale resource for ReCDS and also one of the largest patient\ncollections. Human evaluation and analysis show that PMC-Patients is a diverse\ndataset with high-quality annotations. The evaluation of various ReCDS systems\nshows that the PMC-Patients benchmark is challenging and calls for further\nresearch. Conclusion: We present PMC-Patients, a large-scale, diverse, and\npublicly available patient summary dataset with the largest-scale patient-level\nrelation annotations. Based on PMC-Patients, we formally define two benchmark\ntasks for ReCDS systems and evaluate various existing retrieval methods.\nPMC-Patients can largely facilitate methodology research on ReCDS systems and\nshows real-world clinical utility.", "published": "2022-02-28 15:24:33", "link": "http://arxiv.org/abs/2202.13876v4", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Probing the Robustness of Trained Metrics for Conversational Dialogue\n  Systems", "abstract": "This paper introduces an adversarial method to stress-test trained metrics to\nevaluate conversational dialogue systems. The method leverages Reinforcement\nLearning to find response strategies that elicit optimal scores from the\ntrained metrics. We apply our method to test recently proposed trained metrics.\nWe find that they all are susceptible to giving high scores to responses\ngenerated by relatively simple and obviously flawed strategies that our method\nconverges on. For instance, simply copying parts of the conversation context to\nform a response yields competitive scores or even outperforms responses written\nby humans.", "published": "2022-02-28 15:37:59", "link": "http://arxiv.org/abs/2202.13887v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Combining Modular Skills in Multitask Learning", "abstract": "A modular design encourages neural models to disentangle and recombine\ndifferent facets of knowledge to generalise more systematically to new tasks.\nIn this work, we assume that each task is associated with a subset of latent\ndiscrete skills from a (potentially small) inventory. In turn, skills\ncorrespond to parameter-efficient (sparse / low-rank) model parameterisations.\nBy jointly learning these and a task-skill allocation matrix, the network for\neach task is instantiated as the average of the parameters of active skills. To\nfavour non-trivial soft partitions of skills across tasks, we experiment with a\nseries of inductive biases, such as an Indian Buffet Process prior and a\ntwo-speed learning rate. We evaluate our latent-skill model on two main\nsettings: 1) multitask reinforcement learning for grounded instruction\nfollowing on 8 levels of the BabyAI platform; and 2) few-shot adaptation of\npre-trained text-to-text generative models on CrossFit, a benchmark comprising\n160 NLP tasks. We find that the modular design of a network significantly\nincreases sample efficiency in reinforcement learning and few-shot\ngeneralisation in supervised learning, compared to baselines with fully shared,\ntask-specific, or conditionally generated parameters where knowledge is\nentangled across tasks. In addition, we show how discrete skills help\ninterpretability, as they yield an explicit hierarchy of tasks.", "published": "2022-02-28 16:07:19", "link": "http://arxiv.org/abs/2202.13914v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ParaNames: A Massively Multilingual Entity Name Corpus", "abstract": "We introduce ParaNames, a multilingual parallel name resource consisting of\n118 million names spanning across 400 languages. Names are provided for 13.6\nmillion entities which are mapped to standardized entity types (PER/LOC/ORG).\nUsing Wikidata as a source, we create the largest resource of this type\nto-date. We describe our approach to filtering and standardizing the data to\nprovide the best quality possible. ParaNames is useful for multilingual\nlanguage processing, both in defining tasks for name\ntranslation/transliteration and as supplementary data for tasks such as named\nentity recognition and linking. We demonstrate an application of ParaNames by\ntraining a multilingual model for canonical name translation to and from\nEnglish. Our resource is released under a Creative Commons license (CC BY 4.0)\nat https://github.com/bltlab/paranames.", "published": "2022-02-28 18:58:06", "link": "http://arxiv.org/abs/2202.14035v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Decisions over Sequences", "abstract": "This paper introduces a class of objects called decision rules that map\ninfinite sequences of alternatives to a decision space. These objects can be\nused to model situations where a decision maker encounters alternatives in a\nsequence such as receiving recommendations. Within the class of decision rules,\nwe study natural subclasses: stopping and uniform stopping rules. Our main\nresult establishes the equivalence of these two subclasses of decision rules.\nNext, we introduce the notion of computability of decision rules using Turing\nmachines and show that computable rules can be implemented using a simpler\ncomputational device: a finite automaton. We further show that computability of\nchoice rules -- an important subclass of decision rules -- is implied by their\ncontinuity with respect to a natural topology. Finally, we introduce some\nnatural heuristics in this framework and provide their behavioral\ncharacterization.", "published": "2022-02-28 20:16:24", "link": "http://arxiv.org/abs/2203.00070v2", "categories": ["econ.TH", "cs.CL"], "primary_category": "econ.TH"}
{"title": "Structure Extraction in Task-Oriented Dialogues with Slot Clustering", "abstract": "Extracting structure information from dialogue data can help us better\nunderstand user and system behaviors. In task-oriented dialogues, dialogue\nstructure has often been considered as transition graphs among dialogue states.\nHowever, annotating dialogue states manually is expensive and time-consuming.\nIn this paper, we propose a simple yet effective approach for structure\nextraction in task-oriented dialogues. We first detect and cluster possible\nslot tokens with a pre-trained model to approximate dialogue ontology for a\ntarget domain. Then we track the status of each identified token group and\nderive a state transition structure. Empirical results show that our approach\noutperforms unsupervised baseline models by far in dialogue structure\nextraction. In addition, we show that data augmentation based on extracted\nstructures enriches the surface formats of training data and can achieve a\nsignificant performance boost in dialogue response generation.", "published": "2022-02-28 20:18:12", "link": "http://arxiv.org/abs/2203.00073v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Paper Plain: Making Medical Research Papers Approachable to Healthcare\n  Consumers with Natural Language Processing", "abstract": "When seeking information not covered in patient-friendly documents, like\nmedical pamphlets, healthcare consumers may turn to the research literature.\nReading medical papers, however, can be a challenging experience. To improve\naccess to medical papers, we introduce a novel interactive interface-Paper\nPlain-with four features powered by natural language processing: definitions of\nunfamiliar terms, in-situ plain language section summaries, a collection of key\nquestions that guide readers to answering passages, and plain language\nsummaries of the answering passages. We evaluate Paper Plain, finding that\nparticipants who use Paper Plain have an easier time reading and understanding\nresearch papers without a loss in paper comprehension compared to those who use\na typical PDF reader. Altogether, the study results suggest that guiding\nreaders to relevant passages and providing plain language summaries, or\n\"gists,\" alongside the original paper content can make reading medical papers\neasier and give readers more confidence to approach these papers.", "published": "2022-02-28 22:59:21", "link": "http://arxiv.org/abs/2203.00130v1", "categories": ["cs.HC", "cs.CL", "H.5.2"], "primary_category": "cs.HC"}
{"title": "Did AI get more negative recently?", "abstract": "In this paper, we classify scientific articles in the domain of natural\nlanguage processing (NLP) and machine learning (ML), as core subfields of\nartificial intelligence (AI), into whether (i) they extend the current\nstate-of-the-art by the introduction of novel techniques which beat existing\nmodels or whether (ii) they mainly criticize the existing state-of-the-art,\ni.e. that it is deficient with respect to some property (e.g. wrong evaluation,\nwrong datasets, misleading task specification). We refer to contributions under\n(i) as having a 'positive stance' and contributions under (ii) as having a\n'negative stance' (to related work). We annotate over 1.5 k papers from NLP and\nML to train a SciBERT-based model to automatically predict the stance of a\npaper based on its title and abstract. We then analyse large-scale trends on\nover 41 k papers from the last approximately 35 years in NLP and ML, finding\nthat papers have become substantially more positive over time, but negative\npapers also got more negative and we observe considerably more negative papers\nin recent years. Negative papers are also more influential in terms of\ncitations they receive.", "published": "2022-02-28 08:37:03", "link": "http://arxiv.org/abs/2202.13610v3", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "LISA: Learning Interpretable Skill Abstractions from Language", "abstract": "Learning policies that effectively utilize language instructions in complex,\nmulti-task environments is an important problem in sequential decision-making.\nWhile it is possible to condition on the entire language instruction directly,\nsuch an approach could suffer from generalization issues. In our work, we\npropose \\emph{Learning Interpretable Skill Abstractions (LISA)}, a hierarchical\nimitation learning framework that can learn diverse, interpretable primitive\nbehaviors or skills from language-conditioned demonstrations to better\ngeneralize to unseen instructions. LISA uses vector quantization to learn\ndiscrete skill codes that are highly correlated with language instructions and\nthe behavior of the learned policy. In navigation and robotic manipulation\nenvironments, LISA outperforms a strong non-hierarchical Decision Transformer\nbaseline in the low data regime and is able to compose learned skills to solve\ntasks containing unseen long-range instructions. Our method demonstrates a more\nnatural way to condition on language in sequential decision-making problems and\nachieve interpretable and controllable behavior with the learned skills.", "published": "2022-02-28 19:43:24", "link": "http://arxiv.org/abs/2203.00054v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.LG"}
{"title": "Logical Fallacy Detection", "abstract": "Reasoning is central to human intelligence. However, fallacious arguments are\ncommon, and some exacerbate problems such as spreading misinformation about\nclimate change. In this paper, we propose the task of logical fallacy\ndetection, and provide a new dataset (Logic) of logical fallacies generally\nfound in text, together with an additional challenge set for detecting logical\nfallacies in climate change claims (LogicClimate). Detecting logical fallacies\nis a hard problem as the model must understand the underlying logical structure\nof the argument. We find that existing pretrained large language models perform\npoorly on this task. In contrast, we show that a simple structure-aware\nclassifier outperforms the best language model by 5.46% on Logic and 4.51% on\nLogicClimate. We encourage future work to explore this task as (a) it can serve\nas a new reasoning challenge for language models, and (b) it can have potential\napplications in tackling the spread of misinformation. Our dataset and code are\navailable at https://github.com/causalNLP/logical-fallacy", "published": "2022-02-28 13:18:26", "link": "http://arxiv.org/abs/2202.13758v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Explainable deepfake and spoofing detection: an attack analysis using\n  SHapley Additive exPlanations", "abstract": "Despite several years of research in deepfake and spoofing detection for\nautomatic speaker verification, little is known about the artefacts that\nclassifiers use to distinguish between bona fide and spoofed utterances. An\nunderstanding of these is crucial to the design of trustworthy, explainable\nsolutions. In this paper we report an extension of our previous work to better\nunderstand classifier behaviour to the use of SHapley Additive exPlanations\n(SHAP) to attack analysis. Our goal is to identify the artefacts that\ncharacterise utterances generated by different attacks algorithms. Using a pair\nof classifiers which operate either upon raw waveforms or magnitude\nspectrograms, we show that visualisations of SHAP results can be used to\nidentify attack-specific artefacts and the differences and consistencies\nbetween synthetic speech and converted voice spoofing attacks.", "published": "2022-02-28 11:22:05", "link": "http://arxiv.org/abs/2202.13693v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Magnitude-aware Probabilistic Speaker Embeddings", "abstract": "Recently, hyperspherical embeddings have established themselves as a dominant\ntechnique for face and voice recognition. Specifically, Euclidean space vector\nembeddings are learned to encode person-specific information in their direction\nwhile ignoring the magnitude. However, recent studies have shown that the\nmagnitudes of the embeddings extracted by deep neural networks may indicate the\nquality of the corresponding inputs. This paper explores the properties of the\nmagnitudes of the embeddings related to quality assessment and\nout-of-distribution detection. We propose a new probabilistic speaker embedding\nextractor using the information encoded in the embedding magnitude and leverage\nit in the speaker verification pipeline. We also propose several quality-aware\ndiarization methods and incorporate the magnitudes in those. Our results\nindicate significant improvements over magnitude-agnostic baselines both in\nspeaker verification and diarization tasks.", "published": "2022-02-28 14:34:38", "link": "http://arxiv.org/abs/2202.13826v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Recent Advances and Challenges in Deep Audio-Visual Correlation Learning", "abstract": "Audio-visual correlation learning aims to capture essential correspondences\nand understand natural phenomena between audio and video. With the rapid growth\nof deep learning, an increasing amount of attention has been paid to this\nemerging research issue. Through the past few years, various methods and\ndatasets have been proposed for audio-visual correlation learning, which\nmotivate us to conclude a comprehensive survey. This survey paper focuses on\nstate-of-the-art (SOTA) models used to learn correlations between audio and\nvideo, but also discusses some tasks of definition and paradigm applied in AI\nmultimedia. In addition, we investigate some objective functions frequently\nused for optimizing audio-visual correlation learning models and discuss how\naudio-visual data is exploited in the optimization process. Most importantly,\nwe provide an extensive comparison and summarization of the recent progress of\nSOTA audio-visual correlation learning and discuss future research directions.", "published": "2022-02-28 10:43:01", "link": "http://arxiv.org/abs/2202.13673v1", "categories": ["cs.MM", "cs.CV", "cs.IR", "cs.LG", "eess.AS", "68T99"], "primary_category": "cs.MM"}
