{"title": "A Study of Syntactic Multi-Modality in Non-Autoregressive Machine\n  Translation", "abstract": "It is difficult for non-autoregressive translation (NAT) models to capture\nthe multi-modal distribution of target translations due to their conditional\nindependence assumption, which is known as the \"multi-modality problem\",\nincluding the lexical multi-modality and the syntactic multi-modality. While\nthe first one has been well studied, the syntactic multi-modality brings severe\nchallenge to the standard cross entropy (XE) loss in NAT and is under studied.\nIn this paper, we conduct a systematic study on the syntactic multi-modality\nproblem. Specifically, we decompose it into short- and long-range syntactic\nmulti-modalities and evaluate several recent NAT algorithms with advanced loss\nfunctions on both carefully designed synthesized datasets and real datasets. We\nfind that the Connectionist Temporal Classification (CTC) loss and the\nOrder-Agnostic Cross Entropy (OAXE) loss can better handle short- and\nlong-range syntactic multi-modalities respectively. Furthermore, we take the\nbest of both and design a new loss function to better handle the complicated\nsyntactic multi-modality in real-world datasets. To facilitate practical usage,\nwe provide a guide to use different loss functions for different kinds of\nsyntactic multi-modality.", "published": "2022-07-09 06:48:10", "link": "http://arxiv.org/abs/2207.04206v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Internal Language Model Estimation based Language Model Fusion for\n  Cross-Domain Code-Switching Speech Recognition", "abstract": "Internal Language Model Estimation (ILME) based language model (LM) fusion\nhas been shown significantly improved recognition results over conventional\nshallow fusion in both intra-domain and cross-domain speech recognition tasks.\nIn this paper, we attempt to apply our ILME method to cross-domain\ncode-switching speech recognition (CSSR) work. Specifically, our curiosity\ncomes from several aspects. First, we are curious about how effective the\nILME-based LM fusion is for both intra-domain and cross-domain CSSR tasks. We\nverify this with or without merging two code-switching domains. More\nimportantly, we train an end-to-end (E2E) speech recognition model by means of\nmerging two monolingual data sets and observe the efficacy of the proposed\nILME-based LM fusion for CSSR. Experimental results on SEAME that is from\nSoutheast Asian and another Chinese Mainland CS data set demonstrate the\neffectiveness of the proposed ILME-based LM fusion method.", "published": "2022-07-09 02:08:54", "link": "http://arxiv.org/abs/2207.04176v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Explaining Chest X-ray Pathologies in Natural Language", "abstract": "Most deep learning algorithms lack explanations for their predictions, which\nlimits their deployment in clinical practice. Approaches to improve\nexplainability, especially in medical imaging, have often been shown to convey\nlimited information, be overly reassuring, or lack robustness. In this work, we\nintroduce the task of generating natural language explanations (NLEs) to\njustify predictions made on medical images. NLEs are human-friendly and\ncomprehensive, and enable the training of intrinsically explainable models. To\nthis goal, we introduce MIMIC-NLE, the first, large-scale, medical imaging\ndataset with NLEs. It contains over 38,000 NLEs, which explain the presence of\nvarious thoracic pathologies and chest X-ray findings. We propose a general\napproach to solve the task and evaluate several architectures on this dataset,\nincluding via clinician assessment.", "published": "2022-07-09 22:09:37", "link": "http://arxiv.org/abs/2207.04343v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards Highly Expressive Machine Learning Models of Non-Melanoma Skin\n  Cancer", "abstract": "Pathologists have a rich vocabulary with which they can describe all the\nnuances of cellular morphology. In their world, there is a natural pairing of\nimages and words. Recent advances demonstrate that machine learning models can\nnow be trained to learn high-quality image features and represent them as\ndiscrete units of information. This enables natural language, which is also\ndiscrete, to be jointly modelled alongside the imaging, resulting in a\ndescription of the contents of the imaging. Here we present experiments in\napplying discrete modelling techniques to the problem domain of non-melanoma\nskin cancer, specifically, histological images of Intraepidermal Carcinoma\n(IEC). Implementing a VQ-GAN model to reconstruct high-resolution (256x256)\nimages of IEC images, we trained a sequence-to-sequence transformer to generate\nnatural language descriptions using pathologist terminology. Combined with the\nidea of interactive concept vectors available by using continuous generative\nmethods, we demonstrate an additional angle of interpretability. The result is\na promising means of working towards highly expressive machine learning systems\nwhich are not only useful as predictive/classification tools, but also means to\nfurther our scientific understanding of disease.", "published": "2022-07-09 04:53:25", "link": "http://arxiv.org/abs/2207.05749v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "eess.IV", "I.2.7; I.2.10"], "primary_category": "cs.LG"}
{"title": "Intermediate-layer output Regularization for Attention-based Speech\n  Recognition with Shared Decoder", "abstract": "Intermediate layer output (ILO) regularization by means of multitask training\non encoder side has been shown to be an effective approach to yielding improved\nresults on a wide range of end-to-end ASR frameworks. In this paper, we propose\na novel method to do ILO regularized training differently. Instead of using\nconventional multitask methods that entail more training overhead, we directly\nmake the intermediate layer output as input to the decoder, that is, our\ndecoder not only accepts the output of the final encoder layer as input, it\nalso takes the output of the encoder ILO as input during training. With the\nproposed method, as both encoder and decoder are simultaneously \"regularized\",\nthe network is more sufficiently trained, consistently leading to improved\nresults, over the ILO-based CTC method, as well as over the original\nattention-based modeling method without the proposed method employed.", "published": "2022-07-09 02:21:52", "link": "http://arxiv.org/abs/2207.04177v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning to Separate Voices by Spatial Regions", "abstract": "We consider the problem of audio voice separation for binaural applications,\nsuch as earphones and hearing aids. While today's neural networks perform\nremarkably well (separating $4+$ sources with 2 microphones) they assume a\nknown or fixed maximum number of sources, K. Moreover, today's models are\ntrained in a supervised manner, using training data synthesized from generic\nsources, environments, and human head shapes.\n  This paper intends to relax both these constraints at the expense of a slight\nalteration in the problem definition. We observe that, when a received mixture\ncontains too many sources, it is still helpful to separate them by region,\ni.e., isolating signal mixtures from each conical sector around the user's\nhead. This requires learning the fine-grained spatial properties of each\nregion, including the signal distortions imposed by a person's head. We propose\na two-stage self-supervised framework in which overheard voices from earphones\nare pre-processed to extract relatively clean personalized signals, which are\nthen used to train a region-wise separation model. Results show promising\nperformance, underscoring the importance of personalization over a generic\nsupervised approach. (audio samples available at our project website:\nhttps://uiuc-earable-computing.github.io/binaural/. We believe this result\ncould help real-world applications in selective hearing, noise cancellation,\nand audio augmented reality.", "published": "2022-07-09 06:25:01", "link": "http://arxiv.org/abs/2207.04203v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dual-Path Cross-Modal Attention for better Audio-Visual Speech\n  Extraction", "abstract": "Audio-visual target speech extraction, which aims to extract a certain\nspeaker's speech from the noisy mixture by looking at lip movements, has made\nsignificant progress combining time-domain speech separation models and visual\nfeature extractors (CNN). One problem of fusing audio and video information is\nthat they have different time resolutions. Most current research upsamples the\nvisual features along the time dimension so that audio and video features are\nable to align in time. However, we believe that lip movement should mostly\ncontain long-term, or phone-level information. Based on this assumption, we\npropose a new way to fuse audio-visual features. We observe that for DPRNN\n\\cite{dprnn}, the interchunk dimension's time resolution could be very close to\nthe time resolution of video frames. Like \\cite{sepformer}, the LSTM in DPRNN\nis replaced by intra-chunk and inter-chunk self-attention, but in the proposed\nalgorithm, inter-chunk attention incorporates the visual features as an\nadditional feature stream. This prevents the upsampling of visual cues,\nresulting in more efficient audio-visual fusion. The result shows we achieve\nsuperior results compared with other time-domain based audio-visual fusion\nmodels.", "published": "2022-07-09 07:27:46", "link": "http://arxiv.org/abs/2207.04213v2", "categories": ["cs.MM", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
