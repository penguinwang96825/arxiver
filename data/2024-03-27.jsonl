{"title": "ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech\n  Corpus", "abstract": "We present ZAEBUC-Spoken, a multilingual multidialectal Arabic-English speech\ncorpus. The corpus comprises twelve hours of Zoom meetings involving multiple\nspeakers role-playing a work situation where Students brainstorm ideas for a\ncertain topic and then discuss it with an Interlocutor. The meetings cover\ndifferent topics and are divided into phases with different language setups.\nThe corpus presents a challenging set for automatic speech recognition (ASR),\nincluding two languages (Arabic and English) with Arabic spoken in multiple\nvariants (Modern Standard Arabic, Gulf Arabic, and Egyptian Arabic) and English\nused with various accents. Adding to the complexity of the corpus, there is\nalso code-switching between these languages and dialects. As part of our work,\nwe take inspiration from established sets of transcription guidelines to\npresent a set of guidelines handling issues of conversational speech,\ncode-switching and orthography of both languages. We further enrich the corpus\nwith two layers of annotations; (1) dialectness level annotation for the\nportion of the corpus where mixing occurs between different variants of Arabic,\nand (2) automatic morphological annotations, including tokenization,\nlemmatization, and part-of-speech tagging.", "published": "2024-03-27 01:19:23", "link": "http://arxiv.org/abs/2403.18182v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Since the Scientific Literature Is Multilingual, Our Models Should Be\n  Too", "abstract": "English has long been assumed the $\\textit{lingua franca}$ of scientific\nresearch, and this notion is reflected in the natural language processing (NLP)\nresearch involving scientific document representation. In this position piece,\nwe quantitatively show that the literature is largely multilingual and argue\nthat current models and benchmarks should reflect this linguistic diversity. We\nprovide evidence that text-based models fail to create meaningful\nrepresentations for non-English papers and highlight the negative user-facing\nimpacts of using English-only models non-discriminately across a multilingual\ndomain. We end with suggestions for the NLP community on how to improve\nperformance on non-English documents.", "published": "2024-03-27 04:47:10", "link": "http://arxiv.org/abs/2403.18251v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Metaphor Detection through Soft Labels and Target Word\n  Prediction", "abstract": "Metaphors play a significant role in our everyday communication, yet\ndetecting them presents a challenge. Traditional methods often struggle with\nimproper application of language rules and a tendency to overlook data\nsparsity. To address these issues, we integrate knowledge distillation and\nprompt learning into metaphor detection. Our approach revolves around a\ntailored prompt learning framework specifically designed for metaphor\ndetection. By strategically masking target words and providing relevant prompt\ndata, we guide the model to accurately predict the contextual meanings of these\nwords. This approach not only mitigates confusion stemming from the literal\nmeanings of the words but also ensures effective application of language rules\nfor metaphor detection. Furthermore, we've introduced a teacher model to\ngenerate valuable soft labels. These soft labels provide a similar effect to\nlabel smoothing and help prevent the model from becoming over confident and\neffectively addresses the challenge of data sparsity. Experimental results\ndemonstrate that our model has achieved state-of-the-art performance, as\nevidenced by its remarkable results across various datasets.", "published": "2024-03-27 04:51:42", "link": "http://arxiv.org/abs/2403.18253v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BlendX: Complex Multi-Intent Detection with Blended Patterns", "abstract": "Task-oriented dialogue (TOD) systems are commonly designed with the\npresumption that each utterance represents a single intent. However, this\nassumption may not accurately reflect real-world situations, where users\nfrequently express multiple intents within a single utterance. While there is\nan emerging interest in multi-intent detection (MID), existing in-domain\ndatasets such as MixATIS and MixSNIPS have limitations in their formulation. To\naddress these issues, we present BlendX, a suite of refined datasets featuring\nmore diverse patterns than their predecessors, elevating both its complexity\nand diversity. For dataset construction, we utilize both rule-based heuristics\nas well as a generative tool -- OpenAI's ChatGPT -- which is augmented with a\nsimilarity-driven strategy for utterance selection. To ensure the quality of\nthe proposed datasets, we also introduce three novel metrics that assess the\nstatistical properties of an utterance related to word count, conjunction use,\nand pronoun usage. Extensive experiments on BlendX reveal that state-of-the-art\nMID models struggle with the challenges posed by the new datasets, highlighting\nthe need to reexamine the current state of the MID field. The dataset is\navailable at https://github.com/HYU-NLP/BlendX.", "published": "2024-03-27 06:13:04", "link": "http://arxiv.org/abs/2403.18277v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual Instruction Tuning with Large Language Models for Mathematical\n  Reasoning", "abstract": "Recent advancements highlight the success of instruction tuning with large\nlanguage models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical\nreasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as\nincorrect, missing, and redundant steps in CoT generation leading to\ninaccuracies in answer predictions. To alleviate this problem, we propose a\ndual instruction tuning strategy to meticulously model mathematical reasoning\nfrom both forward and reverse directions. This involves introducing the\nIntermediate Reasoning State Prediction task (forward reasoning) and the\nInstruction Reconstruction task (reverse reasoning) to enhance the LLMs'\nunderstanding and execution of instructions. Training instances for these tasks\nare constructed based on existing mathematical instruction tuning datasets.\nSubsequently, LLMs undergo multi-task fine-tuning using both existing\nmathematical instructions and the newly created data. Comprehensive experiments\nvalidate the effectiveness and domain generalization of the dual instruction\ntuning strategy across various mathematical reasoning tasks.", "published": "2024-03-27 06:43:58", "link": "http://arxiv.org/abs/2403.18295v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IterAlign: Iterative Constitutional Alignment of Large Language Models", "abstract": "With the rapid development of large language models (LLMs), aligning LLMs\nwith human values and societal norms to ensure their reliability and safety has\nbecome crucial. Reinforcement learning with human feedback (RLHF) and\nConstitutional AI (CAI) have been proposed for LLM alignment. However, these\nmethods require either heavy human annotations or explicitly pre-defined\nconstitutions, which are labor-intensive and resource-consuming. To overcome\nthese drawbacks, we study constitution-based LLM alignment and propose a\ndata-driven constitution discovery and self-alignment framework called\nIterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM\nand automatically discovers new constitutions using a stronger LLM. These\nconstitutions are then used to guide self-correction of the base LLM. Such a\nconstitution discovery pipeline can be run iteratively and automatically to\ndiscover new constitutions that specifically target the alignment gaps in the\ncurrent LLM. Empirical results on several safety benchmark datasets and\nmultiple base LLMs show that IterAlign successfully improves truthfulness,\nhelpfulness, harmlessness and honesty, improving the LLM alignment by up to\n$13.5\\%$ in harmlessness.", "published": "2024-03-27 08:32:19", "link": "http://arxiv.org/abs/2403.18341v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rejection Improves Reliability: Training LLMs to Refuse Unknown\n  Questions Using RL from Knowledge Feedback", "abstract": "Large Language Models (LLMs) often generate erroneous outputs, known as\nhallucinations, due to their limitations in discerning questions beyond their\nknowledge scope. While addressing hallucination has been a focal point in\nresearch, previous efforts primarily concentrate on enhancing correctness\nwithout giving due consideration to the significance of rejection mechanisms.\nIn this paper, we conduct a comprehensive examination of the role of rejection,\nintroducing the notion of model reliability along with corresponding metrics.\nThese metrics measure the model's ability to provide accurate responses while\nadeptly rejecting questions exceeding its knowledge boundaries, thereby\nminimizing hallucinations. To improve the inherent reliability of LLMs, we\npresent a novel alignment framework called Reinforcement Learning from\nKnowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically\ndetermine the model's knowledge boundary and trains a reliable reward model to\nencourage the refusal of out-of-knowledge questions. Experimental results on\nmathematical questions affirm the substantial efficacy of RLKF in significantly\nenhancing LLM reliability.", "published": "2024-03-27 08:39:56", "link": "http://arxiv.org/abs/2403.18349v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of Semantic Search and its Role in\n  Retrieved-Augmented-Generation (RAG) for Arabic Language", "abstract": "The latest advancements in machine learning and deep learning have brought\nforth the concept of semantic similarity, which has proven immensely beneficial\nin multiple applications and has largely replaced keyword search. However,\nevaluating semantic similarity and conducting searches for a specific query\nacross various documents continue to be a complicated task. This complexity is\ndue to the multifaceted nature of the task, the lack of standard benchmarks,\nwhereas these challenges are further amplified for Arabic language. This paper\nendeavors to establish a straightforward yet potent benchmark for semantic\nsearch in Arabic. Moreover, to precisely evaluate the effectiveness of these\nmetrics and the dataset, we conduct our assessment of semantic search within\nthe framework of retrieval augmented generation (RAG).", "published": "2024-03-27 08:42:31", "link": "http://arxiv.org/abs/2403.18350v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BLADE: Enhancing Black-box Large Language Models with Small\n  Domain-Specific Models", "abstract": "Large Language Models (LLMs) like ChatGPT and GPT-4 are versatile and capable\nof addressing a diverse range of tasks. However, general LLMs, which are\ndeveloped on open-domain data, may lack the domain-specific knowledge essential\nfor tasks in vertical domains, such as legal, medical, etc. To address this\nissue, previous approaches either conduct continuous pre-training with\ndomain-specific data or employ retrieval augmentation to support general LLMs.\nUnfortunately, these strategies are either cost-intensive or unreliable in\npractical applications. To this end, we present a novel framework named BLADE,\nwhich enhances Black-box LArge language models with small Domain-spEcific\nmodels. BLADE consists of a black-box LLM and a small domain-specific LM. The\nsmall LM preserves domain-specific knowledge and offers specialized insights,\nwhile the general LLM contributes robust language comprehension and reasoning\ncapabilities. Specifically, our method involves three steps: 1) pre-training\nthe small LM with domain-specific data, 2) fine-tuning this model using\nknowledge instruction data, and 3) joint Bayesian optimization of the general\nLLM and the small LM. Extensive experiments conducted on public legal and\nmedical benchmarks reveal that BLADE significantly outperforms existing\napproaches. This shows the potential of BLADE as an effective and\ncost-efficient solution in adapting general LLMs for vertical domains.", "published": "2024-03-27 08:57:21", "link": "http://arxiv.org/abs/2403.18365v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions", "abstract": "Nowadays, individuals tend to engage in dialogues with Large Language Models,\nseeking answers to their questions. In times when such answers are readily\naccessible to anyone, the stimulation and preservation of human's cognitive\nabilities, as well as the assurance of maintaining good reasoning skills by\nhumans becomes crucial. This study addresses such needs by proposing hints\n(instead of final answers or before giving answers) as a viable solution. We\nintroduce a framework for the automatic hint generation for factoid questions,\nemploying it to construct TriviaHG, a novel large-scale dataset featuring\n160,230 hints corresponding to 16,645 questions from the TriviaQA dataset.\nAdditionally, we present an automatic evaluation method that measures the\nConvergence and Familiarity quality attributes of hints. To evaluate the\nTriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals\nto annotate 2,791 hints and tasked 6 humans with answering questions using the\nprovided hints. The effectiveness of hints varied, with success rates of 96%,\n78%, and 36% for questions with easy, medium, and hard answers, respectively.\nMoreover, the proposed automatic evaluation methods showed a robust correlation\nwith annotators' results. Conclusively, the findings highlight three key\ninsights: the facilitative role of hints in resolving unknown questions, the\ndependence of hint quality on answer difficulty, and the feasibility of\nemploying automatic evaluation methods for hint assessment.", "published": "2024-03-27 10:27:28", "link": "http://arxiv.org/abs/2403.18426v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AcTED: Automatic Acquisition of Typical Event Duration for\n  Semi-supervised Temporal Commonsense QA", "abstract": "We propose a voting-driven semi-supervised approach to automatically acquire\nthe typical duration of an event and use it as pseudo-labeled data. The human\nevaluation demonstrates that our pseudo labels exhibit surprisingly high\naccuracy and balanced coverage. In the temporal commonsense QA task,\nexperimental results show that using only pseudo examples of 400 events, we\nachieve performance comparable to the existing BERT-based weakly supervised\napproaches that require a significant amount of training examples. When\ncompared to the RoBERTa baselines, our best approach establishes\nstate-of-the-art performance with a 7% improvement in Exact Match.", "published": "2024-03-27 12:33:42", "link": "http://arxiv.org/abs/2403.18504v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Debiasing Sentence Embedders through Contrastive Word Pairs", "abstract": "Over the last years, various sentence embedders have been an integral part in\nthe success of current machine learning approaches to Natural Language\nProcessing (NLP). Unfortunately, multiple sources have shown that the bias,\ninherent in the datasets upon which these embedding methods are trained, is\nlearned by them. A variety of different approaches to remove biases in\nembeddings exists in the literature. Most of these approaches are applicable to\nword embeddings and in fewer cases to sentence embeddings. It is problematic\nthat most debiasing approaches are directly transferred from word embeddings,\ntherefore these approaches fail to take into account the nonlinear nature of\nsentence embedders and the embeddings they produce. It has been shown in\nliterature that bias information is still present if sentence embeddings are\ndebiased using such methods. In this contribution, we explore an approach to\nremove linear and nonlinear bias information for NLP solutions, without\nimpacting downstream performance. We compare our approach to common debiasing\nmethods on classical bias metrics and on bias metrics which take nonlinear\ninformation into account.", "published": "2024-03-27 13:34:59", "link": "http://arxiv.org/abs/2403.18555v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SDSAT: Accelerating LLM Inference through Speculative Decoding with\n  Semantic Adaptive Tokens", "abstract": "We propose an acceleration scheme for large language models (LLMs) through\nSpeculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary\nobjective of this design is to enhance the LLM model's ability to generate\ndraft tokens more accurately without compromising the model's accuracy. The\ncore strategies involve: 1) Fine-tune the model by incorporating semantic\nadaptive tokens that possess flexible decoding capabilities without changing\nits structure, allowing them to generate high-quality draft tokens. 2) By\nemploying a training method that does not affect the standard tokens, the model\ncan acquire parallel decoding abilities atop its original framework with\nminimal training overhead. 3) We have designed the \"two-step-draft-then-verify\"\ngeneration strategies using both greedy search and nucleus sampling.\nExperiments conducted on the CodeLlama-13B and 7B models have yielded speed\nincreases of over 3.5X and 3.0X, respectively. Please refer to\nhttps://github.com/hasuoshenyun/SDSAT.", "published": "2024-03-27 14:54:27", "link": "http://arxiv.org/abs/2403.18647v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Invalsi Benchmarks: measuring Linguistic and Mathematical\n  understanding of Large Language Models in Italian", "abstract": "While Italian is a high-resource language, there are few Italian-native\nbenchmarks to evaluate generative Large Language Models (LLMs) in this\nlanguage. This work presents three new benchmarks: Invalsi MATE to evaluate\nmodels performance on mathematical understanding in Italian, Invalsi ITA to\nevaluate language understanding in Italian and Olimpiadi MATE for more complex\nmathematical understanding.\n  The first two benchmarks are based on the Invalsi tests, which are\nadministered to students of age between 6 and 18 within the Italian school\nsystem and have been validated by several experts in teaching and pedagogy, the\nthird one comes from the Italian high school math Olympics.\n  We evaluate 10 powerful language models on these benchmarks and find that\nthey are bound by 71% accuracy on Invasli MATE, achieved by Llama 3.1 70b\ninstruct and by 88% on Invalsi ITA. For both Invalsi MATE and Invalsi ITA we\ncompare LLMs with the average performance of Italian students to show that\nLlama 3.1 is the only one to outperform them on Invalsi MATE while most models\ndo so on Invalsi ITA, we then show that Olimpiadi MATE is more challenging than\nInvalsi MATE and the highest accuracy, achieved by Llama 3.1 405b instruct is\n45%.\n  We will make data and evaluation code openly available upon acceptance of the\npaper.", "published": "2024-03-27 15:46:25", "link": "http://arxiv.org/abs/2403.18697v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Neural Protoform Reconstruction via Reflex Prediction", "abstract": "Protolanguage reconstruction is central to historical linguistics. The\ncomparative method, one of the most influential theoretical and methodological\nframeworks in the history of the language sciences, allows linguists to infer\nprotoforms (reconstructed ancestral words) from their reflexes (related modern\nwords) based on the assumption of regular sound change. Not surprisingly,\nnumerous computational linguists have attempted to operationalize comparative\nreconstruction through various computational models, the most successful of\nwhich have been supervised encoder-decoder models, which treat the problem of\npredicting protoforms given sets of reflexes as a sequence-to-sequence problem.\nWe argue that this framework ignores one of the most important aspects of the\ncomparative method: not only should protoforms be inferable from cognate sets\n(sets of related reflexes) but the reflexes should also be inferable from the\nprotoforms. Leveraging another line of research -- reflex prediction -- we\npropose a system in which candidate protoforms from a reconstruction model are\nreranked by a reflex prediction model. We show that this more complete\nimplementation of the comparative method allows us to surpass state-of-the-art\nprotoform reconstruction methods on three of four Chinese and Romance datasets.", "published": "2024-03-27 17:13:38", "link": "http://arxiv.org/abs/2403.18769v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CheckEval: A reliable LLM-as-a-Judge framework for evaluating text\n  generation using checklists", "abstract": "Existing LLM-as-a-Judge approaches for evaluating text generation suffer from\nrating inconsistencies, with low agreement and high rating variance across\ndifferent evaluator models. We attribute this to subjective evaluation criteria\ncombined with Likert scale scoring in existing protocols. To address this\nissue, we introduce CheckEval, a checklist-based evaluation framework that\nimproves rating reliability via decomposed binary questions. Through\nexperiments with 12 evaluator models across multiple datasets, we first\ndemonstrate that CheckEval strongly correlates with human judgments, improving\nthe average correlation with human judgments by 0.10. More importantly,\nCheckEval dramatically improves the average agreement across evaluator models\nby 0.45 and reduces the score variance. CheckEval scores furthermore have the\nbenefit of being more interpretable because it decomposes evaluation criteria\ninto traceable binary decisions, allowing analyses of specific attributes\ndriving quality judgments.", "published": "2024-03-27 17:20:39", "link": "http://arxiv.org/abs/2403.18771v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a World-English Language Model for On-Device Virtual Assistants", "abstract": "Neural Network Language Models (NNLMs) for Virtual Assistants (VAs) are\ngenerally language-, region-, and in some cases, device-dependent, which\nincreases the effort to scale and maintain them. Combining NNLMs for one or\nmore of the categories is one way to improve scalability. In this work, we\ncombine regional variants of English to build a ``World English'' NNLM for\non-device VAs. In particular, we investigate the application of adapter\nbottlenecks to model dialect-specific characteristics in our existing\nproduction NNLMs {and enhance the multi-dialect baselines}. We find that\nadapter modules are more effective in modeling dialects than specializing\nentire sub-networks. Based on this insight and leveraging the design of our\nproduction models, we introduce a new architecture for World English NNLM that\nmeets the accuracy, latency, and memory constraints of our single-dialect\nmodels.", "published": "2024-03-27 17:31:39", "link": "http://arxiv.org/abs/2403.18783v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Projective Methods for Mitigating Gender Bias in Pre-trained Language\n  Models", "abstract": "Mitigation of gender bias in NLP has a long history tied to debiasing static\nword embeddings. More recently, attention has shifted to debiasing pre-trained\nlanguage models. We study to what extent the simplest projective debiasing\nmethods, developed for word embeddings, can help when applied to BERT's\ninternal representations. Projective methods are fast to implement, use a small\nnumber of saved parameters, and make no updates to the existing model\nparameters. We evaluate the efficacy of the methods in reducing both intrinsic\nbias, as measured by BERT's next sentence prediction task, and in mitigating\nobserved bias in a downstream setting when fine-tuned. To this end, we also\nprovide a critical analysis of a popular gender-bias assessment test for\nquantifying intrinsic bias, resulting in an enhanced test set and new bias\nmeasures. We find that projective methods can be effective at both intrinsic\nbias and downstream bias mitigation, but that the two outcomes are not\nnecessarily correlated. This finding serves as a warning that intrinsic bias\ntest sets, based either on language modeling tasks or next sentence prediction,\nshould not be the only benchmark in developing a debiased language model.", "published": "2024-03-27 17:49:31", "link": "http://arxiv.org/abs/2403.18803v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Modularity Transferable? A Case Study through the Lens of Knowledge\n  Distillation", "abstract": "The rise of Modular Deep Learning showcases its potential in various Natural\nLanguage Processing applications. Parameter-efficient fine-tuning (PEFT)\nmodularity has been shown to work for various use cases, from domain adaptation\nto multilingual setups. However, all this work covers the case where the\nmodular components are trained and deployed within one single Pre-trained\nLanguage Model (PLM). This model-specific setup is a substantial limitation on\nthe very modularity that modular architectures are trying to achieve. We ask\nwhether current modular approaches are transferable between models and whether\nwe can transfer the modules from more robust and larger PLMs to smaller ones.\nIn this work, we aim to fill this gap via a lens of Knowledge Distillation,\ncommonly used for model compression, and present an extremely straightforward\napproach to transferring pre-trained, task-specific PEFT modules between\nsame-family PLMs. Moreover, we propose a method that allows the transfer of\nmodules between incompatible PLMs without any change in the inference\ncomplexity. The experiments on Named Entity Recognition, Natural Language\nInference, and Paraphrase Identification tasks over multiple languages and PEFT\nmethods showcase the initial potential of transferable modularity.", "published": "2024-03-27 17:50:00", "link": "http://arxiv.org/abs/2403.18804v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SemEval-2024 Task 1: Semantic Textual Relatedness for African and Asian\n  Languages", "abstract": "We present the first shared task on Semantic Textual Relatedness (STR). While\nearlier shared tasks primarily focused on semantic similarity, we instead\ninvestigate the broader phenomenon of semantic relatedness across 14 languages:\nAfrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian,\nKinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi,\nSpanish, and Telugu. These languages originate from five distinct language\nfamilies and are predominantly spoken in Africa and Asia -- regions\ncharacterised by the relatively limited availability of NLP resources. Each\ninstance in the datasets is a sentence pair associated with a score that\nrepresents the degree of semantic textual relatedness between the two\nsentences. Participating systems were asked to rank sentence pairs by their\ncloseness in meaning (i.e., their degree of semantic relatedness) in the 14\nlanguages in three main tracks: (a) supervised, (b) unsupervised, and (c)\ncrosslingual. The task attracted 163 participants. We received 70 submissions\nin total (across all tasks) from 51 different teams, and 38 system description\npapers. We report on the best-performing systems as well as the most common and\nthe most effective approaches for the three different tracks.", "published": "2024-03-27 18:30:26", "link": "http://arxiv.org/abs/2403.18933v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conformal Intent Classification and Clarification for Fast and Accurate\n  Intent Recognition", "abstract": "We present Conformal Intent Classification and Clarification (CICC), a\nframework for fast and accurate intent classification for task-oriented\ndialogue systems. The framework turns heuristic uncertainty scores of any\nintent classifier into a clarification question that is guaranteed to contain\nthe true intent at a pre-defined confidence level. By disambiguating between a\nsmall number of likely intents, the user query can be resolved quickly and\naccurately. Additionally, we propose to augment the framework for out-of-scope\ndetection. In a comparative evaluation using seven intent recognition datasets\nwe find that CICC generates small clarification questions and is capable of\nout-of-scope detection. CICC can help practitioners and researchers\nsubstantially in improving the user experience of dialogue agents with specific\nclarification questions.", "published": "2024-03-27 19:42:01", "link": "http://arxiv.org/abs/2403.18973v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Corpus of Annotated Medical Imaging Reports and Information\n  Extraction Results Using BERT-based Language Models", "abstract": "Medical imaging is critical to the diagnosis, surveillance, and treatment of\nmany health conditions, including oncological, neurological, cardiovascular,\nand musculoskeletal disorders, among others. Radiologists interpret these\ncomplex, unstructured images and articulate their assessments through narrative\nreports that remain largely unstructured. This unstructured narrative must be\nconverted into a structured semantic representation to facilitate secondary\napplications such as retrospective analyses or clinical decision support. Here,\nwe introduce the Corpus of Annotated Medical Imaging Reports (CAMIR), which\nincludes 609 annotated radiology reports from three imaging modality types:\nComputed Tomography, Magnetic Resonance Imaging, and Positron Emission\nTomography-Computed Tomography. Reports were annotated using an event-based\nschema that captures clinical indications, lesions, and medical problems. Each\nevent consists of a trigger and multiple arguments, and a majority of the\nargument types, including anatomy, normalize the spans to pre-defined concepts\nto facilitate secondary use. CAMIR uniquely combines a granular event structure\nand concept normalization. To extract CAMIR events, we explored two BERT\n(Bi-directional Encoder Representation from Transformers)-based architectures,\nincluding an existing architecture (mSpERT) that jointly extracts all event\ninformation and a multi-step approach (PL-Marker++) that we augmented for the\nCAMIR schema.", "published": "2024-03-27 19:43:45", "link": "http://arxiv.org/abs/2403.18975v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CAUSE: Counterfactual Assessment of User Satisfaction Estimation in\n  Task-Oriented Dialogue Systems", "abstract": "An important unexplored aspect in previous work on user satisfaction\nestimation for Task-Oriented Dialogue (TOD) systems is their evaluation in\nterms of robustness for the identification of user dissatisfaction: current\nbenchmarks for user satisfaction estimation in TOD systems are highly skewed\ntowards dialogues for which the user is satisfied. The effect of having a more\nbalanced set of satisfaction labels on performance is unknown. However,\nbalancing the data with more dissatisfactory dialogue samples requires further\ndata collection and human annotation, which is costly and time-consuming. In\nthis work, we leverage large language models (LLMs) and unlock their ability to\ngenerate satisfaction-aware counterfactual dialogues to augment the set of\noriginal dialogues of a test collection. We gather human annotations to ensure\nthe reliability of the generated samples. We evaluate two open-source LLMs as\nuser satisfaction estimators on our augmented collection against\nstate-of-the-art fine-tuned models. Our experiments show that when used as\nfew-shot user satisfaction estimators, open-source LLMs show higher robustness\nto the increase in the number of dissatisfaction labels in the test collection\nthan the fine-tuned state-of-the-art models. Our results shed light on the need\nfor data augmentation approaches for user satisfaction estimation in TOD\nsystems. We release our aligned counterfactual dialogues, which are curated by\nhuman annotation, to facilitate further research on this topic.", "published": "2024-03-27 23:45:31", "link": "http://arxiv.org/abs/2403.19056v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Comparison of Translationese in Machine Translation and Human\n  Transation in terms of Translation Relations", "abstract": "This study explores the distinctions between neural machine translation (NMT)\nand human translation (HT) through the lens of translation relations. It\nbenchmarks HT to assess the translation techniques produced by an NMT system\nand aims to address three key research questions: the differences in overall\ntranslation relations between NMT and HT, how each utilizes non-literal\ntranslation techniques, and the variations in factors influencing their use of\nspecific non-literal techniques. The research employs two parallel corpora,\neach spanning nine genres with the same source texts with one translated by NMT\nand the other by humans. Translation relations in these corpora are manually\nannotated on aligned pairs, enabling a comparative analysis that draws on\nlinguistic insights, including semantic and syntactic nuances such as hypernyms\nand alterations in part-of-speech tagging. The results indicate that NMT relies\non literal translation significantly more than HT across genres. While NMT\nperforms comparably to HT in employing syntactic non-literal translation\ntechniques, it falls behind in semantic-level performance.", "published": "2024-03-27 19:12:20", "link": "http://arxiv.org/abs/2404.08661v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mechanistic Understanding and Mitigation of Language Model Non-Factual\n  Hallucinations", "abstract": "State-of-the-art language models (LMs) sometimes generate non-factual\nhallucinations that misalign with world knowledge. To explore the mechanistic\ncauses of these hallucinations, we create diagnostic datasets with\nsubject-relation queries and adapt interpretability methods to trace\nhallucinations through internal model representations. We discover two general\nand distinct mechanistic causes of hallucinations shared across LMs (Llama-2,\nPythia, GPT-J): 1) knowledge enrichment hallucinations: insufficient subject\nattribute knowledge in lower layer MLPs, and 2) answer extraction\nhallucinations: failure to select the correct object attribute in upper layer\nattention heads. We also found these two internal mechanistic causes of\nhallucinations are reflected in external manifestations. Based on insights from\nour mechanistic analysis, we propose a novel hallucination mitigation method\nthrough targeted restoration of the LM's internal fact recall pipeline,\ndemonstrating superior performance compared to baselines.", "published": "2024-03-27 00:23:03", "link": "http://arxiv.org/abs/2403.18167v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the Deceptive Power of LLM-Generated Fake News: A Study of\n  Real-World Detection Challenges", "abstract": "Recent advancements in Large Language Models (LLMs) have enabled the creation\nof fake news, particularly in complex fields like healthcare. Studies highlight\nthe gap in the deceptive power of LLM-generated fake news with and without\nhuman assistance, yet the potential of prompting techniques has not been fully\nexplored. Thus, this work aims to determine whether prompting strategies can\neffectively narrow this gap. Current LLM-based fake news attacks require human\nintervention for information gathering and often miss details and fail to\nmaintain context consistency. Therefore, to better understand threat tactics,\nwe propose a strong fake news attack method called conditional\nVariational-autoencoder-Like Prompt (VLPrompt). Unlike current methods,\nVLPrompt eliminates the need for additional data collection while maintaining\ncontextual coherence and preserving the intricacies of the original text. To\npropel future research on detecting VLPrompt attacks, we created a new dataset\nnamed VLPrompt fake news (VLPFN) containing real and fake texts. Our\nexperiments, including various detection methods and novel human study metrics,\nwere conducted to assess their performance on our dataset, yielding numerous\nfindings.", "published": "2024-03-27 04:39:18", "link": "http://arxiv.org/abs/2403.18249v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Toward Interactive Regional Understanding in Vision-Large Language\n  Models", "abstract": "Recent Vision-Language Pre-training (VLP) models have demonstrated\nsignificant advancements. Nevertheless, these models heavily rely on image-text\npairs that capture only coarse and global information of an image, leading to a\nlimitation in their regional understanding ability. In this work, we introduce\n\\textbf{RegionVLM}, equipped with explicit regional modeling capabilities,\nallowing them to understand user-indicated image regions. To achieve this, we\ndesign a simple yet innovative architecture, requiring no modifications to the\nmodel architecture or objective function. Additionally, we leverage a dataset\nthat contains a novel source of information, namely Localized Narratives, which\nhas been overlooked in previous VLP research. Our experiments demonstrate that\nour single generalist model not only achieves an interactive dialogue system\nbut also exhibits superior performance on various zero-shot region\nunderstanding tasks, without compromising its ability for global image\nunderstanding.", "published": "2024-03-27 05:22:06", "link": "http://arxiv.org/abs/2403.18260v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "RankMamba: Benchmarking Mamba's Document Ranking Performance in the Era\n  of Transformers", "abstract": "Transformer structure has achieved great success in multiple applied machine\nlearning communities, such as natural language processing (NLP), computer\nvision (CV) and information retrieval (IR). Transformer architecture's core\nmechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$\ntime complexity in inference. Many works have been proposed to improve the\nattention mechanism's scalability, such as Flash Attention and Multi-query\nAttention. A different line of work aims to design new mechanisms to replace\nattention. Recently, a notable model structure -- Mamba, which is based on\nstate space models, has achieved transformer-equivalent performance in multiple\nsequence modeling tasks.\n  In this work, we examine \\mamba's efficacy through the lens of a classical IR\ntask -- document ranking. A reranker model takes a query and a document as\ninput, and predicts a scalar relevance score. This task demands the language\nmodel's ability to comprehend lengthy contextual inputs and to capture the\ninteraction between query and document tokens. We find that (1) Mamba models\nachieve competitive performance compared to transformer-based models with the\nsame training recipe; (2) but also have a lower training throughput in\ncomparison to efficient transformer implementations such as flash attention. We\nhope this study can serve as a starting point to explore Mamba models in other\nclassical IR tasks. Our code implementation and trained checkpoints are made\npublic to facilitate reproducibility\n(https://github.com/zhichaoxu-shufe/RankMamba).", "published": "2024-03-27 06:07:05", "link": "http://arxiv.org/abs/2403.18276v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Chinese Offensive Language Detection:Current Status and Future\n  Directions", "abstract": "Despite the considerable efforts being made to monitor and regulate\nuser-generated content on social media platforms, the pervasiveness of\noffensive language, such as hate speech or cyberbullying, in the digital space\nremains a significant challenge. Given the importance of maintaining a\ncivilized and respectful online environment, there is an urgent and growing\nneed for automatic systems capable of detecting offensive speech in real time.\nHowever, developing effective systems for processing languages such as Chinese\npresents a significant challenge, owing to the language's complex and nuanced\nnature, which makes it difficult to process automatically. This paper provides\na comprehensive overview of offensive language detection in Chinese, examining\ncurrent benchmarks and approaches and highlighting specific models and tools\nfor addressing the unique challenges of detecting offensive language in this\ncomplex language. The primary objective of this survey is to explore the\nexisting techniques and identify potential avenues for further research that\ncan address the cultural and linguistic complexities of Chinese.", "published": "2024-03-27 07:34:44", "link": "http://arxiv.org/abs/2403.18314v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "$\\forall$uto$\\exists$val: Autonomous Assessment of LLMs in Formal\n  Synthesis and Interpretation Tasks", "abstract": "This paper presents $\\forall$uto$\\exists$val, a new approach for scaling LLM\nassessment in translating formal syntax -- such as first-order logic, regular\nexpressions, etc -- to natural language (interpretation) or vice versa\n(compilation), thereby facilitating their use in applications such as\ngenerating/explaining logic and control flow for programs etc. Existing\napproaches for LLM assessment in these areas require labor-intensive\nground-truth creation, the availability of which undermines the separation of\ntraining and test sets. Furthermore, such datasets typically include relatively\nfew hand-coded test cases over which LLM accuracy is determined, thus making\nthem inadequate for determining the safety or correctness of their generated\noutputs. We introduce a new approach that utilizes context-free grammars (CFGs)\nto generate out-of-distribution datasets on the fly and perform closed-loop\ntesting of LLM capabilities using formal verifiers to guarantee the correctness\nof LLM outputs without any human intervention. We release our dataset and\nbenchmark as open-source code at\n\\url{https://github.com/AAIR-lab/auto-llm-assessment}. We also conduct an\nassessment of several SOTA closed and open-source LLMs to showcase the\nfeasibility and scalability of this paradigm. Our experiments reveal that SOTA\nLLMs are unable to solve the formal translation task adequately.", "published": "2024-03-27 08:08:00", "link": "http://arxiv.org/abs/2403.18327v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Dataset for Pharmacovigilance in German, French, and Japanese:\n  Annotating Adverse Drug Reactions across Languages", "abstract": "User-generated data sources have gained significance in uncovering Adverse\nDrug Reactions (ADRs), with an increasing number of discussions occurring in\nthe digital world. However, the existing clinical corpora predominantly revolve\naround scientific articles in English. This work presents a multilingual corpus\nof texts concerning ADRs gathered from diverse sources, including patient fora,\nsocial media, and clinical reports in German, French, and Japanese. Our corpus\ncontains annotations covering 12 entity types, four attribute types, and 13\nrelation types. It contributes to the development of real-world multilingual\nlanguage models for healthcare. We provide statistics to highlight certain\nchallenges associated with the corpus and conduct preliminary experiments\nresulting in strong baselines for extracting entities and relations between\nthese entities, both within and across languages.", "published": "2024-03-27 08:21:01", "link": "http://arxiv.org/abs/2403.18336v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quantifying and Mitigating Unimodal Biases in Multimodal Large Language\n  Models: A Causal Perspective", "abstract": "Recent advancements in Large Language Models (LLMs) have facilitated the\ndevelopment of Multimodal LLMs (MLLMs). Despite their impressive capabilities,\nMLLMs often suffer from over-reliance on unimodal biases (e.g., language bias\nand vision bias), leading to incorrect answers or hallucinations in complex\nmultimodal tasks. To investigate this issue, we propose a causal framework to\ninterpret the biases in Visual Question Answering (VQA) problems. Within this\nframework, we conduct an in-depth causal analysis to assess the causal effect\nof these biases on MLLM predictions. Based on the analysis, we introduce 1) a\nnovel MORE dataset with 12,000 challenging VQA instances requiring multi-hop\nreasoning and overcoming unimodal biases. 2) a causality-enhanced agent\nframework CAVE that guides models to comprehensively integrate information from\ndifferent modalities and mitigate biases. Our experiments show that MLLMs\nperform poorly on MORE, indicating strong unimodal biases and limited semantic\nunderstanding. However, when integrated with our CAVE, promising improvements\nin reasoning and bias mitigation can be seen. These findings provide important\ninsights for the development of more robust MLLMs and contribute to the broader\ngoal of advancing multimodal AI systems capable of deeper understanding and\nreasoning. Our project page is at https://github.com/OpenCausaLab/MORE.", "published": "2024-03-27 08:38:49", "link": "http://arxiv.org/abs/2403.18346v4", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Improving Attributed Text Generation of Large Language Models via\n  Preference Learning", "abstract": "Large language models have been widely adopted in natural language\nprocessing, yet they face the challenge of generating unreliable content.\nRecent works aim to reduce misinformation and hallucinations by resorting to\nattribution as a means to provide evidence (i.e., citations). However, current\nattribution methods usually focus on the retrieval stage and automatic\nevaluation that neglect mirroring the citation mechanisms in human scholarly\nwriting to bolster credibility. In this paper, we address these challenges by\nmodelling the attribution task as preference learning and introducing an\nAutomatic Preference Optimization (APO) framework. First, we create a curated\ncollection for post-training with 6,330 examples by collecting and filtering\nfrom existing datasets. Second, considering the high cost of labelling\npreference data, we further propose an automatic method to synthesize\nattribution preference data resulting in 95,263 pairs. Moreover, inspired by\nthe human citation process, we further propose a progressive preference\noptimization method by leveraging fine-grained information. Extensive\nexperiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate\nthat APO achieves state-of-the-art citation F1 with higher answer quality.", "published": "2024-03-27 09:19:13", "link": "http://arxiv.org/abs/2403.18381v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text", "abstract": "Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance\non a wide variety of biomedical NLP tasks. However, these models have hundreds\nof billions of parameters, are computationally expensive to run, require users\nto send their input data over the internet, and are trained on unknown data\nsources. Can smaller, more targeted models compete? To address this question,\nwe build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive\nmodel trained exclusively on PubMed abstracts and full articles. When\nfine-tuned, BioMedLM can produce strong multiple-choice biomedical\nquestion-answering results competitive with much larger models, such as\nachieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical\nGenetics exam. BioMedLM can also be fine-tuned to produce useful answers to\npatient questions on medical topics. This demonstrates that smaller models can\npotentially serve as transparent, privacy-preserving, economical and\nenvironmentally friendly foundations for particular NLP applications, such as\nin biomedicine. The model is available on the Hugging Face Hub:\nhttps://huggingface.co/stanford-crfm/BioMedLM.", "published": "2024-03-27 10:18:21", "link": "http://arxiv.org/abs/2403.18421v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SemRoDe: Macro Adversarial Training to Learn Representations That are\n  Robust to Word-Level Attacks", "abstract": "Language models (LMs) are indispensable tools for natural language processing\ntasks, but their vulnerability to adversarial attacks remains a concern. While\ncurrent research has explored adversarial training techniques, their\nimprovements to defend against word-level attacks have been limited. In this\nwork, we propose a novel approach called Semantic Robust Defence (SemRoDe), a\nMacro Adversarial Training strategy to enhance the robustness of LMs. Drawing\ninspiration from recent studies in the image domain, we investigate and later\nconfirm that in a discrete data setting such as language, adversarial samples\ngenerated via word substitutions do indeed belong to an adversarial domain\nexhibiting a high Wasserstein distance from the base domain. Our method learns\na robust representation that bridges these two domains. We hypothesize that if\nsamples were not projected into an adversarial domain, but instead to a domain\nwith minimal shift, it would improve attack robustness. We align the domains by\nincorporating a new distance-based objective. With this, our model is able to\nlearn more generalized representations by aligning the model's high-level\noutput features and therefore better handling unseen adversarial samples. This\nmethod can be generalized across word embeddings, even when they share minimal\noverlap at both vocabulary and word-substitution levels. To evaluate the\neffectiveness of our approach, we conduct experiments on BERT and RoBERTa\nmodels on three datasets. The results demonstrate promising state-of-the-art\nrobustness.", "published": "2024-03-27 10:24:25", "link": "http://arxiv.org/abs/2403.18423v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DELTA: Pre-train a Discriminative Encoder for Legal Case Retrieval via\n  Structural Word Alignment", "abstract": "Recent research demonstrates the effectiveness of using pre-trained language\nmodels for legal case retrieval. Most of the existing works focus on improving\nthe representation ability for the contextualized embedding of the [CLS] token\nand calculate relevance using textual semantic similarity. However, in the\nlegal domain, textual semantic similarity does not always imply that the cases\nare relevant enough. Instead, relevance in legal cases primarily depends on the\nsimilarity of key facts that impact the final judgment. Without proper\ntreatments, the discriminative ability of learned representations could be\nlimited since legal cases are lengthy and contain numerous non-key facts. To\nthis end, we introduce DELTA, a discriminative model designed for legal case\nretrieval. The basic idea involves pinpointing key facts in legal cases and\npulling the contextualized embedding of the [CLS] token closer to the key facts\nwhile pushing away from the non-key facts, which can warm up the case embedding\nspace in an unsupervised manner. To be specific, this study brings the word\nalignment mechanism to the contextual masked auto-encoder. First, we leverage\nshallow decoders to create information bottlenecks, aiming to enhance the\nrepresentation ability. Second, we employ the deep decoder to enable\ntranslation between different structures, with the goal of pinpointing key\nfacts to enhance discriminative ability. Comprehensive experiments conducted on\npublicly available legal benchmarks show that our approach can outperform\nexisting state-of-the-art methods in legal case retrieval. It provides a new\nperspective on the in-depth understanding and processing of legal case\ndocuments.", "published": "2024-03-27 10:40:14", "link": "http://arxiv.org/abs/2403.18435v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Attention-aware semantic relevance predicting Chinese sentence reading", "abstract": "In recent years, several influential computational models and metrics have\nbeen proposed to predict how humans comprehend and process sentence. One\nparticularly promising approach is contextual semantic similarity. Inspired by\nthe attention algorithm in Transformer and human memory mechanisms, this study\nproposes an ``attention-aware'' approach for computing contextual semantic\nrelevance. This new approach takes into account the different contributions of\ncontextual parts and the expectation effect, allowing it to incorporate\ncontextual information fully. The attention-aware approach also facilitates the\nsimulation of existing reading models and evaluate them. The resulting\n``attention-aware'' metrics of semantic relevance can more accurately predict\nfixation durations in Chinese reading tasks recorded in an eye-tracking corpus\nthan those calculated by existing approaches. The study's findings further\nprovide strong support for the presence of semantic preview benefits in Chinese\nnaturalistic reading. Furthermore, the attention-aware metrics of semantic\nrelevance, being memory-based, possess high interpretability from both\nlinguistic and cognitive standpoints, making them a valuable computational tool\nfor modeling eye-movements in reading and further gaining insight into the\nprocess of language comprehension. Our approach underscores the potential of\nthese metrics to advance our comprehension of how humans understand and process\nlanguage, ultimately leading to a better understanding of language\ncomprehension and processing.", "published": "2024-03-27 13:22:38", "link": "http://arxiv.org/abs/2403.18542v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A survey on learning models of spiking neural membrane systems and\n  spiking neural networks", "abstract": "Spiking neural networks (SNN) are a biologically inspired model of neural\nnetworks with certain brain-like properties. In the past few decades, this\nmodel has received increasing attention in computer science community, owing\nalso to the successful phenomenon of deep learning. In SNN, communication\nbetween neurons takes place through the spikes and spike trains. This\ndifferentiates these models from the ``standard'' artificial neural networks\n(ANN) where the frequency of spikes is replaced by real-valued signals. Spiking\nneural P systems (SNPS) can be considered a branch of SNN based more on the\nprinciples of formal automata, with many variants developed within the\nframework of the membrane computing theory. In this paper, we first briefly\ncompare structure and function, advantages and drawbacks of SNN and SNPS. A key\npart of the article is a survey of recent results and applications of machine\nlearning and deep learning models of both SNN and SNPS formalisms.", "published": "2024-03-27 14:26:41", "link": "http://arxiv.org/abs/2403.18609v1", "categories": ["cs.NE", "cs.CL"], "primary_category": "cs.NE"}
{"title": "Vulnerability Detection with Code Language Models: How Far Are We?", "abstract": "In the context of the rising interest in code language models (code LMs) and\nvulnerability detection, we study the effectiveness of code LMs for detecting\nvulnerabilities. Our analysis reveals significant shortcomings in existing\nvulnerability datasets, including poor data quality, low label accuracy, and\nhigh duplication rates, leading to unreliable model performance in realistic\nvulnerability detection scenarios. Additionally, the evaluation methods used\nwith these datasets are not representative of real-world vulnerability\ndetection.\n  To address these challenges, we introduce PrimeVul, a new dataset for\ntraining and evaluating code LMs for vulnerability detection. PrimeVul\nincorporates a novel set of data labeling techniques that achieve comparable\nlabel accuracy to human-verified benchmarks while significantly expanding the\ndataset. It also implements a rigorous data de-duplication and chronological\ndata splitting strategy to mitigate data leakage issues, alongside introducing\nmore realistic evaluation metrics and settings. This comprehensive approach\naims to provide a more accurate assessment of code LMs' performance in\nreal-world conditions.\n  Evaluating code LMs on PrimeVul reveals that existing benchmarks\nsignificantly overestimate the performance of these models. For instance, a\nstate-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on\nPrimeVul. Attempts to improve performance through advanced training techniques\nand larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin\nto random guessing in the most stringent settings. These findings underscore\nthe considerable gap between current capabilities and the practical\nrequirements for deploying code LMs in security roles, highlighting the need\nfor more innovative research in this domain.", "published": "2024-03-27 14:34:29", "link": "http://arxiv.org/abs/2403.18624v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Improving Content Recommendation: Knowledge Graph-Based Semantic\n  Contrastive Learning for Diversity and Cold-Start Users", "abstract": "Addressing the challenges related to data sparsity, cold-start problems, and\ndiversity in recommendation systems is both crucial and demanding. Many current\nsolutions leverage knowledge graphs to tackle these issues by combining both\nitem-based and user-item collaborative signals. A common trend in these\napproaches focuses on improving ranking performance at the cost of escalating\nmodel complexity, reducing diversity, and complicating the task. It is\nessential to provide recommendations that are both personalized and diverse,\nrather than solely relying on achieving high rank-based performance, such as\nClick-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task\nlearning approach, training on user-item and item-item interactions. We apply\nitem-based contrastive learning on descriptive text, sampling positive and\nnegative pairs based on item metadata. Our approach allows the model to better\nunderstand the relationships between entities within the knowledge graph by\nutilizing semantic information from text. It leads to more accurate, relevant,\nand diverse user recommendations and a benefit that extends even to cold-start\nusers who have few interactions with items. We perform extensive experiments on\ntwo widely used datasets to validate the effectiveness of our approach. Our\nfindings demonstrate that jointly training user-item interactions and\nitem-based signals using synopsis text is highly effective. Furthermore, our\nresults provide evidence that item-based contrastive learning enhances the\nquality of entity embeddings, as indicated by metrics such as uniformity and\nalignment.", "published": "2024-03-27 15:11:00", "link": "http://arxiv.org/abs/2403.18667v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Fact Checking Beyond Training Set", "abstract": "Evaluating the veracity of everyday claims is time consuming and in some\ncases requires domain expertise. We empirically demonstrate that the commonly\nused fact checking pipeline, known as the retriever-reader, suffers from\nperformance deterioration when it is trained on the labeled data from one\ndomain and used in another domain. Afterwards, we delve into each component of\nthe pipeline and propose novel algorithms to address this problem. We propose\nan adversarial algorithm to make the retriever component robust against\ndistribution shift. Our core idea is to initially train a bi-encoder on the\nlabeled source data, and then, to adversarially train two separate document and\nclaim encoders using unlabeled target data. We then focus on the reader\ncomponent and propose to train it such that it is insensitive towards the order\nof claims and evidence documents. Our empirical evaluations support the\nhypothesis that such a reader shows a higher robustness against distribution\nshift. To our knowledge, there is no publicly available multi-topic fact\nchecking dataset. Thus, we propose a simple automatic method to re-purpose two\nwell-known fact checking datasets. We then construct eight fact checking\nscenarios from these datasets, and compare our model to a set of strong\nbaseline models, including recent domain adaptation models that use GPT4 for\ngenerating synthetic data.", "published": "2024-03-27 15:15:14", "link": "http://arxiv.org/abs/2403.18671v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Non-Linear Inference Time Intervention: Improving LLM Truthfulness", "abstract": "In this work, we explore LLM's internal representation space to identify\nattention heads that contain the most truthful and accurate information. We\nfurther developed the Inference Time Intervention (ITI) framework, which lets\nbias LLM without the need for fine-tuning. The improvement manifests in\nintroducing a non-linear multi-token probing and multi-token intervention:\nNon-Linear ITI (NL-ITI), which significantly enhances performance on evaluation\nbenchmarks. NL-ITI is tested on diverse multiple-choice datasets, including\nTruthfulQA, on which we report over 16% relative MC1 (accuracy of model\npointing to the correct answer) improvement with respect to the baseline ITI\nresults. Moreover, we achieved a 10% relative improvement over the recently\nreleased Truth Forest (TrFf) method that also focused on ITI improvement.", "published": "2024-03-27 15:22:16", "link": "http://arxiv.org/abs/2403.18680v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling Laws For Dense Retrieval", "abstract": "Scaling up neural models has yielded significant advancements in a wide array\nof tasks, particularly in language generation. Previous studies have found that\nthe performance of neural models frequently adheres to predictable scaling\nlaws, correlated with factors such as training set size and model size. This\ninsight is invaluable, especially as large-scale experiments grow increasingly\nresource-intensive. Yet, such scaling law has not been fully explored in dense\nretrieval due to the discrete nature of retrieval metrics and complex\nrelationships between training data and model sizes in retrieval tasks. In this\nstudy, we investigate whether the performance of dense retrieval models follows\nthe scaling law as other neural models. We propose to use contrastive\nlog-likelihood as the evaluation metric and conduct extensive experiments with\ndense retrieval models implemented with different numbers of parameters and\ntrained with different amounts of annotated data. Results indicate that, under\nour settings, the performance of dense retrieval models follows a precise\npower-law scaling related to the model size and the number of annotations.\nAdditionally, we examine scaling with prevalent data augmentation methods to\nassess the impact of annotation quality, and apply the scaling law to find the\nbest resource allocation strategy under a budget constraint. We believe that\nthese insights will significantly contribute to understanding the scaling\neffect of dense retrieval models and offer meaningful guidance for future\nresearch endeavors.", "published": "2024-03-27 15:27:36", "link": "http://arxiv.org/abs/2403.18684v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "CYCLE: Learning to Self-Refine the Code Generation", "abstract": "Pre-trained code language models have achieved promising performance in code\ngeneration and improved the programming efficiency of human developers.\nHowever, their self-refinement capability is typically overlooked by the\nexisting evaluations of code LMs, which focus only on the accuracy of the\none-time prediction. For the cases when code LMs fail to implement the correct\nprogram, developers actually find it hard to debug and fix the faulty\nprediction since it is not written by the developers themselves. Unfortunately,\nour study reveals that code LMs cannot efficiently self-refine their faulty\ngenerations as well.\n  In this paper, we propose CYCLE framework, learning to self-refine the faulty\ngeneration according to the available feedback, such as the execution results\nreported by the test suites. We evaluate CYCLE on three popular code generation\nbenchmarks, HumanEval, MBPP, and APPS. The results reveal that CYCLE\nsuccessfully maintains, sometimes improves, the quality of one-time code\ngeneration, while significantly improving the self-refinement capability of\ncode LMs. We implement four variants of CYCLE with varied numbers of parameters\nacross 350M, 1B, 2B, and 3B, and the experiments show that CYCLE consistently\nboosts the code generation performance, by up to 63.5%, across benchmarks and\nvaried model sizes. We also notice that CYCLE outperforms code LMs that have\n3$\\times$ more parameters in self-refinement.", "published": "2024-03-27 16:45:02", "link": "http://arxiv.org/abs/2403.18746v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Measuring Political Bias in Large Language Models: What Is Said and How\n  It Is Said", "abstract": "We propose to measure political bias in LLMs by analyzing both the content\nand style of their generated content regarding political issues. Existing\nbenchmarks and measures focus on gender and racial biases. However, political\nbias exists in LLMs and can lead to polarization and other harms in downstream\napplications. In order to provide transparency to users, we advocate that there\nshould be fine-grained and explainable measures of political biases generated\nby LLMs. Our proposed measure looks at different political issues such as\nreproductive rights and climate change, at both the content (the substance of\nthe generation) and the style (the lexical polarity) of such bias. We measured\nthe political bias in eleven open-sourced LLMs and showed that our proposed\nframework is easily scalable to other topics and is explainable.", "published": "2024-03-27 18:22:48", "link": "http://arxiv.org/abs/2403.18932v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reshaping Free-Text Radiology Notes Into Structured Reports With\n  Generative Transformers", "abstract": "BACKGROUND: Radiology reports are typically written in a free-text format,\nmaking clinical information difficult to extract and use. Recently the adoption\nof structured reporting (SR) has been recommended by various medical societies\nthanks to the advantages it offers, e.g. standardization, completeness and\ninformation retrieval. We propose a pipeline to extract information from\nfree-text radiology reports, that fits with the items of the reference SR\nregistry proposed by a national society of interventional and medical\nradiology, focusing on CT staging of patients with lymphoma. METHODS: Our work\naims to leverage the potential of Natural Language Processing (NLP) and\nTransformer-based models to deal with automatic SR registry filling. With the\navailability of 174 radiology reports, we investigate a rule-free generative\nQuestion Answering approach based on a domain-specific version of T5 (IT5). Two\nstrategies (batch-truncation and ex-post combination) are implemented to comply\nwith the model's context length limitations. Performance is evaluated in terms\nof strict accuracy, F1, and format accuracy, and compared with the widely used\nGPT-3.5 Large Language Model. A 5-point Likert scale questionnaire is used to\ncollect human-expert feedback on the similarity between medical annotations and\ngenerated answers. RESULTS: The combination of fine-tuning and batch splitting\nallows IT5 to achieve notable results; it performs on par with GPT-3.5 albeit\nits size being a thousand times smaller in terms of parameters. Human-based\nassessment scores show a high correlation (Spearman's correlation\ncoefficients>0.88, p-values<0.001) with AI performance metrics (F1) and confirm\nthe superior ability of LLMs (i.e., GPT-3.5, 175B of parameters) in generating\nplausible human-like statements.", "published": "2024-03-27 18:38:39", "link": "http://arxiv.org/abs/2403.18938v1", "categories": ["cs.CL", "cs.AI", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "\"Sorry, Come Again?\" Prompting -- Enhancing Comprehension and\n  Diminishing Hallucination with [PAUSE]-injected Optimal Paraphrasing", "abstract": "Hallucination has emerged as the most vulnerable aspect of contemporary Large\nLanguage Models (LLMs). In this paper, we introduce the Sorry, Come Again (SCA)\nprompting, aimed to avoid LLM hallucinations by enhancing comprehension\nthrough: (i) optimal paraphrasing and (ii) injecting [PAUSE] tokens to delay\nLLM generation. First, we provide an in-depth analysis of linguistic nuances:\nformality, readability, and concreteness of prompts for 21 LLMs, and elucidate\nhow these nuances contribute to hallucinated generation. Prompts with lower\nreadability, formality, or concreteness pose comprehension challenges for LLMs,\nsimilar to those faced by humans. In such scenarios, an LLM tends to speculate\nand generate content based on its imagination (associative memory) to fill\nthese information gaps. Although these speculations may occasionally align with\nfactual information, their accuracy is not assured, often resulting in\nhallucination. Recent studies reveal that an LLM often neglects the middle\nsections of extended prompts, a phenomenon termed as lost in the middle. While\na specific paraphrase may suit one LLM, the same paraphrased version may elicit\na different response from another LLM. Therefore, we propose an optimal\nparaphrasing technique to identify the most comprehensible paraphrase of a\ngiven prompt, evaluated using Integrated Gradient (and its variations) to\nguarantee that the LLM accurately processes all words. While reading lengthy\nsentences, humans often pause at various points to better comprehend the\nmeaning read thus far. We have fine-tuned an LLM with injected [PAUSE] tokens,\nallowing the LLM to pause while reading lengthier prompts. This has brought\nseveral key contributions: (i) determining the optimal position to inject\n[PAUSE], (ii) determining the number of [PAUSE] tokens to be inserted, and\n(iii) introducing reverse proxy tuning to fine-tune the LLM for [PAUSE]\ninsertion.", "published": "2024-03-27 19:45:09", "link": "http://arxiv.org/abs/2403.18976v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ReflectSumm: A Benchmark for Course Reflection Summarization", "abstract": "This paper introduces ReflectSumm, a novel summarization dataset specifically\ndesigned for summarizing students' reflective writing. The goal of ReflectSumm\nis to facilitate developing and evaluating novel summarization techniques\ntailored to real-world scenarios with little training data, %practical tasks\nwith potential implications in the opinion summarization domain in general and\nthe educational domain in particular. The dataset encompasses a diverse range\nof summarization tasks and includes comprehensive metadata, enabling the\nexploration of various research questions and supporting different\napplications. To showcase its utility, we conducted extensive evaluations using\nmultiple state-of-the-art baselines. The results provide benchmarks for\nfacilitating further research in this area.", "published": "2024-03-27 21:10:07", "link": "http://arxiv.org/abs/2403.19012v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PhoWhisper: Automatic Speech Recognition for Vietnamese", "abstract": "We introduce PhoWhisper in five versions for Vietnamese automatic speech\nrecognition. PhoWhisper's robustness is achieved through fine-tuning the\nWhisper model on an 844-hour dataset that encompasses diverse Vietnamese\naccents. Our experimental study demonstrates state-of-the-art performances of\nPhoWhisper on benchmark Vietnamese ASR datasets. We have open-sourced\nPhoWhisper at: https://github.com/VinAIResearch/PhoWhisper", "published": "2024-03-27 13:10:06", "link": "http://arxiv.org/abs/2406.02555v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Few-Shot Recalibration of Language Models", "abstract": "Recent work has uncovered promising ways to extract well-calibrated\nconfidence estimates from language models (LMs), where the model's confidence\nscore reflects how likely it is to be correct. However, while LMs may appear\nwell-calibrated over broad distributions, this often hides significant\nmiscalibration within narrower slices (e.g., systemic over-confidence in math\ncan balance out systemic under-confidence in history, yielding perfect\ncalibration in aggregate). To attain well-calibrated confidence estimates for\nany slice of a distribution, we propose a new framework for few-shot\nslice-specific recalibration. Specifically, we train a recalibration model that\ntakes in a few unlabeled examples from any given slice and predicts a curve\nthat remaps confidence scores to be more accurate for that slice. Our trained\nmodel can recalibrate for arbitrary new slices, without using any labeled data\nfrom that slice. This enables us to identify domain-specific confidence\nthresholds above which the LM's predictions can be trusted, and below which it\nshould abstain. Experiments show that our few-shot recalibrator consistently\noutperforms existing calibration methods, for instance improving calibration\nerror for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.", "published": "2024-03-27 06:25:40", "link": "http://arxiv.org/abs/2403.18286v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering\n  Using a VLM", "abstract": "Stimulated by the sophisticated reasoning capabilities of recent Large\nLanguage Models (LLMs), a variety of strategies for bridging video modality\nhave been devised. A prominent strategy involves Video Language Models\n(VideoLMs), which train a learnable interface with video data to connect\nadvanced vision encoders with LLMs. Recently, an alternative strategy has\nsurfaced, employing readily available foundation models, such as VideoLMs and\nLLMs, across multiple stages for modality bridging. In this study, we introduce\na simple yet novel strategy where only a single Vision Language Model (VLM) is\nutilized. Our starting point is the plain insight that a video comprises a\nseries of images, or frames, interwoven with temporal information. The essence\nof video comprehension lies in adeptly managing the temporal aspects along with\nthe spatial details of each frame. Initially, we transform a video into a\nsingle composite image by arranging multiple frames in a grid layout. The\nresulting single image is termed as an image grid. This format, while\nmaintaining the appearance of a solitary image, effectively retains temporal\ninformation within the grid structure. Therefore, the image grid approach\nenables direct application of a single high-performance VLM without\nnecessitating any video-data training. Our extensive experimental analysis\nacross ten zero-shot video question answering benchmarks, including five\nopen-ended and five multiple-choice benchmarks, reveals that the proposed Image\nGrid Vision Language Model (IG-VLM) surpasses the existing methods in nine out\nof ten benchmarks.", "published": "2024-03-27 09:48:23", "link": "http://arxiv.org/abs/2403.18406v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Exploring language relations through syntactic distances and geographic\n  proximity", "abstract": "Languages are grouped into families that share common linguistic traits.\nWhile this approach has been successful in understanding genetic relations\nbetween diverse languages, more analyses are needed to accurately quantify\ntheir relatedness, especially in less studied linguistic levels such as syntax.\nHere, we explore linguistic distances using series of parts of speech (POS)\nextracted from the Universal Dependencies dataset. Within an\ninformation-theoretic framework, we show that employing POS trigrams maximizes\nthe possibility of capturing syntactic variations while being at the same time\ncompatible with the amount of available data. Linguistic connections are then\nestablished by assessing pairwise distances based on the POS distributions.\nIntriguingly, our analysis reveals definite clusters that correspond to well\nknown language families and groups, with exceptions explained by distinct\nmorphological typologies. Furthermore, we obtain a significant correlation\nbetween language similarity and geographic distance, which underscores the\ninfluence of spatial proximity on language kinships.", "published": "2024-03-27 10:36:17", "link": "http://arxiv.org/abs/2403.18430v2", "categories": ["cs.CL", "physics.data-an", "physics.soc-ph", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Can Language Beat Numerical Regression? Language-Based Multimodal\n  Trajectory Prediction", "abstract": "Language models have demonstrated impressive ability in context understanding\nand generative performance. Inspired by the recent success of language\nfoundation models, in this paper, we propose LMTraj (Language-based Multimodal\nTrajectory predictor), which recasts the trajectory prediction task into a sort\nof question-answering problem. Departing from traditional numerical regression\nmodels, which treat the trajectory coordinate sequence as continuous signals,\nwe consider them as discrete signals like text prompts. Specially, we first\ntransform an input space for the trajectory coordinate into the natural\nlanguage space. Here, the entire time-series trajectories of pedestrians are\nconverted into a text prompt, and scene images are described as text\ninformation through image captioning. The transformed numerical and image data\nare then wrapped into the question-answering template for use in a language\nmodel. Next, to guide the language model in understanding and reasoning\nhigh-level knowledge, such as scene context and social relationships between\npedestrians, we introduce an auxiliary multi-task question and answering. We\nthen train a numerical tokenizer with the prompt data. We encourage the\ntokenizer to separate the integer and decimal parts well, and leverage it to\ncapture correlations between the consecutive numbers in the language model.\nLastly, we train the language model using the numerical tokenizer and all of\nthe question-answer prompts. Here, we propose a beam-search-based most-likely\nprediction and a temperature-based multimodal prediction to implement both\ndeterministic and stochastic inferences. Applying our LMTraj, we show that the\nlanguage-based model can be a powerful pedestrian trajectory predictor, and\noutperforms existing numerical-based predictor methods. Code is publicly\navailable at https://github.com/inhwanbae/LMTrajectory .", "published": "2024-03-27 11:06:44", "link": "http://arxiv.org/abs/2403.18447v1", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Language Plays a Pivotal Role in the Object-Attribute Compositional\n  Generalization of CLIP", "abstract": "Vision-language models, such as CLIP, have shown promising\nOut-of-Distribution (OoD) generalization under various types of distribution\nshifts. Recent studies attempted to investigate the leading cause of this\ncapability. In this work, we follow the same path, but focus on a specific type\nof OoD data - images with novel compositions of attribute-object pairs - and\nstudy whether such models can successfully classify those images into\ncomposition classes. We carefully designed an authentic image test dataset\ncalled ImageNet-AO, consisting of attributes for objects that are unlikely\nencountered in the CLIP training sets. We found that CLIPs trained with large\ndatasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude\nimprovement in effective compositional OoD generalization compared to both\nsupervised models and CLIPs trained with smaller datasets, such as CC-12M and\nYFCC-15M. Our results provide evidence that the scale and diversity of training\ndata and language supervision play a key role in unlocking the compositional\ngeneralization abilities of vision-language models.", "published": "2024-03-27 12:59:44", "link": "http://arxiv.org/abs/2403.18525v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Path Towards Legal Autonomy: An interoperable and explainable approach\n  to extracting, transforming, loading and computing legal information using\n  large language models, expert systems and Bayesian networks", "abstract": "Legal autonomy - the lawful activity of artificial intelligence agents - can\nbe achieved in one of two ways. It can be achieved either by imposing\nconstraints on AI actors such as developers, deployers and users, and on AI\nresources such as data, or by imposing constraints on the range and scope of\nthe impact that AI agents can have on the environment. The latter approach\ninvolves encoding extant rules concerning AI driven devices into the software\nof AI agents controlling those devices (e.g., encoding rules about limitations\non zones of operations into the agent software of an autonomous drone device).\nThis is a challenge since the effectivity of such an approach requires a method\nof extracting, loading, transforming and computing legal information that would\nbe both explainable and legally interoperable, and that would enable AI agents\nto reason about the law. In this paper, we sketch a proof of principle for such\na method using large language models (LLMs), expert legal systems known as\nlegal decision paths, and Bayesian networks. We then show how the proposed\nmethod could be applied to extant regulation in matters of autonomous cars,\nsuch as the California Vehicle Code.", "published": "2024-03-27 13:12:57", "link": "http://arxiv.org/abs/2403.18537v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LO"], "primary_category": "cs.AI"}
{"title": "Safety Verification of Wait-Only Non-Blocking Broadcast Protocols", "abstract": "We study networks of processes that all execute the same finite protocol and\ncommunicate synchronously in two different ways: a process can broadcast one\nmessage to all other processes or send it to at most one other process. In both\ncases, if no process can receive the message, it will still be sent. We\nestablish a precise complexity class for two coverability problems with a\nparameterised number of processes: the state coverability problem and the\nconfiguration coverability problem. It is already known that these problems are\nAckermann-hard (but decidable) in the general case. We show that when the\nprotocol is Wait-Only, i.e., it has no state from which a process can send and\nreceive messages, the complexity drops to P and PSPACE, respectively.", "published": "2024-03-27 14:17:33", "link": "http://arxiv.org/abs/2403.18591v1", "categories": ["cs.LO", "cs.CL", "cs.MA"], "primary_category": "cs.LO"}
{"title": "Mitigating Hallucinations in Large Vision-Language Models with\n  Instruction Contrastive Decoding", "abstract": "Large Vision-Language Models (LVLMs) are increasingly adept at generating\ncontextually detailed and coherent responses from visual inputs. However, their\napplication in multimodal decision-making and open-ended generation is hindered\nby a notable rate of hallucinations, where generated text inaccurately\nrepresents the visual contents. To address this issue, this paper introduces\nthe Instruction Contrastive Decoding (ICD) method, a novel approach designed to\nreduce hallucinations during LVLM inference. Our method is inspired by our\nobservation that what we call disturbance instructions significantly exacerbate\nhallucinations in multimodal fusion modules. ICD contrasts distributions from\nstandard and instruction disturbance, thereby increasing alignment uncertainty\nand effectively subtracting hallucinated concepts from the original\ndistribution. Through comprehensive experiments on discriminative benchmarks\n(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that\nICD significantly mitigates both object-level and attribute-level\nhallucinations. Moreover, our method not only addresses hallucinations but also\nsignificantly enhances the general perception and recognition capabilities of\nLVLMs.", "published": "2024-03-27 16:04:47", "link": "http://arxiv.org/abs/2403.18715v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Long-form factuality in large language models", "abstract": "Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can outperform crowdsourced human\nannotators - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality.", "published": "2024-03-27 17:48:55", "link": "http://arxiv.org/abs/2403.18802v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language\n  Models", "abstract": "In this work, we introduce Mini-Gemini, a simple and effective framework\nenhancing multi-modality Vision Language Models (VLMs). Despite the\nadvancements in VLMs facilitating basic visual dialog and reasoning, a\nperformance gap persists compared to advanced models like GPT-4 and Gemini. We\ntry to narrow the gap by mining the potential of VLMs for better performance\nand any-to-any workflow from three aspects, i.e., high-resolution visual\ntokens, high-quality data, and VLM-guided generation. To enhance visual tokens,\nwe propose to utilize an additional visual encoder for high-resolution\nrefinement without increasing the visual token count. We further construct a\nhigh-quality dataset that promotes precise image comprehension and\nreasoning-based generation, expanding the operational scope of current VLMs. In\ngeneral, Mini-Gemini further mines the potential of VLMs and empowers current\nframeworks with image understanding, reasoning, and generation simultaneously.\nMini-Gemini supports a series of dense and MoE Large Language Models (LLMs)\nfrom 2B to 34B. It is demonstrated to achieve leading performance in several\nzero-shot benchmarks and even surpasses the developed private models. Code and\nmodels are available at https://github.com/dvlab-research/MiniGemini.", "published": "2024-03-27 17:59:04", "link": "http://arxiv.org/abs/2403.18814v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Moderating Illicit Online Image Promotion for Unsafe User-Generated\n  Content Games Using Large Vision-Language Models", "abstract": "Online user generated content games (UGCGs) are increasingly popular among\nchildren and adolescents for social interaction and more creative online\nentertainment. However, they pose a heightened risk of exposure to explicit\ncontent, raising growing concerns for the online safety of children and\nadolescents. Despite these concerns, few studies have addressed the issue of\nillicit image-based promotions of unsafe UGCGs on social media, which can\ninadvertently attract young users. This challenge arises from the difficulty of\nobtaining comprehensive training data for UGCG images and the unique nature of\nthese images, which differ from traditional unsafe content. In this work, we\ntake the first step towards studying the threat of illicit promotions of unsafe\nUGCGs. We collect a real-world dataset comprising 2,924 images that display\ndiverse sexually explicit and violent content used to promote UGCGs by their\ngame creators. Our in-depth studies reveal a new understanding of this problem\nand the urgent need for automatically flagging illicit UGCG promotions. We\nadditionally create a cutting-edge system, UGCG-Guard, designed to aid social\nmedia platforms in effectively identifying images used for illicit UGCG\npromotions. This system leverages recently introduced large vision-language\nmodels (VLMs) and employs a novel conditional prompting strategy for zero-shot\ndomain adaptation, along with chain-of-thought (CoT) reasoning for contextual\nidentification. UGCG-Guard achieves outstanding results, with an accuracy rate\nof 94% in detecting these images used for the illicit promotion of such games\nin real-world scenarios.", "published": "2024-03-27 19:02:13", "link": "http://arxiv.org/abs/2403.18957v2", "categories": ["cs.CY", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CY"}
{"title": "IDGenRec: LLM-RecSys Alignment with Textual ID Learning", "abstract": "Generative recommendation based on Large Language Models (LLMs) have\ntransformed the traditional ranking-based recommendation style into a\ntext-to-text generation paradigm. However, in contrast to standard NLP tasks\nthat inherently operate on human vocabulary, current research in generative\nrecommendations struggles to effectively encode recommendation items within the\ntext-to-text framework using concise yet meaningful ID representations. To\nbetter align LLMs with recommendation needs, we propose IDGen, representing\neach item as a unique, concise, semantically rich, platform-agnostic textual ID\nusing human language tokens. This is achieved by training a textual ID\ngenerator alongside the LLM-based recommender, enabling seamless integration of\npersonalized recommendations into natural language generation. Notably, as user\nhistory is expressed in natural language and decoupled from the original\ndataset, our approach suggests the potential for a foundational generative\nrecommendation model. Experiments show that our framework consistently\nsurpasses existing models in sequential recommendation under standard\nexperimental setting. Then, we explore the possibility of training a foundation\nrecommendation model with the proposed method on data collected from 19\ndifferent datasets and tested its recommendation performance on 6 unseen\ndatasets across different platforms under a completely zero-shot setting. The\nresults show that the zero-shot performance of the pre-trained foundation model\nis comparable to or even better than some traditional recommendation models\nbased on supervised training, showing the potential of the IDGen paradigm\nserving as the foundation model for generative recommendation. Code and data\nare open-sourced at https://github.com/agiresearch/IDGenRec.", "published": "2024-03-27 21:22:37", "link": "http://arxiv.org/abs/2403.19021v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Evaluating Large Language Models for Health-Related Text Classification\n  Tasks with Public Social Media Data", "abstract": "Large language models (LLMs) have demonstrated remarkable success in NLP\ntasks. However, there is a paucity of studies that attempt to evaluate their\nperformances on social media-based health-related natural language processing\ntasks, which have traditionally been difficult to achieve high scores in. We\nbenchmarked one supervised classic machine learning model based on Support\nVector Machines (SVMs), three supervised pretrained language models (PLMs)\nbased on RoBERTa, BERTweet, and SocBERT, and two LLM based classifiers (GPT3.5\nand GPT4), across 6 text classification tasks. We developed three approaches\nfor leveraging LLMs for text classification: employing LLMs as zero-shot\nclassifiers, us-ing LLMs as annotators to annotate training data for supervised\nclassifiers, and utilizing LLMs with few-shot examples for augmentation of\nmanually annotated data. Our comprehensive experiments demonstrate that\nemploy-ing data augmentation using LLMs (GPT-4) with relatively small\nhuman-annotated data to train lightweight supervised classification models\nachieves superior results compared to training with human-annotated data alone.\nSupervised learners also outperform GPT-4 and GPT-3.5 in zero-shot settings. By\nleveraging this data augmentation strategy, we can harness the power of LLMs to\ndevelop smaller, more effective domain-specific NLP models. LLM-annotated data\nwithout human guidance for training light-weight supervised classification\nmodels is an ineffective strategy. However, LLM, as a zero-shot classifier,\nshows promise in excluding false negatives and potentially reducing the human\neffort required for data annotation. Future investigations are imperative to\nexplore optimal training data sizes and the optimal amounts of augmented data.", "published": "2024-03-27 22:05:10", "link": "http://arxiv.org/abs/2403.19031v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Capability-aware Prompt Reformulation Learning for Text-to-Image\n  Generation", "abstract": "Text-to-image generation systems have emerged as revolutionary tools in the\nrealm of artistic creation, offering unprecedented ease in transforming textual\nprompts into visual art. However, the efficacy of these systems is intricately\nlinked to the quality of user-provided prompts, which often poses a challenge\nto users unfamiliar with prompt crafting. This paper addresses this challenge\nby leveraging user reformulation data from interaction logs to develop an\nautomatic prompt reformulation model. Our in-depth analysis of these logs\nreveals that user prompt reformulation is heavily dependent on the individual\nuser's capability, resulting in significant variance in the quality of\nreformulation pairs. To effectively use this data for training, we introduce\nthe Capability-aware Prompt Reformulation (CAPR) framework. CAPR innovatively\nintegrates user capability into the reformulation process through two key\ncomponents: the Conditional Reformulation Model (CRM) and Configurable\nCapability Features (CCF). CRM reformulates prompts according to a specified\nuser capability, as represented by CCF. The CCF, in turn, offers the\nflexibility to tune and guide the CRM's behavior. This enables CAPR to\neffectively learn diverse reformulation strategies across various user\ncapacities and to simulate high-capability user reformulation during inference.\nExtensive experiments on standard text-to-image generation benchmarks showcase\nCAPR's superior performance over existing baselines and its remarkable\nrobustness on unseen systems. Furthermore, comprehensive analyses validate the\neffectiveness of different components. CAPR can facilitate user-friendly\ninteraction with text-to-image systems and make advanced artistic creation more\nachievable for a broader range of users.", "published": "2024-03-27 17:41:16", "link": "http://arxiv.org/abs/2403.19716v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Intelligent Learning Rate Distribution to reduce Catastrophic Forgetting\n  in Transformers", "abstract": "Pretraining language models on large text corpora is a common practice in\nnatural language processing. Fine-tuning of these models is then performed to\nachieve the best results on a variety of tasks. In this paper, we investigate\nthe problem of catastrophic forgetting in transformer neural networks and\nquestion the common practice of fine-tuning with a flat learning rate for the\nentire network in this context. We perform a hyperparameter optimization\nprocess to find learning rate distributions that are better than a flat\nlearning rate. We combine the learning rate distributions thus found and show\nthat they generalize to better performance with respect to the problem of\ncatastrophic forgetting. We validate these learning rate distributions with a\nvariety of NLP benchmarks from the GLUE dataset.", "published": "2024-03-27 13:40:09", "link": "http://arxiv.org/abs/2404.01317v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Embeddings: The Promise of Visual Table in Visual Reasoning", "abstract": "Visual representation learning has been a cornerstone in computer vision,\ninvolving typical forms such as visual embeddings, structural symbols, and\ntext-based representations. Despite the success of CLIP-type visual embeddings,\nthey often lack access to world knowledge critical for visual reasoning. In\nthis work, we propose Visual Table, a novel form of visual representation\ntailored for visual reasoning. Visual tables are constructed as hierarchical\ndescriptions of visual scenes, featuring a scene description and multiple\nobject-centric descriptions covering categories, attributes, and knowledge.\nThanks to the structural and textual formats, visual tables offer unique\nadvantages over mere visual embeddings, such as interpretability and\ncontrollable editing. Furthermore, they deliver instance-level world knowledge\nand detailed attributes that are essential for visual reasoning. To create\nvisual tables, we develop a generator trained on the dataset with collected,\nsmall-scale annotations. Extensive results on 11 visual reasoning benchmarks\ndemonstrate that the generated visual tables significantly outperform previous\nstructural and text-based representations. Moreover, they consistently enhance\nstate-of-the-art multimodal large language models across diverse benchmarks,\nshowcasing their potential for advancing visual reasoning tasks. Our code is\navailable at https://github.com/LaVi-Lab/Visual-Table.", "published": "2024-03-27 04:49:23", "link": "http://arxiv.org/abs/2403.18252v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "A Survey on Large Language Models from Concept to Implementation", "abstract": "Recent advancements in Large Language Models (LLMs), particularly those built\non Transformer architectures, have significantly broadened the scope of natural\nlanguage processing (NLP) applications, transcending their initial use in\nchatbot technology. This paper investigates the multifaceted applications of\nthese models, with an emphasis on the GPT series. This exploration focuses on\nthe transformative impact of artificial intelligence (AI) driven tools in\nrevolutionizing traditional tasks like coding and problem-solving, while also\npaving new paths in research and development across diverse industries. From\ncode interpretation and image captioning to facilitating the construction of\ninteractive systems and advancing computational domains, Transformer models\nexemplify a synergy of deep learning, data analysis, and neural network design.\nThis survey provides an in-depth look at the latest research in Transformer\nmodels, highlighting their versatility and the potential they hold for\ntransforming diverse application sectors, thereby offering readers a\ncomprehensive understanding of the current and future landscape of\nTransformer-based LLMs in practical applications.", "published": "2024-03-27 19:35:41", "link": "http://arxiv.org/abs/2403.18969v2", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.CL"}
{"title": "What are human values, and how do we align AI to them?", "abstract": "There is an emerging consensus that we need to align AI systems with human\nvalues (Gabriel, 2020; Ji et al., 2024), but it remains unclear how to apply\nthis to language models in practice. We split the problem of \"aligning to human\nvalues\" into three parts: first, eliciting values from people; second,\nreconciling those values into an alignment target for training ML models; and\nthird, actually training the model. In this paper, we focus on the first two\nparts, and ask the question: what are \"good\" ways to synthesize diverse human\ninputs about values into a target for aligning language models? To answer this\nquestion, we first define a set of 6 criteria that we believe must be satisfied\nfor an alignment target to shape model behavior in accordance with human\nvalues. We then propose a process for eliciting and reconciling values called\nMoral Graph Elicitation (MGE), which uses a large language model to interview\nparticipants about their values in particular contexts; our approach is\ninspired by the philosophy of values advanced by Taylor (1977), Chang (2004),\nand others. We trial MGE with a representative sample of 500 Americans, on 3\nintentionally divisive prompts (e.g. advice about abortion). Our results\ndemonstrate that MGE is promising for improving model alignment across all 6\ncriteria. For example, almost all participants (89.1%) felt well represented by\nthe process, and (89%) thought the final moral graph was fair, even if their\nvalue wasn't voted as the wisest. Our process often results in \"expert\" values\n(e.g. values from women who have solicited abortion advice) rising to the top\nof the moral graph, without defining who is considered an expert in advance.", "published": "2024-03-27 18:12:02", "link": "http://arxiv.org/abs/2404.10636v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Mind the Domain Gap: a Systematic Analysis on Bioacoustic Sound Event\n  Detection", "abstract": "Detecting the presence of animal vocalisations in nature is essential to\nstudy animal populations and their behaviors. A recent development in the field\nis the introduction of the task known as few-shot bioacoustic sound event\ndetection, which aims to train a versatile animal sound detector using only a\nsmall set of audio samples. Previous efforts in this area have utilized\ndifferent architectures and data augmentation techniques to enhance model\nperformance. However, these approaches have not fully bridged the domain gap\nbetween source and target distributions, limiting their applicability in\nreal-world scenarios. In this work, we introduce an new dataset designed to\naugment the diversity and breadth of classes available for few-shot bioacoustic\nevent detection, building on the foundations of our previous datasets. To\nestablish a robust baseline system tailored for the DCASE 2024 Task 5\nchallenge, we delve into an array of acoustic features and adopt negative hard\nsampling as our primary domain adaptation strategy. This approach, chosen in\nalignment with the challenge's guidelines that necessitate the independent\ntreatment of each audio file, sidesteps the use of transductive learning to\nensure compliance while aiming to enhance the system's adaptability to domain\nshifts. Our experiments show that the proposed baseline system achieves a\nbetter performance compared with the vanilla prototypical network. The findings\nalso confirm the effectiveness of each domain adaptation method by ablating\ndifferent components within the networks. This highlights the potential to\nimprove few-shot bioacoustic sound event detection by further reducing the\nimpact of domain shift.", "published": "2024-03-27 14:44:24", "link": "http://arxiv.org/abs/2403.18638v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Dual-path Mamba: Short and Long-term Bidirectional Selective Structured\n  State Space Models for Speech Separation", "abstract": "Transformers have been the most successful architecture for various speech\nmodeling tasks, including speech separation. However, the self-attention\nmechanism in transformers with quadratic complexity is inefficient in\ncomputation and memory. Recent models incorporate new layers and modules along\nwith transformers for better performance but also introduce extra model\ncomplexity. In this work, we replace transformers with Mamba, a selective state\nspace model, for speech separation. We propose dual-path Mamba, which models\nshort-term and long-term forward and backward dependency of speech signals\nusing selective state spaces. Our experimental results on the WSJ0-2mix data\nshow that our dual-path Mamba models of comparably smaller sizes outperform\nstate-of-the-art RNN model DPRNN, CNN model WaveSplit, and transformer model\nSepformer. Code: https://github.com/xi-j/Mamba-TasNet", "published": "2024-03-27 05:00:08", "link": "http://arxiv.org/abs/2403.18257v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ACES: Evaluating Automated Audio Captioning Models on the Semantics of\n  Sounds", "abstract": "Automated Audio Captioning is a multimodal task that aims to convert audio\ncontent into natural language. The assessment of audio captioning systems is\ntypically based on quantitative metrics applied to text data. Previous studies\nhave employed metrics derived from machine translation and image captioning to\nevaluate the quality of generated audio captions. Drawing inspiration from\nauditory cognitive neuroscience research, we introduce a novel metric approach\n-- Audio Captioning Evaluation on Semantics of Sound (ACES). ACES takes into\naccount how human listeners parse semantic information from sounds, providing a\nnovel and comprehensive evaluation perspective for automated audio captioning\nsystems. ACES combines semantic similarities and semantic entity labeling. ACES\noutperforms similar automated audio captioning metrics on the Clotho-Eval FENSE\nbenchmark in two evaluation categories.", "published": "2024-03-27 13:54:17", "link": "http://arxiv.org/abs/2403.18572v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Diffusion-Based Generative Equalizer for Music Restoration", "abstract": "This paper presents a novel approach to audio restoration, focusing on the\nenhancement of low-quality music recordings, and in particular historical ones.\nBuilding upon a previous algorithm called BABE, or Blind Audio Bandwidth\nExtension, we introduce BABE-2, which presents a series of improvements. This\nresearch broadens the concept of bandwidth extension to \\emph{generative\nequalization}, a novel task that, to the best of our knowledge, has not been\nexplicitly addressed in previous studies. BABE-2 is built around an\noptimization algorithm utilizing priors from diffusion models, which are\ntrained or fine-tuned using a curated set of high-quality music tracks. The\nalgorithm simultaneously performs two critical tasks: estimation of the filter\ndegradation magnitude response and hallucination of the restored audio. The\nproposed method is objectively evaluated on historical piano recordings,\nshowing an enhancement over the prior version. The method yields similarly\nimpressive results in rejuvenating the works of renowned vocalists Enrico\nCaruso and Nellie Melba. This research represents an advancement in the\npractical restoration of historical music.", "published": "2024-03-27 14:41:39", "link": "http://arxiv.org/abs/2403.18636v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Noise-Robust Keyword Spotting through Self-supervised Pretraining", "abstract": "Voice assistants are now widely available, and to activate them a keyword\nspotting (KWS) algorithm is used. Modern KWS systems are mainly trained using\nsupervised learning methods and require a large amount of labelled data to\nachieve a good performance. Leveraging unlabelled data through self-supervised\nlearning (SSL) has been shown to increase the accuracy in clean conditions.\nThis paper explores how SSL pretraining such as Data2Vec can be used to enhance\nthe robustness of KWS models in noisy conditions, which is under-explored.\n  Models of three different sizes are pretrained using different pretraining\napproaches and then fine-tuned for KWS. These models are then tested and\ncompared to models trained using two baseline supervised learning methods, one\nbeing standard training using clean data and the other one being multi-style\ntraining (MTR). The results show that pretraining and fine-tuning on clean data\nis superior to supervised learning on clean data across all testing conditions,\nand superior to supervised MTR for testing conditions of SNR above 5 dB. This\nindicates that pretraining alone can increase the model's robustness. Finally,\nit is found that using noisy data for pretraining models, especially with the\nData2Vec-denoising approach, significantly enhances the robustness of KWS\nmodels in noisy conditions.", "published": "2024-03-27 13:42:14", "link": "http://arxiv.org/abs/2403.18560v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "68T10", "I.2.6"], "primary_category": "eess.AS"}
{"title": "Fusion approaches for emotion recognition from speech using acoustic and\n  text-based features", "abstract": "In this paper, we study different approaches for classifying emotions from\nspeech using acoustic and text-based features. We propose to obtain\ncontextualized word embeddings with BERT to represent the information contained\nin speech transcriptions and show that this results in better performance than\nusing Glove embeddings. We also propose and compare different strategies to\ncombine the audio and text modalities, evaluating them on IEMOCAP and\nMSP-PODCAST datasets. We find that fusing acoustic and text-based systems is\nbeneficial on both datasets, though only subtle differences are observed across\nthe evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect\nthat the criteria used to define the cross-validation folds have on results. In\nparticular, the standard way of creating folds for this dataset results in a\nhighly optimistic estimation of performance for the text-based system,\nsuggesting that some previous works may overestimate the advantage of\nincorporating transcriptions.", "published": "2024-03-27 14:40:25", "link": "http://arxiv.org/abs/2403.18635v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance\n  Accompaniment", "abstract": "We introduce a novel task within the field of 3D dance generation, termed\ndance accompaniment, which necessitates the generation of responsive movements\nfrom a dance partner, the \"follower\", synchronized with the lead dancer's\nmovements and the underlying musical rhythm. Unlike existing solo or group\ndance generation tasks, a duet dance scenario entails a heightened degree of\ninteraction between the two participants, requiring delicate coordination in\nboth pose and position. To support this task, we first build a large-scale and\ndiverse duet interactive dance dataset, DD100, by recording about 117 minutes\nof professional dancers' performances. To address the challenges inherent in\nthis task, we propose a GPT-based model, Duolando, which autoregressively\npredicts the subsequent tokenized motion conditioned on the coordinated\ninformation of the music, the leader's and the follower's movements. To further\nenhance the GPT's capabilities of generating stable results on unseen\nconditions (music and leader motions), we devise an off-policy reinforcement\nlearning strategy that allows the model to explore viable trajectories from\nout-of-distribution samplings, guided by human-defined rewards. Based on the\ncollected dataset and proposed method, we establish a benchmark with several\ncarefully designed metrics.", "published": "2024-03-27 17:57:02", "link": "http://arxiv.org/abs/2403.18811v1", "categories": ["cs.CV", "cs.GR", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and\n  Benchmark", "abstract": "We present a new dataset called Real Acoustic Fields (RAF) that captures real\nacoustic room data from multiple modalities. The dataset includes high-quality\nand densely captured room impulse response data paired with multi-view images,\nand precise 6DoF pose tracking data for sound emitters and listeners in the\nrooms. We used this dataset to evaluate existing methods for novel-view\nacoustic synthesis and impulse response generation which previously relied on\nsynthetic data. In our evaluation, we thoroughly assessed existing audio and\naudio-visual models against multiple criteria and proposed settings to enhance\ntheir performance on real-world data. We also conducted experiments to\ninvestigate the impact of incorporating visual data (i.e., images and depth)\ninto neural acoustic field models. Additionally, we demonstrated the\neffectiveness of a simple sim2real approach, where a model is pre-trained with\nsimulated data and fine-tuned with sparse real-world data, resulting in\nsignificant improvements in the few-shot learning approach. RAF is the first\ndataset to provide densely captured room acoustic data, making it an ideal\nresource for researchers working on audio and audio-visual neural acoustic\nfield modeling techniques. Demos and datasets are available on our project\npage: https://facebookresearch.github.io/real-acoustic-fields/", "published": "2024-03-27 17:59:56", "link": "http://arxiv.org/abs/2403.18821v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Robust Active Speaker Detection in Noisy Environments", "abstract": "This paper addresses the issue of active speaker detection (ASD) in noisy\nenvironments and formulates a robust active speaker detection (rASD) problem.\nExisting ASD approaches leverage both audio and visual modalities, but\nnon-speech sounds in the surrounding environment can negatively impact\nperformance. To overcome this, we propose a novel framework that utilizes\naudio-visual speech separation as guidance to learn noise-free audio features.\nThese features are then utilized in an ASD model, and both tasks are jointly\noptimized in an end-to-end framework. Our proposed framework mitigates residual\nnoise and audio quality reduction issues that can occur in a naive cascaded\ntwo-stage framework that directly uses separated speech for ASD, and enables\nthe two tasks to be optimized simultaneously. To further enhance the robustness\nof the audio features and handle inherent speech noises, we propose a dynamic\nweighted loss approach to train the speech separator. We also collected a\nreal-world noise audio dataset to facilitate investigations. Experiments\ndemonstrate that non-speech audio noises significantly impact ASD models, and\nour proposed approach improves ASD performance in noisy environments. The\nframework is general and can be applied to different ASD approaches to improve\ntheir robustness. Our code, models, and data will be released.", "published": "2024-03-27 20:52:30", "link": "http://arxiv.org/abs/2403.19002v2", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
