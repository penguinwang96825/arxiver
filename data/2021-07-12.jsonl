{"title": "Dialogue State Tracking with Multi-Level Fusion of Predicted Dialogue\n  States and Conversations", "abstract": "Most recently proposed approaches in dialogue state tracking (DST) leverage\nthe context and the last dialogue states to track current dialogue states,\nwhich are often slot-value pairs. Although the context contains the complete\ndialogue information, the information is usually indirect and even requires\nreasoning to obtain. The information in the lastly predicted dialogue states is\ndirect, but when there is a prediction error, the dialogue information from\nthis source will be incomplete or erroneous. In this paper, we propose the\nDialogue State Tracking with Multi-Level Fusion of Predicted Dialogue States\nand Conversations network (FPDSC). This model extracts information of each\ndialogue turn by modeling interactions among each turn utterance, the\ncorresponding last dialogue states, and dialogue slots. Then the representation\nof each dialogue turn is aggregated by a hierarchical structure to form the\npassage information, which is utilized in the current turn of DST. Experimental\nresults validate the effectiveness of the fusion network with 55.03% and 59.07%\njoint accuracy on MultiWOZ 2.0 and MultiWOZ 2.1 datasets, which reaches the\nstate-of-the-art performance. Furthermore, we conduct the deleted-value and\nrelated-slot experiments on MultiWOZ 2.1 to evaluate our model.", "published": "2021-07-12 02:30:30", "link": "http://arxiv.org/abs/2107.05168v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Legal Judgment Prediction with Multi-Stage CaseRepresentation Learning\n  in the Real Court Setting", "abstract": "Legal judgment prediction(LJP) is an essential task for legal AI. While prior\nmethods studied on this topic in a pseudo setting by employing the\njudge-summarized case narrative as the input to predict the judgment,\nneglecting critical case life-cycle information in real court setting could\nthreaten the case logic representation quality and prediction correctness. In\nthis paper, we introduce a novel challenging dataset from real courtrooms to\npredict the legal judgment in a reasonably encyclopedic manner by leveraging\nthe genuine input of the case -- plaintiff's claims and court debate data, from\nwhich the case's facts are automatically recognized by comprehensively\nunderstanding the multi-role dialogues of the court debate, and then learnt to\ndiscriminate the claims so as to reach the final judgment through multi-task\nlearning. An extensive set of experiments with a large civil trial data set\nshows that the proposed model can more accurately characterize the interactions\namong claims, fact and debate for legal judgment prediction, achieving\nsignificant improvements over strong state-of-the-art baselines. Moreover, the\nuser study conducted with real judges and law school students shows the neural\npredictions can also be interpretable and easily observed, and thus enhancing\nthe trial efficiency and judgment quality.", "published": "2021-07-12 04:27:14", "link": "http://arxiv.org/abs/2107.05192v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CatVRNN: Generating Category Texts via Multi-task Learning", "abstract": "Controlling the model to generate texts of different categories is a\nchallenging task that is receiving increasing attention. Recently, generative\nadversarial networks (GANs) have shown promising results for category text\ngeneration. However, the texts generated by GANs usually suffer from problems\nof mode collapse and training instability. To avoid the above problems, in this\nstudy, inspired by multi-task learning, a novel model called category-aware\nvariational recurrent neural network (CatVRNN) is proposed. In this model,\ngeneration and classification tasks are trained simultaneously to generate\ntexts of different categories. The use of multi-task learning can improve the\nquality of the generated texts, when the classification task is appropriate. In\naddition, a function is proposed to initialize the hidden state of the CatVRNN\nto force the model to generate texts of a specific category. Experimental\nresults on three datasets demonstrate that the model can outperform\nstate-of-the-art text generation methods based on GAN in terms of diversity of\ngenerated texts.", "published": "2021-07-12 06:42:37", "link": "http://arxiv.org/abs/2107.05219v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hate versus Politics: Detection of Hate against Policy makers in Italian\n  tweets", "abstract": "Accurate detection of hate speech against politicians, policy making and\npolitical ideas is crucial to maintain democracy and free speech.\nUnfortunately, the amount of labelled data necessary for training models to\ndetect hate speech are limited and domain-dependent. In this paper, we address\nthe issue of classification of hate speech against policy makers from Twitter\nin Italian, producing the first resource of this type in this language. We\ncollected and annotated 1264 tweets, examined the cases of disagreements\nbetween annotators, and performed in-domain and cross-domain hate speech\nclassifications with different features and algorithms. We achieved a\nperformance of ROC AUC 0.83 and analyzed the most predictive attributes, also\nfinding the different language features in the anti-policymakers and\nanti-immigration domains. Finally, we visualized networks of hashtags to\ncapture the topics used in hateful and normal tweets.", "published": "2021-07-12 12:24:45", "link": "http://arxiv.org/abs/2107.05357v1", "categories": ["cs.CL", "68T50", "K.4.1; J.4"], "primary_category": "cs.CL"}
{"title": "Few-shot Language Coordination by Modeling Theory of Mind", "abstract": "$\\textit{No man is an island.}$ Humans communicate with a large community by\ncoordinating with different interlocutors within short conversations. This\nability has been understudied by the research on building neural communicative\nagents. We study the task of few-shot $\\textit{language coordination}$: agents\nquickly adapting to their conversational partners' language abilities.\nDifferent from current communicative agents trained with self-play, we require\nthe lead agent to coordinate with a $\\textit{population}$ of agents with\ndifferent linguistic abilities, quickly adapting to communicate with unseen\nagents in the population. This requires the ability to model the partner's\nbeliefs, a vital component of human communication. Drawing inspiration from\ntheory-of-mind (ToM; Premack& Woodruff (1978)), we study the effect of the\nspeaker explicitly modeling the listeners' mental states. The speakers, as\nshown in our experiments, acquire the ability to predict the reactions of their\npartner, which helps it generate instructions that concisely express its\ncommunicative goal. We examine our hypothesis that the instructions generated\nwith ToM modeling yield better communication performance in both a referential\ngame and a language navigation task. Positive results from our experiments hint\nat the importance of explicitly modeling communication as a socio-pragmatic\nprogress.", "published": "2021-07-12 19:26:11", "link": "http://arxiv.org/abs/2107.05697v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Compositional Concept Learning", "abstract": "In this paper, we study the problem of recognizing compositional\nattribute-object concepts within the zero-shot learning (ZSL) framework. We\npropose an episode-based cross-attention (EpiCA) network which combines merits\nof cross-attention mechanism and episode-based training strategy to recognize\nnovel compositional concepts. Firstly, EpiCA bases on cross-attention to\ncorrelate concept-visual information and utilizes the gated pooling layer to\nbuild contextualized representations for both images and concepts. The updated\nrepresentations are used for a more in-depth multi-modal relevance calculation\nfor concept recognition. Secondly, a two-phase episode training strategy,\nespecially the transductive phase, is adopted to utilize unlabeled test\nexamples to alleviate the low-resource learning problem. Experiments on two\nwidely-used zero-shot compositional learning (ZSCL) benchmarks have\ndemonstrated the effectiveness of the model compared with recent approaches on\nboth conventional and generalized ZSCL settings.", "published": "2021-07-12 03:31:56", "link": "http://arxiv.org/abs/2107.05176v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Putting words into the system's mouth: A targeted attack on neural\n  machine translation using monolingual data poisoning", "abstract": "Neural machine translation systems are known to be vulnerable to adversarial\ntest inputs, however, as we show in this paper, these systems are also\nvulnerable to training attacks. Specifically, we propose a poisoning attack in\nwhich a malicious adversary inserts a small poisoned sample of monolingual text\ninto the training set of a system trained using back-translation. This sample\nis designed to induce a specific, targeted translation behaviour, such as\npeddling misinformation. We present two methods for crafting poisoned examples,\nand show that only a tiny handful of instances, amounting to only 0.02% of the\ntraining set, is sufficient to enact a successful attack. We outline a defence\nmethod against said attacks, which partly ameliorates the problem. However, we\nstress that this is a blind-spot in modern NMT, demanding immediate attention.", "published": "2021-07-12 08:07:09", "link": "http://arxiv.org/abs/2107.05243v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "DaCy: A Unified Framework for Danish NLP", "abstract": "Danish natural language processing (NLP) has in recent years obtained\nconsiderable improvements with the addition of multiple new datasets and\nmodels. However, at present, there is no coherent framework for applying\nstate-of-the-art models for Danish. We present DaCy: a unified framework for\nDanish NLP built on SpaCy. DaCy uses efficient multitask models which obtain\nstate-of-the-art performance on named entity recognition, part-of-speech\ntagging, and dependency parsing. DaCy contains tools for easy integration of\nexisting models such as for polarity, emotion, or subjectivity detection. In\naddition, we conduct a series of tests for biases and robustness of Danish NLP\npipelines through augmentation of the test set of DaNE. DaCy large compares\nfavorably and is especially robust to long input lengths and spelling\nvariations and errors. All models except DaCy large display significant biases\nrelated to ethnicity while only Polyglot shows a significant gender bias. We\nargue that for languages with limited benchmark sets, data augmentation can be\nparticularly useful for obtaining more realistic and fine-grained performance\nestimates. We provide a series of augmenters as a first step towards a more\nthorough evaluation of language models for low and medium resource languages\nand encourage further development.", "published": "2021-07-12 10:14:31", "link": "http://arxiv.org/abs/2107.05295v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Flexible Multi-Task Model for BERT Serving", "abstract": "In this demonstration, we present an efficient BERT-based multi-task (MT)\nframework that is particularly suitable for iterative and incremental\ndevelopment of the tasks. The proposed framework is based on the idea of\npartial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the\nother layers frozen. For each task, we train independently a single-task (ST)\nmodel using partial fine-tuning. Then we compress the task-specific layers in\neach ST model using knowledge distillation. Those compressed ST models are\nfinally merged into one MT model so that the frozen layers of the former are\nshared across the tasks. We exemplify our approach on eight GLUE tasks,\ndemonstrating that it is able to achieve both strong performance and\nefficiency. We have implemented our method in the utterance understanding\nsystem of XiaoAI, a commercial AI assistant developed by Xiaomi. We estimate\nthat our model reduces the overall serving cost by 86%.", "published": "2021-07-12 12:42:39", "link": "http://arxiv.org/abs/2107.05377v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MECT: Multi-Metadata Embedding based Cross-Transformer for Chinese Named\n  Entity Recognition", "abstract": "Recently, word enhancement has become very popular for Chinese Named Entity\nRecognition (NER), reducing segmentation errors and increasing the semantic and\nboundary information of Chinese words. However, these methods tend to ignore\nthe information of the Chinese character structure after integrating the\nlexical information. Chinese characters have evolved from pictographs since\nancient times, and their structure often reflects more information about the\ncharacters. This paper presents a novel Multi-metadata Embedding based\nCross-Transformer (MECT) to improve the performance of Chinese NER by fusing\nthe structural information of Chinese characters. Specifically, we use\nmulti-metadata embedding in a two-stream Transformer to integrate Chinese\ncharacter features with the radical-level embedding. With the structural\ncharacteristics of Chinese characters, MECT can better capture the semantic\ninformation of Chinese characters for NER. The experimental results obtained on\nseveral well-known benchmarking datasets demonstrate the merits and superiority\nof the proposed MECT method.\\footnote{The source code of the proposed method is\npublicly available at https://github.com/CoderMusou/MECT4CNER.", "published": "2021-07-12 13:39:06", "link": "http://arxiv.org/abs/2107.05418v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Technical Report of Team GraphMIRAcles in the WikiKG90M-LSC Track of\n  OGB-LSC @ KDD Cup 2021", "abstract": "Link prediction in large-scale knowledge graphs has gained increasing\nattention recently. The OGB-LSC team presented OGB Large-Scale Challenge\n(OGB-LSC), a collection of three real-world datasets for advancing the\nstate-of-the-art in large-scale graph machine learning. In this paper, we\nintroduce the solution of our team GraphMIRAcles in the WikiKG90M-LSC track of\nOGB-LSC @ KDD Cup 2021. In the WikiKG90M-LSC track, the goal is to\nautomatically predict missing links in WikiKG90M, a large scale knowledge graph\nextracted from Wikidata. To address this challenge, we propose a framework that\nintegrates three components -- a basic model ComplEx-CMRC, a rule miner AMIE 3,\nand an inference model to predict missing links. Experiments demonstrate that\nour solution achieves an MRR of 0.9707 on the test dataset. Moreover, as the\nknowledge distillation in the inference model uses test tail candidates --\nwhich are unavailable in practice -- we conduct ablation studies on knowledge\ndistillation. Experiments demonstrate that our model without knowledge\ndistillation achieves an MRR of 0.9533 on the full validation dataset.", "published": "2021-07-12 14:44:16", "link": "http://arxiv.org/abs/2107.05476v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Accenture at CheckThat! 2021: Interesting claim identification and\n  ranking with contextually sensitive lexical training data augmentation", "abstract": "This paper discusses the approach used by the Accenture Team for CLEF2021\nCheckThat! Lab, Task 1, to identify whether a claim made in social media would\nbe interesting to a wide audience and should be fact-checked. Twitter training\nand test data were provided in English, Arabic, Spanish, Turkish, and\nBulgarian. Claims were to be classified (check-worthy/not check-worthy) and\nranked in priority order for the fact-checker. Our method used deep neural\nnetwork transformer models with contextually sensitive lexical augmentation\napplied on the supplied training datasets to create additional training\nsamples. This augmentation approach improved the performance for all languages.\nOverall, our architecture and data augmentation pipeline produced the best\nsubmitted system for Arabic, and performance scales according to the quantity\nof provided training data for English, Spanish, Turkish, and Bulgarian. This\npaper investigates the deep neural network architectures for each language as\nwell as the provided data to examine why the approach worked so effectively for\nArabic, and discusses additional data augmentation measures that should could\nbe useful to this problem.", "published": "2021-07-12 18:46:47", "link": "http://arxiv.org/abs/2107.05684v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Revisiting Uncertainty-based Query Strategies for Active Learning with\n  Transformers", "abstract": "Active learning is the iterative construction of a classification model\nthrough targeted labeling, enabling significant labeling cost savings. As most\nresearch on active learning has been carried out before transformer-based\nlanguage models (\"transformers\") became popular, despite its practical\nimportance, comparably few papers have investigated how transformers can be\ncombined with active learning to date. This can be attributed to the fact that\nusing state-of-the-art query strategies for transformers induces a prohibitive\nruntime overhead, which effectively nullifies, or even outweighs the desired\ncost savings. For this reason, we revisit uncertainty-based query strategies,\nwhich had been largely outperformed before, but are particularly suited in the\ncontext of fine-tuning transformers. In an extensive evaluation, we connect\ntransformers to experiments from previous research, assessing their performance\non five widely used text classification benchmarks. For active learning with\ntransformers, several other uncertainty-based approaches outperform the\nwell-known prediction entropy query strategy, thereby challenging its status as\nmost popular uncertainty baseline in active learning for text classification.", "published": "2021-07-12 18:56:04", "link": "http://arxiv.org/abs/2107.05687v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quantifying Explainability in NLP and Analyzing Algorithms for\n  Performance-Explainability Tradeoff", "abstract": "The healthcare domain is one of the most exciting application areas for\nmachine learning, but a lack of model transparency contributes to a lag in\nadoption within the industry. In this work, we explore the current art of\nexplainability and interpretability within a case study in clinical text\nclassification, using a task of mortality prediction within MIMIC-III clinical\nnotes. We demonstrate various visualization techniques for fully interpretable\nmethods as well as model-agnostic post hoc attributions, and we provide a\ngeneralized method for evaluating the quality of explanations using infidelity\nand local Lipschitz across model types from logistic regression to BERT\nvariants. With these metrics, we introduce a framework through which\npractitioners and researchers can assess the frontier between a model's\npredictive performance and the quality of its available explanations. We make\nour code available to encourage continued refinement of these methods.", "published": "2021-07-12 19:07:24", "link": "http://arxiv.org/abs/2107.05693v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Systematic Literature Review of Automated ICD Coding and\n  Classification Systems using Discharge Summaries", "abstract": "Codification of free-text clinical narratives have long been recognised to be\nbeneficial for secondary uses such as funding, insurance claim processing and\nresearch. The current scenario of assigning codes is a manual process which is\nvery expensive, time-consuming and error prone. In recent years, many\nresearchers have studied the use of Natural Language Processing (NLP), related\nMachine Learning (ML) and Deep Learning (DL) methods and techniques to resolve\nthe problem of manual coding of clinical narratives and to assist human coders\nto assign clinical codes more accurately and efficiently. This systematic\nliterature review provides a comprehensive overview of automated clinical\ncoding systems that utilises appropriate NLP, ML and DL methods and techniques\nto assign ICD codes to discharge summaries. We have followed the Preferred\nReporting Items for Systematic Reviews and Meta-Analyses(PRISMA) guidelines and\nconducted a comprehensive search of publications from January, 2010 to December\n2020 in four academic databases- PubMed, ScienceDirect, Association for\nComputing Machinery(ACM) Digital Library, and the Association for Computational\nLinguistics(ACL) Anthology. We reviewed 7,556 publications; 38 met the\ninclusion criteria. This review identified: datasets having discharge\nsummaries; NLP techniques along with some other data extraction processes,\ndifferent feature extraction and embedding techniques. To measure the\nperformance of classification methods, different evaluation metrics are used.\nLastly, future research directions are provided to scholars who are interested\nin automated ICD code assignment. Efforts are still required to improve ICD\ncode prediction accuracy, availability of large-scale de-identified clinical\ncorpora with the latest version of the classification system. This can be a\nplatform to guide and share knowledge with the less experienced coders and\nresearchers.", "published": "2021-07-12 03:55:17", "link": "http://arxiv.org/abs/2107.10652v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "End-to-End Natural Language Understanding Pipeline for Bangla\n  Conversational Agents", "abstract": "Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).", "published": "2021-07-12 16:09:22", "link": "http://arxiv.org/abs/2107.05541v6", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Direct speech-to-speech translation with discrete units", "abstract": "We present a direct speech-to-speech translation (S2ST) model that translates\nspeech from one language to speech in another language without relying on\nintermediate text generation. We tackle the problem by first applying a\nself-supervised discrete speech encoder on the target speech and then training\na sequence-to-sequence speech-to-unit translation (S2UT) model to predict the\ndiscrete representations of the target speech. When target text transcripts are\navailable, we design a joint speech and text training framework that enables\nthe model to generate dual modality output (speech and text) simultaneously in\nthe same inference pass. Experiments on the Fisher Spanish-English dataset show\nthat the proposed framework yields improvement of 6.7 BLEU compared with a\nbaseline direct S2ST model that predicts spectrogram features. When trained\nwithout any text transcripts, our model performance is comparable to models\nthat predict spectrograms and are trained with text supervision, showing the\npotential of our system for translation between unwritten languages. Audio\nsamples are available at\nhttps://facebookresearch.github.io/speech_translation/direct_s2st_units/index.html .", "published": "2021-07-12 17:40:43", "link": "http://arxiv.org/abs/2107.05604v2", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Combiner: Full Attention Transformer with Sparse Computation Cost", "abstract": "Transformers provide a class of expressive architectures that are extremely\neffective for sequence modeling. However, the key limitation of transformers is\ntheir quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to\nthe sequence length in attention layers, which restricts application in\nextremely long sequences. Most existing approaches leverage sparsity or\nlow-rank assumptions in the attention matrix to reduce cost, but sacrifice\nexpressiveness. Instead, we propose Combiner, which provides full attention\ncapability in each attention head while maintaining low computation and memory\ncomplexity. The key idea is to treat the self-attention mechanism as a\nconditional expectation over embeddings at each location, and approximate the\nconditional distribution with a structured factorization. Each location can\nattend to all other locations, either via direct attention, or through indirect\nattention to abstractions, which are again conditional expectations of\nembeddings from corresponding local regions. We show that most sparse attention\npatterns used in existing sparse transformers are able to inspire the design of\nsuch factorization for full attention, resulting in the same sub-quadratic cost\n($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in\nreplacement for attention layers in existing transformers and can be easily\nimplemented in common frameworks. An experimental evaluation on both\nautoregressive and bidirectional sequence tasks demonstrates the effectiveness\nof this approach, yielding state-of-the-art results on several image and text\nmodeling tasks.", "published": "2021-07-12 22:43:11", "link": "http://arxiv.org/abs/2107.05768v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Improving Speech Translation by Understanding and Learning from the\n  Auxiliary Text Translation Task", "abstract": "Pretraining and multitask learning are widely used to improve the speech to\ntext translation performance. In this study, we are interested in training a\nspeech to text translation model along with an auxiliary text to text\ntranslation task. We conduct a detailed analysis to understand the impact of\nthe auxiliary task on the primary task within the multitask learning framework.\nOur analysis confirms that multitask learning tends to generate similar decoder\nrepresentations from different modalities and preserve more information from\nthe pretrained text translation modules. We observe minimal negative transfer\neffect between the two tasks and sharing more parameters is helpful to transfer\nknowledge from the text task to the speech task. The analysis also reveals that\nthe modality representation difference at the top decoder layers is still not\nnegligible, and those layers are critical for the translation quality. Inspired\nby these findings, we propose three methods to improve translation quality.\nFirst, a parameter sharing and initialization strategy is proposed to enhance\ninformation sharing between the tasks. Second, a novel attention-based\nregularization is proposed for the encoders and pulls the representations from\ndifferent modalities closer. Third, an online knowledge distillation is\nproposed to enhance the knowledge transfer from the text to the speech task.\nOur experiments show that the proposed approach improves translation\nperformance by more than 2 BLEU over a strong baseline and achieves\nstate-of-the-art results on the \\textsc{MuST-C} English-German, English-French\nand English-Spanish language pairs.", "published": "2021-07-12 23:53:40", "link": "http://arxiv.org/abs/2107.05782v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Tortured phrases: A dubious writing style emerging in science. Evidence\n  of critical issues affecting established journals", "abstract": "Probabilistic text generators have been used to produce fake scientific\npapers for more than a decade. Such nonsensical papers are easily detected by\nboth human and machine. Now more complex AI-powered generation techniques\nproduce texts indistinguishable from that of humans and the generation of\nscientific texts from a few keywords has been documented. Our study introduces\nthe concept of tortured phrases: unexpected weird phrases in lieu of\nestablished ones, such as 'counterfeit consciousness' instead of 'artificial\nintelligence.' We combed the literature for tortured phrases and study one\nreputable journal where these concentrated en masse. Hypothesising the use of\nadvanced language models we ran a detector on the abstracts of recent articles\nof this journal and on several control sets. The pairwise comparisons reveal a\nconcentration of abstracts flagged as 'synthetic' in the journal. We also\nhighlight irregularities in its operation, such as abrupt changes in editorial\ntimelines. We substantiate our call for investigation by analysing several\nindividual dubious articles, stressing questionable features: tortured writing\nstyle, citation of non-existent literature, and unacknowledged image reuse.\nSurprisingly, some websites offer to rewrite texts for free, generating\ngobbledegook full of tortured phrases. We believe some authors used rewritten\ntexts to pad their manuscripts. We wish to raise the awareness on publications\ncontaining such questionable AI-generated or rewritten texts that passed (poor)\npeer review. Deception with synthetic texts threatens the integrity of the\nscientific literature.", "published": "2021-07-12 20:47:08", "link": "http://arxiv.org/abs/2107.06751v1", "categories": ["cs.DL", "cs.CL", "cs.CY", "cs.IR"], "primary_category": "cs.DL"}
{"title": "Lumen: A Machine Learning Framework to Expose Influence Cues in Text", "abstract": "Phishing and disinformation are popular social engineering attacks with\nattackers invariably applying influence cues in texts to make them more\nappealing to users. We introduce Lumen, a learning-based framework that exposes\ninfluence cues in text: (i) persuasion, (ii) framing, (iii) emotion, (iv)\nobjectivity/subjectivity, (v) guilt/blame, and (vi) use of emphasis. Lumen was\ntrained with a newly developed dataset of 3K texts comprised of disinformation,\nphishing, hyperpartisan news, and mainstream news. Evaluation of Lumen in\ncomparison to other learning models showed that Lumen and LSTM presented the\nbest F1-micro score, but Lumen yielded better interpretability. Our results\nhighlight the promise of ML to expose influence cues in text, towards the goal\nof application in automatic labeling tools to improve the accuracy of\nhuman-based detection and reduce the likelihood of users falling for deceptive\nonline content.", "published": "2021-07-12 15:53:13", "link": "http://arxiv.org/abs/2107.10655v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Persistent Spatial Semantic Representation for High-level Natural\n  Language Instruction Execution", "abstract": "Natural language provides an accessible and expressive interface to specify\nlong-term tasks for robotic agents. However, non-experts are likely to specify\nsuch tasks with high-level instructions, which abstract over specific robot\nactions through several layers of abstraction. We propose that key to bridging\nthis gap between language and robot actions over long execution horizons are\npersistent representations. We propose a persistent spatial semantic\nrepresentation method, and show how it enables building an agent that performs\nhierarchical reasoning to effectively execute long-term tasks. We evaluate our\napproach on the ALFRED benchmark and achieve state-of-the-art results, despite\ncompletely avoiding the commonly used step-by-step instructions.", "published": "2021-07-12 17:47:19", "link": "http://arxiv.org/abs/2107.05612v3", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "UniSpeech at scale: An Empirical Study of Pre-training Method on\n  Large-Scale Speech Recognition Dataset", "abstract": "Recently, there has been a vast interest in self-supervised learning (SSL)\nwhere the model is pre-trained on large scale unlabeled data and then\nfine-tuned on a small labeled dataset. The common wisdom is that SSL helps\nresource-limited tasks in which only a limited amount of labeled data is\navailable. The benefit of SSL keeps diminishing when the labeled training data\namount increases. To our best knowledge, at most a few thousand hours of\nlabeled data was used in the study of SSL. In contrast, the industry usually\nuses tens of thousands of hours of labeled data to build high-accuracy speech\nrecognition (ASR) systems for resource-rich languages. In this study, we take\nthe challenge to investigate whether and how SSL can improve the ASR accuracy\nof a state-of-the-art production-scale Transformer-Transducer model, which was\nbuilt with 65 thousand hours of anonymized labeled EN-US data.", "published": "2021-07-12 07:30:17", "link": "http://arxiv.org/abs/2107.05233v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Sound Event Detection: A Tutorial", "abstract": "The goal of automatic sound event detection (SED) methods is to recognize\nwhat is happening in an audio signal and when it is happening. In practice, the\ngoal is to recognize at what temporal instances different sounds are active\nwithin an audio signal. This paper gives a tutorial presentation of sound event\ndetection, including its definition, signal processing and machine learning\napproaches, evaluation, and future perspectives.", "published": "2021-07-12 14:30:40", "link": "http://arxiv.org/abs/2107.05463v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "DPCRN: Dual-Path Convolution Recurrent Network for Single Channel Speech\n  Enhancement", "abstract": "The dual-path RNN (DPRNN) was proposed to more effectively model extremely\nlong sequences for speech separation in the time domain. By splitting long\nsequences to smaller chunks and applying intra-chunk and inter-chunk RNNs, the\nDPRNN reached promising performance in speech separation with a limited model\nsize. In this paper, we combine the DPRNN module with Convolution Recurrent\nNetwork (CRN) and design a model called Dual-Path Convolution Recurrent Network\n(DPCRN) for speech enhancement in the time-frequency domain. We replace the\nRNNs in the CRN with DPRNN modules, where the intra-chunk RNNs are used to\nmodel the spectrum pattern in a single frame and the inter-chunk RNNs are used\nto model the dependence between consecutive frames. With only 0.8M parameters,\nthe submitted DPCRN model achieves an overall mean opinion score (MOS) of 3.57\nin the wide band scenario track of the Interspeech 2021 Deep Noise Suppression\n(DNS) challenge. Evaluations on some other test sets also show the efficacy of\nour model.", "published": "2021-07-12 13:50:27", "link": "http://arxiv.org/abs/2107.05429v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Extending Text-to-Speech Synthesis with Articulatory Movement Prediction\n  using Ultrasound Tongue Imaging", "abstract": "In this paper, we present our first experiments in text-to-articulation\nprediction, using ultrasound tongue image targets. We extend a traditional\n(vocoder-based) DNN-TTS framework with predicting PCA-compressed ultrasound\nimages, of which the continuous tongue motion can be reconstructed in synchrony\nwith synthesized speech. We use the data of eight speakers, train fully\nconnected and recurrent neural networks, and show that FC-DNNs are more\nsuitable for the prediction of sequential data than LSTMs, in case of limited\ntraining data. Objective experiments and visualized predictions show that the\nproposed solution is feasible and the generated ultrasound videos are close to\nnatural tongue movement. Articulatory movement prediction from text input can\nbe useful for audiovisual speech synthesis or computer-assisted pronunciation\ntraining.", "published": "2021-07-12 16:19:28", "link": "http://arxiv.org/abs/2107.05550v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Perceptual-based deep-learning denoiser as a defense against adversarial\n  attacks on ASR systems", "abstract": "In this paper we investigate speech denoising as a defense against\nadversarial attacks on automatic speech recognition (ASR) systems. Adversarial\nattacks attempt to force misclassification by adding small perturbations to the\noriginal speech signal. We propose to counteract this by employing a\nneural-network based denoiser as a pre-processor in the ASR pipeline. The\ndenoiser is independent of the downstream ASR model, and thus can be rapidly\ndeployed in existing systems. We found that training the denoisier using a\nperceptually motivated loss function resulted in increased adversarial\nrobustness without compromising ASR performance on benign samples. Our defense\nwas evaluated (as a part of the DARPA GARD program) on the 'Kenansville' attack\nstrategy across a range of attack strengths and speech samples. An average\nimprovement in Word Error Rate (WER) of about 7.7% was observed over the\nundefended model at 20 dB signal-to-noise-ratio (SNR) attack strength.", "published": "2021-07-12 07:00:06", "link": "http://arxiv.org/abs/2107.05222v1", "categories": ["eess.AS", "cs.LG", "eess.SP"], "primary_category": "eess.AS"}
{"title": "BERT-like Pre-training for Symbolic Piano Music Classification Tasks", "abstract": "This article presents a benchmark study of symbolic piano music\nclassification using the masked language modelling approach of the\nBidirectional Encoder Representations from Transformers (BERT). Specifically,\nwe consider two types of MIDI data: MIDI scores, which are musical scores\nrendered directly into MIDI with no dynamics and precisely aligned with the\nmetrical grid notated by its composer and MIDI performances, which are MIDI\nencodings of human performances of musical scoresheets. With five public-domain\ndatasets of single-track piano MIDI files, we pre-train two 12-layer\nTransformer models using the BERT approach, one for MIDI scores and the other\nfor MIDI performances, and fine-tune them for four downstream classification\ntasks. These include two note-level classification tasks (melody extraction and\nvelocity prediction) and two sequence-level classification tasks (style\nclassification and emotion classification). Our evaluation shows that the BERT\napproach leads to higher classification accuracy than recurrent neural network\n(RNN)-based baselines.", "published": "2021-07-12 07:03:57", "link": "http://arxiv.org/abs/2107.05223v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Project Achoo: A Practical Model and Application for COVID-19 Detection\n  from Recordings of Breath, Voice, and Cough", "abstract": "The COVID-19 pandemic created a significant interest and demand for infection\ndetection and monitoring solutions. In this paper we propose a machine learning\nmethod to quickly triage COVID-19 using recordings made on consumer devices.\nThe approach combines signal processing methods with fine-tuned deep learning\nnetworks and provides methods for signal denoising, cough detection and\nclassification. We have also developed and deployed a mobile application that\nuses symptoms checker together with voice, breath and cough signals to detect\nCOVID-19 infection. The application showed robust performance on both open\nsourced datasets and on the noisy data collected during beta testing by the end\nusers.", "published": "2021-07-12 08:07:56", "link": "http://arxiv.org/abs/2107.10716v2", "categories": ["eess.SP", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Codified audio language modeling learns useful representations for music\n  information retrieval", "abstract": "We demonstrate that language models pre-trained on codified\n(discretely-encoded) music audio learn representations that are useful for\ndownstream MIR tasks. Specifically, we explore representations from Jukebox\n(Dhariwal et al. 2020): a music generation system containing a language model\ntrained on codified audio from 1M songs. To determine if Jukebox's\nrepresentations contain useful information for MIR, we use them as input\nfeatures to train shallow models on several MIR tasks. Relative to\nrepresentations from conventional MIR models which are pre-trained on tagging,\nwe find that using representations from Jukebox as input features yields 30%\nstronger performance on average across four MIR tasks: tagging, genre\nclassification, emotion recognition, and key detection. For key detection, we\nobserve that representations from Jukebox are considerably stronger than those\nfrom models pre-trained on tagging, suggesting that pre-training via codified\naudio language modeling may address blind spots in conventional approaches. We\ninterpret the strength of Jukebox's representations as evidence that modeling\naudio instead of tags provides richer representations for MIR.", "published": "2021-07-12 18:28:50", "link": "http://arxiv.org/abs/2107.05677v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
