{"title": "S&P 500 Trend Prediction", "abstract": "This project aims to predict short-term and long-term upward trends in the\nS&P 500 index using machine learning models and feature engineering based on\nthe \"101 Formulaic Alphas\" methodology. The study employed multiple models,\nincluding Logistic Regression, Decision Trees, Random Forests, Neural Networks,\nK-Nearest Neighbors (KNN), and XGBoost, to identify market trends from\nhistorical stock data collected from Yahoo! Finance. Data preprocessing\ninvolved handling missing values, standardization, and iterative feature\nselection to ensure relevance and variability.\n  For short-term predictions, KNN emerged as the most effective model,\ndelivering robust performance with high recall for upward trends, while for\nlong-term forecasts, XGBoost demonstrated the highest accuracy and AUC scores\nafter hyperparameter tuning and class imbalance adjustments using SMOTE.\nFeature importance analysis highlighted the dominance of momentum-based and\nvolume-related indicators in driving predictions. However, models exhibited\nlimitations such as overfitting and low recall for positive market movements,\nparticularly in imbalanced datasets.\n  The study concludes that KNN is ideal for short-term alerts, whereas XGBoost\nis better suited for long-term trend forecasting. Future enhancements could\ninclude advanced architectures like Long Short-Term Memory (LSTM) networks and\nfurther feature refinement to improve precision and generalizability. These\nfindings contribute to developing reliable machine learning tools for market\ntrend prediction and investment decision-making.", "published": "2024-12-16 05:37:30", "link": "http://arxiv.org/abs/2412.11462v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Stochastic optimal self-path-dependent control: A new type of variational inequality and its viscosity solution", "abstract": "In this paper, we explore a new class of stochastic control problems\ncharacterized by specific control constraints. Specifically, the admissible\ncontrols are subject to the ratcheting constraint, meaning they must be\nnon-decreasing over time and are thus self-path-dependent. This type of\nproblems is common in various practical applications, such as optimal\nconsumption problems in financial engineering and optimal dividend payout\nproblems in actuarial science. Traditional stochastic control theory does not\nreadily apply to these problems due to their unique self-path-dependent control\nfeature. To tackle this challenge, we introduce a new class of\nHamilton-Jacobi-Bellman (HJB) equations, which are variational inequalities\nconcerning the derivative of a new spatial argument that represents the\nhistorical maximum control value. Under the standard Lipschitz continuity\ncondition, we demonstrate that the value functions for these\nself-path-dependent control problems are the unique solutions to their\ncorresponding HJB equations in the viscosity sense.", "published": "2024-12-16 02:23:36", "link": "http://arxiv.org/abs/2412.11383v1", "categories": ["math.OC", "math.AP", "q-fin.MF"], "primary_category": "math.OC"}
{"title": "Multivariate Distributions in Non-Stationary Complex Systems II: Empirical Results for Correlated Stock Markets", "abstract": "Multivariate Distributions are needed to capture the correlation structure of\ncomplex systems. In previous works, we developed a Random Matrix Model for such\ncorrelated multivariate joint probability density functions that accounts for\nthe non-stationarity typically found in complex systems. Here, we apply these\nresults to the returns measured in correlated stock markets. Only the knowledge\nof the multivariate return distributions allows for a full-fledged risk\nassessment. We analyze intraday data of 479 US stocks included in the S&P500\nindex during the trading year of 2014. We focus particularly on the tails which\nare algebraic and heavy. The non-stationary fluctuations of the correlations\nmake the tails heavier. With the few-parameter formulae of our Random Matrix\nModel we can describe and quantify how the empirical distributions change for\nvarying time resolution and in the presence of non-stationarity.", "published": "2024-12-16 09:39:55", "link": "http://arxiv.org/abs/2412.11602v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "Multivariate Distributions in Non-Stationary Complex Systems I: Random Matrix Model and Formulae for Data Analysis", "abstract": "Risk assessment for rare events is essential for understanding systemic\nstability in complex systems. As rare events are typically highly correlated,\nit is important to study heavy-tailed multivariate distributions of the\nrelevant variables, i.e. their joint probability density functions. Only for\nfew systems, such investigation have been performed. Statistical models are\ndesirable that describe heavy-tailed multivariate distributions, in particular\nwhen non-stationarity is present as is typically the case in complex systems.\nRecently, we put forward such a model based on a separation of time scales. By\nutilizing random matrices, we showed that the fluctuations of the correlations\nlift the tails. Here, we present formulae and methods to carry out a data\ncomparisons for complex systems. There are only few fit parameters. Compared to\nour previous results, we manage to remove in the algebraic cases one out of the\ntwo, respectively three, fit parameters which considerably facilitates\napplications. Furthermore, we explicitly work out the moments of our model\ndistributions. In a forthcoming paper we will apply our model to financial\nmarkets.", "published": "2024-12-16 09:39:20", "link": "http://arxiv.org/abs/2412.11601v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "A Deep Learning Approach for Trading Factor Residuals", "abstract": "The residuals in factor models prevalent in asset pricing presents\nopportunities to exploit the mis-pricing from unexplained cross-sectional\nvariation for arbitrage. We performed a replication of the methodology of\nGuijarro-Ordonez et al. (2019) (G-P-Z) on Deep Learning Statistical Arbitrage\n(DLSA), originally applied to U.S. equity data from 1998 to 2016, using a more\nrecent out-of-sample period from 2016 to 2024. Adhering strictly to\npoint-in-time (PIT) principles and ensuring no information leakage, we follow\nthe same data pre-processing, factor modeling, and deep learning architectures\n(CNNs and Transformers) as outlined by G-P-Z. Our replication yields unusually\nstrong performance metrics in certain tests, with out-of-sample Sharpe ratios\noccasionally exceeding 10. While such results are intriguing, they may indicate\nmodel overfitting, highly specific market conditions, or insufficient\naccounting for transaction costs and market impact. Further examination and\nrobustness checks are needed to align these findings with the more modest\nimprovements reported in the original study. (This work was conducted as the\nfinal project for IEOR 4576: Data-Driven Methods in Finance at Columbia\nUniversity.)", "published": "2024-12-16 04:04:38", "link": "http://arxiv.org/abs/2412.11432v2", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "A multi-factor market-neutral investment strategy for New York Stock Exchange equities", "abstract": "This report presents a systematic market-neutral, multi-factor investment\nstrategy for New York Stock Exchange equities with the objective of delivering\nsteady returns while minimizing correlation with the market. A robust feature\nset is integrated combining momentum-based indicators, fundamental factors, and\nanalyst recommendations. Using various statistical tests for feature selection,\nthe strategy identifies key drivers of equity performance and ranks stocks to\nbuild a balanced portfolio of long and short positions. Portfolio construction\nmethods, including equally weighted, risk parity, and minimum variance\nbeta-neutral approaches, were evaluated through rigorous backtesting. Risk\nparity demonstrated superior performance with a higher Sharpe ratio, lower\nbeta, and smaller maximum drawdown compared to the Standard and Poor's 500\nindex. Risk parity's market neutrality, combined with its ability to maintain\nsteady returns and mitigate large drawdowns, makes it a suitable approach for\nmanaging significant capital in equity markets.", "published": "2024-12-16 20:42:32", "link": "http://arxiv.org/abs/2412.12350v1", "categories": ["q-fin.TR", "91"], "primary_category": "q-fin.TR"}
