{"title": "Semi-constraint Optimal Transport for Entity Alignment with Dangling\n  Cases", "abstract": "Entity alignment (EA) merges knowledge graphs (KGs) by identifying the\nequivalent entities in different graphs, which can effectively enrich knowledge\nrepresentations of KGs. However, in practice, different KGs often include\ndangling entities whose counterparts cannot be found in the other graph, which\nlimits the performance of EA methods. To improve EA with dangling entities, we\npropose an unsupervised method called Semi-constraint Optimal Transport for\nEntity Alignment in Dangling cases (SoTead). Our main idea is to model the\nentity alignment between two KGs as an optimal transport problem from one KG's\nentities to the others. First, we set pseudo entity pairs between KGs based on\npretrained word embeddings. Then, we conduct contrastive metric learning to\nobtain the transport cost between each entity pair. Finally, we introduce a\nvirtual entity for each KG to \"align\" the dangling entities from the other KGs,\nwhich relaxes the optimization constraints and leads to a semi-constraint\noptimal transport. In the experimental part, we first show the superiority of\nSoTead on a commonly-used entity alignment dataset. Besides, to analyze the\nability for dangling entity detection with other baselines, we construct a\nmedical cross-lingual knowledge graph dataset, MedED, where our SoTead also\nreaches state-of-the-art performance.", "published": "2022-03-11 04:20:18", "link": "http://arxiv.org/abs/2203.05744v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERTopic: Neural topic modeling with a class-based TF-IDF procedure", "abstract": "Topic models can be useful tools to discover latent topics in collections of\ndocuments. Recent studies have shown the feasibility of approach topic modeling\nas a clustering task. We present BERTopic, a topic model that extends this\nprocess by extracting coherent topic representation through the development of\na class-based variation of TF-IDF. More specifically, BERTopic generates\ndocument embedding with pre-trained transformer-based language models, clusters\nthese embeddings, and finally, generates topic representations with the\nclass-based TF-IDF procedure. BERTopic generates coherent topics and remains\ncompetitive across a variety of benchmarks involving classical models and those\nthat follow the more recent clustering approach of topic modeling.", "published": "2022-03-11 08:35:15", "link": "http://arxiv.org/abs/2203.05794v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Long Time No See! Open-Domain Conversation with Long-Term Persona Memory", "abstract": "Most of the open-domain dialogue models tend to perform poorly in the setting\nof long-term human-bot conversations. The possible reason is that they lack the\ncapability of understanding and memorizing long-term dialogue history\ninformation. To address this issue, we present a novel task of Long-term Memory\nConversation (LeMon) and then build a new dialogue dataset DuLeMon and a\ndialogue generation framework with Long-Term Memory (LTM) mechanism (called\nPLATO-LTM). This LTM mechanism enables our system to accurately extract and\ncontinuously update long-term persona memory without requiring multiple-session\ndialogue datasets for model training. To our knowledge, this is the first\nattempt to conduct real-time dynamic management of persona information of both\nparties, including the user and the bot. Results on DuLeMon indicate that\nPLATO-LTM can significantly outperform baselines in terms of long-term dialogue\nconsistency, leading to better dialogue engagingness.", "published": "2022-03-11 08:41:14", "link": "http://arxiv.org/abs/2203.05797v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Discriminative Representations and Decision Boundaries for Open\n  Intent Detection", "abstract": "Open intent detection is a significant problem in natural language\nunderstanding, which aims to identify the unseen open intent while ensuring\nknown intent identification performance. However, current methods face two\nmajor challenges. Firstly, they struggle to learn friendly representations to\ndetect the open intent with prior knowledge of only known intents. Secondly,\nthere is a lack of an effective approach to obtaining specific and compact\ndecision boundaries for known intents. To address these issues, this paper\npresents an original framework called DA-ADB, which successively learns\ndistance-aware intent representations and adaptive decision boundaries for open\nintent detection. Specifically, we first leverage distance information to\nenhance the distinguishing capability of the intent representations. Then, we\ndesign a novel loss function to obtain appropriate decision boundaries by\nbalancing both empirical and open space risks. Extensive experiments\ndemonstrate the effectiveness of the proposed distance-aware and boundary\nlearning strategies. Compared to state-of-the-art methods, our framework\nachieves substantial improvements on three benchmark datasets. Furthermore, it\nyields robust performance with varying proportions of labeled data and known\ncategories.", "published": "2022-03-11 10:02:09", "link": "http://arxiv.org/abs/2203.05823v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Identification and Classification of Bragging in Social Media", "abstract": "Bragging is a speech act employed with the goal of constructing a favorable\nself-image through positive statements about oneself. It is widespread in daily\ncommunication and especially popular in social media, where users aim to build\na positive image of their persona directly or indirectly. In this paper, we\npresent the first large scale study of bragging in computational linguistics,\nbuilding on previous research in linguistics and pragmatics. To facilitate\nthis, we introduce a new publicly available data set of tweets annotated for\nbragging and their types. We empirically evaluate different transformer-based\nmodels injected with linguistic information in (a) binary bragging\nclassification, i.e., if tweets contain bragging statements or not; and (b)\nmulti-class bragging type prediction including not bragging. Our results show\nthat our models can predict bragging with macro F1 up to 72.42 and 35.95 in the\nbinary and multi-class classification tasks respectively. Finally, we present\nan extensive linguistic and error analysis of bragging prediction to guide\nfuture research on this topic.", "published": "2022-03-11 10:33:42", "link": "http://arxiv.org/abs/2203.05840v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Interpretable Neuro-Symbolic Reasoning Framework for Task-Oriented\n  Dialogue Generation", "abstract": "We study the interpretability issue of task-oriented dialogue systems in this\npaper. Previously, most neural-based task-oriented dialogue systems employ an\nimplicit reasoning strategy that makes the model predictions uninterpretable to\nhumans. To obtain a transparent reasoning process, we introduce neuro-symbolic\nto perform explicit reasoning that justifies model decisions by reasoning\nchains. Since deriving reasoning chains requires multi-hop reasoning for\ntask-oriented dialogues, existing neuro-symbolic approaches would induce error\npropagation due to the one-phase design. To overcome this, we propose a\ntwo-phase approach that consists of a hypothesis generator and a reasoner. We\nfirst obtain multiple hypotheses, i.e., potential operations to perform the\ndesired task, through the hypothesis generator. Each hypothesis is then\nverified by the reasoner, and the valid one is selected to conduct the final\nprediction. The whole system is trained by exploiting raw textual dialogues\nwithout using any reasoning chain annotations. Experimental studies on two\npublic benchmark datasets demonstrate that the proposed approach not only\nachieves better results, but also introduces an interpretable decision process.", "published": "2022-03-11 10:44:08", "link": "http://arxiv.org/abs/2203.05843v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Word Embeddings to Analyze Protests News", "abstract": "The first two tasks of the CLEF 2019 ProtestNews events focused on\ndistinguishing between protest and non-protest related news articles and\nsentences in a binary classification task. Among the submissions, two well\nperforming models have been chosen in order to replace the existing word\nembeddings word2vec and FastTest with ELMo and DistilBERT. Unlike bag of words\nor earlier vector approaches, ELMo and DistilBERT represent words as a sequence\nof vectors by capturing the meaning based on contextual information in the\ntext. Without changing the architecture of the original models other than the\nword embeddings, the implementation of DistilBERT improved the performance\nmeasured on the F1-Score of 0.66 compared to the FastText implementation.\nDistilBERT also outperformed ELMo in both tasks and models. Cleaning the\ndatasets by removing stopwords and lemmatizing the words has been shown to make\nthe models more generalizable across different contexts when training on a\ndataset with Indian news articles and evaluating the models on a dataset with\nnews articles from China.", "published": "2022-03-11 12:25:59", "link": "http://arxiv.org/abs/2203.05875v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Sentence is Worth 128 Pseudo Tokens: A Semantic-Aware Contrastive\n  Learning Framework for Sentence Embeddings", "abstract": "Contrastive learning has shown great potential in unsupervised sentence\nembedding tasks, e.g., SimCSE. However, We find that these existing solutions\nare heavily affected by superficial features like the length of sentences or\nsyntactic structures. In this paper, we propose a semantics-aware contrastive\nlearning framework for sentence embeddings, termed Pseudo-Token BERT (PT-BERT),\nwhich is able to exploit the pseudo-token space (i.e., latent semantic space)\nrepresentation of a sentence while eliminating the impact of superficial\nfeatures such as sentence length and syntax. Specifically, we introduce an\nadditional pseudo token embedding layer independent of the BERT encoder to map\neach sentence into a sequence of pseudo tokens in a fixed length. Leveraging\nthese pseudo sequences, we are able to construct same-length positive and\nnegative pairs based on the attention mechanism to perform contrastive\nlearning. In addition, we utilize both the gradient-updating and\nmomentum-updating encoders to encode instances while dynamically maintaining an\nadditional queue to store the representation of sentence embeddings, enhancing\nthe encoder's learning performance for negative examples. Experiments show that\nour model outperforms the state-of-the-art baselines on six standard semantic\ntextual similarity (STS) tasks. Furthermore, experiments on alignments and\nuniformity losses, as well as hard examples with different sentence lengths and\nsyntax, consistently verify the effectiveness of our method.", "published": "2022-03-11 12:29:22", "link": "http://arxiv.org/abs/2203.05877v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Achieving Reliable Human Assessment of Open-Domain Dialogue Systems", "abstract": "Evaluation of open-domain dialogue systems is highly challenging and\ndevelopment of better techniques is highlighted time and again as desperately\nneeded. Despite substantial efforts to carry out reliable live evaluation of\nsystems in recent competitions, annotations have been abandoned and reported as\ntoo unreliable to yield sensible results. This is a serious problem since\nautomatic metrics are not known to provide a good indication of what may or may\nnot be a high-quality conversation. Answering the distress call of competitions\nthat have emphasized the urgent need for better evaluation techniques in\ndialogue, we present the successful development of human evaluation that is\nhighly reliable while still remaining feasible and low cost. Self-replication\nexperiments reveal almost perfectly repeatable results with a correlation of\n$r=0.969$. Furthermore, due to the lack of appropriate methods of statistical\nsignificance testing, the likelihood of potential improvements to systems\noccurring due to chance is rarely taken into account in dialogue evaluation,\nand the evaluation we propose facilitates application of standard tests. Since\nwe have developed a highly reliable evaluation method, new insights into system\nperformance can be revealed. We therefore include a comparison of\nstate-of-the-art models (i) with and without personas, to measure the\ncontribution of personas to conversation quality, as well as (ii) prescribed\nversus freely chosen topics. Interestingly with respect to personas, results\nindicate that personas do not positively contribute to conversation quality as\nexpected.", "published": "2022-03-11 13:08:39", "link": "http://arxiv.org/abs/2203.05899v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When classifying grammatical role, BERT doesn't care about word order...\n  except when it matters", "abstract": "Because meaning can often be inferred from lexical semantics alone, word\norder is often a redundant cue in natural language. For example, the words\nchopped, chef, and onion are more likely used to convey \"The chef chopped the\nonion,\" not \"The onion chopped the chef.\" Recent work has shown large language\nmodels to be surprisingly word order invariant, but crucially has largely\nconsidered natural prototypical inputs, where compositional meaning mostly\nmatches lexical expectations. To overcome this confound, we probe grammatical\nrole representation in English BERT and GPT-2, on instances where lexical\nexpectations are not sufficient, and word order knowledge is necessary for\ncorrect classification. Such non-prototypical instances are naturally occurring\nEnglish sentences with inanimate subjects or animate objects, or sentences\nwhere we systematically swap the arguments to make sentences like \"The onion\nchopped the chef\". We find that, while early layer embeddings are largely\nlexical, word order is in fact crucial in defining the later-layer\nrepresentations of words in semantically non-prototypical positions. Our\nexperiments isolate the effect of word order on the contextualization process,\nand highlight how models use context in the uncommon, but critical, instances\nwhere it matters.", "published": "2022-03-11 19:00:15", "link": "http://arxiv.org/abs/2203.06204v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Staged Training for Transformer Language Models", "abstract": "The current standard approach to scaling transformer language models trains\neach model size from a different random initialization. As an alternative, we\nconsider a staged training setup that begins with a small model and\nincrementally increases the amount of compute used for training by applying a\n\"growth operator\" to increase the model depth and width. By initializing each\nstage with the output of the previous one, the training process effectively\nre-uses the compute from prior stages and becomes more efficient. Our growth\noperators each take as input the entire training state (including model\nparameters, optimizer state, learning rate schedule, etc.) and output a new\ntraining state from which training continues. We identify two important\nproperties of these growth operators, namely that they preserve both the loss\nand the \"training dynamics\" after applying the operator. While the\nloss-preserving property has been discussed previously, to the best of our\nknowledge this work is the first to identify the importance of preserving the\ntraining dynamics (the rate of decrease of the loss during training). To find\nthe optimal schedule for stages, we use the scaling laws from (Kaplan et al.,\n2020) to find a precise schedule that gives the most compute saving by starting\na new stage when training efficiency starts decreasing. We empirically validate\nour growth operators and staged training for autoregressive language models,\nshowing up to 22% compute savings compared to a strong baseline trained from\nscratch. Our code is available at https://github.com/allenai/staged-training.", "published": "2022-03-11 19:05:42", "link": "http://arxiv.org/abs/2203.06211v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Inference with A Chinese Entailment Graph", "abstract": "Predicate entailment detection is a crucial task for question-answering from\ntext, where previous work has explored unsupervised learning of entailment\ngraphs from typed open relation triples. In this paper, we present the first\npipeline for building Chinese entailment graphs, which involves a novel\nhigh-recall open relation extraction (ORE) method and the first Chinese\nfine-grained entity typing dataset under the FIGER type ontology. Through\nexperiments on the Levy-Holt dataset, we verify the strength of our Chinese\nentailment graph, and reveal the cross-lingual complementarity: on the parallel\nLevy-Holt dataset, an ensemble of Chinese and English entailment graphs\noutperforms both monolingual graphs, and raises unsupervised SOTA by 4.7 AUC\npoints.", "published": "2022-03-11 21:45:33", "link": "http://arxiv.org/abs/2203.06264v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tevatron: An Efficient and Flexible Toolkit for Dense Retrieval", "abstract": "Recent rapid advancements in deep pre-trained language models and the\nintroductions of large datasets have powered research in embedding-based dense\nretrieval. While several good research papers have emerged, many of them come\nwith their own software stacks. These stacks are typically optimized for some\nparticular research goals instead of efficiency or code structure. In this\npaper, we present Tevatron, a dense retrieval toolkit optimized for efficiency,\nflexibility, and code simplicity. Tevatron provides a standardized pipeline for\ndense retrieval including text processing, model training, corpus/query\nencoding, and search. This paper presents an overview of Tevatron and\ndemonstrates its effectiveness and efficiency across several IR and QA data\nsets. We also show how Tevatron's flexible design enables easy generalization\nacross datasets, model architectures, and accelerator platforms(GPU/TPU). We\nbelieve Tevatron can serve as an effective software foundation for dense\nretrieval system research including design, modeling, and optimization.", "published": "2022-03-11 05:47:45", "link": "http://arxiv.org/abs/2203.05765v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Integrating Dependency Tree Into Self-attention for Sentence\n  Representation", "abstract": "Recent progress on parse tree encoder for sentence representation learning is\nnotable. However, these works mainly encode tree structures recursively, which\nis not conducive to parallelization. On the other hand, these works rarely take\ninto account the labels of arcs in dependency trees. To address both issues, we\npropose Dependency-Transformer, which applies a relation-attention mechanism\nthat works in concert with the self-attention mechanism. This mechanism aims to\nencode the dependency and the spatial positional relations between nodes in the\ndependency tree of sentences. By a score-based method, we successfully inject\nthe syntax information without affecting Transformer's parallelizability. Our\nmodel outperforms or is comparable to the state-of-the-art methods on four\ntasks for sentence representation and has obvious advantages in computational\nefficiency.", "published": "2022-03-11 13:44:41", "link": "http://arxiv.org/abs/2203.05918v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Are discrete units necessary for Spoken Language Modeling?", "abstract": "Recent work in spoken language modeling shows the possibility of learning a\nlanguage unsupervisedly from raw audio without any text labels. The approach\nrelies first on transforming the audio into a sequence of discrete units (or\npseudo-text) and then training a language model directly on such pseudo-text.\nIs such a discrete bottleneck necessary, potentially introducing irreversible\nerrors in the encoding of the speech signal, or could we learn a language model\nwithout discrete units at all? In this work, we study the role of discrete\nversus continuous representations in spoken language modeling. We show that\ndiscretization is indeed essential for good results in spoken language\nmodeling. We show that discretization removes linguistically irrelevant\ninformation from the continuous features, helping to improve language modeling\nperformances. On the basis of this study, we train a language model on the\ndiscrete units of the HuBERT features, reaching new state-of-the-art results in\nthe lexical, syntactic and semantic metrics of the Zero Resource Speech\nChallenge 2021 (Track 1 - Speech Only).", "published": "2022-03-11 14:14:35", "link": "http://arxiv.org/abs/2203.05936v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Block-Sparse Adversarial Attack to Fool Transformer-Based Text\n  Classifiers", "abstract": "Recently, it has been shown that, in spite of the significant performance of\ndeep neural networks in different fields, those are vulnerable to adversarial\nexamples. In this paper, we propose a gradient-based adversarial attack against\ntransformer-based text classifiers. The adversarial perturbation in our method\nis imposed to be block-sparse so that the resultant adversarial example differs\nfrom the original sentence in only a few words. Due to the discrete nature of\ntextual data, we perform gradient projection to find the minimizer of our\nproposed optimization problem. Experimental results demonstrate that, while our\nadversarial attack maintains the semantics of the sentence, it can reduce the\naccuracy of GPT-2 to less than 5% on different datasets (AG News, MNLI, and\nYelp Reviews). Furthermore, the block-sparsity constraint of the proposed\noptimization problem results in small perturbations in the adversarial example.", "published": "2022-03-11 14:37:41", "link": "http://arxiv.org/abs/2203.05948v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CoDA21: Evaluating Language Understanding Capabilities of NLP Models\n  With Context-Definition Alignment", "abstract": "Pretrained language models (PLMs) have achieved superhuman performance on\nmany benchmarks, creating a need for harder tasks. We introduce CoDA21 (Context\nDefinition Alignment), a challenging benchmark that measures natural language\nunderstanding (NLU) capabilities of PLMs: Given a definition and a context each\nfor k words, but not the words themselves, the task is to align the k\ndefinitions with the k contexts. CoDA21 requires a deep understanding of\ncontexts and definitions, including complex inference and world knowledge. We\nfind that there is a large gap between human and PLM performance, suggesting\nthat CoDA21 measures an aspect of NLU that is not sufficiently covered in\nexisting benchmarks.", "published": "2022-03-11 20:12:49", "link": "http://arxiv.org/abs/2203.06228v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Synopses of Movie Narratives: a Video-Language Dataset for Story\n  Understanding", "abstract": "Despite recent advances of AI, story understanding remains an open and\nunder-investigated problem. We collect, preprocess, and publicly release a\nvideo-language story dataset, Synopses of Movie Narratives (SyMoN), containing\n5,193 video summaries of popular movies and TV series with a total length of\n869 hours. SyMoN captures naturalistic storytelling videos made by human\ncreators and intended for a human audience. As a prototypical and naturalistic\nstory dataset, SyMoN features high coverage of multimodal story events and\nabundant mental-state descriptions. Its use of storytelling techniques cause\ncross-domain semantic gaps that provide appropriate challenges to existing\nmodels. We establish benchmarks on video-text retrieval and zero-shot alignment\non movie summary videos, which showcase the importance of in-domain data and\nlong-term memory in story understanding. With SyMoN, we hope to lay the\ngroundwork for progress in multimodal story understanding.", "published": "2022-03-11 01:45:33", "link": "http://arxiv.org/abs/2203.05711v4", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Active Evaluation: Efficient NLG Evaluation with Few Pairwise\n  Comparisons", "abstract": "Recent studies have shown the advantages of evaluating NLG systems using\npairwise comparisons as opposed to direct assessment. Given $k$ systems, a\nnaive approach for identifying the top-ranked system would be to uniformly\nobtain pairwise comparisons from all ${k \\choose 2}$ pairs of systems. However,\nthis can be very expensive as the number of human annotations required would\ngrow quadratically with $k$. In this work, we introduce Active Evaluation, a\nframework to efficiently identify the top-ranked system by actively choosing\nsystem pairs for comparison using dueling bandit algorithms. We perform\nextensive experiments with 13 dueling bandits algorithms on 13 NLG evaluation\ndatasets spanning 5 tasks and show that the number of human annotations can be\nreduced by 80%. To further reduce the number of human annotations, we propose\nmodel-based dueling bandit algorithms which combine automatic evaluation\nmetrics with human evaluations. Specifically, we eliminate sub-optimal systems\neven before the human annotation process and perform human evaluations only on\ntest examples where the automatic metric is highly uncertain. This reduces the\nnumber of human annotations required further by 89%. In effect, we show that\nidentifying the top-ranked system requires only a few hundred human\nannotations, which grow linearly with $k$. Lastly, we provide practical\nrecommendations and best practices to identify the top-ranked system\nefficiently. Our code has been made publicly available at\nhttps://github.com/akashkm99/duelnlg", "published": "2022-03-11 16:39:15", "link": "http://arxiv.org/abs/2203.06063v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WLASL-LEX: a Dataset for Recognising Phonological Properties in American\n  Sign Language", "abstract": "Signed Language Processing (SLP) concerns the automated processing of signed\nlanguages, the main means of communication of Deaf and hearing impaired\nindividuals. SLP features many different tasks, ranging from sign recognition\nto translation and production of signed speech, but has been overlooked by the\nNLP community thus far. In this paper, we bring to attention the task of\nmodelling the phonology of sign languages. We leverage existing resources to\nconstruct a large-scale dataset of American Sign Language signs annotated with\nsix different phonological properties. We then conduct an extensive empirical\nstudy to investigate whether data-driven end-to-end and feature-based\napproaches can be optimised to automatically recognise these properties. We\nfind that, despite the inherent challenges of the task, graph-based neural\nnetworks that operate over skeleton features extracted from raw videos are able\nto succeed at the task to a varying degree. Most importantly, we show that this\nperformance pertains even on signs unobserved during training.", "published": "2022-03-11 17:21:24", "link": "http://arxiv.org/abs/2203.06096v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LaPraDoR: Unsupervised Pretrained Dense Retriever for Zero-Shot Text\n  Retrieval", "abstract": "In this paper, we propose LaPraDoR, a pretrained dual-tower dense retriever\nthat does not require any supervised data for training. Specifically, we first\npresent Iterative Contrastive Learning (ICoL) that iteratively trains the query\nand document encoders with a cache mechanism. ICoL not only enlarges the number\nof negative instances but also keeps representations of cached examples in the\nsame hidden space. We then propose Lexicon-Enhanced Dense Retrieval (LEDR) as a\nsimple yet effective way to enhance dense retrieval with lexical matching. We\nevaluate LaPraDoR on the recently proposed BEIR benchmark, including 18\ndatasets of 9 zero-shot text retrieval tasks. Experimental results show that\nLaPraDoR achieves state-of-the-art performance compared with supervised dense\nretrieval models, and further analysis reveals the effectiveness of our\ntraining strategy and objectives. Compared to re-ranking, our lexicon-enhanced\napproach can be run in milliseconds (22.5x faster) while achieving superior\nperformance.", "published": "2022-03-11 18:53:12", "link": "http://arxiv.org/abs/2203.06169v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DeepTrust: A Reliable Financial Knowledge Retrieval Framework For\n  Explaining Extreme Pricing Anomalies", "abstract": "Extreme pricing anomalies may occur unexpectedly without a trivial cause, and\nequity traders typically experience a meticulous process to source disparate\ninformation and analyze its reliability before integrating it into the trusted\nknowledge base. We introduce DeepTrust, a reliable financial knowledge\nretrieval framework on Twitter to explain extreme price moves at speed, while\nensuring data veracity using state-of-the-art NLP techniques. Our proposed\nframework consists of three modules, specialized for anomaly detection,\ninformation retrieval and reliability assessment. The workflow starts with\nidentifying anomalous asset price changes using machine learning models trained\nwith historical pricing data, and retrieving correlated unstructured data from\nTwitter using enhanced queries with dynamic search conditions. DeepTrust\nextrapolates information reliability from tweet features, traces of generative\nlanguage model, argumentation structure, subjectivity and sentiment signals,\nand refine a concise collection of credible tweets for market insights. The\nframework is evaluated on two self-annotated financial anomalies, i.e., Twitter\nand Facebook stock price on 29 and 30 April 2021. The optimal setup outperforms\nthe baseline classifier by 7.75% and 15.77% on F0.5-scores, and 10.55% and\n18.88% on precision, respectively, proving its capability in screening\nunreliable information precisely. At the same time, information retrieval and\nreliability assessment modules are analyzed individually on their effectiveness\nand causes of limitations, with identified subjective and objective factors\nthat influence the performance. As a collaborative project with Refinitiv, this\nframework paves a promising path towards building a scalable commercial\nsolution that assists traders to reach investment decisions on pricing\nanomalies with authenticated knowledge from social media platforms in\nreal-time.", "published": "2022-03-11 06:29:22", "link": "http://arxiv.org/abs/2203.08144v1", "categories": ["q-fin.ST", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "q-fin.ST"}
{"title": "Survey on Automated Short Answer Grading with Deep Learning: from Word\n  Embeddings to Transformers", "abstract": "Automated short answer grading (ASAG) has gained attention in education as a\nmeans to scale educational tasks to the growing number of students. Recent\nprogress in Natural Language Processing and Machine Learning has largely\ninfluenced the field of ASAG, of which we survey the recent research\nadvancements. We complement previous surveys by providing a comprehensive\nanalysis of recently published methods that deploy deep learning approaches. In\nparticular, we focus our analysis on the transition from hand engineered\nfeatures to representation learning approaches, which learn representative\nfeatures for the task at hand automatically from large corpora of data. We\nstructure our analysis of deep learning methods along three categories: word\nembeddings, sequential models, and attention-based methods. Deep learning\nimpacted ASAG differently than other fields of NLP, as we noticed that the\nlearned representations alone do not contribute to achieve the best results,\nbut they rather show to work in a complementary way with hand-engineered\nfeatures. The best performance are indeed achieved by methods that combine the\ncarefully hand-engineered features with the power of the semantic descriptions\nprovided by the latest models, like transformers architectures. We identify\nchallenges and provide an outlook on research direction that can be addressed\nin the future", "published": "2022-03-11 13:47:08", "link": "http://arxiv.org/abs/2204.03503v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hierarchical BERT for Medical Document Understanding", "abstract": "Medical document understanding has gained much attention recently. One\nrepresentative task is the International Classification of Disease (ICD)\ndiagnosis code assignment. Existing work adopts either RNN or CNN as the\nbackbone network because the vanilla BERT cannot handle well long documents\n(>2000 to kens). One issue shared across all these approaches is that they are\nover specific to the ICD code assignment task, losing generality to give the\nwhole document-level and sentence-level embedding. As a result, it is not\nstraight-forward to direct them to other downstream NLU tasks. Motivated by\nthese observations, we propose Medical Document BERT (MDBERT) for long medical\ndocument understanding tasks. MDBERT is not only effective in learning\nrepresentations at different levels of semantics but efficient in encoding long\ndocuments by leveraging a bottom-up hierarchical architecture. Compared to\nvanilla BERT solutions: 1, MDBERT boosts the performance up to relatively 20%\non the MIMIC-III dataset, making it comparable to current SOTA solutions; 2, it\ncuts the computational complexity on self-attention modules to less than 1/100.\nOther than the ICD code assignment, we conduct a variety of other NLU tasks on\na large commercial dataset named as TrialTrove, to showcase MDBERT's strength\nin delivering different levels of semantics.", "published": "2022-03-11 03:50:03", "link": "http://arxiv.org/abs/2204.09600v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Transformer-based Streaming ASR with Cumulative Attention", "abstract": "In this paper, we propose an online attention mechanism, known as cumulative\nattention (CA), for streaming Transformer-based automatic speech recognition\n(ASR). Inspired by monotonic chunkwise attention (MoChA) and head-synchronous\ndecoder-end adaptive computation steps (HS-DACS) algorithms, CA triggers the\nASR outputs based on the acoustic information accumulated at each encoding\ntimestep, where the decisions are made using a trainable device, referred to as\nhalting selector. In CA, all the attention heads of the same decoder layer are\nsynchronised to have a unified halting position. This feature effectively\nalleviates the problem caused by the distinct behaviour of individual heads,\nwhich may otherwise give rise to severe latency issues as encountered by MoChA.\nThe ASR experiments conducted on AIShell-1 and Librispeech datasets demonstrate\nthat the proposed CA-based Transformer system can achieve on par or better\nperformance with significant reduction in latency during inference, when\ncompared to other streaming Transformer systems in literature.", "published": "2022-03-11 03:22:37", "link": "http://arxiv.org/abs/2203.05736v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving the transferability of speech separation by meta-learning", "abstract": "Speech separation aims to separate multiple speech sources from a speech\nmixture. Although speech separation is well-solved on some existing English\nspeech separation benchmarks, it is worthy of more investigation on the\ngeneralizability of speech separation models on the accents or languages unseen\nduring training. This paper adopts meta-learning based methods to improve the\ntransferability of speech separation models. With the meta-learning based\nmethods, we discovered that only using speech data with one accent, the native\nEnglish accent, as our training data, the models still can be adapted to new\nunseen accents on the Speech Accent Archive. We compared the results with a\nhuman-rated native-likeness of accents, showing that the transferability of\nMAML methods has less relation to the similarity of data between the training\nand testing phase compared to the typical transfer learning methods.\nFurthermore, we found that models can deal with different language data from\nthe CommonVoice corpus during the testing phase. Most of all, the MAML methods\noutperform typical transfer learning methods when it comes to new accents, new\nspeakers, new languages, and noisy environments.", "published": "2022-03-11 12:38:09", "link": "http://arxiv.org/abs/2203.05882v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploiting Low-Rank Tensor-Train Deep Neural Networks Based on\n  Riemannian Gradient Descent With Illustrations of Speech Processing", "abstract": "This work focuses on designing low complexity hybrid tensor networks by\nconsidering trade-offs between the model complexity and practical performance.\nFirstly, we exploit a low-rank tensor-train deep neural network (TT-DNN) to\nbuild an end-to-end deep learning pipeline, namely LR-TT-DNN. Secondly, a\nhybrid model combining LR-TT-DNN with a convolutional neural network (CNN),\nwhich is denoted as CNN+(LR-TT-DNN), is set up to boost the performance.\nInstead of randomly assigning large TT-ranks for TT-DNN, we leverage Riemannian\ngradient descent to determine a TT-DNN associated with small TT-ranks.\nFurthermore, CNN+(LR-TT-DNN) consists of convolutional layers at the bottom for\nfeature extraction and several TT layers at the top to solve regression and\nclassification problems. We separately assess the LR-TT-DNN and CNN+(LR-TT-DNN)\nmodels on speech enhancement and spoken command recognition tasks. Our\nempirical evidence demonstrates that the LR-TT-DNN and CNN+(LR-TT-DNN) models\nwith fewer model parameters can outperform the TT-DNN and CNN+(TT-DNN)\ncounterparts.", "published": "2022-03-11 15:55:34", "link": "http://arxiv.org/abs/2203.06031v1", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Infrastructure-free, Deep Learned Urban Noise Monitoring at $\\sim$100mW", "abstract": "The Sounds of New York City (SONYC) wireless sensor network (WSN) has been\nfielded in Manhattan and Brooklyn over the past five years, as part of a larger\nhuman-in-the-loop cyber-physical control system for monitoring, analyzing, and\nmitigating urban noise pollution. We describe the evolution of the 2-tier SONYC\nWSN from an acoustic data collection fabric into a 3-tier in situ noise\ncomplaint monitoring WSN, and its current evaluation. The added tier consists\nof long-range (LoRa), multi-hop networks of a new low-power acoustic mote, MKII\n(\"Mach 2\"), that we have designed and fabricated. MKII motes are notable in\nthree ways: First, they advance machine learning capability at mote-scale in\nthis application domain by introducing a real-time Convolutional Neural Network\n(CNN) based embedding model that is competitive with alternatives while also\nrequiring 10$\\times$ lesser training data and $\\sim$2 orders of magnitude fewer\nruntime resources. Second, they are conveniently deployed relatively far from\nhigher-tier base station nodes without assuming power or network infrastructure\nsupport at operationally relevant sites (such as construction zones), yielding\na relatively low-cost solution. And third, their networking is frequency agile,\nunlike conventional LoRa networks: it tolerates in a distributed,\nself-stabilizing way the variable external interference and link fading in the\ncluttered 902-928MHz ISM band urban environment by dynamically choosing good\nfrequencies using an efficient new method that combines passive and active\nmeasurements.", "published": "2022-03-11 19:44:45", "link": "http://arxiv.org/abs/2203.06220v1", "categories": ["cs.SD", "cs.NI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Superconducting qubits as musical synthesizers for live performance", "abstract": "In the frame of a year-long artistic residency at the Yale Quantum Institute\nin 2019, artist and technologist Spencer Topel and quantum physicists Kyle\nSerniak and Luke Burkhart collaborated to create Quantum Sound, the first-ever\nmusic created and performed directly from measurements of superconducting\nquantum devices. Using analog- and digital-signal-processing sonification\ntechniques, the team transformed GHz-frequency signals from experiments inside\ndilution refrigerators into audible sounds. The project was performed live at\nthe International Festival of Arts and Ideas in New Haven, Connecticut on June\n14, 2019 as a structured improvisation using the synthesis methods described in\nthis chapter. At the interface between research and art, Quantum Sound\nrepresents an earnest attempt to produce a sonic reflection of the quantum\nrealm.", "published": "2022-03-11 13:05:04", "link": "http://arxiv.org/abs/2203.07879v1", "categories": ["quant-ph", "cs.SD", "eess.AS"], "primary_category": "quant-ph"}
{"title": "Acoustic To Articulatory Speech Inversion Using Multi-Resolution\n  Spectro-Temporal Representations Of Speech Signals", "abstract": "Multi-resolution spectro-temporal features of a speech signal represent how\nthe brain perceives sounds by tuning cortical cells to different spectral and\ntemporal modulations. These features produce a higher dimensional\nrepresentation of the speech signals. The purpose of this paper is to evaluate\nhow well the auditory cortex representation of speech signals contribute to\nestimate articulatory features of those corresponding signals. Since obtaining\narticulatory features from acoustic features of speech signals has been a\nchallenging topic of interest for different speech communities, we investigate\nthe possibility of using this multi-resolution representation of speech signals\nas acoustic features. We used U. of Wisconsin X-ray Microbeam (XRMB) database\nof clean speech signals to train a feed-forward deep neural network (DNN) to\nestimate articulatory trajectories of six tract variables. The optimal set of\nmulti-resolution spectro-temporal features to train the model were chosen using\nappropriate scale and rate vector parameters to obtain the best performing\nmodel. Experiments achieved a correlation of 0.675 with ground-truth tract\nvariables. We compared the performance of this speech inversion system with\nprior experiments conducted using Mel Frequency Cepstral Coefficients (MFCCs).", "published": "2022-03-11 07:27:42", "link": "http://arxiv.org/abs/2203.05780v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
