{"title": "Multi-Paragraph Reasoning with Knowledge-enhanced Graph Neural Network", "abstract": "Multi-paragraph reasoning is indispensable for open-domain question answering\n(OpenQA), which receives less attention in the current OpenQA systems. In this\nwork, we propose a knowledge-enhanced graph neural network (KGNN), which\nperforms reasoning over multiple paragraphs with entities. To explicitly\ncapture the entities' relatedness, KGNN utilizes relational facts in knowledge\ngraph to build the entity graph. The experimental results show that KGNN\noutperforms in both distractor and full wiki settings than baselines methods on\nHotpotQA dataset. And our further analysis illustrates KGNN is effective and\nrobust with more retrieved paragraphs.", "published": "2019-11-06 02:34:10", "link": "http://arxiv.org/abs/1911.02170v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guiding Non-Autoregressive Neural Machine Translation Decoding with\n  Reordering Information", "abstract": "Non-autoregressive neural machine translation (NAT) generates each target\nword in parallel and has achieved promising inference acceleration. However,\nexisting NAT models still have a big gap in translation quality compared to\nautoregressive neural machine translation models due to the enormous decoding\nspace. To address this problem, we propose a novel NAT framework named\nReorderNAT which explicitly models the reordering information in the decoding\nprocedure. We further introduce deterministic and non-deterministic decoding\nstrategies that utilize reordering information to narrow the decoding search\nspace in our proposed ReorderNAT. Experimental results on various widely-used\ndatasets show that our proposed model achieves better performance compared to\nexisting NAT models, and even achieves comparable translation quality as\nautoregressive translation models with a significant speedup.", "published": "2019-11-06 06:17:16", "link": "http://arxiv.org/abs/1911.02215v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enriching Conversation Context in Retrieval-based Chatbots", "abstract": "Work on retrieval-based chatbots, like most sequence pair matching tasks, can\nbe divided into Cross-encoders that perform word matching over the pair, and\nBi-encoders that encode the pair separately. The latter has better performance,\nhowever since candidate responses cannot be encoded offline, it is also much\nslower. Lately, multi-layer transformer architectures pre-trained as language\nmodels have been used to great effect on a variety of natural language\nprocessing and information retrieval tasks. Recent work has shown that these\nlanguage models can be used in text-matching scenarios to create Bi-encoders\nthat perform almost as well as Cross-encoders while having a much faster\ninference speed. In this paper, we expand upon this work by developing a\nsequence matching architecture that %takes into account contexts in the\ntraining dataset at inference time. utilizes the entire training set as a\nmakeshift knowledge-base during inference. We perform detailed experiments\ndemonstrating that this architecture can be used to further improve Bi-encoders\nperformance while still maintaining a relatively high inference speed.", "published": "2019-11-06 10:24:45", "link": "http://arxiv.org/abs/1911.02290v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guiding Variational Response Generator to Exploit Persona", "abstract": "Leveraging persona information of users in Neural Response Generators (NRG)\nto perform personalized conversations has been considered as an attractive and\nimportant topic in the research of conversational agents over the past few\nyears. Despite of the promising progresses achieved by recent studies in this\nfield, persona information tends to be incorporated into neural networks in the\nform of user embeddings, with the expectation that the persona can be involved\nvia the End-to-End learning. This paper proposes to adopt the\npersonality-related characteristics of human conversations into variational\nresponse generators, by designing a specific conditional variational\nautoencoder based deep model with two new regularization terms employed to the\nloss function, so as to guide the optimization towards the direction of\ngenerating both persona-aware and relevant responses. Besides, to reasonably\nevaluate the performances of various persona modeling approaches, this paper\nfurther presents three direct persona-oriented metrics from different\nperspectives. The experimental results have shown that our proposed methodology\ncan notably improve the performance of persona-aware response generation, and\nthe metrics are reasonable to evaluate the results.", "published": "2019-11-06 13:53:46", "link": "http://arxiv.org/abs/1911.02390v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SentiLARE: Sentiment-Aware Language Representation Learning with\n  Linguistic Knowledge", "abstract": "Most of the existing pre-trained language representation models neglect to\nconsider the linguistic knowledge of texts, which can promote language\nunderstanding in NLP tasks. To benefit the downstream tasks in sentiment\nanalysis, we propose a novel language representation model called SentiLARE,\nwhich introduces word-level linguistic knowledge including part-of-speech tag\nand sentiment polarity (inferred from SentiWordNet) into pre-trained models. We\nfirst propose a context-aware sentiment attention mechanism to acquire the\nsentiment polarity of each word with its part-of-speech tag by querying\nSentiWordNet. Then, we devise a new pre-training task called label-aware masked\nlanguage model to construct knowledge-aware language representation.\nExperiments show that SentiLARE obtains new state-of-the-art performance on a\nvariety of sentiment analysis tasks.", "published": "2019-11-06 17:05:26", "link": "http://arxiv.org/abs/1911.02493v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing the Factual Correctness of a Summary: A Study of Summarizing\n  Radiology Reports", "abstract": "Neural abstractive summarization models are able to generate summaries which\nhave high overlap with human references. However, existing models are not\noptimized for factual correctness, a critical metric in real-world\napplications. In this work, we develop a general framework where we evaluate\nthe factual correctness of a generated summary by fact-checking it\nautomatically against its reference using an information extraction module. We\nfurther propose a training strategy which optimizes a neural summarization\nmodel with a factual correctness reward via reinforcement learning. We apply\nthe proposed method to the summarization of radiology reports, where factual\ncorrectness is a key requirement. On two separate datasets collected from\nhospitals, we show via both automatic and human evaluation that the proposed\napproach substantially improves the factual correctness and overall quality of\noutputs over a competitive neural summarization system, producing radiology\nsummaries that approach the quality of human-authored ones.", "published": "2019-11-06 18:25:00", "link": "http://arxiv.org/abs/1911.02541v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Domain Adaptation from Limited Data for Question Answering Using\n  Deep Neural Networks", "abstract": "This paper explores domain adaptation for enabling question answering (QA)\nsystems to answer questions posed against documents in new specialized domains.\nCurrent QA systems using deep neural network (DNN) technology have proven\neffective for answering general purpose factoid-style questions. However,\ncurrent general purpose DNN models tend to be ineffective for use in new\nspecialized domains. This paper explores the effectiveness of transfer learning\ntechniques for this problem. In experiments on question answering in the\nautomobile manual domain we demonstrate that standard DNN transfer learning\ntechniques work surprisingly well in adapting DNN models to a new domain using\nlimited amounts of annotated training data in the new domain.", "published": "2019-11-06 22:35:00", "link": "http://arxiv.org/abs/1911.02655v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seq2Emo for Multi-label Emotion Classification Based on Latent Variable\n  Chains Transformation", "abstract": "Emotion detection in text is an important task in NLP and is essential in\nmany applications. Most of the existing methods treat this task as a problem of\nsingle-label multi-class text classification. To predict multiple emotions for\none instance, most of the existing works regard it as a general Multi-label\nClassification (MLC) problem, where they usually either apply a manually\ndetermined threshold on the last output layer of their neural network models or\ntrain multiple binary classifiers and make predictions in the fashion of\none-vs-all. However, compared to labels in the general MLC datasets, the number\nof emotion categories are much fewer (less than 10). Additionally, emotions\ntend to have more correlations with each other. For example, the human usually\ndoes not express \"joy\" and \"anger\" at the same time, but it is very likely to\nhave \"joy\" and \"love\" expressed together. Given this intuition, in this paper,\nwe propose a Latent Variable Chain (LVC) transformation and a tailored model --\nSeq2Emo model that not only naturally predicts multiple emotion labels but also\ntakes into consideration their correlations. We perform the experiments on the\nexisting multi-label emotion datasets as well as on our newly collected\ndatasets. The results show that our model compares favorably with existing\nstate-of-the-art methods.", "published": "2019-11-06 00:08:19", "link": "http://arxiv.org/abs/1911.02147v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CoKE: Contextualized Knowledge Graph Embedding", "abstract": "Knowledge graph embedding, which projects symbolic entities and relations\ninto continuous vector spaces, is gaining increasing attention. Previous\nmethods allow a single static embedding for each entity or relation, ignoring\ntheir intrinsic contextual nature, i.e., entities and relations may appear in\ndifferent graph contexts, and accordingly, exhibit different properties. This\nwork presents Contextualized Knowledge Graph Embedding (CoKE), a novel paradigm\nthat takes into account such contextual nature, and learns dynamic, flexible,\nand fully contextualized entity and relation embeddings. Two types of graph\ncontexts are studied: edges and paths, both formulated as sequences of entities\nand relations. CoKE takes a sequence as input and uses a Transformer encoder to\nobtain contextualized representations. These representations are hence\nnaturally adaptive to the input, capturing contextual meanings of entities and\nrelations therein. Evaluation on a wide variety of public benchmarks verifies\nthe superiority of CoKE in link prediction and path query answering. It\nperforms consistently better than, or at least equally well as current\nstate-of-the-art in almost every case, in particular offering an absolute\nimprovement of 21.0% in H@10 on path query answering. Our code is available at\n\\url{https://github.com/PaddlePaddle/Research/tree/master/KG/CoKE}.", "published": "2019-11-06 02:27:39", "link": "http://arxiv.org/abs/1911.02168v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Hierarchical Contextualized Representation for Named Entity Recognition", "abstract": "Named entity recognition (NER) models are typically based on the architecture\nof Bi-directional LSTM (BiLSTM). The constraints of sequential nature and the\nmodeling of single input prevent the full utilization of global information\nfrom larger scope, not only in the entire sentence, but also in the entire\ndocument (dataset). In this paper, we address these two deficiencies and\npropose a model augmented with hierarchical contextualized representation:\nsentence-level representation and document-level representation. In\nsentence-level, we take different contributions of words in a single sentence\ninto consideration to enhance the sentence representation learned from an\nindependent BiLSTM via label embedding attention mechanism. In document-level,\nthe key-value memory network is adopted to record the document-aware\ninformation for each unique word which is sensitive to similarity of context\ninformation. Our two-level hierarchical contextualized representations are\nfused with each input token embedding and corresponding hidden state of BiLSTM,\nrespectively. The experimental results on three benchmark NER datasets\n(CoNLL-2003 and Ontonotes 5.0 English datasets, CoNLL-2002 Spanish dataset)\nshow that we establish new state-of-the-art results.", "published": "2019-11-06 08:52:52", "link": "http://arxiv.org/abs/1911.02257v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gextext: Disease Network Extraction from Biomedical Literature", "abstract": "PURPOSE: We propose a fully unsupervised method to learn latent disease\nnetworks directly from unstructured biomedical text corpora. This method\naddresses current challenges in unsupervised knowledge extraction, such as the\ndetection of long-range dependencies and requirements for large training\ncorpora. METHODS: Let C be a corpus of n text chunks. Let V be a set of p\ndisease terms occurring in the corpus. Let X indicate the occurrence of V in C.\nGextext identifies disease similarities by positively correlated occurrence\npatterns. This information is combined to generate a graph on which geodesic\ndistance describes dissimilarity. Diseasomes were learned by Gextext and GloVE\non corpora of 100-1000 PubMed abstracts. Similarity matrix estimates were\nvalidated against biomedical semantic similarity metrics and gene profile\nsimilarity. RESULTS: Geodesic distance on Gextext-inferred diseasomes\ncorrelated inversely with external measures of semantic similarity. Gene\nprofile similarity also correlated significant with proximity on the inferred\ngraph. Gextext outperformed GloVE in our experiments. The information contained\non the Gextext graph exceeded the explicit information content within the text.\nCONCLUSIONS: Gextext extracts latent relationships from unstructured text,\nenabling fully unsupervised modelling of diseasome graphs from PubMed\nabstracts.", "published": "2019-11-06 10:57:38", "link": "http://arxiv.org/abs/1911.02562v2", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Word Embedding Algorithms as Generalized Low Rank Models and their\n  Canonical Form", "abstract": "Word embedding algorithms produce very reliable feature representations of\nwords that are used by neural network models across a constantly growing\nmultitude of NLP tasks. As such, it is imperative for NLP practitioners to\nunderstand how their word representations are produced, and why they are so\nimpactful.\n  The present work presents the Simple Embedder framework, generalizing the\nstate-of-the-art existing word embedding algorithms (including Word2vec (SGNS)\nand GloVe) under the umbrella of generalized low rank models. We derive that\nboth of these algorithms attempt to produce embedding inner products that\napproximate pointwise mutual information (PMI) statistics in the corpus. Once\ncast as Simple Embedders, comparison of these models reveals that these\nsuccessful embedders all resemble a straightforward maximum likelihood estimate\n(MLE) of the PMI parametrized by the inner product (between embeddings). This\nMLE induces our proposed novel word embedding model, Hilbert-MLE, as the\ncanonical representative of the Simple Embedder framework.\n  We empirically compare these algorithms with evaluations on 17 different\ndatasets. Hilbert-MLE consistently observes second-best performance on every\nextrinsic evaluation (news classification, sentiment analysis, POS-tagging, and\nsupersense tagging), while the first-best model depends varying on the task.\nMoreover, Hilbert-MLE consistently observes the least variance in results with\nrespect to the random initialization of the weights in bidirectional LSTMs. Our\nempirical results demonstrate that Hilbert-MLE is a very consistent word\nembedding algorithm that can be reliably integrated into existing NLP systems\nto obtain high-quality results.", "published": "2019-11-06 21:40:34", "link": "http://arxiv.org/abs/1911.02639v1", "categories": ["cs.CL", "cs.LG", "68Txx"], "primary_category": "cs.CL"}
{"title": "Unsupervised Domain Adaptation of Contextual Embeddings for Low-Resource\n  Duplicate Question Detection", "abstract": "Answering questions is a primary goal of many conversational systems or\nsearch products. While most current systems have focused on answering questions\nagainst structured databases or curated knowledge graphs, on-line community\nforums or frequently asked questions (FAQ) lists offer an alternative source of\ninformation for question answering systems. Automatic duplicate question\ndetection (DQD) is the key technology need for question answering systems to\nutilize existing online forums like StackExchange. Existing annotations of\nduplicate questions in such forums are community-driven, making them sparse or\neven completely missing for many domains. Therefore, it is important to\ntransfer knowledge from related domains and tasks. Recently, contextual\nembedding models such as BERT have been outperforming many baselines by\ntransferring self-supervised information to downstream tasks. In this paper, we\napply BERT to DQD and advance it by unsupervised adaptation to StackExchange\ndomains using self-supervised learning. We show the effectiveness of this\nadaptation for low-resource settings, where little or no training data is\navailable from the target domain. Our analysis reveals that unsupervised BERT\ndomain adaptation on even small amounts of data boosts the performance of BERT.", "published": "2019-11-06 22:01:18", "link": "http://arxiv.org/abs/1911.02645v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Open Domain Web Keyphrase Extraction Beyond Language Modeling", "abstract": "This paper studies keyphrase extraction in real-world scenarios where\ndocuments are from diverse domains and have variant content quality. We curate\nand release OpenKP, a large scale open domain keyphrase extraction dataset with\nnear one hundred thousand web documents and expert keyphrase annotations. To\nhandle the variations of domain and content quality, we develop BLING-KPE, a\nneural keyphrase extraction model that goes beyond language understanding using\nvisual presentations of documents and weak supervision from search queries.\nExperimental results on OpenKP confirm the effectiveness of BLING-KPE and the\ncontributions of its neural architecture, visual features, and search log weak\nsupervision. Zero-shot evaluations on DUC-2001 demonstrate the improved\ngeneralization ability of learning from the open domain data compared to a\nspecific domain.", "published": "2019-11-06 23:12:56", "link": "http://arxiv.org/abs/1911.02671v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Shaping Visual Representations with Language for Few-shot Classification", "abstract": "By describing the features and abstractions of our world, language is a\ncrucial tool for human learning and a promising source of supervision for\nmachine learning models. We use language to improve few-shot visual\nclassification in the underexplored scenario where natural language task\ndescriptions are available during training, but unavailable for novel tasks at\ntest time. Existing models for this setting sample new descriptions at test\ntime and use those to classify images. Instead, we propose language-shaped\nlearning (LSL), an end-to-end model that regularizes visual representations to\npredict language. LSL is conceptually simpler, more data efficient, and\noutperforms baselines in two challenging few-shot domains.", "published": "2019-11-06 23:47:32", "link": "http://arxiv.org/abs/1911.02683v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Fast Transformer Decoding: One Write-Head is All You Need", "abstract": "Multi-head attention layers, as used in the Transformer neural sequence\nmodel, are a powerful alternative to RNNs for moving information across and\nbetween sequences. While training these layers is generally fast and simple,\ndue to parallelizability across the length of the sequence, incremental\ninference (where such paralleization is impossible) is often slow, due to the\nmemory-bandwidth cost of repeatedly loading the large \"keys\" and \"values\"\ntensors. We propose a variant called multi-query attention, where the keys and\nvalues are shared across all of the different attention \"heads\", greatly\nreducing the size of these tensors and hence the memory bandwidth requirements\nof incremental decoding. We verify experimentally that the resulting models can\nindeed be much faster to decode, and incur only minor quality degradation from\nthe baseline.", "published": "2019-11-06 00:19:05", "link": "http://arxiv.org/abs/1911.02150v1", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
{"title": "A comparison of end-to-end models for long-form speech recognition", "abstract": "End-to-end automatic speech recognition (ASR) models, including both\nattention-based models and the recurrent neural network transducer (RNN-T),\nhave shown superior performance compared to conventional systems. However,\nprevious studies have focused primarily on short utterances that typically last\nfor just a few seconds or, at most, a few tens of seconds. Whether such\narchitectures are practical on long utterances that last from minutes to hours\nremains an open question. In this paper, we both investigate and improve the\nperformance of end-to-end models on long-form transcription. We first present\nan empirical comparison of different end-to-end models on a real world\nlong-form task and demonstrate that the RNN-T model is much more robust than\nattention-based systems in this regime. We next explore two improvements to\nattention-based systems that significantly improve its performance: restricting\nthe attention to be monotonic, and applying a novel decoding algorithm that\nbreaks long utterances into shorter overlapping segments. Combining these two\nimprovements, we show that attention-based end-to-end models can be very\ncompetitive to RNN-T on long-form speech recognition.", "published": "2019-11-06 08:01:16", "link": "http://arxiv.org/abs/1911.02242v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Opinion Summarization as Copycat-Review Generation", "abstract": "Opinion summarization is the task of automatically creating summaries that\nreflect subjective information expressed in multiple documents, such as product\nreviews. While the majority of previous work has focused on the extractive\nsetting, i.e., selecting fragments from input reviews to produce a summary, we\nlet the model generate novel sentences and hence produce abstractive summaries.\nRecent progress in summarization has seen the development of supervised models\nwhich rely on large quantities of document-summary pairs. Since such training\ndata is expensive to acquire, we instead consider the unsupervised setting, in\nother words, we do not use any summaries in training. We define a generative\nmodel for a review collection which capitalizes on the intuition that when\ngenerating a new review given a set of other reviews of a product, we should be\nable to control the \"amount of novelty\" going into the new review or,\nequivalently, vary the extent to which it deviates from the input. At test\ntime, when generating summaries, we force the novelty to be minimal, and\nproduce a text reflecting consensus opinions. We capture this intuition by\ndefining a hierarchical variational autoencoder model. Both individual reviews\nand the products they correspond to are associated with stochastic latent\ncodes, and the review generator (\"decoder\") has direct access to the text of\ninput reviews through the pointer-generator mechanism. Experiments on Amazon\nand Yelp datasets, show that setting at test time the review's latent code to\nits mean, allows the model to produce fluent and coherent summaries reflecting\ncommon opinions.", "published": "2019-11-06 08:20:13", "link": "http://arxiv.org/abs/1911.02247v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Learning to Answer by Learning to Ask: Getting the Best of GPT-2 and\n  BERT Worlds", "abstract": "Automatic question generation aims at the generation of questions from a\ncontext, with the corresponding answers being sub-spans of the given passage.\nWhereas, most of the methods mostly rely on heuristic rules to generate\nquestions, more recently also neural network approaches have been proposed. In\nthis work, we propose a variant of the self-attention Transformer network\narchitectures model to generate meaningful and diverse questions. To this end,\nwe propose an easy to use model consisting of the conjunction of the\nTransformer decoder GPT-2 model with Transformer encoder BERT for the\ndownstream task for question answering. The model is trained in an end-to-end\nfashion, where the language model is trained to produce a question-answer-aware\ninput representation that facilitates to generate an answer focused question.\nOur result of neural question generation from text on the SQuAD 1.1 dataset\nsuggests that our method can produce semantically correct and diverse\nquestions. Additionally, we assessed the performance of our proposed method for\nthe downstream task of question answering. The analysis shows that our proposed\ngeneration & answering collaboration framework relatively improves both tasks\nand is particularly powerful in the semi-supervised setup. The results further\nsuggest a robust and comparably lean pipeline facilitating question generation\nin the small-data regime.", "published": "2019-11-06 13:23:41", "link": "http://arxiv.org/abs/1911.02365v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Designing Evaluations of Machine Learning Models for Subjective\n  Inference: The Case of Sentence Toxicity", "abstract": "Machine Learning (ML) is increasingly applied in real-life scenarios, raising\nconcerns about bias in automatic decision making. We focus on bias as a notion\nof opinion exclusion, that stems from the direct application of traditional ML\npipelines to infer subjective properties. We argue that such ML systems should\nbe evaluated with subjectivity and bias in mind. Considering the lack of\nevaluation standards yet to create evaluation benchmarks, we propose an initial\nlist of specifications to define prior to creating evaluation datasets, in\norder to later accurately evaluate the biases. With the example of a sentence\ntoxicity inference system, we illustrate how the specifications support the\nanalysis of biases related to subjectivity. We highlight difficulties in\ninstantiating these specifications and list future work for the crowdsourcing\ncommunity to help the creation of appropriate evaluation datasets.", "published": "2019-11-06 16:38:19", "link": "http://arxiv.org/abs/1911.02471v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Dimensional Emotion Detection from Categorical Emotion", "abstract": "We present a model to predict fine-grained emotions along the continuous\ndimensions of valence, arousal, and dominance (VAD) with a corpus with\ncategorical emotion annotations. Our model is trained by minimizing the EMD\n(Earth Mover's Distance) loss between the predicted VAD score distribution and\nthe categorical emotion distributions sorted along VAD, and it can\nsimultaneously classify the emotion categories and predict the VAD scores for a\ngiven sentence. We use pre-trained RoBERTa-Large and fine-tune on three\ndifferent corpora with categorical labels and evaluate on EmoBank corpus with\nVAD scores. We show that our approach reaches comparable performance to that of\nthe state-of-the-art classifiers in categorical emotion classification and\nshows significant positive correlations with the ground truth VAD scores. Also,\nfurther training with supervision of VAD labels leads to improved performance\nespecially when dataset is small. We also present examples of predictions of\nappropriate emotion words that are not part of the original annotations.", "published": "2019-11-06 17:16:26", "link": "http://arxiv.org/abs/1911.02499v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Spoken Dialogue System for Spatial Question Answering in a Physical\n  Blocks World", "abstract": "The blocks world is a classic toy domain that has long been used to build and\ntest spatial reasoning systems. Despite its relative simplicity, tackling this\ndomain in its full complexity requires the agent to exhibit a rich set of\nfunctional capabilities, ranging from vision to natural language understanding.\nThere is currently a resurgence of interest in solving problems in such limited\ndomains using modern techniques. In this work we tackle spatial question\nanswering in a holistic way, using a vision system, speech input and output\nmediated by an animated avatar, a dialogue system that robustly interprets\nspatial queries, and a constraint solver that derives answers based on 3-D\nspatial modeling. The contributions of this work include a semantic parser that\nmaps spatial questions into logical forms consistent with a general approach to\nmeaning representation, a dialog manager based on a schema representation, and\na constraint solver for spatial questions that provides answers in agreement\nwith human perception. These and other components are integrated into a\nmulti-modal human-computer interaction pipeline.", "published": "2019-11-06 18:05:13", "link": "http://arxiv.org/abs/1911.02524v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Invariance and identifiability issues for word embeddings", "abstract": "Word embeddings are commonly obtained as optimizers of a criterion function\n$f$ of a text corpus, but assessed on word-task performance using a different\nevaluation function $g$ of the test data. We contend that a possible source of\ndisparity in performance on tasks is the incompatibility between classes of\ntransformations that leave $f$ and $g$ invariant. In particular, word\nembeddings defined by $f$ are not unique; they are defined only up to a class\nof transformations to which $f$ is invariant, and this class is larger than the\nclass to which $g$ is invariant. One implication of this is that the apparent\nsuperiority of one word embedding over another, as measured by word task\nperformance, may largely be a consequence of the arbitrary elements selected\nfrom the respective solution sets. We provide a formal treatment of the above\nidentifiability issue, present some numerical examples, and discuss possible\nresolutions.", "published": "2019-11-06 22:41:04", "link": "http://arxiv.org/abs/1911.02656v1", "categories": ["stat.ML", "cs.CL", "cs.LG", "stat.CO"], "primary_category": "stat.ML"}
{"title": "Domain, Translationese and Noise in Synthetic Data for Neural Machine\n  Translation", "abstract": "The quality of neural machine translation can be improved by leveraging\nadditional monolingual resources to create synthetic training data. Source-side\nmonolingual data can be (forward-)translated into the target language for\nself-training; target-side monolingual data can be back-translated. It has been\nwidely reported that back-translation delivers superior results, but could this\nbe due to artefacts in the test sets? We perform a case study using\nFrench-English news translation task and separate test sets based on their\noriginal languages. We show that forward translation delivers superior gains in\nterms of BLEU on sentences that were originally in the source language,\ncomplementing previous studies which show large improvements with\nback-translation on sentences that were originally in the target language. To\nbetter understand when and why forward and back-translation are effective, we\nstudy the role of domains, translationese, and noise. While translationese\neffects are well known to influence MT evaluation, we also find evidence that\nnews data from different languages shows subtle domain differences, which is\nanother explanation for varying performance on different portions of the test\nset. We perform additional low-resource experiments which demonstrate that\nforward translation is more sensitive to the quality of the initial translation\nsystem than back-translation, and tends to perform worse in low-resource\nsettings.", "published": "2019-11-06 17:30:57", "link": "http://arxiv.org/abs/1911.03362v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "To Populate is To Regulate", "abstract": "We examine the effects of instantiating Lewis signaling games within a\npopulation of speaker and listener agents with the aim of producing a set of\ngeneral and robust representations of unstructured pixel data. Preliminary\nexperiments suggest that the set of representations associated with languages\ngenerated within a population outperform those generated between a single\nspeaker-listener pair on this objective, making a case for the adoption of\npopulation-based approaches in emergent communication studies. Furthermore,\npost-hoc analysis reveals that population-based learning induces a number of\nnovel factors to the conventional emergent communication setup, inviting a wide\nrange of future research questions regarding communication dynamics and the\nflow of information within them.", "published": "2019-11-06 23:51:45", "link": "http://arxiv.org/abs/1911.04362v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The sound of my voice: speaker representation loss for target voice\n  separation", "abstract": "Content and style representations have been widely studied in the field of\nstyle transfer. In this paper, we propose a new loss function using speaker\ncontent representation for audio source separation, and we call it speaker\nrepresentation loss. The objective is to extract the target speaker voice from\nthe noisy input and also remove it from the residual components. Compared to\nthe conventional spectral reconstruction, our proposed framework maximizes the\nuse of target speaker information by minimizing the distance between the\nspeaker representations of reference and source separation output. We also\npropose triplet speaker representation loss as an additional criterion to\nremove the target speaker information from residual spectrogram output.\nVoiceFilter framework is adopted to evaluate source separation performance\nusing the VCTK database, and we achieved improved performances compared to the\nbaseline loss function without any additional network parameters.", "published": "2019-11-06 14:39:47", "link": "http://arxiv.org/abs/1911.02411v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Finding Strength in Weakness: Learning to Separate Sounds with Weak\n  Supervision", "abstract": "While there has been much recent progress using deep learning techniques to\nseparate speech and music audio signals, these systems typically require large\ncollections of isolated sources during the training process. When extending\naudio source separation algorithms to more general domains such as\nenvironmental monitoring, it may not be possible to obtain isolated signals for\ntraining. Here, we propose objective functions and network architectures that\nenable training a source separation system with weak labels. In this scenario,\nweak labels are defined in contrast with strong time-frequency (TF) labels such\nas those obtained from isolated sources, and refer either to frame-level weak\nlabels where one only has access to the time periods when different sources are\nactive in an audio mixture, or to clip-level weak labels that only indicate the\npresence or absence of sounds in an entire audio clip. We train a separator\nthat estimates a TF mask for each type of sound event, using a sound event\nclassifier as an assessor of the separator's performance to bridge the gap\nbetween the TF-level separation and the ground truth weak labels only available\nat the frame or clip level. Our objective function requires the classifier\napplied to a separated source to assign high probability to the class\ncorresponding to that source and low probability to all other classes. The\nobjective function also enforces that the separated sources sum up to the\nmixture. We benchmark the performance of our algorithm using synthetic mixtures\nof overlapping events created from a database of sounds recorded in urban\nenvironments. Compared to training a network using isolated sources, our model\nachieves somewhat lower but still significant SI-SDR improvement, even in\nscenarios with significant sound event overlap.", "published": "2019-11-06 03:36:55", "link": "http://arxiv.org/abs/1911.02182v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Addressing Ambiguity of Emotion Labels Through Meta-Learning", "abstract": "Emotion labels in emotion recognition corpora are highly noisy and ambiguous,\ndue to the annotators' subjective perception of emotions. Such ambiguity may\nintroduce errors in automatic classification and affect the overall\nperformance. We therefore propose a dynamic label correction and sample\ncontribution weight estimation model. Our model is based on a standard BLSTM\nmodel with attention with two extra parameters. The first learns a new\ncorrected label distribution, and is aimed to fix the inaccurate labels from\nthe dataset. The other instead estimates the contribution of each sample to the\ntraining process, and is aimed to ignore the ambiguous and noisy samples while\ngiving higher weight to the clear ones. We train our model through an\nalternating optimization method, where in the first epoch we update the neural\nnetwork parameters, and in the second we keep them fixed to update the label\ncorrection and sample importance parameters. When training and evaluating our\nmodel on the IEMOCAP dataset, we obtained a weighted accuracy (WA) and\nunweighted accuracy (UA) of respectively 65.9% and 61.4%. This yielded an\nabsolute improvement of 2.5%, 2.7% respectively compared to a BLSTM with\nattention baseline, trained on the corpus gold labels.", "published": "2019-11-06 06:21:31", "link": "http://arxiv.org/abs/1911.02216v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Speed Submission to DIHARD II: Contributions & Lessons Learned", "abstract": "This paper describes the speaker diarization systems developed for the Second\nDIHARD Speech Diarization Challenge (DIHARD II) by the Speed team. Besides\ndescribing the system, which considerably outperformed the challenge baselines,\nwe also focus on the lessons learned from numerous approaches that we tried for\nsingle and multi-channel systems. We present several components of our\ndiarization system, including categorization of domains, speech enhancement,\nspeech activity detection, speaker embeddings, clustering methods,\nresegmentation, and system fusion. We analyze and discuss the effect of each\nsuch component on the overall diarization performance within the realistic\nsettings of the challenge.", "published": "2019-11-06 13:53:18", "link": "http://arxiv.org/abs/1911.02388v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An End-to-end Approach for Lexical Stress Detection based on Transformer", "abstract": "The dominant automatic lexical stress detection method is to split the\nutterance into syllable segments using phoneme sequence and their time-aligned\nboundaries. Then we extract features from syllable to use classification method\nto classify the lexical stress. However, we can't get very accurate time\nboundaries of each phoneme and we have to design some features in the syllable\nsegments to classify the lexical stress. Therefore, we propose a end-to-end\napproach using sequence to sequence model of transformer to estimate lexical\nstress. For this, we train transformer model using feature sequence of audio\nand their phoneme sequence with lexical stress marks. During the recognition\nprocess, the recognized phoneme sequence is restricted according to the\noriginal standard phoneme sequence without lexical stress marks, but the\nlexical stress mark of each phoneme is not limited. We train the model in\ndifferent subset of Librispeech and do lexical stress recognition in TIMIT and\nL2-ARCTIC dataset. For all subsets, the end-to-end model will perform better\nthan the syllable segments classification method. Our method can achieve a\n6.36% phoneme error rate on the TIMIT dataset, which exceeds the 7.2% error\nrate in other studies.", "published": "2019-11-06 11:29:37", "link": "http://arxiv.org/abs/1911.04862v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
