{"title": "Chat Detection in an Intelligent Assistant: Combining Task-oriented and\n  Non-task-oriented Spoken Dialogue Systems", "abstract": "Recently emerged intelligent assistants on smartphones and home electronics\n(e.g., Siri and Alexa) can be seen as novel hybrids of domain-specific\ntask-oriented spoken dialogue systems and open-domain non-task-oriented ones.\nTo realize such hybrid dialogue systems, this paper investigates determining\nwhether or not a user is going to have a chat with the system. To address the\nlack of benchmark datasets for this task, we construct a new dataset consisting\nof 15; 160 utterances collected from the real log data of a commercial\nintelligent assistant (and will release the dataset to facilitate future\nresearch activity). In addition, we investigate using tweets and Web search\nqueries for handling open-domain user utterances, which characterize the task\nof chat detection. Experiments demonstrated that, while simple supervised\nmethods are effective, the use of the tweets and search queries further\nimproves the F1-score from 86.21 to 87.53.", "published": "2017-05-02 00:23:43", "link": "http://arxiv.org/abs/1705.00746v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Teacher-Student Framework for Zero-Resource Neural Machine Translation", "abstract": "While end-to-end neural machine translation (NMT) has made remarkable\nprogress recently, it still suffers from the data scarcity problem for\nlow-resource language pairs and domains. In this paper, we propose a method for\nzero-resource NMT by assuming that parallel sentences have close probabilities\nof generating a sentence in a third language. Based on this assumption, our\nmethod is able to train a source-to-target NMT model (\"student\") without\nparallel corpora available, guided by an existing pivot-to-target NMT model\n(\"teacher\") on a source-pivot parallel corpus. Experimental results show that\nthe proposed method significantly improves over a baseline pivot-based model by\n+3.0 BLEU points across various language pairs.", "published": "2017-05-02 01:14:06", "link": "http://arxiv.org/abs/1705.00753v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Source Syntax for Neural Machine Translation", "abstract": "Even though a linguistics-free sequence to sequence model in neural machine\ntranslation (NMT) has certain capability of implicitly learning syntactic\ninformation of source sentences, this paper shows that source syntax can be\nexplicitly incorporated into NMT effectively to provide further improvements.\nSpecifically, we linearize parse trees of source sentences to obtain structural\nlabel sequences. On the basis, we propose three different sorts of encoders to\nincorporate source syntax into NMT: 1) Parallel RNN encoder that learns word\nand label annotation vectors parallelly; 2) Hierarchical RNN encoder that\nlearns word and label annotation vectors in a two-level hierarchy; and 3) Mixed\nRNN encoder that stitchingly learns word and label annotation vectors over\nsequences where words and labels are mixed. Experimentation on\nChinese-to-English translation demonstrates that all the three proposed\nsyntactic encoders are able to improve translation accuracy. It is interesting\nto note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best\nperformance with an significant improvement of 1.4 BLEU points. Moreover, an\nin-depth analysis from several perspectives is provided to reveal how source\nsyntax benefits NMT.", "published": "2017-05-02 15:21:46", "link": "http://arxiv.org/abs/1705.01020v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity Linking with people entity on Wikipedia", "abstract": "This paper introduces a new model that uses named entity recognition,\ncoreference resolution, and entity linking techniques, to approach the task of\nlinking people entities on Wikipedia people pages to their corresponding\nWikipedia pages if applicable. Our task is different from general and\ntraditional entity linking because we are working in a limited domain, namely,\npeople entities, and we are including pronouns as entities, whereas in the\npast, pronouns were never considered as entities in entity linking. We have\nbuilt 2 models, both outperforms our baseline model significantly. The purpose\nof our project is to build a model that could be use to generate cleaner data\nfor future entity linking tasks. Our contribution include a clean data set\nconsisting of 50Wikipedia people pages, and 2 entity linking models,\nspecifically tuned for this domain.", "published": "2017-05-02 16:06:03", "link": "http://arxiv.org/abs/1705.01042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STAIR Captions: Constructing a Large-Scale Japanese Image Caption\n  Dataset", "abstract": "In recent years, automatic generation of image descriptions (captions), that\nis, image captioning, has attracted a great deal of attention. In this paper,\nwe particularly consider generating Japanese captions for images. Since most\navailable caption datasets have been constructed for English language, there\nare few datasets for Japanese. To tackle this problem, we construct a\nlarge-scale Japanese image caption dataset based on images from MS-COCO, which\nis called STAIR Captions. STAIR Captions consists of 820,310 Japanese captions\nfor 164,062 images. In the experiment, we show that a neural network trained\nusing STAIR Captions can generate more natural and better Japanese captions,\ncompared to those generated using English-Japanese machine translation after\ngenerating English captions.", "published": "2017-05-02 07:07:55", "link": "http://arxiv.org/abs/1705.00823v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Deep Neural Machine Translation with Linear Associative Unit", "abstract": "Deep Neural Networks (DNNs) have provably enhanced the state-of-the-art\nNeural Machine Translation (NMT) with their capability in modeling complex\nfunctions and capturing complex linguistic structures. However NMT systems with\ndeep architecture in their encoder or decoder RNNs often suffer from severe\ngradient diffusion due to the non-linear recurrent activations, which often\nmake the optimization much more difficult. To address this problem we propose\nnovel linear associative units (LAU) to reduce the gradient propagation length\ninside the recurrent unit. Different from conventional approaches (LSTM unit\nand GRU), LAUs utilizes linear associative connections between input and output\nof the recurrent unit, which allows unimpeded information flow through both\nspace and time direction. The model is quite simple, but it is surprisingly\neffective. Our empirical study on Chinese-English translation shows that our\nmodel with proper configuration can improve by 11.7 BLEU upon Groundhog and the\nbest reported results in the same setting. On WMT14 English-German task and a\nlarger WMT14 English-French task, our model achieves comparable results with\nthe state-of-the-art.", "published": "2017-05-02 08:58:17", "link": "http://arxiv.org/abs/1705.00861v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Increasing Papers' Discoverability with Precise Semantic Labeling: the\n  sci.AI Platform", "abstract": "The number of published findings in biomedicine increases continually. At the\nsame time, specifics of the domain's terminology complicates the task of\nrelevant publications retrieval. In the current research, we investigate\ninfluence of terms' variability and ambiguity on a paper's likelihood of being\nretrieved. We obtained statistics that demonstrate significance of the issue\nand its challenges, followed by presenting the sci.AI platform, which allows\nprecise terms labeling as a resolution.", "published": "2017-05-02 17:04:42", "link": "http://arxiv.org/abs/1705.08321v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Fuzzy Approach Topic Discovery in Health and Medical Corpora", "abstract": "The majority of medical documents and electronic health records (EHRs) are in\ntext format that poses a challenge for data processing and finding relevant\ndocuments. Looking for ways to automatically retrieve the enormous amount of\nhealth and medical knowledge has always been an intriguing topic. Powerful\nmethods have been developed in recent years to make the text processing\nautomatic. One of the popular approaches to retrieve information based on\ndiscovering the themes in health & medical corpora is topic modeling, however,\nthis approach still needs new perspectives. In this research we describe fuzzy\nlatent semantic analysis (FLSA), a novel approach in topic modeling using fuzzy\nperspective. FLSA can handle health & medical corpora redundancy issue and\nprovides a new method to estimate the number of topics. The quantitative\nevaluations show that FLSA produces superior performance and features to latent\nDirichlet allocation (LDA), the most popular topic model.", "published": "2017-05-02 14:29:14", "link": "http://arxiv.org/abs/1705.00995v2", "categories": ["stat.ML", "cs.CL", "cs.IR", "62-07, 62-09, 68T50, 03B52, 03E72", "H.3.1; H.3.3; I.2.7; I.7; I.5; I.2.3"], "primary_category": "stat.ML"}
