{"title": "TTRL: Test-Time Reinforcement Learning", "abstract": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the Maj@N metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model, and\napproach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks, and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL", "published": "2025-04-22 17:59:56", "link": "http://arxiv.org/abs/2504.16084v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Survey of Video Diffusion Models: Foundations, Implementations, and Applications", "abstract": "Recent advances in diffusion models have revolutionized video generation,\noffering superior temporal consistency and visual quality compared to\ntraditional generative adversarial networks-based approaches. While this\nemerging field shows tremendous promise in applications, it faces significant\nchallenges in motion consistency, computational efficiency, and ethical\nconsiderations. This survey provides a comprehensive review of diffusion-based\nvideo generation, examining its evolution, technical foundations, and practical\napplications. We present a systematic taxonomy of current methodologies,\nanalyze architectural innovations and optimization strategies, and investigate\napplications across low-level vision tasks such as denoising and\nsuper-resolution. Additionally, we explore the synergies between diffusionbased\nvideo generation and related domains, including video representation learning,\nquestion answering, and retrieval. Compared to the existing surveys (Lei et\nal., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which\nfocus on specific aspects of video generation, such as human video synthesis\n(Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our\nwork provides a broader, more updated, and more fine-grained perspective on\ndiffusion-based approaches with a special section for evaluation metrics,\nindustry solutions, and training engineering techniques in video generation.\nThis survey serves as a foundational resource for researchers and practitioners\nworking at the intersection of diffusion models and video generation, providing\ninsights into both the theoretical frameworks and practical implementations\nthat drive this rapidly evolving field. A structured list of related works\ninvolved in this survey is also available on\nhttps://github.com/Eyeline-Research/Survey-Video-Diffusion.", "published": "2025-04-22 17:59:17", "link": "http://arxiv.org/abs/2504.16081v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models", "abstract": "We introduce PHYBench, a novel, high-quality benchmark designed for\nevaluating reasoning capabilities of large language models (LLMs) in physical\ncontexts. PHYBench consists of 500 meticulously curated physics problems based\non real-world physical scenarios, designed to assess the ability of models to\nunderstand and reason about realistic physical processes. Covering mechanics,\nelectromagnetism, thermodynamics, optics, modern physics, and advanced physics,\nthe benchmark spans difficulty levels from high school exercises to\nundergraduate problems and Physics Olympiad challenges. Additionally, we\npropose the Expression Edit Distance (EED) Score, a novel evaluation metric\nbased on the edit distance between mathematical expressions, which effectively\ncaptures differences in model reasoning processes and results beyond\ntraditional binary scoring methods. We evaluate various LLMs on PHYBench and\ncompare their performance with human experts. Our results reveal that even\nstate-of-the-art reasoning models significantly lag behind human experts,\nhighlighting their limitations and the need for improvement in complex physical\nreasoning scenarios. Our benchmark results and dataset are publicly available\nat https://phybench-official.github.io/phybench-demo/.", "published": "2025-04-22 17:53:29", "link": "http://arxiv.org/abs/2504.16074v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation", "abstract": "Recent advancements in visual language models (VLMs) have notably enhanced\ntheir capabilities in handling complex Graphical User Interface (GUI)\ninteraction tasks. Despite these improvements, current frameworks often\nstruggle to generate correct actions in challenging GUI environments.\nState-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source\nVLMs for GUI tasks requires significant resources. Additionally, existing\ntrajectory-level evaluation and refinement techniques frequently fall short due\nto delayed feedback and local optimization issues. To address these challenges,\nwe propose an approach that guides VLM agents with process supervision by a\nreward model during GUI navigation and control at inference time. This guidance\nallows the VLM agent to optimize actions at each inference step, thereby\nimproving performance in both static and dynamic environments. In particular,\nour method demonstrates significant performance gains in three GUI navigation\ntasks, achieving a 3.4% improvement in single step action accuracy for static\nenvironments, along with a around 33% increase in task success rate in one\ndynamic environment. With further integration of trajectory reflection and\nretry mechanisms, we also demonstrate even greater enhancement in task success.", "published": "2025-04-22 17:52:42", "link": "http://arxiv.org/abs/2504.16073v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Python Tool for Reconstructing Full News Text from GDELT", "abstract": "News data have become an essential resource across various disciplines,\nincluding economics, finance, management, social sciences, and computer\nscience. Researchers leverage newspaper articles to study economic trends,\nmarket dynamics, corporate strategies, public perception, political discourse,\nand the evolution of public opinion. Additionally, news datasets have been\ninstrumental in training large-scale language models, with applications in\nsentiment analysis, fake news detection, and automated news summarization.\nDespite their significance, access to comprehensive news corpora remains a key\nchallenge. Many full-text news providers, such as Factiva and LexisNexis,\nrequire costly subscriptions, while free alternatives often suffer from\nincomplete data and transparency issues. This paper presents a novel approach\nto obtaining full-text newspaper articles at near-zero cost by leveraging data\nfrom the Global Database of Events, Language, and Tone (GDELT). Specifically,\nwe focus on the GDELT Web News NGrams 3.0 dataset, which provides\nhigh-frequency updates of n-grams extracted from global online news sources. We\nprovide Python code to reconstruct full-text articles from these n-grams by\nidentifying overlapping textual fragments and intelligently merging them. Our\nmethod enables researchers to access structured, large-scale newspaper data for\ntext analysis while overcoming the limitations of existing proprietary\ndatasets. The proposed approach enhances the accessibility of news data for\nempirical research, facilitating applications in economic forecasting,\ncomputational social science, and natural language processing.", "published": "2025-04-22 17:40:42", "link": "http://arxiv.org/abs/2504.16063v1", "categories": ["cs.CL", "cs.DB", "cs.IR", "I.2.7; H.2.8; H.3.1"], "primary_category": "cs.CL"}
{"title": "Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation", "abstract": "Referring Expression Generation (REG) is a core task for evaluating the\npragmatic competence of vision-language systems, requiring not only accurate\nsemantic grounding but also adherence to principles of cooperative\ncommunication (Grice, 1975). However, current evaluations of vision-language\nmodels (VLMs) often overlook the pragmatic dimension, reducing REG to a\nregion-based captioning task and neglecting Gricean maxims. In this work, we\nrevisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of\n1.5k images annotated with both written and spoken referring expressions.\nThrough a systematic evaluation of state-of-the-art VLMs, we identify three key\nfailures of pragmatic competence: (1) failure to uniquely identify the\nreferent, (2) inclusion of excessive or irrelevant information, and (3)\nmisalignment with human pragmatic preference, such as the underuse of minimal\nspatial cues. We also show that standard automatic evaluations fail to capture\nthese pragmatic violations, reinforcing superficial cues rather than genuine\nreferential success. Our findings call for a renewed focus on pragmatically\ninformed models and evaluation frameworks that align with real human\ncommunication.", "published": "2025-04-22 17:37:16", "link": "http://arxiv.org/abs/2504.16060v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability", "abstract": "Artificial Intelligence (AI) has increasingly influenced modern society,\nrecently in particular through significant advancements in Large Language\nModels (LLMs). However, high computational and storage demands of LLMs still\nlimit their deployment in resource-constrained environments. Knowledge\ndistillation addresses this challenge by training a small student model from a\nlarger teacher model. Previous research has introduced several distillation\nmethods for both generating training data and for training the student model.\nDespite their relevance, the effects of state-of-the-art distillation methods\non model performance and explainability have not been thoroughly investigated\nand compared. In this work, we enlarge the set of available methods by applying\ncritique-revision prompting to distillation for data generation and by\nsynthesizing existing methods for training. For these methods, we provide a\nsystematic comparison based on the widely used Commonsense Question-Answering\n(CQA) dataset. While we measure performance via student model accuracy, we\nemploy a human-grounded study to evaluate explainability. We contribute new\ndistillation methods and their comparison in terms of both performance and\nexplainability. This should further advance the distillation of small language\nmodels and, thus, contribute to broader applicability and faster diffusion of\nLLM technology.", "published": "2025-04-22 17:32:48", "link": "http://arxiv.org/abs/2504.16056v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement", "abstract": "State space models (SSMs) have emerged as an efficient alternative to\nTransformer models for language modeling, offering linear computational\ncomplexity and constant memory usage as context length increases. However,\ndespite their efficiency in handling long contexts, recent studies have shown\nthat SSMs, such as Mamba models, generally underperform compared to\nTransformers in long-context understanding tasks. To address this significant\nshortfall and achieve both efficient and accurate long-context understanding,\nwe propose LongMamba, a training-free technique that significantly enhances the\nlong-context capabilities of Mamba models. LongMamba builds on our discovery\nthat the hidden channels in Mamba can be categorized into local and global\nchannels based on their receptive field lengths, with global channels primarily\nresponsible for long-context capability. These global channels can become the\nkey bottleneck as the input context lengthens. Specifically, when input lengths\nlargely exceed the training sequence length, global channels exhibit\nlimitations in adaptively extend their receptive fields, leading to Mamba's\npoor long-context performance. The key idea of LongMamba is to mitigate the\nhidden state memory decay in these global channels by preventing the\naccumulation of unimportant tokens in their memory. This is achieved by first\nidentifying critical tokens in the global channels and then applying token\nfiltering to accumulate only those critical tokens. Through extensive\nbenchmarking across synthetic and real-world long-context scenarios, LongMamba\nsets a new standard for Mamba's long-context performance, significantly\nextending its operational range without requiring additional training. Our code\nis available at https://github.com/GATECH-EIC/LongMamba.", "published": "2025-04-22 17:30:36", "link": "http://arxiv.org/abs/2504.16053v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Certified Mitigation of Worst-Case LLM Copyright Infringement", "abstract": "The exposure of large language models (LLMs) to copyrighted material during\npre-training raises concerns about unintentional copyright infringement post\ndeployment. This has driven the development of \"copyright takedown\" methods,\npost-training approaches aimed at preventing models from generating content\nsubstantially similar to copyrighted ones. While current mitigation approaches\nare somewhat effective for average-case risks, we demonstrate that they\noverlook worst-case copyright risks exhibits by the existence of long, verbatim\nquotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet\nhighly effective inference-time approach that provides certified copyright\ntakedown. Our method repeatedly interleaves quote detection with rewriting\ntechniques to transform potentially infringing segments. By leveraging\nefficient data sketches (Bloom filters), our approach enables scalable\ncopyright screening even for large-scale real-world corpora. When quotes beyond\na length threshold cannot be removed, the system can abstain from responding,\noffering certified risk reduction. Experimental results show that BloomScrub\nreduces infringement risk, preserves utility, and accommodates different levels\nof enforcement stringency with adaptive abstention. Our results suggest that\nlightweight, inference-time methods can be surprisingly effective for copyright\nprevention.", "published": "2025-04-22 17:16:53", "link": "http://arxiv.org/abs/2504.16046v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Methods for Recognizing Nested Terms", "abstract": "In this paper, we describe our participation in the RuTermEval competition\ndevoted to extracting nested terms. We apply the Binder model, which was\npreviously successfully applied to the recognition of nested named entities, to\nextract nested terms. We obtained the best results of term recognition in all\nthree tracks of the RuTermEval competition. In addition, we study the new task\nof recognition of nested terms from flat training data annotated with terms\nwithout nestedness. We can conclude that several approaches we proposed in this\nwork are viable enough to retrieve nested terms effectively without nested\nlabeling of them.", "published": "2025-04-22 16:15:37", "link": "http://arxiv.org/abs/2504.16007v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CAPO: Cost-Aware Prompt Optimization", "abstract": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency.", "published": "2025-04-22 16:14:31", "link": "http://arxiv.org/abs/2504.16005v1", "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "How Private is Your Attention? Bridging Privacy with In-Context Learning", "abstract": "In-context learning (ICL)-the ability of transformer-based models to perform\nnew tasks from examples provided at inference time-has emerged as a hallmark of\nmodern language models. While recent works have investigated the mechanisms\nunderlying ICL, its feasibility under formal privacy constraints remains\nlargely unexplored. In this paper, we propose a differentially private\npretraining algorithm for linear attention heads and present the first\ntheoretical analysis of the privacy-accuracy trade-off for ICL in linear\nregression. Our results characterize the fundamental tension between\noptimization and privacy-induced noise, formally capturing behaviors observed\nin private training via iterative methods. Additionally, we show that our\nmethod is robust to adversarial perturbations of training prompts, unlike\nstandard ridge regression. All theoretical findings are supported by extensive\nsimulations across diverse settings.", "published": "2025-04-22 16:05:26", "link": "http://arxiv.org/abs/2504.16000v1", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Few-shot Hate Speech Detection Based on the MindSpore Framework", "abstract": "The proliferation of hate speech on social media poses a significant threat\nto online communities, requiring effective detection systems. While deep\nlearning models have shown promise, their performance often deteriorates in\nfew-shot or low-resource settings due to reliance on large annotated corpora.\nTo address this, we propose MS-FSLHate, a prompt-enhanced neural framework for\nfew-shot hate speech detection implemented on the MindSpore deep learning\nplatform. The model integrates learnable prompt embeddings, a CNN-BiLSTM\nbackbone with attention pooling, and synonym-based adversarial data\naugmentation to improve generalization. Experimental results on two benchmark\ndatasets-HateXplain and HSOL-demonstrate that our approach outperforms\ncompetitive baselines in precision, recall, and F1-score. Additionally, the\nframework shows high efficiency and scalability, suggesting its suitability for\ndeployment in resource-constrained environments. These findings highlight the\npotential of combining prompt-based learning with adversarial augmentation for\nrobust and adaptable hate speech detection in few-shot scenarios.", "published": "2025-04-22 15:42:33", "link": "http://arxiv.org/abs/2504.15987v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models", "abstract": "The demand for efficient natural language processing (NLP) systems has led to\nthe development of lightweight language models. Previous work in this area has\nprimarily focused on manual design or training-based neural architecture search\n(NAS) methods. Recently, zero-shot NAS methods have been proposed for\nevaluating language models without the need for training. However, prevailing\napproaches to zero-shot NAS often face challenges such as biased evaluation\nmetrics and computational inefficiencies. In this paper, we introduce\nweight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored\nfor lightweight language models. Our approach utilizes two evaluation proxies:\nthe parameter count and the number of principal components with cumulative\ncontribution exceeding $\\eta$ in the feed-forward neural (FFN) layer.\nAdditionally, by eliminating the need for gradient computations, we optimize\nthe evaluation time, thus enhancing the efficiency of designing and evaluating\nlightweight language models. We conduct a comparative analysis on the GLUE and\nSQuAD datasets to evaluate our approach. The results demonstrate that our\nmethod significantly reduces training time compared to one-shot NAS methods and\nachieves higher scores in the testing phase compared to previous\nstate-of-the-art training-based methods. Furthermore, we perform ranking\nevaluations on a dataset sampled from the FlexiBERT search space. Our approach\nexhibits superior ranking correlation and further reduces solving time compared\nto other zero-shot NAS methods that require gradient computation.", "published": "2025-04-22 15:33:01", "link": "http://arxiv.org/abs/2504.15983v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity", "abstract": "Large Language Models (LLMs) are increasingly leveraged for translation tasks\nbut often fall short when translating inclusive language -- such as texts\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\nprotocols. Because these challenges span both computational and societal\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\ntranslation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\ndesigned to evaluate non-binary gender biases in machine translation systems\nfrom English to French. FairTranslate includes 2418 English-French sentence\npairs related to occupations, annotated with rich metadata such as the\nstereotypical alignment of the occupation, grammatical gender indicator\nambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\nreveal substantial biases in gender representation across LLMs, highlighting\npersistent challenges in achieving equitable outcomes in machine translation.\nThese findings underscore the need for focused strategies and interventions\naimed at ensuring fair and inclusive language usage in LLM-based translation\nsystems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and\ndisclose the code for all experiments on GitHub.", "published": "2025-04-22 14:35:16", "link": "http://arxiv.org/abs/2504.15941v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning", "abstract": "Recent work shows that reinforcement learning(RL) can markedly sharpen the\nreasoning ability of large language models (LLMs) by prompting them to \"think\nbefore answering.\" Yet whether and how these gains transfer to audio-language\nreasoning remains largely unexplored. We extend the Group-Relative Policy\nOptimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model\n(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage\nregimen supervised fine-tuning on structured and unstructured\nchains-of-thought, followed by curriculum-guided GRPO, we systematically\ncompare implicit vs. explicit, and structured vs. free form reasoning under\nidentical architectures. Our structured audio reasoning model, SARI (Structured\nAudio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a\n16.35% improvement in average accuracy over the base model\nQwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni\nreaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.\nAblation experiments show that on the base model we use: (i) SFT warm-up is\nimportant for stable RL training, (ii) structured chains yield more robust\ngeneralization than unstructured ones, and (iii) easy-to-hard curricula\naccelerate convergence and improve final performance. These findings\ndemonstrate that explicit, structured reasoning and curriculum learning\nsubstantially enhances audio-language understanding.", "published": "2025-04-22 13:41:26", "link": "http://arxiv.org/abs/2504.15900v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Early Exit in Reasoning Models", "abstract": "Recent advances in large reasoning language models (LRLMs) rely on test-time\nscaling, which extends long chain-of-thought (CoT) generation to solve complex\ntasks. However, overthinking in long CoT not only slows down the efficiency of\nproblem solving, but also risks accuracy loss due to the extremely detailed or\nredundant reasoning steps. We propose a simple yet effective method that allows\nLLMs to self-truncate CoT sequences by early exit during generation. Instead of\nrelying on fixed heuristics, the proposed method monitors model behavior at\npotential reasoning transition points (e.g.,\"Wait\" tokens) and dynamically\nterminates the next reasoning chain's generation when the model exhibits high\nconfidence in a trial answer. Our method requires no additional training and\ncan be seamlessly integrated into existing o1-like reasoning LLMs. Experiments\non multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024\nshow that the proposed method is consistently effective on deepseek-series\nreasoning LLMs, reducing the length of CoT sequences by an average of 31% to\n43% while improving accuracy by 1.7% to 5.7%.", "published": "2025-04-22 13:36:53", "link": "http://arxiv.org/abs/2504.15895v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based Sentiment Analysis", "abstract": "Multimodal aspect-based sentiment classification (MASC) is an emerging task\ndue to an increase in user-generated multimodal content on social platforms,\naimed at predicting sentiment polarity toward specific aspect targets (i.e.,\nentities or attributes explicitly mentioned in text-image pairs). Despite\nextensive efforts and significant achievements in existing MASC, substantial\ngaps remain in understanding fine-grained visual content and the cognitive\nrationales derived from semantic content and impressions (cognitive\ninterpretations of emotions evoked by image content). In this study, we present\nChimera: a cognitive and aesthetic sentiment causality understanding framework\nto derive fine-grained holistic features of aspects and infer the fundamental\ndrivers of sentiment expression from both semantic perspectives and\naffective-cognitive resonance (the synergistic effect between emotional\nresponses and cognitive interpretations). Specifically, this framework first\nincorporates visual patch features for patch-word alignment. Meanwhile, it\nextracts coarse-grained visual features (e.g., overall image representation)\nand fine-grained visual regions (e.g., aspect-related regions) and translates\nthem into corresponding textual descriptions (e.g., facial, aesthetic).\nFinally, we leverage the sentimental causes and impressions generated by a\nlarge language model (LLM) to enhance the model's awareness of sentimental cues\nevoked by semantic content and affective-cognitive resonance. Experimental\nresults on standard MASC datasets demonstrate the effectiveness of the proposed\nmodel, which also exhibits greater flexibility to MASC compared to LLMs such as\nGPT-4o. We have publicly released the complete implementation and dataset at\nhttps://github.com/Xillv/Chimera", "published": "2025-04-22 12:43:37", "link": "http://arxiv.org/abs/2504.15848v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model", "abstract": "Direct Preference Optimization (DPO) simplifies reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\nhuman preferences without an explicit reward model. We find that during DPO\ntraining, the reference model plays the role of a data weight adjuster.\nHowever, the common practice of initializing the policy and reference models\nidentically in DPO can lead to inefficient data utilization and impose a\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\nPreference Optimization (SimPO) reduces training robustness and necessitates\nstricter conditions to prevent catastrophic forgetting. In this work, we\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\nenhances preference optimization performance by leveraging a guiding reference\nmodel. This reference model provides foresight into the optimal policy state\nachievable through the training preference data, serving as a guiding mechanism\nthat adaptively assigns higher weights to samples more suitable for the model\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\nimproves the performance of both DPO and SimPO, without relying on external\nmodels or additional data.", "published": "2025-04-22 12:39:30", "link": "http://arxiv.org/abs/2504.15843v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns", "abstract": "Prompt engineering for large language models is challenging, as even small\nprompt perturbations or model changes can significantly impact the generated\noutput texts. Existing evaluation methods, either automated metrics or human\nevaluation, have limitations, such as providing limited insights or being\nlabor-intensive. We propose Spotlight, a new approach that combines both\nautomation and human analysis. Based on data mining techniques, we\nautomatically distinguish between random (decoding) variations and systematic\ndifferences in language model outputs. This process provides token patterns\nthat describe the systematic differences and guide the user in manually\nanalyzing the effects of their prompt and model changes efficiently. We create\nthree benchmarks to quantitatively test the reliability of token pattern\nextraction methods and demonstrate that our approach provides new insights into\nestablished prompt data. From a human-centric perspective, through\ndemonstration studies and a user study, we show that our token pattern approach\nhelps users understand the systematic differences of language model outputs,\nand we are able to discover relevant differences caused by prompt and model\nchanges (e.g. related to gender or culture), thus supporting the prompt\nengineering process and human-centric model behavior research.", "published": "2025-04-22 11:53:33", "link": "http://arxiv.org/abs/2504.15815v1", "categories": ["cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A closer look at how large language models trust humans: patterns and biases", "abstract": "As large language models (LLMs) and LLM-based agents increasingly interact\nwith humans in decision-making contexts, understanding the trust dynamics\nbetween humans and AI agents becomes a central concern. While considerable\nliterature studies how humans trust AI agents, it is much less understood how\nLLM-based agents develop effective trust in humans. LLM-based agents likely\nrely on some sort of implicit effective trust in trust-related contexts (e.g.,\nevaluating individual loan applications) to assist and affect decision making.\nUsing established behavioral theories, we develop an approach that studies\nwhether LLMs trust depends on the three major trustworthiness dimensions:\ncompetence, benevolence and integrity of the human subject. We also study how\ndemographic variables affect effective trust. Across 43,200 simulated\nexperiments, for five popular language models, across five different scenarios\nwe find that LLM trust development shows an overall similarity to human trust\ndevelopment. We find that in most, but not all cases, LLM trust is strongly\npredicted by trustworthiness, and in some cases also biased by age, religion\nand gender, especially in financial scenarios. This is particularly true for\nscenarios common in the literature and for newer models. While the overall\npatterns align with human-like mechanisms of effective trust formation,\ndifferent models exhibit variation in how they estimate trust; in some cases,\ntrustworthiness and demographic factors are weak predictors of effective trust.\nThese findings call for a better understanding of AI-to-human trust dynamics\nand monitoring of biases and trust development patterns to prevent unintended\nand potentially harmful outcomes in trust-sensitive applications of AI.", "published": "2025-04-22 11:31:50", "link": "http://arxiv.org/abs/2504.15801v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach", "abstract": "Creative writing is a key capability of Large Language Models (LLMs), with\npotential applications in literature, storytelling, and various creative\ndomains. However, evaluating the creativity of machine-generated texts remains\na significant challenge, as existing methods either rely on costly manual\nannotations or fail to align closely with human assessments. In this paper, we\npropose an effective automated evaluation method based on the Torrance Test of\nCreative Writing (TTCW), which evaluates creativity as product. Our method\nemploys a reference-based Likert-style approach, scoring generated creative\ntexts relative to high-quality reference texts across various tests.\nExperimental results demonstrate that our method significantly improves the\nalignment between LLM evaluations and human assessments, achieving a pairwise\naccuracy of 0.75 (+15\\%).", "published": "2025-04-22 10:52:23", "link": "http://arxiv.org/abs/2504.15784v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving", "abstract": "Mathematical geometric problem solving (GPS) often requires effective\nintegration of multimodal information and verifiable logical coherence. Despite\nthe fast development of large language models in general problem solving, it\nremains unresolved regarding with both methodology and benchmarks, especially\ngiven the fact that exiting synthetic GPS benchmarks are often not\nself-verified and contain noise and self-contradicted information due to the\nillusion of LLMs. In this paper, we propose a scalable data engine called\nTrustGeoGen for problem generation, with formal verification to provide a\nprincipled benchmark, which we believe lays the foundation for the further\ndevelopment of methods for GPS. The engine synthesizes geometric data through\nfour key innovations: 1) multimodal-aligned generation of diagrams, textual\ndescriptions, and stepwise solutions; 2) formal verification ensuring\nrule-compliant reasoning paths; 3) a bootstrapping mechanism enabling\ncomplexity escalation via recursive state generation and 4) our devised\nGeoExplore series algorithms simultaneously produce multi-solution variants and\nself-reflective backtracking traces. By formal logical verification,\nTrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,\nalong with GeoTrust-test testset. Experiments reveal the state-of-the-art\nmodels achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its\nevaluation stringency. Crucially, models trained on GeoTrust achieve OOD\ngeneralization on GeoQA, significantly reducing logical inconsistencies\nrelative to pseudo-label annotated by OpenAI-o1. Our code is available at\nhttps://github.com/Alpha-Innovator/TrustGeoGen", "published": "2025-04-22 10:45:23", "link": "http://arxiv.org/abs/2504.15780v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Tina: Tiny Reasoning Models via LoRA", "abstract": "How cost-effectively can strong reasoning abilities be achieved in language\nmodels? Driven by this fundamental question, we present Tina, a family of tiny\nreasoning models achieved with high cost-efficiency. Notably, Tina demonstrates\nthat substantial reasoning performance can be developed using only minimal\nresources, by applying parameter-efficient updates during reinforcement\nlearning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B\nparameter base model. This minimalist approach produces models that achieve\nreasoning performance which is competitive with, and sometimes surpasses, SOTA\nRL reasoning models built upon the same base model. Crucially, this is achieved\nat a tiny fraction of the computational post-training cost employed by existing\nSOTA models. In fact, the best Tina model achieves a >20\\% reasoning\nperformance increase and 43.33\\% Pass@1 accuracy on AIME24, at only \\$9 USD\npost-training and evaluation cost (i.e., an estimated 260x cost reduction). Our\nwork reveals the surprising effectiveness of efficient RL reasoning via LoRA.\nWe validate this across multiple open-source reasoning datasets and various\nablation settings starting with a single, fixed set of hyperparameters.\nFurthermore, we hypothesize that this effectiveness and efficiency stem from\nLoRA rapidly adapting the model to the structural format of reasoning rewarded\nby RL, while largely preserving the base model's underlying knowledge. In\nservice of accessibility and open research, we fully open-source all code,\ntraining logs, and model weights \\& checkpoints.", "published": "2025-04-22 10:38:00", "link": "http://arxiv.org/abs/2504.15777v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Subject islands do not reduce to construction-specific discourse function", "abstract": "The term islands in linguistics refers to phrases from which extracting an\nelement results in ungrammaticality (Ross, 1967). Grammatical subjects are\nconsidered islands because extracting a sub-part of a subject results in an\nill-formed sentence, despite having a clear intended meaning (e.g., \"Which\ntopic did the article about inspire you?\"). The generative tradition, which\nviews syntax as autonomous of meaning and function, attributes this\nungrammaticality to the abstract movement dependency between the wh-phrase and\nthe subject-internal position with which it is associated for interpretation.\nHowever, research on language that emphasizes its communicative function\nsuggests instead that syntactic constraints, including islands, can be\nexplained based on the way different constructions package information.\nAccordingly, Abeill\\'e et al. (2020) suggest that the islandhood of subjects is\nspecific to the information structure of wh-questions, and propose that\nsubjects are not islands for movement, but for focusing, due to their\ndiscourse-backgroundedness. This predicts that other constructions that differ\nin their information structure from wh-questions, but still involve movement,\nshould not create a subject island effect. We test this prediction in three\nlarge-scale acceptability studies, using a super-additive design that singles\nout subject island violations, in three different constructions: wh-questions,\nrelative clauses, and topicalization. We report evidence for a subject island\neffect in each construction type, despite only wh-questions introducing what\nAbeill\\'e et al. (2020) call \"a clash in information structure.\" We argue that\nthis motivates an account of islands in terms of abstract, syntactic\nrepresentations, independent of the communicative function associated with the\nconstructions.", "published": "2025-04-22 08:13:04", "link": "http://arxiv.org/abs/2504.15688v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FinTextSim: Enhancing Financial Text Analysis with BERTopic", "abstract": "Recent advancements in information availability and computational\ncapabilities have transformed the analysis of annual reports, integrating\ntraditional financial metrics with insights from textual data. To extract\nvaluable insights from this wealth of textual data, automated review processes,\nsuch as topic modeling, are crucial. This study examines the effectiveness of\nBERTopic, a state-of-the-art topic model relying on contextual embeddings, for\nanalyzing Item 7 and Item 7A of 10-K filings from S&P 500 companies\n(2016-2022). Moreover, we introduce FinTextSim, a finetuned\nsentence-transformer model optimized for clustering and semantic search in\nfinancial contexts. Compared to all-MiniLM-L6-v2, the most widely used\nsentence-transformer, FinTextSim increases intratopic similarity by 81% and\nreduces intertopic similarity by 100%, significantly enhancing organizational\nclarity. We assess BERTopic's performance using embeddings from both FinTextSim\nand all-MiniLM-L6-v2. Our findings reveal that BERTopic only forms clear and\ndistinct economic topic clusters when paired with FinTextSim's embeddings.\nWithout FinTextSim, BERTopic struggles with misclassification and overlapping\ntopics. Thus, FinTextSim is pivotal for advancing financial text analysis.\nFinTextSim's enhanced contextual embeddings, tailored for the financial domain,\nelevate the quality of future research and financial information. This improved\nquality of financial information will enable stakeholders to gain a competitive\nadvantage, streamlining resource allocation and decision-making processes.\nMoreover, the improved insights have the potential to leverage business\nvaluation and stock price prediction models.", "published": "2025-04-22 08:06:37", "link": "http://arxiv.org/abs/2504.15683v1", "categories": ["cs.CL", "cs.LG", "econ.GN", "q-fin.EC", "q-fin.GN", "68T50", "I.2.7; I.5.1; J.4"], "primary_category": "cs.CL"}
{"title": "VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation", "abstract": "Recent advances in Large Language Models (LLMs) have sparked growing interest\nin applying them to Electronic Design Automation (EDA) tasks, particularly\nRegister Transfer Level (RTL) code generation. While several RTL datasets have\nbeen introduced, most focus on syntactic validity rather than functional\nvalidation with tests, leading to training examples that compile but may not\nimplement the intended behavior. We present VERICODER, a model for RTL code\ngeneration fine-tuned on a dataset validated for functional correctness. This\nfine-tuning dataset is constructed using a novel methodology that combines unit\ntest generation with feedback-directed refinement. Given a natural language\nspecification and an initial RTL design, we prompt a teacher model\n(GPT-4o-mini) to generate unit tests and iteratively revise the RTL design\nbased on its simulation results using the generated tests. If necessary, the\nteacher model also updates the tests to ensure they comply with the natural\nlanguage specification. As a result of this process, every example in our\ndataset is functionally validated, consisting of a natural language\ndescription, an RTL implementation, and passing tests. Fine-tuned on this\ndataset of over 125,000 examples, VERICODER achieves state-of-the-art metrics\nin functional correctness on VerilogEval and RTLLM, with relative gains of up\nto 71.7% and 27.4% respectively. An ablation study further shows that models\ntrained on our functionally validated dataset outperform those trained on\nfunctionally non-validated datasets, underscoring the importance of\nhigh-quality datasets in RTL code generation.", "published": "2025-04-22 07:32:46", "link": "http://arxiv.org/abs/2504.15659v1", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.AR"}
{"title": "Computational Typology", "abstract": "Typology is a subfield of linguistics that focuses on the study and\nclassification of languages based on their structural features. Unlike\ngenealogical classification, which examines the historical relationships\nbetween languages, typology seeks to understand the diversity of human\nlanguages by identifying common properties and patterns, known as universals.\nIn recent years, computational methods have played an increasingly important\nrole in typological research, enabling the analysis of large-scale linguistic\ndata and the testing of hypotheses about language structure and evolution. This\narticle provides an illustration of the benefits of computational statistical\nmodeling in typology.", "published": "2025-04-22 06:59:55", "link": "http://arxiv.org/abs/2504.15642v1", "categories": ["cs.CL", "q-bio.PE"], "primary_category": "cs.CL"}
{"title": "Cost-Effective Text Clustering with Large Language Models", "abstract": "Text clustering aims to automatically partition a collection of text\ndocuments into distinct clusters based on linguistic features. In the\nliterature, this task is usually framed as metric clustering based on text\nembeddings from pre-trained encoders or a graph clustering problem upon\npairwise similarities from an oracle, e.g., a large ML model. Recently, large\nlanguage models (LLMs) bring significant advancement in this field by offering\ncontextualized text embeddings and highly accurate similarity scores, but\nmeanwhile, present grand challenges to cope with substantial computational\nand/or financial overhead caused by numerous API-based queries or inference\ncalls to the models.\n  In response, this paper proposes TECL, a cost-effective framework that taps\ninto the feedback from LLMs for accurate text clustering within a limited\nbudget of queries to LLMs. Under the hood, TECL adopts our EdgeLLM or\nTriangleLLM to construct must-link/cannot-link constraints for text pairs, and\nfurther leverages such constraints as supervision signals input to our weighted\nconstrained clustering approach to generate clusters. Particularly, EdgeLLM\n(resp. TriangleLLM) enables the identification of informative text pairs (resp.\ntriplets) for querying LLMs via well-thought-out greedy algorithms and accurate\nextraction of pairwise constraints through carefully-crafted prompting\ntechniques. Our experiments on multiple benchmark datasets exhibit that TECL\nconsistently and considerably outperforms existing solutions in unsupervised\ntext clustering under the same query cost for LLMs.", "published": "2025-04-22 06:57:49", "link": "http://arxiv.org/abs/2504.15640v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploiting Contextual Knowledge in LLMs through V-usable Information based Layer Enhancement", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks, yet they often struggle with context-faithfulness generations\nthat properly reflect contextual knowledge. While existing approaches focus on\nenhancing the decoding strategies, they ignore the fundamental mechanism of how\ncontextual information is processed within LLMs' internal states. As a result,\nLLMs remain limited in their ability to fully leverage contextual knowledge. In\nthis paper, we propose Context-aware Layer Enhancement (CaLE), a novel\nintervention method that enhances the utilization of contextual knowledge\nwithin LLMs' internal representations. By employing V-usable information\nanalysis, CaLE strategically amplifies the growth of contextual information at\nan optimal layer, thereby enriching representations in the final layer. Our\nexperiments demonstrate that CaLE effectively improves context-faithful\ngeneration in Question-Answering tasks, particularly in scenarios involving\nunknown or conflicting contextual knowledge.", "published": "2025-04-22 06:42:22", "link": "http://arxiv.org/abs/2504.15630v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction", "abstract": "Retrieval Augmented Generation (RAG) has emerged as a powerful application of\nLarge Language Models (LLMs), revolutionizing information search and\nconsumption. RAG systems combine traditional search capabilities with LLMs to\ngenerate comprehensive answers to user queries, ideally with accurate\ncitations. However, in our experience of developing a RAG product, LLMs often\nstruggle with source attribution, aligning with other industry studies\nreporting citation accuracy rates of only about 74% for popular generative\nsearch engines. To address this, we present efficient post-processing\nalgorithms to improve citation accuracy in LLM-generated responses, with\nminimal impact on latency and cost. Our approaches cross-check generated\ncitations against retrieved articles using methods including keyword + semantic\nmatching, fine tuned model with BERTScore, and a lightweight LLM-based\ntechnique. Our experimental results demonstrate a relative improvement of\n15.46% in the overall accuracy metrics of our RAG system. This significant\nenhancement potentially enables a shift from our current larger language model\nto a relatively smaller model that is approximately 12x more cost-effective and\n3x faster in inference time, while maintaining comparable performance. This\nresearch contributes to enhancing the reliability and trustworthiness of\nAI-generated content in information retrieval and summarization tasks which is\ncritical to gain customer trust especially in commercial products.", "published": "2025-04-22 06:41:25", "link": "http://arxiv.org/abs/2504.15629v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models", "abstract": "Language models have made significant progress in generating coherent text\nand predicting next tokens based on input prompts. This study compares the\nnext-token prediction performance of two well-known models: OpenAI's GPT-2 and\nMeta's Llama-2-7b-chat-hf on Theory of Mind (ToM) tasks. To evaluate their\ncapabilities, we built a dataset from 10 short stories sourced from the Explore\nToM Dataset. We enhanced these stories by programmatically inserting additional\nsentences (infills) using GPT-4, creating variations that introduce different\nlevels of contextual complexity. This setup enables analysis of how increasing\ncontext affects model performance. We tested both models under four temperature\nsettings (0.01, 0.5, 1.0, 2.0) and evaluated their ability to predict the next\ntoken across three reasoning levels. Zero-order reasoning involves tracking the\nstate, either current (ground truth) or past (memory). First-order reasoning\nconcerns understanding another's mental state (e.g., \"Does Anne know the apple\nis salted?\"). Second-order reasoning adds recursion (e.g., \"Does Anne think\nthat Charles knows the apple is salted?\").\n  Our results show that adding more infill sentences slightly reduces\nprediction accuracy, as added context increases complexity and ambiguity.\nLlama-2 consistently outperforms GPT-2 in prediction accuracy, especially at\nlower temperatures, demonstrating greater confidence in selecting the most\nprobable token. As reasoning complexity rises, model responses diverge more.\nNotably, GPT-2 and Llama-2 display greater variability in predictions during\nfirst- and second-order reasoning tasks. These findings illustrate how model\narchitecture, temperature, and contextual complexity influence next-token\nprediction, contributing to a better understanding of the strengths and\nlimitations of current language models.", "published": "2025-04-22 05:52:55", "link": "http://arxiv.org/abs/2504.15604v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment", "abstract": "The remarkable success of Large Language Models (LLMs) has illuminated a\npromising pathway toward achieving Artificial General Intelligence for both\nacademic and industrial communities, owing to their unprecedented performance\nacross various applications. As LLMs continue to gain prominence in both\nresearch and commercial domains, their security and safety implications have\nbecome a growing concern, not only for researchers and corporations but also\nfor every nation. Currently, existing surveys on LLM safety primarily focus on\nspecific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning\nphase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs.\nTo address this gap, this paper introduces, for the first time, the concept of\n\"full-stack\" safety to systematically consider safety issues throughout the\nentire process of LLM training, deployment, and eventual commercialization.\nCompared to the off-the-shelf LLM safety surveys, our work demonstrates several\ndistinctive advantages: (I) Comprehensive Perspective. We define the complete\nLLM lifecycle as encompassing data preparation, pre-training, post-training,\ndeployment and final commercialization. To our knowledge, this represents the\nfirst safety survey to encompass the entire lifecycle of LLMs. (II) Extensive\nLiterature Support. Our research is grounded in an exhaustive review of over\n800+ papers, ensuring comprehensive coverage and systematic organization of\nsecurity issues within a more holistic understanding. (III) Unique Insights.\nThrough systematic literature analysis, we have developed reliable roadmaps and\nperspectives for each chapter. Our work identifies promising research\ndirections, including safety in data generation, alignment techniques, model\nediting, and LLM-based agent systems. These insights provide valuable guidance\nfor researchers pursuing future work in this field.", "published": "2025-04-22 05:02:49", "link": "http://arxiv.org/abs/2504.15585v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction", "abstract": "The improvement of LLMs' instruction-following capabilities depends\ncritically on the availability of high-quality instruction-response pairs.\nWhile existing automatic data synthetic methods alleviate the burden of manual\ncuration, they often rely heavily on either the quality of seed data or strong\nassumptions about the structure and content of web documents. To tackle these\nchallenges, we propose Web Reconstruction (WebR), a fully automated framework\nfor synthesizing high-quality instruction-tuning (IT) data directly from raw\nweb documents with minimal assumptions. Leveraging the inherent diversity of\nraw web content, we conceptualize web reconstruction as an instruction-tuning\ndata synthesis task via a novel dual-perspective paradigm--Web as Instruction\nand Web as Response--where each web document is designated as either an\ninstruction or a response to trigger the reconstruction process. Comprehensive\nexperiments show that datasets generated by WebR outperform state-of-the-art\nbaselines by up to 16.65% across four instruction-following benchmarks.\nNotably, WebR demonstrates superior compatibility, data efficiency, and\nscalability, enabling enhanced domain adaptation with minimal effort. The data\nand code are publicly available at https://github.com/YJiangcm/WebR.", "published": "2025-04-22 04:07:13", "link": "http://arxiv.org/abs/2504.15573v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-based Semantic Augmentation for Harmful Content Detection", "abstract": "Recent advances in large language models (LLMs) have demonstrated strong\nperformance on simple text classification tasks, frequently under zero-shot\nsettings. However, their efficacy declines when tackling complex social media\nchallenges such as propaganda detection, hateful meme classification, and\ntoxicity identification. Much of the existing work has focused on using LLMs to\ngenerate synthetic training data, overlooking the potential of LLM-based text\npreprocessing and semantic augmentation. In this paper, we introduce an\napproach that prompts LLMs to clean noisy text and provide context-rich\nexplanations, thereby enhancing training sets without substantial increases in\ndata volume. We systematically evaluate on the SemEval 2024 multi-label\nPersuasive Meme dataset and further validate on the Google Jigsaw toxic\ncomments and Facebook hateful memes datasets to assess generalizability. Our\nresults reveal that zero-shot LLM classification underperforms on these\nhigh-context tasks compared to supervised models. In contrast, integrating\nLLM-based semantic augmentation yields performance on par with approaches that\nrely on human-annotated data, at a fraction of the cost. These findings\nunderscore the importance of strategically incorporating LLMs into machine\nlearning (ML) pipeline for social media classification tasks, offering broad\nimplications for combating harmful content online.", "published": "2025-04-22 02:59:03", "link": "http://arxiv.org/abs/2504.15548v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "llm-jp-modernbert: A ModernBERT Model Trained on a Large-Scale Japanese Corpus with Long Context Length", "abstract": "Encoder-only transformer models like BERT are widely adopted as a pre-trained\nbackbone for tasks like sentence classification and retrieval. However,\npretraining of encoder models with large-scale corpora and long contexts has\nbeen relatively underexplored compared to decoder-only transformers. In this\nwork, we present llm-jp-modernbert, a ModernBERT model trained on a publicly\navailable, massive Japanese corpus with a context length of 8192 tokens. While\nour model does not surpass existing baselines on downstream tasks, it achieves\ngood results on fill-mask test evaluations. We also analyze the effect of\ncontext length expansion through pseudo-perplexity experiments. Furthermore, we\ninvestigate sentence embeddings in detail, analyzing their transitions during\ntraining and comparing them with those from other existing models, confirming\nsimilar trends with models sharing the same architecture. To support\nreproducibility and foster the development of long-context BERT, we release our\nmodel, along with the training and evaluation code.", "published": "2025-04-22 02:45:19", "link": "http://arxiv.org/abs/2504.15544v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compass-V2 Technical Report", "abstract": "Predominant LLMs focus on high-resource languages while leaving low-resource\nlanguages, particularly those in Southeast Asia (SEA), underrepresented. In\naddition, those models are general-purpose and pay limited attention to the\ne-commerce domain. To overcome these limitations, we introduce Compass-v2, a\nlightweight Mixture-of-Experts (MoE) model specifically designed for Southeast\nAsian languages and e-commerce applications. To balance model performance and\ninference cost, the model is designed with 30B total parameters and 5B active\nparameters, incorporating both fine-grained and shared expert modules. To\nenhance multilingual performance, we curated and constructed a high-quality,\nindustry-leading SEA dataset, to the best of our knowledge. To boost\nperformance in the e-commerce domain, we built a dataset comprising hundreds of\nbillions of tokens, sourced through external data mining and internal platform\ncollection. Besides, we pioneered a hybrid reasoning model that supports both\nfast thinking and deep thinking within a unified framework to enhance the\nreasoning capabilities, diverging from the conventional industry practice of\ndeploying two separate models. Through extensive experimental evaluations, our\nmodel demonstrates state-of-the-art SEA multilingual and e-commerce performance\namong sub-30B models, while maintaining significantly lower inference cost.", "published": "2025-04-22 02:08:45", "link": "http://arxiv.org/abs/2504.15527v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property", "abstract": "Intellectual Property (IP) is a unique domain that integrates technical and\nlegal knowledge, making it inherently complex and knowledge-intensive. As large\nlanguage models (LLMs) continue to advance, they show great potential for\nprocessing IP tasks, enabling more efficient analysis, understanding, and\ngeneration of IP-related content. However, existing datasets and benchmarks\neither focus narrowly on patents or cover limited aspects of the IP field,\nlacking alignment with real-world scenarios. To bridge this gap, we introduce\nthe first comprehensive IP task taxonomy and a large, diverse bilingual\nbenchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is\ndesigned to evaluate LLMs in real-world intellectual property applications,\nencompassing both understanding and generation. We benchmark 16 LLMs, ranging\nfrom general-purpose to domain-specific models, and find that even the\nbest-performing model achieves only 75.8% accuracy, revealing substantial room\nfor improvement. Notably, open-source IP and law-oriented models lag behind\nclosed-source general-purpose models. We publicly release all data and code of\nIPBench and will continue to update it with additional IP-related tasks to\nbetter reflect real-world challenges in the intellectual property domain.", "published": "2025-04-22 02:00:41", "link": "http://arxiv.org/abs/2504.15524v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks", "abstract": "As large language models (LLMs) continue to advance in linguistic\ncapabilities, robust multilingual evaluation has become essential for promoting\nequitable technological progress. This position paper examines over 2,000\nmultilingual (non-English) benchmarks from 148 countries, published between\n2021 and 2024, to evaluate past, present, and future practices in multilingual\nbenchmarking. Our findings reveal that, despite significant investments\namounting to tens of millions of dollars, English remains significantly\noverrepresented in these benchmarks. Additionally, most benchmarks rely on\noriginal language content rather than translations, with the majority sourced\nfrom high-resource countries such as China, India, Germany, the UK, and the\nUSA. Furthermore, a comparison of benchmark performance with human judgments\nhighlights notable disparities. STEM-related tasks exhibit strong correlations\nwith human evaluations (0.70 to 0.85), while traditional NLP tasks like\nquestion answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30).\nMoreover, translating English benchmarks into other languages proves\ninsufficient, as localized benchmarks demonstrate significantly higher\nalignment with local human judgments (0.68) than their translated counterparts\n(0.47). This underscores the importance of creating culturally and\nlinguistically tailored benchmarks rather than relying solely on translations.\nThrough this comprehensive analysis, we highlight six key limitations in\ncurrent multilingual evaluation practices, propose the guiding principles\naccordingly for effective multilingual benchmarking, and outline five critical\nresearch directions to drive progress in the field. Finally, we call for a\nglobal collaborative effort to develop human-aligned benchmarks that prioritize\nreal-world applications.", "published": "2025-04-22 01:47:37", "link": "http://arxiv.org/abs/2504.15521v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation", "abstract": "Simultaneous speech translation (SST) outputs translations in parallel with\nstreaming speech input, balancing translation quality and latency. While large\nlanguage models (LLMs) have been extended to handle the speech modality,\nstreaming remains challenging as speech is prepended as a prompt for the entire\ngeneration process. To unlock LLM streaming capability, this paper proposes\nSimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy\nto guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between\ntraining and inference by extracting boundary-aware speech prompts that allows\nit to be better matched with text input data. SimulS2S-LLM achieves\nsimultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete\noutput speech tokens and then synthesising output speech using a pre-trained\nvocoder. An incremental beam search is designed to expand the search space of\nspeech token prediction without increasing latency. Experiments on the CVSS\nspeech data show that SimulS2S-LLM offers a better translation quality-latency\ntrade-off than existing methods that use the same training data, such as\nimproving ASR-BLEU scores by 3 points at similar latency.", "published": "2025-04-22 01:05:32", "link": "http://arxiv.org/abs/2504.15509v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities", "abstract": "The success of Large Language Models (LLMs) has sparked interest in various\nagentic applications. A key hypothesis is that LLMs, leveraging common sense\nand Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently\nsolve complex domains. However, LLM agents have been found to suffer from\nsub-optimal exploration and the knowing-doing gap, the inability to effectively\nact on knowledge present in the model. In this work, we systematically study\nwhy LLMs perform sub-optimally in decision-making scenarios. In particular, we\nclosely examine three prevalent failure modes: greediness, frequency bias, and\nthe knowing-doing gap. We propose mitigation of these shortcomings by\nfine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.\nOur experiments across multi-armed bandits, contextual bandits, and\nTic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making\nabilities of LLMs by increasing exploration and narrowing the knowing-doing\ngap. Finally, we study both classic exploration mechanisms, such as\n$\\epsilon$-greedy, and LLM-specific approaches, such as self-correction and\nself-consistency, to enable more effective fine-tuning of LLMs for\ndecision-making.", "published": "2025-04-22 17:57:14", "link": "http://arxiv.org/abs/2504.16078v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Describe Anything: Detailed Localized Image and Video Captioning", "abstract": "Generating detailed and accurate descriptions for specific regions in images\nand videos remains a fundamental challenge for vision-language models. We\nintroduce the Describe Anything Model (DAM), a model designed for detailed\nlocalized captioning (DLC). DAM preserves both local details and global context\nthrough two key innovations: a focal prompt, which ensures high-resolution\nencoding of targeted regions, and a localized vision backbone, which integrates\nprecise localization with its broader context. To tackle the scarcity of\nhigh-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data\nPipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and\nexpands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark\ndesigned to evaluate DLC without relying on reference captions. DAM sets new\nstate-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and\ndetailed multi-sentence localized image and video captioning.", "published": "2025-04-22 17:51:41", "link": "http://arxiv.org/abs/2504.16072v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Vision language models are unreliable at trivial spatial cognition", "abstract": "Vision language models (VLMs) are designed to extract relevant visuospatial\ninformation from images. Some research suggests that VLMs can exhibit humanlike\nscene understanding, while other investigations reveal difficulties in their\nability to process relational information. To achieve widespread applicability,\nVLMs must perform reliably, yielding comparable competence across a wide\nvariety of related tasks. We sought to test how reliable these architectures\nare at engaging in trivial spatial cognition, e.g., recognizing whether one\nobject is left of another in an uncluttered scene. We developed a benchmark\ndataset -- TableTest -- whose images depict 3D scenes of objects arranged on a\ntable, and used it to evaluate state-of-the-art VLMs. Results show that\nperformance could be degraded by minor variations of prompts that use logically\nequivalent descriptions. These analyses suggest limitations in how VLMs may\nreason about spatial relations in real-world applications. They also reveal\nnovel opportunities for bolstering image caption corpora for more efficient\ntraining and testing.", "published": "2025-04-22 17:38:01", "link": "http://arxiv.org/abs/2504.16061v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis", "abstract": "Foundation models, trained on vast amounts of data using self-supervised\ntechniques, have emerged as a promising frontier for advancing artificial\nintelligence (AI) applications in medicine. This study evaluates three\ndifferent vision-language foundation models (RAD-DINO, CheXagent, and\nBiomedCLIP) on their ability to capture fine-grained imaging features for\nradiology tasks. The models were assessed across classification, segmentation,\nand regression tasks for pneumothorax and cardiomegaly on chest radiographs.\nSelf-supervised RAD-DINO consistently excelled in segmentation tasks, while\ntext-supervised CheXagent demonstrated superior classification performance.\nBiomedCLIP showed inconsistent performance across tasks. A custom segmentation\nmodel that integrates global and local features substantially improved\nperformance for all foundation models, particularly for challenging\npneumothorax segmentation. The findings highlight that pre-training methodology\nsignificantly influences model performance on specific downstream tasks. For\nfine-grained segmentation tasks, models trained without text supervision\nperformed better, while text-supervised models offered advantages in\nclassification and interpretability. These insights provide guidance for\nselecting foundation models based on specific clinical applications in\nradiology.", "published": "2025-04-22 17:20:34", "link": "http://arxiv.org/abs/2504.16047v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Approximate matrices of systems of max-min fuzzy relational equations", "abstract": "In this article, we address the inconsistency of a system of max-min fuzzy\nrelational equations by minimally modifying the matrix governing the system in\norder to achieve consistency. Our method yields consistent systems that\napproximate the original inconsistent system in the following sense: the\nright-hand side vector of each consistent system is that of the inconsistent\nsystem, and the coefficients of the matrix governing each consistent system are\nobtained by modifying, exactly and minimally, the entries of the original\nmatrix that must be corrected to achieve consistency, while leaving all other\nentries unchanged.\n  To obtain a consistent system that closely approximates the considered\ninconsistent system, we study the distance (in terms of a norm among $L_1$,\n$L_2$ or $L_\\infty$) between the matrix of the inconsistent system and the set\nformed by the matrices of consistent systems that use the same right-hand side\nvector as the inconsistent system. We show that our method allows us to\ndirectly compute matrices of consistent systems that use the same right-hand\nside vector as the inconsistent system whose distance in terms of $L_\\infty$\nnorm to the matrix of the inconsistent system is minimal (the computational\ncosts are higher when using $L_1$ norm or $L_2$ norm). We also give an explicit\nanalytical formula for computing this minimal $L_\\infty$ distance. Finally, we\ntranslate our results for systems of min-max fuzzy relational equations and\npresent some potential applications.", "published": "2025-04-22 17:09:02", "link": "http://arxiv.org/abs/2504.16042v1", "categories": ["cs.AI", "cs.LO"], "primary_category": "cs.AI"}
{"title": "Muon Optimizer Accelerates Grokking", "abstract": "This paper investigates the impact of different optimizers on the grokking\nphenomenon, where models exhibit delayed generalization. We conducted\nexperiments across seven numerical tasks (primarily modular arithmetic) using a\nmodern Transformer architecture. The experimental configuration systematically\nvaried the optimizer (Muon vs. AdamW) and the softmax activation function\n(standard softmax, stablemax, and sparsemax) to assess their combined effect on\nlearning dynamics. Our empirical evaluation reveals that the Muon optimizer,\ncharacterized by its use of spectral norm constraints and second-order\ninformation, significantly accelerates the onset of grokking compared to the\nwidely used AdamW optimizer. Specifically, Muon reduced the mean grokking epoch\nfrom 153.09 to 102.89 across all configurations, a statistically significant\ndifference (t = 5.0175, p = 6.33e-08). This suggests that the optimizer choice\nplays a crucial role in facilitating the transition from memorization to\ngeneralization.", "published": "2025-04-22 17:08:09", "link": "http://arxiv.org/abs/2504.16041v1", "categories": ["cs.LG", "cs.AI", "I.2"], "primary_category": "cs.LG"}
{"title": "LLMs meet Federated Learning for Scalable and Secure IoT Management", "abstract": "The rapid expansion of IoT ecosystems introduces severe challenges in\nscalability, security, and real-time decision-making. Traditional centralized\narchitectures struggle with latency, privacy concerns, and excessive resource\nconsumption, making them unsuitable for modern large-scale IoT deployments.\nThis paper presents a novel Federated Learning-driven Large Language Model\n(FL-LLM) framework, designed to enhance IoT system intelligence while ensuring\ndata privacy and computational efficiency. The framework integrates Generative\nIoT (GIoT) models with a Gradient Sensing Federated Strategy (GSFS),\ndynamically optimizing model updates based on real-time network conditions. By\nleveraging a hybrid edge-cloud processing architecture, our approach balances\nintelligence, scalability, and security in distributed IoT environments.\nEvaluations on the IoT-23 dataset demonstrate that our framework improves model\naccuracy, reduces response latency, and enhances energy efficiency,\noutperforming traditional FL techniques (i.e., FedAvg, FedOpt). These findings\nhighlight the potential of integrating LLM-powered federated learning into\nlarge-scale IoT ecosystems, paving the way for more secure, scalable, and\nadaptive IoT management solutions.", "published": "2025-04-22 16:56:59", "link": "http://arxiv.org/abs/2504.16032v1", "categories": ["cs.LG", "cs.AI", "cs.ET"], "primary_category": "cs.LG"}
{"title": "Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs DeepSeek-V3", "abstract": "Determining the most effective Large Language Model for code smell detection\npresents a complex challenge. This study introduces a structured methodology\nand evaluation matrix to tackle this issue, leveraging a curated dataset of\ncode samples consistently annotated with known smells. The dataset spans four\nprominent programming languages Java, Python, JavaScript, and C++; allowing for\ncross language comparison. We benchmark two state of the art LLMs, OpenAI GPT\n4.0 and DeepSeek-V3, using precision, recall, and F1 score as evaluation\nmetrics. Our analysis covers three levels of detail: overall performance,\ncategory level performance, and individual code smell type performance.\nAdditionally, we explore cost effectiveness by comparing the token based\ndetection approach of GPT 4.0 with the pattern-matching techniques employed by\nDeepSeek V3. The study also includes a cost analysis relative to traditional\nstatic analysis tools such as SonarQube. The findings offer valuable guidance\nfor practitioners in selecting an efficient, cost effective solution for\nautomated code smell detection", "published": "2025-04-22 16:44:39", "link": "http://arxiv.org/abs/2504.16027v1", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL"], "primary_category": "cs.SE"}
{"title": "Trends in AI Supercomputers", "abstract": "Frontier AI development relies on powerful AI supercomputers, yet analysis of\nthese systems is limited. We create a dataset of 500 AI supercomputers from\n2019 to 2025 and analyze key trends in performance, power needs, hardware cost,\nownership, and global distribution. We find that the computational performance\nof AI supercomputers has doubled every nine months, while hardware acquisition\ncost and power needs both doubled every year. The leading system in March 2025,\nxAI's Colossus, used 200,000 AI chips, had a hardware cost of \\$7B, and\nrequired 300 MW of power, as much as 250,000 households. As AI supercomputers\nevolved from tools for science to industrial machines, companies rapidly\nexpanded their share of total AI supercomputer performance, while the share of\ngovernments and academia diminished. Globally, the United States accounts for\nabout 75% of total performance in our dataset, with China in second place at\n15%. If the observed trends continue, the leading AI supercomputer in 2030 will\nachieve $2\\times10^{22}$ 16-bit FLOP/s, use two million AI chips, have a\nhardware cost of \\$200 billion, and require 9 GW of power. Our analysis\nprovides visibility into the AI supercomputer landscape, allowing policymakers\nto assess key AI trends like resource needs, ownership, and national\ncompetitiveness.", "published": "2025-04-22 16:44:34", "link": "http://arxiv.org/abs/2504.16026v1", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY"}
{"title": "Navigating the State of Cognitive Flow: Context-Aware AI Interventions for Effective Reasoning Support", "abstract": "Flow theory describes an optimal cognitive state where individuals experience\ndeep focus and intrinsic motivation when a task's difficulty aligns with their\nskill level. In AI-augmented reasoning, interventions that disrupt the state of\ncognitive flow can hinder rather than enhance decision-making. This paper\nproposes a context-aware cognitive augmentation framework that adapts\ninterventions based on three key contextual factors: type, timing, and scale.\nBy leveraging multimodal behavioral cues (e.g., gaze behavior, typing\nhesitation, interaction speed), AI can dynamically adjust cognitive support to\nmaintain or restore flow. We introduce the concept of cognitive flow, an\nextension of flow theory in AI-augmented reasoning, where interventions are\npersonalized, adaptive, and minimally intrusive. By shifting from static\ninterventions to context-aware augmentation, our approach ensures that AI\nsystems support deep engagement in complex decision-making and reasoning\nwithout disrupting cognitive immersion.", "published": "2025-04-22 16:35:39", "link": "http://arxiv.org/abs/2504.16021v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC"}
{"title": "AlphaGrad: Non-Linear Gradient Normalization Optimizer", "abstract": "We introduce AlphaGrad, a memory-efficient, conditionally stateless optimizer\naddressing the memory overhead and hyperparameter complexity of adaptive\nmethods like Adam. AlphaGrad enforces scale invariance via tensor-wise L2\ngradient normalization followed by a smooth hyperbolic tangent transformation,\n$g' = \\tanh(\\alpha \\cdot \\tilde{g})$, controlled by a single steepness\nparameter $\\alpha$. Our contributions include: (1) the AlphaGrad algorithm\nformulation; (2) a formal non-convex convergence analysis guaranteeing\nstationarity; (3) extensive empirical evaluation on diverse RL benchmarks (DQN,\nTD3, PPO). Compared to Adam, AlphaGrad demonstrates a highly context-dependent\nperformance profile. While exhibiting instability in off-policy DQN, it\nprovides enhanced training stability with competitive results in TD3 (requiring\ncareful $\\alpha$ tuning) and achieves substantially superior performance in\non-policy PPO. These results underscore the critical importance of empirical\n$\\alpha$ selection, revealing strong interactions between the optimizer's\ndynamics and the underlying RL algorithm. AlphaGrad presents a compelling\nalternative optimizer for memory-constrained scenarios and shows significant\npromise for on-policy learning regimes where its stability and efficiency\nadvantages can be particularly impactful.", "published": "2025-04-22 16:33:14", "link": "http://arxiv.org/abs/2504.16020v1", "categories": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "OPUS-VFL: Incentivizing Optimal Privacy-Utility Tradeoffs in Vertical Federated Learning", "abstract": "Vertical Federated Learning (VFL) enables organizations with disjoint feature\nspaces but shared user bases to collaboratively train models without sharing\nraw data. However, existing VFL systems face critical limitations: they often\nlack effective incentive mechanisms, struggle to balance privacy-utility\ntradeoffs, and fail to accommodate clients with heterogeneous resource\ncapabilities. These challenges hinder meaningful participation, degrade model\nperformance, and limit practical deployment. To address these issues, we\npropose OPUS-VFL, an Optimal Privacy-Utility tradeoff Strategy for VFL.\nOPUS-VFL introduces a novel, privacy-aware incentive mechanism that rewards\nclients based on a principled combination of model contribution, privacy\npreservation, and resource investment. It employs a lightweight leave-one-out\n(LOO) strategy to quantify feature importance per client, and integrates an\nadaptive differential privacy mechanism that enables clients to dynamically\ncalibrate noise levels to optimize their individual utility. Our framework is\ndesigned to be scalable, budget-balanced, and robust to inference and poisoning\nattacks. Extensive experiments on benchmark datasets (MNIST, CIFAR-10, and\nCIFAR-100) demonstrate that OPUS-VFL significantly outperforms state-of-the-art\nVFL baselines in both efficiency and robustness. It reduces label inference\nattack success rates by up to 20%, increases feature inference reconstruction\nerror (MSE) by over 30%, and achieves up to 25% higher incentives for clients\nthat contribute meaningfully while respecting privacy and cost constraints.\nThese results highlight the practicality and innovation of OPUS-VFL as a\nsecure, fair, and performance-driven solution for real-world VFL.", "published": "2025-04-22 16:00:11", "link": "http://arxiv.org/abs/2504.15995v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Bug Destiny Prediction in Large Open-Source Software Repositories through Sentiment Analysis and BERT Topic Modeling", "abstract": "This study explores a novel approach to predicting key bug-related outcomes,\nincluding the time to resolution, time to fix, and ultimate status of a bug,\nusing data from the Bugzilla Eclipse Project. Specifically, we leverage\nfeatures available before a bug is resolved to enhance predictive accuracy. Our\nmethodology incorporates sentiment analysis to derive both an emotionality\nscore and a sentiment classification (positive or negative). Additionally, we\nintegrate the bug's priority level and its topic, extracted using a BERTopic\nmodel, as features for a Convolutional Neural Network (CNN) and a Multilayer\nPerceptron (MLP). Our findings indicate that the combination of BERTopic and\nsentiment analysis can improve certain model performance metrics. Furthermore,\nwe observe that balancing model inputs enhances practical applicability, albeit\nat the cost of a significant reduction in accuracy in most cases. To address\nour primary objectives, predicting time-to-resolution, time-to-fix, and bug\ndestiny, we employ both binary classification and exact time value predictions,\nallowing for a comparative evaluation of their predictive effectiveness.\nResults demonstrate that sentiment analysis serves as a valuable predictor of a\nbug's eventual outcome, particularly in determining whether it will be fixed.\nHowever, its utility is less pronounced when classifying bugs into more complex\nor unconventional outcome categories.", "published": "2025-04-22 15:18:14", "link": "http://arxiv.org/abs/2504.15972v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "Universal Approximation with Softmax Attention", "abstract": "We prove that with linear transformations, both (i) two-layer self-attention\nand (ii) one-layer self-attention followed by a softmax function are universal\napproximators for continuous sequence-to-sequence functions on compact domains.\nOur main technique is a new interpolation-based method for analyzing\nattention's internal mechanism. This leads to our key insight: self-attention\nis able to approximate a generalized version of ReLU to arbitrary precision,\nand hence subsumes many known universal approximators. Building on these, we\nshow that two-layer multi-head attention alone suffices as a\nsequence-to-sequence universal approximator. In contrast, prior works rely on\nfeed-forward networks to establish universal approximation in Transformers.\nFurthermore, we extend our techniques to show that, (softmax-)attention-only\nlayers are capable of approximating various statistical models in-context. We\nbelieve these techniques hold independent interest.", "published": "2025-04-22 14:51:33", "link": "http://arxiv.org/abs/2504.15956v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models", "abstract": "Diagnostic imaging relies on interpreting both images and radiology reports,\nbut the growing data volumes place significant pressure on medical experts,\nyielding increased errors and workflow backlogs. Medical vision-language models\n(med-VLMs) have emerged as a powerful framework to efficiently process\nmultimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit\ntheir performance hinges on how well image and text representations are\naligned. Existing alignment methods, predominantly based on contrastive\nlearning, prioritize separation between disease classes over segregation of\nfine-grained pathology attributes like location, size or severity, leading to\nsuboptimal representations. Here, we propose MedTrim (Meta-entity-driven\nTriplet mining), a novel method that enhances image-text alignment through\nmultimodal triplet learning synergistically guided by disease class as well as\nadjectival and directional pathology descriptors. Unlike common alignment\nmethods that separate broad disease classes, MedTrim leverages structured\nmeta-entity information to preserve subtle but clinically significant\nintra-class variations. For this purpose, we first introduce an ontology-based\nentity recognition module that extracts pathology-specific meta-entities from\nCXR reports, as annotations on pathology attributes are rare in public\ndatasets. For refined sample selection in triplet mining, we then introduce a\nnovel score function that captures an aggregate measure of inter-sample\nsimilarity based on disease classes and adjectival/directional descriptors.\nLastly, we introduce a multimodal triplet alignment objective for explicit\nwithin- and cross-modal alignment between samples sharing detailed pathology\ncharacteristics. Our demonstrations indicate that MedTrim improves performance\nin downstream retrieval and classification tasks compared to state-of-the-art\nalignment methods.", "published": "2025-04-22 14:17:51", "link": "http://arxiv.org/abs/2504.15929v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "A Clinician-Friendly Platform for Ophthalmic Image Analysis Without Technical Barriers", "abstract": "Artificial intelligence (AI) shows remarkable potential in medical imaging\ndiagnostics, but current models typically require retraining when deployed\nacross different clinical centers, limiting their widespread adoption. We\nintroduce GlobeReady, a clinician-friendly AI platform that enables ocular\ndisease diagnosis without retraining/fine-tuning or technical expertise.\nGlobeReady achieves high accuracy across imaging modalities: 93.9-98.5% for an\n11-category fundus photo dataset and 87.2-92.7% for a 15-category OCT dataset.\nThrough training-free local feature augmentation, it addresses domain shifts\nacross centers and populations, reaching an average accuracy of 88.9% across\nfive centers in China, 86.3% in Vietnam, and 90.2% in the UK. The built-in\nconfidence-quantifiable diagnostic approach further boosted accuracy to\n94.9-99.4% (fundus) and 88.2-96.2% (OCT), while identifying out-of-distribution\ncases at 86.3% (49 CFP categories) and 90.6% (13 OCT categories). Clinicians\nfrom multiple countries rated GlobeReady highly (average 4.6 out of 5) for its\nusability and clinical relevance. These results demonstrate GlobeReady's\nrobust, scalable diagnostic capability and potential to support ophthalmic care\nwithout technical barriers.", "published": "2025-04-22 14:17:22", "link": "http://arxiv.org/abs/2504.15928v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "New Recipe for Semi-supervised Community Detection: Clique Annealing under Crystallization Kinetics", "abstract": "Semi-supervised community detection methods are widely used for identifying\nspecific communities due to the label scarcity. Existing semi-supervised\ncommunity detection methods typically involve two learning stages learning in\nboth initial identification and subsequent adjustment, which often starts from\nan unreasonable community core candidate. Moreover, these methods encounter\nscalability issues because they depend on reinforcement learning and generative\nadversarial networks, leading to higher computational costs and restricting the\nselection of candidates. To address these limitations, we draw a parallel\nbetween crystallization kinetics and community detection to integrate the\nspontaneity of the annealing process into community detection. Specifically, we\nliken community detection to identifying a crystal subgrain (core) that expands\ninto a complete grain (community) through a process similar to annealing. Based\non this finding, we propose CLique ANNealing (CLANN), which applies kinetics\nconcepts to community detection by integrating these principles into the\noptimization process to strengthen the consistency of the community core.\nSubsequently, a learning-free Transitive Annealer was employed to refine the\nfirst-stage candidates by merging neighboring cliques and repositioning the\ncommunity core, enabling a spontaneous growth process that enhances\nscalability. Extensive experiments on \\textbf{43} different network settings\ndemonstrate that CLANN outperforms state-of-the-art methods across multiple\nreal-world datasets, showcasing its exceptional efficacy and efficiency in\ncommunity detection.", "published": "2025-04-22 14:17:15", "link": "http://arxiv.org/abs/2504.15927v1", "categories": ["cs.SI", "cs.AI"], "primary_category": "cs.SI"}
{"title": "Achieving Distributive Justice in Federated Learning via Uncertainty Quantification", "abstract": "Client-level fairness metrics for federated learning are used to ensure that\nall clients in a federation either: a) have similar final performance on their\nlocal data distributions (i.e., client parity), or b) obtain final performance\non their local data distributions relative to their contribution to the\nfederated learning process (i.e., contribution fairness). While a handful of\nworks that propose either client-parity or contribution-based fairness metrics\nground their definitions and decisions in social theories of equality -- such\nas distributive justice -- most works arbitrarily choose what notion of\nfairness to align with which makes it difficult for practitioners to choose\nwhich fairness metric aligns best with their fairness ethics. In this work, we\npropose UDJ-FL (Uncertainty-based Distributive Justice for Federated Learning),\na flexible federated learning framework that can achieve multiple distributive\njustice-based client-level fairness metrics. Namely, by utilizing techniques\ninspired by fair resource allocation, in conjunction with performing aleatoric\nuncertainty-based client weighing, our UDJ-FL framework is able to achieve\negalitarian, utilitarian, Rawls' difference principle, or desert-based\nclient-level fairness. We empirically show the ability of UDJ-FL to achieve all\nfour defined distributive justice-based client-level fairness metrics in\naddition to providing fairness equivalent to (or surpassing) other popular fair\nfederated learning works. Further, we provide justification for why aleatoric\nuncertainty weighing is necessary to the construction of our UDJ-FL framework\nas well as derive theoretical guarantees for the generalization bounds of\nUDJ-FL. Our code is publicly available at\nhttps://github.com/alycia-noel/UDJ-FL.", "published": "2025-04-22 14:07:56", "link": "http://arxiv.org/abs/2504.15924v1", "categories": ["cs.LG", "cs.AI", "stat.ML", "68T01", "I.2.0"], "primary_category": "cs.LG"}
{"title": "Ask2Loc: Learning to Locate Instructional Visual Answers by Asking Questions", "abstract": "Locating specific segments within an instructional video is an efficient way\nto acquire guiding knowledge. Generally, the task of obtaining video segments\nfor both verbal explanations and visual demonstrations is known as visual\nanswer localization (VAL). However, users often need multiple interactions to\nobtain answers that align with their expectations when using the system. During\nthese interactions, humans deepen their understanding of the video content by\nasking themselves questions, thereby accurately identifying the location.\nTherefore, we propose a new task, named In-VAL, to simulate the multiple\ninteractions between humans and videos in the procedure of obtaining visual\nanswers. The In-VAL task requires interactively addressing several semantic gap\nissues, including 1) the ambiguity of user intent in the input questions, 2)\nthe incompleteness of language in video subtitles, and 3) the fragmentation of\ncontent in video segments. To address these issues, we propose Ask2Loc, a\nframework for resolving In-VAL by asking questions. It includes three key\nmodules: 1) a chatting module to refine initial questions and uncover clear\nintentions, 2) a rewriting module to generate fluent language and create\ncomplete descriptions, and 3) a searching module to broaden local context and\nprovide integrated content. We conduct extensive experiments on three\nreconstructed In-VAL datasets. Compared to traditional end-to-end and two-stage\nmethods, our proposed Ask2Loc can improve performance by up to 14.91 (mIoU) on\nthe In-VAL task. Our code and datasets can be accessed at\nhttps://github.com/changzong/Ask2Loc.", "published": "2025-04-22 14:03:16", "link": "http://arxiv.org/abs/2504.15918v1", "categories": ["cs.CV", "cs.AI", "cs.HC", "68T45, 68T20"], "primary_category": "cs.CV"}
{"title": "Automated Bug Report Prioritization in Large Open-Source Projects", "abstract": "Large open-source projects receive a large number of issues (known as bugs),\nincluding software defect (i.e., bug) reports and new feature requests from\ntheir user and developer communities at a fast rate. The often limited project\nresources do not allow them to deal with all issues. Instead, they have to\nprioritize them according to the project's priorities and the issues'\nseverities. In this paper, we propose a novel approach to automated bug\nprioritization based on the natural language text of the bug reports that are\nstored in the open bug repositories of the issue-tracking systems. We conduct\ntopic modeling using a variant of LDA called TopicMiner-MTM and text\nclassification with the BERT large language model to achieve a higher\nperformance level compared to the state-of-the-art. Experimental results using\nan existing reference dataset containing 85,156 bug reports of the Eclipse\nPlatform project indicate that we outperform existing approaches in terms of\nAccuracy, Precision, Recall, and F1-measure of the bug report priority\nprediction.", "published": "2025-04-22 13:57:48", "link": "http://arxiv.org/abs/2504.15912v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "GraphEdge: Dynamic Graph Partition and Task Scheduling for GNNs Computing in Edge Network", "abstract": "With the exponential growth of Internet of Things (IoT) devices, edge\ncomputing (EC) is gradually playing an important role in providing\ncost-effective services. However, existing approaches struggle to perform well\nin graph-structured scenarios where user data is correlated, such as traffic\nflow prediction and social relationship recommender systems. In particular,\ngraph neural network (GNN)-based approaches lead to expensive server\ncommunication cost. To address this problem, we propose GraphEdge, an efficient\nGNN-based EC architecture. It considers the EC system of GNN tasks, where there\nare associations between users and it needs to take into account the task data\nof its neighbors when processing the tasks of a user. Specifically, the\narchitecture first perceives the user topology and represents their data\nassociations as a graph layout at each time step. Then the graph layout is\noptimized by calling our proposed hierarchical traversal graph cut algorithm\n(HiCut), which cuts the graph layout into multiple weakly associated subgraphs\nbased on the aggregation characteristics of GNN, and the communication cost\nbetween different subgraphs during GNN inference is minimized. Finally, based\non the optimized graph layout, our proposed deep reinforcement learning (DRL)\nbased graph offloading algorithm (DRLGO) is executed to obtain the optimal\noffloading strategy for the tasks of users, the offloading strategy is\nsubgraph-based, it tries to offload user tasks in a subgraph to the same edge\nserver as possible while minimizing the task processing time and energy\nconsumption of the EC system. Experimental results show the good effectiveness\nand dynamic adaptation of our proposed architecture and it also performs well\neven in dynamic scenarios.", "published": "2025-04-22 13:45:13", "link": "http://arxiv.org/abs/2504.15905v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Impact of Noise on LLM-Models Performance in Abstraction and Reasoning Corpus (ARC) Tasks with Model Temperature Considerations", "abstract": "Recent advancements in Large Language Models (LLMs) have generated growing\ninterest in their structured reasoning capabilities, particularly in tasks\ninvolving abstraction and pattern recognition. The Abstraction and Reasoning\nCorpus (ARC) benchmark plays a crucial role in evaluating these capabilities by\ntesting how well AI models generalize to novel problems. While GPT-4o\ndemonstrates strong performance by solving all ARC tasks under zero-noise\nconditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any,\nsuggesting limitations in their ability to reason beyond simple pattern\nmatching. To explore this gap, we systematically evaluate these models across\ndifferent noise levels and temperature settings. Our results reveal that the\nintroduction of noise consistently impairs model performance, regardless of\narchitecture. This decline highlights a shared vulnerability: current LLMs,\ndespite showing signs of abstract reasoning, remain highly sensitive to input\nperturbations. Such fragility raises concerns about their real-world\napplicability, where noise and uncertainty are common. By comparing how\ndifferent model architectures respond to these challenges, we offer insights\ninto the structural weaknesses of modern LLMs in reasoning tasks. This work\nunderscores the need for developing more robust and adaptable AI systems\ncapable of handling the ambiguity and variability inherent in real-world\nscenarios. Our findings aim to guide future research toward enhancing model\ngeneralization, robustness, and alignment with human-like cognitive\nflexibility.", "published": "2025-04-22 13:43:58", "link": "http://arxiv.org/abs/2504.15903v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Supporting Data-Frame Dynamics in AI-assisted Decision Making", "abstract": "High stakes decision-making often requires a continuous interplay between\nevolving evidence and shifting hypotheses, a dynamic that is not well supported\nby current AI decision support systems. In this paper, we introduce a\nmixed-initiative framework for AI assisted decision making that is grounded in\nthe data-frame theory of sensemaking and the evaluative AI paradigm. Our\napproach enables both humans and AI to collaboratively construct, validate, and\nadapt hypotheses. We demonstrate our framework with an AI-assisted skin cancer\ndiagnosis prototype that leverages a concept bottleneck model to facilitate\ninterpretable interactions and dynamic updates to diagnostic hypotheses.", "published": "2025-04-22 13:36:06", "link": "http://arxiv.org/abs/2504.15894v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC"}
{"title": "Integrating Non-Linear Radon Transformation for Diabetic Retinopathy Grading", "abstract": "Diabetic retinopathy is a serious ocular complication that poses a\nsignificant threat to patients' vision and overall health. Early detection and\naccurate grading are essential to prevent vision loss. Current automatic\ngrading methods rely heavily on deep learning applied to retinal fundus images,\nbut the complex, irregular patterns of lesions in these images, which vary in\nshape and distribution, make it difficult to capture subtle changes. This study\nintroduces RadFuse, a multi-representation deep learning framework that\nintegrates non-linear RadEx-transformed sinogram images with traditional fundus\nimages to enhance diabetic retinopathy detection and grading. Our RadEx\ntransformation, an optimized non-linear extension of the Radon transform,\ngenerates sinogram representations to capture complex retinal lesion patterns.\nBy leveraging both spatial and transformed domain information, RadFuse enriches\nthe feature set available to deep learning models, improving the\ndifferentiation of severity levels. We conducted extensive experiments on two\nbenchmark datasets, APTOS-2019 and DDR, using three convolutional neural\nnetworks (CNNs): ResNeXt-50, MobileNetV2, and VGG19. RadFuse showed significant\nimprovements over fundus-image-only models across all three CNN architectures\nand outperformed state-of-the-art methods on both datasets. For severity\ngrading across five stages, RadFuse achieved a quadratic weighted kappa of\n93.24%, an accuracy of 87.07%, and an F1-score of 87.17%. In binary\nclassification between healthy and diabetic retinopathy cases, the method\nreached an accuracy of 99.09%, precision of 98.58%, and recall of 99.6%,\nsurpassing previously established models. These results demonstrate RadFuse's\ncapacity to capture complex non-linear features, advancing diabetic retinopathy\nclassification and promoting the integration of advanced mathematical\ntransforms in medical image analysis.", "published": "2025-04-22 13:27:28", "link": "http://arxiv.org/abs/2504.15883v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Bidirectional Task-Motion Planning Based on Hierarchical Reinforcement Learning for Strategic Confrontation", "abstract": "In swarm robotics, confrontation scenarios, including strategic\nconfrontations, require efficient decision-making that integrates discrete\ncommands and continuous actions. Traditional task and motion planning methods\nseparate decision-making into two layers, but their unidirectional structure\nfails to capture the interdependence between these layers, limiting\nadaptability in dynamic environments. Here, we propose a novel bidirectional\napproach based on hierarchical reinforcement learning, enabling dynamic\ninteraction between the layers. This method effectively maps commands to task\nallocation and actions to path planning, while leveraging cross-training\ntechniques to enhance learning across the hierarchical framework. Furthermore,\nwe introduce a trajectory prediction model that bridges abstract task\nrepresentations with actionable planning goals. In our experiments, it achieves\nover 80\\% in confrontation win rate and under 0.01 seconds in decision time,\noutperforming existing approaches. Demonstrations through large-scale tests and\nreal-world robot experiments further emphasize the generalization capabilities\nand practical applicability of our method.", "published": "2025-04-22 13:22:58", "link": "http://arxiv.org/abs/2504.15876v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search", "abstract": "Deep learning (DL) has achieved remarkable progress in the field of medical\nimaging. However, adapting DL models to medical tasks remains a significant\nchallenge, primarily due to two key factors: (1) architecture selection, as\ndifferent tasks necessitate specialized model designs, and (2) weight\ninitialization, which directly impacts the convergence speed and final\nperformance of the models. Although transfer learning from ImageNet is a widely\nadopted strategy, its effectiveness is constrained by the substantial\ndifferences between natural and medical images. To address these challenges, we\nintroduce Medical Neural Network Search (MedNNS), the first Neural Network\nSearch framework for medical imaging applications. MedNNS jointly optimizes\narchitecture selection and weight initialization by constructing a meta-space\nthat encodes datasets and models based on how well they perform together. We\nbuild this space using a Supernetwork-based approach, expanding the model zoo\nsize by 51x times over previous state-of-the-art (SOTA) methods. Moreover, we\nintroduce rank loss and Fr\\'echet Inception Distance (FID) loss into the\nconstruction of the space to capture inter-model and inter-dataset\nrelationships, thereby achieving more accurate alignment in the meta-space.\nExperimental results across multiple datasets demonstrate that MedNNS\nsignificantly outperforms both ImageNet pre-trained DL models and SOTA Neural\nArchitecture Search (NAS) methods, achieving an average accuracy improvement of\n1.7% across datasets while converging substantially faster. The code and the\nprocessed meta-space is available at https://github.com/BioMedIA-MBZUAI/MedNNS.", "published": "2025-04-22 13:04:40", "link": "http://arxiv.org/abs/2504.15865v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "CARE: Compatibility-Aware Incentive Mechanisms for Federated Learning with Budgeted Requesters", "abstract": "Federated learning (FL) is a promising approach that allows requesters (\\eg,\nservers) to obtain local training models from workers (e.g., clients). Since\nworkers are typically unwilling to provide training services/models freely and\nvoluntarily, many incentive mechanisms in FL are designed to incentivize\nparticipation by offering monetary rewards from requesters. However, existing\nstudies neglect two crucial aspects of real-world FL scenarios. First, workers\ncan possess inherent incompatibility characteristics (e.g., communication\nchannels and data sources), which can lead to degradation of FL efficiency\n(e.g., low communication efficiency and poor model generalization). Second, the\nrequesters are budgeted, which limits the amount of workers they can hire for\ntheir tasks. In this paper, we investigate the scenario in FL where multiple\nbudgeted requesters seek training services from incompatible workers with\nprivate training costs. We consider two settings: the cooperative budget\nsetting where requesters cooperate to pool their budgets to improve their\noverall utility and the non-cooperative budget setting where each requester\noptimizes their utility within their own budgets. To address efficiency\ndegradation caused by worker incompatibility, we develop novel\ncompatibility-aware incentive mechanisms, CARE-CO and CARE-NO, for both\nsettings to elicit true private costs and determine workers to hire for\nrequesters and their rewards while satisfying requester budget constraints. Our\nmechanisms guarantee individual rationality, truthfulness, budget feasibility,\nand approximation performance. We conduct extensive experiments using\nreal-world datasets to show that the proposed mechanisms significantly\noutperform existing baselines.", "published": "2025-04-22 12:42:45", "link": "http://arxiv.org/abs/2504.15847v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Generative AI for Research Data Processing: Lessons Learnt From Three Use Cases", "abstract": "There has been enormous interest in generative AI since ChatGPT was launched\nin 2022. However, there are concerns about the accuracy and consistency of the\noutputs of generative AI. We have carried out an exploratory study on the\napplication of this new technology in research data processing. We identified\ntasks for which rule-based or traditional machine learning approaches were\ndifficult to apply, and then performed these tasks using generative AI.\n  We demonstrate the feasibility of using the generative AI model Claude 3 Opus\nin three research projects involving complex data processing tasks:\n  1) Information extraction: We extract plant species names from historical\nseedlists (catalogues of seeds) published by botanical gardens.\n  2) Natural language understanding: We extract certain data points (name of\ndrug, name of health indication, relative effectiveness, cost-effectiveness,\netc.) from documents published by Health Technology Assessment organisations in\nthe EU.\n  3) Text classification: We assign industry codes to projects on the\ncrowdfunding website Kickstarter.\n  We share the lessons we learnt from these use cases: How to determine if\ngenerative AI is an appropriate tool for a given data processing task, and if\nso, how to maximise the accuracy and consistency of the results obtained.", "published": "2025-04-22 12:21:07", "link": "http://arxiv.org/abs/2504.15829v1", "categories": ["cs.AI", "68T50", "I.2.7"], "primary_category": "cs.AI"}
{"title": "DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with Dual Optimizers", "abstract": "Existing machine unlearning (MU) approaches exhibit significant sensitivity\nto hyperparameters, requiring meticulous tuning that limits practical\ndeployment. In this work, we first empirically demonstrate the instability and\nsuboptimal performance of existing popular MU methods when deployed in\ndifferent scenarios. To address this issue, we propose Dual Optimizer\n(DualOptim), which incorporates adaptive learning rate and decoupled momentum\nfactors. Empirical and theoretical evidence demonstrates that DualOptim\ncontributes to effective and stable unlearning. Through extensive experiments,\nwe show that DualOptim can significantly boost MU efficacy and stability across\ndiverse tasks, including image classification, image generation, and large\nlanguage models, making it a versatile approach to empower existing MU\nalgorithms.", "published": "2025-04-22 12:18:26", "link": "http://arxiv.org/abs/2504.15827v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Human-Imperceptible Physical Adversarial Attack for NIR Face Recognition Models", "abstract": "Near-infrared (NIR) face recognition systems, which can operate effectively\nin low-light conditions or in the presence of makeup, exhibit vulnerabilities\nwhen subjected to physical adversarial attacks. To further demonstrate the\npotential risks in real-world applications, we design a novel, stealthy, and\npractical adversarial patch to attack NIR face recognition systems in a\nblack-box setting. We achieved this by utilizing human-imperceptible\ninfrared-absorbing ink to generate multiple patches with digitally optimized\nshapes and positions for infrared images. To address the optimization mismatch\nbetween digital and real-world NIR imaging, we develop a light reflection model\nfor human skin to minimize pixel-level discrepancies by simulating NIR light\nreflection.\n  Compared to state-of-the-art (SOTA) physical attacks on NIR face recognition\nsystems, the experimental results show that our method improves the attack\nsuccess rate in both digital and physical domains, particularly maintaining\neffectiveness across various face postures. Notably, the proposed approach\noutperforms SOTA methods, achieving an average attack success rate of 82.46% in\nthe physical domain across different models, compared to 64.18% for existing\nmethods. The artifact is available at\nhttps://anonymous.4open.science/r/Human-imperceptible-adversarial-patch-0703/.", "published": "2025-04-22 12:10:25", "link": "http://arxiv.org/abs/2504.15823v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Fusing Reward and Dueling Feedback in Stochastic Bandits", "abstract": "This paper investigates the fusion of absolute (reward) and relative\n(dueling) feedback in stochastic bandits, where both feedback types are\ngathered in each decision round. We derive a regret lower bound, demonstrating\nthat an efficient algorithm may incur only the smaller among the reward and\ndueling-based regret for each individual arm. We propose two fusion approaches:\n(1) a simple elimination fusion algorithm that leverages both feedback types to\nexplore all arms and unifies collected information by sharing a common\ncandidate arm set, and (2) a decomposition fusion algorithm that selects the\nmore effective feedback to explore the corresponding arms and randomly assigns\none feedback type for exploration and the other for exploitation in each round.\nThe elimination fusion experiences a suboptimal multiplicative term of the\nnumber of arms in regret due to the intrinsic suboptimality of dueling\nelimination. In contrast, the decomposition fusion achieves regret matching the\nlower bound up to a constant under a common assumption. Extensive experiments\nconfirm the efficacy of our algorithms and theoretical results.", "published": "2025-04-22 11:51:20", "link": "http://arxiv.org/abs/2504.15812v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "DAE-KAN: A Kolmogorov-Arnold Network Model for High-Index Differential-Algebraic Equations", "abstract": "Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to\nMulti-Layer Perceptrons (MLPs) due to their superior function-fitting abilities\nin data-driven modeling. In this paper, we propose a novel framework, DAE-KAN,\nfor solving high-index differential-algebraic equations (DAEs) by integrating\nKANs with Physics-Informed Neural Networks (PINNs). This framework not only\npreserves the ability of traditional PINNs to model complex systems governed by\nphysical laws but also enhances their performance by leveraging the\nfunction-fitting strengths of KANs. Numerical experiments demonstrate that for\nDAE systems ranging from index-1 to index-3, DAE-KAN reduces the absolute\nerrors of both differential and algebraic variables by 1 to 2 orders of\nmagnitude compared to traditional PINNs. To assess the effectiveness of this\napproach, we analyze the drift-off error and find that both PINNs and DAE-KAN\noutperform classical numerical methods in controlling this phenomenon. Our\nresults highlight the potential of neural network methods, particularly\nDAE-KAN, in solving high-index DAEs with substantial computational accuracy and\ngeneralization, offering a promising solution for challenging partial\ndifferential-algebraic equations.", "published": "2025-04-22 11:42:02", "link": "http://arxiv.org/abs/2504.15806v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Insights from Verification: Training a Verilog Generation LLM with Reinforcement Learning with Testbench Feedback", "abstract": "Large language models (LLMs) have shown strong performance in Verilog\ngeneration from natural language description. However, ensuring the functional\ncorrectness of the generated code remains a significant challenge. This paper\nintroduces a method that integrates verification insights from testbench into\nthe training of Verilog generation LLMs, aligning the training with the\nfundamental goal of hardware design: functional correctness. The main obstacle\nin using LLMs for Verilog code generation is the lack of sufficient functional\nverification data, particularly testbenches paired with design specifications\nand code. To address this problem, we introduce an automatic testbench\ngeneration pipeline that decomposes the process and uses feedback from the\nVerilog compiler simulator (VCS) to reduce hallucination and ensure\ncorrectness. We then use the testbench to evaluate the generated codes and\ncollect them for further training, where verification insights are introduced.\nOur method applies reinforcement learning (RL), specifically direct preference\noptimization (DPO), to align Verilog code generation with functional\ncorrectness by training preference pairs based on testbench outcomes. In\nevaluations on VerilogEval-Machine, VerilogEval-Human, RTLLM v1.1, RTLLM v2,\nand VerilogEval v2, our approach consistently outperforms state-of-the-art\nbaselines in generating functionally correct Verilog code. We open source all\ntraining code, data, and models at\nhttps://anonymous.4open.science/r/VeriPrefer-E88B.", "published": "2025-04-22 11:38:14", "link": "http://arxiv.org/abs/2504.15804v1", "categories": ["cs.AR", "cs.AI"], "primary_category": "cs.AR"}
{"title": "Crisp complexity of fuzzy classifiers", "abstract": "Rule-based systems are a very popular form of explainable AI, particularly in\nthe fuzzy community, where fuzzy rules are widely used for control and\nclassification problems. However, fuzzy rule-based classifiers struggle to\nreach bigger traction outside of fuzzy venues, because users sometimes do not\nknow about fuzzy and because fuzzy partitions are not so easy to interpret in\nsome situations. In this work, we propose a methodology to reduce fuzzy\nrule-based classifiers to crisp rule-based classifiers. We study different\npossible crisp descriptions and implement an algorithm to obtain them. Also, we\nanalyze the complexity of the resulting crisp classifiers. We believe that our\nresults can help both fuzzy and non-fuzzy practitioners understand better the\nway in which fuzzy rule bases partition the feature space and how easily one\nsystem can be translated to another and vice versa. Our complexity metric can\nalso help to choose between different fuzzy classifiers based on what the\nequivalent crisp partitions look like.", "published": "2025-04-22 11:06:25", "link": "http://arxiv.org/abs/2504.15791v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents", "abstract": "Can we build accurate world models out of large language models (LLMs)? How\ncan world models benefit LLM agents? The gap between the prior knowledge of\nLLMs and the specified environment's dynamics usually bottlenecks LLMs'\nperformance as world models. To bridge the gap, we propose a training-free\n\"world alignment\" that learns an environment's symbolic knowledge complementary\nto LLMs. The symbolic knowledge covers action rules, knowledge graphs, and\nscene graphs, which are extracted by LLMs from exploration trajectories and\nencoded into executable codes to regulate LLM agents' policies. We further\npropose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive\ncontrol (MPC) framework. Unlike classical MPC requiring costly optimization on\nthe fly, we adopt an LLM agent as an efficient look-ahead optimizer of future\nsteps' actions by interacting with the neurosymbolic world model. While the LLM\nagent's strong heuristics make it an efficient planner in MPC, the quality of\nits planned actions is also secured by the accurate predictions of the aligned\nworld model. They together considerably improve learning efficiency in a new\nenvironment. On open-world challenges in Mars (Minecraft like) and ALFWorld\n(embodied indoor environments), WALL-E 2.0 significantly outperforms existing\nmethods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and\nby at least 61.7% in score. In ALFWorld, it achieves a new record 98% success\nrate after only 4 iterations.", "published": "2025-04-22 10:58:27", "link": "http://arxiv.org/abs/2504.15785v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Shannon invariants: A scalable approach to information decomposition", "abstract": "Distributed systems, such as biological and artificial neural networks,\nprocess information via complex interactions engaging multiple subsystems,\nresulting in high-order patterns with distinct properties across scales.\nInvestigating how these systems process information remains challenging due to\ndifficulties in defining appropriate multivariate metrics and ensuring their\nscalability to large systems. To address these challenges, we introduce a novel\nframework based on what we call \"Shannon invariants\" -- quantities that capture\nessential properties of high-order information processing in a way that depends\nonly on the definition of entropy and can be efficiently calculated for large\nsystems. Our theoretical results demonstrate how Shannon invariants can be used\nto resolve long-standing ambiguities regarding the interpretation of widely\nused multivariate information-theoretic measures. Moreover, our practical\nresults reveal distinctive information-processing signatures of various deep\nlearning architectures across layers, which lead to new insights into how these\nsystems process information and how this evolves during training. Overall, our\nframework resolves fundamental limitations in analyzing high-order phenomena\nand offers broad opportunities for theoretical developments and empirical\nanalyses.", "published": "2025-04-22 10:41:38", "link": "http://arxiv.org/abs/2504.15779v1", "categories": ["cs.IT", "cs.AI", "cs.LG", "math.IT", "nlin.AO", "physics.data-an"], "primary_category": "cs.IT"}
{"title": "Clifford Group Equivariant Diffusion Models for 3D Molecular Generation", "abstract": "This paper explores leveraging the Clifford algebra's expressive power for\n$\\E(n)$-equivariant diffusion models. We utilize the geometric products between\nClifford multivectors and the rich geometric information encoded in Clifford\nsubspaces in \\emph{Clifford Diffusion Models} (CDMs). We extend the diffusion\nprocess beyond just Clifford one-vectors to incorporate all higher-grade\nmultivector subspaces. The data is embedded in grade-$k$ subspaces, allowing us\nto apply latent diffusion across complete multivectors. This enables CDMs to\ncapture the joint distribution across different subspaces of the algebra,\nincorporating richer geometric information through higher-order features. We\nprovide empirical results for unconditional molecular generation on the QM9\ndataset, showing that CDMs provide a promising avenue for generative modeling.", "published": "2025-04-22 10:30:06", "link": "http://arxiv.org/abs/2504.15773v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Dynamic Intent Queries for Motion Transformer-based Trajectory Prediction", "abstract": "In autonomous driving, accurately predicting the movements of other traffic\nparticipants is crucial, as it significantly influences a vehicle's planning\nprocesses. Modern trajectory prediction models strive to interpret complex\npatterns and dependencies from agent and map data. The Motion Transformer (MTR)\narchitecture and subsequent work define the most accurate methods in common\nbenchmarks such as the Waymo Open Motion Benchmark. The MTR model employs\npre-generated static intention points as initial goal points for trajectory\nprediction. However, the static nature of these points frequently leads to\nmisalignment with map data in specific traffic scenarios, resulting in\nunfeasible or unrealistic goal points. Our research addresses this limitation\nby integrating scene-specific dynamic intention points into the MTR model. This\nadaptation of the MTR model was trained and evaluated on the Waymo Open Motion\nDataset. Our findings demonstrate that incorporating dynamic intention points\nhas a significant positive impact on trajectory prediction accuracy, especially\nfor predictions over long time horizons. Furthermore, we analyze the impact on\nground truth trajectories which are not compliant with the map data or are\nillegal maneuvers.", "published": "2025-04-22 10:20:35", "link": "http://arxiv.org/abs/2504.15766v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "iMedic: Towards Smartphone-based Self-Auscultation Tool for AI-Powered Pediatric Respiratory Assessment", "abstract": "Respiratory auscultation is crucial for early detection of pediatric\npneumonia, a condition that can quickly worsen without timely intervention. In\nareas with limited physician access, effective auscultation is challenging. We\npresent a smartphone-based system that leverages built-in microphones and\nadvanced deep learning algorithms to detect abnormal respiratory sounds\nindicative of pneumonia risk. Our end-to-end deep learning framework employs\ndomain generalization to integrate a large electronic stethoscope dataset with\na smaller smartphone-derived dataset, enabling robust feature learning for\naccurate respiratory assessments without expensive equipment. The accompanying\nmobile application guides caregivers in collecting high-quality lung sound\nsamples and provides immediate feedback on potential pneumonia risks. User\nstudies show strong classification performance and high acceptance,\ndemonstrating the system's ability to facilitate proactive interventions and\nreduce preventable childhood pneumonia deaths. By seamlessly integrating into\nubiquitous smartphones, this approach offers a promising avenue for more\nequitable and comprehensive remote pediatric care.", "published": "2025-04-22 09:45:50", "link": "http://arxiv.org/abs/2504.15743v1", "categories": ["cs.HC", "cs.AI", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Collaborative Split Federated Learning with Parallel Training and Aggregation", "abstract": "Federated learning (FL) operates based on model exchanges between the server\nand the clients, and it suffers from significant client-side computation and\ncommunication burden. Split federated learning (SFL) arises a promising\nsolution by splitting the model into two parts, that are trained sequentially:\nthe clients train the first part of the model (client-side model) and transmit\nit to the server that trains the second (server-side model). Existing SFL\nschemes though still exhibit long training delays and significant communication\noverhead, especially when clients of different computing capability\nparticipate. Thus, we propose Collaborative-Split Federated Learning~(C-SFL), a\nnovel scheme that splits the model into three parts, namely the model parts\ntrained at the computationally weak clients, the ones trained at the\ncomputationally strong clients, and the ones at the server. Unlike existing\nworks, C-SFL enables parallel training and aggregation of model's parts at the\nclients and at the server, resulting in reduced training delays and\ncommmunication overhead while improving the model's accuracy. Experiments\nverify the multiple gains of C-SFL against the existing schemes.", "published": "2025-04-22 09:18:57", "link": "http://arxiv.org/abs/2504.15724v1", "categories": ["cs.DC", "cs.AI"], "primary_category": "cs.DC"}
{"title": "Implementing Rational Choice Functions with LLMs and Measuring their Alignment with User Preferences", "abstract": "As large language models (LLMs) become integral to intelligent user\ninterfaces (IUIs), their role as decision-making agents raises critical\nconcerns about alignment. Although extensive research has addressed issues such\nas factuality, bias, and toxicity, comparatively little attention has been paid\nto measuring alignment to preferences, i.e., the relative desirability of\ndifferent alternatives, a concept used in decision making, economics, and\nsocial choice theory. However, a reliable decision-making agent makes choices\nthat align well with user preferences.\n  In this paper, we generalize existing methods that exploit LLMs for ranking\nalternative outcomes by addressing alignment with the broader and more flexible\nconcept of user preferences, which includes both strict preferences and\nindifference among alternatives. To this end, we put forward design principles\nfor using LLMs to implement rational choice functions, and provide the\nnecessary tools to measure preference satisfaction. We demonstrate the\napplicability of our approach through an empirical study in a practical\napplication of an IUI in the automotive domain.", "published": "2025-04-22 09:08:21", "link": "http://arxiv.org/abs/2504.15719v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large Language Models", "abstract": "Effective reasoning remains a core challenge for large language models (LLMs)\nin the financial domain, where tasks often require domain-specific knowledge,\nprecise numerical calculations, and strict adherence to compliance rules. We\npropose DianJin-R1, a reasoning-enhanced framework designed to address these\nchallenges through reasoning-augmented supervision and reinforcement learning.\nCentral to our approach is DianJin-R1-Data, a high-quality dataset constructed\nfrom CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance\nCheck, CCC), combining diverse financial reasoning scenarios with verified\nannotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from\nQwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that\ngenerates both reasoning steps and final answers. To further refine reasoning\nquality, we apply Group Relative Policy Optimization (GRPO), a reinforcement\nlearning method that incorporates dual reward signals: one encouraging\nstructured outputs and another rewarding answer correctness. We evaluate our\nmodels on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and\ntwo general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental\nresults show that DianJin-R1 models consistently outperform their non-reasoning\ncounterparts, especially on complex financial tasks. Moreover, on the\nreal-world CCC dataset, our single-call reasoning models match or even surpass\nthe performance of multi-agent systems that require significantly more\ncomputational cost. These findings demonstrate the effectiveness of DianJin-R1\nin enhancing financial reasoning through structured supervision and\nreward-aligned learning, offering a scalable and practical solution for\nreal-world applications.", "published": "2025-04-22 09:01:04", "link": "http://arxiv.org/abs/2504.15716v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "RePOPE: Impact of Annotation Errors on the POPE Benchmark", "abstract": "Since data annotation is costly, benchmark datasets often incorporate labels\nfrom established image datasets. In this work, we assess the impact of label\nerrors in MSCOCO on the frequently used object hallucination benchmark POPE. We\nre-annotate the benchmark images and identify an imbalance in annotation errors\nacross different subsets. Evaluating multiple models on the revised labels,\nwhich we denote as RePOPE, we observe notable shifts in model rankings,\nhighlighting the impact of label quality. Code and data are available at\nhttps://github.com/YanNeu/RePOPE .", "published": "2025-04-22 08:47:59", "link": "http://arxiv.org/abs/2504.15707v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation", "abstract": "Embodied agents exhibit immense potential across a multitude of domains,\nmaking the assurance of their behavioral safety a fundamental prerequisite for\ntheir widespread deployment. However, existing research predominantly\nconcentrates on the security of general large language models, lacking\nspecialized methodologies for establishing safety benchmarks and input\nmoderation tailored to embodied agents. To bridge this gap, this paper\nintroduces a novel input moderation framework, meticulously designed to\nsafeguard embodied agents. This framework encompasses the entire pipeline,\nincluding taxonomy definition, dataset curation, moderator architecture, model\ntraining, and rigorous evaluation. Notably, we introduce EAsafetyBench, a\nmeticulously crafted safety benchmark engineered to facilitate both the\ntraining and stringent assessment of moderators specifically designed for\nembodied agents. Furthermore, we propose Pinpoint, an innovative\nprompt-decoupled input moderation scheme that harnesses a masked attention\nmechanism to effectively isolate and mitigate the influence of functional\nprompts on moderation tasks. Extensive experiments conducted on diverse\nbenchmark datasets and models validate the feasibility and efficacy of the\nproposed approach. The results demonstrate that our methodologies achieve an\nimpressive average detection accuracy of 94.58%, surpassing the performance of\nexisting state-of-the-art techniques, alongside an exceptional moderation\nprocessing time of merely 0.002 seconds per instance.", "published": "2025-04-22 08:34:35", "link": "http://arxiv.org/abs/2504.15699v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Exploring Inevitable Waypoints for Unsolvability Explanation in Hybrid Planning Problems", "abstract": "Explaining unsolvability of planning problems is of significant research\ninterest in Explainable AI Planning. AI planning literature has reported\nseveral research efforts on generating explanations of solutions to planning\nproblems. However, explaining the unsolvability of planning problems remains a\nlargely open and understudied problem. A widely practiced approach to plan\ngeneration and automated problem solving, in general, is to decompose tasks\ninto sub-problems that help progressively converge towards the goal. In this\npaper, we propose to adopt the same philosophy of sub-problem identification as\na mechanism for analyzing and explaining unsolvability of planning problems in\nhybrid systems. In particular, for a given unsolvable planning problem, we\npropose to identify common waypoints, which are universal obstacles to plan\nexistence; in other words, they appear on every plan from the source to the\nplanning goal. This work envisions such waypoints as sub-problems of the\nplanning problem and the unreachability of any of these waypoints as an\nexplanation for the unsolvability of the original planning problem. We propose\na novel method of waypoint identification by casting the problem as an instance\nof the longest common subsequence problem, a widely popular problem in computer\nscience, typically considered as an illustrative example for the dynamic\nprogramming paradigm. Once the waypoints are identified, we perform symbolic\nreachability analysis on them to identify the earliest unreachable waypoint and\nreport it as the explanation of unsolvability. We present experimental results\non unsolvable planning problems in hybrid domains.", "published": "2025-04-22 07:45:30", "link": "http://arxiv.org/abs/2504.15668v1", "categories": ["cs.AI", "cs.FL", "I.2.0; F.4.3"], "primary_category": "cs.AI"}
{"title": "FADEL: Uncertainty-aware Fake Audio Detection with Evidential Deep Learning", "abstract": "Recently, fake audio detection has gained significant attention, as\nadvancements in speech synthesis and voice conversion have increased the\nvulnerability of automatic speaker verification (ASV) systems to spoofing\nattacks. A key challenge in this task is generalizing models to detect unseen,\nout-of-distribution (OOD) attacks. Although existing approaches have shown\npromising results, they inherently suffer from overconfidence issues due to the\nusage of softmax for classification, which can produce unreliable predictions\nwhen encountering unpredictable spoofing attempts. To deal with this\nlimitation, we propose a novel framework called fake audio detection with\nevidential learning (FADEL). By modeling class probabilities with a Dirichlet\ndistribution, FADEL incorporates model uncertainty into its predictions,\nthereby leading to more robust performance in OOD scenarios. Experimental\nresults on the ASVspoof2019 Logical Access (LA) and ASVspoof2021 LA datasets\nindicate that the proposed method significantly improves the performance of\nbaseline models. Furthermore, we demonstrate the validity of uncertainty\nestimation by analyzing a strong correlation between average uncertainty and\nequal error rate (EER) across different spoofing algorithms.", "published": "2025-04-22 07:40:35", "link": "http://arxiv.org/abs/2504.15663v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "A Vision-Enabled Prosthetic Hand for Children with Upper Limb Disabilities", "abstract": "This paper introduces a novel AI vision-enabled pediatric prosthetic hand\ndesigned to assist children aged 10-12 with upper limb disabilities. The\nprosthesis features an anthropomorphic appearance, multi-articulating\nfunctionality, and a lightweight design that mimics a natural hand, making it\nboth accessible and affordable for low-income families. Using 3D printing\ntechnology and integrating advanced machine vision, sensing, and embedded\ncomputing, the prosthetic hand offers a low-cost, customizable solution that\naddresses the limitations of current myoelectric prostheses. A micro camera is\ninterfaced with a low-power FPGA for real-time object detection and assists\nwith precise grasping. The onboard DL-based object detection and grasp\nclassification models achieved accuracies of 96% and 100% respectively. In the\nforce prediction, the mean absolute error was found to be 0.018. The features\nof the proposed prosthetic hand can thus be summarized as: a) a wrist-mounted\nmicro camera for artificial sensing, enabling a wide range of hand-based tasks;\nb) real-time object detection and distance estimation for precise grasping; and\nc) ultra-low-power operation that delivers high performance within constrained\npower and resource limits.", "published": "2025-04-22 07:23:51", "link": "http://arxiv.org/abs/2504.15654v1", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "DR.FIX: Automatically Fixing Data Races at Industry Scale", "abstract": "Data races are a prevalent class of concurrency bugs in shared-memory\nparallel programs, posing significant challenges to software reliability and\nreproducibility. While there is an extensive body of research on detecting data\nraces and a wealth of practical detection tools across various programming\nlanguages, considerably less effort has been directed toward automatically\nfixing data races at an industrial scale. In large codebases, data races are\ncontinuously introduced and exhibit myriad patterns, making automated fixing\nparticularly challenging.\n  In this paper, we tackle the problem of automatically fixing data races at an\nindustrial scale. We present Dr.Fix, a tool that combines large language models\n(LLMs) with program analysis to generate fixes for data races in real-world\nsettings, effectively addressing a broad spectrum of racy patterns in complex\ncode contexts. Implemented for Go--the programming language widely used in\nmodern microservice architectures where concurrency is pervasive and data races\nare common--Dr.Fix seamlessly integrates into existing development workflows.\nWe detail the design of Dr.Fix and examine how individual design choices\ninfluence the quality of the fixes produced. Over the past 18 months, Dr.Fix\nhas been integrated into developer workflows at Uber demonstrating its\npractical utility. During this period, Dr.Fix produced patches for 224 (55%)\nfrom a corpus of 404 data races spanning various categories; 193 of these\npatches (86%) were accepted by more than a hundred developers via code reviews\nand integrated into the codebase.", "published": "2025-04-22 06:56:15", "link": "http://arxiv.org/abs/2504.15637v1", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PL", "cs.SE"], "primary_category": "cs.DC"}
{"title": "Enhancing Reinforcement learning in 3-Dimensional Hydrophobic-Polar Protein Folding Model with Attention-based layers", "abstract": "Transformer-based architectures have recently propelled advances in sequence\nmodeling across domains, but their application to the hydrophobic-hydrophilic\n(H-P) model for protein folding remains relatively unexplored. In this work, we\nadapt a Deep Q-Network (DQN) integrated with attention mechanisms\n(Transformers) to address the 3D H-P protein folding problem. Our system\nformulates folding decisions as a self-avoiding walk in a reinforced\nenvironment, and employs a specialized reward function based on favorable\nhydrophobic interactions. To improve performance, the method incorporates\nvalidity check including symmetry-breaking constraints, dueling and double\nQ-learning, and prioritized replay to focus learning on critical transitions.\nExperimental evaluations on standard benchmark sequences demonstrate that our\napproach achieves several known best solutions for shorter sequences, and\nobtains near-optimal results for longer chains. This study underscores the\npromise of attention-based reinforcement learning for protein folding, and\ncreated a prototype of Transformer-based Q-network structure for 3-dimensional\nlattice models.", "published": "2025-04-22 06:53:36", "link": "http://arxiv.org/abs/2504.15634v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings", "abstract": "The current study describes a cost-effective method for adapting large\nlanguage models (LLMs) for academic advising with study-abroad contexts in mind\nand for application in low-resource methods for acculturation. With the\nMistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and\na 4-bit quantization method, the model underwent training in two distinct\nstages related to this study's purpose to enhance domain specificity while\nmaintaining computational efficiency. In Phase 1, the model was conditioned\nwith a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained\nwith manually curated datasets from the StudyAbroadGPT project to achieve\nenhanced, contextualized responses. Technical innovations entailed\nmemory-efficient quantization, parameter-efficient adaptation, and continuous\ntraining analytics via Weights & Biases. After training, this study\ndemonstrated a reduction in training loss by 52.7%, 92% accuracy in\ndomain-specific recommendations, achieved 95% markdown-based formatting\nsupport, and a median run-rate of 100 samples per second on off-the-shelf GPU\nequipment. These findings support the effective application of\ninstruction-tuned LLMs within educational advisers, especially in low-resource\ninstitutional scenarios. Limitations included decreased generalizability and\nthe application of a synthetically generated dataset, but this framework is\nscalable for adding new multilingual-augmented and real-time academic advising\nprocesses. Future directions may include plans for the integration of\nretrieval-augmented generation, applying dynamic quantization routines, and\nconnecting to real-time academic databases to increase adaptability and\naccuracy.", "published": "2025-04-22 06:08:13", "link": "http://arxiv.org/abs/2504.15610v1", "categories": ["cs.AI", "68T05 (Learning and adaptive systems), 68T07 (Artificial\n  intelligence and education)"], "primary_category": "cs.AI"}
{"title": "MetaMolGen: A Neural Graph Motif Generation Model for De Novo Molecular Design", "abstract": "Molecular generation plays an important role in drug discovery and materials\nscience, especially in data-scarce scenarios where traditional generative\nmodels often struggle to achieve satisfactory conditional generalization. To\naddress this challenge, we propose MetaMolGen, a first-order\nmeta-learning-based molecular generator designed for few-shot and\nproperty-conditioned molecular generation. MetaMolGen standardizes the\ndistribution of graph motifs by mapping them to a normalized latent space, and\nemploys a lightweight autoregressive sequence model to generate SMILES\nsequences that faithfully reflect the underlying molecular structure. In\naddition, it supports conditional generation of molecules with target\nproperties through a learnable property projector integrated into the\ngenerative process.Experimental results demonstrate that MetaMolGen\nconsistently generates valid and diverse SMILES sequences under low-data\nregimes, outperforming conventional baselines. This highlights its advantage in\nfast adaptation and efficient conditional generation for practical molecular\ndesign.", "published": "2025-04-22 05:04:33", "link": "http://arxiv.org/abs/2504.15587v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "A Large-scale Class-level Benchmark Dataset for Code Generation with LLMs", "abstract": "Recent advancements in large language models (LLMs) have demonstrated\npromising capabilities in code generation tasks. However, most existing\nbenchmarks focus on isolated functions and fail to capture the complexity of\nreal-world, class-level software structures. To address this gap, we introduce\na large-scale, Python class-level dataset curated from $13{,}174$ real-world\nopen-source projects. The dataset contains over 842,000 class skeletons, each\nincluding class and method signatures, along with associated docstrings when\navailable. We preserve structural and contextual dependencies critical to\nrealistic software development scenarios and enrich the dataset with static\ncode metrics to support downstream analysis. To evaluate the usefulness of this\ndataset, we use extracted class skeletons as prompts for GPT-4 to generate full\nclass implementations. Results show that the LLM-generated classes exhibit\nstrong lexical and structural similarity to human-written counterparts, with\naverage ROUGE@L, BLEU, and TSED scores of 0.80, 0.59, and 0.73, respectively.\nThese findings confirm that well-structured prompts derived from real-world\nclass skeletons significantly enhance LLM performance in class-level code\ngeneration. This dataset offers a valuable resource for benchmarking, training,\nand improving LLMs in realistic software engineering contexts.", "published": "2025-04-22 03:33:57", "link": "http://arxiv.org/abs/2504.15564v1", "categories": ["cs.SE", "cs.AI", "cs.LG"], "primary_category": "cs.SE"}
{"title": "A Multi-Agent Framework for Automated Qinqiang Opera Script Generation Using Large Language Models", "abstract": "This paper introduces a novel multi-Agent framework that automates the end to\nend production of Qinqiang opera by integrating Large Language Models , visual\ngeneration, and Text to Speech synthesis. Three specialized agents collaborate\nin sequence: Agent1 uses an LLM to craft coherent, culturally grounded\nscripts;Agent2 employs visual generation models to render contextually accurate\nstage scenes; and Agent3 leverages TTS to produce synchronized, emotionally\nexpressive vocal performances. In a case study on Dou E Yuan, the system\nachieved expert ratings of 3.8 for script fidelity, 3.5 for visual coherence,\nand 3.8 for speech accuracy-culminating in an overall score of 3.6, a 0.3 point\nimprovement over a Single Agent baseline. Ablation experiments demonstrate that\nremoving Agent2 or Agent3 leads to drops of 0.4 and 0.5 points, respectively,\nunderscoring the value of modular collaboration. This work showcases how AI\ndriven pipelines can streamline and scale the preservation of traditional\nperforming arts, and points toward future enhancements in cross modal\nalignment, richer emotional nuance, and support for additional opera genres.", "published": "2025-04-22 03:14:29", "link": "http://arxiv.org/abs/2504.15552v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Do It For Me vs. Do It With Me: Investigating User Perceptions of Different Paradigms of Automation in Copilots for Feature-Rich Software", "abstract": "Large Language Model (LLM)-based in-application assistants, or copilots, can\nautomate software tasks, but users often prefer learning by doing, raising\nquestions about the optimal level of automation for an effective user\nexperience. We investigated two automation paradigms by designing and\nimplementing a fully automated copilot (AutoCopilot) and a semi-automated\ncopilot (GuidedCopilot) that automates trivial steps while offering\nstep-by-step visual guidance. In a user study (N=20) across data analysis and\nvisual design tasks, GuidedCopilot outperformed AutoCopilot in user control,\nsoftware utility, and learnability, especially for exploratory and creative\ntasks, while AutoCopilot saved time for simpler visual tasks. A follow-up\ndesign exploration (N=10) enhanced GuidedCopilot with task-and state-aware\nfeatures, including in-context preview clips and adaptive instructions. Our\nfindings highlight the critical role of user control and tailored guidance in\ndesigning the next generation of copilots that enhance productivity, support\ndiverse skill levels, and foster deeper software engagement.", "published": "2025-04-22 03:11:10", "link": "http://arxiv.org/abs/2504.15549v1", "categories": ["cs.HC", "cs.AI", "cs.LG"], "primary_category": "cs.HC"}
{"title": "A Framework for Testing and Adapting REST APIs as LLM Tools", "abstract": "Large Language Models (LLMs) are enabling autonomous agents to perform\ncomplex workflows using external tools or functions, often provided via REST\nAPIs in enterprise systems. However, directly utilizing these APIs as tools\nposes challenges due to their complex input schemas, elaborate responses, and\noften ambiguous documentation. Current benchmarks for tool testing do not\nadequately address these complexities, leading to a critical gap in evaluating\nAPI readiness for agent-driven automation. In this work, we present a novel\ntesting framework aimed at evaluating and enhancing the readiness of REST APIs\nto function as tools for LLM-based agents. Our framework transforms apis as\ntools, generates comprehensive test cases for the APIs, translates tests cases\ninto natural language instructions suitable for agents, enriches tool\ndefinitions and evaluates the agent's ability t correctly invoke the API and\nprocess its inputs and responses. To provide actionable insights, we analyze\nthe outcomes of 750 test cases, presenting a detailed taxonomy of errors,\nincluding input misinterpretation, output handling inconsistencies, and schema\nmismatches. Additionally, we classify these test cases to streamline debugging\nand refinement of tool integrations. This work offers a foundational step\ntoward enabling enterprise APIs as tools, improving their usability in\nagent-based applications.", "published": "2025-04-22 02:52:08", "link": "http://arxiv.org/abs/2504.15546v1", "categories": ["cs.SE", "cs.AI", "I.2.7"], "primary_category": "cs.SE"}
{"title": "Transport f divergences", "abstract": "We define a class of divergences to measure differences between probability\ndensity functions in one-dimensional sample space. The construction is based on\nthe convex function with the Jacobi operator of mapping function that\npushforwards one density to the other. We call these information measures {\\em\ntransport $f$-divergences}. We present several properties of transport\n$f$-divergences, including invariances, convexities, variational formulations,\nand Taylor expansions in terms of mapping functions. Examples of transport\n$f$-divergences in generative models are provided.", "published": "2025-04-22 01:25:41", "link": "http://arxiv.org/abs/2504.15515v1", "categories": ["math.ST", "cs.AI", "cs.IT", "math.IT", "stat.TH"], "primary_category": "math.ST"}
{"title": "Guillotine: Hypervisors for Isolating Malicious AIs", "abstract": "As AI models become more embedded in critical sectors like finance,\nhealthcare, and the military, their inscrutable behavior poses ever-greater\nrisks to society. To mitigate this risk, we propose Guillotine, a hypervisor\narchitecture for sandboxing powerful AI models -- models that, by accident or\nmalice, can generate existential threats to humanity. Although Guillotine\nborrows some well-known virtualization techniques, Guillotine must also\nintroduce fundamentally new isolation mechanisms to handle the unique threat\nmodel posed by existential-risk AIs. For example, a rogue AI may try to\nintrospect upon hypervisor software or the underlying hardware substrate to\nenable later subversion of that control plane; thus, a Guillotine hypervisor\nrequires careful co-design of the hypervisor software and the CPUs, RAM, NIC,\nand storage devices that support the hypervisor software, to thwart side\nchannel leakage and more generally eliminate mechanisms for AI to exploit\nreflection-based vulnerabilities. Beyond such isolation at the software,\nnetwork, and microarchitectural layers, a Guillotine hypervisor must also\nprovide physical fail-safes more commonly associated with nuclear power plants,\navionic platforms, and other types of mission critical systems. Physical\nfail-safes, e.g., involving electromechanical disconnection of network cables,\nor the flooding of a datacenter which holds a rogue AI, provide defense in\ndepth if software, network, and microarchitectural isolation is compromised and\na rogue AI must be temporarily shut down or permanently destroyed.", "published": "2025-04-22 00:29:18", "link": "http://arxiv.org/abs/2504.15499v1", "categories": ["cs.CR", "cs.AI", "cs.OS"], "primary_category": "cs.CR"}
{"title": "Scalable APT Malware Classification via Parallel Feature Extraction and GPU-Accelerated Learning", "abstract": "This paper presents an underlying framework for both automating and\naccelerating malware classification, more specifically, mapping malicious\nexecutables to known Advanced Persistent Threat (APT) groups. The main feature\nof this analysis is the assembly-level instructions present in executables\nwhich are also known as opcodes. The collection of such opcodes on many\nmalicious samples is a lengthy process; hence, open-source reverse engineering\ntools are used in tandem with scripts that leverage parallel computing to\nanalyze multiple files at once. Traditional and deep learning models are\napplied to create models capable of classifying malware samples. One-gram and\ntwo-gram datasets are constructed and used to train models such as SVM, KNN,\nand Decision Tree; however, they struggle to provide adequate results without\nrelying on metadata to support n-gram sequences. The computational limitations\nof such models are overcome with convolutional neural networks (CNNs) and\nheavily accelerated using graphical compute unit (GPU) resources.", "published": "2025-04-22 00:05:05", "link": "http://arxiv.org/abs/2504.15497v1", "categories": ["cs.CR", "cs.AI", "I.2.0; I.2.6; K.6.5"], "primary_category": "cs.CR"}
{"title": "MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention", "abstract": "The integration of long-context capabilities with visual understanding\nunlocks unprecedented potential for Vision Language Models (VLMs). However, the\nquadratic attention complexity during the pre-filling phase remains a\nsignificant obstacle to real-world deployment. To overcome this limitation, we\nintroduce MMInference (Multimodality Million tokens Inference), a dynamic\nsparse attention method that accelerates the prefilling stage for long-context\nmulti-modal inputs. First, our analysis reveals that the temporal and spatial\nlocality of video input leads to a unique sparse pattern, the Grid pattern.\nSimultaneously, VLMs exhibit markedly different sparse distributions across\ndifferent modalities. We introduce a permutation-based method to leverage the\nunique Grid pattern and handle modality boundary issues. By offline search the\noptimal sparse patterns for each head, MMInference constructs the sparse\ndistribution dynamically based on the input. We also provide optimized GPU\nkernels for efficient sparse computations. Notably, MMInference integrates\nseamlessly into existing VLM pipelines without any model modifications or\nfine-tuning. Experiments on multi-modal benchmarks-including Video QA,\nCaptioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art\nlong-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that\nMMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while\nmaintaining accuracy. Our code is available at https://aka.ms/MMInference.", "published": "2025-04-22 17:59:51", "link": "http://arxiv.org/abs/2504.16083v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding", "abstract": "We propose MR. Video, an agentic long video understanding framework that\ndemonstrates the simple yet effective MapReduce principle for processing long\nvideos: (1) Map: independently and densely perceiving short video clips, and\n(2) Reduce: jointly aggregating information from all clips. Compared with\nsequence-to-sequence vision-language models (VLMs), MR. Video performs detailed\nshort video perception without being limited by context length. Compared with\nexisting video agents that typically rely on sequential key segment selection,\nthe Map operation enables simpler and more scalable sequence parallel\nperception of short video segments. Its Reduce step allows for more\ncomprehensive context aggregation and reasoning, surpassing explicit key\nsegment retrieval. This MapReduce principle is applicable to both VLMs and\nvideo agents, and we use LLM agents to validate its effectiveness.\n  In practice, MR. Video employs two MapReduce stages: (A) Captioning:\ngenerating captions for short video clips (map), then standardizing repeated\ncharacters and objects into shared names (reduce); (B) Analysis: for each user\nquestion, analyzing relevant information from individual short videos (map),\nand integrating them into a final answer (reduce). MR. Video achieves over 10%\naccuracy improvement on the challenging LVBench compared to state-of-the-art\nVLMs and video agents.\n  Code is available at: https://github.com/ziqipang/MR-Video", "published": "2025-04-22 17:59:41", "link": "http://arxiv.org/abs/2504.16082v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning", "abstract": "Recent text-to-image diffusion models achieve impressive visual quality\nthrough extensive scaling of training data and model parameters, yet they often\nstruggle with complex scenes and fine-grained details. Inspired by the\nself-reflection capabilities emergent in large language models, we propose\nReflectionFlow, an inference-time framework enabling diffusion models to\niteratively reflect upon and refine their outputs. ReflectionFlow introduces\nthree complementary inference-time scaling axes: (1) noise-level scaling to\noptimize latent initialization; (2) prompt-level scaling for precise semantic\nguidance; and most notably, (3) reflection-level scaling, which explicitly\nprovides actionable reflections to iteratively assess and correct previous\ngenerations. To facilitate reflection-level scaling, we construct GenRef, a\nlarge-scale dataset comprising 1 million triplets, each containing a\nreflection, a flawed image, and an enhanced image. Leveraging this dataset, we\nefficiently perform reflection tuning on state-of-the-art diffusion\ntransformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified\nframework. Experimental results show that ReflectionFlow significantly\noutperforms naive noise-level scaling methods, offering a scalable and\ncompute-efficient solution toward higher-quality image synthesis on challenging\ntasks.", "published": "2025-04-22 17:58:07", "link": "http://arxiv.org/abs/2504.16080v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis", "abstract": "Latent diffusion models (LDMs) dominate high-quality image generation, yet\nintegrating representation learning with generative modeling remains a\nchallenge. We introduce a novel generative image modeling framework that\nseamlessly bridges this gap by leveraging a diffusion model to jointly model\nlow-level image latents (from a variational autoencoder) and high-level\nsemantic features (from a pretrained self-supervised encoder like DINO). Our\nlatent-semantic diffusion approach learns to generate coherent image-feature\npairs from pure noise, significantly enhancing both generative quality and\ntraining efficiency, all while requiring only minimal modifications to standard\nDiffusion Transformer architectures. By eliminating the need for complex\ndistillation objectives, our unified design simplifies training and unlocks a\npowerful new inference strategy: Representation Guidance, which leverages\nlearned semantics to steer and refine image generation. Evaluated in both\nconditional and unconditional settings, our method delivers substantial\nimprovements in image quality and training convergence speed, establishing a\nnew direction for representation-aware generative modeling.", "published": "2025-04-22 17:41:42", "link": "http://arxiv.org/abs/2504.16064v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ForesightNav: Learning Scene Imagination for Efficient Exploration", "abstract": "Understanding how humans leverage prior knowledge to navigate unseen\nenvironments while making exploratory decisions is essential for developing\nautonomous robots with similar abilities. In this work, we propose\nForesightNav, a novel exploration strategy inspired by human imagination and\nreasoning. Our approach equips robotic agents with the capability to predict\ncontextual information, such as occupancy and semantic details, for unexplored\nregions. These predictions enable the robot to efficiently select meaningful\nlong-term navigation goals, significantly enhancing exploration in unseen\nenvironments. We validate our imagination-based approach using the Structured3D\ndataset, demonstrating accurate occupancy prediction and superior performance\nin anticipating unseen scene geometry. Our experiments show that the\nimagination module improves exploration efficiency in unseen environments,\nachieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav\non the Structured3D Validation split. These contributions demonstrate the power\nof imagination-driven reasoning for autonomous systems to enhance generalizable\nand efficient exploration.", "published": "2025-04-22 17:38:38", "link": "http://arxiv.org/abs/2504.16062v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale", "abstract": "Recent video large language models (Video LLMs) often depend on costly human\nannotations or proprietary model APIs (e.g., GPT-4o) to produce training data,\nwhich limits their training at scale. In this paper, we explore large-scale\ntraining for Video LLM with cheap automatic speech recognition (ASR)\ntranscripts. Specifically, we propose a novel streaming training approach that\ndensely interleaves the ASR words and video frames according to their\ntimestamps. Compared to previous studies in vision-language representation with\nASR, our method naturally fits the streaming characteristics of ASR, thus\nenabling the model to learn temporally-aligned, fine-grained vision-language\nmodeling. To support the training algorithm, we introduce a data production\npipeline to process YouTube videos and their closed captions (CC, same as ASR),\nresulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset\nfor high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,\nthe ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general\nvideo QA performance and exhibits a new capability in real-time video\ncommentary. To evaluate this, we carefully design a new LiveSports-3K\nbenchmark, using LLM-as-a-judge to measure the free-form commentary.\nExperiments show our final LiveCC-7B-Instruct model can surpass advanced 72B\nmodels (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even\nworking in a real-time mode. Meanwhile, it achieves state-of-the-art results at\nthe 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,\ndemonstrating the broad generalizability of our approach. All resources of this\npaper have been released at https://showlab.github.io/livecc.", "published": "2025-04-22 16:52:09", "link": "http://arxiv.org/abs/2504.16030v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "PointLoRA: Low-Rank Adaptation with Token Selection for Point Cloud Learning", "abstract": "Self-supervised representation learning for point cloud has demonstrated\neffectiveness in improving pre-trained model performance across diverse tasks.\nHowever, as pre-trained models grow in complexity, fully fine-tuning them for\ndownstream applications demands substantial computational and storage\nresources. Parameter-efficient fine-tuning (PEFT) methods offer a promising\nsolution to mitigate these resource requirements, yet most current approaches\nrely on complex adapter and prompt mechanisms that increase tunable parameters.\nIn this paper, we propose PointLoRA, a simple yet effective method that\ncombines low-rank adaptation (LoRA) with multi-scale token selection to\nefficiently fine-tune point cloud models. Our approach embeds LoRA layers\nwithin the most parameter-intensive components of point cloud transformers,\nreducing the need for tunable parameters while enhancing global feature\ncapture. Additionally, multi-scale token selection extracts critical local\ninformation to serve as prompts for downstream fine-tuning, effectively\ncomplementing the global context captured by LoRA. The experimental results\nacross various pre-trained models and three challenging public datasets\ndemonstrate that our approach achieves competitive performance with only 3.43%\nof the trainable parameters, making it highly effective for\nresource-constrained applications. Source code is available at:\nhttps://github.com/songw-zju/PointLoRA.", "published": "2025-04-22 16:41:21", "link": "http://arxiv.org/abs/2504.16023v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Efficient Temporal Consistency in Diffusion-Based Video Editing with Adaptor Modules: A Theoretical Framework", "abstract": "Adapter-based methods are commonly used to enhance model performance with\nminimal additional complexity, especially in video editing tasks that require\nframe-to-frame consistency. By inserting small, learnable modules into\npretrained diffusion models, these adapters can maintain temporal coherence\nwithout extensive retraining. Approaches that incorporate prompt learning with\nboth shared and frame-specific tokens are particularly effective in preserving\ncontinuity across frames at low training cost. In this work, we want to provide\na general theoretical framework for adapters that maintain frame consistency in\nDDIM-based models under a temporal consistency loss. First, we prove that the\ntemporal consistency objective is differentiable under bounded feature norms,\nand we establish a Lipschitz bound on its gradient. Second, we show that\ngradient descent on this objective decreases the loss monotonically and\nconverges to a local minimum if the learning rate is within an appropriate\nrange. Finally, we analyze the stability of modules in the DDIM inversion\nprocedure, showing that the associated error remains controlled. These\ntheoretical findings will reinforce the reliability of diffusion-based video\nediting methods that rely on adapter strategies and provide theoretical\ninsights in video generation tasks.", "published": "2025-04-22 16:28:35", "link": "http://arxiv.org/abs/2504.16016v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "MVQA: Mamba with Unified Sampling for Efficient Video Quality Assessment", "abstract": "The rapid growth of long-duration, high-definition videos has made efficient\nvideo quality assessment (VQA) a critical challenge. Existing research\ntypically tackles this problem through two main strategies: reducing model\nparameters and resampling inputs. However, light-weight Convolution Neural\nNetworks (CNN) and Transformers often struggle to balance efficiency with high\nperformance due to the requirement of long-range modeling capabilities.\nRecently, the state-space model, particularly Mamba, has emerged as a promising\nalternative, offering linear complexity with respect to sequence length.\nMeanwhile, efficient VQA heavily depends on resampling long sequences to\nminimize computational costs, yet current resampling methods are often weak in\npreserving essential semantic information. In this work, we present MVQA, a\nMamba-based model designed for efficient VQA along with a novel Unified\nSemantic and Distortion Sampling (USDS) approach. USDS combines semantic patch\nsampling from low-resolution videos and distortion patch sampling from\noriginal-resolution videos. The former captures semantically dense regions,\nwhile the latter retains critical distortion details. To prevent computation\nincrease from dual inputs, we propose a fusion mechanism using pre-defined\nmasks, enabling a unified sampling strategy that captures both semantic and\nquality information without additional computational burden. Experiments show\nthat the proposed MVQA, equipped with USDS, achieve comparable performance to\nstate-of-the-art methods while being $2\\times$ as fast and requiring only $1/5$\nGPU memory.", "published": "2025-04-22 16:08:23", "link": "http://arxiv.org/abs/2504.16003v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Efficient Adaptation of Deep Neural Networks for Semantic Segmentation in Space Applications", "abstract": "In recent years, the application of Deep Learning techniques has shown\nremarkable success in various computer vision tasks, paving the way for their\ndeployment in extraterrestrial exploration. Transfer learning has emerged as a\npowerful strategy for addressing the scarcity of labeled data in these novel\nenvironments. This paper represents one of the first efforts in evaluating the\nfeasibility of employing adapters toward efficient transfer learning for rock\nsegmentation in extraterrestrial landscapes, mainly focusing on lunar and\nmartian terrains. Our work suggests that the use of adapters, strategically\nintegrated into a pre-trained backbone model, can be successful in reducing\nboth bandwidth and memory requirements for the target extraterrestrial device.\nIn this study, we considered two memory-saving strategies: layer fusion (to\nreduce to zero the inference overhead) and an ``adapter ranking'' (to also\nreduce the transmission cost). Finally, we evaluate these results in terms of\ntask performance, memory, and computation on embedded devices, evidencing\ntrade-offs that open the road to more research in the field.", "published": "2025-04-22 15:53:59", "link": "http://arxiv.org/abs/2504.15991v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A New Graph Grammar Formalism for Robust Syntactic Pattern Recognition", "abstract": "I introduce a formalism for representing the syntax of recursively structured\ngraph-like patterns. It does not use production rules, like a conventional\ngraph grammar, but represents the syntactic structure in a more direct and\ndeclarative way. The grammar and the pattern are both represented as networks,\nand parsing is seen as the construction of a homomorphism from the pattern to\nthe grammar. The grammars can represent iterative, hierarchical and nested\nrecursive structure in more than one dimension.\n  This supports a highly parallel style of parsing, in which all aspects of\npattern recognition (feature detection, segmentation, parsing, filling in\nmissing symbols, top-down and bottom-up inference) are integrated into a single\nprocess, to exploit the synergy between them.\n  The emphasis of this paper is on underlying theoretical issues, but I also\ngive some example runs to illustrate the error-tolerant parsing of complex\nrecursively structured patterns of 50-1000 symbols, involving variability in\ngeometric relationships, blurry and indistinct symbols, overlapping symbols,\ncluttered images, and erased patches.", "published": "2025-04-22 15:23:37", "link": "http://arxiv.org/abs/2504.15975v1", "categories": ["cs.FL", "cs.CV", "F.4.2; F.4.3"], "primary_category": "cs.FL"}
{"title": "Recent Advances and Future Directions in Extended Reality (XR): Exploring AI-Powered Spatial Intelligence", "abstract": "Extended Reality (XR), encompassing Augmented Reality (AR), Virtual Reality\n(VR) and Mixed Reality (MR), is a transformative technology bridging the\nphysical and virtual world and it has diverse potential which will be\nubiquitous in the future. This review examines XR's evolution through\nfoundational framework - hardware ranging from monitors to sensors and software\nranging from visual tasks to user interface; highlights state of the art (SOTA)\nXR products with the comparison and analysis of performance based on their\nfoundational framework; discusses how commercial XR devices can support the\ndemand of high-quality performance focusing on spatial intelligence. For future\ndirections, attention should be given to the integration of multi-modal AI and\nIoT-driven digital twins to enable adaptive XR systems. With the concept of\nspatial intelligence, future XR should establish a new digital space with\nrealistic experience that benefits humanity. This review underscores the\npivotal role of AI in unlocking XR as the next frontier in human-computer\ninteraction.", "published": "2025-04-22 15:11:55", "link": "http://arxiv.org/abs/2504.15970v1", "categories": ["cs.HC", "cs.CV", "cs.MA"], "primary_category": "cs.HC"}
{"title": "FreeGraftor: Training-Free Cross-Image Feature Grafting for Subject-Driven Text-to-Image Generation", "abstract": "Subject-driven image generation aims to synthesize novel scenes that\nfaithfully preserve subject identity from reference images while adhering to\ntextual guidance, yet existing methods struggle with a critical trade-off\nbetween fidelity and efficiency. Tuning-based approaches rely on time-consuming\nand resource-intensive subject-specific optimization, while zero-shot methods\nfail to maintain adequate subject consistency. In this work, we propose\nFreeGraftor, a training-free framework that addresses these limitations through\ncross-image feature grafting. Specifically, FreeGraftor employs semantic\nmatching and position-constrained attention fusion to transfer visual details\nfrom reference subjects to the generated image. Additionally, our framework\nincorporates a novel noise initialization strategy to preserve geometry priors\nof reference subjects for robust feature matching. Extensive qualitative and\nquantitative experiments demonstrate that our method enables precise subject\nidentity transfer while maintaining text-aligned scene synthesis. Without\nrequiring model fine-tuning or additional training, FreeGraftor significantly\noutperforms existing zero-shot and training-free approaches in both subject\nfidelity and text alignment. Furthermore, our framework can seamlessly extend\nto multi-subject generation, making it practical for real-world deployment. Our\ncode is available at https://github.com/Nihukat/FreeGraftor.", "published": "2025-04-22 14:55:23", "link": "http://arxiv.org/abs/2504.15958v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Visual Place Cell Encoding: A Computational Model for Spatial Representation and Cognitive Mapping", "abstract": "This paper presents the Visual Place Cell Encoding (VPCE) model, a\nbiologically inspired computational framework for simulating place cell-like\nactivation using visual input. Drawing on evidence that visual landmarks play a\ncentral role in spatial encoding, the proposed VPCE model activates visual\nplace cells by clustering high-dimensional appearance features extracted from\nimages captured by a robot-mounted camera. Each cluster center defines a\nreceptive field, and activation is computed based on visual similarity using a\nradial basis function. We evaluate whether the resulting activation patterns\ncorrelate with key properties of biological place cells, including spatial\nproximity, orientation alignment, and boundary differentiation. Experiments\ndemonstrate that the VPCE can distinguish between visually similar yet\nspatially distinct locations and adapt to environment changes such as the\ninsertion or removal of walls. These results suggest that structured visual\ninput, even in the absence of motion cues or reward-driven learning, is\nsufficient to generate place-cell-like spatial representations and support\nbiologically inspired cognitive mapping.", "published": "2025-04-22 14:49:30", "link": "http://arxiv.org/abs/2504.15953v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Reasoning Physical Video Generation with Diffusion Timestep Tokens via Reinforcement Learning", "abstract": "Despite recent progress in video generation, producing videos that adhere to\nphysical laws remains a significant challenge. Traditional diffusion-based\nmethods struggle to extrapolate to unseen physical conditions (eg, velocity)\ndue to their reliance on data-driven approximations. To address this, we\npropose to integrate symbolic reasoning and reinforcement learning to enforce\nphysical consistency in video generation. We first introduce the Diffusion\nTimestep Tokenizer (DDT), which learns discrete, recursive visual tokens by\nrecovering visual attributes lost during the diffusion process. The recursive\nvisual tokens enable symbolic reasoning by a large language model. Based on it,\nwe propose the Phys-AR framework, which consists of two stages: The first stage\nuses supervised fine-tuning to transfer symbolic knowledge, while the second\nstage applies reinforcement learning to optimize the model's reasoning\nabilities through reward functions based on physical conditions. Our approach\nallows the model to dynamically adjust and improve the physical properties of\ngenerated videos, ensuring adherence to physical laws. Experimental results\ndemonstrate that PhysAR can generate videos that are physically consistent.", "published": "2025-04-22 14:20:59", "link": "http://arxiv.org/abs/2504.15932v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Benchmarking the Reproducibility of Brain MRI Segmentation Across Scanners and Time", "abstract": "Accurate and reproducible brain morphometry from structural MRI is critical\nfor monitoring neuroanatomical changes across time and across imaging domains.\nAlthough deep learning has accelerated segmentation workflows, scanner-induced\nvariability and reproducibility limitations remain-especially in longitudinal\nand multi-site settings. In this study, we benchmark two modern segmentation\npipelines, FastSurfer and SynthSeg, both integrated into FreeSurfer, one of the\nmost widely adopted tools in neuroimaging.\n  Using two complementary datasets - a 17-year longitudinal cohort (SIMON) and\na 9-site test-retest cohort (SRPBS)-we quantify inter-scan segmentation\nvariability using Dice coefficient, Surface Dice, Hausdorff Distance (HD95),\nand Mean Absolute Percentage Error (MAPE). Our results reveal up to 7-8% volume\nvariation in small subcortical structures such as the amygdala and ventral\ndiencephalon, even under controlled test-retest conditions. This raises a key\nquestion: is it feasible to detect subtle longitudinal changes on the order of\n5-10% in pea-sized brain regions, given the magnitude of domain-induced\nmorphometric noise?\n  We further analyze the effects of registration templates and interpolation\nmodes, and propose surface-based quality filtering to improve segmentation\nreliability. This study provides a reproducible benchmark for morphometric\nreproducibility and emphasizes the need for harmonization strategies in\nreal-world neuroimaging studies.\n  Code and figures: https://github.com/kondratevakate/brain-mri-segmentation", "published": "2025-04-22 14:20:18", "link": "http://arxiv.org/abs/2504.15931v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting", "abstract": "We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a\nsystem to summarise hour long videos with no-supervision. Most existing video\nunderstanding models work well on short videos of pre-segmented events, yet\nthey struggle to summarise longer videos where relevant events are sparsely\ndistributed and not pre-segmented. Moreover, long-form video understanding\noften relies on supervised hierarchical training that needs extensive\nannotations which are costly, slow and prone to inconsistency. With ViSMaP we\nbridge the gap between short videos (where annotated data is plentiful) and\nlong ones (where it's not). We rely on LLMs to create optimised\npseudo-summaries of long videos using segment descriptions from short ones.\nThese pseudo-summaries are used as training data for a model that generates\nlong-form video summaries, bypassing the need for expensive annotations of long\nvideos. Specifically, we adopt a meta-prompting strategy to iteratively\ngenerate and refine creating pseudo-summaries of long videos. The strategy\nleverages short clip descriptions obtained from a supervised short video model\nto guide the summary. Each iteration uses three LLMs working in sequence: one\nto generate the pseudo-summary from clip descriptions, another to evaluate it,\nand a third to optimise the prompt of the generator. This iteration is\nnecessary because the quality of the pseudo-summaries is highly dependent on\nthe generator prompt, and varies widely among videos. We evaluate our summaries\nextensively on multiple datasets; our results show that ViSMaP achieves\nperformance comparable to fully supervised state-of-the-art models while\ngeneralising across domains without sacrificing performance. Code will be\nreleased upon publication.", "published": "2025-04-22 14:06:01", "link": "http://arxiv.org/abs/2504.15921v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "RaSCL: Radar to Satellite Crossview Localization", "abstract": "GNSS is unreliable, inaccurate, and insufficient in many real-time autonomous\nfield applications. In this work, we present a GNSS-free global localization\nsolution that contains a method of registering imaging radar on the ground with\noverhead RGB imagery, with joint optimization of relative poses from odometry\nand global poses from our overhead registration. Previous works have used\nvarious combinations of ground sensors and overhead imagery, and different\nfeature extraction and matching methods. These include various handcrafted and\ndeep-learning-based methods for extracting features from overhead imagery. Our\nwork presents insights on extracting essential features from RGB overhead\nimages for effective global localization against overhead imagery using only\nground radar and a single georeferenced initial guess. We motivate our method\nby evaluating it on datasets in diverse geographic conditions and robotic\nplatforms, including on an Unmanned Surface Vessel (USV) as well as urban and\nsuburban driving datasets.", "published": "2025-04-22 13:41:04", "link": "http://arxiv.org/abs/2504.15899v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "MS-Occ: Multi-Stage LiDAR-Camera Fusion for 3D Semantic Occupancy Prediction", "abstract": "Accurate 3D semantic occupancy perception is essential for autonomous driving\nin complex environments with diverse and irregular objects. While\nvision-centric methods suffer from geometric inaccuracies, LiDAR-based\napproaches often lack rich semantic information. To address these limitations,\nMS-Occ, a novel multi-stage LiDAR-camera fusion framework which includes\nmiddle-stage fusion and late-stage fusion, is proposed, integrating LiDAR's\ngeometric fidelity with camera-based semantic richness via hierarchical\ncross-modal fusion. The framework introduces innovations at two critical\nstages: (1) In the middle-stage feature fusion, the Gaussian-Geo module\nleverages Gaussian kernel rendering on sparse LiDAR depth maps to enhance 2D\nimage features with dense geometric priors, and the Semantic-Aware module\nenriches LiDAR voxels with semantic context via deformable cross-attention; (2)\nIn the late-stage voxel fusion, the Adaptive Fusion (AF) module dynamically\nbalances voxel features across modalities, while the High Classification\nConfidence Voxel Fusion (HCCVF) module resolves semantic inconsistencies using\nself-attention-based refinement. Experiments on the nuScenes-OpenOccupancy\nbenchmark show that MS-Occ achieves an Intersection over Union (IoU) of 32.1%\nand a mean IoU (mIoU) of 25.3%, surpassing the state-of-the-art by +0.7% IoU\nand +2.4% mIoU. Ablation studies further validate the contribution of each\nmodule, with substantial improvements in small-object perception, demonstrating\nthe practical value of MS-Occ for safety-critical autonomous driving scenarios.", "published": "2025-04-22 13:33:26", "link": "http://arxiv.org/abs/2504.15888v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DERD-Net: Learning Depth from Event-based Ray Densities", "abstract": "Event cameras offer a promising avenue for multi-view stereo depth estimation\nand Simultaneous Localization And Mapping (SLAM) due to their ability to detect\nblur-free 3D edges at high-speed and over broad illumination conditions.\nHowever, traditional deep learning frameworks designed for conventional cameras\nstruggle with the asynchronous, stream-like nature of event data, as their\narchitectures are optimized for discrete, image-like inputs. We propose a\nscalable, flexible and adaptable framework for pixel-wise depth estimation with\nevent cameras in both monocular and stereo setups. The 3D scene structure is\nencoded into disparity space images (DSIs), representing spatial densities of\nrays obtained by back-projecting events into space via known camera poses. Our\nneural network processes local subregions of the DSIs combining 3D convolutions\nand a recurrent structure to recognize valuable patterns for depth prediction.\nLocal processing enables fast inference with full parallelization and ensures\nconstant ultra-low model complexity and memory costs, regardless of camera\nresolution. Experiments on standard benchmarks (MVSEC and DSEC datasets)\ndemonstrate unprecedented effectiveness: (i) using purely monocular data, our\nmethod achieves comparable results to existing stereo methods; (ii) when\napplied to stereo data, it strongly outperforms all state-of-the-art (SOTA)\napproaches, reducing the mean absolute error by at least 42%; (iii) our method\nalso allows for increases in depth completeness by more than 3-fold while still\nyielding a reduction in median absolute error of at least 30%. Given its\nremarkable performance and effective processing of event-data, our framework\nholds strong potential to become a standard approach for using deep learning\nfor event-based depth estimation and SLAM. Project page:\nhttps://github.com/tub-rip/DERD-Net", "published": "2025-04-22 12:58:05", "link": "http://arxiv.org/abs/2504.15863v1", "categories": ["cs.CV", "cs.LG", "cs.RO", "eess.SP"], "primary_category": "cs.CV"}
{"title": "Text-based Animatable 3D Avatars with Morphable Model Alignment", "abstract": "The generation of high-quality, animatable 3D head avatars from text has\nenormous potential in content creation applications such as games, movies, and\nembodied virtual assistants. Current text-to-3D generation methods typically\ncombine parametric head models with 2D diffusion models using score\ndistillation sampling to produce 3D-consistent results. However, they struggle\nto synthesize realistic details and suffer from misalignments between the\nappearance and the driving parametric model, resulting in unnatural animation\nresults. We discovered that these limitations stem from ambiguities in the 2D\ndiffusion predictions during 3D avatar distillation, specifically: i) the\navatar's appearance and geometry is underconstrained by the text input, and ii)\nthe semantic alignment between the predictions and the parametric head model is\ninsufficient because the diffusion model alone cannot incorporate information\nfrom the parametric model. In this work, we propose a novel framework,\nAnimPortrait3D, for text-based realistic animatable 3DGS avatar generation with\nmorphable model alignment, and introduce two key strategies to address these\nchallenges. First, we tackle appearance and geometry ambiguities by utilizing\nprior information from a pretrained text-to-3D model to initialize a 3D avatar\nwith robust appearance, geometry, and rigging relationships to the morphable\nmodel. Second, we refine the initial 3D avatar for dynamic expressions using a\nControlNet that is conditioned on semantic and normal maps of the morphable\nmodel to ensure accurate alignment. As a result, our method outperforms\nexisting approaches in terms of synthesis quality, alignment, and animation\nfidelity. Our experiments show that the proposed method advances the state of\nthe art in text-based, animatable 3D head avatar generation.", "published": "2025-04-22 12:29:14", "link": "http://arxiv.org/abs/2504.15835v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Locating and Mitigating Gradient Conflicts in Point Cloud Domain Adaptation via Saliency Map Skewness", "abstract": "Object classification models utilizing point cloud data are fundamental for\n3D media understanding, yet they often struggle with unseen or\nout-of-distribution (OOD) scenarios. Existing point cloud unsupervised domain\nadaptation (UDA) methods typically employ a multi-task learning (MTL) framework\nthat combines primary classification tasks with auxiliary self-supervision\ntasks to bridge the gap between cross-domain feature distributions. However,\nour further experiments demonstrate that not all gradients from\nself-supervision tasks are beneficial and some may negatively impact the\nclassification performance. In this paper, we propose a novel solution, termed\nSaliency Map-based Data Sampling Block (SM-DSB), to mitigate these gradient\nconflicts. Specifically, our method designs a new scoring mechanism based on\nthe skewness of 3D saliency maps to estimate gradient conflicts without\nrequiring target labels. Leveraging this, we develop a sample selection\nstrategy that dynamically filters out samples whose self-supervision gradients\nare not beneficial for the classification. Our approach is scalable,\nintroducing modest computational overhead, and can be integrated into all the\npoint cloud UDA MTL frameworks. Extensive evaluations demonstrate that our\nmethod outperforms state-of-the-art approaches. In addition, we provide a new\nperspective on understanding the UDA problem through back-propagation analysis.", "published": "2025-04-22 11:16:19", "link": "http://arxiv.org/abs/2504.15796v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Development and evaluation of a deep learning algorithm for German word recognition from lip movements", "abstract": "When reading lips, many people benefit from additional visual information\nfrom the lip movements of the speaker, which is, however, very error prone.\nAlgorithms for lip reading with artificial intelligence based on artificial\nneural networks significantly improve word recognition but are not available\nfor the German language. A total of 1806 video clips with only one\nGerman-speaking person each were selected, split into word segments, and\nassigned to word classes using speech-recognition software. In 38,391 video\nsegments with 32 speakers, 18 polysyllabic, visually distinguishable words were\nused to train and validate a neural network. The 3D Convolutional Neural\nNetwork and Gated Recurrent Units models and a combination of both models\n(GRUConv) were compared, as were different image sections and color spaces of\nthe videos. The accuracy was determined in 5000 training epochs. Comparison of\nthe color spaces did not reveal any relevant different correct classification\nrates in the range from 69% to 72%. With a cut to the lips, a significantly\nhigher accuracy of 70% was achieved than when cut to the entire speaker's face\n(34%). With the GRUConv model, the maximum accuracies were 87% with known\nspeakers and 63% in the validation with unknown speakers. The neural network\nfor lip reading, which was first developed for the German language, shows a\nvery high level of accuracy, comparable to English-language algorithms. It\nworks with unknown speakers as well and can be generalized with more word\nclasses.", "published": "2025-04-22 11:12:00", "link": "http://arxiv.org/abs/2504.15792v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Satellite to GroundScape -- Large-scale Consistent Ground View Generation from Satellite Views", "abstract": "Generating consistent ground-view images from satellite imagery is\nchallenging, primarily due to the large discrepancies in viewing angles and\nresolution between satellite and ground-level domains. Previous efforts mainly\nconcentrated on single-view generation, often resulting in inconsistencies\nacross neighboring ground views. In this work, we propose a novel cross-view\nsynthesis approach designed to overcome these challenges by ensuring\nconsistency across ground-view images generated from satellite views. Our\nmethod, based on a fixed latent diffusion model, introduces two conditioning\nmodules: satellite-guided denoising, which extracts high-level scene layout to\nguide the denoising process, and satellite-temporal denoising, which captures\ncamera motion to maintain consistency across multiple generated views. We\nfurther contribute a large-scale satellite-ground dataset containing over\n100,000 perspective pairs to facilitate extensive ground scene or video\ngeneration. Experimental results demonstrate that our approach outperforms\nexisting methods on perceptual and temporal metrics, achieving high\nphotorealism and consistency in multi-view outputs.", "published": "2025-04-22 10:58:42", "link": "http://arxiv.org/abs/2504.15786v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Towards prediction of morphological heart age from computed tomography angiography", "abstract": "Age prediction from medical images or other health-related non-imaging data\nis an important approach to data-driven aging research, providing knowledge of\nhow much information a specific tissue or organ carries about the chronological\nage of the individual. In this work, we studied the prediction of age from\ncomputed tomography angiography (CTA) images, which provide detailed\nrepresentations of the heart morphology, with the goals of (i) studying the\nrelationship between morphology and aging, and (ii) developing a novel\n\\emph{morphological heart age} biomarker. We applied an image\nregistration-based method that standardizes the images from the whole cohort\ninto a single space. We then extracted supervoxels (using unsupervised\nsegmentation), and corresponding robust features of density and local volume,\nwhich provide a detailed representation of the heart morphology while being\nrobust to registration errors. Machine learning models are then trained to fit\nregression models from these features to the chronological age. We applied the\nmethod to a subset of the images from the Swedish CArdioPulomonary bioImage\nStudy (SCAPIS) dataset, consisting of 721 females and 666 males. We observe a\nmean absolute error of $2.74$ years for females and $2.77$ years for males. The\npredictions from different sub-regions of interest were observed to be more\nhighly correlated with the predictions from the whole heart, compared to the\nchronological age, revealing a high consistency in the predictions from\nmorphology. Saliency analysis was also performed on the prediction models to\nstudy what regions are associated positively and negatively with the predicted\nage. This resulted in detailed association maps where the density and volume of\nknown, as well as some novel sub-regions of interest, are determined to be\nimportant. The saliency analysis aids in the interpretability of the models and\ntheir predictions.", "published": "2025-04-22 10:48:27", "link": "http://arxiv.org/abs/2504.15783v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Model-based Metric 3D Shape and Motion Reconstruction of Wild Bottlenose Dolphins in Drone-Shot Videos", "abstract": "We address the problem of estimating the metric 3D shape and motion of wild\ndolphins from monocular video, with the aim of assessing their body condition.\nWhile considerable progress has been made in reconstructing 3D models of\nterrestrial quadrupeds, aquatic animals remain unexplored due to the difficulty\nof observing them in their natural underwater environment. To address this, we\npropose a model-based approach that incorporates a transmission model to\naccount for water-induced occlusion. We apply our method to video captured\nunder different sea conditions. We estimate mass and volume, and compare our\nresults to a manual 2D measurements-based method.", "published": "2025-04-22 10:47:29", "link": "http://arxiv.org/abs/2504.15782v1", "categories": ["cs.CV", "cs.GR", "I.4.8; J.3"], "primary_category": "cs.CV"}
{"title": "Pose Optimization for Autonomous Driving Datasets using Neural Rendering Models", "abstract": "Autonomous driving systems rely on accurate perception and localization of\nthe ego car to ensure safety and reliability in challenging real-world driving\nscenarios. Public datasets play a vital role in benchmarking and guiding\nadvancement in research by providing standardized resources for model\ndevelopment and evaluation. However, potential inaccuracies in sensor\ncalibration and vehicle poses within these datasets can lead to erroneous\nevaluations of downstream tasks, adversely impacting the reliability and\nperformance of the autonomous systems. To address this challenge, we propose a\nrobust optimization method based on Neural Radiance Fields (NeRF) to refine\nsensor poses and calibration parameters, enhancing the integrity of dataset\nbenchmarks. To validate improvement in accuracy of our optimized poses without\nground truth, we present a thorough evaluation process, relying on reprojection\nmetrics, Novel View Synthesis rendering quality, and geometric alignment. We\ndemonstrate that our method achieves significant improvements in sensor pose\naccuracy. By optimizing these critical parameters, our approach not only\nimproves the utility of existing datasets but also paves the way for more\nreliable autonomous driving models. To foster continued progress in this field,\nwe make the optimized sensor poses publicly available, providing a valuable\nresource for the research community.", "published": "2025-04-22 10:33:01", "link": "http://arxiv.org/abs/2504.15776v1", "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Multi-Scale Tensorial Summation and Dimensional Reduction Guided Neural Network for Edge Detection", "abstract": "Edge detection has attracted considerable attention thanks to its exceptional\nability to enhance performance in downstream computer vision tasks. In recent\nyears, various deep learning methods have been explored for edge detection\ntasks resulting in a significant performance improvement compared to\nconventional computer vision algorithms. In neural networks, edge detection\ntasks require considerably large receptive fields to provide satisfactory\nperformance. In a typical convolutional operation, such a large receptive field\ncan be achieved by utilizing a significant number of consecutive layers, which\nyields deep network structures. Recently, a Multi-scale Tensorial Summation\n(MTS) factorization operator was presented, which can achieve very large\nreceptive fields even from the initial layers. In this paper, we propose a\nnovel MTS Dimensional Reduction (MTS-DR) module guided neural network,\nMTS-DR-Net, for the edge detection task. The MTS-DR-Net uses MTS layers, and\ncorresponding MTS-DR blocks as a new backbone to remove redundant information\ninitially. Such a dimensional reduction module enables the neural network to\nfocus specifically on relevant information (i.e., necessary subspaces).\nFinally, a weight U-shaped refinement module follows MTS-DR blocks in the\nMTS-DR-Net. We conducted extensive experiments on two benchmark edge detection\ndatasets: BSDS500 and BIPEDv2 to verify the effectiveness of our model. The\nimplementation of the proposed MTS-DR-Net can be found at\nhttps://github.com/LeiXuAI/MTS-DR-Net.git.", "published": "2025-04-22 10:28:10", "link": "http://arxiv.org/abs/2504.15770v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DSDNet: Raw Domain Demoir\u00e9ing via Dual Color-Space Synergy", "abstract": "With the rapid advancement of mobile imaging, capturing screens using\nsmartphones has become a prevalent practice in distance learning and conference\nrecording. However, moir\\'e artifacts, caused by frequency aliasing between\ndisplay screens and camera sensors, are further amplified by the image signal\nprocessing pipeline, leading to severe visual degradation. Existing sRGB domain\ndemoir\\'eing methods struggle with irreversible information loss, while recent\ntwo-stage raw domain approaches suffer from information bottlenecks and\ninference inefficiency. To address these limitations, we propose a single-stage\nraw domain demoir\\'eing framework, Dual-Stream Demoir\\'eing Network (DSDNet),\nwhich leverages the synergy of raw and YCbCr images to remove moir\\'e while\npreserving luminance and color fidelity. Specifically, to guide luminance\ncorrection and moir\\'e removal, we design a raw-to-YCbCr mapping pipeline and\nintroduce the Synergic Attention with Dynamic Modulation (SADM) module. This\nmodule enriches the raw-to-sRGB conversion with cross-domain contextual\nfeatures. Furthermore, to better guide color fidelity, we develop a\nLuminance-Chrominance Adaptive Transformer (LCAT), which decouples luminance\nand chrominance representations. Extensive experiments demonstrate that DSDNet\noutperforms state-of-the-art methods in both visual quality and quantitative\nevaluation, and achieves an inference speed $\\mathrm{\\textbf{2.4x}}$ faster\nthan the second-best method, highlighting its practical advantages. We provide\nan anonymous online demo at https://xxxxxxxxdsdnet.github.io/DSDNet/.", "published": "2025-04-22 10:09:33", "link": "http://arxiv.org/abs/2504.15756v1", "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV"}
{"title": "GADS: A Super Lightweight Model for Head Pose Estimation", "abstract": "In human-computer interaction, head pose estimation profoundly influences\napplication functionality. Although utilizing facial landmarks is valuable for\nthis purpose, existing landmark-based methods prioritize precision over\nsimplicity and model size, limiting their deployment on edge devices and in\ncompute-poor environments. To bridge this gap, we propose \\textbf{Grouped\nAttention Deep Sets (GADS)}, a novel architecture based on the Deep Set\nframework. By grouping landmarks into regions and employing small Deep Set\nlayers, we reduce computational complexity. Our multihead attention mechanism\nextracts and combines inter-group information, resulting in a model that is\n$7.5\\times$ smaller and executes $25\\times$ faster than the current lightest\nstate-of-the-art model. Notably, our method achieves an impressive reduction,\nbeing $4321\\times$ smaller than the best-performing model. We introduce vanilla\nGADS and Hybrid-GADS (landmarks + RGB) and evaluate our models on three\nbenchmark datasets -- AFLW2000, BIWI, and 300W-LP. We envision our architecture\nas a robust baseline for resource-constrained head pose estimation methods.", "published": "2025-04-22 09:53:25", "link": "http://arxiv.org/abs/2504.15751v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SAGA: Semantic-Aware Gray color Augmentation for Visible-to-Thermal Domain Adaptation across Multi-View Drone and Ground-Based Vision Systems", "abstract": "Domain-adaptive thermal object detection plays a key role in facilitating\nvisible (RGB)-to-thermal (IR) adaptation by reducing the need for co-registered\nimage pairs and minimizing reliance on large annotated IR datasets. However,\ninherent limitations of IR images, such as the lack of color and texture cues,\npose challenges for RGB-trained models, leading to increased false positives\nand poor-quality pseudo-labels. To address this, we propose Semantic-Aware Gray\ncolor Augmentation (SAGA), a novel strategy for mitigating color bias and\nbridging the domain gap by extracting object-level features relevant to IR\nimages. Additionally, to validate the proposed SAGA for drone imagery, we\nintroduce the IndraEye, a multi-sensor (RGB-IR) dataset designed for diverse\napplications. The dataset contains 5,612 images with 145,666 instances,\ncaptured from diverse angles, altitudes, backgrounds, and times of day,\noffering valuable opportunities for multimodal learning, domain adaptation for\nobject detection and segmentation, and exploration of sensor-specific strengths\nand weaknesses. IndraEye aims to enhance the development of more robust and\naccurate aerial perception systems, especially in challenging environments.\nExperimental results show that SAGA significantly improves RGB-to-IR adaptation\nfor autonomous driving and IndraEye dataset, achieving consistent performance\ngains of +0.4% to +7.6% (mAP) when integrated with state-of-the-art domain\nadaptation techniques. The dataset and codes are available at\nhttps://github.com/airliisc/IndraEye.", "published": "2025-04-22 09:22:11", "link": "http://arxiv.org/abs/2504.15728v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Structure-Preserving Zero-Shot Image Editing via Stage-Wise Latent Injection in Diffusion Models", "abstract": "We propose a diffusion-based framework for zero-shot image editing that\nunifies text-guided and reference-guided approaches without requiring\nfine-tuning. Our method leverages diffusion inversion and timestep-specific\nnull-text embeddings to preserve the structural integrity of the source image.\nBy introducing a stage-wise latent injection strategy-shape injection in early\nsteps and attribute injection in later steps-we enable precise, fine-grained\nmodifications while maintaining global consistency. Cross-attention with\nreference latents facilitates semantic alignment between the source and\nreference. Extensive experiments across expression transfer, texture\ntransformation, and style infusion demonstrate state-of-the-art performance,\nconfirming the method's scalability and adaptability to diverse image editing\nscenarios.", "published": "2025-04-22 09:18:16", "link": "http://arxiv.org/abs/2504.15723v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "You Sense Only Once Beneath: Ultra-Light Real-Time Underwater Object Detection", "abstract": "Despite the remarkable achievements in object detection, the model's accuracy\nand efficiency still require further improvement under challenging underwater\nconditions, such as low image quality and limited computational resources. To\naddress this, we propose an Ultra-Light Real-Time Underwater Object Detection\nframework, You Sense Only Once Beneath (YSOOB). Specifically, we utilize a\nMulti-Spectrum Wavelet Encoder (MSWE) to perform frequency-domain encoding on\nthe input image, minimizing the semantic loss caused by underwater optical\ncolor distortion. Furthermore, we revisit the unique characteristics of\neven-sized and transposed convolutions, allowing the model to dynamically\nselect and enhance key information during the resampling process, thereby\nimproving its generalization ability. Finally, we eliminate model redundancy\nthrough a simple yet effective channel compression and reconstructed large\nkernel convolution (RLKC) to achieve model lightweight. As a result, forms a\nhigh-performance underwater object detector YSOOB with only 1.2 million\nparameters. Extensive experimental results demonstrate that, with the fewest\nparameters, YSOOB achieves mAP50 of 83.1% and 82.9% on the URPC2020 and DUO\ndatasets, respectively, comparable to the current SOTA detectors. The inference\nspeed reaches 781.3 FPS and 57.8 FPS on the T4 GPU (TensorRT FP16) and the edge\ncomputing device Jetson Xavier NX (TensorRT FP16), surpassing YOLOv12-N by\n28.1% and 22.5%, respectively.", "published": "2025-04-22 08:26:35", "link": "http://arxiv.org/abs/2504.15694v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Vidi: Large Multimodal Models for Video Understanding and Editing", "abstract": "Humans naturally share information with those they are connected to, and\nvideo has become one of the dominant mediums for communication and expression\non the Internet. To support the creation of high-quality large-scale video\ncontent, a modern pipeline requires a comprehensive understanding of both the\nraw input materials (e.g., the unedited footage captured by cameras) and the\nediting components (e.g., visual effects). In video editing scenarios, models\nmust process multiple modalities (e.g., vision, audio, text) with strong\nbackground knowledge and handle flexible input lengths (e.g., hour-long raw\nvideos), which poses significant challenges for traditional models. In this\nreport, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a\nwide range of video understand editing scenarios. The first release focuses on\ntemporal retrieval, i.e., identifying the time ranges within the input videos\ncorresponding to a given text query, which plays a critical role in intelligent\nediting. The model is capable of processing hour-long videos with strong\ntemporal understanding capability, e.g., retrieve time ranges for certain\nqueries. To support a comprehensive evaluation in real-world scenarios, we also\npresent the VUE-TR benchmark, which introduces five key advancements. 1) Video\nduration: significantly longer than existing temporal retrival datasets, 2)\nAudio support: includes audio-based queries, 3) Query format: diverse query\nlengths/formats, 4) Annotation quality: ground-truth time ranges are manually\nannotated. 5) Evaluation metric: a refined IoU metric to support evaluation\nover multiple time ranges. Remarkably, Vidi significantly outperforms leading\nproprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task,\nindicating its superiority in video editing scenarios.", "published": "2025-04-22 08:04:45", "link": "http://arxiv.org/abs/2504.15681v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DINOv2-powered Few-Shot Semantic Segmentation: A Unified Framework via Cross-Model Distillation and 4D Correlation Mining", "abstract": "Few-shot semantic segmentation has gained increasing interest due to its\ngeneralization capability, i.e., segmenting pixels of novel classes requiring\nonly a few annotated images. Prior work has focused on meta-learning for\nsupport-query matching, with extensive development in both prototype-based and\naggregation-based methods. To address data scarcity, recent approaches have\nturned to foundation models to enhance representation transferability for novel\nclass segmentation. Among them, a hybrid dual-modal framework including both\nDINOv2 and SAM has garnered attention due to their complementary capabilities.\nWe wonder \"can we build a unified model with knowledge from both foundation\nmodels?\" To this end, we propose FS-DINO, with only DINOv2's encoder and a\nlightweight segmenter. The segmenter features a bottleneck adapter, a\nmeta-visual prompt generator based on dense similarities and semantic\nembeddings, and a decoder. Through coarse-to-fine cross-model distillation, we\neffectively integrate SAM's knowledge into our lightweight segmenter, which can\nbe further enhanced by 4D correlation mining on support-query pairs. Extensive\nexperiments on COCO-20i, PASCAL-5i, and FSS-1000 demonstrate the effectiveness\nand superiority of our method.", "published": "2025-04-22 07:47:06", "link": "http://arxiv.org/abs/2504.15669v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Performance Estimation for Supervised Medical Image Segmentation Models on Unlabeled Data Using UniverSeg", "abstract": "The performance of medical image segmentation models is usually evaluated\nusing metrics like the Dice score and Hausdorff distance, which compare\npredicted masks to ground truth annotations. However, when applying the model\nto unseen data, such as in clinical settings, it is often impractical to\nannotate all the data, making the model's performance uncertain. To address\nthis challenge, we propose the Segmentation Performance Evaluator (SPE), a\nframework for estimating segmentation models' performance on unlabeled data.\nThis framework is adaptable to various evaluation metrics and model\narchitectures. Experiments on six publicly available datasets across six\nevaluation metrics including pixel-based metrics such as Dice score and\ndistance-based metrics like HD95, demonstrated the versatility and\neffectiveness of our approach, achieving a high correlation (0.956$\\pm$0.046)\nand low MAE (0.025$\\pm$0.019) compare with real Dice score on the independent\ntest set. These results highlight its ability to reliably estimate model\nperformance without requiring annotations. The SPE framework integrates\nseamlessly into any model training process without adding training overhead,\nenabling performance estimation and facilitating the real-world application of\nmedical image segmentation algorithms. The source code is publicly available", "published": "2025-04-22 07:42:48", "link": "http://arxiv.org/abs/2504.15667v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Motion-Enhanced Nonlocal Similarity Implicit Neural Representation for Infrared Dim and Small Target Detection", "abstract": "Infrared dim and small target detection presents a significant challenge due\nto dynamic multi-frame scenarios and weak target signatures in the infrared\nmodality. Traditional low-rank plus sparse models often fail to capture dynamic\nbackgrounds and global spatial-temporal correlations, which results in\nbackground leakage or target loss. In this paper, we propose a novel\nmotion-enhanced nonlocal similarity implicit neural representation (INR)\nframework to address these challenges. We first integrate motion estimation via\noptical flow to capture subtle target movements, and propose multi-frame fusion\nto enhance motion saliency. Second, we leverage nonlocal similarity to\nconstruct patch tensors with strong low-rank properties, and propose an\ninnovative tensor decomposition-based INR model to represent the nonlocal patch\ntensor, effectively encoding both the nonlocal low-rankness and\nspatial-temporal correlations of background through continuous neural\nrepresentations. An alternating direction method of multipliers is developed\nfor the nonlocal INR model, which enjoys theoretical fixed-point convergence.\nExperimental results show that our approach robustly separates dim targets from\ncomplex infrared backgrounds, outperforming state-of-the-art methods in\ndetection accuracy and robustness.", "published": "2025-04-22 07:42:00", "link": "http://arxiv.org/abs/2504.15665v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "An XAI-based Analysis of Shortcut Learning in Neural Networks", "abstract": "Machine learning models tend to learn spurious features - features that\nstrongly correlate with target labels but are not causal. Existing approaches\nto mitigate models' dependence on spurious features work in some cases, but\nfail in others. In this paper, we systematically analyze how and where neural\nnetworks encode spurious correlations. We introduce the neuron spurious score,\nan XAI-based diagnostic measure to quantify a neuron's dependence on spurious\nfeatures. We analyze both convolutional neural networks (CNNs) and vision\ntransformers (ViTs) using architecture-specific methods. Our results show that\nspurious features are partially disentangled, but the degree of disentanglement\nvaries across model architectures. Furthermore, we find that the assumptions\nbehind existing mitigation methods are incomplete. Our results lay the\ngroundwork for the development of novel methods to mitigate spurious\ncorrelations and make AI models safer to use in practice.", "published": "2025-04-22 07:40:45", "link": "http://arxiv.org/abs/2504.15664v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "DiTPainter: Efficient Video Inpainting with Diffusion Transformers", "abstract": "Many existing video inpainting algorithms utilize optical flows to construct\nthe corresponding maps and then propagate pixels from adjacent frames to\nmissing areas by mapping. Despite the effectiveness of the propagation\nmechanism, they might encounter blurry and inconsistencies when dealing with\ninaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT)\nhas emerged as a revolutionary technique for video generation tasks. However,\npretrained DiT models for video generation all contain a large amount of\nparameters, which makes it very time consuming to apply to video inpainting\ntasks. In this paper, we present DiTPainter, an end-to-end video inpainting\nmodel based on Diffusion Transformer (DiT). DiTPainter uses an efficient\ntransformer network designed for video inpainting, which is trained from\nscratch instead of initializing from any large pretrained models. DiTPainter\ncan address videos with arbitrary lengths and can be applied to video\ndecaptioning and video completion tasks with an acceptable time cost.\nExperiments show that DiTPainter outperforms existing video inpainting\nalgorithms with higher quality and better spatial-temporal consistency.", "published": "2025-04-22 07:36:45", "link": "http://arxiv.org/abs/2504.15661v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "AffordanceSAM: Segment Anything Once More in Affordance Grounding", "abstract": "Improving the generalization ability of an affordance grounding model to\nrecognize regions for unseen objects and affordance functions is crucial for\nreal-world application. However, current models are still far away from such\nstandards. To address this problem, we introduce AffordanceSAM, an effective\napproach that extends SAM's generalization capacity to the domain of affordance\ngrounding. For the purpose of thoroughly transferring SAM's robust performance\nin segmentation to affordance, we initially propose an affordance-adaption\nmodule in order to help modify SAM's segmentation output to be adapted to the\nspecific functional regions required for affordance grounding. We concurrently\nmake a coarse-to-fine training recipe to make SAM first be aware of affordance\nobjects and actions coarsely, and then be able to generate affordance heatmaps\nfinely. Both quantitative and qualitative experiments show the strong\ngeneralization capacity of our AffordanceSAM, which not only surpasses previous\nmethods under AGD20K benchmark but also shows evidence to handle the task with\nnovel objects and affordance functions.", "published": "2025-04-22 07:16:56", "link": "http://arxiv.org/abs/2504.15650v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "RepNet-VSR: Reparameterizable Architecture for High-Fidelity Video Super-Resolution", "abstract": "As a fundamental challenge in visual computing, video super-resolution (VSR)\nfocuses on reconstructing highdefinition video sequences from their degraded\nlowresolution counterparts. While deep convolutional neural networks have\ndemonstrated state-of-the-art performance in spatial-temporal super-resolution\ntasks, their computationally intensive nature poses significant deployment\nchallenges for resource-constrained edge devices, particularly in real-time\nmobile video processing scenarios where power efficiency and latency\nconstraints coexist. In this work, we propose a Reparameterizable Architecture\nfor High Fidelity Video Super Resolution method, named RepNet-VSR, for\nreal-time 4x video super-resolution. On the REDS validation set, the proposed\nmodel achieves 27.79 dB PSNR when processing 180p to 720p frames in 103 ms per\n10 frames on a MediaTek Dimensity NPU. The competition results demonstrate an\nexcellent balance between restoration quality and deployment efficiency. The\nproposed method scores higher than the previous champion algorithm of MAI video\nsuper-resolution challenge.", "published": "2025-04-22 07:15:07", "link": "http://arxiv.org/abs/2504.15649v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "ZeroSlide: Is Zero-Shot Classification Adequate for Lifelong Learning in Whole-Slide Image Analysis in the Era of Pathology Vision-Language Foundation Models?", "abstract": "Lifelong learning for whole slide images (WSIs) poses the challenge of\ntraining a unified model to perform multiple WSI-related tasks, such as cancer\nsubtyping and tumor classification, in a distributed, continual fashion. This\nis a practical and applicable problem in clinics and hospitals, as WSIs are\nlarge, require storage, processing, and transfer time. Training new models\nwhenever new tasks are defined is time-consuming. Recent work has applied\nregularization- and rehearsal-based methods to this setting. However, the rise\nof vision-language foundation models that align diagnostic text with pathology\nimages raises the question: are these models alone sufficient for lifelong WSI\nlearning using zero-shot classification, or is further investigation into\ncontinual learning strategies needed to improve performance? To our knowledge,\nthis is the first study to compare conventional continual-learning approaches\nwith vision-language zero-shot classification for WSIs. Our source code and\nexperimental results will be available soon.", "published": "2025-04-22 06:38:37", "link": "http://arxiv.org/abs/2504.15627v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "FaceInsight: A Multimodal Large Language Model for Face Perception", "abstract": "Recent advances in multimodal large language models (MLLMs) have demonstrated\nstrong capabilities in understanding general visual content. However, these\ngeneral-domain MLLMs perform poorly in face perception tasks, often producing\ninaccurate or misleading responses to face-specific queries. To address this\ngap, we propose FaceInsight, the versatile face perception MLLM that provides\nfine-grained facial information. Our approach introduces visual-textual\nalignment of facial knowledge to model both uncertain dependencies and\ndeterministic relationships among facial information, mitigating the\nlimitations of language-driven reasoning. Additionally, we incorporate face\nsegmentation maps as an auxiliary perceptual modality, enriching the visual\ninput with localized structural cues to enhance semantic understanding.\nComprehensive experiments and analyses across three face perception tasks\ndemonstrate that FaceInsight consistently outperforms nine compared MLLMs under\nboth training-free and fine-tuned settings.", "published": "2025-04-22 06:31:57", "link": "http://arxiv.org/abs/2504.15624v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "AdaViP: Aligning Multi-modal LLMs via Adaptive Vision-enhanced Preference Optimization", "abstract": "Preference alignment through Direct Preference Optimization (DPO) has\ndemonstrated significant effectiveness in aligning multimodal large language\nmodels (MLLMs) with human preferences. However, existing methods focus\nprimarily on language preferences while neglecting the critical visual context.\nIn this paper, we propose an Adaptive Vision-enhanced Preference optimization\n(AdaViP) that addresses these limitations through two key innovations: (1)\nvision-based preference pair construction, which integrates multiple visual\nfoundation models to strategically remove key visual elements from the image,\nenhancing MLLMs' sensitivity to visual details; and (2) adaptive preference\noptimization that dynamically balances vision- and language-based preferences\nfor more accurate alignment. Extensive evaluations across different benchmarks\ndemonstrate our effectiveness. Notably, our AdaViP-7B achieves 93.7% and 96.4%\nreductions in response-level and mentioned-level hallucination respectively on\nthe Object HalBench, significantly outperforming current state-of-the-art\nmethods.", "published": "2025-04-22 06:19:38", "link": "http://arxiv.org/abs/2504.15619v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SocialMOIF: Multi-Order Intention Fusion for Pedestrian Trajectory Prediction", "abstract": "The analysis and prediction of agent trajectories are crucial for\ndecision-making processes in intelligent systems, with precise short-term\ntrajectory forecasting being highly significant across a range of applications.\nAgents and their social interactions have been quantified and modeled by\nresearchers from various perspectives; however, substantial limitations exist\nin the current work due to the inherent high uncertainty of agent intentions\nand the complex higher-order influences among neighboring groups. SocialMOIF is\nproposed to tackle these challenges, concentrating on the higher-order\nintention interactions among neighboring groups while reinforcing the primary\nrole of first-order intention interactions between neighbors and the target\nagent. This method develops a multi-order intention fusion model to achieve a\nmore comprehensive understanding of both direct and indirect intention\ninformation. Within SocialMOIF, a trajectory distribution approximator is\ndesigned to guide the trajectories toward values that align more closely with\nthe actual data, thereby enhancing model interpretability. Furthermore, a\nglobal trajectory optimizer is introduced to enable more accurate and efficient\nparallel predictions. By incorporating a novel loss function that accounts for\ndistance and direction during training, experimental results demonstrate that\nthe model outperforms previous state-of-the-art baselines across multiple\nmetrics in both dynamic and static datasets.", "published": "2025-04-22 06:14:49", "link": "http://arxiv.org/abs/2504.15616v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "HS-Mamba: Full-Field Interaction Multi-Groups Mamba for Hyperspectral Image Classification", "abstract": "Hyperspectral image (HSI) classification has been one of the hot topics in\nremote sensing fields. Recently, the Mamba architecture based on selective\nstate-space models (S6) has demonstrated great advantages in long sequence\nmodeling. However, the unique properties of hyperspectral data, such as high\ndimensionality and feature inlining, pose challenges to the application of\nMamba to HSI classification. To compensate for these shortcomings, we propose\nan full-field interaction multi-groups Mamba framework (HS-Mamba), which adopts\na strategy different from pixel-patch based or whole-image based, but combines\nthe advantages of both. The patches cut from the whole image are sent to\nmulti-groups Mamba, combined with positional information to perceive local\ninline features in the spatial and spectral domains, and the whole image is\nsent to a lightweight attention module to enhance the global feature\nrepresentation ability. Specifically, HS-Mamba consists of a dual-channel\nspatial-spectral encoder (DCSS-encoder) module and a lightweight global inline\nattention (LGI-Att) branch. The DCSS-encoder module uses multiple groups of\nMamba to decouple and model the local features of dual-channel sequences with\nnon-overlapping patches. The LGI-Att branch uses a lightweight compressed and\nextended attention module to perceive the global features of the spatial and\nspectral domains of the unsegmented whole image. By fusing local and global\nfeatures, high-precision classification of hyperspectral images is achieved.\nExtensive experiments demonstrate the superiority of the proposed HS-Mamba,\noutperforming state-of-the-art methods on four benchmark HSI datasets.", "published": "2025-04-22 06:13:02", "link": "http://arxiv.org/abs/2504.15612v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SonarT165: A Large-scale Benchmark and STFTrack Framework for Acoustic Object Tracking", "abstract": "Underwater observation systems typically integrate optical cameras and\nimaging sonar systems. When underwater visibility is insufficient, only sonar\nsystems can provide stable data, which necessitates exploration of the\nunderwater acoustic object tracking (UAOT) task. Previous studies have explored\ntraditional methods and Siamese networks for UAOT. However, the absence of a\nunified evaluation benchmark has significantly constrained the value of these\nmethods. To alleviate this limitation, we propose the first large-scale UAOT\nbenchmark, SonarT165, comprising 165 square sequences, 165 fan sequences, and\n205K high-quality annotations. Experimental results demonstrate that SonarT165\nreveals limitations in current state-of-the-art SOT trackers. To address these\nlimitations, we propose STFTrack, an efficient framework for acoustic object\ntracking. It includes two novel modules, a multi-view template fusion module\n(MTFM) and an optimal trajectory correction module (OTCM). The MTFM module\nintegrates multi-view feature of both the original image and the binary image\nof the dynamic template, and introduces a cross-attention-like layer to fuse\nthe spatio-temporal target representations. The OTCM module introduces the\nacoustic-response-equivalent pixel property and proposes normalized pixel\nbrightness response scores, thereby suppressing suboptimal matches caused by\ninaccurate Kalman filter prediction boxes. To further improve the model\nfeature, STFTrack introduces a acoustic image enhancement method and a\nFrequency Enhancement Module (FEM) into its tracking pipeline. Comprehensive\nexperiments show the proposed STFTrack achieves state-of-the-art performance on\nthe proposed benchmark. The code is available at\nhttps://github.com/LiYunfengLYF/SonarT165.", "published": "2025-04-22 06:02:32", "link": "http://arxiv.org/abs/2504.15609v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Multi-Modal Fusion of In-Situ Video Data and Process Parameters for Online Forecasting of Cookie Drying Readiness", "abstract": "Food drying is essential for food production, extending shelf life, and\nreducing transportation costs. Accurate real-time forecasting of drying\nreadiness is crucial for minimizing energy consumption, improving productivity,\nand ensuring product quality. However, this remains challenging due to the\ndynamic nature of drying, limited data availability, and the lack of effective\npredictive analytical methods. To address this gap, we propose an end-to-end\nmulti-modal data fusion framework that integrates in-situ video data with\nprocess parameters for real-time food drying readiness forecasting. Our\napproach leverages a new encoder-decoder architecture with modality-specific\nencoders and a transformer-based decoder to effectively extract features while\npreserving the unique structure of each modality. We apply our approach to\nsugar cookie drying, where time-to-ready is predicted at each timestamp.\nExperimental results demonstrate that our model achieves an average prediction\nerror of only 15 seconds, outperforming state-of-the-art data fusion methods by\n65.69% and a video-only model by 11.30%. Additionally, our model balances\nprediction accuracy, model size, and computational efficiency, making it\nwell-suited for heterogenous industrial datasets. The proposed model is\nextensible to various other industrial modality fusion tasks for online\ndecision-making.", "published": "2025-04-22 05:37:55", "link": "http://arxiv.org/abs/2504.15599v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Analytical Softmax Temperature Setting from Feature Dimensions for Model- and Domain-Robust Classification", "abstract": "In deep learning-based classification tasks, the softmax function's\ntemperature parameter $T$ critically influences the output distribution and\noverall performance. This study presents a novel theoretical insight that the\noptimal temperature $T^*$ is uniquely determined by the dimensionality of the\nfeature representations, thereby enabling training-free determination of $T^*$.\nDespite this theoretical grounding, empirical evidence reveals that $T^*$\nfluctuates under practical conditions owing to variations in models, datasets,\nand other confounding factors. To address these influences, we propose and\noptimize a set of temperature determination coefficients that specify how $T^*$\nshould be adjusted based on the theoretical relationship to feature\ndimensionality. Additionally, we insert a batch normalization layer immediately\nbefore the output layer, effectively stabilizing the feature space. Building on\nthese coefficients and a suite of large-scale experiments, we develop an\nempirical formula to estimate $T^*$ without additional training while also\nintroducing a corrective scheme to refine $T^*$ based on the number of classes\nand task complexity. Our findings confirm that the derived temperature not only\naligns with the proposed theoretical perspective but also generalizes\neffectively across diverse tasks, consistently enhancing classification\nperformance and offering a practical, training-free solution for determining\n$T^*$.", "published": "2025-04-22 05:14:38", "link": "http://arxiv.org/abs/2504.15594v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Bayesian Autoencoder for Medical Anomaly Detection: Uncertainty-Aware Approach for Brain 2 MRI Analysis", "abstract": "In medical imaging, anomaly detection is a vital element of healthcare\ndiagnostics, especially for neurological conditions which can be\nlife-threatening. Conventional deterministic methods often fall short when it\ncomes to capturing the inherent uncertainty of anomaly detection tasks. This\npaper introduces a Bayesian Variational Autoencoder (VAE) equipped with\nmulti-head attention mechanisms for detecting anomalies in brain magnetic\nresonance imaging (MRI). For the purpose of improving anomaly detection\nperformance, we incorporate both epistemic and aleatoric uncertainty estimation\nthrough Bayesian inference. The model was tested on the BraTS2020 dataset, and\nthe findings were a 0.83 ROC AUC and a 0.83 PR AUC. The data in our paper\nsuggests that modeling uncertainty is an essential component of anomaly\ndetection, enhancing both performance and interpretability and providing\nconfidence estimates, as well as anomaly predictions, for clinicians to\nleverage in making medical decisions.", "published": "2025-04-22 03:30:42", "link": "http://arxiv.org/abs/2504.15562v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "VLM-based Prompts as the Optimal Assistant for Unpaired Histopathology Virtual Staining", "abstract": "In histopathology, tissue sections are typically stained using common H&E\nstaining or special stains (MAS, PAS, PASM, etc.) to clearly visualize specific\ntissue structures. The rapid advancement of deep learning offers an effective\nsolution for generating virtually stained images, significantly reducing the\ntime and labor costs associated with traditional histochemical staining.\nHowever, a new challenge arises in separating the fundamental visual\ncharacteristics of tissue sections from the visual differences induced by\nstaining agents. Additionally, virtual staining often overlooks essential\npathological knowledge and the physical properties of staining, resulting in\nonly style-level transfer. To address these issues, we introduce, for the first\ntime in virtual staining tasks, a pathological vision-language large model\n(VLM) as an auxiliary tool. We integrate contrastive learnable prompts,\nfoundational concept anchors for tissue sections, and staining-specific concept\nanchors to leverage the extensive knowledge of the pathological VLM. This\napproach is designed to describe, frame, and enhance the direction of virtual\nstaining. Furthermore, we have developed a data augmentation method based on\nthe constraints of the VLM. This method utilizes the VLM's powerful image\ninterpretation capabilities to further integrate image style and structural\ninformation, proving beneficial in high-precision pathological diagnostics.\nExtensive evaluations on publicly available multi-domain unpaired staining\ndatasets demonstrate that our method can generate highly realistic images and\nenhance the accuracy of downstream tasks, such as glomerular detection and\nsegmentation. Our code is available at:\nhttps://github.com/CZZZZZZZZZZZZZZZZZ/VPGAN-HARBOR", "published": "2025-04-22 02:46:13", "link": "http://arxiv.org/abs/2504.15545v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "InstaRevive: One-Step Image Enhancement via Dynamic Score Matching", "abstract": "Image enhancement finds wide-ranging applications in real-world scenarios due\nto complex environments and the inherent limitations of imaging devices. Recent\ndiffusion-based methods yield promising outcomes but necessitate prolonged and\ncomputationally intensive iterative sampling. In response, we propose\nInstaRevive, a straightforward yet powerful image enhancement framework that\nemploys score-based diffusion distillation to harness potent generative\ncapability and minimize the sampling steps. To fully exploit the potential of\nthe pre-trained diffusion model, we devise a practical and effective diffusion\ndistillation pipeline using dynamic control to address inaccuracies in updating\ndirection during score matching. Our control strategy enables a dynamic\ndiffusing scope, facilitating precise learning of denoising trajectories within\nthe diffusion model and ensuring accurate distribution matching gradients\nduring training. Additionally, to enrich guidance for the generative power, we\nincorporate textual prompts via image captioning as auxiliary conditions,\nfostering further exploration of the diffusion model. Extensive experiments\nsubstantiate the efficacy of our framework across a diverse array of\nchallenging tasks and datasets, unveiling the compelling efficacy and\nefficiency of InstaRevive in delivering high-quality and visually appealing\nresults. Code is available at https://github.com/EternalEvan/InstaRevive.", "published": "2025-04-22 01:19:53", "link": "http://arxiv.org/abs/2504.15513v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Fluorescence Reference Target Quantitative Analysis Library", "abstract": "Standardized performance evaluation of fluorescence imaging systems remains a\ncritical unmet need in the field of fluorescence-guided surgery (FGS). While\nthe American Association of Physicists in Medicine (AAPM) TG311 report and\nrecent FDA draft guidance provide recommended metrics for system\ncharacterization, practical tools for extracting these metrics remain limited,\ninconsistent, and often inaccessible. We present QUEL-QAL, an open-source\nPython library designed to streamline and standardize the quantitative analysis\nof fluorescence images using solid reference targets. The library provides a\nmodular, reproducible workflow that includes region of interest (ROI)\ndetection, statistical analysis, and visualization capabilities. QUEL-QAL\nsupports key metrics such as response linearity, limit of detection, depth\nsensitivity, and spatial resolution, in alignment with regulatory and academic\nguidance. Built on widely adopted Python packages, the library is designed to\nbe extensible, enabling users to adapt it to novel target designs and analysis\nprotocols. By promoting transparency, reproducibility, and regulatory\nalignment, QUEL-QAL offers a foundational tool to support standardized\nbenchmarking and accelerate the development and evaluation of fluorescence\nimaging systems.", "published": "2025-04-22 00:03:55", "link": "http://arxiv.org/abs/2504.15496v1", "categories": ["physics.med-ph", "cs.CV", "eess.IV", "q-bio.QM"], "primary_category": "physics.med-ph"}
{"title": "Structural Properties of Non-Linear Cellular Automata: Permutivity, Surjectivity and Reversibility", "abstract": "This paper explores the algebraic conditions under which a cellular automaton\nwith a non-linear local rule exhibits surjectivity and reversibility. We also\nanalyze the role of permutivity as a key factor influencing these properties\nand provide conditions that determine whether a non-linear CA is (bi)permutive.\nThrough theoretical results and illustrative examples, we characterize the\nrelationships between these fundamental properties, offering new insights into\nthe dynamical behavior of non-linear CA.", "published": "2025-04-22 14:47:16", "link": "http://arxiv.org/abs/2504.15949v1", "categories": ["cs.DM", "cs.CR", "math.DS"], "primary_category": "cs.DM"}
{"title": "Circularity and repetitiveness in non-injective DF0L systems", "abstract": "We study circularity in DF0L systems, a generalization of D0L systems. We\nfocus on two different types of circularity, called weak and strong\ncircularity. When the morphism is injective on the language of the system, the\ntwo notions are equivalent, but they may differ otherwise. Our main result\nshows that failure of weak circularity implies unbounded repetitiveness, and\nthat unbounded repetitiveness implies failure of strong circularity. This\nextends previous work by the second and third authors for injective systems. To\nhelp motivate this work, we also give examples of non-injective but strongly\ncircular systems.", "published": "2025-04-22 12:20:34", "link": "http://arxiv.org/abs/2504.15828v1", "categories": ["cs.DM", "68R15, 68Q42"], "primary_category": "cs.DM"}
{"title": "Intent-aware Diffusion with Contrastive Learning for Sequential Recommendation", "abstract": "Contrastive learning has proven effective in training sequential\nrecommendation models by incorporating self-supervised signals from augmented\nviews. Most existing methods generate multiple views from the same interaction\nsequence through stochastic data augmentation, aiming to align their\nrepresentations in the embedding space. However, users typically have specific\nintents when purchasing items (e.g., buying clothes as gifts or cosmetics for\nbeauty). Random data augmentation used in existing methods may introduce noise,\ndisrupting the latent intent information implicit in the original interaction\nsequence. Moreover, using noisy augmented sequences in contrastive learning may\nmislead the model to focus on irrelevant features, distorting the embedding\nspace and failing to capture users' true behavior patterns and intents. To\naddress these issues, we propose Intent-aware Diffusion with contrastive\nlearning for sequential Recommendation (InDiRec). The core idea is to generate\nitem sequences aligned with users' purchasing intents, thus providing more\nreliable augmented views for contrastive learning. Specifically, InDiRec first\nperforms intent clustering on sequence representations using K-means to build\nintent-guided signals. Next, it retrieves the intent representation of the\ntarget interaction sequence to guide a conditional diffusion model, generating\npositive views that share the same underlying intent. Finally, contrastive\nlearning is applied to maximize representation consistency between these\nintent-aligned views and the original sequence. Extensive experiments on five\npublic datasets demonstrate that InDiRec achieves superior performance compared\nto existing baselines, learning more robust representations even under noisy\nand sparse data conditions.", "published": "2025-04-22 17:55:56", "link": "http://arxiv.org/abs/2504.16077v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs", "abstract": "Memory is the process of encoding, storing, and retrieving information,\nallowing humans to retain experiences, knowledge, skills, and facts over time,\nand serving as the foundation for growth and effective interaction with the\nworld. It plays a crucial role in shaping our identity, making decisions,\nlearning from past experiences, building relationships, and adapting to\nchanges. In the era of large language models (LLMs), memory refers to the\nability of an AI system to retain, recall, and use information from past\ninteractions to improve future responses and interactions. Although previous\nresearch and reviews have provided detailed descriptions of memory mechanisms,\nthere is still a lack of a systematic review that summarizes and analyzes the\nrelationship between the memory of LLM-driven AI systems and human memory, as\nwell as how we can be inspired by human memory to construct more powerful\nmemory systems. To achieve this, in this paper, we propose a comprehensive\nsurvey on the memory of LLM-driven AI systems. In particular, we first conduct\na detailed analysis of the categories of human memory and relate them to the\nmemory of AI systems. Second, we systematically organize existing\nmemory-related work and propose a categorization method based on three\ndimensions (object, form, and time) and eight quadrants. Finally, we illustrate\nsome open problems regarding the memory of current AI systems and outline\npossible future directions for memory in the era of large language models.", "published": "2025-04-22 15:05:04", "link": "http://arxiv.org/abs/2504.15965v1", "categories": ["cs.IR", "H.0"], "primary_category": "cs.IR"}
{"title": "Synergizing RAG and Reasoning: A Systematic Review", "abstract": "Recent breakthroughs in large language models (LLMs), particularly in\nreasoning capabilities, have propelled Retrieval-Augmented Generation (RAG) to\nunprecedented levels. By synergizing retrieval mechanisms with advanced\nreasoning, LLMs can now tackle increasingly complex problems. This paper\npresents a systematic review of the collaborative interplay between RAG and\nreasoning, clearly defining \"reasoning\" within the RAG context. It construct a\ncomprehensive taxonomy encompassing multi-dimensional collaborative objectives,\nrepresentative paradigms, and technical implementations, and analyze the\nbidirectional synergy methods. Additionally, we critically evaluate current\nlimitations in RAG assessment, including the absence of intermediate\nsupervision for multi-step reasoning and practical challenges related to\ncost-risk trade-offs. To bridge theory and practice, we provide practical\nguidelines tailored to diverse real-world applications. Finally, we identify\npromising research directions, such as graph-based knowledge integration,\nhybrid model collaboration, and RL-driven optimization. Overall, this work\npresents a theoretical framework and practical foundation to advance RAG\nsystems in academia and industry, fostering the next generation of RAG\nsolutions.", "published": "2025-04-22 13:55:13", "link": "http://arxiv.org/abs/2504.15909v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "NLCTables: A Dataset for Marrying Natural Language Conditions with Table Discovery", "abstract": "With the growing abundance of repositories containing tabular data,\ndiscovering relevant tables for in-depth analysis remains a challenging task.\nExisting table discovery methods primarily retrieve desired tables based on a\nquery table or several vague keywords, leaving users to manually filter large\nresult sets. To address this limitation, we propose a new task: NL-conditional\ntable discovery (nlcTD), where users combine a query table with natural\nlanguage (NL) requirements to refine search results. To advance research in\nthis area, we present nlcTables, a comprehensive benchmark dataset comprising\n627 diverse queries spanning NL-only, union, join, and fuzzy conditions, 22,080\ncandidate tables, and 21,200 relevance annotations. Our evaluation of six\nstate-of-the-art table discovery methods on nlcTables reveals substantial\nperformance gaps, highlighting the need for advanced techniques to tackle this\nchallenging nlcTD scenario. The dataset, construction framework, and baseline\nimplementations are publicly available at\nhttps://github.com/SuDIS-ZJU/nlcTables to foster future research.", "published": "2025-04-22 12:44:59", "link": "http://arxiv.org/abs/2504.15849v1", "categories": ["cs.IR", "68P20"], "primary_category": "cs.IR"}
{"title": "FinDER: Financial Dataset for Question Answering and Evaluating Retrieval-Augmented Generation", "abstract": "In the fast-paced financial domain, accurate and up-to-date information is\ncritical to addressing ever-evolving market conditions. Retrieving this\ninformation correctly is essential in financial Question-Answering (QA), since\nmany language models struggle with factual accuracy in this domain. We present\nFinDER, an expert-generated dataset tailored for Retrieval-Augmented Generation\n(RAG) in finance. Unlike existing QA datasets that provide predefined contexts\nand rely on relatively clear and straightforward queries, FinDER focuses on\nannotating search-relevant evidence by domain experts, offering 5,703\nquery-evidence-answer triplets derived from real-world financial inquiries.\nThese queries frequently include abbreviations, acronyms, and concise\nexpressions, capturing the brevity and ambiguity common in the realistic search\nbehavior of professionals. By challenging models to retrieve relevant\ninformation from large corpora rather than relying on readily determined\ncontexts, FinDER offers a more realistic benchmark for evaluating RAG systems.\nWe further present a comprehensive evaluation of multiple state-of-the-art\nretrieval models and Large Language Models, showcasing challenges derived from\na realistic benchmark to drive future research on truthful and precise RAG in\nthe financial domain.", "published": "2025-04-22 11:30:13", "link": "http://arxiv.org/abs/2504.15800v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Assessing FAIRness of the Digital Shadow Reference Model", "abstract": "Models play a critical role in managing the vast amounts of data and\nincreasing complexity found in the IoT, IIoT, and IoP domains. The Digital\nShadow Reference Model, which serves as a foundational metadata schema for\nlinking data and metadata in these environments, is an example of such a model.\nEnsuring FAIRness (adherence to the FAIR Principles) is critical because it\nimproves data findability, accessibility, interoperability, and reusability,\nfacilitating efficient data management and integration across systems.\n  This paper presents an evaluation of the FAIRness of the Digital Shadow\nReference Model using a structured evaluation framework based on the FAIR Data\nPrinciples. Using the concept of FAIR Implementation Profiles (FIPs),\nsupplemented by a mini-questionnaire, we systematically evaluate the model's\nadherence to these principles. Our analysis identifies key strengths, including\nthe model's metadata schema that supports rich descriptions and authentication\ntechniques, and highlights areas for improvement, such as the need for globally\nunique identifiers and consequent support for different Web standards. The\nresults provide actionable insights for improving the FAIRness of the model and\npromoting better data management and reuse. This research contributes to the\nfield by providing a detailed assessment of the Digital Shadow Reference Model\nand recommending next steps to improve its FAIRness and usability.", "published": "2025-04-22 08:58:48", "link": "http://arxiv.org/abs/2504.15715v1", "categories": ["cs.DB", "cs.CY", "cs.IR", "H.1; H.3; H.4; H.m; I.6"], "primary_category": "cs.DB"}
{"title": "The Viability of Crowdsourcing for RAG Evaluation", "abstract": "How good are humans at writing and judging responses in retrieval-augmented\ngeneration (RAG) scenarios? To answer this question, we investigate the\nefficacy of crowdsourcing for RAG through two complementary studies: response\nwriting and response utility judgment. We present the Crowd RAG Corpus 2025\n(CrowdRAG-25), which consists of 903 human-written and 903 LLM-generated\nresponses for the 301 topics of the TREC RAG'24 track, across the three\ndiscourse styles 'bulleted list', 'essay', and 'news'. For a selection of 65\ntopics, the corpus further contains 47,320 pairwise human judgments and 10,556\npairwise LLM judgments across seven utility dimensions (e.g., coverage and\ncoherence). Our analyses give insights into human writing behavior for RAG and\nthe viability of crowdsourcing for RAG evaluation. Human pairwise judgments\nprovide reliable and cost-effective results compared to LLM-based pairwise or\nhuman/LLM-based pointwise judgments, as well as automated comparisons with\nhuman-written reference responses. All our data and tools are freely available.", "published": "2025-04-22 08:13:34", "link": "http://arxiv.org/abs/2504.15689v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Comprehensive List Generation for Multi-Generator Reranking", "abstract": "Reranking models solve the final recommendation lists that best fulfill\nusers' demands. While existing solutions focus on finding parametric models\nthat approximate optimal policies, recent approaches find that it is better to\ngenerate multiple lists to compete for a ``pass'' ticket from an evaluator,\nwhere the evaluator serves as the supervisor who accurately estimates the\nperformance of the candidate lists. In this work, we show that we can achieve a\nmore efficient and effective list proposal with a multi-generator framework and\nprovide empirical evidence on two public datasets and online A/B tests. More\nimportantly, we verify that the effectiveness of a generator is closely related\nto how much it complements the views of other generators with sufficiently\ndifferent rerankings, which derives the metric of list comprehensiveness. With\nthis intuition, we design an automatic complementary generator-finding\nframework that learns a policy that simultaneously aligns the users'\npreferences and maximizes the list comprehensiveness metric. The experimental\nresults indicate that the proposed framework can further improve the\nmulti-generator reranking performance.", "published": "2025-04-22 06:34:57", "link": "http://arxiv.org/abs/2504.15625v1", "categories": ["cs.IR", "H.3.3"], "primary_category": "cs.IR"}
{"title": "A Markov Chain Monte Carlo Method for Efficient Finite-Length LDPC Code Design", "abstract": "Low-density parity-check (LDPC) codes are among the most prominent\nerror-correction schemes. They find application to fortify various modern\nstorage, communication, and computing systems. Protograph-based (PB) LDPC codes\noffer many degrees of freedom in the code design and enable fast encoding and\ndecoding. In particular, spatially-coupled (SC) and multi-dimensional (MD)\ncirculant-based codes are PB-LDPC codes with excellent performance. Efficient\nfinite-length (FL) algorithms are required in order to effectively exploit the\navailable degrees of freedom offered by SC partitioning, lifting, and MD\nrelocations. In this paper, we propose a novel Markov chain Monte Carlo (MCMC\nor MC$^2$) method to perform this FL optimization, addressing the removal of\nshort cycles. While iterating, we draw samples from a defined distribution\nwhere the probability decreases as the number of short cycles from the previous\niteration increases. We analyze our MC$^2$ method theoretically as we prove the\ninvariance of the Markov chain where each state represents a possible\npartitioning or lifting arrangement. Via our simulations, we then fit the\ndistribution of the number of cycles resulting from a given arrangement on a\nGaussian distribution. We derive estimates for cycle counts that are close to\nthe actual counts. Furthermore, we derive the order of the expected number of\niterations required by our approach to reach a local minimum as well as the\nsize of the Markov chain recurrent class. Our approach is compatible with code\ndesign techniques based on gradient-descent. Numerical results show that our\nMC$^2$ method generates SC codes with remarkably less number of short cycles\ncompared with the current state-of-the-art. Moreover, to reach the same number\nof cycles, our method requires orders of magnitude less overall time compared\nwith the available literature methods.", "published": "2025-04-22 17:51:40", "link": "http://arxiv.org/abs/2504.16071v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Over-the-Air Transmission of Zak-OTFS with Spread Pilots on Sub-THz Communications Testbed", "abstract": "Looking towards 6G wireless systems, frequency bands like the sub-terahertz\n(sub-THz) band (100 GHz - 300 GHz) are gaining traction for their promises of\nlarge available swaths of bandwidth to support the ever-growing data demands.\nHowever, challenges with harsh channel conditions and hardware nonlinearities\nin the sub-THz band require robust communication techniques with favorable\nproperties, such as good spectral efficiency and low peak-to-average power\nratio (PAPR). Recently, OTFS and its variants have garnered significant\nattention for their performance in severe conditions (like high delay and\nDoppler), making it a promising candidate for future communications. In this\nwork, we implement Zak-OTFS for the over-the-air experiments with traditional\npoint pilots and the new spread pilots. Notably, we design our spread-pilot\nwaveforms with communications and sensing coexisting in the same radio\nresources. We define the system model and the signal design for integration\nonto our state-of-the-art sub-THz wireless testbed. We show successful data\ntransmission over-the-air at 140 GHz and 240 GHz in a variety of\nsignal-to-noise ratio (SNR) conditions. In addition, we demonstrate integrated\nsensing and communications (ISAC) capabilities and show PAPR improvement of\nover 5 dB with spread pilots compared to point pilots.", "published": "2025-04-22 14:46:11", "link": "http://arxiv.org/abs/2504.15947v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Cryptoanalysis of a public key exchange based on circulant matrix over digital semiring", "abstract": "We present a cryptanalysis of a key exchange protocol based on the digital\nsemiring. For this purpose, we find the maximal solution of a linear system\nover such semiring, and use the properties of circulant matrix to demonstrate\nthat the protocol is vulnerable. Specifically, we provide an efficient attack\nthat recovers the shared secret key from publicly exchanged information for any\ninstance of the digital semiring in polynomial time.", "published": "2025-04-22 13:25:29", "link": "http://arxiv.org/abs/2504.15880v1", "categories": ["cs.CR", "cs.IT", "math.AC", "math.IT"], "primary_category": "cs.CR"}
{"title": "A new method for erasure decoding of convolutional codes", "abstract": "In this paper, we propose a new erasure decoding algorithm for convolutional\ncodes using the generator matrix. This implies that our decoding method also\napplies to catastrophic convolutional codes in opposite to the classic approach\nusing the parity-check matrix. We compare the performance of both decoding\nalgorithms. Moreover, we enlarge the family of optimal convolutional codes\n(complete-MDP) based on the generator matrix.", "published": "2025-04-22 13:18:17", "link": "http://arxiv.org/abs/2504.15873v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Energy-Efficient SIM-assisted Communications: How Many Layers Do We Need?", "abstract": "The stacked intelligent metasurface (SIM), comprising multiple layers of\nreconfigurable transmissive metasurfaces, is becoming an increasingly viable\nsolution for future wireless communication systems. In this paper, we explore\nthe integration of SIM in a multi-antenna base station for application to\ndownlink multi-user communications, and a realistic power consumption model for\nSIM-assisted systems is presented. Specifically, we focus on maximizing the\nenergy efficiency (EE) for hybrid precoding design, i.e., the base station\ndigital precoding and SIM wave-based beamforming. Due to the non-convexity and\nhigh complexity of the formulated problem, we employ the quadratic\ntransformation method to reformulate the optimization problem and propose an\nalternating optimization (AO)-based joint precoding framework. Specifically, a\nsuccessive convex approximation (SCA) algorithm is adopted for the base station\nprecoding design. For the SIM wave-based beamforming, two algorithms are\nemployed: the high-performance semidefinite programming (SDP) method and the\nlow-complexity projected gradient ascent (PGA) algorithm. In particular, the\nresults indicate that while the optimal number of SIM layers for maximizing the\nEE and spectral efficiency differs, a design of 2 to 5 layers can achieve\nsatisfactory performance for both. Finally, numerical results are illustrated\nto evaluate the effectiveness of the proposed hybrid precoding framework and to\nshowcase the performance enhancement achieved by the algorithm in comparison to\nbenchmark schemes.", "published": "2025-04-22 09:31:55", "link": "http://arxiv.org/abs/2504.15737v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Distributed Compression for Computation and Bounds on the Optimal Rate", "abstract": "We address the problem of distributed computation of arbitrary functions of\ntwo correlated sources $X_1$ and $X_2$, residing in two distributed source\nnodes, respectively. We exploit the structure of a computation task by coding\nsource characteristic graphs (and multiple instances using the $n$-fold OR\nproduct of this graph with itself). For regular graphs and general graphs, we\nestablish bounds on the optimal rate -- characterized by the chromatic entropy\nfor the $n$-fold graph products -- that allows a receiver for asymptotically\nlossless computation of arbitrary functions over finite fields. For the special\nclass of cycle graphs (i.e., $2$-regular graphs), we establish an exact\ncharacterization of chromatic numbers and derive bounds on the required rates.\nNext, focusing on the more general class of $d$-regular graphs, we establish\nconnections between $d$-regular graphs and expansion rates for $n$-fold graph\npowers using graph spectra. Finally, for general graphs, we leverage the\nGershgorin Circle Theorem (GCT) to provide a characterization of the spectra,\nwhich allows us to build new bounds on the optimal rate. Our codes leverage the\nspectra of the computation and provide a graph expansion-based characterization\nto efficiently/succinctly capture the computation structure, providing new\ninsights into the problem of distributed computation of arbitrary functions.", "published": "2025-04-22 08:47:40", "link": "http://arxiv.org/abs/2504.15706v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Validation of 3GPP TR 38.901 Indoor Hotspot Path Loss Model Based on Measurements Conducted at 6.75, 16.95, 28, and 73 GHz for 6G and Beyond", "abstract": "This paper presents a thorough validation of the Third Generation Partnership\nProject (3GPP) Technical Report (TR) 38.901 indoor hotspot (InH) path loss\nmodel, as part of the 3GPP Release 19 study on \"Channel model validation of TR\n38.901 for 7-24 GHz,\" for 6G standardization. Specifically, we validate the\n3GPP TR 38.901 path loss model for the InH scenario in both line of sight (LOS)\nand non line of sight (NLOS) channel conditions, using the floating intercept\n(FI) and alpha-beta-gamma (ABG) path loss models. The validation focuses on\nspecific frequencies, including 6.75 GHz and 16.95 GHz, as well as the broader\n7-24 GHz and 0.5-100 GHz frequency ranges. The validation is based on\nreal-world measurements conducted at 6.75 GHz, 16.95 GHz, 28 GHz, and 73 GHz by\nNYU WIRELESS using a 1 GHz wideband time domain based sliding correlation\nchannel sounder in the InH scenario for both LOS and NLOS channel conditions.\nOur results confirm that the 3GPP TR 38.901 path loss model for the InH\nscenario remains valid for the 7-24 GHz range in both LOS and NLOS conditions\nand provide valuable input for 6G standardization efforts.", "published": "2025-04-22 05:08:09", "link": "http://arxiv.org/abs/2504.15589v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Bayesian information theoretic model-averaging stochastic item selection for computer adaptive testing: compromise-free item exposure", "abstract": "The goal of Computer Adaptive Testing (CAT) is to reliably estimate an\nindividual's ability as modeled by an item response theory (IRT) instrument\nusing only a subset of the instrument's items. A secondary goal is to vary the\nitems presented across different testing sessions so that the sequence of items\ndoes not become overly stereotypical -- we want all items to have an exposure\nrate sufficiently far from zero. We formulate the optimization problem for CAT\nin terms of Bayesian information theory, where one chooses the item at each\nstep based on the criterion of the ability model discrepancy -- the statistical\ndistance between the ability estimate at the next step and the full-test\nability estimate. This viewpoint of CAT naturally motivates a stochastic\nselection procedure that equates choosing the next item to sampling from a\nmodel-averaging ensemble ability model. Using the NIH Work Disability\nFunctional Assessment Battery (WD-FAB), we evaluate our new methods in\ncomparison to pre-existing methods found in the literature. We find that our\nstochastic selector has superior properties in terms of both item exposure and\ntest accuracy/efficiency.", "published": "2025-04-22 02:45:16", "link": "http://arxiv.org/abs/2504.15543v1", "categories": ["stat.ME", "cs.IT", "math.IT", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Explainable Unsupervised Anomaly Detection with Random Forest", "abstract": "We describe the use of an unsupervised Random Forest for similarity learning\nand improved unsupervised anomaly detection. By training a Random Forest to\ndiscriminate between real data and synthetic data sampled from a uniform\ndistribution over the real data bounds, a distance measure is obtained that\nanisometrically transforms the data, expanding distances at the boundary of the\ndata manifold. We show that using distances recovered from this transformation\nimproves the accuracy of unsupervised anomaly detection, compared to other\ncommonly used detectors, demonstrated over a large number of benchmark\ndatasets. As well as improved performance, this method has advantages over\nother unsupervised anomaly detection methods, including minimal requirements\nfor data preprocessing, native handling of missing data, and potential for\nvisualizations. By relating outlier scores to partitions of the Random Forest,\nwe develop a method for locally explainable anomaly predictions in terms of\nfeature importance.", "published": "2025-04-22 17:54:44", "link": "http://arxiv.org/abs/2504.16075v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "High-performance training and inference for deep equivariant interatomic potentials", "abstract": "Machine learning interatomic potentials, particularly those based on deep\nequivariant neural networks, have demonstrated state-of-the-art accuracy and\ncomputational efficiency in atomistic modeling tasks like molecular dynamics\nand high-throughput screening. The size of datasets and demands of downstream\nworkflows are growing rapidly, making robust and scalable software essential.\nThis work presents a major overhaul of the NequIP framework focusing on\nmulti-node parallelism, computational performance, and extensibility. The\nredesigned framework supports distributed training on large datasets and\nremoves barriers preventing full utilization of the PyTorch 2.0 compiler at\ntrain time. We demonstrate this acceleration in a case study by training\nAllegro models on the SPICE 2 dataset of organic molecular systems. For\ninference, we introduce the first end-to-end infrastructure that uses the\nPyTorch Ahead-of-Time Inductor compiler for machine learning interatomic\npotentials. Additionally, we implement a custom kernel for the Allegro model's\nmost expensive operation, the tensor product. Together, these advancements\nspeed up molecular dynamics calculations on system sizes of practical relevance\nby up to a factor of 18.", "published": "2025-04-22 17:47:01", "link": "http://arxiv.org/abs/2504.16068v1", "categories": ["physics.comp-ph", "cs.LG", "physics.chem-ph"], "primary_category": "physics.comp-ph"}
{"title": "$\u03c0_{0.5}$: a Vision-Language-Action Model with Open-World Generalization", "abstract": "In order for robots to be useful, they must perform practically relevant\ntasks in the real world, outside of the lab. While vision-language-action (VLA)\nmodels have demonstrated impressive results for end-to-end robot control, it\nremains an open question how far such models can generalize in the wild. We\ndescribe $\\pi_{0.5}$, a new model based on $\\pi_{0}$ that uses co-training on\nheterogeneous tasks to enable broad generalization. $\\pi_{0.5}$\\ uses data from\nmultiple robots, high-level semantic prediction, web data, and other sources to\nenable broadly generalizable real-world robotic manipulation. Our system uses a\ncombination of co-training and hybrid multi-modal examples that combine image\nobservations, language commands, object detections, semantic subtask\nprediction, and low-level actions. Our experiments show that this kind of\nknowledge transfer is essential for effective generalization, and we\ndemonstrate for the first time that an end-to-end learning-enabled robotic\nsystem can perform long-horizon and dexterous manipulation skills, such as\ncleaning a kitchen or bedroom, in entirely new homes.", "published": "2025-04-22 17:31:29", "link": "http://arxiv.org/abs/2504.16054v1", "categories": ["cs.LG", "cs.RO"], "primary_category": "cs.LG"}
{"title": "The Formation of Production Networks: How Supply Chains Arise from Simple Learning with Minimal Information", "abstract": "We develop a model where firms determine the price at which they sell their\ndifferentiable goods, the volume that they produce, and the inputs (types and\namounts) that they purchase from other firms. A steady-state production network\nemerges endogenously without resorting to assumptions such as equilibrium or\nperfect knowledge about production technologies. Through a simple version of\nreinforcement learning, firms with heterogeneous technologies cope with\nuncertainty and maximize profits. Due to this learning process, firms can adapt\nto shocks such as demand shifts, suppliers/clients closure, productivity\nchanges, and production technology modifications; effectively reshaping the\nproduction network. To demonstrate the potential of this model, we analyze the\nupstream and downstream impact of demand and productivity shocks.", "published": "2025-04-22 16:18:19", "link": "http://arxiv.org/abs/2504.16010v1", "categories": ["cs.MA", "cs.LG", "econ.GN", "q-fin.EC"], "primary_category": "cs.MA"}
{"title": "Benchmarking machine learning models for predicting aerofoil performance", "abstract": "This paper investigates the capability of Neural Networks (NNs) as\nalternatives to the traditional methods to analyse the performance of aerofoils\nused in the wind and tidal energy industry. The current methods used to assess\nthe characteristic lift and drag coefficients include Computational Fluid\nDynamics (CFD), thin aerofoil and panel methods, all face trade-offs between\ncomputational speed and the accuracy of the results and as such NNs have been\ninvestigated as an alternative with the aim that it would perform both quickly\nand accurately. As such, this paper provides a benchmark for the windAI_bench\ndataset published by the National Renewable Energy Laboratory (NREL) in the\nUSA. In order to validate the methodology of the benchmarking, the AirfRANS\n{\\tt arXiv:2212.07564v3} dataset is used as both a starting point and a point\nof comparison. This study evaluates four neural networks (MLP, PointNet,\nGraphSAGE, GUNet) trained on a range aerofoils at 25 angles of attack\n(4$^\\circ$ to 20$^\\circ$). to predict fluid flow and calculate lift\ncoefficients ($C_L$) via the panel method. GraphSAGE and GUNet performed well\nduring the testing phase, but underperformed during validation. Accordingly,\nthis paper has identified PointNet and MLP as the two strongest models tested,\nhowever whilst the results from MLP are more commonly correct for predicting\nthe behaviour of the fluid, the results from PointNet provide the more accurate\nresults for calculating $C_L$.", "published": "2025-04-22 15:54:49", "link": "http://arxiv.org/abs/2504.15993v1", "categories": ["physics.flu-dyn", "cs.LG"], "primary_category": "physics.flu-dyn"}
{"title": "Efficient Discovery of Motif Transition Process for Large-Scale Temporal Graphs", "abstract": "Understanding the dynamic transition of motifs in temporal graphs is\nessential for revealing how graph structures evolve over time, identifying\ncritical patterns, and predicting future behaviors, yet existing methods often\nfocus on predefined motifs, limiting their ability to comprehensively capture\ntransitions and interrelationships. We propose a parallel motif transition\nprocess discovery algorithm, PTMT, a novel parallel method for discovering\nmotif transition processes in large-scale temporal graphs. PTMT integrates a\ntree-based framework with the temporal zone partitioning (TZP) strategy, which\npartitions temporal graphs by time and structure while preserving lossless\nmotif transitions and enabling massive parallelism. PTMT comprises three\nphases: growth zone parallel expansion, overlap-aware result aggregation, and\ndeterministic encoding of motif transitions, ensuring accurate tracking of\ndynamic transitions and interactions. Results on 10 real-world datasets\ndemonstrate that PTMT achieves speedups ranging from 12.0$\\times$ to\n50.3$\\times$ compared to the SOTA method.", "published": "2025-04-22 15:30:04", "link": "http://arxiv.org/abs/2504.15979v1", "categories": ["cs.DB", "cs.LG"], "primary_category": "cs.DB"}
{"title": "Adversarial Observations in Weather Forecasting", "abstract": "AI-based systems, such as Google's GenCast, have recently redefined the state\nof the art in weather forecasting, offering more accurate and timely\npredictions of both everyday weather and extreme events. While these systems\nare on the verge of replacing traditional meteorological methods, they also\nintroduce new vulnerabilities into the forecasting process. In this paper, we\ninvestigate this threat and present a novel attack on autoregressive diffusion\nmodels, such as those used in GenCast, capable of manipulating weather\nforecasts and fabricating extreme events, including hurricanes, heat waves, and\nintense rainfall. The attack introduces subtle perturbations into weather\nobservations that are statistically indistinguishable from natural noise and\nchange less than 0.1% of the measurements - comparable to tampering with data\nfrom a single meteorological satellite. As modern forecasting integrates data\nfrom nearly a hundred satellites and many other sources operated by different\ncountries, our findings highlight a critical security risk with the potential\nto cause large-scale disruptions and undermine public trust in weather\nprediction.", "published": "2025-04-22 14:38:13", "link": "http://arxiv.org/abs/2504.15942v1", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Low-Rank Adaptation of Neural Fields", "abstract": "Processing visual data often involves small adjustments or sequences of\nchanges, such as in image filtering, surface smoothing, and video storage.\nWhile established graphics techniques like normal mapping and video compression\nexploit redundancy to encode such small changes efficiently, the problem of\nencoding small changes to neural fields (NF) -- neural network\nparameterizations of visual or physical functions -- has received less\nattention.\n  We propose a parameter-efficient strategy for updating neural fields using\nlow-rank adaptations (LoRA). LoRA, a method from the parameter-efficient\nfine-tuning LLM community, encodes small updates to pre-trained models with\nminimal computational overhead. We adapt LoRA to instance-specific neural\nfields, avoiding the need for large pre-trained models yielding a pipeline\nsuitable for low-compute hardware.\n  We validate our approach with experiments in image filtering, video\ncompression, and geometry editing, demonstrating its effectiveness and\nversatility for representing neural field updates.", "published": "2025-04-22 14:21:34", "link": "http://arxiv.org/abs/2504.15933v1", "categories": ["cs.GR", "cs.LG"], "primary_category": "cs.GR"}
{"title": "StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with Disaggregated Stream Generation", "abstract": "Reinforcement learning (RL) has become the core post-training technique for\nlarge language models (LLMs). RL for LLMs involves two stages: generation and\ntraining. The LLM first generates samples online, which are then used to derive\nrewards for training. The conventional view holds that the colocated\narchitecture, where the two stages share resources via temporal multiplexing,\noutperforms the disaggregated architecture, in which dedicated resources are\nassigned to each stage. However, in real-world deployments, we observe that the\ncolocated architecture suffers from resource coupling, where the two stages are\nconstrained to use the same resources. This coupling compromises the\nscalability and cost-efficiency of colocated RL in large-scale training. In\ncontrast, the disaggregated architecture allows for flexible resource\nallocation, supports heterogeneous training setups, and facilitates\ncross-datacenter deployment.\n  StreamRL is designed with disaggregation from first principles and fully\nunlocks its potential by addressing two types of performance bottlenecks in\nexisting disaggregated RL frameworks: pipeline bubbles, caused by stage\ndependencies, and skewness bubbles, resulting from long-tail output length\ndistributions. To address pipeline bubbles, StreamRL breaks the traditional\nstage boundary in synchronous RL algorithms through stream generation and\nachieves full overlapping in asynchronous RL. To address skewness bubbles,\nStreamRL employs an output-length ranker model to identify long-tail samples\nand reduces generation time via skewness-aware dispatching and scheduling.\nExperiments show that StreamRL improves throughput by up to 2.66x compared to\nexisting state-of-the-art systems, and improves cost-effectiveness by up to\n1.33x in a heterogeneous, cross-datacenter setting.", "published": "2025-04-22 14:19:06", "link": "http://arxiv.org/abs/2504.15930v1", "categories": ["cs.LG", "cs.DC"], "primary_category": "cs.LG"}
{"title": "ScaleGNN: Towards Scalable Graph Neural Networks via Adaptive High-order Neighboring Feature Fusion", "abstract": "Graph Neural Networks (GNNs) have demonstrated strong performance across\nvarious graph-based tasks by effectively capturing relational information\nbetween nodes. These models rely on iterative message passing to propagate node\nfeatures, enabling nodes to aggregate information from their neighbors. Recent\nresearch has significantly improved the message-passing mechanism, enhancing\nGNN scalability on large-scale graphs. However, GNNs still face two main\nchallenges: over-smoothing, where excessive message passing results in\nindistinguishable node representations, especially in deep networks\nincorporating high-order neighbors; and scalability issues, as traditional\narchitectures suffer from high model complexity and increased inference time\ndue to redundant information aggregation. This paper proposes a novel framework\nfor large-scale graphs named ScaleGNN that simultaneously addresses both\nchallenges by adaptively fusing multi-level graph features. We first construct\nneighbor matrices for each order, learning their relative information through\ntrainable weights through an adaptive high-order feature fusion module. This\nallows the model to selectively emphasize informative high-order neighbors\nwhile reducing unnecessary computational costs. Additionally, we introduce a\nHigh-order redundant feature masking mechanism based on a Local Contribution\nScore (LCS), which enables the model to retain only the most relevant neighbors\nat each order, preventing redundant information propagation. Furthermore,\nlow-order enhanced feature aggregation adaptively integrates low-order and\nhigh-order features based on task relevance, ensuring effective capture of both\nlocal and global structural information without excessive complexity. Extensive\nexperiments on real-world datasets demonstrate that our approach consistently\noutperforms state-of-the-art GNN models in both accuracy and computational\nefficiency.", "published": "2025-04-22 14:05:11", "link": "http://arxiv.org/abs/2504.15920v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "SUPRA: Subspace Parameterized Attention for Neural Operator on General Domains", "abstract": "Neural operators are efficient surrogate models for solving partial\ndifferential equations (PDEs), but their key components face challenges: (1) in\norder to improve accuracy, attention mechanisms suffer from computational\ninefficiency on large-scale meshes, and (2) spectral convolutions rely on the\nFast Fourier Transform (FFT) on regular grids and assume a flat geometry, which\ncauses accuracy degradation on irregular domains. To tackle these problems, we\nregard the matrix-vector operations in the standard attention mechanism on\nvectors in Euclidean space as bilinear forms and linear operators in vector\nspaces and generalize the attention mechanism to function spaces. This new\nattention mechanism is fully equivalent to the standard attention but\nimpossible to compute due to the infinite dimensionality of function spaces. To\naddress this, inspired by model reduction techniques, we propose a Subspace\nParameterized Attention (SUPRA) neural operator, which approximates the\nattention mechanism within a finite-dimensional subspace. To construct a\nsubspace on irregular domains for SUPRA, we propose using the Laplacian\neigenfunctions, which naturally adapt to domains' geometry and guarantee the\noptimal approximation for smooth functions. Experiments show that the SUPRA\nneural operator reduces error rates by up to 33% on various PDE datasets while\nmaintaining state-of-the-art computational efficiency.", "published": "2025-04-22 13:40:04", "link": "http://arxiv.org/abs/2504.15897v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Consistent Causal Inference of Group Effects in Non-Targeted Trials with Finitely Many Effect Levels", "abstract": "A treatment may be appropriate for some group (the ``sick\" group) on whom it\nhas a positive effect, but it can also have a detrimental effect on subjects\nfrom another group (the ``healthy\" group). In a non-targeted trial both sick\nand healthy subjects may be treated, producing heterogeneous effects within the\ntreated group. Inferring the correct treatment effect on the sick population is\nthen difficult, because the effects on the different groups get tangled. We\npropose an efficient nonparametric approach to estimating the group effects,\ncalled {\\bf PCM} (pre-cluster and merge). We prove its asymptotic consistency\nin a general setting and show, on synthetic data, more than a 10x improvement\nin accuracy over existing state-of-the-art. Our approach applies more generally\nto consistent estimation of functions with a finite range.", "published": "2025-04-22 12:51:07", "link": "http://arxiv.org/abs/2504.15854v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Adaptive PCA-Based Outlier Detection for Multi-Feature Time Series in Space Missions", "abstract": "Analyzing multi-featured time series data is critical for space missions\nmaking efficient event detection, potentially onboard, essential for automatic\nanalysis. However, limited onboard computational resources and data downlink\nconstraints necessitate robust methods for identifying regions of interest in\nreal time. This work presents an adaptive outlier detection algorithm based on\nthe reconstruction error of Principal Component Analysis (PCA) for feature\nreduction, designed explicitly for space mission applications. The algorithm\nadapts dynamically to evolving data distributions by using Incremental PCA,\nenabling deployment without a predefined model for all possible conditions. A\npre-scaling process normalizes each feature's magnitude while preserving\nrelative variance within feature types. We demonstrate the algorithm's\neffectiveness in detecting space plasma events, such as distinct space\nenvironments, dayside and nightside transients phenomena, and transition layers\nthrough NASA's MMS mission observations. Additionally, we apply the method to\nNASA's THEMIS data, successfully identifying a dayside transient using\nonboard-available measurements.", "published": "2025-04-22 12:42:38", "link": "http://arxiv.org/abs/2504.15846v1", "categories": ["cs.LG", "physics.space-ph"], "primary_category": "cs.LG"}
{"title": "Full waveform inversion with CNN-based velocity representation extension", "abstract": "Full waveform inversion (FWI) updates the velocity model by minimizing the\ndiscrepancy between observed and simulated data. However, discretization errors\nin numerical modeling and incomplete seismic data acquisition can introduce\nnoise, which propagates through the adjoint operator and affects the accuracy\nof the velocity gradient, thereby impacting the FWI inversion accuracy. To\nmitigate the influence of noise on the gradient, we employ a convolutional\nneural network (CNN) to refine the velocity model before performing the forward\nsimulation, aiming to reduce noise and provide a more accurate velocity update\ndirection. We use the same data misfit loss to update both the velocity and\nnetwork parameters, thereby forming a self-supervised learning procedure. We\npropose two implementation schemes, which differ in whether the velocity update\npasses through the CNN. In both methodologies, the velocity representation is\nextended (VRE) by using a neural network in addition to the grid-based\nvelocities. Thus, we refer to this general approach as VRE-FWI. Synthetic and\nreal data tests demonstrate that the proposed VRE-FWI achieves higher velocity\ninversion accuracy compared to traditional FWI, at a marginal additional\ncomputational cost of approximately 1%.", "published": "2025-04-22 12:14:38", "link": "http://arxiv.org/abs/2504.15826v1", "categories": ["physics.geo-ph", "cs.LG"], "primary_category": "physics.geo-ph"}
{"title": "Grounded in Context: Retrieval-Based Method for Hallucination Detection", "abstract": "Despite advancements in grounded content generation, production Large\nLanguage Models (LLMs) based applications still suffer from hallucinated\nanswers. We present \"Grounded in Context\" - Deepchecks' hallucination detection\nframework, designed for production-scale long-context data and tailored to\ndiverse use cases, including summarization, data extraction, and RAG. Inspired\nby RAG architecture, our method integrates retrieval and Natural Language\nInference (NLI) models to predict factual consistency between premises and\nhypotheses using an encoder-based model with only a 512-token context window.\nOur framework identifies unsupported claims with an F1 score of 0.83 in\nRAGTruth's response-level classification task, matching methods that trained on\nthe dataset, and outperforming all comparable frameworks using similar-sized\nmodels.", "published": "2025-04-22 10:28:23", "link": "http://arxiv.org/abs/2504.15771v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Observability conditions for neural state-space models with eigenvalues and their roots of unity", "abstract": "We operate through the lens of ordinary differential equations and control\ntheory to study the concept of observability in the context of neural\nstate-space models and the Mamba architecture. We develop strategies to enforce\nobservability, which are tailored to a learning context, specifically where the\nhidden states are learnable at initial time, in conjunction to over its\ncontinuum, and high-dimensional. We also highlight our methods emphasize\neigenvalues, roots of unity, or both. Our methods effectuate computational\nefficiency when enforcing observability, sometimes at great scale. We formulate\nobservability conditions in machine learning based on classical control theory\nand discuss their computational complexity. Our nontrivial results are\nfivefold. We discuss observability through the use of permutations in neural\napplications with learnable matrices without high precision. We present two\nresults built upon the Fourier transform that effect observability with high\nprobability up to the randomness in the learning. These results are worked with\nthe interplay of representations in Fourier space and their eigenstructure,\nnonlinear mappings, and the observability matrix. We present a result for Mamba\nthat is similar to a Hautus-type condition, but instead employs an argument\nusing a Vandermonde matrix instead of eigenvectors. Our final result is a\nshared-parameter construction of the Mamba system, which is computationally\nefficient in high exponentiation. We develop a training algorithm with this\ncoupling, showing it satisfies a Robbins-Monro condition under certain\northogonality, while a more classical training procedure fails to satisfy a\ncontraction with high Lipschitz constant.", "published": "2025-04-22 10:10:52", "link": "http://arxiv.org/abs/2504.15758v1", "categories": ["cs.LG", "cs.SY", "eess.SY", "math.DS", "math.OC"], "primary_category": "cs.LG"}
{"title": "Markov Kernels, Distances and Optimal Control: A Parable of Linear Quadratic Non-Gaussian Distribution Steering", "abstract": "For a controllable linear time-varying (LTV) pair\n$(\\boldsymbol{A}_t,\\boldsymbol{B}_t)$ and $\\boldsymbol{Q}_{t}$ positive\nsemidefinite, we derive the Markov kernel for the It\\^{o} diffusion\n${\\mathrm{d}}\\boldsymbol{x}_{t}=\\boldsymbol{A}_{t}\\boldsymbol{x}_t {\\mathrm{d}}\nt + \\sqrt{2}\\boldsymbol{B}_{t}{\\mathrm{d}}\\boldsymbol{w}_{t}$ with an\naccompanying killing of probability mass at rate\n$\\frac{1}{2}\\boldsymbol{x}^{\\top}\\boldsymbol{Q}_{t}\\boldsymbol{x}$. This Markov\nkernel is the Green's function for an associated linear\nreaction-advection-diffusion partial differential equation. Our result\ngeneralizes the recently derived kernel for the special case\n$\\left(\\boldsymbol{A}_t,\\boldsymbol{B}_t\\right)=\\left(\\boldsymbol{0},\\boldsymbol{I}\\right)$,\nand depends on the solution of an associated Riccati matrix ODE. A consequence\nof this result is that the linear quadratic non-Gaussian Schr\\\"{o}dinger bridge\nis exactly solvable. This means that the problem of steering a controlled LTV\ndiffusion from a given non-Gaussian distribution to another over a fixed\ndeadline while minimizing an expected quadratic cost can be solved using\ndynamic Sinkhorn recursions performed with the derived kernel. Our derivation\nfor the\n$\\left(\\boldsymbol{A}_t,\\boldsymbol{B}_t,\\boldsymbol{Q}_t\\right)$-parametrized\nkernel pursues a new idea that relies on finding a state-time dependent\ndistance-like functional given by the solution of a deterministic optimal\ncontrol problem. This technique breaks away from existing methods, such as\ngeneralizing Hermite polynomials or Weyl calculus, which have seen limited\nsuccess in the reaction-diffusion context. Our technique uncovers a new\nconnection between Markov kernels, distances, and optimal control. This\nconnection is of interest beyond its immediate application in solving the\nlinear quadratic Schr\\\"{o}dinger bridge problem.", "published": "2025-04-22 10:07:43", "link": "http://arxiv.org/abs/2504.15753v1", "categories": ["math.OC", "cs.LG", "cs.SY", "eess.SY", "math.PR", "math.ST", "stat.TH"], "primary_category": "math.OC"}
{"title": "Riemannian Neural Geodesic Interpolant", "abstract": "Stochastic interpolants are efficient generative models that bridge two\narbitrary probability density functions in finite time, enabling flexible\ngeneration from the source to the target distribution or vice versa. These\nmodels are primarily developed in Euclidean space, and are therefore limited in\ntheir application to many distribution learning problems defined on Riemannian\nmanifolds in real-world scenarios. In this work, we introduce the Riemannian\nNeural Geodesic Interpolant (RNGI) model, which interpolates between two\nprobability densities on a Riemannian manifold along the stochastic geodesics,\nand then samples from one endpoint as the final state using the continuous flow\noriginating from the other endpoint. We prove that the temporal marginal\ndensity of RNGI solves a transport equation on the Riemannian manifold. After\ntraining the model's the neural velocity and score fields, we propose the\nEmbedding Stochastic Differential Equation (E-SDE) algorithm for stochastic\nsampling of RNGI. E-SDE significantly improves the sampling quality by reducing\nthe accumulated error caused by the excessive intrinsic discretization of\nRiemannian Brownian motion in the classical Geodesic Random Walk (GRW)\nalgorithm. We also provide theoretical bounds on the generative bias measured\nin terms of KL-divergence. Finally, we demonstrate the effectiveness of the\nproposed RNGI and E-SDE through experiments conducted on both collected and\nsynthetic distributions on S2 and SO(3).", "published": "2025-04-22 09:28:29", "link": "http://arxiv.org/abs/2504.15736v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "From predictions to confidence intervals: an empirical study of conformal prediction methods for in-context learning", "abstract": "Transformers have become a standard architecture in machine learning,\ndemonstrating strong in-context learning (ICL) abilities that allow them to\nlearn from the prompt at inference time. However, uncertainty quantification\nfor ICL remains an open challenge, particularly in noisy regression tasks. This\npaper investigates whether ICL can be leveraged for distribution-free\nuncertainty estimation, proposing a method based on conformal prediction to\nconstruct prediction intervals with guaranteed coverage. While traditional\nconformal methods are computationally expensive due to repeated model fitting,\nwe exploit ICL to efficiently generate confidence intervals in a single forward\npass. Our empirical analysis compares this approach against ridge\nregression-based conformal methods, showing that conformal prediction with\nin-context learning (CP with ICL) achieves robust and scalable uncertainty\nestimates. Additionally, we evaluate its performance under distribution shifts\nand establish scaling laws to guide model training. These findings bridge ICL\nand conformal prediction, providing a theoretically grounded and new framework\nfor uncertainty quantification in transformer-based models.", "published": "2025-04-22 09:11:48", "link": "http://arxiv.org/abs/2504.15722v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Transfer Learning for High-dimensional Reduced Rank Time Series Models", "abstract": "The objective of transfer learning is to enhance estimation and inference in\na target data by leveraging knowledge gained from additional sources. Recent\nstudies have explored transfer learning for independent observations in\ncomplex, high-dimensional models assuming sparsity, yet research on time series\nmodels remains limited. Our focus is on transfer learning for sequences of\nobservations with temporal dependencies and a more intricate model parameter\nstructure. Specifically, we investigate the vector autoregressive model (VAR),\na widely recognized model for time series data, where the transition matrix can\nbe deconstructed into a combination of a sparse matrix and a low-rank one. We\npropose a new transfer learning algorithm tailored for estimating\nhigh-dimensional VAR models characterized by low-rank and sparse structures.\nAdditionally, we present a novel approach for selecting informative\nobservations from auxiliary datasets. Theoretical guarantees are established,\nencompassing model parameter consistency, informative set selection, and the\nasymptotic distribution of estimators under mild conditions. The latter\nfacilitates the construction of entry-wise confidence intervals for model\nparameters. Finally, we demonstrate the empirical efficacy of our methodologies\nthrough both simulated and real-world datasets.", "published": "2025-04-22 08:15:59", "link": "http://arxiv.org/abs/2504.15691v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Invariant Learning with Annotation-free Environments", "abstract": "Invariant learning is a promising approach to improve domain generalization\ncompared to Empirical Risk Minimization (ERM). However, most invariant learning\nmethods rely on the assumption that training examples are pre-partitioned into\ndifferent known environments. We instead infer environments without the need\nfor additional annotations, motivated by observations of the properties within\nthe representation space of a trained ERM model. We show the preliminary\neffectiveness of our approach on the ColoredMNIST benchmark, achieving\nperformance comparable to methods requiring explicit environment labels and on\npar with an annotation-free method that poses strong restrictions on the ERM\nreference model.", "published": "2025-04-22 08:10:06", "link": "http://arxiv.org/abs/2504.15686v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Policy-Based Radiative Transfer: Solving the $2$-Level Atom Non-LTE Problem using Soft Actor-Critic Reinforcement Learning", "abstract": "We present a novel reinforcement learning (RL) approach for solving the\nclassical 2-level atom non-LTE radiative transfer problem by framing it as a\ncontrol task in which an RL agent learns a depth-dependent source function\n$S(\\tau)$ that self-consistently satisfies the equation of statistical\nequilibrium (SE). The agent's policy is optimized entirely via reward-based\ninteractions with a radiative transfer engine, without explicit knowledge of\nthe ground truth. This method bypasses the need for constructing approximate\nlambda operators ($\\Lambda^*$) common in accelerated iterative schemes.\nAdditionally, it requires no extensive precomputed labeled datasets to extract\na supervisory signal, and avoids backpropagating gradients through the complex\nRT solver itself. Finally, we show through experiment that a simple feedforward\nneural network trained greedily cannot solve for SE, possibly due to the moving\ntarget nature of the problem. Our $\\Lambda^*-\\text{Free}$ method offers\npotential advantages for complex scenarios (e.g., atmospheres with enhanced\nvelocity fields, multi-dimensional geometries, or complex microphysics) where\n$\\Lambda^*$ construction or solver differentiability is challenging.\nAdditionally, the agent can be incentivized to find more efficient policies by\nmanipulating the discount factor, leading to a reprioritization of immediate\nrewards. If demonstrated to generalize past its training data, this RL\nframework could serve as an alternative or accelerated formalism to achieve SE.\nTo the best of our knowledge, this study represents the first application of\nreinforcement learning in solar physics that directly solves for a fundamental\nphysical constraint.", "published": "2025-04-22 08:03:09", "link": "http://arxiv.org/abs/2504.15679v1", "categories": ["astro-ph.SR", "cs.LG"], "primary_category": "astro-ph.SR"}
{"title": "TrojanDam: Detection-Free Backdoor Defense in Federated Learning through Proactive Model Robustification utilizing OOD Data", "abstract": "Federated learning (FL) systems allow decentralized data-owning clients to\njointly train a global model through uploading their locally trained updates to\na centralized server. The property of decentralization enables adversaries to\ncraft carefully designed backdoor updates to make the global model misclassify\nonly when encountering adversary-chosen triggers. Existing defense mechanisms\nmainly rely on post-training detection after receiving updates. These methods\neither fail to identify updates which are deliberately fabricated statistically\nclose to benign ones, or show inconsistent performance in different FL training\nstages. The effect of unfiltered backdoor updates will accumulate in the global\nmodel, and eventually become functional. Given the difficulty of ruling out\nevery backdoor update, we propose a backdoor defense paradigm, which focuses on\nproactive robustification on the global model against potential backdoor\nattacks. We first reveal that the successful launching of backdoor attacks in\nFL stems from the lack of conflict between malicious and benign updates on\nredundant neurons of ML models. We proceed to prove the feasibility of\nactivating redundant neurons utilizing out-of-distribution (OOD) samples in\ncentralized settings, and migrating to FL settings to propose a novel backdoor\ndefense mechanism, TrojanDam. The proposed mechanism has the FL server\ncontinuously inject fresh OOD mappings into the global model to activate\nredundant neurons, canceling the effect of backdoor updates during aggregation.\nWe conduct systematic and extensive experiments to illustrate the superior\nperformance of TrojanDam, over several SOTA backdoor defense methods across a\nwide range of FL settings.", "published": "2025-04-22 07:56:51", "link": "http://arxiv.org/abs/2504.15674v1", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Neural Kinematic Bases for Fluids", "abstract": "We propose mesh-free fluid simulations that exploit a kinematic neural basis\nfor velocity fields represented by an MLP. We design a set of losses that\nensures that these neural bases satisfy fundamental physical properties such as\northogonality, divergence-free, boundary alignment, and smoothness. Our neural\nbases can then be used to fit an input sketch of a flow, which will inherit the\nsame fundamental properties from the bases. We then can animate such flow in\nreal-time using standard time integrators. Our neural bases can accommodate\ndifferent domains and naturally extend to three dimensions.", "published": "2025-04-22 07:28:28", "link": "http://arxiv.org/abs/2504.15657v1", "categories": ["cs.GR", "cs.LG", "physics.flu-dyn"], "primary_category": "cs.GR"}
{"title": "A Study On Mixup-inspired Augmentation Methods For Software Vulnerability Detection", "abstract": "Various Deep Learning (DL) methods have recently been utilized to detect\nsoftware vulnerabilities. Real-world software vulnerability datasets are rare\nand hard to acquire as there's no simple metric for classifying vulnerability.\nSuch datasets are heavily imbalanced, and none of the current datasets are\nconsidered huge for DL models. To tackle these problems a recent work has tried\nto augment the dataset using the source code and generate realistic\nsingle-statement vulnerabilities which is not quite practical and requires\nmanual checking of the generated vulnerabilities. In this regard, we aim to\nexplore the augmentation of vulnerabilities at the representation level to help\ncurrent models learn better which has never been done before to the best of our\nknowledge. We implement and evaluate the 5 augmentation techniques that augment\nthe embedding of the data and recently have been used for code search which is\na completely different software engineering task. We also introduced a\nconditioned version of those augmentation methods, which ensures the\naugmentation does not change the vulnerable section of the vector\nrepresentation. We show that such augmentation methods can be helpful and\nincrease the f1-score by up to 9.67%, yet they cannot beat Random Oversampling\nwhen balancing datasets which increases the f1-score by 10.82%!", "published": "2025-04-22 06:47:39", "link": "http://arxiv.org/abs/2504.15632v1", "categories": ["cs.SE", "cs.CR", "cs.LG"], "primary_category": "cs.SE"}
{"title": "RadioDiff-$k^2$: Helmholtz Equation Informed Generative Diffusion Model for Multi-Path Aware Radio Map Construction", "abstract": "In this paper, we propose a novel physics-informed generative learning\napproach, termed RadioDiff-$\\bm{k^2}$, for accurate and efficient\nmultipath-aware radio map (RM) construction. As wireless communication evolves\ntowards environment-aware paradigms, driven by the increasing demand for\nintelligent and proactive optimization in sixth-generation (6G) networks,\naccurate construction of RMs becomes crucial yet highly challenging.\nConventional electromagnetic (EM)-based methods, such as full-wave solvers and\nray-tracing approaches, exhibit substantial computational overhead and limited\nadaptability to dynamic scenarios. Although, existing neural network (NN)\napproaches have efficient inferencing speed, they lack sufficient consideration\nof the underlying physics of EM wave propagation, limiting their effectiveness\nin accurately modeling critical EM singularities induced by complex multipath\nenvironments. To address these fundamental limitations, we propose a novel\nphysics-inspired RM construction method guided explicitly by the Helmholtz\nequation, which inherently governs EM wave propagation. Specifically, we\ntheoretically establish a direct correspondence between EM singularities, which\ncorrespond to the critical spatial features influencing wireless propagation,\nand regions defined by negative wave numbers in the Helmholtz equation. Based\non this insight, we design an innovative dual generative diffusion model (DM)\nframework comprising one DM dedicated to accurately inferring EM singularities\nand another DM responsible for reconstructing the complete RM using these\nsingularities along with environmental contextual information. Our\nphysics-informed approach uniquely combines the efficiency advantages of\ndata-driven methods with rigorous physics-based EM modeling, significantly\nenhancing RM accuracy, particularly in complex propagation environments\ndominated by multipath effects.", "published": "2025-04-22 06:28:13", "link": "http://arxiv.org/abs/2504.15623v1", "categories": ["cs.LG", "cs.SY", "eess.SY"], "primary_category": "cs.LG"}
{"title": "Dimension-Free Decision Calibration for Nonlinear Loss Functions", "abstract": "When model predictions inform downstream decision making, a natural question\nis under what conditions can the decision-makers simply respond to the\npredictions as if they were the true outcomes. Calibration suffices to\nguarantee that simple best-response to predictions is optimal. However,\ncalibration for high-dimensional prediction outcome spaces requires exponential\ncomputational and statistical complexity. The recent relaxation known as\ndecision calibration ensures the optimality of the simple best-response rule\nwhile requiring only polynomial sample complexity in the dimension of outcomes.\nHowever, known results on calibration and decision calibration crucially rely\non linear loss functions for establishing best-response optimality. A natural\napproach to handle nonlinear losses is to map outcomes $y$ into a feature space\n$\\phi(y)$ of dimension $m$, then approximate losses with linear functions of\n$\\phi(y)$. Unfortunately, even simple classes of nonlinear functions can demand\nexponentially large or infinite feature dimensions $m$. A key open problem is\nwhether it is possible to achieve decision calibration with sample complexity\nindependent of~$m$. We begin with a negative result: even verifying decision\ncalibration under standard deterministic best response inherently requires\nsample complexity polynomial in~$m$. Motivated by this lower bound, we\ninvestigate a smooth version of decision calibration in which decision-makers\nfollow a smooth best-response. This smooth relaxation enables dimension-free\ndecision calibration algorithms. We introduce algorithms that, given\n$\\mathrm{poly}(|A|,1/\\epsilon)$ samples and any initial predictor~$p$, can\nefficiently post-process it to satisfy decision calibration without worsening\naccuracy. Our algorithms apply broadly to function classes that can be\nwell-approximated by bounded-norm functions in (possibly infinite-dimensional)\nseparable RKHS.", "published": "2025-04-22 06:14:23", "link": "http://arxiv.org/abs/2504.15615v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning Dynamic Graphs via Tensorized and Lightweight Graph Convolutional Networks", "abstract": "A dynamic graph (DG) is frequently encountered in numerous real-world\nscenarios. Consequently, A dynamic graph convolutional network (DGCN) has been\nsuccessfully applied to perform precise representation learning on a DG.\nHowever, conventional DGCNs typically consist of a static GCN coupled with a\nsequence neural network (SNN) to model spatial and temporal patterns\nseparately. This decoupled modeling mechanism inherently disrupts the intricate\nspatio-temporal dependencies. To address the issue, this study proposes a novel\nTensorized Lightweight Graph Convolutional Network (TLGCN) for accurate dynamic\ngraph learning. It mainly contains the following two key concepts: a) designing\na novel spatio-temporal information propagation method for joint propagation of\nspatio-temporal information based on the tensor M-product framework; b)\nproposing a tensorized lightweight graph convolutional network based on the\nabove method, which significantly reduces the memory occupation of the model by\nomitting complex feature transformation and nonlinear activation. Numerical\nexperiments on four real-world datasets demonstrate that the proposed TLGCN\noutperforms the state-of-the-art models in the weight estimation task on DGs.", "published": "2025-04-22 06:13:32", "link": "http://arxiv.org/abs/2504.15613v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Smooth Calibration and Decision Making", "abstract": "Calibration requires predictor outputs to be consistent with their Bayesian\nposteriors. For machine learning predictors that do not distinguish between\nsmall perturbations, calibration errors are continuous in predictions, e.g.,\nsmooth calibration error (Foster and Hart, 2018), Distance to Calibration\n(Blasiok et al., 2023a). On the contrary, decision-makers who use predictions\nmake optimal decisions discontinuously in probabilistic space, experiencing\nloss from miscalibration discontinuously. Calibration errors for\ndecision-making are thus discontinuous, e.g., Expected Calibration Error\n(Foster and Vohra, 1997), and Calibration Decision Loss (Hu and Wu, 2024).\nThus, predictors with a low calibration error for machine learning may suffer a\nhigh calibration error for decision-making, i.e., they may not be trustworthy\nfor decision-makers optimizing assuming their predictions are correct. It is\nnatural to ask if post-processing a predictor with a low calibration error for\nmachine learning is without loss to achieve a low calibration error for\ndecision-making. In our paper, we show that post-processing an online predictor\nwith $\\epsilon$ distance to calibration achieves $O(\\sqrt{\\epsilon})$ ECE and\nCDL, which is asymptotically optimal. The post-processing algorithm adds noise\nto make predictions differentially private. The optimal bound from low distance\nto calibration predictors from post-processing is non-optimal compared with\nexisting online calibration algorithms that directly optimize for ECE and CDL.", "published": "2025-04-22 04:55:41", "link": "http://arxiv.org/abs/2504.15582v1", "categories": ["cs.LG", "cs.DS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "On the Price of Differential Privacy for Hierarchical Clustering", "abstract": "Hierarchical clustering is a fundamental unsupervised machine learning task\nwith the aim of organizing data into a hierarchy of clusters. Many applications\nof hierarchical clustering involve sensitive user information, therefore\nmotivating recent studies on differentially private hierarchical clustering\nunder the rigorous framework of Dasgupta's objective. However, it has been\nshown that any privacy-preserving algorithm under edge-level differential\nprivacy necessarily suffers a large error. To capture practical applications of\nthis problem, we focus on the weight privacy model, where each edge of the\ninput graph is at least unit weight. We present a novel algorithm in the weight\nprivacy model that shows significantly better approximation than known\nimpossibility results in the edge-level DP setting. In particular, our\nalgorithm achieves $O(\\log^{1.5}n/\\varepsilon)$ multiplicative error for\n$\\varepsilon$-DP and runs in polynomial time, where $n$ is the size of the\ninput graph, and the cost is never worse than the optimal additive error in\nexisting work. We complement our algorithm by showing if the unit-weight\nconstraint does not apply, the lower bound for weight-level DP hierarchical\nclustering is essentially the same as the edge-level DP, i.e.\n$\\Omega(n^2/\\varepsilon)$ additive error. As a result, we also obtain a new\nlower bound of $\\tilde{\\Omega}(1/\\varepsilon)$ additive error for balanced\nsparsest cuts in the weight-level DP model, which may be of independent\ninterest. Finally, we evaluate our algorithm on synthetic and real-world\ndatasets. Our experimental results show that our algorithm performs well in\nterms of extra cost and has good scalability to large graphs.", "published": "2025-04-22 04:39:40", "link": "http://arxiv.org/abs/2504.15580v1", "categories": ["cs.DS", "cs.CR", "cs.LG"], "primary_category": "cs.DS"}
{"title": "Real-Time Optimal Design of Experiment for Parameter Identification of Li-Ion Cell Electrochemical Model", "abstract": "Accurately identifying the parameters of electrochemical models of li-ion\nbattery (LiB) cells is a critical task for enhancing the fidelity and\npredictive ability. Traditional parameter identification methods often require\nextensive data collection experiments and lack adaptability in dynamic\nenvironments. This paper describes a Reinforcement Learning (RL) based approach\nthat dynamically tailors the current profile applied to a LiB cell to optimize\nthe parameters identifiability of the electrochemical model. The proposed\nframework is implemented in real-time using a Hardware-in-the-Loop (HIL) setup,\nwhich serves as a reliable testbed for evaluating the RL-based design strategy.\nThe HIL validation confirms that the RL-based experimental design outperforms\nconventional test protocols used for parameter identification in terms of both\nreducing the modeling errors on a verification test and minimizing the duration\nof the experiment used for parameter identification.", "published": "2025-04-22 04:25:50", "link": "http://arxiv.org/abs/2504.15578v1", "categories": ["eess.SY", "cs.LG", "cs.SY"], "primary_category": "eess.SY"}
{"title": "State-Aware IoT Scheduling Using Deep Q-Networks and Edge-Based Coordination", "abstract": "This paper addresses the challenge of energy efficiency management faced by\nintelligent IoT devices in complex application environments. A novel\noptimization method is proposed, combining Deep Q-Network (DQN) with an edge\ncollaboration mechanism. The method builds a state-action-reward interaction\nmodel and introduces edge nodes as intermediaries for state aggregation and\npolicy scheduling. This enables dynamic resource coordination and task\nallocation among multiple devices. During the modeling process, device status,\ntask load, and network resources are jointly incorporated into the state space.\nThe DQN is used to approximate and learn the optimal scheduling strategy. To\nenhance the model's ability to perceive inter-device relationships, a\ncollaborative graph structure is introduced to model the multi-device\nenvironment and assist in decision optimization. Experiments are conducted\nusing real-world IoT data collected from the FastBee platform. Several\ncomparative and validation tests are performed, including energy efficiency\ncomparisons across different scheduling strategies, robustness analysis under\nvarying task loads, and evaluation of state dimension impacts on policy\nconvergence speed. The results show that the proposed method outperforms\nexisting baseline approaches in terms of average energy consumption, processing\nlatency, and resource utilization. This confirms its effectiveness and\npracticality in intelligent IoT scenarios.", "published": "2025-04-22 04:24:16", "link": "http://arxiv.org/abs/2504.15577v1", "categories": ["cs.NI", "cs.LG"], "primary_category": "cs.NI"}
{"title": "SPECI: Skill Prompts based Hierarchical Continual Imitation Learning for Robot Manipulation", "abstract": "Real-world robot manipulation in dynamic unstructured environments requires\nlifelong adaptability to evolving objects, scenes and tasks. Traditional\nimitation learning relies on static training paradigms, which are ill-suited\nfor lifelong adaptation. Although Continual Imitation Learnin (CIL) enables\nincremental task adaptation while preserving learned knowledge, current CIL\nmethods primarily overlook the intrinsic skill characteristics of robot\nmanipulation or depend on manually defined and rigid skills, leading to\nsuboptimal cross-task knowledge transfer. To address these issues, we propose\nSkill Prompts-based HiErarchical Continual Imitation Learning (SPECI), a novel\nend-to-end hierarchical CIL policy architecture for robot manipulation. The\nSPECI framework consists of a multimodal perception and fusion module for\nheterogeneous sensory information encoding, a high-level skill inference module\nfor dynamic skill extraction and selection, and a low-level action execution\nmodule for precise action generation. To enable efficient knowledge transfer on\nboth skill and task levels, SPECI performs continual implicit skill acquisition\nand reuse via an expandable skill codebook and an attention-driven skill\nselection mechanism. Furthermore, we introduce mode approximation to augment\nthe last two modules with task-specific and task-sharing parameters, thereby\nenhancing task-level knowledge transfer. Extensive experiments on diverse\nmanipulation task suites demonstrate that SPECI consistently outperforms\nstate-of-the-art CIL methods across all evaluated metrics, revealing\nexceptional bidirectional knowledge transfer and superior overall performance.", "published": "2025-04-22 03:30:38", "link": "http://arxiv.org/abs/2504.15561v1", "categories": ["cs.RO", "cs.LG"], "primary_category": "cs.RO"}
{"title": "RiskNet: Interaction-Aware Risk Forecasting for Autonomous Driving in Long-Tail Scenarios", "abstract": "Ensuring the safety of autonomous vehicles (AVs) in long-tail scenarios\nremains a critical challenge, particularly under high uncertainty and complex\nmulti-agent interactions. To address this, we propose RiskNet, an\ninteraction-aware risk forecasting framework, which integrates deterministic\nrisk modeling with probabilistic behavior prediction for comprehensive risk\nassessment. At its core, RiskNet employs a field-theoretic model that captures\ninteractions among ego vehicle, surrounding agents, and infrastructure via\ninteraction fields and force. This model supports multidimensional risk\nevaluation across diverse scenarios (highways, intersections, and roundabouts),\nand shows robustness under high-risk and long-tail settings. To capture the\nbehavioral uncertainty, we incorporate a graph neural network (GNN)-based\ntrajectory prediction module, which learns multi-modal future motion\ndistributions. Coupled with the deterministic risk field, it enables dynamic,\nprobabilistic risk inference across time, enabling proactive safety assessment\nunder uncertainty. Evaluations on the highD, inD, and rounD datasets, spanning\nlane changes, turns, and complex merges, demonstrate that our method\nsignificantly outperforms traditional approaches (e.g., TTC, THW, RSS, NC\nField) in terms of accuracy, responsiveness, and directional sensitivity, while\nmaintaining strong generalization across scenarios. This framework supports\nreal-time, scenario-adaptive risk forecasting and demonstrates strong\ngeneralization across uncertain driving environments. It offers a unified\nfoundation for safety-critical decision-making in long-tail scenarios.", "published": "2025-04-22 02:36:54", "link": "http://arxiv.org/abs/2504.15541v1", "categories": ["cs.RO", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Interpretable Deep Learning for Polar Mechanistic Reaction Prediction", "abstract": "Accurately predicting chemical reactions is essential for driving innovation\nin synthetic chemistry, with broad applications in medicine, manufacturing, and\nagriculture. At the same time, reaction prediction is a complex problem which\ncan be both time-consuming and resource-intensive for chemists to solve. Deep\nlearning methods offer an appealing solution by enabling high-throughput\nreaction prediction. However, many existing models are trained on the US Patent\nOffice dataset and treat reactions as overall transformations: mapping\nreactants directly to products with limited interpretability or mechanistic\ninsight. To address this, we introduce PMechRP (Polar Mechanistic Reaction\nPredictor), a system that trains machine learning models on the PMechDB\ndataset, which represents reactions as polar elementary steps that capture\nelectron flow and mechanistic detail. To further expand model coverage and\nimprove generalization, we augment PMechDB with a diverse set of\ncombinatorially generated reactions. We train and compare a range of machine\nlearning models, including transformer-based, graph-based, and two-step siamese\narchitectures. Our best-performing approach was a hybrid model, which combines\na 5-ensemble of Chemformer models with a two-step Siamese framework to leverage\nthe accuracy of transformer architectures, while filtering away \"alchemical\"\nproducts using the two-step network predictions. For evaluation, we use a test\nsplit of the PMechDB dataset and additionally curate a human benchmark dataset\nconsisting of complete mechanistic pathways extracted from an organic chemistry\ntextbook. Our hybrid model achieves a top-10 accuracy of 94.9% on the PMechDB\ntest set and a target recovery rate of 84.9% on the pathway dataset.", "published": "2025-04-22 02:31:23", "link": "http://arxiv.org/abs/2504.15539v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Federated Latent Factor Learning for Recovering Wireless Sensor Networks Signal with Privacy-Preserving", "abstract": "Wireless Sensor Networks (WSNs) are a cutting-edge domain in the field of\nintelligent sensing. Due to sensor failures and energy-saving strategies, the\ncollected data often have massive missing data, hindering subsequent analysis\nand decision-making. Although Latent Factor Learning (LFL) has been proven\neffective in recovering missing data, it fails to sufficiently consider data\nprivacy protection. To address this issue, this paper innovatively proposes a\nfederated latent factor learning (FLFL) based spatial signal recovery (SSR)\nmodel, named FLFL-SSR. Its main idea is two-fold: 1) it designs a sensor-level\nfederated learning framework, where each sensor uploads only gradient updates\ninstead of raw data to optimize the global model, and 2) it proposes a local\nspatial sharing strategy, allowing sensors within the same spatial region to\nshare their latent feature vectors, capturing spatial correlations and\nenhancing recovery accuracy. Experimental results on two real-world WSNs\ndatasets demonstrate that the proposed model outperforms existing federated\nmethods in terms of recovery performance.", "published": "2025-04-22 02:01:19", "link": "http://arxiv.org/abs/2504.15525v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Few-Shot Vision-Language Action-Incremental Policy Learning", "abstract": "Recently, Transformer-based robotic manipulation methods utilize multi-view\nspatial representations and language instructions to learn robot motion\ntrajectories by leveraging numerous robot demonstrations. However, the\ncollection of robot data is extremely challenging, and existing methods lack\nthe capability for continuous learning on new tasks with only a few\ndemonstrations. In this paper, we formulate these challenges as the Few-Shot\nAction-Incremental Learning (FSAIL) task, and accordingly design a Task-prOmpt\ngraPh evolutIon poliCy (TOPIC) to address these issues. Specifically, to\naddress the data scarcity issue in robotic imitation learning, TOPIC learns\nTask-Specific Prompts (TSP) through the deep interaction of multi-modal\ninformation within few-shot demonstrations, thereby effectively extracting the\ntask-specific discriminative information. On the other hand, to enhance the\ncapability for continual learning on new tasks and mitigate the issue of\ncatastrophic forgetting, TOPIC adopts a Continuous Evolution Strategy (CES).\nCES leverages the intrinsic relationships between tasks to construct a task\nrelation graph, which effectively facilitates the adaptation of new tasks by\nreusing skills learned from previous tasks. TOPIC pioneers few-shot continual\nlearning in the robotic manipulation task, and extensive experimental results\ndemonstrate that TOPIC outperforms state-of-the-art baselines by over 26$\\%$ in\nsuccess rate, significantly enhancing the continual learning capabilities of\nexisting Transformer-based policies.", "published": "2025-04-22 01:30:47", "link": "http://arxiv.org/abs/2504.15517v1", "categories": ["cs.RO", "cs.LG"], "primary_category": "cs.RO"}
{"title": "T2VShield: Model-Agnostic Jailbreak Defense for Text-to-Video Models", "abstract": "The rapid development of generative artificial intelligence has made text to\nvideo models essential for building future multimodal world simulators.\nHowever, these models remain vulnerable to jailbreak attacks, where specially\ncrafted prompts bypass safety mechanisms and lead to the generation of harmful\nor unsafe content. Such vulnerabilities undermine the reliability and security\nof simulation based applications. In this paper, we propose T2VShield, a\ncomprehensive and model agnostic defense framework designed to protect text to\nvideo models from jailbreak threats. Our method systematically analyzes the\ninput, model, and output stages to identify the limitations of existing\ndefenses, including semantic ambiguities in prompts, difficulties in detecting\nmalicious content in dynamic video outputs, and inflexible model centric\nmitigation strategies. T2VShield introduces a prompt rewriting mechanism based\non reasoning and multimodal retrieval to sanitize malicious inputs, along with\na multi scope detection module that captures local and global inconsistencies\nacross time and modalities. The framework does not require access to internal\nmodel parameters and works with both open and closed source systems. Extensive\nexperiments on five platforms show that T2VShield can reduce jailbreak success\nrates by up to 35 percent compared to strong baselines. We further develop a\nhuman centered audiovisual evaluation protocol to assess perceptual safety,\nemphasizing the importance of visual level defense in enhancing the\ntrustworthiness of next generation multimodal simulators.", "published": "2025-04-22 01:18:42", "link": "http://arxiv.org/abs/2504.15512v1", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Hessian Riemannian Flow For Multi-Population Wardrop Equilibrium", "abstract": "In this paper, we address the problem of optimizing flows on generalized\ngraphs that feature multiple entry points and multiple populations, each with\nvarying cost structures. We tackle this problem by considering the\nmulti-population Wardrop equilibrium, defined through variational inequalities.\nWe rigorously analyze the existence and uniqueness of the Wardrop equilibrium.\nFurthermore, we introduce an efficient numerical method to find the solution.\nIn particular, we reformulate the equilibrium problem as a distributed\noptimization problem over subgraphs and introduce a novel Hessian Riemannian\nflow method, a Riemannian-manifold-projected Hessian flow, to efficiently\ncompute a solution. Finally, we demonstrate the effectiveness of our approach\nthrough examples in urban traffic management, including routing for diverse\nvehicle types and strategies for minimizing emissions in congested\nenvironments.", "published": "2025-04-22 16:45:29", "link": "http://arxiv.org/abs/2504.16028v1", "categories": ["eess.SY", "cs.MA", "cs.SY", "math.OC"], "primary_category": "eess.SY"}
{"title": "Trustworthy Decentralized Autonomous Machines: A New Paradigm in Automation Economy", "abstract": "Decentralized Autonomous Machines (DAMs) represent a transformative paradigm\nin automation economy, integrating artificial intelligence (AI), blockchain\ntechnology, and Internet of Things (IoT) devices to create self-governing\neconomic agents participating in Decentralized Physical Infrastructure Networks\n(DePIN). Capable of managing both digital and physical assets and unlike\ntraditional Decentralized Autonomous Organizations (DAOs), DAMs extend autonomy\ninto the physical world, enabling trustless systems for Real and Digital World\nAssets (RDWAs). In this paper, we explore the technological foundations, and\nchallenges of DAMs and argue that DAMs are pivotal in transitioning from\ntrust-based to trustless economic models, offering scalable, transparent, and\nequitable solutions for asset management. The integration of AI-driven\ndecision-making, IoT-enabled operational autonomy, and blockchain-based\ngovernance allows DAMs to decentralize ownership, optimize resource allocation,\nand democratize access to economic opportunities. Therefore, in this research,\nwe highlight the potential of DAMs to address inefficiencies in centralized\nsystems, reduce wealth disparities, and foster a post-labor economy.", "published": "2025-04-22 07:59:46", "link": "http://arxiv.org/abs/2504.15676v1", "categories": ["cs.MA", "cs.CR"], "primary_category": "cs.MA"}
{"title": "Reconstruction of source function in a parabolic equation using partial boundary measurements", "abstract": "In this paper, we present the analytical and numerical study of the\noptimization approach for determining the space-dependent source function in\nthe parabolic inverse source problem using partial boundary measurements. The\nLagrangian approach for the solution of the optimization problem is presented,\nand optimality conditions are derived. The proof of the Fr\\'echet\ndifferentiability of the regularized Tikhonov functional and the existence\nresult for the solution of the inverse source problem are established. A local\nstability estimate for the unknown source term is also presented. The numerical\nexamples justify the theoretical investigations using the conjugate gradient\nmethod (CGM) in 2D and 3D tests with noisy data.", "published": "2025-04-22 17:51:31", "link": "http://arxiv.org/abs/2504.16070v1", "categories": ["math.NA", "cs.NA", "math.OC", "65M06, 65J22, 65K10, 65M32, 65M55, 65N21, 35K05, 35R30"], "primary_category": "math.NA"}
{"title": "Bayesian Parameter Identification in the Landau-de Gennes Theory for Nematic Liquid Crystals", "abstract": "This manuscript establishes a pathway to reconstruct material parameters from\nmeasurements within the Landau-de Gennes model for nematic liquid crystals. We\npresent a Bayesian approach to this inverse problem and analyse its properties\nusing given, simulated data for benchmark problems of a planar bistable nematic\ndevice. In particular, we discuss the accuracy of the Markov chain Monte Carlo\napproximations, confidence intervals and the limits of identifiability.", "published": "2025-04-22 16:49:43", "link": "http://arxiv.org/abs/2504.16029v1", "categories": ["math.NA", "cond-mat.soft", "cs.NA", "math.ST", "stat.TH"], "primary_category": "math.NA"}
{"title": "Interpolation error analysis using a new geometric parameter", "abstract": "This article presents novel proof methods for estimating interpolation\nerrors, predicated on the understanding that one has already studied\nfoundational error analysis using the finite element method.", "published": "2025-04-22 16:20:54", "link": "http://arxiv.org/abs/2504.16012v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Understanding the Role of Covariates in Numerical Reconstructions of Real-World Vehicle-to-Pedestrian Collisions", "abstract": "Traumatic Brain Injuries (TBIs) are a pressing global public health issue,\nimpacting tens of millions of individuals annually. Vulnerable road users\n(VRUs), such as pedestrians, are vastly overrepresented in the worldwide TBI\nstatistics. To evaluate the effectiveness of injury prevention measures,\nresearchers often employ Finite Element (FE) models of the human body to\nvirtually simulate the human response to impact in real-world road traffic\naccident scenarios. However, VRU accidents occur in a highly uncontrolled\nenvironment and, in consequence, there is a large amount of variables\n(covariates), e.g. the vehicle impact speed and VRU body posture, that together\ndictate the injurious outcome of the collision. At the same time, since FE\nanalysis is a computationally heavy task, researchers often need to apply\nextensive simplifications to FE models when attempting to predict real-world\nVRU head trauma. To help researchers make informed decisions when conducting FE\naccident reconstructions, this literature review aims to create an overarching\nsummary of covariates that have been reported influential in literature. The\nreview provides researchers with an overview of variables proven to have an\ninfluence on head injury predictions. The material could potentially be useful\nas a basis for choosing parameters to include when performing sensitivity\nanalyses of car-to-pedestrian impact simulations.", "published": "2025-04-22 14:48:06", "link": "http://arxiv.org/abs/2504.15951v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Toward optimal-scaling DFT: stochastic Hartree theory in the thermodynamic and complete basis set limits at arbitrary temperature", "abstract": "We present the first mathematical analysis of stochastic density functional\ntheory (DFT) in the context of the Hartree approximation. We motivate our\nanalysis via the notion of nearly-optimal or $\\tilde{O}(n)$ scaling with\nrespect to the number $n$ of computational degrees of freedom, independent of\nthe number of electrons, in both the thermodynamic and complete basis set\nlimits. Indeed, the promise of such scaling is the primary motivation for\nstochastic DFT relative to conventional orbital-based approaches, as well as\ndeterministic orbital-free alternatives. We highlight three key targets for\nmathematical attention, which are synthesized in our algorithm and analysis.\nFirst, we identify a particular stochastic estimator for the Hartree potential\nwhose sample complexity is essentially independent of the discretization size.\nSecond, we reformulate the self-consistent field iteration as a stochastic\nmirror descent method where the Fermi-Dirac entropy plays the role of the\nBregman potential, and we prove a nearly discretization-independent bound on\nthe number of iterations needed to reach fixed accuracy. Third, motivated by\nthe estimator, we introduce a novel pole expansion scheme for the square-root\nFermi-Dirac operator, preserving $\\tilde{O}(n)$ cost per mirror descent\niteration even in the complete basis set limit. Combining these ingredients, we\nestablish nearly-optimal scaling in both limits of interest under reasonable\nassumptions on the basis sets chosen for discretization. Extensive numerical\nexperiments on problems with as many as $10^{6}$ degrees of freedom validate\nour algorithm and support the theory of nearly-optimal scaling.", "published": "2025-04-22 11:58:01", "link": "http://arxiv.org/abs/2504.15816v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Multilevel lattice-based kernel approximation for elliptic PDEs with random coefficients", "abstract": "This paper introduces a multilevel kernel-based approximation method to\nestimate efficiently solutions to elliptic partial differential equations\n(PDEs) with periodic random coefficients. Building upon the work of Kaarnioja,\nKazashi, Kuo, Nobile, Sloan (Numer. Math., 2022) on kernel interpolation with\nquasi-Monte Carlo (QMC) lattice point sets, we leverage multilevel techniques\nto enhance computational efficiency while maintaining a given level of\naccuracy. In the function space setting with product-type weight parameters,\nthe single-level approximation can achieve an accuracy of $\\varepsilon>0$ with\ncost $\\mathcal{O}(\\varepsilon^{-\\eta-\\nu-\\theta})$ for positive constants\n$\\eta, \\nu, \\theta $ depending on the rates of convergence associated with\ndimension truncation, kernel approximation, and finite element approximation,\nrespectively. Our multilevel approximation can achieve the same $\\varepsilon$\naccuracy at a reduced cost $\\mathcal{O}(\\varepsilon^{-\\eta-\\max(\\nu,\\theta)})$.\nFull regularity theory and error analysis are provided, followed by numerical\nexperiments that validate the efficacy of the proposed multilevel approximation\nin comparison to the single-level approach.", "published": "2025-04-22 11:49:51", "link": "http://arxiv.org/abs/2504.15810v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Operator Inference for Elliptic Eigenvalue Problems", "abstract": "Eigenvalue problems for elliptic operators play an important role in science\nand engineering applications, where efficient and accurate numerical\ncomputation is essential. In this work, we propose a novel operator inference\napproach for elliptic eigenvalue problems based on neural network\napproximations that directly maps computational domains to their associated\neigenvalues and eigenfunctions. Motivated by existing neural network\narchitectures and the mathematical characteristics of eigenvalue problems, we\nrepresent computational domains as pixelated images and decompose the task into\ntwo subtasks: eigenvalue prediction and eigenfunction prediction. For the\neigenvalue prediction, we design a convolutional neural network (CNN), while\nfor the eigenfunction prediction, we employ a Fourier Neural Operator (FNO).\nAdditionally, we introduce a critical preprocessing module that integrates\ndomain scaling, detailed boundary pixelization, and main-axis alignment. This\npreprocessing step not only simplifies the learning task but also enhances the\nperformance of the neural networks. Finally, we present numerical results to\ndemonstrate the effectiveness of the proposed method.", "published": "2025-04-22 09:25:36", "link": "http://arxiv.org/abs/2504.15733v1", "categories": ["math.NA", "cs.NA", "35J15, 65N25, 68T07, 65T50"], "primary_category": "math.NA"}
{"title": "Bayesian Parameter Estimation for Partially Observed McKean-Vlasov Diffusions Using Multilevel Markov chain Monte Carlo", "abstract": "In this article we consider Bayesian estimation of static parameters for a\nclass of partially observed McKean-Vlasov diffusion processes with\ndiscrete-time observations over a fixed time interval. This problem features\nseveral obstacles to its solution, which include that the posterior density is\nnumerically intractable in continuous-time, even if the transition\nprobabilities are available and even when one uses a time-discretization, the\nposterior still cannot be used by adopting well-known computational methods\nsuch as Markov chain Monte Carlo (MCMC). In this paper we provide a solution to\nthis problem by using new MCMC algorithms which can solve the afore-mentioned\nissues. This MCMC algorithm is extended to use multilevel Monte Carlo (MLMC)\nmethods. We prove convergence bounds on our parameter estimators and show that\nthe MLMC-based MCMC algorithm reduces the computational cost to achieve a mean\nsquare error versus ordinary MCMC by an order of magnitude. We numerically\nillustrate our results on two models.", "published": "2025-04-22 05:06:17", "link": "http://arxiv.org/abs/2504.15588v1", "categories": ["stat.CO", "cs.NA", "math.NA"], "primary_category": "stat.CO"}
{"title": "Minimization of Curve Length through Energy Minimization using Finite Difference and Numerical Integration in Real Coordinate Space", "abstract": "The problem of determining the minimal length is garnering attention in\nvarious fields such as computer vision, robotics, and machine learning. One\nsolution to this problem involves linearly interpolating the solution of a\nnonlinear optimization problem that approximates the curve's energy\nminimization problem using finite differences and numerical integration. This\nmethod tends to be easier to implement compared to others. However, it was\npreviously unknown whether this approach successfully minimizes the curve's\nlength under the Riemannian metric in real coordinate spaces. In this paper, we\nprove that the length of a curve obtained by linear interpolation of the\nsolution to an optimization problem, where the energy of the curve is\napproximated using finite differences and the trapezoidal rule, converges to\nthe minimal curve length at a rate of $1/2$ in terms of the number of points\nused in the numerical integration. Similarly, we prove that when using the\nleft-point rule, the approximated curve's length likewise converges to the\nminimal curve length at a rate of $1/2$ in terms of the number of points used\nin the numerical integration.", "published": "2025-04-22 03:35:08", "link": "http://arxiv.org/abs/2504.15566v1", "categories": ["math.NA", "cs.NA", "math.DG", "math.OC", "65K10(primary), 54-04, 53C22, 58E10, 65N06(secondary)"], "primary_category": "math.NA"}
{"title": "Derivatives of tree tensor networks and its applications in Runge--Kutta methods", "abstract": "Tree tensor networks (TTNs) provide a compact and structured representation\nof high-dimensional data, making them valuable in various areas of\ncomputational mathematics and physics. In this paper, we present a rigorous\nmathematical framework for expressing high-order derivatives of functional\nTTNs, both with or without constraints. Our framework decomposes the total\nderivative of a given TTN into a summation of TTNs, each corresponding to the\npartial derivatives of the original TTN. Using this decomposition, we derive\nthe Taylor expansion of vector-valued functions subject to ordinary\ndifferential equation constraints or algebraic constraints imposed by\nRunge--Kutta (RK) methods. As a concrete application, we employ this framework\nto construct order conditions for RK methods. Due to the intrinsic tensor\nproperties of partial derivatives and the separable tensor structure in RK\nmethods, the Taylor expansion of numerical solutions can be obtained in a\nmanner analogous to that of exact solutions using tensor operators. This\nenables the order conditions of RK methods to be established by directly\ncomparing the Taylor expansions of the exact and numerical solutions,\neliminating the need for mathematical induction. For a given function\n$\\vector{f}$, we derive sharper order conditions that go beyond the classical\nones, enabling the identification of situations where a standard RK scheme of\norder {\\it p} achieves unexpectedly higher convergence order for the particular\nfunction. These results establish new connections between tensor network theory\nand classical numerical methods, potentially opening new avenues for both\nanalytical exploration and practical computation.", "published": "2025-04-22 01:26:51", "link": "http://arxiv.org/abs/2504.15516v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A Line Graph-Based Framework for Identifying Optimal Routing Paths in Decentralized Exchanges", "abstract": "Decentralized exchanges, such as those employing constant product market\nmakers (CPMMs) like Uniswap V2, play a crucial role in the blockchain ecosystem\nby enabling peer-to-peer token swaps without intermediaries. Despite the\nincreasing volume of transactions, there remains limited research on\nidentifying optimal trading paths across multiple DEXs. This paper presents a\nnovel line-graph-based algorithm (LG) designed to efficiently discover\nprofitable trading routes within DEX environments. We benchmark LG against the\nwidely adopted Depth-First Search (DFS) algorithm under a linear routing\nscenario, encompassing platforms such as Uniswap, SushiSwap, and PancakeSwap.\nExperimental results demonstrate that LG consistently identifies trading paths\nthat are as profitable as, or more profitable than, those found by DFS, while\nincurring comparable gas costs. Evaluations on Uniswap V2 token graphs across\ntwo temporal snapshots further validate LG's performance. Although LG exhibits\nexponential runtime growth with respect to graph size in empirical tests, it\nremains viable for practical, real-world use cases. Our findings underscore the\npotential of the LG algorithm for industrial adoption, offering tangible\nbenefits to traders and market participants in the DeFi space.", "published": "2025-04-22 11:45:49", "link": "http://arxiv.org/abs/2504.15809v1", "categories": ["q-fin.CP", "cs.GT"], "primary_category": "q-fin.CP"}
{"title": "Realized Local Volatility Surface", "abstract": "For quantitative trading risk management purposes, we present a novel idea:\nthe realized local volatility surface. Concisely, it stands for the conditional\nexpected volatility when sudden market behaviors of the underlying occur. One\nis able to explore risk management usages by following the orthotical\nDelta-Gamma dynamic hedging framework. The realized local volatility surface\nis, mathematically, a generalized Wiener measure from historical prices. It is\nreconstructed via employing high-frequency trading market data. A\nStick-Breaking Gaussian Mixture Model is fitted via Hamiltonian Monte Carlo,\nproducing a local volatility surface with 95% credible intervals. A practically\nvalidated Bayesian nonparametric estimation workflow. Empirical results on TSLA\nhigh-frequency data illustrate its ability to capture counterfactual\nvolatility. We also discuss its application in improving volatility-based risk\nmanagement.", "published": "2025-04-22 06:35:34", "link": "http://arxiv.org/abs/2504.15626v1", "categories": ["q-fin.RM", "math.OC", "q-fin.CP", "q-fin.TR", "91G70 (Primary) 91G20, 91G60 (Secondary)"], "primary_category": "q-fin.RM"}
{"title": "Asian Basket Spread Options: A New Approximation Based on Stochastic Taylor Expansions", "abstract": "We present closed analytical approximations for the pricing of Asian basket\nspread options under the Black-Scholes model. The formulae are obtained by\nusing a stochastic Taylor expansion around a log-normal proxy model and are\nfound to be highly accurate for Asian and spread options in practice. Unlike\nother approaches, they do not require any numerical integration or root\nsolving.", "published": "2025-04-22 16:19:27", "link": "http://arxiv.org/abs/2504.16011v1", "categories": ["q-fin.PR", "q-fin.MF"], "primary_category": "q-fin.PR"}
{"title": "Modeling and Forecasting Realized Volatility with Multivariate Fractional Brownian Motion", "abstract": "A multivariate fractional Brownian motion (mfBm) with component-wise Hurst\nexponents is used to model and forecast realized volatility. We investigate the\ninterplay between correlation coefficients and Hurst exponents and propose a\nnovel estimation method for all model parameters, establishing consistency and\nasymptotic normality of the estimators. Additionally, we develop a\ntime-reversibility test, which is typically not rejected by real volatility\ndata. When the data-generating process is a time-reversible mfBm, we derive\noptimal forecasting formulae and analyze their properties. A key insight is\nthat an mfBm with different Hurst exponents and non-zero correlations can\nreduce forecasting errors compared to a one-dimensional model. Consistent with\noptimal forecasting theory, out-of-sample forecasts using the time-reversible\nmfBm show improvements over univariate fBm, particularly when the estimated\nHurst exponents differ significantly. Empirical results demonstrate that\nmfBm-based forecasts outperform the (vector) HAR model.", "published": "2025-04-22 15:38:31", "link": "http://arxiv.org/abs/2504.15985v1", "categories": ["q-fin.ST", "62M10, 60G18, 62P05"], "primary_category": "q-fin.ST"}
{"title": "Learning the Spoofability of Limit Order Books With Interpretable Probabilistic Neural Networks", "abstract": "This paper investigates real-time detection of spoofing activity in limit\norder books, focusing on cryptocurrency centralized exchanges. We first\nintroduce novel order flow variables based on multi-scale Hawkes processes that\naccount both for the size and placement distance from current best prices of\nnew limit orders. Using a Level-3 data set, we train a neural network model to\npredict the conditional probability distribution of mid price movements based\non these features. Our empirical analysis highlights the critical role of the\nposting distance of limit orders in the price formation process, showing that\nspoofing detection models that do not take the posting distance into account\nare inadequate to describe the data. Next, we propose a spoofing detection\nframework based on the probabilistic market manipulation gain of a spoofing\nagent and use the previously trained neural network to compute the expected\ngain. Running this algorithm on all submitted limit orders in the period\n2024-12-04 to 2024-12-07, we find that 31% of large orders could spoof the\nmarket. Because of its simple neuronal architecture, our model can be run in\nreal time. This work contributes to enhancing market integrity by providing a\nrobust tool for monitoring and mitigating spoofing in both cryptocurrency\nexchanges and traditional financial markets.", "published": "2025-04-22 13:52:55", "link": "http://arxiv.org/abs/2504.15908v1", "categories": ["q-fin.TR", "q-fin.ST"], "primary_category": "q-fin.TR"}
{"title": "Microstructure and Manipulation: Quantifying Pump-and-Dump Dynamics in Cryptocurrency Markets", "abstract": "Building on our prior threshold-based analysis of six months of Poloniex\ntrading data, we have extended both the temporal span and granularity of our\nstudy by incorporating minute-level OHLCV records for 1021 tokens around each\nconfirmed pump-and-dump event. First, we algorithmically identify the\naccumulation phase, marking the initial and final insider volume spikes, and\nobserve that 70% of pre-event volume transacts within one hour of the pump\nannouncement. Second, we compute conservative lower bounds on insider profits\nunder both a single-point liquidation at 70% of peak and a tranche-based\nstrategy (selling 20% at 50%, 30% at 60%, and 50% at 80% of peak), yielding\nmedian returns above 100% and upper-quartile returns exceeding 2000%. Third, by\nunfolding the full pump structure and integrating social-media verification\n(e.g., Telegram announcements), we confirm numerous additional events that\neluded our initial model. We also categorize schemes into \"pre-accumulation\"\nversus \"on-the-spot\" archetypes-insights that sharpen detection algorithms,\ninform risk assessments, and underpin actionable strategies for real-time\nmarket-integrity enforcement.", "published": "2025-04-22 11:05:31", "link": "http://arxiv.org/abs/2504.15790v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Quantifying Source Speaker Leakage in One-to-One Voice Conversion", "abstract": "Using a multi-accented corpus of parallel utterances for use with commercial\nspeech devices, we present a case study to show that it is possible to quantify\na degree of confidence about a source speaker's identity in the case of\none-to-one voice conversion. Following voice conversion using a HiFi-GAN\nvocoder, we compare information leakage for a range speaker characteristics;\nassuming a \"worst-case\" white-box scenario, we quantify our confidence to\nperform inference and narrow the pool of likely source speakers, reinforcing\nthe regulatory obligation and moral duty that providers of synthetic voices\nhave to ensure the privacy of their speakers' data.", "published": "2025-04-22 12:09:03", "link": "http://arxiv.org/abs/2504.15822v1", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring the User Experience of AI-Assisted Sound Searching Systems for Creative Workflows", "abstract": "Locating the right sound effect efficiently is an important yet challenging\ntopic for audio production. Most current sound-searching systems rely on\npre-annotated audio labels created by humans, which can be time-consuming to\nproduce and prone to inaccuracies, limiting the efficiency of audio production.\nFollowing the recent advancement of contrastive language-audio pre-training\n(CLAP) models, we explore an alternative CLAP-based sound-searching system\n(CLAP-UI) that does not rely on human annotations. To evaluate the\neffectiveness of CLAP-UI, we conducted comparative experiments with a widely\nused sound effect searching platform, the BBC Sound Effect Library. Our study\nevaluates user performance, cognitive load, and satisfaction through\necologically valid tasks based on professional sound-searching workflows. Our\nresult shows that CLAP-UI demonstrated significantly enhanced productivity and\nreduced frustration while maintaining comparable cognitive demands. We also\nqualitatively analyzed the participants' feedback, which offered valuable\nperspectives on the design of future AI-assisted sound search systems.", "published": "2025-04-22 04:20:12", "link": "http://arxiv.org/abs/2504.15575v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Comparative and Measurement-Based Study on Real-Time Network KPI Extraction Methods for 5G and Beyond Applications", "abstract": "Key performance indicators (KPIs), which can be extracted from the\nstandardized interfaces of network equipment defined by current standards,\nconstitute a primary data source that can be leveraged in the development of\nnon-standardized new equipment, architectures, and computational tools. In\nnext-generation technologies, the demand for data has evolved beyond the\nconventional log generation or export capabilities provided by existing\nlicensed network monitoring tools. There is now a growing need to collect such\ndata at specific time intervals and with defined granularities. At this stage,\nthe development of real-time KPI extraction methods and enabling their exchange\nbetween both standardized/commercialized and non-standardized components or\ntools has become increasingly critical. This study presents a comprehensive\nevaluation of three distinct KPI extraction methodologies applied to two\ncommercially available devices. The analysis aims to uncover the strengths,\nweaknesses, and overall efficacy of these approaches under varying conditions,\nand highlights the critical insights into the practical capabilities and\nlimitations. The findings serve as a foundational guide for the seamless\nintegration and robust testing of novel technologies and approaches within\ncommercial telecommunication networks. This work aspires to bridge the gap\nbetween technological innovation and real-world applicability, fostering\nenhanced decision-making in network deployment and optimization.", "published": "2025-04-22 17:04:02", "link": "http://arxiv.org/abs/2504.16039v1", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI"}
{"title": "Rotational ultrasound and photoacoustic tomography of the human body", "abstract": "Imaging the human body's morphological and angiographic information is\nessential for diagnosing, monitoring, and treating medical conditions.\nUltrasonography performs the morphological assessment of the soft tissue based\non acoustic impedance variations, whereas photoacoustic tomography (PAT) can\nvisualize blood vessels based on intrinsic hemoglobin absorption.\nThree-dimensional (3D) panoramic imaging of the vasculature is generally not\npractical in conventional ultrasonography with limited field-of-view (FOV)\nprobes, and PAT does not provide sufficient scattering-based soft tissue\nmorphological contrast. Complementing each other, fast panoramic rotational\nultrasound tomography (RUST) and PAT are integrated for hybrid rotational\nultrasound and photoacoustic tomography (RUS-PAT), which obtains 3D ultrasound\nstructural and PAT angiographic images of the human body quasi-simultaneously.\nThe RUST functionality is achieved in a cost-effective manner using a\nsingle-element ultrasonic transducer for ultrasound transmission and rotating\narc-shaped arrays for 3D panoramic detection. RUST is superior to conventional\nultrasonography, which either has a limited FOV with a linear array or is\nhigh-cost with a hemispherical array that requires both transmission and\nreceiving. By switching the acoustic source to a light source, the system is\nconveniently converted to PAT mode to acquire angiographic images in the same\nregion. Using RUS-PAT, we have successfully imaged the human head, breast,\nhand, and foot with a 10 cm diameter FOV, submillimeter isotropic resolution,\nand 10 s imaging time for each modality. The 3D RUS-PAT is a powerful tool for\nhigh-speed, 3D, dual-contrast imaging of the human body with potential for\nrapid clinical translation.", "published": "2025-04-22 17:02:12", "link": "http://arxiv.org/abs/2504.16036v1", "categories": ["physics.med-ph", "eess.SP", "physics.app-ph"], "primary_category": "physics.med-ph"}
{"title": "A UAV-Aided Digital Twin Framework for IoT Networks with High Accuracy and Synchronization", "abstract": "With the continued growth of its core technologies, including the Internet of\nThings (IoT), artificial intelligence (AI), Big Data and data analytics, and\nedge computing, digital twin (DT) technology has witnessed a significant\nincrease in industrial applications, helping the industry become more\nsustainable, smart, and adaptable. Hence, DT technology has emerged as a\npromising link between the physical and virtual worlds, enabling simulation,\nprediction, and real-time performance optimization. This work aims to explore\nthe development of a high-fidelity digital twin framework, focusing on\nsynchronization and accuracy between physical and digital systems to enhance\ndata-driven decision making. To achieve this, we deploy several stationary UAVs\nin optimized locations to collect data from industrial IoT devices, which were\nused to monitor multiple physical entities and perform computations to evaluate\ntheir status. We consider a practical setup in which multiple IoT devices may\nmonitor a single physical entity, and as a result, the measurements are\ncombined and processed together to determine the status of the physical entity.\nThe resulting status updates are subsequently uploaded from the UAVs to the\nbase station, where the DT resides. In this work, we consider a novel metric\nbased on the Age of Information (AoI), coined as the Age of Digital Twin\n(AoDT), to reflect the status freshness of the digital twin. Factoring AoDT in\nthe problem formulation ensures that the DT reliably mirrors the physical\nsystem with high accuracy and synchronization. We formulate a mixed-integer\nnon-convex program to maximize the total amount of data collected from all IoT\ndevices while ensuring a constrained AoDT. Using successive convex\napproximations, we solve the problem, conduct extensive simulations and compare\nthe results with baseline approaches to demonstrate the effectiveness of the\nproposed solution.", "published": "2025-04-22 15:07:07", "link": "http://arxiv.org/abs/2504.15967v1", "categories": ["cs.ET", "eess.SP"], "primary_category": "cs.ET"}
{"title": "Joint Security-Latency Design for Short Packet-Based Low-Altitude Communications", "abstract": "In this article, a joint security and latency analysis of short packet-based\nlow-altitude communications when the eavesdropper is close to the receiver is\naddressed. To reveal the impacts of the signal-to-noise ratio (SNR) and\nblock-length on latency in communications, we propose a new metric named secure\nlatency (SL) and derive the expressions for the effective secure probability\n(ESP) and the average SL. To minimize the average SL, different transmission\ndesigns are analyzed, in which the optimal solutions of SNR and block-length\nare provided. Numerical results validate our analysis and reveal the trade-off\nbetween reliability and security and the impacts of the block-length, SNR, and\npacket-generating rate on average SL, of which SNR and the block-length account\nfor main factors. In addition, we find that the performance of SL can be\nenhanced by allocating less SNR.", "published": "2025-04-22 06:40:28", "link": "http://arxiv.org/abs/2504.15628v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Element-Grouping Strategy for Intelligent Reflecting Surface: Performance Analysis and Algorithm Optimization", "abstract": "As a revolutionary paradigm for intelligently controlling wireless channels,\nintelligent reflecting surface (IRS) has emerged as a promising technology for\nfuture sixth-generation (6G) wireless communications. While IRS-aided\ncommunication systems can achieve attractive high channel gains, existing\nschemes require plenty of IRS elements to mitigate the ``multiplicative\nfading'' effect in cascaded channels, leading to high complexity for real-time\nbeamforming and high signaling overhead for channel estimation. In this paper,\nthe concept of sustainable intelligent element-grouping IRS (IEG-IRS) is\nproposed to overcome those fundamental bottlenecks. Specifically, based on the\nstatistical channel state information (S-CSI), the proposed grouping strategy\nintelligently pre-divide the IEG-IRS elements into multiple groups based on the\nbeam-domain grouping method, with each group sharing the common reflection\ncoefficient and being optimized in real time using the instantaneous channel\nstate information (I-CSI). Then, we further analyze the asymptotic performance\nof the IEG-IRS to reveal the substantial capacity gain in an extremely\nlarge-scale IRS (XL-IRS) aided single-user single-input single-output (SU-SISO)\nsystem. In particular, when a line-of-sight (LoS) component exists, it\ndemonstrates that the combined cascaded link can be considered as a\n``deterministic virtual LoS'' channel, resulting in a sustainable squared array\ngain achieved by the IEG-IRS. Finally, we formulate a weighted-sum-rate (WSR)\nmaximization problem for an IEG-IRS-aided multiuser multiple-input\nsingle-output (MU-MISO) system and a two-stage algorithm for optimizing the\nbeam-domain grouping strategy and the multi-user active-passive beamforming is\nproposed.", "published": "2025-04-22 01:46:07", "link": "http://arxiv.org/abs/2504.15520v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Learning-Based Two-Way Communications: Algorithmic Framework and Comparative Analysis", "abstract": "Machine learning (ML)-based feedback channel coding has garnered significant\nresearch interest in the past few years. However, there has been limited\nresearch exploring ML approaches in the so-called \"two-way\" setting where two\nusers jointly encode messages and feedback for each other over a shared\nchannel. In this work, we present a general architecture for ML-based two-way\nfeedback coding, and show how several popular one-way schemes can be converted\nto the two-way setting through our framework. We compare such schemes against\ntheir one-way counterparts, revealing error-rate benefits of ML-based two-way\ncoding in certain signal-to-noise ratio (SNR) regimes. We then analyze the\ntradeoffs between error performance and computational overhead for three\nstate-of-the-art neural network coding models instantiated in the two-way\nparadigm.", "published": "2025-04-22 01:22:13", "link": "http://arxiv.org/abs/2504.15514v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
