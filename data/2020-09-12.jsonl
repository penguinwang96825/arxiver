{"title": "Syntax Role for Neural Semantic Role Labeling", "abstract": "Semantic role labeling (SRL) is dedicated to recognizing the semantic\npredicate-argument structure of a sentence. Previous studies in terms of\ntraditional models have shown syntactic information can make remarkable\ncontributions to SRL performance; however, the necessity of syntactic\ninformation was challenged by a few recent neural SRL studies that demonstrate\nimpressive performance without syntactic backbones and suggest that syntax\ninformation becomes much less important for neural semantic role labeling,\nespecially when paired with recent deep neural network and large-scale\npre-trained language models. Despite this notion, the neural SRL field still\nlacks a systematic and full investigation on the relevance of syntactic\ninformation in SRL, for both dependency and both monolingual and multilingual\nsettings. This paper intends to quantify the importance of syntactic\ninformation for neural SRL in the deep learning framework. We introduce three\ntypical SRL frameworks (baselines), sequence-based, tree-based, and\ngraph-based, which are accompanied by two categories of exploiting syntactic\ninformation: syntax pruning-based and syntax feature-based. Experiments are\nconducted on the CoNLL-2005, 2009, and 2012 benchmarks for all languages\navailable, and results show that neural SRL models can still benefit from\nsyntactic information under certain conditions. Furthermore, we show the\nquantitative significance of syntax to neural SRL models together with a\nthorough empirical survey using existing models.", "published": "2020-09-12 07:01:12", "link": "http://arxiv.org/abs/2009.05737v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intent Detection with WikiHow", "abstract": "Modern task-oriented dialog systems need to reliably understand users'\nintents. Intent detection is most challenging when moving to new domains or new\nlanguages, since there is little annotated data. To address this challenge, we\npresent a suite of pretrained intent detection models. Our models are able to\npredict a broad range of intended goals from many actions because they are\ntrained on wikiHow, a comprehensive instructional website. Our models achieve\nstate-of-the-art results on the Snips dataset, the Schema-Guided Dialogue\ndataset, and all 3 languages of the Facebook multilingual dialog datasets. Our\nmodels also demonstrate strong zero- and few-shot performance, reaching over\n75% accuracy using only 100 training examples in all datasets.", "published": "2020-09-12 12:53:47", "link": "http://arxiv.org/abs/2009.05781v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CIA_NITT at WNUT-2020 Task 2: Classification of COVID-19 Tweets Using\n  Pre-trained Language Models", "abstract": "This paper presents our models for WNUT 2020 shared task2. The shared task2\ninvolves identification of COVID-19 related informative tweets. We treat this\nas binary text classification problem and experiment with pre-trained language\nmodels. Our first model which is based on CT-BERT achieves F1-score of 88.7%\nand second model which is an ensemble of CT-BERT, RoBERTa and SVM achieves\nF1-score of 88.52%.", "published": "2020-09-12 12:59:54", "link": "http://arxiv.org/abs/2009.05782v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Machine Reading Comprehension with Contextualized Commonsense\n  Knowledge", "abstract": "In this paper, we aim to extract commonsense knowledge to improve machine\nreading comprehension. We propose to represent relations implicitly by\nsituating structured knowledge in a context instead of relying on a pre-defined\nset of relations, and we call it contextualized knowledge. Each piece of\ncontextualized knowledge consists of a pair of interrelated verbal and\nnonverbal messages extracted from a script and the scene in which they occur as\ncontext to implicitly represent the relation between the verbal and nonverbal\nmessages, which are originally conveyed by different modalities within the\nscript. We propose a two-stage fine-tuning strategy to use the large-scale\nweakly-labeled data based on a single type of contextualized knowledge and\nemploy a teacher-student paradigm to inject multiple types of contextualized\nknowledge into a student machine reader. Experimental results demonstrate that\nour method outperforms a state-of-the-art baseline by a 4.3% improvement in\naccuracy on the machine reading comprehension dataset C^3, wherein most of the\nquestions require unstated prior knowledge.", "published": "2020-09-12 17:20:01", "link": "http://arxiv.org/abs/2009.05831v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Relation Detection for Indonesian Language using Deep Neural Network --\n  Support Vector Machine", "abstract": "Relation Detection is a task to determine whether two entities are related or\nnot. In this paper, we employ neural network to do relation detection between\ntwo named entities for Indonesian Language. We used feature such as word\nembedding, position embedding, POS-Tag embedding, and character embedding. For\nthe model, we divide the model into two parts: Front-part classifier\n(Convolutional layer or LSTM layer) and Back-part classifier (Dense layer or\nSVM). We did grid search method of neural network hyper parameter and SVM. We\nused 6000 Indonesian sentences for training process and 1,125 for testing. The\nbest result is 0.8083 on F1-Score using Convolutional Layer as front-part and\nSVM as back-part.", "published": "2020-09-12 01:45:08", "link": "http://arxiv.org/abs/2009.05698v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Improving Indonesian Text Classification Using Multilingual Language\n  Model", "abstract": "Compared to English, the amount of labeled data for Indonesian text\nclassification tasks is very small. Recently developed multilingual language\nmodels have shown its ability to create multilingual representations\neffectively. This paper investigates the effect of combining English and\nIndonesian data on building Indonesian text classification (e.g., sentiment\nanalysis and hate speech) using multilingual language models. Using the\nfeature-based approach, we observe its performance on various data sizes and\ntotal added English data. The experiment showed that the addition of English\ndata, especially if the amount of Indonesian data is small, improves\nperformance. Using the fine-tuning approach, we further showed its\neffectiveness in utilizing the English language to build Indonesian text\nclassification models.", "published": "2020-09-12 03:16:25", "link": "http://arxiv.org/abs/2009.05713v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Improving Bi-LSTM Performance for Indonesian Sentiment Analysis Using\n  Paragraph Vector", "abstract": "Bidirectional Long Short-Term Memory Network (Bi-LSTM) has shown promising\nperformance in sentiment classification task. It processes inputs as sequence\nof information. Due to this behavior, sentiment predictions by Bi-LSTM were\ninfluenced by words sequence and the first or last phrases of the texts tend to\nhave stronger features than other phrases. Meanwhile, in the problem scope of\nIndonesian sentiment analysis, phrases that express the sentiment of a document\nmight not appear in the first or last part of the document that can lead to\nincorrect sentiment classification. To this end, we propose the using of an\nexisting document representation method called paragraph vector as additional\ninput features for Bi-LSTM. This vector provides information context of the\ndocument for each sequence processing. The paragraph vector is simply\nconcatenated to each word vector of the document. This representation also\nhelps to differentiate ambiguous Indonesian words. Bi-LSTM and paragraph vector\nwere previously used as separate methods. Combining the two methods has shown a\nsignificant performance improvement of Indonesian sentiment analysis model.\nSeveral case studies on testing data showed that the proposed method can handle\nthe sentiment phrases position problem encountered by Bi-LSTM.", "published": "2020-09-12 03:43:30", "link": "http://arxiv.org/abs/2009.05720v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Exploring the Hierarchy in Relation Labels for Scene Graph Generation", "abstract": "By assigning each relationship a single label, current approaches formulate\nthe relationship detection as a classification problem. Under this formulation,\npredicate categories are treated as completely different classes. However,\ndifferent from the object labels where different classes have explicit\nboundaries, predicates usually have overlaps in their semantic meanings. For\nexample, sit\\_on and stand\\_on have common meanings in vertical relationships\nbut different details of how these two objects are vertically placed. In order\nto leverage the inherent structures of the predicate categories, we propose to\nfirst build the language hierarchy and then utilize the Hierarchy Guided\nFeature Learning (HGFL) strategy to learn better region features of both the\ncoarse-grained level and the fine-grained level. Besides, we also propose the\nHierarchy Guided Module (HGM) to utilize the coarse-grained level to guide the\nlearning of fine-grained level features. Experiments show that the proposed\nsimple yet effective method can improve several state-of-the-art baselines by a\nlarge margin (up to $33\\%$ relative gain) in terms of Recall@50 on the task of\nScene Graph Generation in different datasets.", "published": "2020-09-12 17:36:53", "link": "http://arxiv.org/abs/2009.05834v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Fine-tuning Pre-trained Contextual Embeddings for Citation Content\n  Analysis in Scholarly Publication", "abstract": "Citation function and citation sentiment are two essential aspects of\ncitation content analysis (CCA), which are useful for influence analysis, the\nrecommendation of scientific publications. However, existing studies are mostly\ntraditional machine learning methods, although deep learning techniques have\nalso been explored, the improvement of the performance seems not significant\ndue to insufficient training data, which brings difficulties to applications.\nIn this paper, we propose to fine-tune pre-trained contextual embeddings\nULMFiT, BERT, and XLNet for the task. Experiments on three public datasets show\nthat our strategy outperforms all the baselines in terms of the F1 score. For\ncitation function identification, the XLNet model achieves 87.2%, 86.90%, and\n81.6% on DFKI, UMICH, and TKDE2019 datasets respectively, while it achieves\n91.72% and 91.56% on DFKI and UMICH in term of citation sentiment\nidentification. Our method can be used to enhance the influence analysis of\nscholars and scholarly publications.", "published": "2020-09-12 17:46:24", "link": "http://arxiv.org/abs/2009.05836v1", "categories": ["cs.CL", "cs.AI", "I.2.1"], "primary_category": "cs.CL"}
{"title": "Country Image in COVID-19 Pandemic: A Case Study of China", "abstract": "Country image has a profound influence on international relations and\neconomic development. In the worldwide outbreak of COVID-19, countries and\ntheir people display different reactions, resulting in diverse perceived images\namong foreign public. Therefore, in this study, we take China as a specific and\ntypical case and investigate its image with aspect-based sentiment analysis on\na large-scale Twitter dataset. To our knowledge, this is the first study to\nexplore country image in such a fine-grained way. To perform the analysis, we\nfirst build a manually-labeled Twitter dataset with aspect-level sentiment\nannotations. Afterward, we conduct the aspect-based sentiment analysis with\nBERT to explore the image of China. We discover an overall sentiment change\nfrom non-negative to negative in the general public, and explain it with the\nincreasing mentions of negative ideology-related aspects and decreasing\nmentions of non-negative fact-based aspects. Further investigations into\ndifferent groups of Twitter users, including U.S. Congress members, English\nmedia, and social bots, reveal different patterns in their attitudes toward\nChina. This study provides a deeper understanding of the changing image of\nChina in COVID-19 pandemic. Our research also demonstrates how aspect-based\nsentiment analysis can be applied in social science researches to deliver\nvaluable insights.", "published": "2020-09-12 15:54:51", "link": "http://arxiv.org/abs/2009.05817v1", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Visual-speech Synthesis of Exaggerated Corrective Feedback", "abstract": "To provide more discriminative feedback for the second language (L2) learners\nto better identify their mispronunciation, we propose a method for exaggerated\nvisual-speech feedback in computer-assisted pronunciation training (CAPT). The\nspeech exaggeration is realized by an emphatic speech generation neural network\nbased on Tacotron, while the visual exaggeration is accomplished by ADC Viseme\nBlending, namely increasing Amplitude of movement, extending the phone's\nDuration and enhancing the color Contrast. User studies show that exaggerated\nfeedback outperforms non-exaggerated version on helping learners with\npronunciation identification and pronunciation improvement.", "published": "2020-09-12 08:37:22", "link": "http://arxiv.org/abs/2009.05748v2", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
