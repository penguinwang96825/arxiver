{"title": "Coupling Artificial Neurons in BERT and Biological Neurons in the Human\n  Brain", "abstract": "Linking computational natural language processing (NLP) models and neural\nresponses to language in the human brain on the one hand facilitates the effort\ntowards disentangling the neural representations underpinning language\nperception, on the other hand provides neurolinguistics evidence to evaluate\nand improve NLP models. Mappings of an NLP model's representations of and the\nbrain activities evoked by linguistic input are typically deployed to reveal\nthis symbiosis. However, two critical problems limit its advancement: 1) The\nmodel's representations (artificial neurons, ANs) rely on layer-level\nembeddings and thus lack fine-granularity; 2) The brain activities (biological\nneurons, BNs) are limited to neural recordings of isolated cortical unit (i.e.,\nvoxel/region) and thus lack integrations and interactions among brain\nfunctions. To address those problems, in this study, we 1) define ANs with\nfine-granularity in transformer-based NLP models (BERT in this study) and\nmeasure their temporal activations to input text sequences; 2) define BNs as\nfunctional brain networks (FBNs) extracted from functional magnetic resonance\nimaging (fMRI) data to capture functional interactions in the brain; 3) couple\nANs and BNs by maximizing the synchronization of their temporal activations.\nOur experimental results demonstrate 1) The activations of ANs and BNs are\nsignificantly synchronized; 2) the ANs carry meaningful linguistic/semantic\ninformation and anchor to their BN signatures; 3) the anchored BNs are\ninterpretable in a neurolinguistic context. Overall, our study introduces a\nnovel, general, and effective framework to link transformer-based NLP models\nand neural activities in response to language and may provide novel insights\nfor future studies such as brain-inspired evaluation and development of NLP\nmodels.", "published": "2023-03-27 01:41:48", "link": "http://arxiv.org/abs/2303.14871v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unified Text Structuralization with Instruction-tuned Language Models", "abstract": "Text structuralization is one of the important fields of natural language\nprocessing (NLP) consists of information extraction (IE) and structure\nformalization. However, current studies of text structuralization suffer from a\nshortage of manually annotated high-quality datasets from different domains and\nlanguages, which require specialized professional knowledge. In addition, most\nIE methods are designed for a specific type of structured data, e.g., entities,\nrelations, and events, making them hard to generalize to others. In this work,\nwe propose a simple and efficient approach to instruct large language model\n(LLM) to extract a variety of structures from texts. More concretely, we add a\nprefix and a suffix instruction to indicate the desired IE task and structure\ntype, respectively, before feeding the text into a LLM. Experiments on two LLMs\nshow that this approach can enable language models to perform comparable with\nother state-of-the-art methods on datasets of a variety of languages and\nknowledge, and can generalize to other IE sub-tasks via changing the content of\ninstruction. Another benefit of our approach is that it can help researchers to\nbuild datasets in low-source and domain-specific scenarios, e.g., fields in\nfinance and law, with low cost.", "published": "2023-03-27 07:39:05", "link": "http://arxiv.org/abs/2303.14956v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Variation and Instability in Dialect-Based Embedding Spaces", "abstract": "This paper measures variation in embedding spaces which have been trained on\ndifferent regional varieties of English while controlling for instability in\nthe embeddings. While previous work has shown that it is possible to\ndistinguish between similar varieties of a language, this paper experiments\nwith two follow-up questions: First, does the variety represented in the\ntraining data systematically influence the resulting embedding space after\ntraining? This paper shows that differences in embeddings across varieties are\nsignificantly higher than baseline instability. Second, is such dialect-based\nvariation spread equally throughout the lexicon? This paper shows that specific\nparts of the lexicon are particularly subject to variation. Taken together,\nthese experiments confirm that embedding spaces are significantly influenced by\nthe dialect represented in the training data. This finding implies that there\nis semantic variation across dialects, in addition to previously-studied\nlexical and syntactic variation.", "published": "2023-03-27 07:53:23", "link": "http://arxiv.org/abs/2303.14963v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Diverse Role-Players for Summarization\n  Evaluation", "abstract": "Text summarization has a wide range of applications in many scenarios. The\nevaluation of the quality of the generated text is a complex problem. A big\nchallenge to language evaluation is that there is a clear divergence between\nexisting metrics and human evaluation. A document summary's quality can be\nassessed by human annotators on various criteria, both objective ones like\ngrammar and correctness, and subjective ones like informativeness,\nsuccinctness, and appeal. Most of the automatic evaluation methods like\nBLUE/ROUGE may be not able to adequately capture the above dimensions. In this\npaper, we propose a new evaluation framework based on LLMs, which provides a\ncomprehensive evaluation framework by comparing generated text and reference\ntext from both objective and subjective aspects. First, we propose to model\nobjective and subjective dimensions of generated text based on roleplayers\nprompting mechanism. Furthermore, we introduce a context-based prompting\nmechanism that is able to generate dynamic roleplayer profiles based on input\ncontext. Finally, we design a multi-roleplayer prompting technology based on\nbatch prompting and integrate multiple outputs into the final evaluation\nresults. Experimental results on three real datasets for summarization show\nthat our model is highly competitive and has a very high consistency with human\nannotators.", "published": "2023-03-27 10:40:59", "link": "http://arxiv.org/abs/2303.15078v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Information Extraction Study: Take In Mind the Tokenization!", "abstract": "Current research on the advantages and trade-offs of using characters,\ninstead of tokenized text, as input for deep learning models, has evolved\nsubstantially. New token-free models remove the traditional tokenization step;\nhowever, their efficiency remains unclear. Moreover, the effect of tokenization\nis relatively unexplored in sequence tagging tasks. To this end, we investigate\nthe impact of tokenization when extracting information from documents and\npresent a comparative study and analysis of subword-based and character-based\nmodels. Specifically, we study Information Extraction (IE) from biomedical\ntexts. The main outcome is twofold: tokenization patterns can introduce\ninductive bias that results in state-of-the-art performance, and the\ncharacter-based models produce promising results; thus, transitioning to\ntoken-free IE models is feasible.", "published": "2023-03-27 11:08:35", "link": "http://arxiv.org/abs/2303.15100v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An ontology-aided, natural language-based approach for multi-constraint\n  BIM model querying", "abstract": "Being able to efficiently retrieve the required building information is\ncritical for construction project stakeholders to carry out their engineering\nand management activities. Natural language interface (NLI) systems are\nemerging as a time and cost-effective way to query Building Information Models\n(BIMs). However, the existing methods cannot logically combine different\nconstraints to perform fine-grained queries, dampening the usability of natural\nlanguage (NL)-based BIM queries. This paper presents a novel ontology-aided\nsemantic parser to automatically map natural language queries (NLQs) that\ncontain different attribute and relational constraints into computer-readable\ncodes for querying complex BIM models. First, a modular ontology was developed\nto represent NL expressions of Industry Foundation Classes (IFC) concepts and\nrelationships, and was then populated with entities from target BIM models to\nassimilate project-specific information. Hereafter, the ontology-aided semantic\nparser progressively extracts concepts, relationships, and value restrictions\nfrom NLQs to fully identify constraint conditions, resulting in standard SPARQL\nqueries with reasoning rules to successfully retrieve IFC-based BIM models. The\napproach was evaluated based on 225 NLQs collected from BIM users, with a 91%\naccuracy rate. Finally, a case study about the design-checking of a real-world\nresidential building demonstrates the practical value of the proposed approach\nin the construction industry.", "published": "2023-03-27 11:35:40", "link": "http://arxiv.org/abs/2303.15116v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Causal schema induction for knowledge discovery", "abstract": "Making sense of familiar yet new situations typically involves making\ngeneralizations about causal schemas, stories that help humans reason about\nevent sequences. Reasoning about events includes identifying cause and effect\nrelations shared across event instances, a process we refer to as causal schema\ninduction. Statistical schema induction systems may leverage structural\nknowledge encoded in discourse or the causal graphs associated with event\nmeaning, however resources to study such causal structure are few in number and\nlimited in size. In this work, we investigate how to apply schema induction\nmodels to the task of knowledge discovery for enhanced search of\nEnglish-language news texts. To tackle the problem of data scarcity, we present\nTorquestra, a manually curated dataset of text-graph-schema units integrating\ntemporal, event, and causal structures. We benchmark our dataset on three\nknowledge discovery tasks, building and evaluating models for each. Results\nshow that systems that harness causal structure are effective at identifying\ntexts sharing similar causal meaning components rather than relying on lexical\ncues alone. We make our dataset and models available for research purposes.", "published": "2023-03-27 16:55:49", "link": "http://arxiv.org/abs/2303.15381v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KPEval: Towards Fine-Grained Semantic-Based Keyphrase Evaluation", "abstract": "Despite the significant advancements in keyphrase extraction and keyphrase\ngeneration methods, the predominant approach for evaluation mainly relies on\nexact matching with human references. This scheme fails to recognize systems\nthat generate keyphrases semantically equivalent to the references or diverse\nkeyphrases that carry practical utility. To better assess the capability of\nkeyphrase systems, we propose KPEval, a comprehensive evaluation framework\nconsisting of four critical aspects: reference agreement, faithfulness,\ndiversity, and utility. For each aspect, we design semantic-based metrics to\nreflect the evaluation objectives. Meta-evaluation studies demonstrate that our\nevaluation strategy correlates better with human preferences compared to a\nrange of previously proposed metrics. Using KPEval, we re-evaluate 23 keyphrase\nsystems and discover that (1) established model comparison results have\nblind-spots especially when considering reference-free evaluation; (2) large\nlanguage models are underestimated by prior evaluation works; and (3) there is\nno single best model that can excel in all the aspects.", "published": "2023-03-27 17:45:38", "link": "http://arxiv.org/abs/2303.15422v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistically Informed ChatGPT Prompts to Enhance Japanese-Chinese\n  Machine Translation: A Case Study on Attributive Clauses", "abstract": "In the field of Japanese-Chinese translation linguistics, the issue of\ncorrectly translating attributive clauses has persistently proven to be\nchallenging. Present-day machine translation tools often fail to accurately\ntranslate attributive clauses from Japanese to Chinese. In light of this, this\npaper investigates the linguistic problem underlying such difficulties, namely\nhow does the semantic role of the modified noun affect the selection of\ntranslation patterns for attributive clauses, from a linguistic perspective. To\nad-dress these difficulties, a pre-edit scheme is proposed, which aims to\nenhance the accuracy of translation. Furthermore, we propose a novel two-step\nprompt strategy, which combines this pre-edit scheme with ChatGPT, currently\nthe most widely used large language model. This prompt strategy is capable of\noptimizing translation input in zero-shot scenarios and has been demonstrated\nto improve the average translation accuracy score by over 35%.", "published": "2023-03-27 20:33:40", "link": "http://arxiv.org/abs/2303.15587v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatGPT as a Factual Inconsistency Evaluator for Text Summarization", "abstract": "The performance of text summarization has been greatly boosted by pre-trained\nlanguage models. A main concern of existing methods is that most generated\nsummaries are not factually inconsistent with their source documents. To\nalleviate the problem, many efforts have focused on developing effective\nfactuality evaluation metrics based on natural language inference, question\nanswering, and syntactic dependency et al. However, these approaches are\nlimited by either their high computational complexity or the uncertainty\nintroduced by multi-component pipelines, resulting in only partial agreement\nwith human judgement. Most recently, large language models(LLMs) have shown\nexcellent performance in not only text generation but also language\ncomprehension. In this paper, we particularly explore ChatGPT's ability to\nevaluate factual inconsistency under a zero-shot setting by examining it on\nboth coarse-grained and fine-grained evaluation tasks including binary\nentailment inference, summary ranking, and consistency rating. Experimental\nresults indicate that ChatGPT generally outperforms previous evaluation metrics\nacross the three tasks, indicating its great potential for factual\ninconsistency evaluation. However, a closer inspection of ChatGPT's output\nreveals certain limitations including its preference for more lexically similar\ncandidates, false reasoning, and inadequate understanding of instructions.", "published": "2023-03-27 22:30:39", "link": "http://arxiv.org/abs/2303.15621v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scaling Pre-trained Language Models to Deeper via Parameter-efficient\n  Architecture", "abstract": "In this paper, we propose a highly parameter-efficient approach to scaling\npre-trained language models (PLMs) to a deeper model depth. Unlike prior work\nthat shares all parameters or uses extra blocks, we design a more capable\nparameter-sharing architecture based on matrix product operator (MPO). MPO\ndecomposition can reorganize and factorize the information of a parameter\nmatrix into two parts: the major part that contains the major information\n(central tensor) and the supplementary part that only has a small proportion of\nparameters (auxiliary tensors). Based on such a decomposition, our architecture\nshares the central tensor across all layers for reducing the model size and\nmeanwhile keeps layer-specific auxiliary tensors (also using adapters) for\nenhancing the adaptation flexibility. To improve the model training, we further\npropose a stable initialization algorithm tailored for the MPO-based\narchitecture. Extensive experiments have demonstrated the effectiveness of our\nproposed model in reducing the model size and achieving highly competitive\nperformance.", "published": "2023-03-27 02:34:09", "link": "http://arxiv.org/abs/2303.16753v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meeting Action Item Detection with Regularized Context Modeling", "abstract": "Meetings are increasingly important for collaborations. Action items in\nmeeting transcripts are crucial for managing post-meeting to-do tasks, which\nusually are summarized laboriously. The Action Item Detection task aims to\nautomatically detect meeting content associated with action items. However,\ndatasets manually annotated with action item detection labels are scarce and in\nsmall scale. We construct and release the first Chinese meeting corpus with\nmanual action item annotations. In addition, we propose a Context-Drop approach\nto utilize both local and global contexts by contrastive learning, and achieve\nbetter accuracy and robustness for action item detection. We also propose a\nLightweight Model Ensemble method to exploit different pre-trained models.\nExperimental results on our Chinese meeting corpus and the English AMI corpus\ndemonstrate the effectiveness of the proposed approaches.", "published": "2023-03-27 03:27:08", "link": "http://arxiv.org/abs/2303.16763v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its\n  Applications, Advantages, Limitations, and Future Directions in Natural\n  Language Processing", "abstract": "Large language models, pivotal in artificial intelligence, find diverse\napplications. ChatGPT (Chat Generative Pre-trained Transformer), an OpenAI\ncreation, stands out as a widely adopted, powerful tool. It excels in chatbots,\ncontent generation, language translation, recommendations, and medical\napplications, due to its ability to generate human-like responses, comprehend\nnatural language, and adapt contextually. Its versatility and accuracy make it\na potent force in natural language processing (NLP). Despite successes, ChatGPT\nhas limitations, including biased responses and potential reinforcement of\nharmful language patterns. This article offers a comprehensive overview of\nChatGPT, detailing its applications, advantages, and limitations. It also\ndescribes the main advancements from GPT-3 to GPT-4 Omni, comparing them with\nother LLMs like LLaMA 3, Gemini and Deepseek. The paper underscores the ethical\nimperative when utilizing this robust tool in practical settings. Furthermore,\nit contributes to ongoing discussions on artificial intelligence's impact on\nvision and NLP domains, providing insights into prompt engineering techniques.", "published": "2023-03-27 21:27:58", "link": "http://arxiv.org/abs/2304.02017v13", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Pretrained Language Models for Solving Tabular Prediction\n  Problems in the Electronic Health Record", "abstract": "We propose an approach for adapting the DeBERTa model for electronic health\nrecord (EHR) tasks using domain adaptation. We pretrain a small DeBERTa model\non a dataset consisting of MIMIC-III discharge summaries, clinical notes,\nradiology reports, and PubMed abstracts. We compare this model's performance\nwith a DeBERTa model pre-trained on clinical texts from our institutional EHR\n(MeDeBERTa) and an XGBoost model. We evaluate performance on three benchmark\ntasks for emergency department outcomes using the MIMIC-IV-ED dataset. We\npreprocess the data to convert it into text format and generate four versions\nof the original datasets to compare data processing and data inclusion. The\nresults show that our proposed approach outperforms the alternative models on\ntwo of three tasks (p<0.001) and matches performance on the third task, with\nthe use of descriptive columns improving performance over the original column\nnames.", "published": "2023-03-27 05:34:19", "link": "http://arxiv.org/abs/2303.14920v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TabIQA: Table Questions Answering on Business Document Images", "abstract": "Table answering questions from business documents has many challenges that\nrequire understanding tabular structures, cross-document referencing, and\nadditional numeric computations beyond simple search queries. This paper\nintroduces a novel pipeline, named TabIQA, to answer questions about business\ndocument images. TabIQA combines state-of-the-art deep learning techniques 1)\nto extract table content and structural information from images and 2) to\nanswer various questions related to numerical data, text-based information, and\ncomplex queries from structured tables. The evaluation results on VQAonBD 2023\ndataset demonstrate the effectiveness of TabIQA in achieving promising\nperformance in answering table-related questions. The TabIQA repository is\navailable at https://github.com/phucty/itabqa.", "published": "2023-03-27 06:31:21", "link": "http://arxiv.org/abs/2303.14935v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving Contextualized Topic Models with Negative Sampling", "abstract": "Topic modeling has emerged as a dominant method for exploring large document\ncollections. Recent approaches to topic modeling use large contextualized\nlanguage models and variational autoencoders. In this paper, we propose a\nnegative sampling mechanism for a contextualized topic model to improve the\nquality of the generated topics. In particular, during model training, we\nperturb the generated document-topic vector and use a triplet loss to encourage\nthe document reconstructed from the correct document-topic vector to be similar\nto the input document and dissimilar to the document reconstructed from the\nperturbed vector. Experiments for different topic counts on three publicly\navailable benchmark datasets show that in most cases, our approach leads to an\nincrease in topic coherence over that of the baselines. Our model also achieves\nvery high topic diversity.", "published": "2023-03-27 07:28:46", "link": "http://arxiv.org/abs/2303.14951v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "unarXive 2022: All arXiv Publications Pre-Processed for NLP, Including\n  Structured Full-Text and Citation Network", "abstract": "Large-scale data sets on scholarly publications are the basis for a variety\nof bibliometric analyses and natural language processing (NLP) applications.\nEspecially data sets derived from publication's full-text have recently gained\nattention. While several such data sets already exist, we see key shortcomings\nin terms of their domain and time coverage, citation network completeness, and\nrepresentation of full-text content. To address these points, we propose a new\nversion of the data set unarXive. We base our data processing pipeline and\noutput format on two existing data sets, and improve on each of them. Our\nresulting data set comprises 1.9 M publications spanning multiple disciplines\nand 32 years. It furthermore has a more complete citation network than its\npredecessors and retains a richer representation of document structure as well\nas non-textual publication content such as mathematical notation. In addition\nto the data set, we provide ready-to-use training/test data for citation\nrecommendation and IMRaD classification. All data and source code is publicly\navailable at https://github.com/IllDepence/unarXive.", "published": "2023-03-27 07:40:59", "link": "http://arxiv.org/abs/2303.14957v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "InterviewBot: Real-Time End-to-End Dialogue System to Interview Students\n  for College Admission", "abstract": "We present the InterviewBot that dynamically integrates conversation history\nand customized topics into a coherent embedding space to conduct 10 mins\nhybrid-domain (open and closed) conversations with foreign students applying to\nU.S. colleges for assessing their academic and cultural readiness. To build a\nneural-based end-to-end dialogue model, 7,361 audio recordings of\nhuman-to-human interviews are automatically transcribed, where 440 are manually\ncorrected for finetuning and evaluation. To overcome the input/output size\nlimit of a transformer-based encoder-decoder model, two new methods are\nproposed, context attention and topic storing, allowing the model to make\nrelevant and consistent interactions. Our final model is tested both\nstatistically by comparing its responses to the interview data and dynamically\nby inviting professional interviewers and various students to interact with it\nin real-time, finding it highly satisfactory in fluency and context awareness.", "published": "2023-03-27 09:46:56", "link": "http://arxiv.org/abs/2303.15049v3", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks", "abstract": "Many NLP applications require manual data annotations for a variety of tasks,\nnotably to train classifiers or evaluate the performance of unsupervised\nmodels. Depending on the size and degree of complexity, the tasks may be\nconducted by crowd-workers on platforms such as MTurk as well as trained\nannotators, such as research assistants. Using a sample of 2,382 tweets, we\ndemonstrate that ChatGPT outperforms crowd-workers for several annotation\ntasks, including relevance, stance, topics, and frames detection. Specifically,\nthe zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of\nfive tasks, while ChatGPT's intercoder agreement exceeds that of both\ncrowd-workers and trained annotators for all tasks. Moreover, the\nper-annotation cost of ChatGPT is less than $0.003 -- about twenty times\ncheaper than MTurk. These results show the potential of large language models\nto drastically increase the efficiency of text classification.", "published": "2023-03-27 09:59:48", "link": "http://arxiv.org/abs/2303.15056v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Beyond Toxic: Toxicity Detection Datasets are Not Enough for Brand\n  Safety", "abstract": "The rapid growth in user generated content on social media has resulted in a\nsignificant rise in demand for automated content moderation. Various methods\nand frameworks have been proposed for the tasks of hate speech detection and\ntoxic comment classification. In this work, we combine common datasets to\nextend these tasks to brand safety. Brand safety aims to protect commercial\nbranding by identifying contexts where advertisements should not appear and\ncovers not only toxicity, but also other potentially harmful content. As these\ndatasets contain different label sets, we approach the overall problem as a\nbinary classification task. We demonstrate the need for building brand safety\nspecific datasets via the application of common toxicity detection datasets to\na subset of brand safety and empirically analyze the effects of weighted\nsampling strategies in text classification.", "published": "2023-03-27 11:29:09", "link": "http://arxiv.org/abs/2303.15110v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LMCanvas: Object-Oriented Interaction to Personalize Large Language\n  Model-Powered Writing Environments", "abstract": "Large language models (LLMs) can enhance writing by automating or supporting\nspecific tasks in writers' workflows (e.g., paraphrasing, creating analogies).\nLeveraging this capability, a collection of interfaces have been developed that\nprovide LLM-powered tools for specific writing tasks. However, these interfaces\nprovide limited support for writers to create personal tools for their own\nunique tasks, and may not comprehensively fulfill a writer's needs -- requiring\nthem to continuously switch between interfaces during writing. In this work, we\nenvision LMCanvas, an interface that enables writers to create their own\nLLM-powered writing tools and arrange their personal writing environment by\ninteracting with \"blocks\" in a canvas. In this interface, users can create text\nblocks to encapsulate writing and LLM prompts, model blocks for model parameter\nconfigurations, and connect these to create pipeline blocks that output\ngenerations. In this workshop paper, we discuss the design for LMCanvas and our\nplans to develop this concept.", "published": "2023-03-27 11:56:26", "link": "http://arxiv.org/abs/2303.15125v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Evaluating self-attention interpretability through human-grounded\n  experimental protocol", "abstract": "Attention mechanisms have played a crucial role in the development of complex\narchitectures such as Transformers in natural language processing. However,\nTransformers remain hard to interpret and are considered as black-boxes. This\npaper aims to assess how attention coefficients from Transformers can help in\nproviding interpretability. A new attention-based interpretability method\ncalled CLaSsification-Attention (CLS-A) is proposed. CLS-A computes an\ninterpretability score for each word based on the attention coefficient\ndistribution related to the part specific to the classification task within the\nTransformer architecture. A human-grounded experiment is conducted to evaluate\nand compare CLS-A to other interpretability methods. The experimental protocol\nrelies on the capacity of an interpretability method to provide explanation in\nline with human reasoning. Experiment design includes measuring reaction times\nand correct response rates by human subjects. CLS-A performs comparably to\nusual interpretability methods regarding average participant reaction time and\naccuracy. The lower computational cost of CLS-A compared to other\ninterpretability methods and its availability by design within the classifier\nmake it particularly interesting. Data analysis also highlights the link\nbetween the probability score of a classifier prediction and adequate\nexplanations. Finally, our work confirms the relevancy of the use of CLS-A and\nshows to which extent self-attention contains rich information to explain\nTransformer classifiers.", "published": "2023-03-27 13:26:02", "link": "http://arxiv.org/abs/2303.15190v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CoCon: A Data Set on Combined Contextualized Research Artifact Use", "abstract": "In the wake of information overload in academia, methodologies and systems\nfor search, recommendation, and prediction to aid researchers in identifying\nrelevant research are actively studied and developed. Existing work, however,\nis limited in terms of granularity, focusing only on the level of papers or a\nsingle type of artifact, such as data sets. To enable more holistic analyses\nand systems dealing with academic publications and their content, we propose\nCoCon, a large scholarly data set reflecting the combined use of research\nartifacts, contextualized in academic publications' full-text. Our data set\ncomprises 35 k artifacts (data sets, methods, models, and tasks) and 340 k\npublications. We additionally formalize a link prediction task for \"combined\nresearch artifact use prediction\" and provide code to utilize analyses of and\nthe development of ML applications on our data. All data and code is publicly\navailable at https://github.com/IllDepence/contextgraph.", "published": "2023-03-27 13:29:09", "link": "http://arxiv.org/abs/2303.15193v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "TextMI: Textualize Multimodal Information for Integrating Non-verbal\n  Cues in Pre-trained Language Models", "abstract": "Pre-trained large language models have recently achieved ground-breaking\nperformance in a wide variety of language understanding tasks. However, the\nsame model can not be applied to multimodal behavior understanding tasks (e.g.,\nvideo sentiment/humor detection) unless non-verbal features (e.g., acoustic and\nvisual) can be integrated with language. Jointly modeling multiple modalities\nsignificantly increases the model complexity, and makes the training process\ndata-hungry. While an enormous amount of text data is available via the web,\ncollecting large-scale multimodal behavioral video datasets is extremely\nexpensive, both in terms of time and money. In this paper, we investigate\nwhether large language models alone can successfully incorporate non-verbal\ninformation when they are presented in textual form. We present a way to\nconvert the acoustic and visual information into corresponding textual\ndescriptions and concatenate them with the spoken text. We feed this augmented\ninput to a pre-trained BERT model and fine-tune it on three downstream\nmultimodal tasks: sentiment, humor, and sarcasm detection. Our approach,\nTextMI, significantly reduces model complexity, adds interpretability to the\nmodel's decision, and can be applied for a diverse set of tasks while achieving\nsuperior (multimodal sarcasm detection) or near SOTA (multimodal sentiment\nanalysis and multimodal humor detection) performance. We propose TextMI as a\ngeneral, competitive baseline for multimodal behavioral analysis tasks,\nparticularly in a low-resource setting.", "published": "2023-03-27 17:54:32", "link": "http://arxiv.org/abs/2303.15430v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Typhoon: Towards an Effective Task-Specific Masking Strategy for\n  Pre-trained Language Models", "abstract": "Through exploiting a high level of parallelism enabled by graphics processing\nunits, transformer architectures have enabled tremendous strides forward in the\nfield of natural language processing. In a traditional masked language model,\nspecial MASK tokens are used to prompt our model to gather contextual\ninformation from surrounding words to restore originally hidden information. In\nthis paper, we explore a task-specific masking framework for pre-trained large\nlanguage models that enables superior performance on particular downstream\ntasks on the datasets in the GLUE benchmark. We develop our own masking\nalgorithm, Typhoon, based on token input gradients, and compare this with other\nstandard baselines. We find that Typhoon offers performance competitive with\nwhole-word masking on the MRPC dataset. Our implementation can be found in a\npublic Github Repository.", "published": "2023-03-27 22:27:23", "link": "http://arxiv.org/abs/2303.15619v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mutually-paced Knowledge Distillation for Cross-lingual Temporal\n  Knowledge Graph Reasoning", "abstract": "This paper investigates cross-lingual temporal knowledge graph reasoning\nproblem, which aims to facilitate reasoning on Temporal Knowledge Graphs (TKGs)\nin low-resource languages by transfering knowledge from TKGs in high-resource\nones. The cross-lingual distillation ability across TKGs becomes increasingly\ncrucial, in light of the unsatisfying performance of existing reasoning methods\non those severely incomplete TKGs, especially in low-resource languages.\nHowever, it poses tremendous challenges in two aspects. First, the\ncross-lingual alignments, which serve as bridges for knowledge transfer, are\nusually too scarce to transfer sufficient knowledge between two TKGs. Second,\ntemporal knowledge discrepancy of the aligned entities, especially when\nalignments are unreliable, can mislead the knowledge distillation process. We\ncorrespondingly propose a mutually-paced knowledge distillation model MP-KD,\nwhere a teacher network trained on a source TKG can guide the training of a\nstudent network on target TKGs with an alignment module. Concretely, to deal\nwith the scarcity issue, MP-KD generates pseudo alignments between TKGs based\non the temporal information extracted by our representation module. To maximize\nthe efficacy of knowledge transfer and control the noise caused by the temporal\nknowledge discrepancy, we enhance MP-KD with a temporal cross-lingual attention\nmechanism to dynamically estimate the alignment strength. The two procedures\nare mutually paced along with model training. Extensive experiments on twelve\ncross-lingual TKG transfer tasks in the EventKG benchmark demonstrate the\neffectiveness of the proposed MP-KD method.", "published": "2023-03-27 03:15:27", "link": "http://arxiv.org/abs/2303.14898v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Curriculum Learning for Compositional Visual Reasoning", "abstract": "Visual Question Answering (VQA) is a complex task requiring large datasets\nand expensive training. Neural Module Networks (NMN) first translate the\nquestion to a reasoning path, then follow that path to analyze the image and\nprovide an answer. We propose an NMN method that relies on predefined\ncross-modal embeddings to ``warm start'' learning on the GQA dataset, then\nfocus on Curriculum Learning (CL) as a way to improve training and make a\nbetter use of the data. Several difficulty criteria are employed for defining\nCL methods. We show that by an appropriate selection of the CL method the cost\nof training and the amount of training data can be greatly reduced, with a\nlimited impact on the final VQA accuracy. Furthermore, we introduce\nintermediate losses during training and find that this allows to simplify the\nCL strategy.", "published": "2023-03-27 08:47:18", "link": "http://arxiv.org/abs/2303.15006v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Borrowing Human Senses: Comment-Aware Self-Training for Social Media\n  Multimodal Classification", "abstract": "Social media is daily creating massive multimedia content with paired image\nand text, presenting the pressing need to automate the vision and language\nunderstanding for various multimodal classification tasks. Compared to the\ncommonly researched visual-lingual data, social media posts tend to exhibit\nmore implicit image-text relations. To better glue the cross-modal semantics\ntherein, we capture hinting features from user comments, which are retrieved\nvia jointly leveraging visual and lingual similarity. Afterwards, the\nclassification tasks are explored via self-training in a teacher-student\nframework, motivated by the usually limited labeled data scales in existing\nbenchmarks. Substantial experiments are conducted on four multimodal social\nmedia benchmarks for image text relation classification, sarcasm detection,\nsentiment classification, and hate speech detection. The results show that our\nmethod further advances the performance of previous state-of-the-art models,\nwhich do not employ comment modeling or self-training.", "published": "2023-03-27 08:59:55", "link": "http://arxiv.org/abs/2303.15016v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Cross-utterance ASR Rescoring with Graph-based Label Propagation", "abstract": "We propose a novel approach for ASR N-best hypothesis rescoring with\ngraph-based label propagation by leveraging cross-utterance acoustic\nsimilarity. In contrast to conventional neural language model (LM) based ASR\nrescoring/reranking models, our approach focuses on acoustic information and\nconducts the rescoring collaboratively among utterances, instead of\nindividually. Experiments on the VCTK dataset demonstrate that our approach\nconsistently improves ASR performance, as well as fairness across speaker\ngroups with different accents. Our approach provides a low-cost solution for\nmitigating the majoritarian bias of ASR systems, without the need to train new\ndomain- or accent-specific models.", "published": "2023-03-27 12:08:05", "link": "http://arxiv.org/abs/2303.15132v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot\n  Learning", "abstract": "Recent compositional zero-shot learning (CZSL) methods adapt pre-trained\nvision-language models (VLMs) by constructing trainable prompts only for\ncomposed state-object pairs. Relying on learning the joint representation of\nseen compositions, these methods ignore the explicit modeling of the state and\nobject, thus limiting the exploitation of pre-trained knowledge and\ngeneralization to unseen compositions. With a particular focus on the\nuniversality of the solution, in this work, we propose a novel paradigm for\nCZSL models that establishes three identification branches (i.e., Multi-Path)\nto jointly model the state, object, and composition. The presented Troika is\nour implementation that aligns the branch-specific prompt representations with\ndecomposed visual features. To calibrate the bias between semantically similar\nmulti-modal representations, we further devise a Cross-Modal Traction module\ninto Troika that shifts the prompt representation towards the current visual\ncontent. We conduct extensive experiments on three popular benchmarks, where\nour method significantly outperforms existing methods in both closed-world and\nopen-world settings. The code will be available at\nhttps://github.com/bighuang624/Troika.", "published": "2023-03-27 14:10:26", "link": "http://arxiv.org/abs/2303.15230v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Zero-Shot Composed Image Retrieval with Textual Inversion", "abstract": "Composed Image Retrieval (CIR) aims to retrieve a target image based on a\nquery composed of a reference image and a relative caption that describes the\ndifference between the two images. The high effort and cost required for\nlabeling datasets for CIR hamper the widespread usage of existing methods, as\nthey rely on supervised learning. In this work, we propose a new task,\nZero-Shot CIR (ZS-CIR), that aims to address CIR without requiring a labeled\ntraining dataset. Our approach, named zero-Shot composEd imAge Retrieval with\ntextuaL invErsion (SEARLE), maps the visual features of the reference image\ninto a pseudo-word token in CLIP token embedding space and integrates it with\nthe relative caption. To support research on ZS-CIR, we introduce an\nopen-domain benchmarking dataset named Composed Image Retrieval on Common\nObjects in context (CIRCO), which is the first dataset for CIR containing\nmultiple ground truths for each query. The experiments show that SEARLE\nexhibits better performance than the baselines on the two main datasets for CIR\ntasks, FashionIQ and CIRR, and on the proposed CIRCO. The dataset, the code and\nthe model are publicly available at https://github.com/miccunifi/SEARLE.", "published": "2023-03-27 14:31:25", "link": "http://arxiv.org/abs/2303.15247v2", "categories": ["cs.CV", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Bilex Rx: Lexical Data Augmentation for Massively Multilingual Machine\n  Translation", "abstract": "Neural machine translation (NMT) has progressed rapidly over the past several\nyears, and modern models are able to achieve relatively high quality using only\nmonolingual text data, an approach dubbed Unsupervised Machine Translation\n(UNMT). However, these models still struggle in a variety of ways, including\naspects of translation that for a human are the easiest - for instance,\ncorrectly translating common nouns. This work explores a cheap and abundant\nresource to combat this problem: bilingual lexica. We test the efficacy of\nbilingual lexica in a real-world set-up, on 200-language translation models\ntrained on web-crawled text. We present several findings: (1) using lexical\ndata augmentation, we demonstrate sizable performance gains for unsupervised\ntranslation; (2) we compare several families of data augmentation,\ndemonstrating that they yield similar improvements, and can be combined for\neven greater improvements; (3) we demonstrate the importance of carefully\ncurated lexica over larger, noisier ones, especially with larger models; and\n(4) we compare the efficacy of multilingual lexicon data versus\nhuman-translated parallel data. Finally, we open-source GATITOS (available at\nhttps://github.com/google-research/url-nlp/tree/main/gatitos), a new\nmultilingual lexicon for 26 low-resource languages, which had the highest\nperformance among lexica in our experiments.", "published": "2023-03-27 14:54:43", "link": "http://arxiv.org/abs/2303.15265v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Improving Neural Topic Models with Wasserstein Knowledge Distillation", "abstract": "Topic modeling is a dominant method for exploring document collections on the\nweb and in digital libraries. Recent approaches to topic modeling use\npretrained contextualized language models and variational autoencoders.\nHowever, large neural topic models have a considerable memory footprint. In\nthis paper, we propose a knowledge distillation framework to compress a\ncontextualized topic model without loss in topic quality. In particular, the\nproposed distillation objective is to minimize the cross-entropy of the soft\nlabels produced by the teacher and the student models, as well as to minimize\nthe squared 2-Wasserstein distance between the latent distributions learned by\nthe two models. Experiments on two publicly available datasets show that the\nstudent trained with knowledge distillation achieves topic coherence much\nhigher than that of the original student model, and even surpasses the teacher\nwhile containing far fewer parameters than the teacher's. The distilled model\nalso outperforms several other competitive topic models on topic coherence.", "published": "2023-03-27 16:07:44", "link": "http://arxiv.org/abs/2303.15350v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Debiasing Scores and Prompts of 2D Diffusion for View-consistent\n  Text-to-3D Generation", "abstract": "Existing score-distilling text-to-3D generation techniques, despite their\nconsiderable promise, often encounter the view inconsistency problem. One of\nthe most notable issues is the Janus problem, where the most canonical view of\nan object (\\textit{e.g}., face or head) appears in other views. In this work,\nwe explore existing frameworks for score-distilling text-to-3D generation and\nidentify the main causes of the view inconsistency problem -- the embedded bias\nof 2D diffusion models. Based on these findings, we propose two approaches to\ndebias the score-distillation frameworks for view-consistent text-to-3D\ngeneration. Our first approach, called score debiasing, involves cutting off\nthe score estimated by 2D diffusion models and gradually increasing the\ntruncation value throughout the optimization process. Our second approach,\ncalled prompt debiasing, identifies conflicting words between user prompts and\nview prompts using a language model, and adjusts the discrepancy between view\nprompts and the viewing direction of an object. Our experimental results show\nthat our methods improve the realism of the generated 3D objects by\nsignificantly reducing artifacts and achieve a good trade-off between\nfaithfulness to the 2D diffusion models and 3D consistency with little\noverhead. Our project page is available\nat~\\url{https://susunghong.github.io/Debiased-Score-Distillation-Sampling/}.", "published": "2023-03-27 17:31:13", "link": "http://arxiv.org/abs/2303.15413v5", "categories": ["cs.CV", "cs.CL", "cs.GR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Zero-shot Model Diagnosis", "abstract": "When it comes to deploying deep vision models, the behavior of these systems\nmust be explicable to ensure confidence in their reliability and fairness. A\ncommon approach to evaluate deep learning models is to build a labeled test set\nwith attributes of interest and assess how well it performs. However, creating\na balanced test set (i.e., one that is uniformly sampled over all the important\ntraits) is often time-consuming, expensive, and prone to mistakes. The question\nwe try to address is: can we evaluate the sensitivity of deep learning models\nto arbitrary visual attributes without an annotated test set? This paper argues\nthe case that Zero-shot Model Diagnosis (ZOOM) is possible without the need for\na test set nor labeling. To avoid the need for test sets, our system relies on\na generative model and CLIP. The key idea is enabling the user to select a set\nof prompts (relevant to the problem) and our system will automatically search\nfor semantic counterfactual images (i.e., synthesized images that flip the\nprediction in the case of a binary classifier) using the generative model. We\nevaluate several visual tasks (classification, key-point detection, and\nsegmentation) in multiple visual domains to demonstrate the viability of our\nmethodology. Extensive experiments demonstrate that our method is capable of\nproducing counterfactual images and offering sensitivity analysis for model\ndiagnosis without the need for a test set.", "published": "2023-03-27 17:59:33", "link": "http://arxiv.org/abs/2303.15441v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "IRFL: Image Recognition of Figurative Language", "abstract": "Figures of speech such as metaphors, similes, and idioms are integral parts\nof human communication. They are ubiquitous in many forms of discourse,\nallowing people to convey complex, abstract ideas and evoke emotion. As\nfigurative forms are often conveyed through multiple modalities (e.g., both\ntext and images), understanding multimodal figurative language is an important\nAI challenge, weaving together profound vision, language, commonsense and\ncultural knowledge. In this work, we develop the Image Recognition of\nFigurative Language (IRFL) dataset. We leverage human annotation and an\nautomatic pipeline we created to generate a multimodal dataset, and introduce\ntwo novel tasks as a benchmark for multimodal figurative language\nunderstanding. We experimented with state-of-the-art vision and language models\nand found that the best (22%) performed substantially worse than humans (97%).\nWe release our dataset, benchmark, and code, in hopes of driving the\ndevelopment of models that can better understand figurative language.", "published": "2023-03-27 17:59:55", "link": "http://arxiv.org/abs/2303.15445v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "ACO-tagger: A Novel Method for Part-of-Speech Tagging using Ant Colony\n  Optimization", "abstract": "Swarm Intelligence algorithms have gained significant attention in recent\nyears as a means of solving complex and non-deterministic problems. These\nalgorithms are inspired by the collective behavior of natural creatures, and\nthey simulate this behavior to develop intelligent agents for computational\ntasks. One such algorithm is Ant Colony Optimization (ACO), which is inspired\nby the foraging behavior of ants and their pheromone laying mechanism. ACO is\nused for solving difficult problems that are discrete and combinatorial in\nnature. Part-of-Speech (POS) tagging is a fundamental task in natural language\nprocessing that aims to assign a part-of-speech role to each word in a\nsentence. In this research paper, proposed a high-performance POS-tagging\nmethod based on ACO called ACO-tagger. This method achieved a high accuracy\nrate of 96.867%, outperforming several state-of-the-art methods. The proposed\nmethod is fast and efficient, making it a viable option for practical\napplications.", "published": "2023-03-27 11:48:40", "link": "http://arxiv.org/abs/2303.16760v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Not cool, calm or collected: Using emotional language to detect COVID-19\n  misinformation", "abstract": "COVID-19 misinformation on social media platforms such as twitter is a threat\nto effective pandemic management. Prior works on tweet COVID-19 misinformation\nnegates the role of semantic features common to twitter such as charged\nemotions. Thus, we present a novel COVID-19 misinformation model, which uses\nboth a tweet emotion encoder and COVID-19 misinformation encoder to predict\nwhether a tweet contains COVID-19 misinformation. Our emotion encoder was\nfine-tuned on a novel annotated dataset and our COVID-19 misinformation encoder\nwas fine-tuned on a subset of the COVID-HeRA dataset. Experimental results show\nsuperior results using the combination of emotion and misinformation encoders\nas opposed to a misinformation classifier alone. Furthermore, extensive result\nanalysis was conducted, highlighting low quality labels and mismatched label\ndistributions as key limitations to our study.", "published": "2023-03-27 22:24:05", "link": "http://arxiv.org/abs/2303.16777v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Assorted, Archetypal and Annotated Two Million (3A2M) Cooking Recipes\n  Dataset based on Active Learning", "abstract": "Cooking recipes allow individuals to exchange culinary ideas and provide food\npreparation instructions. Due to a lack of adequate labeled data, categorizing\nraw recipes found online to the appropriate food genres is a challenging task\nin this domain. Utilizing the knowledge of domain experts to categorize recipes\ncould be a solution. In this study, we present a novel dataset of two million\nculinary recipes labeled in respective categories leveraging the knowledge of\nfood experts and an active learning technique. To construct the dataset, we\ncollect the recipes from the RecipeNLG dataset. Then, we employ three human\nexperts whose trustworthiness score is higher than 86.667% to categorize 300K\nrecipe by their Named Entity Recognition (NER) and assign it to one of the nine\ncategories: bakery, drinks, non-veg, vegetables, fast food, cereals, meals,\nsides and fusion. Finally, we categorize the remaining 1900K recipes using\nActive Learning method with a blend of Query-by-Committee and Human In The Loop\n(HITL) approaches. There are more than two million recipes in our dataset, each\nof which is categorized and has a confidence score linked with it. For the 9\ngenres, the Fleiss Kappa score of this massive dataset is roughly 0.56026. We\nbelieve that the research community can use this dataset to perform various\nmachine learning tasks such as recipe genre classification, recipe generation\nof a specific genre, new recipe creation, etc. The dataset can also be used to\ntrain and evaluate the performance of various NLP tasks such as named entity\nrecognition, part-of-speech tagging, semantic role labeling, and so on. The\ndataset will be available upon publication: https://tinyurl.com/3zu4778y.", "published": "2023-03-27 07:53:18", "link": "http://arxiv.org/abs/2303.16778v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Creativity of Large Language Models", "abstract": "Large Language Models (LLMs) are revolutionizing several areas of Artificial\nIntelligence. One of the most remarkable applications is creative writing,\ne.g., poetry or storytelling: the generated outputs are often of astonishing\nquality. However, a natural question arises: can LLMs be really considered\ncreative? In this article, we first analyze the development of LLMs under the\nlens of creativity theories, investigating the key open questions and\nchallenges. In particular, we focus our discussion on the dimensions of value,\nnovelty, and surprise as proposed by Margaret Boden in her work. Then, we\nconsider different classic perspectives, namely product, process, press, and\nperson. We discuss a set of ``easy'' and ``hard'' problems in machine\ncreativity, presenting them in relation to LLMs. Finally, we examine the\nsocietal impact of these technologies with a particular focus on the creative\nindustries, analyzing the opportunities offered, the challenges arising from\nthem, and the potential associated risks, from both legal and ethical points of\nview.", "published": "2023-03-27 18:00:01", "link": "http://arxiv.org/abs/2304.00008v5", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI"}
{"title": "Data Augmentation for Environmental Sound Classification Using Diffusion\n  Probabilistic Model with Top-k Selection Discriminator", "abstract": "Despite consistent advancement in powerful deep learning techniques in recent\nyears, large amounts of training data are still necessary for the models to\navoid overfitting. Synthetic datasets using generative adversarial networks\n(GAN) have recently been generated to overcome this problem. Nevertheless,\ndespite advancements, GAN-based methods are usually hard to train or fail to\ngenerate high-quality data samples. In this paper, we propose an environmental\nsound classification augmentation technique based on the diffusion\nprobabilistic model with DPM-Solver$++$ for fast sampling. In addition, to\nensure the quality of the generated spectrograms, we train a top-k selection\ndiscriminator on the dataset. According to the experiment results, the\nsynthesized spectrograms have similar features to the original dataset and can\nsignificantly increase the classification accuracy of different\nstate-of-the-art models compared with traditional data augmentation techniques.\nThe public code is available on\nhttps://github.com/JNAIC/DPMs-for-Audio-Data-Augmentation.", "published": "2023-03-27 12:51:33", "link": "http://arxiv.org/abs/2303.15161v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Text is All You Need: Personalizing ASR Models using Controllable Speech\n  Synthesis", "abstract": "Adapting generic speech recognition models to specific individuals is a\nchallenging problem due to the scarcity of personalized data. Recent works have\nproposed boosting the amount of training data using personalized text-to-speech\nsynthesis. Here, we ask two fundamental questions about this strategy: when is\nsynthetic data effective for personalization, and why is it effective in those\ncases? To address the first question, we adapt a state-of-the-art automatic\nspeech recognition (ASR) model to target speakers from four benchmark datasets\nrepresentative of different speaker types. We show that ASR personalization\nwith synthetic data is effective in all cases, but particularly when (i) the\ntarget speaker is underrepresented in the global data, and (ii) the capacity of\nthe global model is limited. To address the second question of why personalized\nsynthetic data is effective, we use controllable speech synthesis to generate\nspeech with varied styles and content. Surprisingly, we find that the text\ncontent of the synthetic data, rather than style, is important for speaker\nadaptation. These results lead us to propose a data selection strategy for ASR\npersonalization based on speech content.", "published": "2023-03-27 02:50:02", "link": "http://arxiv.org/abs/2303.14885v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Partially Adaptive Multichannel Joint Reduction of Ego-noise and\n  Environmental Noise", "abstract": "Human-robot interaction relies on a noise-robust audio processing module\ncapable of estimating target speech from audio recordings impacted by\nenvironmental noise, as well as self-induced noise, so-called ego-noise. While\nexternal ambient noise sources vary from environment to environment, ego-noise\nis mainly caused by the internal motors and joints of a robot. Ego-noise and\nenvironmental noise reduction are often decoupled, i.e., ego-noise reduction is\nperformed without considering environmental noise. Recently, a variational\nautoencoder (VAE)-based speech model has been combined with a fully adaptive\nnon-negative matrix factorization (NMF) noise model to recover clean speech\nunder different environmental noise disturbances. However, its enhancement\nperformance is limited in adverse acoustic scenarios involving, e.g. ego-noise.\nIn this paper, we propose a multichannel partially adaptive scheme to jointly\nmodel ego-noise and environmental noise utilizing the VAE-NMF framework, where\nwe take advantage of spatially and spectrally structured characteristics of\nego-noise by pre-training the ego-noise model, while retaining the ability to\nadapt to unknown environmental noise. Experimental results show that our\nproposed approach outperforms the methods based on a completely fixed scheme\nand a fully adaptive scheme when ego-noise and environmental noise are present\nsimultaneously.", "published": "2023-03-27 09:40:14", "link": "http://arxiv.org/abs/2303.15042v1", "categories": ["eess.AS", "cs.LG", "cs.RO", "cs.SD"], "primary_category": "eess.AS"}
