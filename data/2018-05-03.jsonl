{"title": "What you can cram into a single vector: Probing sentence embeddings for\n  linguistic properties", "abstract": "Although much effort has recently been devoted to training high-quality\nsentence embeddings, we still have a poor understanding of what they are\ncapturing. \"Downstream\" tasks, often based on sentence classification, are\ncommonly used to evaluate the quality of sentence representations. The\ncomplexity of the tasks makes it however difficult to infer what kind of\ninformation is present in the representations. We introduce here 10 probing\ntasks designed to capture simple linguistic features of sentences, and we use\nthem to study embeddings generated by three different encoders trained in eight\ndistinct ways, uncovering intriguing properties of both encoders and training\nmethods.", "published": "2018-05-03 00:46:56", "link": "http://arxiv.org/abs/1805.01070v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformation Networks for Target-Oriented Sentiment Classification", "abstract": "Target-oriented sentiment classification aims at classifying sentiment\npolarities over individual opinion targets in a sentence. RNN with attention\nseems a good fit for the characteristics of this task, and indeed it achieves\nthe state-of-the-art performance. After re-examining the drawbacks of attention\nmechanism and the obstacles that block CNN to perform well in this\nclassification task, we propose a new model to overcome these issues. Instead\nof attention, our model employs a CNN layer to extract salient features from\nthe transformed word representations originated from a bi-directional RNN\nlayer. Between the two layers, we propose a component to generate\ntarget-specific representations of words in the sentence, meanwhile incorporate\na mechanism for preserving the original contextual information from the RNN\nlayer. Experiments show that our model achieves a new state-of-the-art\nperformance on a few benchmarks.", "published": "2018-05-03 02:16:27", "link": "http://arxiv.org/abs/1805.01086v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Binarizer at SemEval-2018 Task 3: Parsing dependency and deep learning\n  for irony detection", "abstract": "In this paper, we describe the system submitted for the SemEval 2018 Task 3\n(Irony detection in English tweets) Subtask A by the team Binarizer. Irony\ndetection is a key task for many natural language processing works. Our method\ntreats ironical tweets to consist of smaller parts containing different\nemotions. We break down tweets into separate phrases using a dependency parser.\nWe then embed those phrases using an LSTM-based neural network model which is\npre-trained to predict emoticons for tweets. Finally, we train a\nfully-connected network to achieve classification.", "published": "2018-05-03 04:53:06", "link": "http://arxiv.org/abs/1805.01112v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Fine Line between Linguistic Generalization and Failure in\n  Seq2Seq-Attention Models", "abstract": "Seq2Seq based neural architectures have become the go-to architecture to\napply to sequence to sequence language tasks. Despite their excellent\nperformance on these tasks, recent work has noted that these models usually do\nnot fully capture the linguistic structure required to generalize beyond the\ndense sections of the data distribution \\cite{ettinger2017towards}, and as\nsuch, are likely to fail on samples from the tail end of the distribution (such\nas inputs that are noisy \\citep{belkinovnmtbreak} or of different lengths\n\\citep{bentivoglinmtlength}). In this paper, we look at a model's ability to\ngeneralize on a simple symbol rewriting task with a clearly defined structure.\nWe find that the model's ability to generalize this structure beyond the\ntraining distribution depends greatly on the chosen random seed, even when\nperformance on the standard test set remains the same. This suggests that a\nmodel's ability to capture generalizable structure is highly sensitive.\nMoreover, this sensitivity may not be apparent when evaluating it on standard\ntest sets.", "published": "2018-05-03 17:45:33", "link": "http://arxiv.org/abs/1805.01445v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast and Scalable Expansion of Natural Language Understanding\n  Functionality for Intelligent Agents", "abstract": "Fast expansion of natural language functionality of intelligent virtual\nagents is critical for achieving engaging and informative interactions.\nHowever, developing accurate models for new natural language domains is a time\nand data intensive process. We propose efficient deep neural network\narchitectures that maximally re-use available resources through transfer\nlearning. Our methods are applied for expanding the understanding capabilities\nof a popular commercial agent and are evaluated on hundreds of new domains,\ndesigned by internal or external developers. We demonstrate that our proposed\nmethods significantly increase accuracy in low resource settings and enable\nrapid development of accurate models with less data.", "published": "2018-05-03 21:21:16", "link": "http://arxiv.org/abs/1805.01542v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An End-to-end Approach for Handling Unknown Slot Values in Dialogue\n  State Tracking", "abstract": "We highlight a practical yet rarely discussed problem in dialogue state\ntracking (DST), namely handling unknown slot values. Previous approaches\ngenerally assume predefined candidate lists and thus are not designed to output\nunknown values, especially when the spoken language understanding (SLU) module\nis absent as in many end-to-end (E2E) systems. We describe in this paper an E2E\narchitecture based on the pointer network (PtrNet) that can effectively extract\nunknown slot values while still obtains state-of-the-art accuracy on the\nstandard DSTC2 benchmark. We also provide extensive empirical evidence to show\nthat tracking unknown values can be challenging and our approach can bring\nsignificant improvement with the help of an effective feature dropout\ntechnique.", "published": "2018-05-03 21:59:13", "link": "http://arxiv.org/abs/1805.01555v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Chinese Radicals Into Neural Machine Translation: Deeper\n  Than Character Level", "abstract": "In neural machine translation (NMT), researchers face the challenge of\nun-seen (or out-of-vocabulary OOV) words translation. To solve this, some\nresearchers propose the splitting of western languages such as English and\nGerman into sub-words or compounds. In this paper, we try to address this OOV\nissue and improve the NMT adequacy with a harder language Chinese whose\ncharacters are even more sophisticated in composition. We integrate the Chinese\nradicals into the NMT model with different settings to address the unseen words\nchallenge in Chinese to English translation. On the other hand, this also can\nbe considered as semantic part of the MT system since the Chinese radicals\nusually carry the essential meaning of the words they are constructed in.\nMeaningful radicals and new characters can be integrated into the NMT systems\nwith our models. We use an attention-based NMT system as a strong baseline\nsystem. The experiments on standard Chinese-to-English NIST translation shared\ntask data 2006 and 2008 show that our designed models outperform the baseline\nmodel in a wide range of state-of-the-art evaluation metrics including LEPOR,\nBEER, and CharacTER, in addition to BLEU and NIST scores, especially on the\nadequacy-level translation. We also have some interesting findings from the\nresults of our various experiment settings about the performance of words and\ncharacters in Chinese NMT, which is different with other languages. For\ninstance, the fully character level NMT may perform well or the state of the\nart in some other languages as researchers demonstrated recently, however, in\nthe Chinese NMT model, word boundary knowledge is important for the model\nlearning.", "published": "2018-05-03 22:58:54", "link": "http://arxiv.org/abs/1805.01565v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scalable Semantic Querying of Text", "abstract": "We present the KOKO system that takes declarative information extraction to a\nnew level by incorporating advances in natural language processing techniques\nin its extraction language. KOKO is novel in that its extraction language\nsimultaneously supports conditions on the surface of the text and on the\nstructure of the dependency parse tree of sentences, thereby allowing for more\nrefined extractions. KOKO also supports conditions that are forgiving to\nlinguistic variation of expressing concepts and allows to aggregate evidence\nfrom the entire document in order to filter extractions.\n  To scale up, KOKO exploits a multi-indexing scheme and heuristics for\nefficient extractions. We extensively evaluate KOKO over publicly available\ntext corpora. We show that KOKO indices take up the smallest amount of space,\nare notably faster and more effective than a number of prior indexing schemes.\nFinally, we demonstrate KOKO's scale up on a corpus of 5 million Wikipedia\narticles.", "published": "2018-05-03 01:57:31", "link": "http://arxiv.org/abs/1805.01083v1", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Stack-Pointer Networks for Dependency Parsing", "abstract": "We introduce a novel architecture for dependency parsing: \\emph{stack-pointer\nnetworks} (\\textbf{\\textsc{StackPtr}}). Combining pointer\nnetworks~\\citep{vinyals2015pointer} with an internal stack, the proposed model\nfirst reads and encodes the whole sentence, then builds the dependency tree\ntop-down (from root-to-leaf) in a depth-first fashion. The stack tracks the\nstatus of the depth-first search and the pointer networks select one child for\nthe word at the top of the stack at each step. The \\textsc{StackPtr} parser\nbenefits from the information of the whole sentence and all previously derived\nsubtree structures, and removes the left-to-right restriction in classical\ntransition-based parsers. Yet, the number of steps for building any (including\nnon-projective) parse tree is linear in the length of the sentence just as\nother transition-based parsers, yielding an efficient decoding algorithm with\n$O(n^2)$ time complexity. We evaluate our model on 29 treebanks spanning 20\nlanguages and different dependency annotation schemas, and achieve\nstate-of-the-art performance on 21 of them.", "published": "2018-05-03 02:23:28", "link": "http://arxiv.org/abs/1805.01087v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Hierarchical End-to-End Model for Jointly Improving Text Summarization\n  and Sentiment Classification", "abstract": "Text summarization and sentiment classification both aim to capture the main\nideas of the text but at different levels. Text summarization is to describe\nthe text within a few sentences, while sentiment classification can be regarded\nas a special type of summarization which \"summarizes\" the text into a even more\nabstract fashion, i.e., a sentiment class. Based on this idea, we propose a\nhierarchical end-to-end model for joint learning of text summarization and\nsentiment classification, where the sentiment classification label is treated\nas the further \"summarization\" of the text summarization output. Hence, the\nsentiment classification layer is put upon the text summarization layer, and a\nhierarchical structure is derived. Experimental results on Amazon online\nreviews datasets show that our model achieves better performance than the\nstrong baseline systems on both abstractive summarization and sentiment\nclassification.", "published": "2018-05-03 02:30:07", "link": "http://arxiv.org/abs/1805.01089v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Better Text Understanding and Retrieval through Kernel Entity\n  Salience Modeling", "abstract": "This paper presents a Kernel Entity Salience Model (KESM) that improves text\nunderstanding and retrieval by better estimating entity salience (importance)\nin documents. KESM represents entities by knowledge enriched distributed\nrepresentations, models the interactions between entities and words by kernels,\nand combines the kernel scores to estimate entity salience. The whole model is\nlearned end-to-end using entity salience labels. The salience model also\nimproves ad hoc search accuracy, providing effective ranking features by\nmodeling the salience of query entities in candidate documents. Our experiments\non two entity salience corpora and two TREC ad hoc search datasets demonstrate\nthe effectiveness of KESM over frequency-based and feature-based methods. We\nalso provide examples showing how KESM conveys its text understanding ability\nlearned from entity salience to search.", "published": "2018-05-03 14:46:12", "link": "http://arxiv.org/abs/1805.01334v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Reinforcement Learning Approach to Interactive-Predictive Neural\n  Machine Translation", "abstract": "We present an approach to interactive-predictive neural machine translation\nthat attempts to reduce human effort from three directions: Firstly, instead of\nrequiring humans to select, correct, or delete segments, we employ the idea of\nlearning from human reinforcements in form of judgments on the quality of\npartial translations. Secondly, human effort is further reduced by using the\nentropy of word predictions as uncertainty criterion to trigger feedback\nrequests. Lastly, online updates of the model parameters after every\ninteraction allow the model to adapt quickly. We show in simulation experiments\nthat reward signals on partial translations significantly improve character\nF-score and BLEU compared to feedback on full translations only, while human\neffort can be reduced to an average number of $5$ feedback requests for every\ninput.", "published": "2018-05-03 21:50:34", "link": "http://arxiv.org/abs/1805.01553v3", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Multimodal Emotion Recognition for One-Minute-Gradual Emotion Challenge", "abstract": "The continuous dimensional emotion modelled by arousal and valence can depict\ncomplex changes of emotions. In this paper, we present our works on arousal and\nvalence predictions for One-Minute-Gradual (OMG) Emotion Challenge. Multimodal\nrepresentations are first extracted from videos using a variety of acoustic,\nvideo and textual models and support vector machine (SVM) is then used for\nfusion of multimodal signals to make final predictions. Our solution achieves\nConcordant Correlation Coefficient (CCC) scores of 0.397 and 0.520 on arousal\nand valence respectively for the validation dataset, which outperforms the\nbaseline systems with the best CCC scores of 0.15 and 0.23 on arousal and\nvalence by a large margin.", "published": "2018-05-03 00:10:10", "link": "http://arxiv.org/abs/1805.01060v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Disentangling Language and Knowledge in Task-Oriented Dialogs", "abstract": "The Knowledge Base (KB) used for real-world applications, such as booking a\nmovie or restaurant reservation, keeps changing over time. End-to-end neural\nnetworks trained for these task-oriented dialogs are expected to be immune to\nany changes in the KB. However, existing approaches breakdown when asked to\nhandle such changes. We propose an encoder-decoder architecture (BoSsNet) with\na novel Bag-of-Sequences (BoSs) memory, which facilitates the disentangled\nlearning of the response's language model and its knowledge incorporation.\nConsequently, the KB can be modified with new knowledge without a drop in\ninterpretability. We find that BoSsNet outperforms state-of-the-art models,\nwith considerable improvements (> 10\\%) on bAbI OOV test sets and other\nhuman-human datasets. We also systematically modify existing datasets to\nmeasure disentanglement and show BoSsNet to be robust to KB modifications.", "published": "2018-05-03 10:52:26", "link": "http://arxiv.org/abs/1805.01216v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Improving a Neural Semantic Parser by Counterfactual Learning from Human\n  Bandit Feedback", "abstract": "Counterfactual learning from human bandit feedback describes a scenario where\nuser feedback on the quality of outputs of a historic system is logged and used\nto improve a target system. We show how to apply this learning framework to\nneural semantic parsing. From a machine learning perspective, the key challenge\nlies in a proper reweighting of the estimator so as to avoid known degeneracies\nin counterfactual learning, while still being applicable to stochastic gradient\noptimization. To conduct experiments with human users, we devise an easy-to-use\ninterface to collect human feedback on semantic parses. Our work is the first\nto show that semantic parsers can be improved significantly by counterfactual\nlearning from logged human feedback data.", "published": "2018-05-03 12:24:39", "link": "http://arxiv.org/abs/1805.01252v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Framewise approach in multimodal emotion recognition in OMG challenge", "abstract": "In this report we described our approach achieves $53\\%$ of unweighted\naccuracy over $7$ emotions and $0.05$ and $0.09$ mean squared errors for\narousal and valence in OMG emotion recognition challenge. Our results were\nobtained with ensemble of single modality models trained on voice and face data\nfrom video separately. We consider each stream as a sequence of frames. Next we\nestimated features from frames and handle it with recurrent neural network. As\naudio frame we mean short $0.4$ second spectrogram interval. For features\nestimation for face pictures we used own ResNet neural network pretrained on\nAffectNet database. Each short spectrogram was considered as a picture and\nprocessed by convolutional network too. As a base audio model we used ResNet\npretrained in speaker recognition task. Predictions from both modalities were\nfused on decision level and improve single-channel approaches by a few percent", "published": "2018-05-03 15:21:44", "link": "http://arxiv.org/abs/1805.01369v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Dimensional emotion recognition using visual and textual cues", "abstract": "This paper addresses the problem of automatic emotion recognition in the\nscope of the One-Minute Gradual-Emotional Behavior challenge (OMG-Emotion\nchallenge). The underlying objective of the challenge is the automatic\nestimation of emotion expressions in the two-dimensional emotion representation\nspace (i.e., arousal and valence). The adopted methodology is a weighted\nensemble of several models from both video and text modalities. For video-based\nrecognition, two different types of visual cues (i.e., face and facial\nlandmarks) were considered to feed a multi-input deep neural network. Regarding\nthe text modality, a sequential model based on a simple recurrent architecture\nwas implemented. In addition, we also introduce a model based on high-level\nfeatures in order to embed domain knowledge in the learning process.\nExperimental results on the OMG-Emotion validation set demonstrate the\neffectiveness of the implemented ensemble model as it clearly outperforms the\ncurrent baseline methods.", "published": "2018-05-03 16:42:20", "link": "http://arxiv.org/abs/1805.01416v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Supervector Compression Strategies to Speed up I-Vector System\n  Development", "abstract": "The front-end factor analysis (FEFA), an extension of principal component\nanalysis (PPCA) tailored to be used with Gaussian mixture models (GMMs), is\ncurrently the prevalent approach to extract compact utterance-level features\n(i-vectors) for automatic speaker verification (ASV) systems. Little research\nhas been conducted comparing FEFA to the conventional PPCA applied to maximum a\nposteriori (MAP) adapted GMM supervectors. We study several alternative\nmethods, including PPCA, factor analysis (FA), and two supervised approaches,\nsupervised PPCA (SPPCA) and the recently proposed probabilistic partial least\nsquares (PPLS), to compress MAP-adapted GMM supervectors. The resulting\ni-vectors are used in ASV tasks with a probabilistic linear discriminant\nanalysis (PLDA) back-end. We experiment on two different datasets, on the\ntelephone condition of NIST SRE 2010 and on the recent VoxCeleb corpus\ncollected from YouTube videos containing celebrity interviews recorded in\nvarious acoustical and technical conditions. The results suggest that, in terms\nof ASV accuracy, the supervector compression approaches are on a par with FEFA.\nThe supervised approaches did not result in improved performance. In comparison\nto FEFA, we obtained more than hundred-fold (100x) speedups in the total\nvariability model (TVM) training using the PPCA and FA supervector compression\napproaches.", "published": "2018-05-03 08:12:39", "link": "http://arxiv.org/abs/1805.01156v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Deep Denoising for Hearing Aid Applications", "abstract": "Reduction of unwanted environmental noises is an important feature of today's\nhearing aids (HA), which is why noise reduction is nowadays included in almost\nevery commercially available device. The majority of these algorithms, however,\nis restricted to the reduction of stationary noises. In this work, we propose a\ndenoising approach based on a three hidden layer fully connected deep learning\nnetwork that aims to predict a Wiener filtering gain with an asymmetric input\ncontext, enabling real-time applications with high constraints on signal delay.\nThe approach is employing a hearing instrument-grade filter bank and complies\nwith typical hearing aid demands, such as low latency and on-line processing.\nIt can further be well integrated with other algorithms in an existing HA\nsignal processing chain. We can show on a database of real world noise signals\nthat our algorithm is able to outperform a state of the art baseline approach,\nboth using objective metrics and subject tests.", "published": "2018-05-03 10:04:39", "link": "http://arxiv.org/abs/1805.01198v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Single-Channel Blind Source Separation for Singing Voice Detection: A\n  Comparative Study", "abstract": "We propose a novel unsupervised singing voice detection method which use\nsingle-channel Blind Audio Source Separation (BASS) algorithm as a preliminary\nstep. To reach this goal, we investigate three promising BASS approaches which\noperate through a morphological filtering of the analyzed mixture spectrogram.\nThe contributions of this paper are manyfold. First, the investigated BASS\nmethods are reworded with the same formalism and we investigate their\nrespective hyperparameters by numerical simulations. Second, we propose an\nextension of the KAM method for which we propose a novel training algorithm\nused to compute a source-specific kernel from a given isolated source signal.\nSecond, the BASS methods are compared together in terms of source separation\naccuracy and in terms of singing voice detection accuracy when they are used in\nour new singing voice detection framework. Finally, we do an exhaustive singing\nvoice detection evaluation for which we compare both supervised and\nunsupervised singing voice detection methods. Our comparison explores different\ncombination of the proposed BASS methods with new features such as the new\nproposed KAM features and the scattering transform through a machine learning\nframework and also considers convolutional neural networks methods.", "published": "2018-05-03 10:10:33", "link": "http://arxiv.org/abs/1805.01201v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Discriminant Analysis for i-vector Based Robust Speaker Recognition", "abstract": "Linear Discriminant Analysis (LDA) has been used as a standard\npost-processing procedure in many state-of-the-art speaker recognition tasks.\nThrough maximizing the inter-speaker difference and minimizing the\nintra-speaker variation, LDA projects i-vectors to a lower-dimensional and more\ndiscriminative sub-space. In this paper, we propose a neural network based\ncompensation scheme(termed as deep discriminant analysis, DDA) for i-vector\nbased speaker recognition, which shares the spirit with LDA. Optimized against\nsoftmax loss and center loss at the same time, the proposed method learns a\nmore compact and discriminative embedding space. Compared with the Gaussian\ndistribution assumption of data and the learnt linear projection in LDA, the\nproposed method doesn't pose any assumptions on data and can learn a non-linear\nprojection function. Experiments are carried out on a short-duration\ntext-independent dataset based on the SRE Corpus, noticeable performance\nimprovement can be observed against the normal LDA or PLDA methods.", "published": "2018-05-03 14:55:56", "link": "http://arxiv.org/abs/1805.01344v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Noise Invariant Frame Selection: A Simple Method to Address the\n  Background Noise Problem for Text-independent Speaker Verification", "abstract": "The performance of speaker-related systems usually degrades heavily in\npractical applications largely due to the presence of background noise. To\nimprove the robustness of such systems in unknown noisy environments, this\npaper proposes a simple pre-processing method called Noise Invariant Frame\nSelection (NIFS). Based on several noisy constraints, it selects noise\ninvariant frames from utterances to represent speakers. Experiments conducted\non the TIMIT database showed that the NIFS can significantly improve the\nperformance of Vector Quantization (VQ), Gaussian Mixture Model-Universal\nBackground Model (GMM-UBM) and i-vector-based speaker verification systems in\ndifferent unknown noisy environments with different SNRs, in comparison to\ntheir baselines. Meanwhile, the proposed NIFS-based speaker verification\nsystems achieves similar performance when we change the constraints\n(hyper-parameters) or features, which indicates that it is robust and easy to\nreproduce. Since NIFS is designed as a general algorithm, it could be further\napplied to other similar tasks.", "published": "2018-05-03 12:35:06", "link": "http://arxiv.org/abs/1805.01259v1", "categories": ["cs.SD", "cs.CV", "eess.AS", "68T10"], "primary_category": "cs.SD"}
{"title": "Transformer for Emotion Recognition", "abstract": "This paper describes the UMONS solution for the OMG-Emotion Challenge. We\nexplore a context-dependent architecture where the arousal and valence of an\nutterance are predicted according to its surrounding context (i.e. the\npreceding and following utterances of the video). We report an improvement when\ntaking into account context for both unimodal and multimodal predictions.", "published": "2018-05-03 19:42:57", "link": "http://arxiv.org/abs/1805.02489v2", "categories": ["cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "audEERING's approach to the One-Minute-Gradual Emotion Challenge", "abstract": "This paper describes audEERING's submissions as well as additional\nevaluations for the One-Minute-Gradual (OMG) emotion recognition challenge. We\nprovide the results for audio and video processing on subject (in)dependent\nevaluations. On the provided Development set, we achieved 0.343 Concordance\nCorrelation Coefficient (CCC) for arousal (from audio) and .401 for valence\n(from video).", "published": "2018-05-03 11:06:23", "link": "http://arxiv.org/abs/1805.01222v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.CV"}
