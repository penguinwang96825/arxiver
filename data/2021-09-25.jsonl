{"title": "Weakly Supervised Contrastive Learning for Chest X-Ray Report Generation", "abstract": "Radiology report generation aims at generating descriptive text from\nradiology images automatically, which may present an opportunity to improve\nradiology reporting and interpretation. A typical setting consists of training\nencoder-decoder models on image-report pairs with a cross entropy loss, which\nstruggles to generate informative sentences for clinical diagnoses since normal\nfindings dominate the datasets. To tackle this challenge and encourage more\nclinically-accurate text outputs, we propose a novel weakly supervised\ncontrastive loss for medical report generation. Experimental results\ndemonstrate that our method benefits from contrasting target reports with\nincorrect but semantically-close ones. It outperforms previous work on both\nclinical correctness and text generation metrics for two public benchmarks.", "published": "2021-09-25 00:06:23", "link": "http://arxiv.org/abs/2109.12242v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Systematic Generalization on gSCAN: What is Nearly Solved and What is\n  Next?", "abstract": "We analyze the grounded SCAN (gSCAN) benchmark, which was recently proposed\nto study systematic generalization for grounded language understanding. First,\nwe study which aspects of the original benchmark can be solved by commonly used\nmethods in multi-modal research. We find that a general-purpose\nTransformer-based model with cross-modal attention achieves strong performance\non a majority of the gSCAN splits, surprisingly outperforming more specialized\napproaches from prior work. Furthermore, our analysis suggests that many of the\nremaining errors reveal the same fundamental challenge in systematic\ngeneralization of linguistic constructs regardless of visual context. Second,\ninspired by this finding, we propose challenging new tasks for gSCAN by\ngenerating data to incorporate relations between objects in the visual\nenvironment. Finally, we find that current models are surprisingly data\ninefficient given the narrow scope of commands in gSCAN, suggesting another\nchallenge for future work.", "published": "2021-09-25 00:08:42", "link": "http://arxiv.org/abs/2109.12243v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jointly Learning to Repair Code and Generate Commit Message", "abstract": "We propose a novel task of jointly repairing program codes and generating\ncommit messages. Code repair and commit message generation are two essential\nand related tasks for software development. However, existing work usually\nperforms the two tasks independently. We construct a multilingual triple\ndataset including buggy code, fixed code, and commit messages for this novel\ntask. We provide the cascaded models as baseline, which are enhanced with\ndifferent training approaches, including the teacher-student method, the\nmulti-task method, and the back-translation method. To deal with the error\npropagation problem of the cascaded method, the joint model is proposed that\ncan both repair the code and generate the commit message in a unified\nframework. Experimental results show that the enhanced cascaded model with\nteacher-student method and multitask-learning method achieves the best score on\ndifferent metrics of automated code repair, and the joint model behaves better\nthan the cascaded model on commit message generation.", "published": "2021-09-25 07:08:28", "link": "http://arxiv.org/abs/2109.12296v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Graph-Based Neural Model for End-to-End Frame Semantic Parsing", "abstract": "Frame semantic parsing is a semantic analysis task based on FrameNet which\nhas received great attention recently. The task usually involves three subtasks\nsequentially: (1) target identification, (2) frame classification and (3)\nsemantic role labeling. The three subtasks are closely related while previous\nstudies model them individually, which ignores their intern connections and\nmeanwhile induces error propagation problem. In this work, we propose an\nend-to-end neural model to tackle the task jointly. Concretely, we exploit a\ngraph-based method, regarding frame semantic parsing as a graph construction\nproblem. All predicates and roles are treated as graph nodes, and their\nrelations are taken as graph edges. Experiment results on two benchmark\ndatasets of frame semantic parsing show that our method is highly competitive,\nresulting in better performance than pipeline models.", "published": "2021-09-25 08:54:33", "link": "http://arxiv.org/abs/2109.12319v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph Reasoning with Context-Aware Linearization for Interpretable Fact\n  Extraction and Verification", "abstract": "This paper presents an end-to-end system for fact extraction and verification\nusing textual and tabular evidence, the performance of which we demonstrate on\nthe FEVEROUS dataset. We experiment with both a multi-task learning paradigm to\njointly train a graph attention network for both the task of evidence\nextraction and veracity prediction, as well as a single objective graph model\nfor solely learning veracity prediction and separate evidence extraction. In\nboth instances, we employ a framework for per-cell linearization of tabular\nevidence, thus allowing us to treat evidence from tables as sequences. The\ntemplates we employ for linearizing tables capture the context as well as the\ncontent of table data. We furthermore provide a case study to show the\ninterpretability our approach. Our best performing system achieves a FEVEROUS\nscore of 0.23 and 53% label accuracy on the blind test data.", "published": "2021-09-25 12:05:28", "link": "http://arxiv.org/abs/2109.12349v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Model Priming for Cross-Lingual Event Extraction", "abstract": "We present a novel, language-agnostic approach to \"priming\" language models\nfor the task of event extraction, providing particularly effective performance\nin low-resource and zero-shot cross-lingual settings. With priming, we augment\nthe input to the transformer stack's language model differently depending on\nthe question(s) being asked of the model at runtime. For instance, if the model\nis being asked to identify arguments for the trigger \"protested\", we will\nprovide that trigger as part of the input to the language model, allowing it to\nproduce different representations for candidate arguments than when it is asked\nabout arguments for the trigger \"arrest\" elsewhere in the same sentence. We\nshow that by enabling the language model to better compensate for the deficits\nof sparse and noisy training data, our approach improves both trigger and\nargument detection and classification significantly over the state of the art\nin a zero-shot cross-lingual setting.", "published": "2021-09-25 15:19:32", "link": "http://arxiv.org/abs/2109.12383v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sorting through the noise: Testing robustness of information processing\n  in pre-trained language models", "abstract": "Pre-trained LMs have shown impressive performance on downstream NLP tasks,\nbut we have yet to establish a clear understanding of their sophistication when\nit comes to processing, retaining, and applying information presented in their\ninput. In this paper we tackle a component of this question by examining\nrobustness of models' ability to deploy relevant context information in the\nface of distracting content. We present models with cloze tasks requiring use\nof critical context information, and introduce distracting content to test how\nrobustly the models retain and use that critical information for prediction. We\nalso systematically manipulate the nature of these distractors, to shed light\non dynamics of models' use of contextual cues. We find that although models\nappear in simple contexts to make predictions based on understanding and\napplying relevant facts from prior context, the presence of distracting but\nirrelevant content has clear impact in confusing model predictions. In\nparticular, models appear particularly susceptible to factors of semantic\nsimilarity and word position. The findings are consistent with the conclusion\nthat LM predictions are driven in large part by superficial contextual cues,\nrather than by robust representations of context meaning.", "published": "2021-09-25 16:02:23", "link": "http://arxiv.org/abs/2109.12393v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Self-Enhancing Multi-filter Sequence-to-Sequence Model", "abstract": "Representation learning is important for solving sequence-to-sequence\nproblems in natural language processing. Representation learning transforms raw\ndata into vector-form representations while preserving their features. However,\ndata with significantly different features leads to heterogeneity in their\nrepresentations, which may increase the difficulty of convergence. We design a\nmulti-filter encoder-decoder model to resolve the heterogeneity problem in\nsequence-to-sequence tasks. The multi-filter model divides the latent space\ninto subspaces using a clustering algorithm and trains a set of decoders\n(filters) in which each decoder only concentrates on the features from its\ncorresponding subspace. As for the main contribution, we design a\nself-enhancing mechanism that uses a reinforcement learning algorithm to\noptimize the clustering algorithm without additional training data. We run\nsemantic parsing and machine translation experiments to indicate that the\nproposed model can outperform most benchmarks by at least 5\\%. We also\nempirically show the self-enhancing mechanism can improve performance by over\n10\\% and provide evidence to demonstrate the positive correlation between the\nmodel's performance and the latent space clustering.", "published": "2021-09-25 16:36:31", "link": "http://arxiv.org/abs/2109.12399v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deciding Whether to Ask Clarifying Questions in Large-Scale Spoken\n  Language Understanding", "abstract": "A large-scale conversational agent can suffer from understanding user\nutterances with various ambiguities such as ASR ambiguity, intent ambiguity,\nand hypothesis ambiguity. When ambiguities are detected, the agent should\nengage in a clarifying dialog to resolve the ambiguities before committing to\nactions. However, asking clarifying questions for all the ambiguity occurrences\ncould lead to asking too many questions, essentially hampering the user\nexperience. To trigger clarifying questions only when necessary for the user\nsatisfaction, we propose a neural self-attentive model that leverages the\nhypotheses with ambiguities and contextual signals. We conduct extensive\nexperiments on five common ambiguity types using real data from a large-scale\ncommercial conversational agent and demonstrate significant improvement over a\nset of baseline approaches.", "published": "2021-09-25 22:32:10", "link": "http://arxiv.org/abs/2109.12451v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Selectively Learn for Weakly-supervised Paraphrase\n  Generation", "abstract": "Paraphrase generation is a longstanding NLP task that has diverse\napplications for downstream NLP tasks. However, the effectiveness of existing\nefforts predominantly relies on large amounts of golden labeled data. Though\nunsupervised endeavors have been proposed to address this issue, they may fail\nto generate meaningful paraphrases due to the lack of supervision signals. In\nthis work, we go beyond the existing paradigms and propose a novel approach to\ngenerate high-quality paraphrases with weak supervision data. Specifically, we\ntackle the weakly-supervised paraphrase generation problem by: (1) obtaining\nabundant weakly-labeled parallel sentences via retrieval-based pseudo\nparaphrase expansion; and (2) developing a meta-learning framework to\nprogressively select valuable samples for fine-tuning a pre-trained language\nmodel, i.e., BART, on the sentential paraphrasing task. We demonstrate that our\napproach achieves significant improvements over existing unsupervised\napproaches, and is even comparable in performance with supervised\nstate-of-the-arts.", "published": "2021-09-25 23:31:13", "link": "http://arxiv.org/abs/2109.12457v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pushing on Text Readability Assessment: A Transformer Meets Handcrafted\n  Linguistic Features", "abstract": "We report two essential improvements in readability assessment: 1. three\nnovel features in advanced semantics and 2. the timely evidence that\ntraditional ML models (e.g. Random Forest, using handcrafted features) can\ncombine with transformers (e.g. RoBERTa) to augment model performance. First,\nwe explore suitable transformers and traditional ML models. Then, we extract\n255 handcrafted linguistic features using self-developed extraction software.\nFinally, we assemble those to create several hybrid models, achieving\nstate-of-the-art (SOTA) accuracy on popular datasets in readability assessment.\nThe use of handcrafted features help model performance on smaller datasets.\nNotably, our RoBERTA-RF-T1 hybrid achieves the near-perfect classification\naccuracy of 99%, a 20.3% increase from the previous SOTA.", "published": "2021-09-25 01:48:22", "link": "http://arxiv.org/abs/2109.12258v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "More Than Reading Comprehension: A Survey on Datasets and Metrics of\n  Textual Question Answering", "abstract": "Textual Question Answering (QA) aims to provide precise answers to user's\nquestions in natural language using unstructured data. One of the most popular\napproaches to this goal is machine reading comprehension(MRC). In recent years,\nmany novel datasets and evaluation metrics based on classical MRC tasks have\nbeen proposed for broader textual QA tasks. In this paper, we survey 47 recent\ntextual QA benchmark datasets and propose a new taxonomy from an application\npoint of view. In addition, We summarize 8 evaluation metrics of textual QA\ntasks. Finally, we discuss current trends in constructing textual QA benchmarks\nand suggest directions for future work.", "published": "2021-09-25 02:36:53", "link": "http://arxiv.org/abs/2109.12264v2", "categories": ["cs.CL", "cs.AI", "68T50"], "primary_category": "cs.CL"}
{"title": "Finetuning Transformer Models to Build ASAG System", "abstract": "Research towards creating systems for automatic grading of student answers to\nquiz and exam questions in educational settings has been ongoing since 1966.\nOver the years, the problem was divided into many categories. Among them,\ngrading text answers were divided into short answer grading, and essay grading.\nThe goal of this work was to develop an ML-based short answer grading system. I\nhence built a system which uses finetuning on Roberta Large Model pretrained on\nSTS benchmark dataset and have also created an interface to show the production\nreadiness of the system. I evaluated the performance of the system on the\nMohler extended dataset and SciEntsBank Dataset. The developed system achieved\na Pearsons Correlation of 0.82 and RMSE of 0.7 on the Mohler Dataset which\nbeats the SOTA performance on this dataset which is correlation of 0.805 and\nRMSE of 0.793. Additionally, Pearsons Correlation of 0.79 and RMSE of 0.56 was\nachieved on the SciEntsBank Dataset, which only reconfirms the robustness of\nthe system. A few observations during achieving these results included usage of\nbatch size of 1 produced better results than using batch size of 16 or 32 and\nusing huber loss as loss function performed well on this regression task. The\nsystem was tried and tested on train and validation splits using various random\nseeds and still has been tweaked to achieve a minimum of 0.76 of correlation\nand a maximum 0.15 (out of 1) RMSE on any dataset.", "published": "2021-09-25 07:21:04", "link": "http://arxiv.org/abs/2109.12300v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Neural Templates for Recommender Dialogue System", "abstract": "Though recent end-to-end neural models have shown promising progress on\nConversational Recommender System (CRS), two key challenges still remain.\nFirst, the recommended items cannot be always incorporated into the generated\nreplies precisely and appropriately. Second, only the items mentioned in the\ntraining corpus have a chance to be recommended in the conversation. To tackle\nthese challenges, we introduce a novel framework called NTRD for recommender\ndialogue system that decouples the dialogue generation from the item\nrecommendation. NTRD has two key components, i.e., response template generator\nand item selector. The former adopts an encoder-decoder model to generate a\nresponse template with slot locations tied to target items, while the latter\nfills in slot locations with the proper items using a sufficient attention\nmechanism. Our approach combines the strengths of both classical slot filling\napproaches (that are generally controllable) and modern neural NLG approaches\n(that are generally more natural and accurate). Extensive experiments on the\nbenchmark ReDial show our NTRD significantly outperforms the previous\nstate-of-the-art methods. Besides, our approach has the unique advantage to\nproduce novel items that do not appear in the training set of dialogue corpus.\nThe code is available at \\url{https://github.com/jokieleung/NTRD}.", "published": "2021-09-25 07:33:07", "link": "http://arxiv.org/abs/2109.12302v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DziriBERT: a Pre-trained Language Model for the Algerian Dialect", "abstract": "Pre-trained transformers are now the de facto models in Natural Language\nProcessing given their state-of-the-art results in many tasks and languages.\nHowever, most of the current models have been trained on languages for which\nlarge text resources are already available (such as English, French, Arabic,\netc.). Therefore, there are still a number of low-resource languages that need\nmore attention from the community. In this paper, we study the Algerian dialect\nwhich has several specificities that make the use of Arabic or multilingual\nmodels inappropriate. To address this issue, we collected more than one million\nAlgerian tweets, and pre-trained the first Algerian language model: DziriBERT.\nWhen compared with existing models, DziriBERT achieves better results,\nespecially when dealing with the Roman script. The obtained results show that\npre-training a dedicated model on a small dataset (150 MB) can outperform\nexisting models that have been trained on much more data (hundreds of GB).\nFinally, our model is publicly available to the community.", "published": "2021-09-25 11:51:35", "link": "http://arxiv.org/abs/2109.12346v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Coreference Resolution for the Biomedical Domain: A Survey", "abstract": "Issues with coreference resolution are one of the most frequently mentioned\nchallenges for information extraction from the biomedical literature. Thus, the\nbiomedical genre has long been the second most researched genre for coreference\nresolution after the news domain, and the subject of a great deal of research\nfor NLP in general. In recent years this interest has grown enormously leading\nto the development of a number of substantial datasets, of domain-specific\ncontextual language models, and of several architectures. In this paper we\nreview the state-of-the-art of coreference in the biomedical domain with a\nparticular attention on these most recent developments.", "published": "2021-09-25 19:09:47", "link": "http://arxiv.org/abs/2109.12424v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improved statistical machine translation using monolingual paraphrases", "abstract": "We propose a novel monolingual sentence paraphrasing method for augmenting\nthe training data for statistical machine translation systems \"for free\" -- by\ncreating it from data that is already available rather than having to create\nmore aligned data. Starting with a syntactic tree, we recursively generate new\nsentence variants where noun compounds are paraphrased using suitable\nprepositions, and vice-versa -- preposition-containing noun phrases are turned\ninto noun compounds. The evaluation shows an improvement equivalent to 33%-50%\nof that of doubling the amount of training data.", "published": "2021-09-25 16:29:47", "link": "http://arxiv.org/abs/2109.15119v1", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "MINIMAL: Mining Models for Data Free Universal Adversarial Triggers", "abstract": "It is well known that natural language models are vulnerable to adversarial\nattacks, which are mostly input-specific in nature. Recently, it has been shown\nthat there also exist input-agnostic attacks in NLP models, called universal\nadversarial triggers. However, existing methods to craft universal triggers are\ndata intensive. They require large amounts of data samples to generate\nadversarial triggers, which are typically inaccessible by attackers. For\ninstance, previous works take 3000 data samples per class for the SNLI dataset\nto generate adversarial triggers. In this paper, we present a novel data-free\napproach, MINIMAL, to mine input-agnostic adversarial triggers from models.\nUsing the triggers produced with our data-free algorithm, we reduce the\naccuracy of Stanford Sentiment Treebank's positive class from 93.6% to 9.6%.\nSimilarly, for the Stanford Natural Language Inference (SNLI), our single-word\ntrigger reduces the accuracy of the entailment class from 90.95% to less than\n0.6\\%. Despite being completely data-free, we get equivalent accuracy drops as\ndata-dependent methods.", "published": "2021-09-25 17:24:48", "link": "http://arxiv.org/abs/2109.12406v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Overview of the CLEF-2019 CheckThat!: Automatic Identification and\n  Verification of Claims", "abstract": "We present an overview of the second edition of the CheckThat! Lab at CLEF\n2019. The lab featured two tasks in two different languages: English and\nArabic. Task 1 (English) challenged the participating systems to predict which\nclaims in a political debate or speech should be prioritized for fact-checking.\nTask 2 (Arabic) asked to (A) rank a given set of Web pages with respect to a\ncheck-worthy claim based on their usefulness for fact-checking that claim, (B)\nclassify these same Web pages according to their degree of usefulness for\nfact-checking the target claim, (C) identify useful passages from these pages,\nand (D) use the useful pages to predict the claim's factuality. CheckThat!\nprovided a full evaluation framework, consisting of data in English (derived\nfrom fact-checking sources) and Arabic (gathered and annotated from scratch)\nand evaluation based on mean average precision (MAP) and normalized discounted\ncumulative gain (nDCG) for ranking, and F1 for classification. A total of 47\nteams registered to participate in this lab, and fourteen of them actually\nsubmitted runs (compared to nine last year). The evaluation results show that\nthe most successful approaches to Task 1 used various neural networks and\nlogistic regression. As for Task 2, learning-to-rank was used by the highest\nscoring runs for subtask A, while different classifiers were used in the other\nsubtasks. We release to the research community all datasets from the lab as\nwell as the evaluation scripts, which should enable further research in the\nimportant tasks of check-worthiness estimation and automatic claim\nverification.", "published": "2021-09-25 16:08:09", "link": "http://arxiv.org/abs/2109.15118v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.SI", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
