{"title": "indicnlp@kgp at DravidianLangTech-EACL2021: Offensive Language\n  Identification in Dravidian Languages", "abstract": "The paper presents the submission of the team indicnlp@kgp to the EACL 2021\nshared task \"Offensive Language Identification in Dravidian Languages.\" The\ntask aimed to classify different offensive content types in 3 code-mixed\nDravidian language datasets. The work leverages existing state of the art\napproaches in text classification by incorporating additional data and transfer\nlearning on pre-trained models. Our final submission is an ensemble of an\nAWD-LSTM based model along with 2 different transformer model architectures\nbased on BERT and RoBERTa. We achieved weighted-average F1 scores of 0.97,\n0.77, and 0.72 in the Malayalam-English, Tamil-English, and Kannada-English\ndatasets ranking 1st, 2nd, and 3rd on the respective tasks.", "published": "2021-02-14 13:24:01", "link": "http://arxiv.org/abs/2102.07150v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Error-driven Pruning of Language Models for Virtual Assistants", "abstract": "Language models (LMs) for virtual assistants (VAs) are typically trained on\nlarge amounts of data, resulting in prohibitively large models which require\nexcessive memory and/or cannot be used to serve user requests in real-time.\nEntropy pruning results in smaller models but with significant degradation of\neffectiveness in the tail of the user request distribution. We customize\nentropy pruning by allowing for a keep list of infrequent n-grams that require\na more relaxed pruning threshold, and propose three methods to construct the\nkeep list. Each method has its own advantages and disadvantages with respect to\nLM size, ASR accuracy and cost of constructing the keep list. Our best LM gives\n8% average Word Error Rate (WER) reduction on a targeted test set, but is 3\ntimes larger than the baseline. We also propose discriminative methods to\nreduce the size of the LM while retaining the majority of the WER gains\nachieved by the largest LM.", "published": "2021-02-14 18:47:01", "link": "http://arxiv.org/abs/2102.07219v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query-by-Example Keyword Spotting system using Multi-head Attention and\n  Softtriple Loss", "abstract": "This paper proposes a neural network architecture for tackling the\nquery-by-example user-defined keyword spotting task. A multi-head attention\nmodule is added on top of a multi-layered GRU for effective feature extraction,\nand a normalized multi-head attention module is proposed for feature\naggregation. We also adopt the softtriple loss - a combination of triplet loss\nand softmax loss - and showcase its effectiveness. We demonstrate the\nperformance of our model on internal datasets with different languages and the\npublic Hey-Snips dataset. We compare the performance of our model to a baseline\nsystem and conduct an ablation study to show the benefit of each component in\nour architecture. The proposed work shows solid performance while preserving\nsimplicity.", "published": "2021-02-14 03:37:37", "link": "http://arxiv.org/abs/2102.07061v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reasoning Over Virtual Knowledge Bases With Open Predicate Relations", "abstract": "We present the Open Predicate Query Language (OPQL); a method for\nconstructing a virtual KB (VKB) trained entirely from text. Large Knowledge\nBases (KBs) are indispensable for a wide-range of industry applications such as\nquestion answering and recommendation. Typically, KBs encode world knowledge in\na structured, readily accessible form derived from laborious human annotation\nefforts. Unfortunately, while they are extremely high precision, KBs are\ninevitably highly incomplete and automated methods for enriching them are far\ntoo inaccurate. Instead, OPQL constructs a VKB by encoding and indexing a set\nof relation mentions in a way that naturally enables reasoning and can be\ntrained without any structured supervision. We demonstrate that OPQL\noutperforms prior VKB methods on two different KB reasoning tasks and,\nadditionally, can be used as an external memory integrated into a language\nmodel (OPQL-LM) leading to improvements on two open-domain question answering\ntasks.", "published": "2021-02-14 01:29:54", "link": "http://arxiv.org/abs/2102.07043v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Thank you for Attention: A survey on Attention-based Artificial Neural\n  Networks for Automatic Speech Recognition", "abstract": "Attention is a very popular and effective mechanism in artificial neural\nnetwork-based sequence-to-sequence models. In this survey paper, a\ncomprehensive review of the different attention models used in developing\nautomatic speech recognition systems is provided. The paper focuses on the\ndevelopment and evolution of attention models for offline and streaming speech\nrecognition within recurrent neural network- and Transformer- based\narchitectures.", "published": "2021-02-14 22:28:55", "link": "http://arxiv.org/abs/2102.07259v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Image Captioning using Multiple Transformers for Self-Attention\n  Mechanism", "abstract": "Real-time image captioning, along with adequate precision, is the main\nchallenge of this research field. The present work, Multiple Transformers for\nSelf-Attention Mechanism (MTSM), utilizes multiple transformers to address\nthese problems. The proposed algorithm, MTSM, acquires region proposals using a\ntransformer detector (DETR). Consequently, MTSM achieves the self-attention\nmechanism by transferring these region proposals and their visual and\ngeometrical features through another transformer and learns the objects' local\nand global interconnections. The qualitative and quantitative results of the\nproposed algorithm, MTSM, are shown on the MSCOCO dataset.", "published": "2021-02-14 05:35:54", "link": "http://arxiv.org/abs/2103.05103v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Inverted Vocal Tract Variables and Facial Action Units to Quantify\n  Neuromotor Coordination in Schizophrenia", "abstract": "This study investigates the speech articulatory coordination in schizophrenia\nsubjects exhibiting strong positive symptoms (e.g.hallucinations and\ndelusions), using a time delay embedded correlation analysis. We show that the\nschizophrenia subjects with strong positive symptoms and who are markedly ill\npose complex coordination patterns in facial and speech gestures than what is\nobserved in healthy subjects. This observation is in contrast to what previous\nstudies have shown in Major Depressive Disorder (MDD), where subjects with MDD\nshow a simpler coordination pattern with respect to healthy controls or\nsubjects in remission. This difference is not surprising given MDD is\nnecessarily accompanied by Psychomotor slowing (i.e.,negative symptoms) which\naffects speech, ideation and motility. With respect to speech, psychomotor\nslowing results in slowed speech with more and longer pauses than what occurs\nin speech from the same speaker when they are in remission and from a healthy\nsubject. Time delay embedded correlation analysis has been used to quantify the\ndifferences in coordination patterns of speech articulation. The current study\nis based on 17 Facial Action Units (FAUs) extracted from video data and 6 Vocal\nTract Variables (TVs) obtained from simultaneously recorded audio data. The TVs\nare extracted using a speech inversion system based on articulatory phonology\nthat maps the acoustic signal to vocal tract variables. The high-level time\ndelay embedded correlation features computed from TVs and FAUs are used to\ntrain a stacking ensemble classifier fusing audio and video modalities. The\nresults show that there is a promising distinction between healthy and\nschizophrenia subjects (with strong positive symptoms) in terms of neuromotor\ncoordination in speech.", "published": "2021-02-14 02:50:12", "link": "http://arxiv.org/abs/2102.07054v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Adversarial defense for automatic speaker verification by cascaded\n  self-supervised learning models", "abstract": "Automatic speaker verification (ASV) is one of the core technologies in\nbiometric identification. With the ubiquitous usage of ASV systems in\nsafety-critical applications, more and more malicious attackers attempt to\nlaunch adversarial attacks at ASV systems. In the midst of the arms race\nbetween attack and defense in ASV, how to effectively improve the robustness of\nASV against adversarial attacks remains an open question. We note that the\nself-supervised learning models possess the ability to mitigate superficial\nperturbations in the input after pretraining. Hence, with the goal of effective\ndefense in ASV against adversarial attacks, we propose a standard and\nattack-agnostic method based on cascaded self-supervised learning models to\npurify the adversarial perturbations. Experimental results demonstrate that the\nproposed method achieves effective defense performance and can successfully\ncounter adversarial attacks in scenarios where attackers may either be aware or\nunaware of the self-supervised learning models.", "published": "2021-02-14 01:56:43", "link": "http://arxiv.org/abs/2102.07047v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Parametric Optimization of Violin Top Plates using Machine Learning", "abstract": "We recently developed a neural network that receives as input the geometrical\nand mechanical parameters that define a violin top plate and gives as output\nits first ten eigenfrequencies computed in free boundary conditions. In this\nmanuscript, we use the network to optimize several error functions, with the\ngoal of analyzing the relationship between the eigenspectrum problem for violin\ntop plates and their geometry. First, we focus on the violin outline. Given a\nvibratory feature, we find which is the best geometry of the plate to obtain\nit. Second, we investigate whether, from the vibrational point of view, a\nchange in the outline shape can be compensated by one in the thickness\ndistribution and vice versa. Finally, we analyze how to modify the violin shape\nto keep its response constant as its material properties vary. This is an\noriginal technique in musical acoustics, where artificial intelligence is not\nwidely used yet. It allows us to both compute the vibrational behavior of an\ninstrument from its geometry and optimize its shape for a given response.\nFurthermore, this method can be of great help to violin makers, who can thus\neasily understand the effects of the geometry changes in the violins they\nbuild, shedding light on one of the most relevant and, at the same time, less\nunderstood aspects of the construction process of musical instruments.", "published": "2021-02-14 11:54:21", "link": "http://arxiv.org/abs/2102.07133v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
