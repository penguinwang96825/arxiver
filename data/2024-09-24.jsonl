{"title": "Transfer learning for financial data predictions: a systematic review", "abstract": "Literature highlighted that financial time series data pose significant\nchallenges for accurate stock price prediction, because these data are\ncharacterized by noise and susceptibility to news; traditional statistical\nmethodologies made assumptions, such as linearity and normality, which are not\nsuitable for the non-linear nature of financial time series; on the other hand,\nmachine learning methodologies are able to capture non linear relationship in\nthe data. To date, neural network is considered the main machine learning tool\nfor the financial prices prediction. Transfer Learning, as a method aimed at\ntransferring knowledge from source tasks to target tasks, can represent a very\nuseful methodological tool for getting better financial prediction capability.\nCurrent reviews on the above body of knowledge are mainly focused on neural\nnetwork architectures, for financial prediction, with very little emphasis on\nthe transfer learning methodology; thus, this paper is aimed at going deeper on\nthis topic by developing a systematic review with respect to application of\nTransfer Learning for financial market predictions and to challenges/potential\nfuture directions of the transfer learning methodologies for stock market\npredictions.", "published": "2024-09-24 20:52:32", "link": "http://arxiv.org/abs/2409.17183v1", "categories": ["q-fin.TR", "cs.AI", "cs.LG", "q-fin.CP"], "primary_category": "q-fin.TR"}
{"title": "Predicting Distance matrix with large language models", "abstract": "Structural prediction has long been considered critical in RNA research,\nespecially following the success of AlphaFold2 in protein studies, which has\ndrawn significant attention to the field. While recent advances in machine\nlearning and data accumulation have effectively addressed many biological\ntasks, particularly in protein related research. RNA structure prediction\nremains a significant challenge due to data limitations. Obtaining RNA\nstructural data is difficult because traditional methods such as nuclear\nmagnetic resonance spectroscopy, Xray crystallography, and electron microscopy\nare expensive and time consuming. Although several RNA 3D structure prediction\nmethods have been proposed, their accuracy is still limited. Predicting RNA\nstructural information at another level, such as distance maps, remains highly\nvaluable. Distance maps provide a simplified representation of spatial\nconstraints between nucleotides, capturing essential relationships without\nrequiring a full 3D model. This intermediate level of structural information\ncan guide more accurate 3D modeling and is computationally less intensive,\nmaking it a useful tool for improving structural predictions. In this work, we\ndemonstrate that using only primary sequence information, we can accurately\ninfer the distances between RNA bases by utilizing a large pretrained RNA\nlanguage model coupled with a well trained downstream transformer.", "published": "2024-09-24 10:28:55", "link": "http://arxiv.org/abs/2409.16333v1", "categories": ["q-bio.BM", "cs.CV", "cs.LG", "q-fin.CP"], "primary_category": "q-bio.BM"}
{"title": "Lighter And Better: Towards Flexible Context Adaptation For Retrieval\n  Augmented Generation", "abstract": "The existing Retrieval-Augmented Generation (RAG) systems face significant\nchallenges in terms of cost and effectiveness. On one hand, they need to encode\nthe lengthy retrieved contexts before responding to the input tasks, which\nimposes substantial computational overhead. On the other hand, directly using\ngeneric Large Language Models (LLMs) often leads to sub-optimal answers, while\ntask-specific fine-tuning may compromise the LLMs' general capabilities. To\naddress these challenges, we introduce a novel approach called FlexRAG\n(Flexible Context Adaptation for RAG). In this approach, the retrieved contexts\nare compressed into compact embeddings before being encoded by the LLMs.\nSimultaneously, these compressed embeddings are optimized to enhance downstream\nRAG performance. A key feature of FlexRAG is its flexibility, which enables\neffective support for diverse compression ratios and selective preservation of\nimportant contexts. Thanks to these technical designs, FlexRAG achieves\nsuperior generation quality while significantly reducing running costs.\nComprehensive experiments on various question-answering datasets validate our\napproach as a cost-effective and flexible solution for RAG systems.", "published": "2024-09-24 03:25:36", "link": "http://arxiv.org/abs/2409.15699v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XTRUST: On the Multilingual Trustworthiness of Large Language Models", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\na range of natural language processing (NLP) tasks, capturing the attention of\nboth practitioners and the broader public. A key question that now preoccupies\nthe AI community concerns the capabilities and limitations of these models,\nwith trustworthiness emerging as a central issue, particularly as LLMs are\nincreasingly applied in sensitive fields like healthcare and finance, where\nerrors can have serious consequences. However, most previous studies on the\ntrustworthiness of LLMs have been limited to a single language, typically the\npredominant one in the dataset, such as English. In response to the growing\nglobal deployment of LLMs, we introduce XTRUST, the first comprehensive\nmultilingual trustworthiness benchmark. XTRUST encompasses a diverse range of\ntopics, including illegal activities, hallucination, out-of-distribution (OOD)\nrobustness, physical and mental health, toxicity, fairness, misinformation,\nprivacy, and machine ethics, across 10 different languages. Using XTRUST, we\nconduct an empirical evaluation of the multilingual trustworthiness of five\nwidely used LLMs, offering an in-depth analysis of their performance across\nlanguages and tasks. Our results indicate that many LLMs struggle with certain\nlow-resource languages, such as Arabic and Russian, highlighting the\nconsiderable room for improvement in the multilingual trustworthiness of\ncurrent language models. The code is available at\nhttps://github.com/LluckyYH/XTRUST.", "published": "2024-09-24 05:38:33", "link": "http://arxiv.org/abs/2409.15762v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CHBench: A Chinese Dataset for Evaluating Health in Large Language\n  Models", "abstract": "With the rapid development of large language models (LLMs), assessing their\nperformance on health-related inquiries has become increasingly essential. The\nuse of these models in real-world contexts-where misinformation can lead to\nserious consequences for individuals seeking medical advice and\nsupport-necessitates a rigorous focus on safety and trustworthiness. In this\nwork, we introduce CHBench, the first comprehensive safety-oriented Chinese\nhealth-related benchmark designed to evaluate LLMs' capabilities in\nunderstanding and addressing physical and mental health issues with a safety\nperspective across diverse scenarios. CHBench comprises 6,493 entries on mental\nhealth and 2,999 entries on physical health, spanning a wide range of topics.\nOur extensive evaluations of four popular Chinese LLMs highlight significant\ngaps in their capacity to deliver safe and accurate health information,\nunderscoring the urgent need for further advancements in this critical domain.\nThe code is available at https://github.com/TracyGuo2001/CHBench.", "published": "2024-09-24 05:44:46", "link": "http://arxiv.org/abs/2409.15766v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NER-Luxury: Named entity recognition for the fashion and luxury domain", "abstract": "In this study, we address multiple challenges of developing a named-entity\nrecognition model in English for the fashion and luxury industry, namely the\nentity disambiguation, French technical jargon in multiple sub-sectors,\nscarcity of the ESG methodology, and a disparate company structures of the\nsector with small and medium-sized luxury houses to large conglomerate\nleveraging economy of scale.\n  In this work, we introduce a taxonomy of 36+ entity types with a\nluxury-oriented annotation scheme, and create a dataset of more than 40K\nsentences respecting a clear hierarchical classification. We also present five\nsupervised fine-tuned models NER-Luxury for fashion, beauty, watches, jewelry,\nfragrances, cosmetics, and overall luxury, focusing equally on the aesthetic\nside and the quantitative side.\n  In an additional experiment, we compare in a quantitative empirical\nassessment of the NER performance of our models against the state-of-the-art\nopen-source large language models that show promising results and highlights\nthe benefits of incorporating a bespoke NER model in existing machine learning\npipelines.", "published": "2024-09-24 06:58:28", "link": "http://arxiv.org/abs/2409.15804v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling Language Competence Neurons: A Psycholinguistic Approach to\n  Model Interpretability", "abstract": "As large language models (LLMs) advance in their linguistic capacity,\nunderstanding how they capture aspects of language competence remains a\nsignificant challenge. This study therefore employs psycholinguistic paradigms\nin English, which are well-suited for probing deeper cognitive aspects of\nlanguage processing, to explore neuron-level representations in language model\nacross three tasks: sound-shape association, sound-gender association, and\nimplicit causality. Our findings indicate that while GPT-2-XL struggles with\nthe sound-shape task, it demonstrates human-like abilities in both sound-gender\nassociation and implicit causality. Targeted neuron ablation and activation\nmanipulation reveal a crucial relationship: When GPT-2-XL displays a linguistic\nability, specific neurons correspond to that competence; conversely, the\nabsence of such an ability indicates a lack of specialized neurons. This study\nis the first to utilize psycholinguistic experiments to investigate deep\nlanguage competence at the neuron level, providing a new level of granularity\nin model interpretability and insights into the internal mechanisms driving\nlanguage ability in the transformer-based LLM.", "published": "2024-09-24 07:40:33", "link": "http://arxiv.org/abs/2409.15827v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HLB: Benchmarking LLMs' Humanlikeness in Language Use", "abstract": "As synthetic data becomes increasingly prevalent in training language models,\nparticularly through generated dialogue, concerns have emerged that these\nmodels may deviate from authentic human language patterns, potentially losing\nthe richness and creativity inherent in human communication. This highlights\nthe critical need to assess the humanlikeness of language models in real-world\nlanguage use. In this paper, we present a comprehensive humanlikeness benchmark\n(HLB) evaluating 20 large language models (LLMs) using 10 psycholinguistic\nexperiments designed to probe core linguistic aspects, including sound, word,\nsyntax, semantics, and discourse (see\nhttps://huggingface.co/spaces/XufengDuan/HumanLikeness). To anchor these\ncomparisons, we collected responses from over 2,000 human participants and\ncompared them to outputs from the LLMs in these experiments.\n  For rigorous evaluation, we developed a coding algorithm that accurately\nidentified language use patterns, enabling the extraction of response\ndistributions for each task. By comparing the response distributions between\nhuman participants and LLMs, we quantified humanlikeness through distributional\nsimilarity. Our results reveal fine-grained differences in how well LLMs\nreplicate human responses across various linguistic levels. Importantly, we\nfound that improvements in other performance metrics did not necessarily lead\nto greater humanlikeness, and in some cases, even resulted in a decline. By\nintroducing psycholinguistic methods to model evaluation, this benchmark offers\nthe first framework for systematically assessing the humanlikeness of LLMs in\nlanguage use.", "published": "2024-09-24 09:02:28", "link": "http://arxiv.org/abs/2409.15890v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Konstruktor: A Strong Baseline for Simple Knowledge Graph Question\n  Answering", "abstract": "While being one of the most popular question types, simple questions such as\n\"Who is the author of Cinderella?\", are still not completely solved.\nSurprisingly, even the most powerful modern Large Language Models are prone to\nerrors when dealing with such questions, especially when dealing with rare\nentities. At the same time, as an answer may be one hop away from the question\nentity, one can try to develop a method that uses structured knowledge graphs\n(KGs) to answer such questions. In this paper, we introduce Konstruktor - an\nefficient and robust approach that breaks down the problem into three steps:\n(i) entity extraction and entity linking, (ii) relation prediction, and (iii)\nquerying the knowledge graph. Our approach integrates language models and\nknowledge graphs, exploiting the power of the former and the interpretability\nof the latter. We experiment with two named entity recognition and entity\nlinking methods and several relation detection techniques. We show that for\nrelation detection, the most challenging step of the workflow, a combination of\nrelation classification/generation and ranking outperforms other methods. We\nreport Konstruktor's strong results on four datasets.", "published": "2024-09-24 09:19:11", "link": "http://arxiv.org/abs/2409.15902v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explaining word embeddings with perfect fidelity: Case study in research\n  impact prediction", "abstract": "Best performing approaches for scholarly document quality prediction are\nbased on embedding models, which do not allow direct explanation of classifiers\nas distinct words no longer correspond to the input features for model\ntraining. Although model-agnostic explanation methods such as Local\ninterpretable model-agnostic explanations (LIME) can be applied, these produce\nresults with questionable correspondence to the ML model. We introduce a new\nfeature importance method, Self-model Rated Entities (SMER), for logistic\nregression-based classification models trained on word embeddings. We show that\nSMER has theoretically perfect fidelity with the explained model, as its\nprediction corresponds exactly to the average of predictions for individual\nwords in the text. SMER allows us to reliably determine which words or entities\npositively contribute to predicting impactful articles. Quantitative and\nqualitative evaluation is performed through five diverse experiments conducted\non 50.000 research papers from the CORD-19 corpus. Through an AOPC curve\nanalysis, we experimentally demonstrate that SMER produces better explanations\nthan LIME for logistic regression.", "published": "2024-09-24 09:28:24", "link": "http://arxiv.org/abs/2409.15912v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tuning Into Bias: A Computational Study of Gender Bias in Song Lyrics", "abstract": "The application of text mining methods is becoming increasingly prevalent,\nparticularly within Humanities and Computational Social Sciences, as well as in\na broader range of disciplines. This paper presents an analysis of gender bias\nin English song lyrics using topic modeling and bias measurement techniques.\nLeveraging BERTopic, we cluster a dataset of 537,553 English songs into\ndistinct topics and analyze their temporal evolution. Our results reveal a\nsignificant thematic shift in song lyrics over time, transitioning from\nromantic themes to a heightened focus on the sexualization of women.\nAdditionally, we observe a substantial prevalence of profanity and misogynistic\ncontent across various topics, with a particularly high concentration in the\nlargest thematic cluster. To further analyse gender bias across topics and\ngenres in a quantitative way, we employ the Single Category Word Embedding\nAssociation Test (SC-WEAT) to calculate bias scores for word embeddings trained\non the most prominent topics as well as individual genres. The results indicate\na consistent male bias in words associated with intelligence and strength,\nwhile appearance and weakness words show a female bias. Further analysis\nhighlights variations in these biases across topics, illustrating the interplay\nbetween thematic content and gender stereotypes in song lyrics.", "published": "2024-09-24 10:24:53", "link": "http://arxiv.org/abs/2409.15949v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finetuning LLMs for Comparative Assessment Tasks", "abstract": "Automated assessment in natural language generation is a challenging task.\nInstruction-tuned large language models (LLMs) have shown promise in\nreference-free evaluation, particularly through comparative assessment.\nHowever, the quadratic computational complexity of pairwise comparisons limits\nits scalability. To address this, efficient comparative assessment has been\nexplored by applying comparative strategies on zero-shot LLM probabilities. We\npropose a framework for finetuning LLMs for comparative assessment to align the\nmodel's output with the target distribution of comparative probabilities. By\ntraining on soft probabilities, our approach improves state-of-the-art\nperformance while maintaining high performance with an efficient subset of\ncomparisons.", "published": "2024-09-24 11:21:43", "link": "http://arxiv.org/abs/2409.15979v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlocking Markets: A Multilingual Benchmark to Cross-Market Question\n  Answering", "abstract": "Users post numerous product-related questions on e-commerce platforms,\naffecting their purchase decisions. Product-related question answering (PQA)\nentails utilizing product-related resources to provide precise responses to\nusers. We propose a novel task of Multilingual Cross-market Product-based\nQuestion Answering (MCPQA) and define the task as providing answers to\nproduct-related questions in a main marketplace by utilizing information from\nanother resource-rich auxiliary marketplace in a multilingual context. We\nintroduce a large-scale dataset comprising over 7 million questions from 17\nmarketplaces across 11 languages. We then perform automatic translation on the\nElectronics category of our dataset, naming it as McMarket. We focus on two\nsubtasks: review-based answer generation and product-related question ranking.\nFor each subtask, we label a subset of McMarket using an LLM and further\nevaluate the quality of the annotations via human assessment. We then conduct\nexperiments to benchmark our dataset, using models ranging from traditional\nlexical models to LLMs in both single-market and cross-market scenarios across\nMcMarket and the corresponding LLM subset. Results show that incorporating\ncross-market information significantly enhances performance in both tasks.", "published": "2024-09-24 12:24:34", "link": "http://arxiv.org/abs/2409.16025v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlling Risk of Retrieval-augmented Generation: A Counterfactual\n  Prompting Framework", "abstract": "Retrieval-augmented generation (RAG) has emerged as a popular solution to\nmitigate the hallucination issues of large language models. However, existing\nstudies on RAG seldom address the issue of predictive uncertainty, i.e., how\nlikely it is that a RAG model's prediction is incorrect, resulting in\nuncontrollable risks in real-world applications. In this work, we emphasize the\nimportance of risk control, ensuring that RAG models proactively refuse to\nanswer questions with low confidence. Our research identifies two critical\nlatent factors affecting RAG's confidence in its predictions: the quality of\nthe retrieved results and the manner in which these results are utilized. To\nguide RAG models in assessing their own confidence based on these two latent\nfactors, we develop a counterfactual prompting framework that induces the\nmodels to alter these factors and analyzes the effect on their answers. We also\nintroduce a benchmarking procedure to collect answers with the option to\nabstain, facilitating a series of experiments. For evaluation, we introduce\nseveral risk-related metrics and the experimental results demonstrate the\neffectiveness of our approach. Our code and benchmark dataset are available at\nhttps://github.com/ict-bigdatalab/RC-RAG.", "published": "2024-09-24 14:52:14", "link": "http://arxiv.org/abs/2409.16146v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HelloBench: Evaluating Long Text Generation Capabilities of Large\n  Language Models", "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities in various tasks (e.g., long-context understanding), and many\nbenchmarks have been proposed. However, we observe that long text generation\ncapabilities are not well investigated. Therefore, we introduce the\nHierarchical Long Text Generation Benchmark (HelloBench), a comprehensive,\nin-the-wild, and open-ended benchmark to evaluate LLMs' performance in\ngenerating long text. Based on Bloom's Taxonomy, HelloBench categorizes long\ntext generation tasks into five subtasks: open-ended QA, summarization, chat,\ntext completion, and heuristic text generation. Besides, we propose\nHierarchical Long Text Evaluation (HelloEval), a human-aligned evaluation\nmethod that significantly reduces the time and effort required for human\nevaluation while maintaining a high correlation with human evaluation. We have\nconducted extensive experiments across around 30 mainstream LLMs and observed\nthat the current LLMs lack long text generation capabilities. Specifically,\nfirst, regardless of whether the instructions include explicit or implicit\nlength constraints, we observe that most LLMs cannot generate text that is\nlonger than 4000 words. Second, we observe that while some LLMs can generate\nlonger text, many issues exist (e.g., severe repetition and quality\ndegradation). Third, to demonstrate the effectiveness of HelloEval, we compare\nHelloEval with traditional metrics (e.g., ROUGE, BLEU, etc.) and LLM-as-a-Judge\nmethods, which show that HelloEval has the highest correlation with human\nevaluation. We release our code in https://github.com/Quehry/HelloBench.", "published": "2024-09-24 15:38:11", "link": "http://arxiv.org/abs/2409.16191v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EuroLLM: Multilingual Language Models for Europe", "abstract": "The quality of open-weight LLMs has seen significant improvement, yet they\nremain predominantly focused on English. In this paper, we introduce the\nEuroLLM project, aimed at developing a suite of open-weight multilingual LLMs\ncapable of understanding and generating text in all official European Union\nlanguages, as well as several additional relevant languages. We outline the\nprogress made to date, detailing our data collection and filtering process, the\ndevelopment of scaling laws, the creation of our multilingual tokenizer, and\nthe data mix and modeling configurations. Additionally, we release our initial\nmodels: EuroLLM-1.7B and EuroLLM-1.7B-Instruct and report their performance on\nmultilingual general benchmarks and machine translation.", "published": "2024-09-24 16:51:36", "link": "http://arxiv.org/abs/2409.16235v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A fast and sound tagging method for discontinuous named-entity\n  recognition", "abstract": "We introduce a novel tagging scheme for discontinuous named entity\nrecognition based on an explicit description of the inner structure of\ndiscontinuous mentions. We rely on a weighted finite state automaton for both\nmarginal and maximum a posteriori inference. As such, our method is sound in\nthe sense that (1) well-formedness of predicted tag sequences is ensured via\nthe automaton structure and (2) there is an unambiguous mapping between\nwell-formed sequences of tags and (discontinuous) mentions. We evaluate our\napproach on three English datasets in the biomedical domain, and report\ncomparable results to state-of-the-art while having a way simpler and faster\nmodel.", "published": "2024-09-24 17:07:45", "link": "http://arxiv.org/abs/2409.16243v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do the Right Thing, Just Debias! Multi-Category Bias Mitigation Using\n  LLMs", "abstract": "This paper tackles the challenge of building robust and generalizable bias\nmitigation models for language. Recognizing the limitations of existing\ndatasets, we introduce ANUBIS, a novel dataset with 1507 carefully curated\nsentence pairs encompassing nine social bias categories. We evaluate\nstate-of-the-art models like T5, utilizing Supervised Fine-Tuning (SFT),\nReinforcement Learning (PPO, DPO), and In-Context Learning (ICL) for effective\nbias mitigation. Our analysis focuses on multi-class social bias reduction,\ncross-dataset generalizability, and environmental impact of the trained models.\nANUBIS and our findings offer valuable resources for building more equitable AI\nsystems and contribute to the development of responsible and unbiased\ntechnologies with broad societal impact.", "published": "2024-09-24 18:05:10", "link": "http://arxiv.org/abs/2409.16371v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RISCORE: Enhancing In-Context Riddle Solving in Language Models through\n  Context-Reconstructed Example Augmentation", "abstract": "Riddle-solving requires advanced reasoning skills, pushing LLMs to engage in\nabstract thinking and creative problem-solving, often revealing limitations in\ntheir cognitive abilities. In this paper, we examine the riddle-solving\ncapabilities of LLMs using a multiple-choice format, exploring how different\nprompting techniques impact performance on riddles that demand diverse\nreasoning skills. To enhance results, we introduce RISCORE (RIddle Solving with\nCOntext REcontruciton) a novel fully automated prompting method that generates\nand utilizes contextually reconstructed sentence-based puzzles in conjunction\nwith the original examples to create few-shot exemplars. Our experiments\ndemonstrate that RISCORE significantly improves the performance of language\nmodels in both vertical and lateral thinking tasks, surpassing traditional\nexemplar selection strategies across a variety of few-shot settings.", "published": "2024-09-24 18:35:09", "link": "http://arxiv.org/abs/2409.16383v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FMDLlama: Financial Misinformation Detection based on Large Language\n  Models", "abstract": "The emergence of social media has made the spread of misinformation easier.\nIn the financial domain, the accuracy of information is crucial for various\naspects of financial market, which has made financial misinformation detection\n(FMD) an urgent problem that needs to be addressed. Large language models\n(LLMs) have demonstrated outstanding performance in various fields. However,\ncurrent studies mostly rely on traditional methods and have not explored the\napplication of LLMs in the field of FMD. The main reason is the lack of FMD\ninstruction tuning datasets and evaluation benchmarks. In this paper, we\npropose FMDLlama, the first open-sourced instruction-following LLMs for FMD\ntask based on fine-tuning Llama3.1 with instruction data, the first multi-task\nFMD instruction dataset (FMDID) to support LLM instruction tuning, and a\ncomprehensive FMD evaluation benchmark (FMD-B) with classification and\nexplanation generation tasks to test the FMD ability of LLMs. We compare our\nmodels with a variety of LLMs on FMD-B, where our model outperforms other\nopen-sourced LLMs as well as OpenAI's products. This project is available at\nhttps://github.com/lzw108/FMD.", "published": "2024-09-24 20:44:30", "link": "http://arxiv.org/abs/2409.16452v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Strategies for Improving NL-to-FOL Translation with LLMs: Data\n  Generation, Incremental Fine-Tuning, and Verification", "abstract": "Logical reasoning is a fundamental task in natural language processing that\npresents significant challenges to Large Language Models (LLMs). The inherent\ncharacteristics of logical reasoning makes it well-suited for symbolic\nrepresentations such as first-order logic (FOL). Research in symbolic logical\nreasoning explored FOL generation using state-of-the-art LLMs (i.e., GPT-4) to\nproduce FOL translations of natural language (NL) statements, but errors in\ntranslation are usually not the focus. We address this by categorizing the\ntranslation errors in FOL statements generated by LLMs. To make progress\ntowards improving the quality of FOL translations for smaller language models\nsuch as LLaMA-2 13B and Mistral 7B, we create ProofFOL, a high-quality\nFOL-annotated subset of ProofWriter dataset using GPT-4o. The models fine-tuned\non this silver standard data achieve a significant gain in performance when\ncompared to larger language models such as LLaMA-2 70B. In addition to\nimproving the model using large data, we also tackle the issue of data scarcity\nand introduce an incremental framework encompassing of data augmentation and\nverification steps. In the augmentation process, a single pair of (premises,\nconclusion) is split into multiple new instances based on the predicates and\nFOLs. This data is used for fine-tuning, and the inference on this model\ngenerates FOLs with fewer errors over the model trained on the original data.\nOur investigation on the translation errors leads to generation of a\nperturbation dataset, which is used to train a verifier that corrects potential\nsyntactic and semantic FOL translation errors. We demonstrate an efficient\nmethod for making the most of a limited existing human-annotated dataset. Our\nresults show state-of-the-art performance for ProofWriter and ProntoQA datasets\nusing ProofFOL on LLaMA-2 and Mistral models.", "published": "2024-09-24 21:24:07", "link": "http://arxiv.org/abs/2409.16461v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Qualitative Insights Tool (QualIT): LLM Enhanced Topic Modeling", "abstract": "Topic modeling is a widely used technique for uncovering thematic structures\nfrom large text corpora. However, most topic modeling approaches e.g. Latent\nDirichlet Allocation (LDA) struggle to capture nuanced semantics and contextual\nunderstanding required to accurately model complex narratives. Recent\nadvancements in this area include methods like BERTopic, which have\ndemonstrated significantly improved topic coherence and thus established a new\nstandard for benchmarking. In this paper, we present a novel approach, the\nQualitative Insights Tool (QualIT) that integrates large language models (LLMs)\nwith existing clustering-based topic modeling approaches. Our method leverages\nthe deep contextual understanding and powerful language generation capabilities\nof LLMs to enrich the topic modeling process using clustering. We evaluate our\napproach on a large corpus of news articles and demonstrate substantial\nimprovements in topic coherence and topic diversity compared to baseline topic\nmodeling techniques. On the 20 ground-truth topics, our method shows 70% topic\ncoherence (vs 65% & 57% benchmarks) and 95.5% topic diversity (vs 85% & 72%\nbenchmarks). Our findings suggest that the integration of LLMs can unlock new\nopportunities for topic modeling of dynamic and complex text data, as is common\nin talent management research contexts.", "published": "2024-09-24 00:09:41", "link": "http://arxiv.org/abs/2409.15626v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Mitigating Semantic Leakage in Cross-lingual Embeddings via\n  Orthogonality Constraint", "abstract": "Accurately aligning contextual representations in cross-lingual sentence\nembeddings is key for effective parallel data mining. A common strategy for\nachieving this alignment involves disentangling semantics and language in\nsentence embeddings derived from multilingual pre-trained models. However, we\ndiscover that current disentangled representation learning methods suffer from\nsemantic leakage - a term we introduce to describe when a substantial amount of\nlanguage-specific information is unintentionally leaked into semantic\nrepresentations. This hinders the effective disentanglement of semantic and\nlanguage representations, making it difficult to retrieve embeddings that\ndistinctively represent the meaning of the sentence. To address this challenge,\nwe propose a novel training objective, ORthogonAlity Constraint LEarning\n(ORACLE), tailored to enforce orthogonality between semantic and language\nembeddings. ORACLE builds upon two components: intra-class clustering and\ninter-class separation. Through experiments on cross-lingual retrieval and\nsemantic textual similarity tasks, we demonstrate that training with the ORACLE\nobjective effectively reduces semantic leakage and enhances semantic alignment\nwithin the embedding space.", "published": "2024-09-24 02:01:52", "link": "http://arxiv.org/abs/2409.15664v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Making Text Embedders Few-Shot Learners", "abstract": "Large language models (LLMs) with decoder-only architectures demonstrate\nremarkable in-context learning (ICL) capabilities. This feature enables them to\neffectively handle both familiar and novel tasks by utilizing examples provided\nwithin their input context. Recognizing the potential of this capability, we\npropose leveraging the ICL feature in LLMs to enhance the process of text\nembedding generation. To this end, we introduce a novel model bge-en-icl, which\nemploys few-shot examples to produce high-quality text embeddings. Our approach\nintegrates task-related examples directly into the query side, resulting in\nsignificant improvements across various tasks. Additionally, we have\ninvestigated how to effectively utilize LLMs as embedding models, including\nvarious attention mechanisms, pooling methods, etc. Our findings suggest that\nretaining the original framework often yields the best results, underscoring\nthat simplicity is best. Experimental results on the MTEB and AIR-Bench\nbenchmarks demonstrate that our approach sets new state-of-the-art (SOTA)\nperformance. Our model, code and dataset are freely available at\nhttps://github.com/FlagOpen/FlagEmbedding .", "published": "2024-09-24 03:30:19", "link": "http://arxiv.org/abs/2409.15700v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Federated Large Language Models: Current Progress and Future Directions", "abstract": "Large language models are rapidly gaining popularity and have been widely\nadopted in real-world applications. While the quality of training data is\nessential, privacy concerns arise during data collection. Federated learning\noffers a solution by allowing multiple clients to collaboratively train LLMs\nwithout sharing local data. However, FL introduces new challenges, such as\nmodel convergence issues due to heterogeneous data and high communication\ncosts. A comprehensive study is required to address these challenges and guide\nfuture research. This paper surveys Federated learning for LLMs (FedLLM),\nhighlighting recent advances and future directions. We focus on two key\naspects: fine-tuning and prompt learning in a federated setting, discussing\nexisting work and associated research challenges. We finally propose potential\nresearch directions for federated LLMs, including pre-training and how LLMs can\nfurther enhance federated learning.", "published": "2024-09-24 04:14:33", "link": "http://arxiv.org/abs/2409.15723v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For\n  Asthma Patient Support", "abstract": "Asthma rates have risen globally, driven by environmental and lifestyle\nfactors. Access to immediate medical care is limited, particularly in\ndeveloping countries, necessitating automated support systems. Large Language\nModels like ChatGPT (Chat Generative Pre-trained Transformer) and Gemini have\nadvanced natural language processing in general and question answering in\nparticular, however, they are prone to producing factually incorrect responses\n(i.e. hallucinations). Retrieval-augmented generation systems, integrating\ncurated documents, can improve large language models' performance and reduce\nthe incidence of hallucination. We introduce AsthmaBot, a multi-lingual,\nmulti-modal retrieval-augmented generation system for asthma support.\nEvaluation of an asthma-related frequently asked questions dataset shows\nAsthmaBot's efficacy. AsthmaBot has an added interactive and intuitive\ninterface that integrates different data modalities (text, images, videos) to\nmake it accessible to the larger public. AsthmaBot is available online via\n\\url{asthmabot.datanets.org}.", "published": "2024-09-24 07:24:01", "link": "http://arxiv.org/abs/2409.15815v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Supervised Fine-Tuning Achieve Rapid Task Adaption Via Alternating\n  Attention Head Activation Patterns", "abstract": "LLMs' performance on complex tasks is still unsatisfactory. A key issue is\nthat presently LLMs learn in a data-driven schema, while the instructions about\nthese complex tasks are both scarce and hard to collect or construct. On the\ncontrary, a prominent phenomenon is that LLMs can learn rather fast on simpler\ntasks with adequate prior knowledge captured during pretraining stage. Thus, if\nthe prerequisite and mechanism of such rapid generalization could be\nelucidated, it could enhance the efficiency and effectiveness of the LLM's\nability to learn complex tasks. Thus, in this paper, we employ a gradient-based\nmethod, to dissect the process that the SFT process adapts LLMs to downstream\ntasks via the perspective of attention patterns. We find that: (1) LLMs\nselectively activate task-specific attention heads during SFT; (2) activation\npatterns for complex tasks are combinations of basic task patterns; and (3)\nchanges in a few parameters can significantly impact activation patterns after\nSFT on a small number of samples.Based on these insights, experiments are\nconducted to actually enhance the efficiency and effectiveness of SFT.", "published": "2024-09-24 07:34:50", "link": "http://arxiv.org/abs/2409.15820v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "60 Data Points are Sufficient to Fine-Tune LLMs for Question-Answering", "abstract": "Large language models (LLMs) encode extensive world knowledge through\npre-training on massive datasets, which can then be fine-tuned for the\nquestion-answering (QA) task. However, effective strategies for fine-tuning\nLLMs for the QA task remain largely unexplored. To address this gap, we\ncategorize supervised fine-tuning (SFT) data based on the extent of knowledge\nmemorized by the pretrained LLMs and conduct a series of empirical analyses.\nOur experiments, involving four LLMs from three different model families, focus\non three key factors: the amount of data required for SFT, the impact of\ndifferent SFT datasets on model performance, and how data requirements vary\nacross LLMs. The results show that as few as 60 data points during the SFT\nstage can activate the knowledge encoded during pre-training, enabling LLMs to\nperform the QA task. Additionally, SFT with data of varying memory levels has a\nsignificant impact on LLM performance, with the optimal dataset differing based\non the specific model being fine-tuned. Future research will delve deeper into\nthe mechanisms underlying these phenomena.", "published": "2024-09-24 07:38:38", "link": "http://arxiv.org/abs/2409.15825v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "iGAiVA: Integrated Generative AI and Visual Analytics in a Machine\n  Learning Workflow for Text Classification", "abstract": "In developing machine learning (ML) models for text classification, one\ncommon challenge is that the collected data is often not ideally distributed,\nespecially when new classes are introduced in response to changes of data and\ntasks. In this paper, we present a solution for using visual analytics (VA) to\nguide the generation of synthetic data using large language models. As VA\nenables model developers to identify data-related deficiency, data synthesis\ncan be targeted to address such deficiency. We discuss different types of data\ndeficiency, describe different VA techniques for supporting their\nidentification, and demonstrate the effectiveness of targeted data synthesis in\nimproving model accuracy. In addition, we present a software tool, iGAiVA,\nwhich maps four groups of ML tasks into four VA views, integrating generative\nAI and VA into an ML workflow for developing and improving text classification\nmodels.", "published": "2024-09-24 08:19:45", "link": "http://arxiv.org/abs/2409.15848v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding", "abstract": "Dialogue State Tracking (DST) is crucial for understanding user needs and\nexecuting appropriate system actions in task-oriented dialogues. Majority of\nexisting DST methods are designed to work within predefined ontologies and\nassume the availability of gold domain labels, struggling with adapting to new\nslots values. While Large Language Models (LLMs)-based systems show promising\nzero-shot DST performance, they either require extensive computational\nresources or they underperform existing fully-trained systems, limiting their\npracticality. To address these limitations, we propose a zero-shot,\nopen-vocabulary system that integrates domain classification and DST in a\nsingle pipeline. Our approach includes reformulating DST as a\nquestion-answering task for less capable models and employing self-refining\nprompts for more adaptable ones. Our system does not rely on fixed slot values\ndefined in the ontology allowing the system to adapt dynamically. We compare\nour approach with existing SOTA, and show that it provides up to 20% better\nJoint Goal Accuracy (JGA) over previous methods on datasets like Multi-WOZ 2.1,\nwith up to 90% fewer requests to the LLM API.", "published": "2024-09-24 08:33:41", "link": "http://arxiv.org/abs/2409.15861v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Privacy Evaluation Benchmarks for NLP Models", "abstract": "By inducing privacy attacks on NLP models, attackers can obtain sensitive\ninformation such as training data and model parameters, etc. Although\nresearchers have studied, in-depth, several kinds of attacks in NLP models,\nthey are non-systematic analyses. It lacks a comprehensive understanding of the\nimpact caused by the attacks. For example, we must consider which scenarios can\napply to which attacks, what the common factors are that affect the performance\nof different attacks, the nature of the relationships between different\nattacks, and the influence of various datasets and models on the effectiveness\nof the attacks, etc. Therefore, we need a benchmark to holistically assess the\nprivacy risks faced by NLP models. In this paper, we present a privacy attack\nand defense evaluation benchmark in the field of NLP, which includes the\nconventional/small models and large language models (LLMs). This benchmark\nsupports a variety of models, datasets, and protocols, along with standardized\nmodules for comprehensive evaluation of attacks and defense strategies. Based\non the above framework, we present a study on the association between auxiliary\ndata from different domains and the strength of privacy attacks. And we provide\nan improved attack method in this scenario with the help of Knowledge\nDistillation (KD). Furthermore, we propose a chained framework for privacy\nattacks. Allowing a practitioner to chain multiple attacks to achieve a\nhigher-level attack objective. Based on this, we provide some defense and\nenhanced attack strategies. The code for reproducing the results can be found\nat https://github.com/user2311717757/nlp_doctor.", "published": "2024-09-24 08:41:26", "link": "http://arxiv.org/abs/2409.15868v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Machine Translation Advancements of Low-Resource Indian Languages by\n  Transfer Learning", "abstract": "This paper introduces the submission by Huawei Translation Center (HW-TSC) to\nthe WMT24 Indian Languages Machine Translation (MT) Shared Task. To develop a\nreliable machine translation system for low-resource Indian languages, we\nemployed two distinct knowledge transfer strategies, taking into account the\ncharacteristics of the language scripts and the support available from existing\nopen-source models for Indian languages. For Assamese(as) and Manipuri(mn), we\nfine-tuned the existing IndicTrans2 open-source model to enable bidirectional\ntranslation between English and these languages. For Khasi (kh) and Mizo (mz),\nWe trained a multilingual model as a baseline using bilingual data from these\nfour language pairs, along with an additional about 8kw English-Bengali\nbilingual data, all of which share certain linguistic features. This was\nfollowed by fine-tuning to achieve bidirectional translation between English\nand Khasi, as well as English and Mizo. Our transfer learning experiments\nproduced impressive results: 23.5 BLEU for en-as, 31.8 BLEU for en-mn, 36.2\nBLEU for as-en, and 47.9 BLEU for mn-en on their respective test sets.\nSimilarly, the multilingual model transfer learning experiments yielded\nimpressive outcomes, achieving 19.7 BLEU for en-kh, 32.8 BLEU for en-mz, 16.1\nBLEU for kh-en, and 33.9 BLEU for mz-en on their respective test sets. These\nresults not only highlight the effectiveness of transfer learning techniques\nfor low-resource languages but also contribute to advancing machine translation\ncapabilities for low-resource Indian languages.", "published": "2024-09-24 08:53:19", "link": "http://arxiv.org/abs/2409.15879v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Text-to-SQL Capabilities of Large Language Models via Domain\n  Database Knowledge Injection", "abstract": "Text-to-SQL is a subtask in semantic parsing that has seen rapid progress\nwith the evolution of Large Language Models (LLMs). However, LLMs face\nchallenges due to hallucination issues and a lack of domain-specific database\nknowledge(such as table schema and cell values). As a result, they can make\nerrors in generating table names, columns, and matching values to the correct\ncolumns in SQL statements. This paper introduces a method of knowledge\ninjection to enhance LLMs' ability to understand schema contents by\nincorporating prior knowledge. This approach improves their performance in\nText-to-SQL tasks. Experimental results show that pre-training LLMs on\ndomain-specific database knowledge and fine-tuning them on downstream\nText-to-SQL tasks significantly improves the Execution Match (EX) and Exact\nMatch (EM) metrics across various models. This effectively reduces errors in\ngenerating column names and matching values to the columns. Furthermore, the\nknowledge-injected models can be applied to many downstream Text-to-SQL tasks,\ndemonstrating the generalizability of the approach presented in this paper.", "published": "2024-09-24 09:24:03", "link": "http://arxiv.org/abs/2409.15907v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multilingual Transfer and Domain Adaptation for Low-Resource Languages\n  of Spain", "abstract": "This article introduces the submission status of the Translation into\nLow-Resource Languages of Spain task at (WMT 2024) by Huawei Translation\nService Center (HW-TSC). We participated in three translation tasks: spanish to\naragonese (es-arg), spanish to aranese (es-arn), and spanish to asturian\n(es-ast). For these three translation tasks, we use training strategies such as\nmultilingual transfer, regularized dropout, forward translation and back\ntranslation, labse denoising, transduction ensemble learning and other\nstrategies to neural machine translation (NMT) model based on training deep\ntransformer-big architecture. By using these enhancement strategies, our\nsubmission achieved a competitive result in the final evaluation.", "published": "2024-09-24 09:46:27", "link": "http://arxiv.org/abs/2409.15924v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SLIMER-IT: Zero-Shot NER on Italian Language", "abstract": "Traditional approaches to Named Entity Recognition (NER) frame the task into\na BIO sequence labeling problem. Although these systems often excel in the\ndownstream task at hand, they require extensive annotated data and struggle to\ngeneralize to out-of-distribution input domains and unseen entity types. On the\ncontrary, Large Language Models (LLMs) have demonstrated strong zero-shot\ncapabilities. While several works address Zero-Shot NER in English, little has\nbeen done in other languages. In this paper, we define an evaluation framework\nfor Zero-Shot NER, applying it to the Italian language. Furthermore, we\nintroduce SLIMER-IT, the Italian version of SLIMER, an instruction-tuning\napproach for zero-shot NER leveraging prompts enriched with definition and\nguidelines. Comparisons with other state-of-the-art models, demonstrate the\nsuperiority of SLIMER-IT on never-seen-before entity tags.", "published": "2024-09-24 09:57:25", "link": "http://arxiv.org/abs/2409.15933v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "AI Can Be Cognitively Biased: An Exploratory Study on Threshold Priming\n  in LLM-Based Batch Relevance Assessment", "abstract": "Cognitive biases are systematic deviations in thinking that lead to\nirrational judgments and problematic decision-making, extensively studied\nacross various fields. Recently, large language models (LLMs) have shown\nadvanced understanding capabilities but may inherit human biases from their\ntraining data. While social biases in LLMs have been well-studied, cognitive\nbiases have received less attention, with existing research focusing on\nspecific scenarios. The broader impact of cognitive biases on LLMs in various\ndecision-making contexts remains underexplored. We investigated whether LLMs\nare influenced by the threshold priming effect in relevance judgments, a core\ntask and widely-discussed research topic in the Information Retrieval (IR)\ncoummunity. The priming effect occurs when exposure to certain stimuli\nunconsciously affects subsequent behavior and decisions. Our experiment\nemployed 10 topics from the TREC 2019 Deep Learning passage track collection,\nand tested AI judgments under different document relevance scores, batch\nlengths, and LLM models, including GPT-3.5, GPT-4, LLaMa2-13B and LLaMa2-70B.\nResults showed that LLMs tend to give lower scores to later documents if\nearlier ones have high relevance, and vice versa, regardless of the combination\nand model used. Our finding demonstrates that LLM%u2019s judgments, similar to\nhuman judgments, are also influenced by threshold priming biases, and suggests\nthat researchers and system engineers should take into account potential\nhuman-like cognitive biases in designing, evaluating, and auditing LLMs in IR\ntasks and beyond.", "published": "2024-09-24 12:23:15", "link": "http://arxiv.org/abs/2409.16022v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Hint Generation Approaches in Open-Domain Question Answering", "abstract": "Automatic Question Answering (QA) systems rely on contextual information to\nprovide accurate answers. Commonly, contexts are prepared through either\nretrieval-based or generation-based methods. The former involves retrieving\nrelevant documents from a corpus like Wikipedia, whereas the latter uses\ngenerative models such as Large Language Models (LLMs) to generate the context.\nIn this paper, we introduce a novel context preparation approach called HINTQA,\nwhich employs Automatic Hint Generation (HG) techniques. Unlike traditional\nmethods, HINTQA prompts LLMs to produce hints about potential answers for the\nquestion rather than generating relevant context. We evaluate our approach\nacross three QA datasets including TriviaQA, NaturalQuestions, and Web\nQuestions, examining how the number and order of hints impact performance. Our\nfindings show that the HINTQA surpasses both retrieval-based and\ngeneration-based approaches. We demonstrate that hints enhance the accuracy of\nanswers more than retrieved and generated contexts.", "published": "2024-09-24 13:50:32", "link": "http://arxiv.org/abs/2409.16096v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Exploring the traditional NMT model and Large Language Model for chat\n  translation", "abstract": "This paper describes the submissions of Huawei Translation Services\nCenter(HW-TSC) to WMT24 chat translation shared task on\nEnglish$\\leftrightarrow$Germany (en-de) bidirection. The experiments involved\nfine-tuning models using chat data and exploring various strategies, including\nMinimum Bayesian Risk (MBR) decoding and self-training. The results show\nsignificant performance improvements in certain directions, with the MBR\nself-training method achieving the best results. The Large Language Model also\ndiscusses the challenges and potential avenues for further research in the\nfield of chat translation.", "published": "2024-09-24 08:48:25", "link": "http://arxiv.org/abs/2409.16331v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Unified Hallucination Mitigation Framework for Large Vision-Language\n  Models", "abstract": "Hallucination is a common problem for Large Vision-Language Models (LVLMs)\nwith long generations which is difficult to eradicate. The generation with\nhallucinations is partially inconsistent with the image content. To mitigate\nhallucination, current studies either focus on the process of model inference\nor the results of model generation, but the solutions they design sometimes do\nnot deal appropriately with various types of queries and the hallucinations of\nthe generations about these queries. To accurately deal with various\nhallucinations, we present a unified framework, Dentist, for hallucination\nmitigation. The core step is to first classify the queries, then perform\ndifferent processes of hallucination mitigation based on the classification\nresult, just like a dentist first observes the teeth and then makes a plan. In\na simple deployment, Dentist can classify queries as perception or reasoning\nand easily mitigate potential hallucinations in answers which has been\ndemonstrated in our experiments. On MMbench, we achieve a 13.44%/10.2%/15.8%\nimprovement in accuracy on Image Quality, a Coarse Perception visual question\nanswering (VQA) task, over the baseline InstructBLIP/LLaVA/VisualGLM.", "published": "2024-09-24 22:36:58", "link": "http://arxiv.org/abs/2409.16494v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "RAGProbe: An Automated Approach for Evaluating RAG Applications", "abstract": "Retrieval Augmented Generation (RAG) is increasingly being used when building\nGenerative AI applications. Evaluating these applications and RAG pipelines is\nmostly done manually, via a trial and error process. Automating evaluation of\nRAG pipelines requires overcoming challenges such as context misunderstanding,\nwrong format, incorrect specificity, and missing content. Prior works therefore\nfocused on improving evaluation metrics as well as enhancing components within\nthe pipeline using available question and answer datasets. However, they have\nnot focused on 1) providing a schema for capturing different types of\nquestion-answer pairs or 2) creating a set of templates for generating\nquestion-answer pairs that can support automation of RAG pipeline evaluation.\nIn this paper, we present a technique for generating variations in\nquestion-answer pairs to trigger failures in RAG pipelines. We validate 5\nopen-source RAG pipelines using 3 datasets. Our approach revealed the highest\nfailure rates when prompts combine multiple questions: 91% for questions when\nspanning multiple documents and 78% for questions from a single document;\nindicating a need for developers to prioritise handling these combined\nquestions. 60% failure rate was observed in academic domain dataset and 53% and\n62% failure rates were observed in open-domain datasets. Our automated approach\noutperforms the existing state-of-the-art methods, by increasing the failure\nrate by 51% on average per dataset. Our work presents an automated approach for\ncontinuously monitoring the health of RAG pipelines, which can be integrated\ninto existing CI/CD pipelines, allowing for improved quality.", "published": "2024-09-24 23:33:07", "link": "http://arxiv.org/abs/2409.19019v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "English offensive text detection using CNN based Bi-GRU model", "abstract": "Over the years, the number of users of social media has increased\ndrastically. People frequently share their thoughts through social platforms,\nand this leads to an increase in hate content. In this virtual community,\nindividuals share their views, express their feelings, and post photos, videos,\nblogs, and more. Social networking sites like Facebook and Twitter provide\nplatforms to share vast amounts of content with a single click. However, these\nplatforms do not impose restrictions on the uploaded content, which may include\nabusive language and explicit images unsuitable for social media. To resolve\nthis issue, a new idea must be implemented to divide the inappropriate content.\nNumerous studies have been done to automate the process. In this paper, we\npropose a new Bi-GRU-CNN model to classify whether the text is offensive or\nnot. The combination of the Bi-GRU and CNN models outperforms the existing\nmodel.", "published": "2024-09-24 01:29:24", "link": "http://arxiv.org/abs/2409.15652v3", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "M$^2$PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning", "abstract": "Multimodal Large Language Models (MLLMs) demonstrate remarkable performance\nacross a wide range of domains, with increasing emphasis on enhancing their\nzero-shot generalization capabilities for unseen tasks across various\nmodalities. Instruction tuning has emerged as an effective strategy for\nachieving zero-shot generalization by finetuning pretrained models on diverse\nmultimodal tasks. As the scale of MLLMs continues to grow, parameter-efficient\nfinetuning becomes increasingly critical. However, most existing\nparameter-efficient approaches focus only on single modalities and often\noverlook the multimodal characteristics during finetuning. In this work, we\nintroduce a novel Multimodal Prompt Tuning (M$^2$PT) approach for efficient\ninstruction tuning of MLLMs. M$^2$PT effectively integrates visual and textual\nprompts into the vision encoder and language processor respectively during\nfinetuning, facilitating the extraction and alignment of features across\nmodalities. Empirical results on various multimodal evaluation datasets\ndemonstrate the superior performance of our approach compared to several\nstate-of-the-art baselines. A comprehensive set of ablation studies validates\nthe effectiveness of our prompt design and the efficiency of our approach.", "published": "2024-09-24 01:40:24", "link": "http://arxiv.org/abs/2409.15657v4", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Language-based Audio Moment Retrieval", "abstract": "In this paper, we propose and design a new task called audio moment retrieval\n(AMR). Unlike conventional language-based audio retrieval tasks that search for\nshort audio clips from an audio database, AMR aims to predict relevant moments\nin untrimmed long audio based on a text query. Given the lack of prior work in\nAMR, we first build a dedicated dataset, Clotho-Moment, consisting of\nlarge-scale simulated audio recordings with moment annotations. We then propose\na DETR-based model, named Audio Moment DETR (AM-DETR), as a fundamental\nframework for AMR tasks. This model captures temporal dependencies within audio\nfeatures, inspired by similar video moment retrieval tasks, thus surpassing\nconventional clip-level audio retrieval methods. Additionally, we provide\nmanually annotated datasets to properly measure the effectiveness and\nrobustness of our methods on real data. Experimental results show that AM-DETR,\ntrained with Clotho-Moment, outperforms a baseline model that applies a\nclip-level audio retrieval method with a sliding window on all metrics,\nparticularly improving Recall1@0.7 by 9.00 points. Our datasets and code are\npublicly available in\nhttps://h-munakata.github.io/Language-based-Audio-Moment-Retrieval.", "published": "2024-09-24 02:24:48", "link": "http://arxiv.org/abs/2409.15672v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Survey of Stance Detection on Social Media: New Directions and\n  Perspectives", "abstract": "In modern digital environments, users frequently express opinions on\ncontentious topics, providing a wealth of information on prevailing attitudes.\nThe systematic analysis of these opinions offers valuable insights for\ndecision-making in various sectors, including marketing and politics. As a\nresult, stance detection has emerged as a crucial subfield within affective\ncomputing, enabling the automatic detection of user stances in social media\nconversations and providing a nuanced understanding of public sentiment on\ncomplex issues. Recent years have seen a surge of research interest in\ndeveloping effective stance detection methods, with contributions from multiple\ncommunities, including natural language processing, web science, and social\ncomputing. This paper provides a comprehensive survey of stance detection\ntechniques on social media, covering task definitions, datasets, approaches,\nand future works. We review traditional stance detection models, as well as\nstate-of-the-art methods based on large language models, and discuss their\nstrengths and limitations. Our survey highlights the importance of stance\ndetection in understanding public opinion and sentiment, and identifies gaps in\ncurrent research. We conclude by outlining potential future directions for\nstance detection on social media, including the need for more robust and\ngeneralizable models, and the importance of addressing emerging challenges such\nas multi-modal stance detection and stance detection in low-resource languages.", "published": "2024-09-24 03:06:25", "link": "http://arxiv.org/abs/2409.15690v2", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "dnaGrinder: a lightweight and high-capacity genomic foundation model", "abstract": "The task of understanding and interpreting the complex information encoded\nwithin genomic sequences remains a grand challenge in biological research and\nclinical applications. In this context, recent advancements in large language\nmodel research have led to the development of both encoder-only and\ndecoder-only foundation models designed to decode intricate information in DNA\nsequences. However, several issues persist, particularly regarding the\nefficient management of long-range dependencies inherent in genomic sequences,\nthe effective representation of nucleotide variations, and the considerable\ncomputational costs associated with large model architectures and extensive\npretraining datasets. Current genomic foundation models often face a critical\ntradeoff: smaller models with mediocre performance versus large models with\nimproved performance. To address these challenges, we introduce dnaGrinder, a\nunique and efficient genomic foundation model. dnaGrinder excels at managing\nlong-range dependencies within genomic sequences while minimizing computational\ncosts without compromising performance. It achieves results that are not just\ncomparable but often superior to leading DNA models such as Nucleotide\nTransformer and DNABERT-2. Furthermore, dnaGrinder is designed for easy\nfine-tuning on workstation-grade GPUs, accommodating input lengths exceeding\n17,000 tokens. On a single high-performance GPU, it supports sequences longer\nthan 140,000 tokens, making it a highly efficient and accessible tool for both\nbasic biological research and clinical applications.", "published": "2024-09-24 03:20:07", "link": "http://arxiv.org/abs/2409.15697v1", "categories": ["q-bio.GN", "cs.AI", "cs.CE", "cs.CL"], "primary_category": "q-bio.GN"}
{"title": "Hypothesis Clustering and Merging: Novel MultiTalker Speech Recognition\n  with Speaker Tokens", "abstract": "In many real-world scenarios, such as meetings, multiple speakers are present\nwith an unknown number of participants, and their utterances often overlap. We\naddress these multi-speaker challenges by a novel attention-based\nencoder-decoder method augmented with special speaker class tokens obtained by\nspeaker clustering. During inference, we select multiple recognition hypotheses\nconditioned on predicted speaker cluster tokens, and these hypotheses are\nmerged by agglomerative hierarchical clustering (AHC) based on the normalized\nedit distance. The clustered hypotheses result in the multi-speaker\ntranscriptions with the appropriate number of speakers determined by AHC. Our\nexperiments on the LibriMix dataset demonstrate that our proposed method was\nparticularly effective in complex 3-mix environments, achieving a 55% relative\nerror reduction on clean data and a 36% relative error reduction on noisy data\ncompared with conventional serialized output training.", "published": "2024-09-24 04:31:46", "link": "http://arxiv.org/abs/2409.15732v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Small Language Models: Survey, Measurements, and Insights", "abstract": "Small language models (SLMs), despite their widespread adoption in modern\nsmart devices, have received significantly less academic attention compared to\ntheir large language model (LLM) counterparts, which are predominantly deployed\nin data centers and cloud environments. While researchers continue to improve\nthe capabilities of LLMs in the pursuit of artificial general intelligence, SLM\nresearch aims to make machine intelligence more accessible, affordable, and\nefficient for everyday tasks. Focusing on transformer-based, decoder-only\nlanguage models with 100M-5B parameters, we survey 70 state-of-the-art\nopen-source SLMs, analyzing their technical innovations across three axes:\narchitectures, training datasets, and training algorithms. In addition, we\nevaluate their capabilities in various domains, including commonsense\nreasoning, mathematics, in-context learning, and long context. To gain further\ninsight into their on-device runtime costs, we benchmark their inference\nlatency and memory footprints. Through in-depth analysis of our benchmarking\ndata, we offer valuable insights to advance research in this field.", "published": "2024-09-24 06:36:56", "link": "http://arxiv.org/abs/2409.15790v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BeSimulator: A Large Language Model Powered Text-based Behavior\n  Simulator", "abstract": "Traditional robot simulators focus on physical process modeling and realistic\nrendering, often suffering from high computational costs, inefficiencies, and\nlimited adaptability. To handle this issue, we propose Behavior Simulation in\nrobotics to emphasize checking the behavior logic of robots and achieving\nsufficient alignment between the outcome of robot actions and real scenarios.\nIn this paper, we introduce BeSimulator, a modular and novel LLM-powered\nframework, as an attempt towards behavior simulation in the context of\ntext-based environments. By constructing text-based virtual environments and\nperforming semantic-level simulation, BeSimulator can generalize across\nscenarios and achieve long-horizon complex simulation. Inspired by human\ncognition processes, it employs a \"consider-decide-capture-transfer\"\nmethodology, termed Chain of Behavior Simulation, which excels at analyzing\naction feasibility and state transitions. Additionally, BeSimulator\nincorporates code-driven reasoning to enable arithmetic operations and enhance\nreliability, as well as integrates reflective feedback to refine simulation.\nBased on our manually constructed behavior-tree-based simulation benchmark\nBTSIMBENCH, our experiments show a significant performance improvement in\nbehavior simulation compared to baselines, ranging from 14.7% to 26.6%.", "published": "2024-09-24 08:37:04", "link": "http://arxiv.org/abs/2409.15865v1", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "A Modular-based Strategy for Mitigating Gradient Conflicts in\n  Simultaneous Speech Translation", "abstract": "Simultaneous Speech Translation (SimulST) involves generating target language\ntext while continuously processing streaming speech input, presenting\nsignificant real-time challenges. Multi-task learning is often employed to\nenhance SimulST performance but introduces optimization conflicts between\nprimary and auxiliary tasks, potentially compromising overall efficiency. The\nexisting model-level conflict resolution methods are not well-suited for this\ntask which exacerbates inefficiencies and leads to high GPU memory consumption.\nTo address these challenges, we propose a Modular Gradient Conflict Mitigation\n(MGCM) strategy that detects conflicts at a finer-grained modular level and\nresolves them utilizing gradient projection. Experimental results demonstrate\nthat MGCM significantly improves SimulST performance, particularly under medium\nand high latency conditions, achieving a 0.68 BLEU score gain in offline tasks.\nAdditionally, MGCM reduces GPU memory consumption by over 95\\% compared to\nother conflict mitigation methods, establishing it as a robust solution for\nSimulST tasks.", "published": "2024-09-24 09:27:43", "link": "http://arxiv.org/abs/2409.15911v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Automated test generation to evaluate tool-augmented LLMs as\n  conversational AI agents", "abstract": "Tool-augmented LLMs are a promising approach to create AI agents that can\nhave realistic conversations, follow procedures, and call appropriate\nfunctions. However, evaluating them is challenging due to the diversity of\npossible conversations, and existing datasets focus only on single interactions\nand function-calling. We present a test generation pipeline to evaluate LLMs as\nconversational AI agents. Our framework uses LLMs to generate diverse tests\ngrounded on user-defined procedures. For that, we use intermediate graphs to\nlimit the LLM test generator's tendency to hallucinate content that is not\ngrounded on input procedures, and enforces high coverage of the possible\nconversations. Additionally, we put forward ALMITA, a manually curated dataset\nfor evaluating AI agents in customer support, and use it to evaluate existing\nLLMs. Our results show that while tool-augmented LLMs perform well in single\ninteractions, they often struggle to handle complete conversations. While our\nfocus is on customer support, our method is general and capable of AI agents\nfor different domains.", "published": "2024-09-24 09:57:43", "link": "http://arxiv.org/abs/2409.15934v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and\n  Multi-Level Style Control", "abstract": "Zero-shot singing voice synthesis (SVS) with style transfer and style control\naims to generate high-quality singing voices with unseen timbres and styles\n(including singing method, emotion, rhythm, technique, and pronunciation) from\naudio and text prompts. However, the multifaceted nature of singing styles\nposes a significant challenge for effective modeling, transfer, and control.\nFurthermore, current SVS models often fail to generate singing voices rich in\nstylistic nuances for unseen singers. To address these challenges, we introduce\nTCSinger, the first zero-shot SVS model for style transfer across cross-lingual\nspeech and singing styles, along with multi-level style control. Specifically,\nTCSinger proposes three primary modules: 1) the clustering style encoder\nemploys a clustering vector quantization model to stably condense style\ninformation into a compact latent space; 2) the Style and Duration Language\nModel (S\\&D-LM) concurrently predicts style information and phoneme duration,\nwhich benefits both; 3) the style adaptive decoder uses a novel mel-style\nadaptive normalization method to generate singing voices with enhanced details.\nExperimental results show that TCSinger outperforms all baseline models in\nsynthesis quality, singer similarity, and style controllability across various\ntasks, including zero-shot style transfer, multi-level style control,\ncross-lingual style transfer, and speech-to-singing style transfer. Singing\nvoice samples can be accessed at https://aaronz345.github.io/TCSingerDemo/.", "published": "2024-09-24 11:18:09", "link": "http://arxiv.org/abs/2409.15977v5", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Bridging Speech and Text: Enhancing ASR with Pinyin-to-Character\n  Pre-training in LLMs", "abstract": "The integration of large language models (LLMs) with pre-trained speech\nmodels has opened up new avenues in automatic speech recognition (ASR). While\nLLMs excel in multimodal understanding tasks, effectively leveraging their\ncapabilities for ASR remains a significant challenge. This paper presents a\nnovel training approach to enhance LLM performance in ASR tasks. We propose\npre-training LLMs on Pinyin embedding sequences, which represent pronunciation\nfeatures, to generate corresponding Chinese characters. This step enables the\nLLM to adapt to generating text from pronunciation features before encountering\nreal speech data. Furthermore, we fine-tune the LoRA parameters to enhance the\nLLM's understanding of speech modality information. In AISHELL-1 corpus, our\napproach yields a 9.5% relative improvement in ASR tasks compared to the\nbaseline without Pinyi-to-Character pre-training. Additionally, incorporating\nauxiliary text data for Pinyi-to-Character pre-training further boosts\nperformance, achieving a 19.0% relative improvement.", "published": "2024-09-24 12:06:31", "link": "http://arxiv.org/abs/2409.16005v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MOSS: Enabling Code-Driven Evolution and Context Management for AI\n  Agents", "abstract": "Developing AI agents powered by large language models (LLMs) faces\nsignificant challenges in achieving true Turing completeness and adaptive,\ncode-driven evolution. Current approaches often generate code independently of\nits runtime context, relying heavily on the LLM's memory, which results in\ninefficiencies and limits adaptability. Manual protocol development in sandbox\nenvironments further constrains the agent's autonomous adaptability. Crucially,\nachieving consistency in code and context across multi-turn interactions and\nensuring isolation of local variables within each interaction remains an\nunsolved problem.\n  We introduce MOSS (llM-oriented Operating System Simulation), a novel\nframework that addresses these challenges by integrating code generation with a\ndynamic context management system. MOSS ensures consistency and adaptability by\nusing a mechanism that maintains the Python context across interactions,\nincluding isolation of local variables and preservation of runtime integrity.\nAt its core, the framework employs an Inversion of Control (IoC) container in\nconjunction with decorators to enforce the least knowledge principle, allowing\nagents to focus on abstract interfaces rather than concrete implementations.\nThis facilitates seamless integration of new tools and libraries, enables\nruntime instance replacement, and reduces prompt complexity, providing a \"what\nyou see is what you get\" environment for the agent.\n  Through a series of case studies, we show how this framework can enhance the\nefficiency and capabilities of agent development and highlight its advantages\nin moving towards Turing-complete agents capable of evolving through code.", "published": "2024-09-24 14:30:21", "link": "http://arxiv.org/abs/2409.16120v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Implicit assessment of language learning during practice as accurate as\n  explicit testing", "abstract": "Assessment of proficiency of the learner is an essential part of Intelligent\nTutoring Systems (ITS). We use Item Response Theory (IRT) in computer-aided\nlanguage learning for assessment of student ability in two contexts: in test\nsessions, and in exercises during practice sessions. Exhaustive testing across\na wide range of skills can provide a detailed picture of proficiency, but may\nbe undesirable for a number of reasons. Therefore, we first aim to replace\nexhaustive tests with efficient but accurate adaptive tests. We use learner\ndata collected from exhaustive tests under imperfect conditions, to train an\nIRT model to guide adaptive tests. Simulations and experiments with real\nlearner data confirm that this approach is efficient and accurate. Second, we\nexplore whether we can accurately estimate learner ability directly from the\ncontext of practice with exercises, without testing. We transform learner data\ncollected from exercise sessions into a form that can be used for IRT modeling.\nThis is done by linking the exercises to {\\em linguistic constructs}; the\nconstructs are then treated as \"items\" within IRT. We present results from\nlarge-scale studies with thousands of learners. Using teacher assessments of\nstudent ability as \"ground truth,\" we compare the estimates obtained from tests\nvs. those from exercises. The experiments confirm that the IRT models can\nproduce accurate ability estimation based on exercises.", "published": "2024-09-24 14:40:44", "link": "http://arxiv.org/abs/2409.16133v1", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI"}
{"title": "HA-FGOVD: Highlighting Fine-grained Attributes via Explicit Linear\n  Composition for Open-Vocabulary Object Detection", "abstract": "Open-vocabulary object detection (OVD) models are considered to be Large\nMulti-modal Models (LMM), due to their extensive training data and a large\nnumber of parameters. Mainstream OVD models prioritize object coarse-grained\ncategory rather than focus on their fine-grained attributes, e.g., colors or\nmaterials, thus failed to identify objects specified with certain attributes.\nHowever, OVD models are pretrained on large-scale image-text pairs with rich\nattribute words, whose latent feature space can represent the global text\nfeature as a linear composition of fine-grained attribute tokens without\nhighlighting them. Therefore, we propose in this paper a universal and explicit\napproach for frozen mainstream OVD models that boosts their attribute-level\ndetection capabilities by highlighting fine-grained attributes in explicit\nlinear space. Firstly, a LLM is leveraged to highlight attribute words within\nthe input text as a zero-shot prompted task. Secondly, by strategically\nadjusting the token masks, the text encoders of OVD models extract both global\ntext and attribute-specific features, which are then explicitly composited as\ntwo vectors in linear space to form the new attribute-highlighted feature for\ndetection tasks, where corresponding scalars are hand-crafted or learned to\nreweight both two vectors. Notably, these scalars can be seamlessly transferred\namong different OVD models, which proves that such an explicit linear\ncomposition is universal. Empirical evaluation on the FG-OVD dataset\ndemonstrates that our proposed method uniformly improves fine-grained\nattribute-level OVD of various mainstream models and achieves new\nstate-of-the-art performance.", "published": "2024-09-24 14:43:14", "link": "http://arxiv.org/abs/2409.16136v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to\n  Extremes Through Rank-Wise Clustering", "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning\nlarge language models (LLMs) to various domains due to its modular design and\nwidespread availability on platforms like Huggingface. This modularity has\nsparked interest in combining multiple LoRAs to enhance LLM capabilities.\nHowever, existing methods for LoRA composition primarily focus on task-specific\nadaptations that require additional training, and current model merging\ntechniques often fail to fully leverage LoRA's modular nature, leading to\nparameter interference and performance degradation. In this paper, we\ninvestigate the feasibility of disassembling and reassembling multiple LoRAs at\na finer granularity, analogous to assembling LEGO blocks. We introduce the\nconcept of Minimal Semantic Units (MSUs), where the parameters corresponding to\neach rank in LoRA function as independent units. These MSUs demonstrate\npermutation invariance and concatenation-summation equivalence properties,\nenabling flexible combinations to create new LoRAs. Building on these insights,\nwe propose the LoRA-LEGO framework. This framework conducts rank-wise parameter\nclustering by grouping MSUs from different LoRAs into $k$ clusters. The\ncentroid of each cluster serves as a representative MSU, enabling the assembly\nof a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual\nreweighting strategy to optimize the scale of the merged LoRA. Experiments\nacross various benchmarks demonstrate that our method outperforms existing\napproaches in LoRA merging.", "published": "2024-09-24 15:08:41", "link": "http://arxiv.org/abs/2409.16167v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Enhancing Linked Data Retrieval in Conversational UIs using\n  Large Language Models", "abstract": "Despite the recent broad adoption of Large Language Models (LLMs) across\nvarious domains, their potential for enriching information systems in\nextracting and exploring Linked Data (LD) and Resource Description Framework\n(RDF) triplestores has not been extensively explored. This paper examines the\nintegration of LLMs within existing systems, emphasising the enhancement of\nconversational user interfaces (UIs) and their capabilities for data extraction\nby producing more accurate SPARQL queries without the requirement for model\nretraining. Typically, conversational UI models necessitate retraining with the\nintroduction of new datasets or updates, limiting their functionality as\ngeneral-purpose extraction tools. Our approach addresses this limitation by\nincorporating LLMs into the conversational UI workflow, significantly enhancing\ntheir ability to comprehend and process user queries effectively. By leveraging\nthe advanced natural language understanding capabilities of LLMs, our method\nimproves RDF entity extraction within web systems employing conventional\nchatbots. This integration facilitates a more nuanced and context-aware\ninteraction model, critical for handling the complex query patterns often\nencountered in RDF datasets and Linked Open Data (LOD) endpoints. The\nevaluation of this methodology shows a marked enhancement in system\nexpressivity and the accuracy of responses to user queries, indicating a\npromising direction for future research in this area. This investigation not\nonly underscores the versatility of LLMs in enhancing existing information\nsystems but also sets the stage for further explorations into their potential\napplications within more specialised domains of web information systems.", "published": "2024-09-24 16:31:33", "link": "http://arxiv.org/abs/2409.16220v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs", "abstract": "Training large language models (LLMs) for external tool usage is a rapidly\nexpanding field, with recent research focusing on generating synthetic data to\naddress the shortage of available data. However, the absence of systematic data\nquality checks poses complications for properly training and testing models. To\nthat end, we propose two approaches for assessing the reliability of data for\ntraining LLMs to use external tools. The first approach uses intuitive,\nhuman-defined correctness criteria. The second approach uses a model-driven\nassessment with in-context evaluation. We conduct a thorough evaluation of data\nquality on two popular benchmarks, followed by an extrinsic evaluation that\nshowcases the impact of data quality on model performance. Our results\ndemonstrate that models trained on high-quality data outperform those trained\non unvalidated data, even when trained with a smaller quantity of data. These\nfindings empirically support the significance of assessing and ensuring the\nreliability of training data for tool-using LLMs.", "published": "2024-09-24 17:20:02", "link": "http://arxiv.org/abs/2409.16341v2", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "Revisiting Acoustic Features for Robust ASR", "abstract": "Automatic Speech Recognition (ASR) systems must be robust to the myriad types\nof noises present in real-world environments including environmental noise,\nroom impulse response, special effects as well as attacks by malicious actors\n(adversarial attacks). Recent works seek to improve accuracy and robustness by\ndeveloping novel Deep Neural Networks (DNNs) and curating diverse training\ndatasets for them, while using relatively simple acoustic features. While this\napproach improves robustness to the types of noise present in the training\ndata, it confers limited robustness against unseen noises and negligible\nrobustness to adversarial attacks. In this paper, we revisit the approach of\nearlier works that developed acoustic features inspired by biological auditory\nperception that could be used to perform accurate and robust ASR. In contrast,\nSpecifically, we evaluate the ASR accuracy and robustness of several\nbiologically inspired acoustic features. In addition to several features from\nprior works, such as gammatone filterbank features (GammSpec), we also propose\ntwo new acoustic features called frequency masked spectrogram (FreqMask) and\ndifference of gammatones spectrogram (DoGSpec) to simulate the\nneuro-psychological phenomena of frequency masking and lateral suppression.\nExperiments on diverse models and datasets show that (1) DoGSpec achieves\nsignificantly better robustness than the highly popular log mel spectrogram\n(LogMelSpec) with minimal accuracy degradation, and (2) GammSpec achieves\nbetter accuracy and robustness to non-adversarial noises from the Speech Robust\nBench benchmark, but it is outperformed by DoGSpec against adversarial attacks.", "published": "2024-09-24 18:58:23", "link": "http://arxiv.org/abs/2409.16399v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Comprehensive Survey of Bias in LLMs: Current Landscape and Future\n  Directions", "abstract": "Large Language Models(LLMs) have revolutionized various applications in\nnatural language processing (NLP) by providing unprecedented text generation,\ntranslation, and comprehension capabilities. However, their widespread\ndeployment has brought to light significant concerns regarding biases embedded\nwithin these models. This paper presents a comprehensive survey of biases in\nLLMs, aiming to provide an extensive review of the types, sources, impacts, and\nmitigation strategies related to these biases. We systematically categorize\nbiases into several dimensions. Our survey synthesizes current research\nfindings and discusses the implications of biases in real-world applications.\nAdditionally, we critically assess existing bias mitigation techniques and\npropose future research directions to enhance fairness and equity in LLMs. This\nsurvey serves as a foundational resource for researchers, practitioners, and\npolicymakers concerned with addressing and understanding biases in LLMs.", "published": "2024-09-24 19:50:38", "link": "http://arxiv.org/abs/2409.16430v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Spelling Correction through Rewriting of Non-Autoregressive ASR Lattices", "abstract": "For end-to-end Automatic Speech Recognition (ASR) models, recognizing\npersonal or rare phrases can be hard. A promising way to improve accuracy is\nthrough spelling correction (or rewriting) of the ASR lattice, where\npotentially misrecognized phrases are replaced with acoustically similar and\ncontextually relevant alternatives. However, rewriting is challenging for ASR\nmodels trained with connectionist temporal classification (CTC) due to noisy\nhypotheses produced by a non-autoregressive, context-independent beam search.\n  We present a finite-state transducer (FST) technique for rewriting wordpiece\nlattices generated by Transformer-based CTC models. Our algorithm performs\ngrapheme-to-phoneme (G2P) conversion directly from wordpieces into phonemes,\navoiding explicit word representations and exploiting the richness of the CTC\nlattice. Our approach requires no retraining or modification of the ASR model.\nWe achieved up to a 15.2% relative reduction in sentence error rate (SER) on a\ntest set with contextually relevant entities.", "published": "2024-09-24 21:42:25", "link": "http://arxiv.org/abs/2409.16469v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Exploring Knowledge Tracing in Tutor-Student Dialogues using LLMs", "abstract": "Recent advances in large language models (LLMs) have led to the development\nof artificial intelligence (AI)-powered tutoring chatbots, showing promise in\nproviding broad access to high-quality personalized education. Existing works\nhave studied how to make LLMs follow tutoring principles, but have not studied\nbroader uses of LLMs for supporting tutoring. Up until now, tracing student\nknowledge and analyzing misconceptions has been difficult and time-consuming to\nimplement for open-ended dialogue tutoring. In this work, we investigate\nwhether LLMs can be supportive of this task: we first use LLM prompting methods\nto identify the knowledge components/skills involved in each dialogue turn,\ni.e., a tutor utterance posing a task or a student utterance that responds to\nit. We also evaluate whether the student responds correctly to the tutor and\nverify the LLM's accuracy using human expert annotations. We then apply a range\nof knowledge tracing (KT) methods on the resulting labeled data to track\nstudent knowledge levels over an entire dialogue. We conduct experiments on two\ntutoring dialogue datasets, and show that a novel yet simple LLM-based method,\nLLMKT, significantly outperforms existing KT methods in predicting student\nresponse correctness in dialogues. We perform extensive qualitative analyses to\nhighlight the challenges in dialogueKT and outline multiple avenues for future\nwork.", "published": "2024-09-24 22:31:39", "link": "http://arxiv.org/abs/2409.16490v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark", "abstract": "Text-to-SQL systems have become crucial for translating natural language into\nSQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, the Execution Accuracy\n(EX), the most prevalent evaluation metric, still shows many false positives\nand negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our metric improves\nagreement with human experts (from 62 to 87.04 in Cohen's kappa) with\ncomprehensive context and sophisticated criteria. Our extensive experiments\nyield several key insights: (1) Models' performance increases by over 2.6\npoints on average, substantially affecting rankings on Spider and BIRD\nbenchmarks; (2) The underestimation of models in EX primarily stems from\nannotation quality issues; and (3) Model performance on particularly\nchallenging questions tends to be overestimated. This work contributes to a\nmore accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field.", "published": "2024-09-24 01:40:50", "link": "http://arxiv.org/abs/2409.19014v4", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Textless NLP -- Zero Resource Challenge with Low Resource Compute", "abstract": "This work addresses the persistent challenges of substantial training time\nand GPU resource requirements even when training lightweight encoder-vocoder\nmodels for Textless NLP. We reduce training steps significantly while improving\nperformance by a) leveraging learning rate schedulers for efficient and faster\nconvergence b) optimizing hop length and c) tuning the interpolation scale\nfactors for better audio quality. Additionally, we explore the latent space\nrepresentation for Indian languages such as Tamil and Bengali for the acoustic\nunit discovery and voice conversion task. Our approach leverages a quantized\nencoder architecture, in conjunction with a vocoder which utilizes the proposed\nmixture of optimized hop length, tuned interpolation scale factors and a cyclic\nlearning rate scheduler. We obtain consistently good results across English,\nTamil and Bengali datasets. The proposed method excels in capturing complex\nlinguistic patterns, resulting in clear reconstructed audio during voice\nconversion with significantly reduced training time.", "published": "2024-09-24 08:08:05", "link": "http://arxiv.org/abs/2409.19015v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "StyleFusion TTS: Multimodal Style-control and Enhanced Feature Fusion\n  for Zero-shot Text-to-speech Synthesis", "abstract": "We introduce StyleFusion-TTS, a prompt and/or audio referenced, style and\nspeaker-controllable, zero-shot text-to-speech (TTS) synthesis system designed\nto enhance the editability and naturalness of current research literature. We\npropose a general front-end encoder as a compact and effective module to\nutilize multimodal inputs including text prompts, audio references, and speaker\ntimbre references in a fully zero-shot manner and produce disentangled style\nand speaker control embeddings. Our novel approach also leverages a\nhierarchical conformer structure for the fusion of style and speaker control\nembeddings, aiming to achieve optimal feature fusion within the current\nadvanced TTS architecture. StyleFusion-TTS is evaluated through multiple\nmetrics, both subjectively and objectively. The system shows promising\nperformance across our evaluations, suggesting its potential to contribute to\nthe advancement of the field of zero-shot text-to-speech synthesis.", "published": "2024-09-24 04:55:17", "link": "http://arxiv.org/abs/2409.15741v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhancing Open-Set Speaker Identification through Rapid Tuning with\n  Speaker Reciprocal Points and Negative Sample", "abstract": "This paper introduces a novel framework for open-set speaker identification\nin household environments, playing a crucial role in facilitating seamless\nhuman-computer interactions. Addressing the limitations of current speaker\nmodels and classification approaches, our work integrates an pretrained WavLM\nfrontend with a few-shot rapid tuning neural network (NN) backend for\nenrollment, employing task-optimized Speaker Reciprocal Points Learning (SRPL)\nto enhance discrimination across multiple target speakers. Furthermore, we\npropose an enhanced version of SRPL (SRPL+), which incorporates negative sample\nlearning with both speech-synthesized and real negative samples to\nsignificantly improve open-set SID accuracy. Our approach is thoroughly\nevaluated across various multi-language text-dependent speaker recognition\ndatasets, demonstrating its effectiveness in achieving high usability for\ncomplex household multi-speaker recognition scenarios. The proposed system\nenhanced open-set performance by up to 27\\% over the directly use of efficient\nWavLM base+ model.", "published": "2024-09-24 04:55:52", "link": "http://arxiv.org/abs/2409.15742v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VoiceGuider: Enhancing Out-of-Domain Performance in Parameter-Efficient\n  Speaker-Adaptive Text-to-Speech via Autoguidance", "abstract": "When applying parameter-efficient finetuning via LoRA onto speaker adaptive\ntext-to-speech models, adaptation performance may decline compared to\nfull-finetuned counterparts, especially for out-of-domain speakers. Here, we\npropose VoiceGuider, a parameter-efficient speaker adaptive text-to-speech\nsystem reinforced with autoguidance to enhance the speaker adaptation\nperformance, reducing the gap against full-finetuned models. We carefully\nexplore various ways of strengthening autoguidance, ultimately finding the\noptimal strategy. VoiceGuider as a result shows robust adaptation performance\nespecially on extreme out-of-domain speech data. We provide audible samples in\nour demo page.", "published": "2024-09-24 05:26:49", "link": "http://arxiv.org/abs/2409.15759v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NanoVoice: Efficient Speaker-Adaptive Text-to-Speech for Multiple\n  Speakers", "abstract": "We present NanoVoice, a personalized text-to-speech model that efficiently\nconstructs voice adapters for multiple speakers simultaneously. NanoVoice\nintroduces a batch-wise speaker adaptation technique capable of fine-tuning\nmultiple references in parallel, significantly reducing training time. Beyond\nbuilding separate adapters for each speaker, we also propose a parameter\nsharing technique that reduces the number of parameters used for speaker\nadaptation. By incorporating a novel trainable scale matrix, NanoVoice\nmitigates potential performance degradation during parameter sharing. NanoVoice\nachieves performance comparable to the baselines, while training 4 times faster\nand using 45 percent fewer parameters for speaker adaptation with 40 reference\nvoices. Extensive ablation studies and analysis further validate the efficiency\nof our model.", "published": "2024-09-24 05:27:32", "link": "http://arxiv.org/abs/2409.15760v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Representation Loss Minimization with Randomized Selection Strategy for\n  Efficient Environmental Fake Audio Detection", "abstract": "The adaptation of foundation models has significantly advanced environmental\naudio deepfake detection (EADD), a rapidly growing area of research. These\nmodels are typically fine-tuned or utilized in their frozen states for\ndownstream tasks. However, the dimensionality of their representations can\nsubstantially lead to a high parameter count of downstream models, leading to\nhigher computational demands. So, a general way is to compress these\nrepresentations by leveraging state-of-the-art (SOTA) unsupervised\ndimensionality reduction techniques (PCA, SVD, KPCA, GRP) for efficient EADD.\nHowever, with the application of such techniques, we observe a drop in\nperformance. So in this paper, we show that representation vectors contain\nredundant information, and randomly selecting 40-50% of representation values\nand building downstream models on it preserves or sometimes even improves\nperformance. We show that such random selection preserves more performance than\nthe SOTA dimensionality reduction techniques while reducing model parameters\nand inference time by almost over half.", "published": "2024-09-24 05:46:52", "link": "http://arxiv.org/abs/2409.15767v1", "categories": ["eess.AS", "cs.SD", "68T45", "I.2.7"], "primary_category": "eess.AS"}
{"title": "M-Vec: Matryoshka Speaker Embeddings with Flexible Dimensions", "abstract": "Fixed-dimensional speaker embeddings have become the dominant approach in\nspeaker modeling, typically spanning hundreds to thousands of dimensions. These\ndimensions are hyperparameters that are not specifically picked, nor are they\nhierarchically ordered in terms of importance. In large-scale speaker\nrepresentation databases, reducing the dimensionality of embeddings can\nsignificantly lower storage and computational costs. However, directly training\nlow-dimensional representations often yields suboptimal performance. In this\npaper, we introduce the Matryoshka speaker embedding, a method that allows\ndynamic extraction of sub-dimensions from the embedding while maintaining\nperformance. Our approach is validated on the VoxCeleb dataset, demonstrating\nthat it can achieve extremely low-dimensional embeddings, such as 8 dimensions,\nwhile preserving high speaker verification performance.", "published": "2024-09-24 06:26:14", "link": "http://arxiv.org/abs/2409.15782v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "WeSep: A Scalable and Flexible Toolkit Towards Generalizable Target\n  Speaker Extraction", "abstract": "Target speaker extraction (TSE) focuses on isolating the speech of a specific\ntarget speaker from overlapped multi-talker speech, which is a typical setup in\nthe cocktail party problem. In recent years, TSE draws increasing attention due\nto its potential for various applications such as user-customized interfaces\nand hearing aids, or as a crutial front-end processing technologies for\nsubsequential tasks such as speech recognition and speaker recongtion. However,\nthere are currently few open-source toolkits or available pre-trained models\nfor off-the-shelf usage. In this work, we introduce WeSep, a toolkit designed\nfor research and practical applications in TSE. WeSep is featured with flexible\ntarget speaker modeling, scalable data management, effective on-the-fly data\nsimulation, structured recipes and deployment support. The toolkit is publicly\navaliable at \\url{https://github.com/wenet-e2e/WeSep.}", "published": "2024-09-24 06:47:12", "link": "http://arxiv.org/abs/2409.15799v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ESPnet-Codec: Comprehensive Training and Evaluation of Neural Codecs for\n  Audio, Music, and Speech", "abstract": "Neural codecs have become crucial to recent speech and audio generation\nresearch. In addition to signal compression capabilities, discrete codecs have\nalso been found to enhance downstream training efficiency and compatibility\nwith autoregressive language models. However, as extensive downstream\napplications are investigated, challenges have arisen in ensuring fair\ncomparisons across diverse applications. To address these issues, we present a\nnew open-source platform ESPnet-Codec, which is built on ESPnet and focuses on\nneural codec training and evaluation. ESPnet-Codec offers various recipes in\naudio, music, and speech for training and evaluation using several widely\nadopted codec models. Together with ESPnet-Codec, we present VERSA, a\nstandalone evaluation toolkit, which provides a comprehensive evaluation of\ncodec performance over 20 audio evaluation metrics. Notably, we demonstrate\nthat ESPnet-Codec can be integrated into six ESPnet tasks, supporting diverse\napplications.", "published": "2024-09-24 09:16:11", "link": "http://arxiv.org/abs/2409.15897v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Generative Speech Foundation Model Pretraining for High-Quality Speech\n  Extraction and Restoration", "abstract": "This paper proposes a generative pretraining foundation model for\nhigh-quality speech restoration tasks. By directly operating on complex-valued\nshort-time Fourier transform coefficients, our model does not rely on any\nvocoders for time-domain signal reconstruction. As a result, our model\nsimplifies the synthesis process and removes the quality upper-bound introduced\nby any mel-spectrogram vocoder compared to prior work SpeechFlow. The proposed\nmethod is evaluated on multiple speech restoration tasks, including speech\ndenoising, bandwidth extension, codec artifact removal, and target speaker\nextraction. In all scenarios, finetuning our pretrained model results in\nsuperior performance over strong baselines. Notably, in the target speaker\nextraction task, our model outperforms existing systems, including those\nleveraging SSL-pretrained encoders like WavLM. The code and the pretrained\ncheckpoints are publicly available in the NVIDIA NeMo framework.", "published": "2024-09-24 14:24:47", "link": "http://arxiv.org/abs/2409.16117v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Explicit Consistency-Preserving Loss Function for Phase\n  Reconstruction and Speech Enhancement", "abstract": "In this work, we propose a novel consistency-preserving loss function for\nrecovering the phase information in the context of phase reconstruction (PR)\nand speech enhancement (SE). Different from conventional techniques that\ndirectly estimate the phase using a deep model, our idea is to exploit ad-hoc\nconstraints to directly generate a consistent pair of magnitude and phase.\nSpecifically, the proposed loss forces a set of complex numbers to be a\nconsistent short-time Fourier transform (STFT) representation, i.e., to be the\nspectrogram of a real signal. Our approach thus avoids the difficulty of\nestimating the original phase, which is highly unstructured and sensitive to\ntime shift. The influence of our proposed loss is first assessed on a PR task,\nexperimentally demonstrating that our approach is viable. Next, we show its\neffectiveness on an SE task, using both the VB-DMD and WSJ0-CHiME3 data sets.\nOn VB-DMD, our approach is competitive with conventional solutions. On the\nchallenging WSJ0-CHiME3 set, the proposed framework compares favourably over\nthose techniques that explicitly estimate the phase.", "published": "2024-09-24 17:53:46", "link": "http://arxiv.org/abs/2409.16282v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Whisper in Medusa's Ear: Multi-head Efficient Decoding for\n  Transformer-based ASR", "abstract": "Large transformer-based models have significant potential for speech\ntranscription and translation. Their self-attention mechanisms and parallel\nprocessing enable them to capture complex patterns and dependencies in audio\nsequences. However, this potential comes with challenges, as these large and\ncomputationally intensive models lead to slow inference speeds. Various\noptimization strategies have been proposed to improve performance, including\nefficient hardware utilization and algorithmic enhancements. In this paper, we\nintroduce Whisper-Medusa, a novel approach designed to enhance processing speed\nwith minimal impact on Word Error Rate (WER). The proposed model extends the\nOpenAI's Whisper architecture by predicting multiple tokens per iteration,\nresulting in a 50% reduction in latency. We showcase the effectiveness of\nWhisper-Medusa across different learning setups and datasets.", "published": "2024-09-24 08:42:31", "link": "http://arxiv.org/abs/2409.15869v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Interpolation Filter Design for Sample Rate Independent Audio Effect\n  RNNs", "abstract": "Recurrent neural networks (RNNs) are effective at emulating the non-linear,\nstateful behavior of analog guitar amplifiers and distortion effects. Unlike\nthe case of direct circuit simulation, RNNs have a fixed sample rate encoded in\ntheir model weights, making the sample rate non-adjustable during inference.\nRecent work has proposed increasing the sample rate of RNNs at inference\n(oversampling) by increasing the feedback delay length in samples, using a\nfractional delay filter for non-integer conversions. Here, we investigate the\ntask of lowering the sample rate at inference (undersampling), and propose\nusing an extrapolation filter to approximate the required fractional signal\nadvance. We consider two filter design methods and analyse the impact of filter\norder on audio quality. Our results show that the correct choice of filter can\ngive high quality results for both oversampling and undersampling; however, in\nsome cases the sample rate adjustment leads to unwanted artefacts in the output\nsignal. We analyse these failure cases through linearised stability analysis,\nshowing that they result from instability around a fixed point. This approach\nenables an informed prediction of suitable interpolation filters for a given\nRNN model before runtime.", "published": "2024-09-24 08:56:25", "link": "http://arxiv.org/abs/2409.15884v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "On the calibration of powerset speaker diarization models", "abstract": "End-to-end neural diarization models have usually relied on a\nmultilabel-classification formulation of the speaker diarization problem.\nRecently, we proposed a powerset multiclass formulation that has beaten the\nstate-of-the-art on multiple datasets. In this paper, we propose to study the\ncalibration of a powerset speaker diarization model, and explore some of its\nuses. We study the calibration in-domain, as well as out-of-domain, and explore\nthe data in low-confidence regions. The reliability of model confidence is then\ntested in practice: we use the confidence of the pretrained model to\nselectively create training and validation subsets out of unannotated data, and\ncompare this to random selection. We find that top-label confidence can be used\nto reliably predict high-error regions. Moreover, training on low-confidence\nregions provides a better calibrated model, and validating on low-confidence\nregions can be more annotation-efficient than random regions.", "published": "2024-09-24 08:56:42", "link": "http://arxiv.org/abs/2409.15885v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Boosting Code-Switching ASR with Mixture of Experts Enhanced\n  Speech-Conditioned LLM", "abstract": "In this paper, we introduce a speech-conditioned Large Language Model (LLM)\nintegrated with a Mixture of Experts (MoE) based connector to address the\nchallenge of Code-Switching (CS) in Automatic Speech Recognition (ASR).\nSpecifically, we propose an Insertion and Deletion of Interruption Token (IDIT)\nmechanism for better transfer text generation ability of LLM to speech\nrecognition task. We also present a connecter with MoE architecture that\nmanages multiple languages efficiently. To further enhance the collaboration of\nmultiple experts and leverage the understanding capabilities of LLM, we propose\na two-stage progressive training strategy: 1) The connector is unfrozen and\ntrained with language-specialized experts to map speech representations to the\ntext space. 2) The connector and LLM LoRA adaptor are trained with the proposed\nIDIT mechanism and all experts are activated to learn general representations.\nExperimental results demonstrate that our method significantly outperforms\nstate-of-the-art models, including end-to-end and large-scale audio-language\nmodels.", "published": "2024-09-24 09:20:22", "link": "http://arxiv.org/abs/2409.15905v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ASD-Diffusion: Anomalous Sound Detection with Diffusion Models", "abstract": "Unsupervised Anomalous Sound Detection (ASD) aims to design a generalizable\nmethod that can be used to detect anomalies when only normal sounds are given.\nIn this paper, Anomalous Sound Detection based on Diffusion Models\n(ASD-Diffusion) is proposed for ASD in real-world factories. In our pipeline,\nthe anomalies in acoustic features are reconstructed from their noisy corrupted\nfeatures into their approximate normal pattern. Secondly, a post-processing\nanomalies filter algorithm is proposed to detect anomalies that exhibit\nsignificant deviation from the original input after reconstruction.\nFurthermore, denoising diffusion implicit model is introduced to accelerate the\ninference speed by a longer sampling interval of the denoising process. The\nproposed method is innovative in the application of diffusion models as a new\nscheme. Experimental results on the development set of DCASE 2023 challenge\ntask 2 outperform the baseline by 7.75%, demonstrating the effectiveness of the\nproposed method.", "published": "2024-09-24 10:42:23", "link": "http://arxiv.org/abs/2409.15957v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Disentangling Age and Identity with a Mutual Information Minimization\n  Approach for Cross-Age Speaker Verification", "abstract": "There has been an increasing research interest in cross-age speaker\nverification~(CASV). However, existing speaker verification systems perform\npoorly in CASV due to the great individual differences in voice caused by\naging. In this paper, we propose a disentangled representation learning\nframework for CASV based on mutual information~(MI) minimization. In our\nmethod, a backbone model is trained to disentangle the identity- and\nage-related embeddings from speaker information, and an MI estimator is trained\nto minimize the correlation between age- and identity-related embeddings via MI\nminimization, resulting in age-invariant speaker embeddings. Furthermore, by\nusing the age gaps between positive and negative samples, we propose an\naging-aware MI minimization loss function that allows the backbone model to\nfocus more on the vocal changes with large age gaps. Experimental results show\nthat the proposed method outperforms other methods on multiple Cross-Age test\nsets of Vox-CA.", "published": "2024-09-24 11:08:23", "link": "http://arxiv.org/abs/2409.15974v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Leveraging Mixture of Experts for Improved Speech Deepfake Detection", "abstract": "Speech deepfakes pose a significant threat to personal security and content\nauthenticity. Several detectors have been proposed in the literature, and one\nof the primary challenges these systems have to face is the generalization over\nunseen data to identify fake signals across a wide range of datasets. In this\npaper, we introduce a novel approach for enhancing speech deepfake detection\nperformance using a Mixture of Experts architecture. The Mixture of Experts\nframework is well-suited for the speech deepfake detection task due to its\nability to specialize in different input types and handle data variability\nefficiently. This approach offers superior generalization and adaptability to\nunseen data compared to traditional single models or ensemble methods.\nAdditionally, its modular structure supports scalable updates, making it more\nflexible in managing the evolving complexity of deepfake techniques while\nmaintaining high detection accuracy. We propose an efficient, lightweight\ngating mechanism to dynamically assign expert weights for each input,\noptimizing detection performance. Experimental results across multiple datasets\ndemonstrate the effectiveness and potential of our proposed approach.", "published": "2024-09-24 13:24:03", "link": "http://arxiv.org/abs/2409.16077v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Scenario of Use Scheme: Threat Model Specification for Speaker Privacy\n  Protection in the Medical Domain", "abstract": "Speech recordings are being more frequently used to detect and monitor\ndisease, leading to privacy concerns. Beyond cryptography, protection of speech\ncan be addressed by approaches, such as perturbation, disentanglement, and\nre-synthesis, that eliminate sensitive information of the speaker, leaving the\ninformation necessary for medical analysis purposes. In order for such privacy\nprotective approaches to be developed, clear and systematic specifications of\nassumptions concerning medical settings and the needs of medical professionals\nare necessary. In this paper, we propose a Scenario of Use Scheme that\nincorporates an Attacker Model, which characterizes the adversary against whom\nthe speaker's privacy must be defended, and a Protector Model, which specifies\nthe defense. We discuss the connection of the scheme with previous work on\nspeech privacy. Finally, we present a concrete example of a specified Scenario\nof Use and a set of experiments about protecting speaker data against gender\ninference attacks while maintaining utility for Parkinson's detection.", "published": "2024-09-24 14:07:47", "link": "http://arxiv.org/abs/2409.16106v2", "categories": ["eess.AS", "cs.AI", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Evaluation of state-of-the-art ASR Models in Child-Adult Interactions", "abstract": "The ability to reliably transcribe child-adult conversations in a clinical\nsetting is valuable for diagnosis and understanding of numerous developmental\ndisorders such as Autism Spectrum Disorder. Recent advances in deep learning\narchitectures and availability of large scale transcribed data has led to\ndevelopment of speech foundation models that have shown dramatic improvements\nin ASR performance. However, the ability of these models to translate well to\nconversational child-adult interactions is under studied. In this work, we\nprovide a comprehensive evaluation of ASR performance on a dataset containing\nchild-adult interactions from autism diagnostic sessions, using Whisper,\nWav2Vec2, HuBERT, and WavLM. We find that speech foundation models show a\nnoticeable performance drop (15-20% absolute WER) for child speech compared to\nadult speech in the conversational setting. Then, we employ LoRA on the best\nperforming zero shot model (whisper-large) to probe the effectiveness of\nfine-tuning in a low resource setting, resulting in ~8% absolute WER\nimprovement for child speech and ~13% absolute WER improvement for adult\nspeech.", "published": "2024-09-24 14:42:37", "link": "http://arxiv.org/abs/2409.16135v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Facial Expression-Enhanced TTS: Combining Face Representation and\n  Emotion Intensity for Adaptive Speech", "abstract": "We propose FEIM-TTS, an innovative zero-shot text-to-speech (TTS) model that\nsynthesizes emotionally expressive speech, aligned with facial images and\nmodulated by emotion intensity. Leveraging deep learning, FEIM-TTS transcends\ntraditional TTS systems by interpreting facial cues and adjusting to emotional\nnuances without dependence on labeled datasets. To address sparse\naudio-visual-emotional data, the model is trained using LRS3, CREMA-D, and MELD\ndatasets, demonstrating its adaptability. FEIM-TTS's unique capability to\nproduce high-quality, speaker-agnostic speech makes it suitable for creating\nadaptable voices for virtual characters. Moreover, FEIM-TTS significantly\nenhances accessibility for individuals with visual impairments or those who\nhave trouble seeing. By integrating emotional nuances into TTS, our model\nenables dynamic and engaging auditory experiences for webcomics, allowing\nvisually impaired users to enjoy these narratives more fully. Comprehensive\nevaluation evidences its proficiency in modulating emotion and intensity,\nadvancing emotional speech synthesis and accessibility. Samples are available\nat: https://feim-tts.github.io/.", "published": "2024-09-24 16:01:12", "link": "http://arxiv.org/abs/2409.16203v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FastTalker: Jointly Generating Speech and Conversational Gestures from\n  Text", "abstract": "Generating 3D human gestures and speech from a text script is critical for\ncreating realistic talking avatars. One solution is to leverage separate\npipelines for text-to-speech (TTS) and speech-to-gesture (STG), but this\napproach suffers from poor alignment of speech and gestures and slow inference\ntimes. In this paper, we introduce FastTalker, an efficient and effective\nframework that simultaneously generates high-quality speech audio and 3D human\ngestures at high inference speeds. Our key insight is reusing the intermediate\nfeatures from speech synthesis for gesture generation, as these features\ncontain more precise rhythmic information than features re-extracted from\ngenerated speech. Specifically, 1) we propose an end-to-end framework that\nconcurrently generates speech waveforms and full-body gestures, using\nintermediate speech features such as pitch, onset, energy, and duration\ndirectly for gesture decoding; 2) we redesign the causal network architecture\nto eliminate dependencies on future inputs for real applications; 3) we employ\nReinforcement Learning-based Neural Architecture Search (NAS) to enhance both\nperformance and inference speed by optimizing our network architecture.\nExperimental results on the BEAT2 dataset demonstrate that FastTalker achieves\nstate-of-the-art performance in both speech synthesis and gesture generation,\nprocessing speech and gestures in 0.17 seconds per second on an NVIDIA 3090.", "published": "2024-09-24 19:06:18", "link": "http://arxiv.org/abs/2409.16404v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
