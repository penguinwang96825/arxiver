{"title": "Neural Transition-based Syntactic Linearization", "abstract": "The task of linearization is to find a grammatical order given a set of\nwords. Traditional models use statistical methods. Syntactic linearization\nsystems, which generate a sentence along with its syntactic tree, have shown\nstate-of-the-art performance. Recent work shows that a multi-layer LSTM\nlanguage model outperforms competitive statistical syntactic linearization\nsystems without using syntax. In this paper, we study neural syntactic\nlinearization, building a transition-based syntactic linearizer leveraging a\nfeed-forward neural network, observing significantly better results compared to\nLSTM language models on this task.", "published": "2018-10-23 00:47:24", "link": "http://arxiv.org/abs/1810.09609v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-supervised acoustic model training for speech with code-switching", "abstract": "In the FAME! project, we aim to develop an automatic speech recognition (ASR)\nsystem for Frisian-Dutch code-switching (CS) speech extracted from the archives\nof a local broadcaster with the ultimate goal of building a spoken document\nretrieval system. Unlike Dutch, Frisian is a low-resourced language with a very\nlimited amount of manually annotated speech data. In this paper, we describe\nseveral automatic annotation approaches to enable using of a large amount of\nraw bilingual broadcast data for acoustic model training in a semi-supervised\nsetting. Previously, it has been shown that the best-performing ASR system is\nobtained by two-stage multilingual deep neural network (DNN) training using 11\nhours of manually annotated CS speech (reference) data together with speech\ndata from other high-resourced languages. We compare the quality of\ntranscriptions provided by this bilingual ASR system with several other\napproaches that use a language recognition system for assigning language labels\nto raw speech segments at the front-end and using monolingual ASR resources for\ntranscription. We further investigate automatic annotation of the speakers\nappearing in the raw broadcast data by first labeling with (pseudo) speaker\ntags using a speaker diarization system and then linking to the known speakers\nappearing in the reference data using a speaker recognition system. These\nspeaker labels are essential for speaker-adaptive training in the proposed\nsetting. We train acoustic models using the manually and automatically\nannotated data and run recognition experiments on the development and test data\nof the FAME! speech corpus to quantify the quality of the automatic\nannotations. The ASR and CS detection results demonstrate the potential of\nusing automatic language and speaker tagging in semi-supervised bilingual\nacoustic model training.", "published": "2018-10-23 07:33:06", "link": "http://arxiv.org/abs/1810.09699v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Testing the Generalization Power of Neural Network Models Across NLI\n  Benchmarks", "abstract": "Neural network models have been very successful in natural language\ninference, with the best models reaching 90% accuracy in some benchmarks.\nHowever, the success of these models turns out to be largely benchmark\nspecific. We show that models trained on a natural language inference dataset\ndrawn from one benchmark fail to perform well in others, even if the notion of\ninference assumed in these benchmarks is the same or similar. We train six high\nperforming neural network models on different datasets and show that each one\nof these has problems of generalizing when we replace the original test set\nwith a test set taken from another corpus designed for the same task. In light\nof these results, we argue that most of the current neural network models are\nnot able to generalize well in the task of natural language inference. We find\nthat using large pre-trained language models helps with transfer learning when\nthe datasets are similar enough. Our results also highlight that the current\nNLI datasets do not cover the different nuances of inference extensively\nenough.", "published": "2018-10-23 11:06:55", "link": "http://arxiv.org/abs/1810.09774v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Object-oriented lexical encoding of multiword expressions: Short and\n  sweet", "abstract": "Multiword expressions (MWEs) exhibit both regular and idiosyncratic\nproperties. Their idiosyncrasy requires lexical encoding in parallel with their\ncomponent words. Their (at times intricate) regularity, on the other hand,\ncalls for means of flexible factorization to avoid redundant descriptions of\nshared properties. However, so far, non-redundant general-purpose lexical\nencoding of MWEs has not received a satisfactory solution. We offer a proof of\nconcept that this challenge might be effectively addressed within eXtensible\nMetaGrammar (XMG), an object-oriented metagrammar framework. We first make an\nexisting metagrammatical resource, the FrenchTAG grammar, MWE-aware. We then\nevaluate the factorization gain during incremental implementation with XMG on a\ndataset extracted from an MWE-annotated reference corpus.", "published": "2018-10-23 16:27:07", "link": "http://arxiv.org/abs/1810.09947v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Graph Convolutional Encoders for Structured Data to Text Generation", "abstract": "Most previous work on neural text generation from graph-structured data\nrelies on standard sequence-to-sequence methods. These approaches linearise the\ninput graph to be fed to a recurrent neural network. In this paper, we propose\nan alternative encoder based on graph convolutional networks that directly\nexploits the input structure. We report results on two graph-to-sequence\ndatasets that empirically show the benefits of explicitly encoding the input\ngraph structure.", "published": "2018-10-23 17:56:23", "link": "http://arxiv.org/abs/1810.09995v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Modeling at Scale", "abstract": "We show how Zipf's Law can be used to scale up language modeling (LM) to take\nadvantage of more training data and more GPUs. LM plays a key role in many\nimportant natural language applications such as speech recognition and machine\ntranslation. Scaling up LM is important since it is widely accepted by the\ncommunity that there is no data like more data. Eventually, we would like to\ntrain on terabytes (TBs) of text (trillions of words). Modern training methods\nare far from this goal, because of various bottlenecks, especially memory\n(within GPUs) and communication (across GPUs). This paper shows how Zipf's Law\ncan address these bottlenecks by grouping parameters for common words and\ncharacter sequences, because $U \\ll N$, where $U$ is the number of unique words\n(types) and $N$ is the size of the training set (tokens). For a local batch\nsize $K$ with $G$ GPUs and a $D$-dimension embedding matrix, we reduce the\noriginal per-GPU memory and communication asymptotic complexity from\n$\\Theta(GKD)$ to $\\Theta(GK + UD)$. Empirically, we find $U \\propto\n(GK)^{0.64}$ on four publicly available large datasets. When we scale up the\nnumber of GPUs to 64, a factor of 8, training time speeds up by factors up to\n6.7$\\times$ (for character LMs) and 6.3$\\times$ (for word LMs) with negligible\nloss of accuracy. Our weak scaling on 192 GPUs on the Tieba dataset shows a\n35\\% improvement in LM prediction accuracy by training on 93 GB of data\n(2.5$\\times$ larger than publicly available SOTA dataset), but taking only\n1.25$\\times$ increase in training time, compared to 3 GB of the same dataset\nrunning on 6 GPUs.", "published": "2018-10-23 18:44:55", "link": "http://arxiv.org/abs/1810.10045v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta-Learning Multi-task Communication", "abstract": "In this paper, we describe a general framework: Parameters Read-Write\nNetworks (PRaWNs) to systematically analyze current neural models for\nmulti-task learning, in which we find that existing models expect to\ndisentangle features into different spaces while features learned in practice\nare still entangled in shared space, leaving potential hazards for other\ntraining or unseen tasks.\n  We propose to alleviate this problem by incorporating an inductive bias into\nthe process of multi-task learning, that each task can keep informed of not\nonly the knowledge stored in other tasks but the way how other tasks maintain\ntheir knowledge.\n  In practice, we achieve above inductive bias by allowing different tasks to\ncommunicate by passing both hidden variables and gradients explicitly.\n  Experimentally, we evaluate proposed methods on three groups of tasks and two\ntypes of settings (\\textsc{in-task} and \\textsc{out-of-task}). Quantitative and\nqualitative results show their effectiveness.", "published": "2018-10-23 17:42:17", "link": "http://arxiv.org/abs/1810.09988v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A Neural Compositional Paradigm for Image Captioning", "abstract": "Mainstream captioning models often follow a sequential structure to generate\ncaptions, leading to issues such as introduction of irrelevant semantics, lack\nof diversity in the generated captions, and inadequate generalization\nperformance. In this paper, we present an alternative paradigm for image\ncaptioning, which factorizes the captioning procedure into two stages: (1)\nextracting an explicit semantic representation from the given image; and (2)\nconstructing the caption based on a recursive compositional procedure in a\nbottom-up manner. Compared to conventional ones, our paradigm better preserves\nthe semantic content through an explicit factorization of semantics and syntax.\nBy using the compositional generation procedure, caption construction follows a\nrecursive structure, which naturally fits the properties of human language.\nMoreover, the proposed compositional procedure requires less data to train,\ngeneralizes better, and yields more diverse captions.", "published": "2018-10-23 02:16:12", "link": "http://arxiv.org/abs/1810.09630v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference\n  Resolution", "abstract": "We introduce PreCo, a large-scale English dataset for coreference resolution.\nThe dataset is designed to embody the core challenges in coreference, such as\nentity representation, by alleviating the challenge of low overlap between\ntraining and test sets and enabling separated analysis of mention detection and\nmention clustering. To strengthen the training-test overlap, we collect a large\ncorpus of about 38K documents and 12.4M words which are mostly from the\nvocabulary of English-speaking preschoolers. Experiments show that with higher\ntraining-test overlap, error analysis on PreCo is more efficient than the one\non OntoNotes, a popular existing dataset. Furthermore, we annotate singleton\nmentions making it possible for the first time to quantify the influence that a\nmention detector makes on coreference resolution performance. The dataset is\nfreely available at https://preschool-lab.github.io/PreCo/.", "published": "2018-10-23 12:09:37", "link": "http://arxiv.org/abs/1810.09807v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Stepwise Acquisition of Dialogue Act Through Human-Robot Interaction", "abstract": "A dialogue act (DA) represents the meaning of an utterance at the\nillocutionary force level (Austin 1962) such as a question, a request, and a\ngreeting. Since DAs take charge of the most fundamental part of communication,\nwe believe that the elucidation of DA learning mechanism is important for\ncognitive science and artificial intelligence. The purpose of this study is to\nverify that scaffolding takes place when a human teaches a robot, and to let a\nrobot learn to estimate DAs and to make a response based on them step by step\nutilizing scaffolding provided by a human. To realize that, it is necessary for\nthe robot to detect changes in utterance and rewards given by the partner and\ncontinue learning accordingly. Experimental results demonstrated that\nparticipants who continued interaction for a sufficiently long time often gave\nscaffolding for the robot. Although the number of experiments is still\ninsufficient to obtain a definite conclusion, we observed that 1) the robot\nquickly learned to respond to DAs in most cases if the participants only spoke\nutterances that match the situation, 2) in the case of participants who builds\nscaffolding differently from what we assumed, learning did not proceed quickly,\nand 3) the robot could learn to estimate DAs almost exactly if the participants\nkept interaction for a sufficiently long time even if the scaffolding was\nunexpected.", "published": "2018-10-23 16:28:27", "link": "http://arxiv.org/abs/1810.09949v2", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Area Attention", "abstract": "Existing attention mechanisms are trained to attend to individual items in a\ncollection (the memory) with a predefined, fixed granularity, e.g., a word\ntoken or an image grid. We propose area attention: a way to attend to areas in\nthe memory, where each area contains a group of items that are structurally\nadjacent, e.g., spatially for a 2D memory such as images, or temporally for a\n1D memory such as natural language sentences. Importantly, the shape and the\nsize of an area are dynamically determined via learning, which enables a model\nto attend to information with varying granularity. Area attention can easily\nwork with existing model architectures such as multi-head attention for\nsimultaneously attending to multiple areas in the memory. We evaluate area\nattention on two tasks: neural machine translation (both character and\ntoken-level) and image captioning, and improve upon strong (state-of-the-art)\nbaselines in all the cases. These improvements are obtainable with a basic form\nof area attention that is parameter free.", "published": "2018-10-23 23:14:27", "link": "http://arxiv.org/abs/1810.10126v7", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "On the difference-to-sum power ratio of speech and wind noise based on\n  the Corcos model", "abstract": "The difference-to-sum power ratio was proposed and used to suppress wind\nnoise under specific acoustic conditions. In this contribution, a general\nformulation of the difference-to-sum power ratio associated with a mixture of\nspeech and wind noise is proposed and analyzed. In particular, it is assumed\nthat the complex coherence of convective turbulence can be modelled by the\nCorcos model. In contrast to the work in which the power ratio was first\npresented, the employed Corcos model holds for every possible air stream\ndirection and takes into account the lateral coherence decay rate. The obtained\nexpression is subsequently validated with real data for a dual microphone\nset-up. Finally, the difference-to- sum power ratio is exploited as a spatial\nfeature to indicate the frame-wise presence of wind noise, obtaining improved\ndetection performance when compared to an existing multi-channel wind noise\ndetection approach.", "published": "2018-10-23 07:57:33", "link": "http://arxiv.org/abs/1810.09708v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "SING: Symbol-to-Instrument Neural Generator", "abstract": "Recent progress in deep learning for audio synthesis opens the way to models\nthat directly produce the waveform, shifting away from the traditional paradigm\nof relying on vocoders or MIDI synthesizers for speech or music generation.\nDespite their successes, current state-of-the-art neural audio synthesizers\nsuch as WaveNet and SampleRNN suffer from prohibitive training and inference\ntimes because they are based on autoregressive models that generate audio\nsamples one at a time at a rate of 16kHz. In this work, we study the more\ncomputationally efficient alternative of generating the waveform frame-by-frame\nwith large strides. We present SING, a lightweight neural audio synthesizer for\nthe original task of generating musical notes given desired instrument, pitch\nand velocity. Our model is trained end-to-end to generate notes from nearly\n1000 instruments with a single decoder, thanks to a new loss function that\nminimizes the distances between the log spectrograms of the generated and\ntarget waveforms. On the generalization task of synthesizing notes for pairs of\npitch and instrument not seen during training, SING produces audio with\nsignificantly improved perceptual quality compared to a state-of-the-art\nautoencoder based on WaveNet as measured by a Mean Opinion Score (MOS), and is\nabout 32 times faster for training and 2, 500 times faster for inference.", "published": "2018-10-23 11:27:06", "link": "http://arxiv.org/abs/1810.09785v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
