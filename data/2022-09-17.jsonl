{"title": "APPDIA: A Discourse-aware Transformer-based Style Transfer Model for\n  Offensive Social Media Conversations", "abstract": "Using style-transfer models to reduce offensiveness of social media comments\ncan help foster a more inclusive environment. However, there are no sizable\ndatasets that contain offensive texts and their inoffensive counterparts, and\nfine-tuning pretrained models with limited labeled data can lead to the loss of\noriginal meaning in the style-transferred text. To address this issue, we\nprovide two major contributions. First, we release the first\npublicly-available, parallel corpus of offensive Reddit comments and their\nstyle-transferred counterparts annotated by expert sociolinguists. Then, we\nintroduce the first discourse-aware style-transfer models that can effectively\nreduce offensiveness in Reddit text while preserving the meaning of the\noriginal text. These models are the first to examine inferential links between\nthe comment and the text it is replying to when transferring the style of\noffensive Reddit text. We propose two different methods of integrating\ndiscourse relations with pretrained transformer models and evaluate them on our\ndataset of offensive comments from Reddit and their inoffensive counterparts.\nImprovements over the baseline with respect to both automatic metrics and human\nevaluation indicate that our discourse-aware models are better at preserving\nmeaning in style-transferred text when compared to the state-of-the-art\ndiscourse-agnostic models.", "published": "2022-09-17 00:50:24", "link": "http://arxiv.org/abs/2209.08207v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structured Knowledge Grounding for Question Answering", "abstract": "Can language models (LM) ground question-answering (QA) tasks in the\nknowledge base via inherent relational reasoning ability? While previous models\nthat use only LMs have seen some success on many QA tasks, more recent methods\ninclude knowledge graphs (KG) to complement LMs with their more logic-driven\nimplicit knowledge. However, effectively extracting information from structured\ndata, like KGs, empowers LMs to remain an open question, and current models\nrely on graph techniques to extract knowledge. In this paper, we propose to\nsolely leverage the LMs to combine the language and knowledge for knowledge\nbased question-answering with flexibility, breadth of coverage and structured\nreasoning. Specifically, we devise a knowledge construction method that\nretrieves the relevant context with a dynamic hop, which expresses more\ncomprehensivenes than traditional GNN-based techniques. And we devise a deep\nfusion mechanism to further bridge the information exchanging bottleneck\nbetween the language and the knowledge. Extensive experiments show that our\nmodel consistently demonstrates its state-of-the-art performance over\nCommensenseQA benchmark, showcasing the possibility to leverage LMs solely to\nrobustly ground QA into the knowledge base.", "published": "2022-09-17 08:48:50", "link": "http://arxiv.org/abs/2209.08284v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Disfluency Detection to Intent Detection and Slot Filling", "abstract": "We present the first empirical study investigating the influence of\ndisfluency detection on downstream tasks of intent detection and slot filling.\nWe perform this study for Vietnamese -- a low-resource language that has no\nprevious study as well as no public dataset available for disfluency detection.\nFirst, we extend the fluent Vietnamese intent detection and slot filling\ndataset PhoATIS by manually adding contextual disfluencies and annotating them.\nThen, we conduct experiments using strong baselines for disfluency detection\nand joint intent detection and slot filling, which are based on pre-trained\nlanguage models. We find that: (i) disfluencies produce negative effects on the\nperformances of the downstream intent detection and slot filling tasks, and\n(ii) in the disfluency context, the pre-trained multilingual language model\nXLM-R helps produce better intent detection and slot filling performances than\nthe pre-trained monolingual language model PhoBERT, and this is opposite to\nwhat generally found in the fluency context.", "published": "2022-09-17 16:03:57", "link": "http://arxiv.org/abs/2209.08359v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IMDB Spoiler Dataset", "abstract": "User-generated reviews are often our first point of contact when we consider\nwatching a movie or a TV show. However, beyond telling us the qualitative\naspects of the media we want to consume, reviews may inevitably contain\nundesired revelatory information (i.e. 'spoilers') such as the surprising fate\nof a character in a movie, or the identity of a murderer in a crime-suspense\nmovie, etc. In this paper, we present a high-quality movie-review based spoiler\ndataset to tackle the problem of spoiler detection and describe various\nresearch questions it can answer.", "published": "2022-09-17 22:31:06", "link": "http://arxiv.org/abs/2212.06034v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "News Headlines Dataset For Sarcasm Detection", "abstract": "Past studies in Sarcasm Detection mostly make use of Twitter datasets\ncollected using hashtag-based supervision but such datasets are noisy in terms\nof labels and language. Furthermore, many tweets are replies to other tweets,\nand detecting sarcasm in these requires the availability of contextual tweets.\nTo overcome the limitations related to noise in Twitter datasets, we curate\nNews Headlines Dataset from two news websites: TheOnion aims at producing\nsarcastic versions of current events, whereas HuffPost publishes real news. The\ndataset contains about 28K headlines out of which 13K are sarcastic. To make it\nmore useful, we have included the source links of the news articles so that\nmore data can be extracted as needed. In this paper, we describe various\ndetails about the dataset and potential use cases apart from Sarcasm Detection.", "published": "2022-09-17 22:25:36", "link": "http://arxiv.org/abs/2212.06035v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Selective Token Generation for Few-shot Natural Language Generation", "abstract": "Natural language modeling with limited training data is a challenging\nproblem, and many algorithms make use of large-scale pretrained language models\n(PLMs) for this due to its great generalization ability. Among them, additive\nlearning that incorporates a task-specific adapter on top of the fixed\nlarge-scale PLM has been popularly used in the few-shot setting. However, this\nadded adapter is still easy to disregard the knowledge of the PLM especially\nfor few-shot natural language generation (NLG) since an entire sequence is\nusually generated by only the newly trained adapter. Therefore, in this work,\nwe develop a novel additive learning algorithm based on reinforcement learning\n(RL) that selectively outputs language tokens between the task-general PLM and\nthe task-specific adapter during both training and inference. This output token\nselection over the two generators allows the adapter to take into account\nsolely the task-relevant parts in sequence generation, and therefore makes it\nmore robust to overfitting as well as more stable in RL training. In addition,\nto obtain the complementary adapter from the PLM for each few-shot task, we\nexploit a separate selecting module that is also simultaneously trained using\nRL. Experimental results on various few-shot NLG tasks including question\nanswering, data-to-text generation and text summarization demonstrate that the\nproposed selective token generation significantly outperforms the previous\nadditive learning algorithms based on the PLMs.", "published": "2022-09-17 00:48:52", "link": "http://arxiv.org/abs/2209.08206v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Lexical Substitution with Decontextualised Embeddings", "abstract": "We propose a new unsupervised method for lexical substitution using\npre-trained language models. Compared to previous approaches that use the\ngenerative capability of language models to predict substitutes, our method\nretrieves substitutes based on the similarity of contextualised and\ndecontextualised word embeddings, i.e. the average contextual representation of\na word in multiple contexts. We conduct experiments in English and Italian, and\nshow that our method substantially outperforms strong baselines and establishes\na new state-of-the-art without any explicit supervision or fine-tuning. We\nfurther show that our method performs particularly well at predicting\nlow-frequency substitutes, and also generates a diverse list of substitute\ncandidates, reducing morphophonetic or morphosyntactic biases induced by\narticle-noun agreement.", "published": "2022-09-17 03:51:47", "link": "http://arxiv.org/abs/2209.08236v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FR: Folded Rationalization with a Unified Encoder", "abstract": "Conventional works generally employ a two-phase model in which a generator\nselects the most important pieces, followed by a predictor that makes\npredictions based on the selected pieces. However, such a two-phase model may\nincur the degeneration problem where the predictor overfits to the noise\ngenerated by a not yet well-trained generator and in turn, leads the generator\nto converge to a sub-optimal model that tends to select senseless pieces. To\ntackle this challenge, we propose Folded Rationalization (FR) that folds the\ntwo phases of the rationale model into one from the perspective of text\nsemantic extraction. The key idea of FR is to employ a unified encoder between\nthe generator and predictor, based on which FR can facilitate a better\npredictor by access to valuable information blocked by the generator in the\ntraditional two-phase model and thus bring a better generator. Empirically, we\nshow that FR improves the F1 score by up to 10.3% as compared to\nstate-of-the-art methods.", "published": "2022-09-17 08:49:45", "link": "http://arxiv.org/abs/2209.08285v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Parameter-Efficient Conformers via Sharing Sparsely-Gated Experts for\n  End-to-End Speech Recognition", "abstract": "While transformers and their variant conformers show promising performance in\nspeech recognition, the parameterized property leads to much memory cost during\ntraining and inference. Some works use cross-layer weight-sharing to reduce the\nparameters of the model. However, the inevitable loss of capacity harms the\nmodel performance. To address this issue, this paper proposes a\nparameter-efficient conformer via sharing sparsely-gated experts. Specifically,\nwe use sparsely-gated mixture-of-experts (MoE) to extend the capacity of a\nconformer block without increasing computation. Then, the parameters of the\ngrouped conformer blocks are shared so that the number of parameters is\nreduced. Next, to ensure the shared blocks with the flexibility of adapting\nrepresentations at different levels, we design the MoE routers and\nnormalization individually. Moreover, we use knowledge distillation to further\nimprove the performance. Experimental results show that the proposed model\nachieves competitive performance with 1/3 of the parameters of the encoder,\ncompared with the full-parameter model.", "published": "2022-09-17 13:22:19", "link": "http://arxiv.org/abs/2209.08326v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "CodeQueries: A Dataset of Semantic Queries over Code", "abstract": "Developers often have questions about semantic aspects of code they are\nworking on, e.g., \"Is there a class whose parent classes declare a conflicting\nattribute?\". Answering them requires understanding code semantics such as\nattributes and inheritance relation of classes. An answer to such a question\nshould identify code spans constituting the answer (e.g., the declaration of\nthe subclass) as well as supporting facts (e.g., the definitions of the\nconflicting attributes). The existing work on question-answering over code has\nconsidered yes/no questions or method-level context. We contribute a labeled\ndataset, called CodeQueries, of semantic queries over Python code. Compared to\nthe existing datasets, in CodeQueries, the queries are about code semantics,\nthe context is file level and the answers are code spans. We curate the dataset\nbased on queries supported by a widely-used static analysis tool, CodeQL, and\ninclude both positive and negative examples, and queries requiring single-hop\nand multi-hop reasoning.\n  To assess the value of our dataset, we evaluate baseline neural approaches.\nWe study a large language model (GPT3.5-Turbo) in zero-shot and few-shot\nsettings on a subset of CodeQueries. We also evaluate a BERT style model\n(CuBERT) with fine-tuning. We find that these models achieve limited success on\nCodeQueries. CodeQueries is thus a challenging dataset to test the ability of\nneural models, to understand code semantics, in the extractive\nquestion-answering setting.", "published": "2022-09-17 17:09:30", "link": "http://arxiv.org/abs/2209.08372v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Survey of Query-based Text Summarization", "abstract": "Query-based text summarization is an important real world problem that\nrequires to condense the prolix text data into a summary under the guidance of\nthe query information provided by users. The topic has been studied for a long\ntime and there are many existing interesting research related to query-based\ntext summarization. Yet much of the work is not systematically surveyed. This\nsurvey aims at summarizing some interesting work in query-based text\nsummarization methods as well as related generic text summarization methods.\nNot all taxonomies in this paper exist the related work to the best of our\nknowledge and some analysis will be presented.", "published": "2022-09-17 05:34:32", "link": "http://arxiv.org/abs/2211.11548v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Detecting Generated Scientific Papers using an Ensemble of Transformer\n  Models", "abstract": "The paper describes neural models developed for the DAGPap22 shared task\nhosted at the Third Workshop on Scholarly Document Processing. This shared task\ntargets the automatic detection of generated scientific papers. Our work\nfocuses on comparing different transformer-based models as well as using\nadditional datasets and techniques to deal with imbalanced classes. As a final\nsubmission, we utilized an ensemble of SciBERT, RoBERTa, and DeBERTa fine-tuned\nusing random oversampling technique. Our model achieved 99.24% in terms of\nF1-score. The official evaluation results have put our system at the third\nplace.", "published": "2022-09-17 08:43:25", "link": "http://arxiv.org/abs/2209.08283v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; I.7.m; H.3.3"], "primary_category": "cs.CL"}
{"title": "An Empathetic AI Coach for Self-Attachment Therapy", "abstract": "In this work, we present a new dataset and a computational strategy for a\ndigital coach that aims to guide users in practicing the protocols of\nself-attachment therapy. Our framework augments a rule-based conversational\nagent with a deep-learning classifier for identifying the underlying emotion in\na user's text response, as well as a deep-learning assisted retrieval method\nfor producing novel, fluent and empathetic utterances. We also craft a set of\nhuman-like personas that users can choose to interact with. Our goal is to\nachieve a high level of engagement during virtual therapy sessions. We evaluate\nthe effectiveness of our framework in a non-clinical trial with N=16\nparticipants, all of whom have had at least four interactions with the agent\nover the course of five days. We find that our platform is consistently rated\nhigher for empathy, user engagement and usefulness than the simple rule-based\nframework. Finally, we provide guidelines to further improve the design and\nperformance of the application, in accordance with the feedback received.", "published": "2022-09-17 12:01:35", "link": "http://arxiv.org/abs/2209.08316v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Constrained Policy Optimization for Controlled Self-Learning in\n  Conversational AI Systems", "abstract": "Recently, self-learning methods based on user satisfaction metrics and\ncontextual bandits have shown promising results to enable consistent\nimprovements in conversational AI systems. However, directly targeting such\nmetrics by off-policy bandit learning objectives often increases the risk of\nmaking abrupt policy changes that break the current user experience. In this\nstudy, we introduce a scalable framework for supporting fine-grained\nexploration targets for individual domains via user-defined constraints. For\nexample, we may want to ensure fewer policy deviations in business-critical\ndomains such as shopping, while allocating more exploration budget to domains\nsuch as music. Furthermore, we present a novel meta-gradient learning approach\nthat is scalable and practical to address this problem. The proposed method\nadjusts constraint violation penalty terms adaptively through a meta objective\nthat encourages balanced constraint satisfaction across domains. We conduct\nextensive experiments using data from a real-world conversational AI on a set\nof realistic constraint benchmarks. Based on the experimental results, we\ndemonstrate that the proposed approach is capable of achieving the best balance\nbetween the policy value and constraint satisfaction rate.", "published": "2022-09-17 23:44:13", "link": "http://arxiv.org/abs/2209.08429v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Watch What You Pretrain For: Targeted, Transferable Adversarial Examples\n  on Self-Supervised Speech Recognition models", "abstract": "A targeted adversarial attack produces audio samples that can force an\nAutomatic Speech Recognition (ASR) system to output attacker-chosen text. To\nexploit ASR models in real-world, black-box settings, an adversary can leverage\nthe transferability property, i.e. that an adversarial sample produced for a\nproxy ASR can also fool a different remote ASR. However recent work has shown\nthat transferability against large ASR models is very difficult. In this work,\nwe show that modern ASR architectures, specifically ones based on\nSelf-Supervised Learning, are in fact vulnerable to transferability. We\nsuccessfully demonstrate this phenomenon by evaluating state-of-the-art\nself-supervised ASR models like Wav2Vec2, HuBERT, Data2Vec and WavLM. We show\nthat with low-level additive noise achieving a 30dB Signal-Noise Ratio, we can\nachieve target transferability with up to 80% accuracy. Next, we 1) use an\nablation study to show that Self-Supervised learning is the main cause of that\nphenomenon, and 2) we provide an explanation for this phenomenon. Through this\nwe show that modern ASR architectures are uniquely vulnerable to adversarial\nsecurity threats.", "published": "2022-09-17 15:01:26", "link": "http://arxiv.org/abs/2209.13523v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Compose & Embellish: Well-Structured Piano Performance Generation via A\n  Two-Stage Approach", "abstract": "Even with strong sequence models like Transformers, generating expressive\npiano performances with long-range musical structures remains challenging.\nMeanwhile, methods to compose well-structured melodies or lead sheets (melody +\nchords), i.e., simpler forms of music, gained more success. Observing the\nabove, we devise a two-stage Transformer-based framework that Composes a lead\nsheet first, and then Embellishes it with accompaniment and expressive touches.\nSuch a factorization also enables pretraining on non-piano data. Our objective\nand subjective experiments show that Compose & Embellish shrinks the gap in\nstructureness between a current state of the art and real performances by half,\nand improves other musical aspects such as richness and coherence as well.", "published": "2022-09-17 01:20:59", "link": "http://arxiv.org/abs/2209.08212v4", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Representation Learning Strategies to Model Pathological Speech: Effect\n  of Multiple Spectral Resolutions", "abstract": "This paper considers a representation learning strategy to model speech\nsignals from patients with Parkinson's disease and cleft lip and palate. In\nparticular, it compares different parametrized representation types such as\nwideband and narrowband spectrograms, and wavelet-based scalograms, with the\ngoal of quantifying the representation capacity of each. Methods for\nquantification include the ability of the proposed model to classify different\npathologies and the associated disease severity. Additionally, this paper\nproposes a novel fusion strategy called multi-spectral fusion that combines\nwideband and narrowband spectral resolutions using a representation learning\nstrategy based on autoencoders. The proposed models are able to classify the\nspeech from Parkinson's disease patients with accuracy up to 95\\%. The proposed\nmodels were also able to asses the dysarthria severity of Parkinson's disease\npatients with a Spearman correlation up to 0.75. These results outperform those\nobserved in literature where the same problem was addressed with the same\ncorpus.", "published": "2022-09-17 17:46:45", "link": "http://arxiv.org/abs/2209.08379v1", "categories": ["eess.AS", "cs.SD", "q-bio.QM"], "primary_category": "eess.AS"}
