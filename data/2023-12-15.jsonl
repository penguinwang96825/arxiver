{"title": "Riveter: Measuring Power and Social Dynamics Between Entities", "abstract": "Riveter provides a complete easy-to-use pipeline for analyzing verb\nconnotations associated with entities in text corpora. We prepopulate the\npackage with connotation frames of sentiment, power, and agency, which have\ndemonstrated usefulness for capturing social phenomena, such as gender bias, in\na broad range of corpora. For decades, lexical frameworks have been\nfoundational tools in computational social science, digital humanities, and\nnatural language processing, facilitating multifaceted analysis of text\ncorpora. But working with verb-centric lexica specifically requires natural\nlanguage processing skills, reducing their accessibility to other researchers.\nBy organizing the language processing pipeline, providing complete lexicon\nscores and visualizations for all entities in a corpus, and providing\nfunctionality for users to target specific research questions, Riveter greatly\nimproves the accessibility of verb lexica and can facilitate a broad range of\nfuture research.", "published": "2023-12-15 05:03:24", "link": "http://arxiv.org/abs/2312.09536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Picking the Underused Heads: A Network Pruning Perspective of Attention\n  Head Selection for Fusing Dialogue Coreference Information", "abstract": "The Transformer-based models with the multi-head self-attention mechanism are\nwidely used in natural language processing, and provide state-of-the-art\nresults. While the pre-trained language backbones are shown to implicitly\ncapture certain linguistic knowledge, explicitly incorporating structure-aware\nfeatures can bring about further improvement on the downstream tasks. However,\nsuch enhancement often requires additional neural components and increases\ntraining parameter size. In this work, we investigate the attention head\nselection and manipulation strategy for feature injection from a network\npruning perspective, and conduct a case study on dialogue summarization. We\nfirst rank attention heads in a Transformer-based summarizer with layer-wise\nimportance. We then select the underused heads through extensive analysis, and\ninject structure-aware features by manipulating the selected heads.\nExperimental results show that the importance-based head selection is effective\nfor feature injection, and dialogue summarization can be improved by\nincorporating coreference information via head manipulation.", "published": "2023-12-15 05:27:24", "link": "http://arxiv.org/abs/2312.09541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Marathon: A Race Through the Realm of Long Context with Large Language\n  Models", "abstract": "With the advancement of large language models (LLMs) and the expansion of\ntheir context windows, existing long-context benchmarks fall short in\neffectively evaluating the models' comprehension and reasoning abilities in\nextended texts. Moreover, conventional benchmarks relying on F1 metrics often\ninaccurately score responses: they may undervalue correct answers that differ\nfrom the reference responses and overvalue incorrect ones that resemble the\nreference texts. In response to these limitations, we introduce Marathon, a\nnovel evaluation benchmark that adopts a multiple-choice question format. It is\nspecifically designed to overcome the constraints of previous benchmarks and\nprovide a rapid, precise, and unbiased appraisal of the long-context\ncomprehension skills of large language models. We conducted comprehensive\nevaluations on the Marathon benchmark with a range of state-of-the-art LLMs and\nassessed the effectiveness of various optimization strategies tailored for\nlong-context generation. We anticipate that the Marathon benchmark and its\nassociated leaderboard will enable a more precise and equitable evaluation of\nLLMs' capabilities in understanding and reasoning over extended contexts.\nMarathon is available at https://github.com/Hambaobao/Marathon.", "published": "2023-12-15 05:30:14", "link": "http://arxiv.org/abs/2312.09542v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discovering Highly Influential Shortcut Reasoning: An Automated\n  Template-Free Approach", "abstract": "Shortcut reasoning is an irrational process of inference, which degrades the\nrobustness of an NLP model. While a number of previous work has tackled the\nidentification of shortcut reasoning, there are still two major limitations:\n(i) a method for quantifying the severity of the discovered shortcut reasoning\nis not provided; (ii) certain types of shortcut reasoning may be missed. To\naddress these issues, we propose a novel method for identifying shortcut\nreasoning. The proposed method quantifies the severity of the shortcut\nreasoning by leveraging out-of-distribution data and does not make any\nassumptions about the type of tokens triggering the shortcut reasoning. Our\nexperiments on Natural Language Inference and Sentiment Analysis demonstrate\nthat our framework successfully discovers known and unknown shortcut reasoning\nin the previous work.", "published": "2023-12-15 11:45:42", "link": "http://arxiv.org/abs/2312.09718v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RJUA-QA: A Comprehensive QA Dataset for Urology", "abstract": "We introduce RJUA-QA, a novel medical dataset for question answering (QA) and\nreasoning with clinical evidence, contributing to bridge the gap between\ngeneral large language models (LLMs) and medical-specific LLM applications.\nRJUA-QA is derived from realistic clinical scenarios and aims to facilitate\nLLMs in generating reliable diagnostic and advice. The dataset contains 2,132\ncurated Question-Context-Answer pairs, corresponding about 25,000 diagnostic\nrecords and clinical cases. The dataset covers 67 common urological disease\ncategories, where the disease coverage exceeds 97.6\\% of the population seeking\nmedical services in urology. Each data instance in RJUA-QA comprises: (1) a\nquestion mirroring real patient to inquiry about clinical symptoms and medical\nconditions, (2) a context including comprehensive expert knowledge, serving as\na reference for medical examination and diagnosis, (3) a doctor response\noffering the diagnostic conclusion and suggested examination guidance, (4) a\ndiagnosed clinical disease as the recommended diagnostic outcome, and (5)\nclinical advice providing recommendations for medical examination. RJUA-QA is\nthe first medical QA dataset for clinical reasoning over the patient inquiries,\nwhere expert-level knowledge and experience are required for yielding\ndiagnostic conclusions and medical examination advice. A comprehensive\nevaluation is conducted to evaluate the performance of both medical-specific\nand general LLMs on the RJUA-QA dataset. Our data is are publicly available at\n\\url{https://github.com/alipay/RJU_Ant_QA}.", "published": "2023-12-15 13:40:25", "link": "http://arxiv.org/abs/2312.09785v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ProCoT: Stimulating Critical Thinking and Writing of Students through\n  Engagement with Large Language Models (LLMs)", "abstract": "We introduce a novel writing method called Probing Chain-of-Thought (ProCoT),\nwhich potentially prevents students from cheating using a Large Language Model\n(LLM), such as ChatGPT, while enhancing their active learning. LLMs have\ndisrupted education and many other fields. For fear of students cheating, many\nhave resorted to banning their use. These LLMs are also known for\nhallucinations. We conduct studies with ProCoT in two different courses with 65\nstudents. The students in each course were asked to prompt an LLM of their\nchoice with one question from a set of four and required to affirm or refute\nstatements in the LLM output by using peer-reviewed references. The results\nshow two things: (1) ProCoT stimulates creative/critical thinking and writing\nof students through engagement with LLMs when we compare the LLM-only output to\nProCoT output and (2) ProCoT can prevent cheating because of clear limitations\nin existing LLMs, particularly ChatGPT, when we compare students' ProCoT output\nto LLM ProCoT output. We also discover that most students prefer to give\nanswers in fewer words than LLMs, which are typically verbose. The average word\ncounts for students in the first course, ChatGPT (v3.5), and Phind (v8) are\n208, 391 and 383, respectively.", "published": "2023-12-15 14:01:46", "link": "http://arxiv.org/abs/2312.09801v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grammatical information in BERT sentence embeddings as two-dimensional\n  arrays", "abstract": "Sentence embeddings induced with various transformer architectures encode\nmuch semantic and syntactic information in a distributed manner in a\none-dimensional array. We investigate whether specific grammatical information\ncan be accessed in these distributed representations. Using data from a task\ndeveloped to test rule-like generalizations, our experiments on detecting\nsubject-verb agreement yield several promising results. First, we show that\nwhile the usual sentence representations encoded as one-dimensional arrays do\nnot easily support extraction of rule-like regularities, a two-dimensional\nreshaping of these vectors allows various learning architectures to access such\ninformation. Next, we show that various architectures can detect patterns in\nthese two-dimensional reshaped sentence embeddings and successfully learn a\nmodel based on smaller amounts of simpler training data, which performs well on\nmore complex test data. This indicates that current sentence embeddings contain\ninformation that is regularly distributed, and which can be captured when the\nembeddings are reshaped into higher dimensional arrays. Our results cast light\non representations produced by language models and help move towards developing\nfew-shot learning approaches.", "published": "2023-12-15 15:41:52", "link": "http://arxiv.org/abs/2312.09890v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models\n  via MoE-Style Plugin", "abstract": "Supervised fine-tuning (SFT) is a crucial step for large language models\n(LLMs), enabling them to align with human instructions and enhance their\ncapabilities in downstream tasks. Increasing instruction data substantially is\na direct solution to align the model with a broader range of downstream tasks\nor notably improve its performance on a specific task. However, we find that\nlarge-scale increases in instruction data can damage the world knowledge\npreviously stored in LLMs. To address this challenge, we propose LoRAMoE, a\nnovelty framework that introduces several low-rank adapters (LoRA) and\nintegrates them by using a router network, like a plugin version of Mixture of\nExperts (MoE). It freezes the backbone model and forces a portion of LoRAs to\nfocus on leveraging world knowledge to solve downstream tasks, to alleviate\nworld knowledge-edge forgetting. Experimental results show that, as the\ninstruction data increases, LoRAMoE can significantly improve the ability to\nprocess downstream tasks, while maintaining the world knowledge stored in the\nLLM.", "published": "2023-12-15 17:45:06", "link": "http://arxiv.org/abs/2312.09979v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian\n  Language", "abstract": "Large Language Models represent state-of-the-art linguistic models designed\nto equip computers with the ability to comprehend natural language. With its\nexceptional capacity to capture complex contextual relationships, the LLaMA\n(Large Language Model Meta AI) family represents a novel advancement in the\nfield of natural language processing by releasing foundational models designed\nto improve the natural language understanding abilities of the transformer\narchitecture thanks to their large amount of trainable parameters (7, 13, and\n70 billion parameters). In many natural language understanding tasks, these\nmodels obtain the same performances as private company models such as OpenAI\nChat-GPT with the advantage to make publicly available weights and code for\nresearch and commercial uses. In this work, we investigate the possibility of\nLanguage Adaptation for LLaMA models, explicitly focusing on addressing the\nchallenge of Italian Language coverage. Adopting an open science approach, we\nexplore various tuning approaches to ensure a high-quality text generated in\nItalian suitable for common tasks in this underrepresented language in the\noriginal models' datasets. We aim to release effective text generation models\nwith strong linguistic properties for many tasks that seem challenging using\nmultilingual or general-purpose LLMs. By leveraging an open science philosophy,\nthis study contributes to Language Adaptation strategies for the Italian\nlanguage by introducing the novel LLaMAntino family of Italian LLMs.", "published": "2023-12-15 18:06:22", "link": "http://arxiv.org/abs/2312.09993v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent", "abstract": "Answering complex natural language questions often necessitates multi-step\nreasoning and integrating external information. Several systems have combined\nknowledge retrieval with a large language model (LLM) to answer such questions.\nThese systems, however, suffer from various failure cases, and we cannot\ndirectly train them end-to-end to fix such failures, as interaction with\nexternal knowledge is non-differentiable. To address these deficiencies, we\ndefine a ReAct-style LLM agent with the ability to reason and act upon external\nknowledge. We further refine the agent through a ReST-like method that\niteratively trains on previous trajectories, employing growing-batch\nreinforcement learning with AI feedback for continuous self-improvement and\nself-distillation. Starting from a prompted large model and after just two\niterations of the algorithm, we can produce a fine-tuned small model that\nachieves comparable performance on challenging compositional question-answering\nbenchmarks with two orders of magnitude fewer parameters.", "published": "2023-12-15 18:20:15", "link": "http://arxiv.org/abs/2312.10003v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Text Simplification Systems Preserve Meaning? A Human Evaluation via\n  Reading Comprehension", "abstract": "Automatic text simplification (TS) aims to automate the process of rewriting\ntext to make it easier for people to read. A pre-requisite for TS to be useful\nis that it should convey information that is consistent with the meaning of the\noriginal text. However, current TS evaluation protocols assess system outputs\nfor simplicity and meaning preservation without regard for the document context\nin which output sentences occur and for how people understand them. In this\nwork, we introduce a human evaluation framework to assess whether simplified\ntexts preserve meaning using reading comprehension questions. With this\nframework, we conduct a thorough human evaluation of texts by humans and by\nnine automatic systems. Supervised systems that leverage pre-training knowledge\nachieve the highest scores on the reading comprehension (RC) tasks amongst the\nautomatic controllable TS systems. However, even the best-performing supervised\nsystem struggles with at least 14% of the questions, marking them as\n\"unanswerable'' based on simplified content. We further investigate how\nexisting TS evaluation metrics and automatic question-answering systems\napproximate the human judgments we obtained.", "published": "2023-12-15 14:26:06", "link": "http://arxiv.org/abs/2312.10126v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do LVLMs Understand Charts? Analyzing and Correcting Factual Errors in\n  Chart Captioning", "abstract": "Recent advancements in large vision-language models (LVLMs) have led to\nsignificant progress in generating natural language descriptions for visual\ncontent and thus enhancing various applications. One issue with these powerful\nmodels is that they sometimes produce texts that are factually inconsistent\nwith the visual input. While there has been some effort to mitigate such\ninconsistencies in natural image captioning, the factuality of generated\ncaptions for structured document images, such as charts, has not received as\nmuch scrutiny, posing a potential threat to information reliability in critical\napplications. This work delves into the factuality aspect by introducing a\ncomprehensive typology of factual errors in generated chart captions. A\nlarge-scale human annotation effort provides insight into the error patterns\nand frequencies in captions crafted by various chart captioning models,\nultimately forming the foundation of a novel dataset, CHOCOLATE. Our analysis\nreveals that even state-of-the-art models, including GPT-4V, frequently produce\ncaptions laced with factual inaccuracies. In response to this challenge, we\nestablish the new task of Chart Caption Factual Error Correction and introduce\nCHARTVE, a model for visual entailment that outperforms proprietary and\nopen-source LVLMs in evaluating factual consistency. Furthermore, we propose\nC2TFEC, an interpretable two-stage framework that excels at correcting factual\nerrors. This work inaugurates a new domain in factual error correction for\nchart captions, presenting a novel evaluation mechanism, and demonstrating an\neffective approach to ensuring the factuality of generated chart captions. The\ncode and data as well as the continuously updated benchmark can be found at:\nhttps://khuangaf.github.io/CHOCOLATE/.", "published": "2023-12-15 19:16:21", "link": "http://arxiv.org/abs/2312.10160v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Review of Unsupervised POS Tagging and Its Implications on Language\n  Acquisition", "abstract": "An ability that underlies human syntactic knowledge is determining which\nwords can appear in the similar structures (i.e. grouping words by their\nsyntactic categories). These groupings enable humans to combine structures in\norder to communicate complex meanings. A foundational question is how do\nchildren acquire this ability underlying syntactic knowledge. In exploring this\nprocess, we will review various engineering approaches whose goal is similar to\nthat of a child's -- without prior syntactic knowledge, correctly identify the\nparts of speech (POS) of the words in a sample of text. In reviewing these\nunsupervised tagging efforts, we will discuss common themes that support the\nadvances in the models and their relevance for language acquisition. For\nexample, we discuss how each model judges success (evaluation metrics), the\n\"additional information\" that constrains the POS learning (such as orthographic\ninformation), and the context used to determine POS (only previous word, words\nbefore and after the target, etc). The identified themes pave the way for\nfuture investigations into the cognitive processes that underpin the\nacquisition of syntactic categories and provide a useful layout of current\nstate of the art unsupervised POS tagging models.", "published": "2023-12-15 19:31:00", "link": "http://arxiv.org/abs/2312.10169v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pipeline and Dataset Generation for Automated Fact-checking in Almost\n  Any Language", "abstract": "This article presents a pipeline for automated fact-checking leveraging\npublicly available Language Models and data. The objective is to assess the\naccuracy of textual claims using evidence from a ground-truth evidence corpus.\nThe pipeline consists of two main modules -- the evidence retrieval and the\nclaim veracity evaluation. Our primary focus is on the ease of deployment in\nvarious languages that remain unexplored in the field of automated\nfact-checking. Unlike most similar pipelines, which work with evidence\nsentences, our pipeline processes data on a paragraph level, simplifying the\noverall architecture and data requirements. Given the high cost of annotating\nlanguage-specific fact-checking training data, our solution builds on the\nQuestion Answering for Claim Generation (QACG) method, which we adapt and use\nto generate the data for all models of the pipeline. Our strategy enables the\nintroduction of new languages through machine translation of only two fixed\ndatasets of moderate size. Subsequently, any number of training samples can be\ngenerated based on an evidence corpus in the target language. We provide open\naccess to all data and fine-tuned models for Czech, English, Polish, and Slovak\npipelines, as well as to our codebase that may be used to reproduce the\nresults.We comprehensively evaluate the pipelines for all four languages,\nincluding human annotations and per-sample difficulty assessment using\nPointwise V-information. The presented experiments are based on full Wikipedia\nsnapshots to promote reproducibility. To facilitate implementation and user\ninteraction, we develop the FactSearch application featuring the proposed\npipeline and the preliminary feedback on its performance.", "published": "2023-12-15 19:43:41", "link": "http://arxiv.org/abs/2312.10171v1", "categories": ["cs.CL", "I.2.7; I.5.4"], "primary_category": "cs.CL"}
{"title": "Low-resource classification of mobility functioning information in\n  clinical sentences using large language models", "abstract": "Objective: Function is increasingly recognized as an important indicator of\nwhole-person health. This study evaluates the ability of publicly available\nlarge language models (LLMs) to accurately identify the presence of functioning\ninformation from clinical notes. We explore various strategies to improve the\nperformance on this task. Materials and Methods: We collect a balanced binary\nclassification dataset of 1000 sentences from the Mobility NER dataset, which\nwas curated from n2c2 clinical notes. For evaluation, we construct zero-shot\nand few-shot prompts to query the LLMs whether a given sentence contains\nmobility functioning information. Two sampling techniques, random sampling and\nk-nearest neighbor (kNN)-based sampling, are used to select the few-shot\nexamples. Furthermore, we apply a parameter-efficient prompt-based fine-tuning\nmethod to the LLMs and evaluate their performance under various training\nsettings. Results: Flan-T5-xxl outperforms all other models in both zero-shot\nand few-shot settings, achieving a F1 score of 0.865 with a single\ndemonstrative example selected by kNN sampling. In prompt-based fine-tuning\nexperiments, this foundation model also demonstrates superior performance\nacross all low-resource settings, particularly achieving an impressive F1 score\nof 0.922 using the full training dataset. The smaller model, Flan-T5-xl,\nrequires fine-tuning with only 2.3M additional parameters to achieve comparable\nperformance to the fully fine-tuned Gatortron-base model, both surpassing 0.9\nF1 score. Conclusion: Open-source instruction-tuned LLMs demonstrate impressive\nin-context learning capability in the mobility functioning classification task.\nThe performance of these models can be further improved by continuing\nfine-tuning on a task-specific dataset.", "published": "2023-12-15 20:59:17", "link": "http://arxiv.org/abs/2312.10202v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VK-G2T: Vision and Context Knowledge enhanced Gloss2Text", "abstract": "Existing sign language translation methods follow a two-stage pipeline: first\nconverting the sign language video to a gloss sequence (i.e. Sign2Gloss) and\nthen translating the generated gloss sequence into a spoken language sentence\n(i.e. Gloss2Text). While previous studies have focused on boosting the\nperformance of the Sign2Gloss stage, we emphasize the optimization of the\nGloss2Text stage. However, this task is non-trivial due to two distinct\nfeatures of Gloss2Text: (1) isolated gloss input and (2) low-capacity gloss\nvocabulary. To address these issues, we propose a vision and context knowledge\nenhanced Gloss2Text model, named VK-G2T, which leverages the visual content of\nthe sign language video to learn the properties of the target sentence and\nexploit the context knowledge to facilitate the adaptive translation of gloss\nwords. Extensive experiments conducted on a Chinese benchmark validate the\nsuperiority of our model.", "published": "2023-12-15 21:09:34", "link": "http://arxiv.org/abs/2312.10210v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Catwalk: A Unified Language Model Evaluation Framework for Many Datasets", "abstract": "The success of large language models has shifted the evaluation paradigms in\nnatural language processing (NLP). The community's interest has drifted towards\ncomparing NLP models across many tasks, domains, and datasets, often at an\nextreme scale. This imposes new engineering challenges: efforts in constructing\ndatasets and models have been fragmented, and their formats and interfaces are\nincompatible. As a result, it often takes extensive (re)implementation efforts\nto make fair and controlled comparisons at scale.\n  Catwalk aims to address these issues. Catwalk provides a unified interface to\na broad range of existing NLP datasets and models, ranging from both canonical\nsupervised training and fine-tuning, to more modern paradigms like in-context\nlearning. Its carefully-designed abstractions allow for easy extensions to many\nothers. Catwalk substantially lowers the barriers to conducting controlled\nexperiments at scale. For example, we finetuned and evaluated over 64 models on\nover 86 datasets with a single command, without writing any code. Maintained by\nthe AllenNLP team at the Allen Institute for Artificial Intelligence (AI2),\nCatwalk is an ongoing open-source effort: https://github.com/allenai/catwalk.", "published": "2023-12-15 23:11:45", "link": "http://arxiv.org/abs/2312.10253v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Taxonomy-based CheckList for Large Language Model Evaluation", "abstract": "As large language models (LLMs) have been used in many downstream tasks, the\ninternal stereotypical representation may affect the fairness of the outputs.\nIn this work, we introduce human knowledge into natural language interventions\nand study pre-trained language models' (LMs) behaviors within the context of\ngender bias. Inspired by CheckList behavioral testing, we present a\nchecklist-style task that aims to probe and quantify LMs' unethical behaviors\nthrough question-answering (QA). We design three comparison studies to evaluate\nLMs from four aspects: consistency, biased tendency, model preference, and\ngender preference switch. We probe one transformer-based QA model trained on\nSQuAD-v2 dataset and one autoregressive large language model. Our results\nindicate that transformer-based QA model's biased tendency positively\ncorrelates with its consistency, whereas LLM shows the opposite relation. Our\nproposed task provides the first dataset that involves human knowledge for LLM\nbias evaluation.", "published": "2023-12-15 12:58:07", "link": "http://arxiv.org/abs/2402.10899v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based\n  Language Models", "abstract": "To reduce the computation cost and the energy consumption in large language\nmodels (LLM), skimming-based acceleration dynamically drops unimportant tokens\nof the input sequence progressively along layers of the LLM while preserving\nthe tokens of semantic importance. However, our work for the first time reveals\nthe acceleration may be vulnerable to Denial-of-Service (DoS) attacks. In this\npaper, we propose No-Skim, a general framework to help the owners of\nskimming-based LLM to understand and measure the robustness of their\nacceleration scheme. Specifically, our framework searches minimal and\nunnoticeable perturbations at character-level and token-level to generate\nadversarial inputs that sufficiently increase the remaining token ratio, thus\nincreasing the computation cost and energy consumption. We systematically\nevaluate the vulnerability of the skimming acceleration in various LLM\narchitectures including BERT and RoBERTa on the GLUE benchmark. In the worst\ncase, the perturbation found by No-Skim substantially increases the running\ncost of LLM by over 145% on average. Moreover, No-Skim extends the evaluation\nframework to various scenarios, making the evaluation conductible with\ndifferent level of knowledge.", "published": "2023-12-15 02:42:05", "link": "http://arxiv.org/abs/2312.09494v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "IndicIRSuite: Multilingual Dataset and Neural Information Models for\n  Indian Languages", "abstract": "In this paper, we introduce Neural Information Retrieval resources for 11\nwidely spoken Indian Languages (Assamese, Bengali, Gujarati, Hindi, Kannada,\nMalayalam, Marathi, Oriya, Punjabi, Tamil, and Telugu) from two major Indian\nlanguage families (Indo-Aryan and Dravidian). These resources include (a)\nINDIC-MARCO, a multilingual version of the MSMARCO dataset in 11 Indian\nLanguages created using Machine Translation, and (b) Indic-ColBERT, a\ncollection of 11 distinct Monolingual Neural Information Retrieval models, each\ntrained on one of the 11 languages in the INDIC-MARCO dataset. To the best of\nour knowledge, IndicIRSuite is the first attempt at building large-scale Neural\nInformation Retrieval resources for a large number of Indian languages, and we\nhope that it will help accelerate research in Neural IR for Indian Languages.\nExperiments demonstrate that Indic-ColBERT achieves 47.47% improvement in the\nMRR@10 score averaged over the INDIC-MARCO baselines for all 11 Indian\nlanguages except Oriya, 12.26% improvement in the NDCG@10 score averaged over\nthe MIRACL Bengali and Hindi Language baselines, and 20% improvement in the\nMRR@100 Score over the Mr.Tydi Bengali Language baseline. IndicIRSuite is\navailable at https://github.com/saifulhaq95/IndicIRSuite", "published": "2023-12-15 03:19:53", "link": "http://arxiv.org/abs/2312.09508v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Weakly-Supervised 3D Visual Grounding based on Visual Linguistic\n  Alignment", "abstract": "Learning to ground natural language queries to target objects or regions in\n3D point clouds is quite essential for 3D scene understanding. Nevertheless,\nexisting 3D visual grounding approaches require a substantial number of\nbounding box annotations for text queries, which is time-consuming and\nlabor-intensive to obtain. In this paper, we propose 3D-VLA, a weakly\nsupervised approach for 3D visual grounding based on Visual Linguistic\nAlignment. Our 3D-VLA exploits the superior ability of current large-scale\nvision-language models (VLMs) on aligning the semantics between texts and 2D\nimages, as well as the naturally existing correspondences between 2D images and\n3D point clouds, and thus implicitly constructs correspondences between texts\nand 3D point clouds with no need for fine-grained box annotations in the\ntraining procedure. During the inference stage, the learned text-3D\ncorrespondence will help us ground the text queries to the 3D target objects\neven without 2D images. To the best of our knowledge, this is the first work to\ninvestigate 3D visual grounding in a weakly supervised manner by involving\nlarge scale vision-language models, and extensive experiments on ReferIt3D and\nScanRefer datasets demonstrate that our 3D-VLA achieves comparable and even\nsuperior results over the fully supervised methods.", "published": "2023-12-15 09:08:14", "link": "http://arxiv.org/abs/2312.09625v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Probing Pretrained Language Models with Hierarchy Properties", "abstract": "Since Pretrained Language Models (PLMs) are the cornerstone of the most\nrecent Information Retrieval (IR) models, the way they encode semantic\nknowledge is particularly important. However, little attention has been given\nto studying the PLMs' capability to capture hierarchical semantic knowledge.\nTraditionally, evaluating such knowledge encoded in PLMs relies on their\nperformance on a task-dependent evaluation approach based on proxy tasks, such\nas hypernymy detection. Unfortunately, this approach potentially ignores other\nimplicit and complex taxonomic relations. In this work, we propose a\ntask-agnostic evaluation method able to evaluate to what extent PLMs can\ncapture complex taxonomy relations, such as ancestors and siblings. The\nevaluation is based on intrinsic properties that capture the hierarchical\nnature of taxonomies. Our experimental evaluation shows that the\nlexico-semantic knowledge implicitly encoded in PLMs does not always capture\nhierarchical relations. We further demonstrate that the proposed properties can\nbe injected into PLMs to improve their understanding of hierarchy. Through\nevaluations on taxonomy reconstruction, hypernym discovery and reading\ncomprehension tasks, we show that the knowledge about hierarchy is moderately\nbut not systematically transferable across tasks.", "published": "2023-12-15 10:31:36", "link": "http://arxiv.org/abs/2312.09670v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "GSQA: An End-to-End Model for Generative Spoken Question Answering", "abstract": "In recent advancements in spoken question answering (QA), end-to-end models\nhave made significant strides. However, previous research has primarily focused\non extractive span selection. While this extractive-based approach is effective\nwhen answers are present directly within the input, it falls short in\naddressing abstractive questions, where answers are not directly extracted but\ninferred from the given information. To bridge this gap, we introduce the first\nend-to-end Generative Spoken Question Answering (GSQA) model that empowers the\nsystem to engage in abstractive reasoning. The challenge in training our GSQA\nmodel lies in the absence of a spoken abstractive QA dataset. We propose using\ntext models for initialization and leveraging the extractive QA dataset to\ntransfer knowledge from the text generative model to the spoken generative\nmodel. Experimental results indicate that our model surpasses the previous\nextractive model by 3% on extractive QA datasets. Furthermore, the GSQA model\nhas only been fine-tuned on the spoken extractive QA dataset. Despite not\nhaving seen any spoken abstractive QA data, it can still closely match the\nperformance of the cascade model. In conclusion, our GSQA model shows the\npotential to generalize to a broad spectrum of questions, thus further\nexpanding the spoken question answering capabilities of abstractive QA. Our\ncode is available at https://voidful.github.io/GSQA", "published": "2023-12-15 13:33:18", "link": "http://arxiv.org/abs/2312.09781v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SMILE: Multimodal Dataset for Understanding Laughter in Video with\n  Language Models", "abstract": "Despite the recent advances of the artificial intelligence, building social\nintelligence remains a challenge. Among social signals, laughter is one of the\ndistinctive expressions that occurs during social interactions between humans.\nIn this work, we tackle a new challenge for machines to understand the\nrationale behind laughter in video, Video Laugh Reasoning. We introduce this\nnew task to explain why people laugh in a particular video and a dataset for\nthis task. Our proposed dataset, SMILE, comprises video clips and language\ndescriptions of why people laugh. We propose a baseline by leveraging the\nreasoning capacity of large language models (LLMs) with textual video\nrepresentation. Experiments show that our baseline can generate plausible\nexplanations for laughter. We further investigate the scalability of our\nbaseline by probing other video understanding tasks and in-the-wild videos. We\nrelease our dataset, code, and model checkpoints on\nhttps://github.com/postech-ami/SMILE-Dataset.", "published": "2023-12-15 14:17:45", "link": "http://arxiv.org/abs/2312.09818v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Automatic Text Simplification of German Narrative Documents", "abstract": "In this paper, we apply transformer-based Natural Language Generation (NLG)\ntechniques to the problem of text simplification. Currently, there are only a\nfew German datasets available for text simplification, even fewer with larger\nand aligned documents, and not a single one with narrative texts. In this\npaper, we explore to which degree modern NLG techniques can be applied to\nGerman narrative text simplifications. We use Longformer attention and a\npre-trained mBART model. Our findings indicate that the existing approaches for\nGerman are not able to solve the task properly. We conclude on a few directions\nfor future research to address this problem.", "published": "2023-12-15 16:10:44", "link": "http://arxiv.org/abs/2312.09907v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Red AI? Inconsistent Responses from GPT3.5 Models on Political Issues in\n  the US and China", "abstract": "The rising popularity of ChatGPT and other AI-powered large language models\n(LLMs) has led to increasing studies highlighting their susceptibility to\nmistakes and biases. However, most of these studies focus on models trained on\nEnglish texts. Taking an innovative approach, this study investigates political\nbiases in GPT's multilingual models. We posed the same question about\nhigh-profile political issues in the United States and China to GPT in both\nEnglish and simplified Chinese, and our analysis of the bilingual responses\nrevealed that GPT's bilingual models' political \"knowledge\" (content) and the\npolitical \"attitude\" (sentiment) are significantly more inconsistent on\npolitical issues in China. The simplified Chinese GPT models not only tended to\nprovide pro-China information but also presented the least negative sentiment\ntowards China's problems, whereas the English GPT was significantly more\nnegative towards China. This disparity may stem from Chinese state censorship\nand US-China geopolitical tensions, which influence the training corpora of GPT\nbilingual models. Moreover, both Chinese and English models tended to be less\ncritical towards the issues of \"their own\" represented by the language used,\nthan the issues of \"the other.\" This suggests that GPT multilingual models\ncould potentially develop a \"political identity\" and an associated sentiment\nbias based on their training language. We discussed the implications of our\nfindings for information transmission and communication in an increasingly\ndivided world.", "published": "2023-12-15 16:25:56", "link": "http://arxiv.org/abs/2312.09917v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RDR: the Recap, Deliberate, and Respond Method for Enhanced Language\n  Understanding", "abstract": "Natural language understanding (NLU) using neural network pipelines often\nrequires additional context that is not solely present in the input data.\nThrough Prior research, it has been evident that NLU benchmarks are susceptible\nto manipulation by neural models, wherein these models exploit statistical\nartifacts within the encoded external knowledge to artificially inflate\nperformance metrics for downstream tasks. Our proposed approach, known as the\nRecap, Deliberate, and Respond (RDR) paradigm, addresses this issue by\nincorporating three distinct objectives within the neural network pipeline.\nFirstly, the Recap objective involves paraphrasing the input text using a\nparaphrasing model in order to summarize and encapsulate its essence. Secondly,\nthe Deliberation objective entails encoding external graph information related\nto entities mentioned in the input text, utilizing a graph embedding model.\nFinally, the Respond objective employs a classification head model that\nutilizes representations from the Recap and Deliberation modules to generate\nthe final prediction. By cascading these three models and minimizing a combined\nloss, we mitigate the potential for gaming the benchmark and establish a robust\nmethod for capturing the underlying semantic patterns, thus enabling accurate\npredictions. To evaluate the effectiveness of the RDR method, we conduct tests\non multiple GLUE benchmark tasks. Our results demonstrate improved performance\ncompared to competitive baselines, with an enhancement of up to 2\\% on standard\nmetrics. Furthermore, we analyze the observed evidence for semantic\nunderstanding exhibited by RDR models, emphasizing their ability to avoid\ngaming the benchmark and instead accurately capture the true underlying\nsemantic patterns.", "published": "2023-12-15 16:41:48", "link": "http://arxiv.org/abs/2312.09932v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data and Approaches for German Text simplification -- towards an\n  Accessibility-enhanced Communication", "abstract": "This paper examines the current state-of-the-art of German text\nsimplification, focusing on parallel and monolingual German corpora. It reviews\nneural language models for simplifying German texts and assesses their\nsuitability for legal texts and accessibility requirements. Our findings\nhighlight the need for additional training data and more appropriate approaches\nthat consider the specific linguistic characteristics of German, as well as the\nimportance of the needs and preferences of target groups with cognitive or\nlanguage impairments. The authors launched the interdisciplinary OPEN-LS\nproject in April 2023 to address these research gaps. The project aims to\ndevelop a framework for text formats tailored to individuals with low literacy\nlevels, integrate legal texts, and enhance comprehensibility for those with\nlinguistic or cognitive impairments. It will also explore cost-effective ways\nto enhance the data with audience-specific illustrations using image-generating\nAI.\n  For more and up-to-date information, please visit our project homepage\nhttps://open-ls.entavis.com", "published": "2023-12-15 17:23:33", "link": "http://arxiv.org/abs/2312.09966v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Faithful Persona-based Conversational Dataset Generation with Large\n  Language Models", "abstract": "High-quality conversational datasets are essential for developing AI models\nthat can communicate with users. One way to foster deeper interactions between\na chatbot and its user is through personas, aspects of the user's character\nthat provide insights into their personality, motivations, and behaviors.\nTraining Natural Language Processing (NLP) models on a diverse and\ncomprehensive persona-based dataset can lead to conversational models that\ncreate a deeper connection with the user, and maintain their engagement. In\nthis paper, we leverage the power of Large Language Models (LLMs) to create a\nlarge, high-quality conversational dataset from a seed dataset. We propose a\nGenerator-Critic architecture framework to expand the initial dataset, while\nimproving the quality of its conversations. The Generator is an LLM prompted to\noutput conversations. The Critic consists of a mixture of expert LLMs that\ncontrol the quality of the generated conversations. These experts select the\nbest generated conversations, which we then use to improve the Generator. We\nrelease Synthetic-Persona-Chat, consisting of 20k conversations seeded from\nPersona-Chat. We evaluate the quality of Synthetic-Persona-Chat and our\ngeneration framework on different dimensions through extensive experiments, and\nobserve that the losing rate of Synthetic-Persona-Chat against Persona-Chat\nduring Turing test decreases from 17.2% to 8.8% over three iterations.", "published": "2023-12-15 18:23:50", "link": "http://arxiv.org/abs/2312.10007v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Review of Repository Level Prompting for LLMs", "abstract": "As coding challenges become more complex, recent advancements in Large\nLanguage Models (LLMs) have led to notable successes, such as achieving a\n94.6\\% solve rate on the HumanEval benchmark. Concurrently, there is an\nincreasing commercial push for repository-level inline code completion tools,\nsuch as GitHub Copilot and Tab Nine, aimed at enhancing developer productivity.\nThis paper delves into the transition from individual coding problems to\nrepository-scale solutions, presenting a thorough review of the current\nliterature on effective LLM prompting for code generation at the repository\nlevel. We examine approaches that will work with black-box LLMs such that they\nwill be useful and applicable to commercial use cases, and their applicability\nin interpreting code at a repository scale. We juxtapose the Repository-Level\nPrompt Generation technique with RepoCoder, an iterative retrieval and\ngeneration method, to highlight the trade-offs inherent in each approach and to\nestablish best practices for their application in cutting-edge coding\nbenchmarks. The interplay between iterative refinement of prompts and the\ndevelopment of advanced retrieval systems forms the core of our discussion,\noffering a pathway to significantly improve LLM performance in code generation\ntasks. Insights from this study not only guide the application of these methods\nbut also chart a course for future research to integrate such techniques into\nbroader software engineering contexts.", "published": "2023-12-15 00:34:52", "link": "http://arxiv.org/abs/2312.10101v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Student as an Inherent Denoiser of Noisy Teacher", "abstract": "Knowledge distillation (KD) has been widely employed to transfer knowledge\nfrom a large language model (LLM) to a specialized model in low-data regimes\nthrough pseudo label learning. However, pseudo labels generated by teacher\nmodels are usually noisy and may influence KD performance. This study delves\ninto KD with noisy teachers and uncovers that the student model can already\ngenerate more accurate predictions than the teacher labels used to train it\nduring KD, indicating its inherent ability to denoise noisy teacher labels.\nMotivated by this finding, we propose Peer-Advised KD to improve vanilla KD\nfrom noisy teachers. Experiments show that Peer-Advised KD can outperform LLM\nby approximately 5% with 50 human-labeled data, and even competitive to\nstandard supervised finetuning with 750 human-labeled data.", "published": "2023-12-15 20:21:45", "link": "http://arxiv.org/abs/2312.10185v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "OTOv3: Automatic Architecture-Agnostic Neural Network Training and\n  Compression from Structured Pruning to Erasing Operators", "abstract": "Compressing a predefined deep neural network (DNN) into a compact sub-network\nwith competitive performance is crucial in the efficient machine learning\nrealm. This topic spans various techniques, from structured pruning to neural\narchitecture search, encompassing both pruning and erasing operators\nperspectives. Despite advancements, existing methods suffers from complex,\nmulti-stage processes that demand substantial engineering and domain knowledge,\nlimiting their broader applications. We introduce the third-generation\nOnly-Train-Once (OTOv3), which first automatically trains and compresses a\ngeneral DNN through pruning and erasing operations, creating a compact and\ncompetitive sub-network without the need of fine-tuning. OTOv3 simplifies and\nautomates the training and compression process, minimizes the engineering\nefforts required from users. It offers key technological advancements: (i)\nautomatic search space construction for general DNNs based on dependency graph\nanalysis; (ii) Dual Half-Space Projected Gradient (DHSPG) and its enhanced\nversion with hierarchical search (H2SPG) to reliably solve (hierarchical)\nstructured sparsity problems and ensure sub-network validity; and (iii)\nautomated sub-network construction using solutions from DHSPG/H2SPG and\ndependency graphs. Our empirical results demonstrate the efficacy of OTOv3\nacross various benchmarks in structured pruning and neural architecture search.\nOTOv3 produces sub-networks that match or exceed the state-of-the-arts. The\nsource code will be available at https://github.com/tianyic/only_train_once.", "published": "2023-12-15 00:22:55", "link": "http://arxiv.org/abs/2312.09411v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "GPT-4 Surpassing Human Performance in Linguistic Pragmatics", "abstract": "As Large Language Models (LLMs) become increasingly integrated into everyday\nlife, their capabilities to understand and emulate human cognition are under\nsteady examination. This study investigates the ability of LLMs to comprehend\nand interpret linguistic pragmatics, an aspect of communication that considers\ncontext and implied meanings. Using Grice's communication principles, LLMs and\nhuman subjects (N=76) were evaluated based on their responses to various\ndialogue-based tasks. The findings revealed the superior performance and speed\nof LLMs, particularly GPT4, over human subjects in interpreting pragmatics.\nGPT4 also demonstrated accuracy in the pre-testing of human-written samples,\nindicating its potential in text analysis. In a comparative analysis of LLMs\nusing human individual and average scores, the models exhibited significant\nchronological improvement. The models were ranked from lowest to highest score,\nwith GPT2 positioned at 78th place, GPT3 ranking at 23rd, Bard at 10th, GPT3.5\nplacing 5th, Best Human scoring 2nd, and GPT4 achieving the top spot. The\nfindings highlight the remarkable progress made in the development and\nperformance of these LLMs. Future studies should consider diverse subjects,\nmultiple languages, and other cognitive aspects to fully comprehend the\ncapabilities of LLMs. This research holds significant implications for the\ndevelopment and application of AI-based models in communication-centered\nsectors.", "published": "2023-12-15 05:40:15", "link": "http://arxiv.org/abs/2312.09545v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Extending Context Window of Large Language Models via Semantic\n  Compression", "abstract": "Transformer-based Large Language Models (LLMs) often impose limitations on\nthe length of the text input to ensure the generation of fluent and relevant\nresponses. This constraint restricts their applicability in scenarios involving\nlong texts. We propose a novel semantic compression method that enables\ngeneralization to texts that are 6-8 times longer, without incurring\nsignificant computational costs or requiring fine-tuning. Our proposed\nframework draws inspiration from source coding in information theory and\nemploys a pre-trained model to reduce the semantic redundancy of long inputs\nbefore passing them to the LLMs for downstream tasks. Experimental results\ndemonstrate that our method effectively extends the context window of LLMs\nacross a range of tasks including question answering, summarization, few-shot\nlearning, and information retrieval. Furthermore, the proposed semantic\ncompression method exhibits consistent fluency in text generation while\nreducing the associated computational overhead.", "published": "2023-12-15 07:04:33", "link": "http://arxiv.org/abs/2312.09571v1", "categories": ["cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "IR-UWB Radar-Based Contactless Silent Speech Recognition of Vowels,\n  Consonants, Words, and Phrases", "abstract": "Several sensing techniques have been proposed for silent speech recognition\n(SSR); however, many of these methods require invasive processes or sensor\nattachment to the skin using adhesive tape or glue, rendering them unsuitable\nfor frequent use in daily life. By contrast, impulse radio ultra-wideband\n(IR-UWB) radar can operate without physical contact with users' articulators\nand related body parts, offering several advantages for SSR. These advantages\ninclude high range resolution, high penetrability, low power consumption,\nrobustness to external light or sound interference, and the ability to be\nembedded in space-constrained handheld devices. This study demonstrated IR-UWB\nradar-based contactless SSR using four types of speech stimuli (vowels,\nconsonants, words, and phrases). To achieve this, a novel speech feature\nextraction algorithm specifically designed for IR-UWB radar-based SSR is\nproposed. Each speech stimulus is recognized by applying a classification\nalgorithm to the extracted speech features. Two different algorithms,\nmultidimensional dynamic time warping (MD-DTW) and deep neural network-hidden\nMarkov model (DNN-HMM), were compared for the classification task.\nAdditionally, a favorable radar antenna position, either in front of the user's\nlips or below the user's chin, was determined to achieve higher recognition\naccuracy. Experimental results demonstrated the efficacy of the proposed speech\nfeature extraction algorithm combined with DNN-HMM for classifying vowels,\nconsonants, words, and phrases. Notably, this study represents the first\ndemonstration of phoneme-level SSR using contactless radar.", "published": "2023-12-15 07:04:40", "link": "http://arxiv.org/abs/2312.09572v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Phoneme-aware Encoding for Prefix-tree-based Contextual ASR", "abstract": "In speech recognition applications, it is important to recognize\ncontext-specific rare words, such as proper nouns. Tree-constrained Pointer\nGenerator (TCPGen) has shown promise for this purpose, which efficiently biases\nsuch words with a prefix tree. While the original TCPGen relies on\ngrapheme-based encoding, we propose extending it with phoneme-aware encoding to\nbetter recognize words of unusual pronunciations. As TCPGen handles biasing\nwords as subword units, we propose obtaining subword-level phoneme-aware\nencoding by using alignment between phonemes and subwords. Furthermore, we\npropose injecting phoneme-level predictions from CTC into queries of TCPGen so\nthat the model better interprets the phoneme-aware encodings. We conducted ASR\nexperiments with TCPGen for RNN transducer. We observed that proposed\nphoneme-aware encoding outperformed ordinary grapheme-based encoding on both\nthe English LibriSpeech and Japanese CSJ datasets, demonstrating the robustness\nof our approach across linguistically diverse languages.", "published": "2023-12-15 07:37:09", "link": "http://arxiv.org/abs/2312.09582v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Leveraging Language ID to Calculate Intermediate CTC Loss for Enhanced\n  Code-Switching Speech Recognition", "abstract": "In recent years, end-to-end speech recognition has emerged as a technology\nthat integrates the acoustic, pronunciation dictionary, and language model\ncomponents of the traditional Automatic Speech Recognition model. It is\npossible to achieve human-like recognition without the need to build a\npronunciation dictionary in advance. However, due to the relative scarcity of\ntraining data on code-switching, the performance of ASR models tends to degrade\ndrastically when encountering this phenomenon. Most past studies have\nsimplified the learning complexity of the model by splitting the code-switching\ntask into multiple tasks dealing with a single language and then learning the\ndomain-specific knowledge of each language separately. Therefore, in this\npaper, we attempt to introduce language identification information into the\nmiddle layer of the ASR model's encoder. We aim to generate acoustic features\nthat imply language distinctions in a more implicit way, reducing the model's\nconfusion when dealing with language switching.", "published": "2023-12-15 07:46:35", "link": "http://arxiv.org/abs/2312.09583v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Binary Code Summarization: Benchmarking ChatGPT/GPT-4 and Other Large\n  Language Models", "abstract": "Binary code summarization, while invaluable for understanding code semantics,\nis challenging due to its labor-intensive nature. This study delves into the\npotential of large language models (LLMs) for binary code comprehension. To\nthis end, we present BinSum, a comprehensive benchmark and dataset of over 557K\nbinary functions and introduce a novel method for prompt synthesis and\noptimization. To more accurately gauge LLM performance, we also propose a new\nsemantic similarity metric that surpasses traditional exact-match approaches.\nOur extensive evaluation of prominent LLMs, including ChatGPT, GPT-4, Llama 2,\nand Code Llama, reveals 10 pivotal insights. This evaluation generates 4\nbillion inference tokens, incurred a total expense of 11,418 US dollars and 873\nNVIDIA A100 GPU hours. Our findings highlight both the transformative potential\nof LLMs in this field and the challenges yet to be overcome.", "published": "2023-12-15 08:32:28", "link": "http://arxiv.org/abs/2312.09601v1", "categories": ["cs.CR", "cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.CR"}
{"title": "HEAR: Hearing Enhanced Audio Response for Video-grounded Dialogue", "abstract": "Video-grounded Dialogue (VGD) aims to answer questions regarding a given\nmulti-modal input comprising video, audio, and dialogue history. Although there\nhave been numerous efforts in developing VGD systems to improve the quality of\ntheir responses, existing systems are competent only to incorporate the\ninformation in the video and text and tend to struggle in extracting the\nnecessary information from the audio when generating appropriate responses to\nthe question. The VGD system seems to be deaf, and thus, we coin this symptom\nof current systems' ignoring audio data as a deaf response. To overcome the\ndeaf response problem, Hearing Enhanced Audio Response (HEAR) framework is\nproposed to perform sensible listening by selectively attending to audio\nwhenever the question requires it. The HEAR framework enhances the accuracy and\naudibility of VGD systems in a model-agnostic manner. HEAR is validated on VGD\ndatasets (i.e., AVSD@DSTC7 and AVSD@DSTC8) and shows effectiveness with various\nVGD systems.", "published": "2023-12-15 12:20:24", "link": "http://arxiv.org/abs/2312.09736v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving Biomedical Entity Linking with Retrieval-enhanced Learning", "abstract": "Biomedical entity linking (BioEL) has achieved remarkable progress with the\nhelp of pre-trained language models. However, existing BioEL methods usually\nstruggle to handle rare and difficult entities due to long-tailed distribution.\nTo address this limitation, we introduce a new scheme $k$NN-BioEL, which\nprovides a BioEL model with the ability to reference similar instances from the\nentire training corpus as clues for prediction, thus improving the\ngeneralization capabilities. Moreover, we design a contrastive learning\nobjective with dynamic hard negative sampling (DHNS) that improves the quality\nof the retrieved neighbors during inference. Extensive experimental results\nshow that $k$NN-BioEL outperforms state-of-the-art baselines on several\ndatasets.", "published": "2023-12-15 14:04:23", "link": "http://arxiv.org/abs/2312.09806v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generative Context-aware Fine-tuning of Self-supervised Speech Models", "abstract": "When performing tasks like automatic speech recognition or spoken language\nunderstanding for a given utterance, access to preceding text or audio provides\ncontextual information can improve performance. Considering the recent advances\nin generative large language models (LLM), we hypothesize that an LLM could\ngenerate useful context information using the preceding text. With appropriate\nprompts, LLM could generate a prediction of the next sentence or abstractive\ntext like titles or topics. In this paper, we study the use of LLM-generated\ncontext information and propose an approach to distill the generated\ninformation during fine-tuning of self-supervised speech models, which we refer\nto as generative context-aware fine-tuning. This approach allows the fine-tuned\nmodel to make improved predictions without access to the true surrounding\nsegments or to the LLM at inference time, while requiring only a very small\nadditional context module. We evaluate the proposed approach using the SLUE and\nLibri-light benchmarks for several downstream tasks: automatic speech\nrecognition, named entity recognition, and sentiment analysis. The results show\nthat generative context-aware fine-tuning outperforms a context injection\nfine-tuning approach that accesses the ground-truth previous text, and is\ncompetitive with a generative context injection fine-tuning approach that\nrequires the LLM at inference time.", "published": "2023-12-15 15:46:02", "link": "http://arxiv.org/abs/2312.09895v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Lever LM: Configuring In-Context Sequence to Lever Large Vision Language\n  Models", "abstract": "As Archimedes famously said, ``Give me a lever long enough and a fulcrum on\nwhich to place it, and I shall move the world'', in this study, we propose to\nuse a tiny Language Model (LM), \\eg, a Transformer with 67M parameters, to\nlever much larger Vision-Language Models (LVLMs) with 9B parameters.\nSpecifically, we use this tiny \\textbf{Lever-LM} to configure effective\nin-context demonstration (ICD) sequences to improve the In-Context Learinng\n(ICL) performance of LVLMs. Previous studies show that diverse ICD\nconfigurations like the selection and ordering of the demonstrations heavily\naffect the ICL performance, highlighting the significance of configuring\neffective ICD sequences. Motivated by this and by re-considering the the\nprocess of configuring ICD sequence, we find this is a mirror process of human\nsentence composition and further assume that effective ICD configurations may\ncontain internal statistical patterns that can be captured by Lever-LM. Then a\ndataset with effective ICD sequences is constructed to train Lever-LM. After\ntraining, given novel queries, new ICD sequences are configured by the trained\nLever-LM to solve vision-language tasks through ICL. Experiments show that\nthese ICD sequences can improve the ICL performance of two LVLMs compared with\nsome strong baselines in Visual Question Answering and Image Captioning,\nvalidating that Lever-LM can really capture the statistical patterns for\nlevering LVLMs. The code is available at\n\\url{https://github.com/ForJadeForest/Lever-LM}.", "published": "2023-12-15 03:11:03", "link": "http://arxiv.org/abs/2312.10104v4", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "CRNNet: Copy Recurrent Neural Network Structure Network", "abstract": "The target of Electronic Health Record (EHR) coding is to find the diagnostic\ncodes according to the EHRs. In previous research, researchers have preferred\nto do multi-classification on the EHR coding task; most of them encode the EHR\nfirst and then process it to get the probability of each code based on the EHR\nrepresentation. However, the question of complicating diseases is neglected\namong all these methods. In this paper, we propose a novel EHR coding\nframework, which is the first attempt at detecting complicating diseases,\ncalled Copy Recurrent Neural Network Structure Network (CRNNet). This method\nrefers to the idea of adversarial learning; a Path Generator and a Path\nDiscriminator are designed to more efficiently finish the task of EHR coding.\nWe propose a copy module to detect complicating diseases; by the proposed copy\nmodule and the adversarial learning strategy, we identify complicating diseases\nefficiently. Extensive experiments show that our method achieves a 57.30\\%\nratio of complicating diseases in predictions, demonstrating the effectiveness\nof our proposed model. According to the ablation study, the proposed copy\nmechanism plays a crucial role in detecting complicating diseases.", "published": "2023-12-15 23:19:42", "link": "http://arxiv.org/abs/2312.10259v1", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided\n  Document Generation", "abstract": "This paper introduces a novel approach for topic modeling utilizing latent\ncodebooks from Vector-Quantized Variational Auto-Encoder~(VQ-VAE), discretely\nencapsulating the rich information of the pre-trained embeddings such as the\npre-trained language model. From the novel interpretation of the latent\ncodebooks and embeddings as conceptual bag-of-words, we propose a new\ngenerative topic model called Topic-VQ-VAE~(TVQ-VAE) which inversely generates\nthe original documents related to the respective latent codebook. The TVQ-VAE\ncan visualize the topics with various generative distributions including the\ntraditional BoW distribution and the autoregressive image generation. Our\nexperimental results on document analysis and image generation demonstrate that\nTVQ-VAE effectively captures the topic context which reveals the underlying\nstructures of the dataset and supports flexible forms of document generation.\nOfficial implementation of the proposed TVQ-VAE is available at\nhttps://github.com/clovaai/TVQ-VAE.", "published": "2023-12-15 15:01:10", "link": "http://arxiv.org/abs/2312.11532v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KGLens: Towards Efficient and Effective Knowledge Probing of Large\n  Language Models with Knowledge Graphs", "abstract": "Large Language Models (LLMs) might hallucinate facts, while curated Knowledge\nGraph (KGs) are typically factually reliable especially with domain-specific\nknowledge. Measuring the alignment between KGs and LLMs can effectively probe\nthe factualness and identify the knowledge blind spots of LLMs. However,\nverifying the LLMs over extensive KGs can be expensive. In this paper, we\npresent KGLens, a Thompson-sampling-inspired framework aimed at effectively and\nefficiently measuring the alignment between KGs and LLMs. KGLens features a\ngraph-guided question generator for converting KGs into natural language, along\nwith a carefully designed importance sampling strategy based on parameterized\nKG structure to expedite KG traversal. Our simulation experiment compares the\nbrute force method with KGLens under six different sampling methods,\ndemonstrating that our approach achieves superior probing efficiency.\nLeveraging KGLens, we conducted in-depth analyses of the factual accuracy of\nten LLMs across three large domain-specific KGs from Wikidata, composing over\n19K edges, 700 relations, and 21K entities. Human evaluation results indicate\nthat KGLens can assess LLMs with a level of accuracy nearly equivalent to that\nof human annotators, achieving 95.7% of the accuracy rate.", "published": "2023-12-15 23:34:05", "link": "http://arxiv.org/abs/2312.11539v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "A Deep Representation Learning-based Speech Enhancement Method Using\n  Complex Convolution Recurrent Variational Autoencoder", "abstract": "Generally, the performance of deep neural networks (DNNs) heavily depends on\nthe quality of data representation learning. Our preliminary work has\nemphasized the significance of deep representation learning (DRL) in the\ncontext of speech enhancement (SE) applications. Specifically, our initial SE\nalgorithm employed a gated recurrent unit variational autoencoder (VAE) with a\nGaussian distribution to enhance the performance of certain existing SE\nsystems. Building upon our preliminary framework, this paper introduces a novel\napproach for SE using deep complex convolutional recurrent networks with a VAE\n(DCCRN-VAE). DCCRN-VAE assumes that the latent variables of signals follow\ncomplex Gaussian distributions that are modeled by DCCRN, as these\ndistributions can better capture the behaviors of complex signals.\nAdditionally, we propose the application of a residual loss in DCCRN-VAE to\nfurther improve the quality of the enhanced speech. {Compared to our\npreliminary work, DCCRN-VAE introduces a more sophisticated DCCRN structure and\nprobability distribution for DRL. Furthermore, in comparison to DCCRN,\nDCCRN-VAE employs a more advanced DRL strategy. The experimental results\ndemonstrate that the proposed SE algorithm outperforms both our preliminary SE\nframework and the state-of-the-art DCCRN SE method in terms of scale-invariant\nsignal-to-distortion ratio, speech quality, and speech intelligibility.", "published": "2023-12-15 09:03:49", "link": "http://arxiv.org/abs/2312.09620v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Automatic channel selection and spatial feature integration for\n  multi-channel speech recognition across various array topologies", "abstract": "Automatic Speech Recognition (ASR) has shown remarkable progress, yet it\nstill faces challenges in real-world distant scenarios across various array\ntopologies each with multiple recording devices. The focal point of the CHiME-7\nDistant ASR task is to devise a unified system capable of generalizing various\narray topologies that have multiple recording devices and offering reliable\nrecognition performance in real-world environments. Addressing this task, we\nintroduce an ASR system that demonstrates exceptional performance across\nvarious array topologies. First of all, we propose two attention-based\nautomatic channel selection modules to select the most advantageous subset of\nmulti-channel signals from multiple recording devices for each utterance.\nFurthermore, we introduce inter-channel spatial features to augment the\neffectiveness of multi-frame cross-channel attention, aiding it in improving\nthe capability of spatial information awareness. Finally, we propose a\nmulti-layer convolution fusion module drawing inspiration from the U-Net\narchitecture to integrate the multi-channel output into a single-channel\noutput. Experimental results on the CHiME-7 corpus with oracle segmentation\ndemonstrate that the improvements introduced in our proposed ASR system lead to\na relative reduction of 40.1% in the Macro Diarization Attributed Word Error\nRates (DA-WER) when compared to the baseline ASR system on the Eval sets.", "published": "2023-12-15 12:35:59", "link": "http://arxiv.org/abs/2312.09746v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SELM: Speech Enhancement Using Discrete Tokens and Language Models", "abstract": "Language models (LMs) have shown superior performances in various speech\ngeneration tasks recently, demonstrating their powerful ability for semantic\ncontext modeling. Given the intrinsic similarity between speech generation and\nspeech enhancement, harnessing semantic information holds potential advantages\nfor speech enhancement tasks. In light of this, we propose SELM, a novel\nparadigm for speech enhancement, which integrates discrete tokens and leverages\nlanguage models. SELM comprises three stages: encoding, modeling, and decoding.\nWe transform continuous waveform signals into discrete tokens using pre-trained\nself-supervised learning (SSL) models and a k-means tokenizer. Language models\nthen capture comprehensive contextual information within these tokens. Finally,\na detokenizer and HiFi-GAN restore them into enhanced speech. Experimental\nresults demonstrate that SELM achieves comparable performance in objective\nmetrics alongside superior results in subjective perception. Our demos are\navailable https://honee-w.github.io/SELM/.", "published": "2023-12-15 12:36:05", "link": "http://arxiv.org/abs/2312.09747v2", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "U2-KWS: Unified Two-pass Open-vocabulary Keyword Spotting with Keyword\n  Bias", "abstract": "Open-vocabulary keyword spotting (KWS), which allows users to customize\nkeywords, has attracted increasingly more interest. However, existing methods\nbased on acoustic models and post-processing train the acoustic model with ASR\ntraining criteria to model all phonemes, making the acoustic model\nunder-optimized for the KWS task. To solve this problem, we propose a novel\nunified two-pass open-vocabulary KWS (U2-KWS) framework inspired by the\ntwo-pass ASR model U2. Specifically, we employ the CTC branch as the first\nstage model to detect potential keyword candidates and the decoder branch as\nthe second stage model to validate candidates. In order to enhance any\ncustomized keywords, we redesign the U2 training procedure for U2-KWS and add\nkeyword information by audio and text cross-attention into both branches. We\nperform experiments on our internal dataset and Aishell-1. The results show\nthat U2-KWS can achieve a significant relative wake-up rate improvement of 41%\ncompared to the traditional customized KWS systems when the false alarm rate is\nfixed to 0.5 times per hour.", "published": "2023-12-15 13:00:42", "link": "http://arxiv.org/abs/2312.09760v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Decoding Envelope and Frequency-Following EEG Responses to Continuous\n  Speech Using Deep Neural Networks", "abstract": "The electroencephalogram (EEG) offers a non-invasive means by which a\nlistener's auditory system may be monitored during continuous speech\nperception. Reliable auditory-EEG decoders could facilitate the objective\ndiagnosis of hearing disorders, or find applications in cognitively-steered\nhearing aids. Previously, we developed decoders for the ICASSP Auditory EEG\nSignal Processing Grand Challenge (SPGC). These decoders aimed to solve the\nmatch-mismatch task: given a short temporal segment of EEG recordings, and two\ncandidate speech segments, the task is to identify which of the two speech\nsegments is temporally aligned, or matched, with the EEG segment. The decoders\nmade use of cortical responses to the speech envelope, as well as\nspeech-related frequency-following responses, to relate the EEG recordings to\nthe speech stimuli. Here we comprehensively document the methods by which the\ndecoders were developed. We extend our previous analysis by exploring the\nassociation between speaker characteristics (pitch and sex) and classification\naccuracy, and provide a full statistical analysis of the final performance of\nthe decoders as evaluated on a heldout portion of the dataset. Finally, the\ngeneralisation capabilities of the decoders are characterised, by evaluating\nthem using an entirely different dataset which contains EEG recorded under a\nvariety of speech-listening conditions. The results show that the\nmatch-mismatch decoders achieve accurate and robust classification accuracies,\nand they can even serve as auditory attention decoders without additional\ntraining.", "published": "2023-12-15 13:16:24", "link": "http://arxiv.org/abs/2312.09768v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On the compression of shallow non-causal ASR models using knowledge\n  distillation and tied-and-reduced decoder for low-latency on-device speech\n  recognition", "abstract": "Recently, the cascaded two-pass architecture has emerged as a strong\ncontender for on-device automatic speech recognition (ASR). A cascade of causal\nand shallow non-causal encoders coupled with a shared decoder enables operation\nin both streaming and look-ahead modes. In this paper, we propose shallow\ncascaded model by combining various model compression techniques such as\nknowledge distillation, shared decoder, and tied-and-reduced transducer network\nin order to reduce the model footprint. The shared decoder is changed into a\ntied-and-reduced network. The cascaded two-pass model is further compressed\nusing knowledge distillation using a Kullback-Leibler divergence loss on the\nmodel posteriors. We demonstrate a 50% reduction in the size of a 41 M\nparameter cascaded teacher model with no noticeable degradation in ASR accuracy\nand a 30% reduction in latency", "published": "2023-12-15 14:48:14", "link": "http://arxiv.org/abs/2312.09842v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Amphion: An Open-Source Audio, Music and Speech Generation Toolkit", "abstract": "Amphion is an open-source toolkit for Audio, Music, and Speech Generation,\ntargeting to ease the way for junior researchers and engineers into these\nfields. It presents a unified framework that includes diverse generation tasks\nand models, with the added bonus of being easily extendable for new\nincorporation. The toolkit is designed with beginner-friendly workflows and\npre-trained models, allowing both beginners and seasoned researchers to\nkick-start their projects with relative ease. The initial release of Amphion\nv0.1 supports a range of tasks including Text to Speech (TTS), Text to Audio\n(TTA), and Singing Voice Conversion (SVC), supplemented by essential components\nlike data preprocessing, state-of-the-art vocoders, and evaluation metrics.\nThis paper presents a high-level overview of Amphion. Amphion is open-sourced\nat https://github.com/open-mmlab/Amphion.", "published": "2023-12-15 16:23:21", "link": "http://arxiv.org/abs/2312.09911v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-level graph learning for audio event classification and\n  human-perceived annoyance rating prediction", "abstract": "WHO's report on environmental noise estimates that 22 M people suffer from\nchronic annoyance related to noise caused by audio events (AEs) from various\nsources. Annoyance may lead to health issues and adverse effects on metabolic\nand cognitive systems. In cities, monitoring noise levels does not provide\ninsights into noticeable AEs, let alone their relations to annoyance. To create\nannoyance-related monitoring, this paper proposes a graph-based model to\nidentify AEs in a soundscape, and explore relations between diverse AEs and\nhuman-perceived annoyance rating (AR). Specifically, this paper proposes a\nlightweight multi-level graph learning (MLGL) based on local and global\nsemantic graphs to simultaneously perform audio event classification (AEC) and\nhuman annoyance rating prediction (ARP). Experiments show that: 1) MLGL with\n4.1 M parameters improves AEC and ARP results by using semantic node\ninformation in local and global context aware graphs; 2) MLGL captures\nrelations between coarse and fine-grained AEs and AR well; 3) Statistical\nanalysis of MLGL results shows that some AEs from different sources\nsignificantly correlate with AR, which is consistent with previous research on\nhuman perception of these sound sources.", "published": "2023-12-15 17:02:16", "link": "http://arxiv.org/abs/2312.09952v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-Supervised Learning for Anomalous Sound Detection", "abstract": "State-of-the-art anomalous sound detection (ASD) systems are often trained by\nusing an auxiliary classification task to learn an embedding space. Doing so\nenables the system to learn embeddings that are robust to noise and are\nignoring non-target sound events but requires manually annotated meta\ninformation to be used as class labels. However, the less difficult the\nclassification task becomes, the less informative are the embeddings and the\nworse is the resulting ASD performance. A solution to this problem is to\nutilize self-supervised learning (SSL). In this work, feature exchange\n(FeatEx), a simple yet effective SSL approach for ASD, is proposed. In\naddition, FeatEx is compared to and combined with existing SSL approaches. As\nthe main result, a new state-of-the-art performance for the DCASE2023 ASD\ndataset is obtained that outperforms all other published results on this\ndataset by a large margin.", "published": "2023-12-15 07:16:12", "link": "http://arxiv.org/abs/2312.09578v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A 1.6-mW Sparse Deep Learning Accelerator for Speech Separation", "abstract": "Low power deep learning accelerators on the speech processing enable\nreal-time applications on edge devices. However, most of the existing\naccelerators suffer from high power consumption and focus on image applications\nonly. This paper presents a low power accelerator for speech separation through\nalgorithm and hardware optimizations. At the algorithm level, the model is\ncompressed with structured sensitivity as well as unstructured pruning, and\nfurther quantized to the shifted 8-bit floating-point format instead of the\n32-bit floating-point format. The computations with the zero kernel and zero\nactivation values are skipped by decomposition of the dilated and transposed\nconvolutions. At the hardware level, the compressed model is then supported by\nan architecture with eight independent multipliers and accumulators (MACs) with\na simple zero-skipping hardware to take advantage of the activation sparsity\nand low power processing. The proposed approach reduces the model size by\n95.44\\% and computation complexity by 93.88\\%. The final implementation with\nthe TSMC 40 $nm$ process can achieve real-time speech separation and consumes\n1.6 mW power when operated at 150 MHz. The normalized energy efficiency and\narea efficiency are 2.344 TOPS/W and 14.42 GOPS/mm$^2$, respectively.", "published": "2023-12-15 07:22:28", "link": "http://arxiv.org/abs/2312.09580v1", "categories": ["cs.SD", "cs.AR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Stethoscope-guided Supervised Contrastive Learning for Cross-domain\n  Adaptation on Respiratory Sound Classification", "abstract": "Despite the remarkable advances in deep learning technology, achieving\nsatisfactory performance in lung sound classification remains a challenge due\nto the scarcity of available data. Moreover, the respiratory sound samples are\ncollected from a variety of electronic stethoscopes, which could potentially\nintroduce biases into the trained models. When a significant distribution shift\noccurs within the test dataset or in a practical scenario, it can substantially\ndecrease the performance. To tackle this issue, we introduce cross-domain\nadaptation techniques, which transfer the knowledge from a source domain to a\ndistinct target domain. In particular, by considering different stethoscope\ntypes as individual domains, we propose a novel stethoscope-guided supervised\ncontrastive learning approach. This method can mitigate any domain-related\ndisparities and thus enables the model to distinguish respiratory sounds of the\nrecording variation of the stethoscope. The experimental results on the ICBHI\ndataset demonstrate that the proposed methods are effective in reducing the\ndomain dependency and achieving the ICBHI Score of 61.71%, which is a\nsignificant improvement of 2.16% over the baseline.", "published": "2023-12-15 08:34:31", "link": "http://arxiv.org/abs/2312.09603v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fine-Tuned Self-Supervised Speech Representations for Language\n  Diarization in Multilingual Code-Switched Speech", "abstract": "Annotating a multilingual code-switched corpus is a painstaking process\nrequiring specialist linguistic expertise. This is partly due to the large\nnumber of language combinations that may appear within and across utterances,\nwhich might require several annotators with different linguistic expertise to\nconsider an utterance sequentially. This is time-consuming and costly. It would\nbe useful if the spoken languages in an utterance and the boundaries thereof\nwere known before annotation commences, to allow segments to be assigned to the\nrelevant language experts in parallel. To address this, we investigate the\ndevelopment of a continuous multilingual language diarizer using fine-tuned\nspeech representations extracted from a large pre-trained self-supervised\narchitecture (WavLM). We experiment with a code-switched corpus consisting of\nfive South African languages (isiZulu, isiXhosa, Setswana, Sesotho and English)\nand show substantial diarization error rate improvements for language families,\nlanguage groups, and individual languages over baseline systems.", "published": "2023-12-15 09:40:41", "link": "http://arxiv.org/abs/2312.09645v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "What to Remember: Self-Adaptive Continual Learning for Audio Deepfake\n  Detection", "abstract": "The rapid evolution of speech synthesis and voice conversion has raised\nsubstantial concerns due to the potential misuse of such technology, prompting\na pressing need for effective audio deepfake detection mechanisms. Existing\ndetection models have shown remarkable success in discriminating known deepfake\naudio, but struggle when encountering new attack types. To address this\nchallenge, one of the emergent effective approaches is continual learning. In\nthis paper, we propose a continual learning approach called Radian Weight\nModification (RWM) for audio deepfake detection. The fundamental concept\nunderlying RWM involves categorizing all classes into two groups: those with\ncompact feature distributions across tasks, such as genuine audio, and those\nwith more spread-out distributions, like various types of fake audio. These\ndistinctions are quantified by means of the in-class cosine distance, which\nsubsequently serves as the basis for RWM to introduce a trainable gradient\nmodification direction for distinct data types. Experimental evaluations\nagainst mainstream continual learning methods reveal the superiority of RWM in\nterms of knowledge acquisition and mitigating forgetting in audio deepfake\ndetection. Furthermore, RWM's applicability extends beyond audio deepfake\ndetection, demonstrating its potential significance in diverse machine learning\ndomains such as image recognition.", "published": "2023-12-15 09:52:17", "link": "http://arxiv.org/abs/2312.09651v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Toward Deep Drum Source Separation", "abstract": "In the past, the field of drum source separation faced significant challenges\ndue to limited data availability, hindering the adoption of cutting-edge deep\nlearning methods that have found success in other related audio applications.\nIn this manuscript, we introduce StemGMD, a large-scale audio dataset of\nisolated single-instrument drum stems. Each audio clip is synthesized from MIDI\nrecordings of expressive drums performances using ten real-sounding acoustic\ndrum kits. Totaling 1224 hours, StemGMD is the largest audio dataset of drums\nto date and the first to comprise isolated audio clips for every instrument in\na canonical nine-piece drum kit. We leverage StemGMD to develop LarsNet, a\nnovel deep drum source separation model. Through a bank of dedicated U-Nets,\nLarsNet can separate five stems from a stereo drum mixture faster than\nreal-time and is shown to significantly outperform state-of-the-art nonnegative\nspectro-temporal factorization methods.", "published": "2023-12-15 10:23:07", "link": "http://arxiv.org/abs/2312.09663v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LiteVSR: Efficient Visual Speech Recognition by Learning from Speech\n  Representations of Unlabeled Data", "abstract": "This paper proposes a novel, resource-efficient approach to Visual Speech\nRecognition (VSR) leveraging speech representations produced by any trained\nAutomatic Speech Recognition (ASR) model. Moving away from the\nresource-intensive trends prevalent in recent literature, our method distills\nknowledge from a trained Conformer-based ASR model, achieving competitive\nperformance on standard VSR benchmarks with significantly less resource\nutilization. Using unlabeled audio-visual data only, our baseline model\nachieves a word error rate (WER) of 47.4% and 54.7% on the LRS2 and LRS3 test\nbenchmarks, respectively. After fine-tuning the model with limited labeled\ndata, the word error rate reduces to 35% (LRS2) and 45.7% (LRS3). Our model can\nbe trained on a single consumer-grade GPU within a few days and is capable of\nperforming real-time end-to-end VSR on dated hardware, suggesting a path\ntowards more accessible and resource-efficient VSR methodologies.", "published": "2023-12-15 12:04:24", "link": "http://arxiv.org/abs/2312.09727v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Understanding Probe Behaviors through Variational Bounds of Mutual\n  Information", "abstract": "With the success of self-supervised representations, researchers seek a\nbetter understanding of the information encapsulated within a representation.\nAmong various interpretability methods, we focus on classification-based linear\nprobing. We aim to foster a solid understanding and provide guidelines for\nlinear probing by constructing a novel mathematical framework leveraging\ninformation theory. First, we connect probing with the variational bounds of\nmutual information (MI) to relax the probe design, equating linear probing with\nfine-tuning. Then, we investigate empirical behaviors and practices of probing\nthrough our mathematical framework. We analyze the layer-wise performance curve\nbeing convex, which seemingly violates the data processing inequality. However,\nwe show that the intermediate representations can have the biggest MI estimate\nbecause of the tradeoff between better separability and decreasing MI. We\nfurther suggest that the margin of linearly separable representations can be a\ncriterion for measuring the \"goodness of representation.\" We also compare\naccuracy with MI as the measuring criteria. Finally, we empirically validate\nour claims by observing the self-supervised speech models on retaining word and\nphoneme information.", "published": "2023-12-15 18:38:18", "link": "http://arxiv.org/abs/2312.10019v1", "categories": ["cs.IT", "cs.LG", "eess.AS", "math.IT"], "primary_category": "cs.IT"}
{"title": "VoCopilot: Voice-Activated Tracking of Everyday Interactions", "abstract": "Voice plays an important role in our lives by facilitating communication,\nconveying emotions, and indicating health. Therefore, tracking vocal\ninteractions can provide valuable insight into many aspects of our lives. This\npaper presents our ongoing efforts to design a new vocal tracking system we\ncall VoCopilot. VoCopilot is an end-to-end system centered around an\nenergy-efficient acoustic hardware and firmware combined with advanced machine\nlearning models. As a result, VoCopilot is able to continuously track\nconversations, record them, transcribe them, and then extract useful insights\nfrom them. By utilizing large language models, VoCopilot ensures the user can\nextract useful insights from recorded interactions without having to learn\ncomplex machine learning techniques. In order to protect the privacy of end\nusers, VoCopilot uses a novel wake-up mechanism that only records conversations\nof end users. Additionally, all the rest of pipeline can be run on a commodity\ncomputer (Mac Mini M2). In this work, we show the effectiveness of VoCopilot in\nreal-world environment for two use cases.", "published": "2023-12-15 23:46:52", "link": "http://arxiv.org/abs/2312.10265v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
