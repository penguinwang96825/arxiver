{"title": "Frame-Semantic Parsing with Softmax-Margin Segmental RNNs and a\n  Syntactic Scaffold", "abstract": "We present a new, efficient frame-semantic parser that labels semantic\narguments to FrameNet predicates. Built using an extension to the segmental RNN\nthat emphasizes recall, our basic system achieves competitive performance\nwithout any calls to a syntactic parser. We then introduce a method that uses\nphrase-syntactic annotations from the Penn Treebank during training only,\nthrough a multitask objective; no parsing is required at training or test time.\nThis \"syntactic scaffold\" offers a cheaper alternative to traditional syntactic\npipelining, and achieves state-of-the-art performance.", "published": "2017-06-29 00:42:10", "link": "http://arxiv.org/abs/1706.09528v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Frame-Based Continuous Lexical Semantics through Exponential Family\n  Tensor Factorization and Semantic Proto-Roles", "abstract": "We study how different frame annotations complement one another when learning\ncontinuous lexical semantics. We learn the representations from a tensorized\nskip-gram model that consistently encodes syntactic-semantic content better,\nwith multiple 10% gains over baselines.", "published": "2017-06-29 03:19:39", "link": "http://arxiv.org/abs/1706.09562v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recurrent neural networks with specialized word embeddings for\n  health-domain named-entity recognition", "abstract": "Background. Previous state-of-the-art systems on Drug Name Recognition (DNR)\nand Clinical Concept Extraction (CCE) have focused on a combination of text\n\"feature engineering\" and conventional machine learning algorithms such as\nconditional random fields and support vector machines. However, developing good\nfeatures is inherently heavily time-consuming. Conversely, more modern machine\nlearning approaches such as recurrent neural networks (RNNs) have proved\ncapable of automatically learning effective features from either random\nassignments or automated word \"embeddings\". Objectives. (i) To create a highly\naccurate DNR and CCE system that avoids conventional, time-consuming feature\nengineering. (ii) To create richer, more specialized word embeddings by using\nhealth domain datasets such as MIMIC-III. (iii) To evaluate our systems over\nthree contemporary datasets. Methods. Two deep learning methods, namely the\nBidirectional LSTM and the Bidirectional LSTM-CRF, are evaluated. A CRF model\nis set as the baseline to compare the deep learning systems to a traditional\nmachine learning approach. The same features are used for all the models.\nResults. We have obtained the best results with the Bidirectional LSTM-CRF\nmodel, which has outperformed all previously proposed systems. The specialized\nembeddings have helped to cover unusual words in DDI-DrugBank and DDI-MedLine,\nbut not in the 2010 i2b2/VA IRB Revision dataset. Conclusion. We present a\nstate-of-the-art system for DNR and CCE. Automated word embeddings has allowed\nus to avoid costly feature engineering and achieve higher accuracy.\nNevertheless, the embeddings need to be retrained over datasets that are\nadequate for the domain, in order to adequately cover the domain-specific\nvocabulary.", "published": "2017-06-29 03:53:55", "link": "http://arxiv.org/abs/1706.09569v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Distributed Representations of Tweets - Present and Future", "abstract": "Unsupervised representation learning for tweets is an important research\nfield which helps in solving several business applications such as sentiment\nanalysis, hashtag prediction, paraphrase detection and microblog ranking. A\ngood tweet representation learning model must handle the idiosyncratic nature\nof tweets which poses several challenges such as short length, informal words,\nunusual grammar and misspellings. However, there is a lack of prior work which\nsurveys the representation learning models with a focus on tweets. In this\nwork, we organize the models based on its objective function which aids the\nunderstanding of the literature. We also provide interesting future directions,\nwhich we believe are fruitful in advancing this field by building high-quality\ntweet representation learning models.", "published": "2017-06-29 10:54:43", "link": "http://arxiv.org/abs/1706.09673v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stronger Baselines for Trustable Results in Neural Machine Translation", "abstract": "Interest in neural machine translation has grown rapidly as its effectiveness\nhas been demonstrated across language and data scenarios. New research\nregularly introduces architectural and algorithmic improvements that lead to\nsignificant gains over \"vanilla\" NMT implementations. However, these new\ntechniques are rarely evaluated in the context of previously published\ntechniques, specifically those that are widely used in state-of-theart\nproduction and shared-task systems. As a result, it is often difficult to\ndetermine whether improvements from research will carry over to systems\ndeployed for real-world use. In this work, we recommend three specific methods\nthat are relatively easy to implement and result in much stronger experimental\nsystems. Beyond reporting significantly higher BLEU scores, we conduct an\nin-depth analysis of where improvements originate and what inherent weaknesses\nof basic NMT models are being addressed. We then compare the relative gains\nafforded by several other techniques proposed in the literature when starting\nwith vanilla systems versus our stronger baselines, showing that experimental\nconclusions may change depending on the baseline chosen. This indicates that\nchoosing a strong baseline is crucial for reporting reliable experimental\nresults.", "published": "2017-06-29 13:02:46", "link": "http://arxiv.org/abs/1706.09733v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Two-Stage Synthesis Networks for Transfer Learning in Machine\n  Comprehension", "abstract": "We develop a technique for transfer learning in machine comprehension (MC)\nusing a novel two-stage synthesis network (SynNet). Given a high-performing MC\nmodel in one domain, our technique aims to answer questions about documents in\nanother domain, where we use no labeled data of question-answer pairs. Using\nthe proposed SynNet with a pretrained model from the SQuAD dataset on the\nchallenging NewsQA dataset, we achieve an F1 measure of 44.3% with a single\nmodel and 46.6% with an ensemble, approaching performance of in-domain models\n(F1 measure of 50.0%) and outperforming the out-of-domain baseline of 7.6%,\nwithout use of provided annotations.", "published": "2017-06-29 14:58:47", "link": "http://arxiv.org/abs/1706.09789v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Relevance of Unsupervised Metrics in Task-Oriented Dialogue for\n  Evaluating Natural Language Generation", "abstract": "Automated metrics such as BLEU are widely used in the machine translation\nliterature. They have also been used recently in the dialogue community for\nevaluating dialogue response generation. However, previous work in dialogue\nresponse generation has shown that these metrics do not correlate strongly with\nhuman judgment in the non task-oriented dialogue setting. Task-oriented\ndialogue responses are expressed on narrower domains and exhibit lower\ndiversity. It is thus reasonable to think that these automated metrics would\ncorrelate well with human judgment in the task-oriented setting where the\ngeneration task consists of translating dialogue acts into a sentence. We\nconduct an empirical study to confirm whether this is the case. Our findings\nindicate that these automated metrics have stronger correlation with human\njudgments in the task-oriented setting compared to what has been observed in\nthe non task-oriented setting. We also observe that these metrics correlate\neven better for datasets which provide multiple ground truth reference\nsentences. In addition, we show that some of the currently available corpora\nfor task-oriented language generation can be solved with simple models and\nadvocate for more challenging datasets.", "published": "2017-06-29 15:14:07", "link": "http://arxiv.org/abs/1706.09799v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Mapping of French Discourse Connectives to PDTB Discourse\n  Relations", "abstract": "In this paper, we present an approach to exploit phrase tables generated by\nstatistical machine translation in order to map French discourse connectives to\ndiscourse relations. Using this approach, we created ConcoLeDisCo, a lexicon of\nFrench discourse connectives and their PDTB relations. When evaluated against\nLEXCONN, ConcoLeDisCo achieves a recall of 0.81 and an Average Precision of\n0.68 for the Concession and Condition relations.", "published": "2017-06-29 17:04:48", "link": "http://arxiv.org/abs/1706.09856v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-scale Multi-band DenseNets for Audio Source Separation", "abstract": "This paper deals with the problem of audio source separation. To handle the\ncomplex and ill-posed nature of the problems of audio source separation, the\ncurrent state-of-the-art approaches employ deep neural networks to obtain\ninstrumental spectra from a mixture. In this study, we propose a novel network\narchitecture that extends the recently developed densely connected\nconvolutional network (DenseNet), which has shown excellent results on image\nclassification tasks. To deal with the specific problem of audio source\nseparation, an up-sampling layer, block skip connection and band-dedicated\ndense blocks are incorporated on top of DenseNet. The proposed approach takes\nadvantage of long contextual information and outperforms state-of-the-art\nresults on SiSEC 2016 competition by a large margin in terms of\nsignal-to-distortion ratio. Moreover, the proposed architecture requires\nsignificantly fewer parameters and considerably less training time compared\nwith other methods.", "published": "2017-06-29 05:56:06", "link": "http://arxiv.org/abs/1706.09588v1", "categories": ["cs.SD", "cs.CL", "cs.MM"], "primary_category": "cs.SD"}
