{"title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning", "abstract": "The capacity for complex mathematical reasoning is a key benchmark for\nartificial intelligence. While reinforcement learning (RL) applied to LLMs\nshows promise, progress is significantly hindered by the lack of large-scale\ntraining data that is sufficiently challenging, possesses verifiable answer\nformats suitable for RL, and is free from contamination with evaluation\nbenchmarks. To address these limitations, we introduce DeepMath-103K, a new,\nlarge-scale dataset comprising approximately 103K mathematical problems,\nspecifically designed to train advanced reasoning models via RL. DeepMath-103K\nis curated through a rigorous pipeline involving source analysis, stringent\ndecontamination against numerous benchmarks, and filtering for high difficulty\n(primarily Levels 5-9), significantly exceeding existing open resources in\nchallenge. Each problem includes a verifiable final answer, enabling rule-based\nRL, and three distinct R1-generated solutions suitable for diverse training\nparadigms like supervised fine-tuning or distillation. Spanning a wide range of\nmathematical topics, DeepMath-103K promotes the development of generalizable\nreasoning. We demonstrate that models trained on DeepMath-103K achieve\nsignificant improvements on challenging mathematical benchmarks, validating its\neffectiveness. We release DeepMath-103K publicly to facilitate community\nprogress in building more capable AI reasoning systems:\nhttps://github.com/zwhe99/DeepMath.", "published": "2025-04-15 17:59:51", "link": "http://arxiv.org/abs/2504.11456v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TextArena", "abstract": "TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.", "published": "2025-04-15 17:55:20", "link": "http://arxiv.org/abs/2504.11442v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "TADACap: Time-series Adaptive Domain-Aware Captioning", "abstract": "While image captioning has gained significant attention, the potential of\ncaptioning time-series images, prevalent in areas like finance and healthcare,\nremains largely untapped. Existing time-series captioning methods typically\noffer generic, domain-agnostic descriptions of time-series shapes and struggle\nto adapt to new domains without substantial retraining. To address these\nlimitations, we introduce TADACap, a retrieval-based framework to generate\ndomain-aware captions for time-series images, capable of adapting to new\ndomains without retraining. Building on TADACap, we propose a novel retrieval\nstrategy that retrieves diverse image-caption pairs from a target domain\ndatabase, namely TADACap-diverse. We benchmarked TADACap-diverse against\nstate-of-the-art methods and ablation variants. TADACap-diverse demonstrates\ncomparable semantic accuracy while requiring significantly less annotation\neffort.", "published": "2025-04-15 17:54:59", "link": "http://arxiv.org/abs/2504.11441v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Masculine Defaults via Gendered Discourse in Podcasts and Large Language Models", "abstract": "Masculine defaults are widely recognized as a significant type of gender\nbias, but they are often unseen as they are under-researched. Masculine\ndefaults involve three key parts: (i) the cultural context, (ii) the masculine\ncharacteristics or behaviors, and (iii) the reward for, or simply acceptance\nof, those masculine characteristics or behaviors. In this work, we study\ndiscourse-based masculine defaults, and propose a twofold framework for (i) the\nlarge-scale discovery and analysis of gendered discourse words in spoken\ncontent via our Gendered Discourse Correlation Framework (GDCF); and (ii) the\nmeasurement of the gender bias associated with these gendered discourse words\nin LLMs via our Discourse Word-Embedding Association Test (D-WEAT). We focus\nour study on podcasts, a popular and growing form of social media, analyzing\n15,117 podcast episodes. We analyze correlations between gender and discourse\nwords -- discovered via LDA and BERTopic -- to automatically form gendered\ndiscourse word lists. We then study the prevalence of these gendered discourse\nwords in domain-specific contexts, and find that gendered discourse-based\nmasculine defaults exist in the domains of business, technology/politics, and\nvideo games. Next, we study the representation of these gendered discourse\nwords from a state-of-the-art LLM embedding model from OpenAI, and find that\nthe masculine discourse words have a more stable and robust representation than\nthe feminine discourse words, which may result in better system performance on\ndownstream tasks for men. Hence, men are rewarded for their discourse patterns\nwith better system performance by one of the state-of-the-art language models\n-- and this embedding disparity is a representational harm and a masculine\ndefault.", "published": "2025-04-15 17:41:54", "link": "http://arxiv.org/abs/2504.11431v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "A Dual-Space Framework for General Knowledge Distillation of Large Language Models", "abstract": "Knowledge distillation (KD) is a promising solution to compress large\nlanguage models (LLMs) by transferring their knowledge to smaller models.\nDuring this process, white-box KD methods usually minimize the distance between\nthe output distributions of the teacher model and the student model to transfer\nmore information. However, we reveal that the current white-box KD framework\nexhibits two limitations: a) bridging probability distributions from different\noutput spaces will limit the similarity between the teacher model and the\nstudent model; b) this framework cannot be applied to LLMs with different\nvocabularies. One of the root causes for these limitations is that the\ndistributions from the teacher and the student for KD are output by different\nprediction heads, which yield distributions in different output spaces and\ndimensions. Therefore, in this paper, we propose a dual-space knowledge\ndistillation (DSKD) framework that unifies the prediction heads of the teacher\nand the student models for KD. Specifically, we first introduce two projectors\nwith ideal initialization to project the teacher/student hidden states into the\nstudent/teacher representation spaces. After this, the hidden states from\ndifferent models can share the same head and unify the output spaces of the\ndistributions. Furthermore, we develop an exact token alignment (ETA) algorithm\nto align the same tokens in two differently-tokenized sequences. Based on the\nabove, our DSKD framework is a general KD framework that supports both\noff-policy and on-policy KD, and KD between any two LLMs regardless of their\nvocabularies. Extensive experiments on instruction-following, mathematical\nreasoning, and code generation benchmarks show that DSKD significantly\noutperforms existing methods based on the current white-box KD framework and\nsurpasses other cross-tokenizer KD methods for LLMs with different\nvocabularies.", "published": "2025-04-15 17:38:47", "link": "http://arxiv.org/abs/2504.11426v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reinforcing Compositional Retrieval: Retrieving Step-by-Step for Composing Informative Contexts", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet they often rely on external context to handle complex\ntasks. While retrieval-augmented frameworks traditionally focus on selecting\ntop-ranked documents in a single pass, many real-world scenarios demand\ncompositional retrieval, where multiple sources must be combined in a\ncoordinated manner. In this work, we propose a tri-encoder sequential retriever\nthat models this process as a Markov Decision Process (MDP), decomposing the\nprobability of retrieving a set of elements into a sequence of conditional\nprobabilities and allowing each retrieval step to be conditioned on previously\nselected examples. We train the retriever in two stages: first, we efficiently\nconstruct supervised sequential data for initial policy training; we then\nrefine the policy to align with the LLM's preferences using a reward grounded\nin the structural correspondence of generated programs. Experimental results\nshow that our method consistently and significantly outperforms baselines,\nunderscoring the importance of explicitly modeling inter-example dependencies.\nThese findings highlight the potential of compositional retrieval for tasks\nrequiring multiple pieces of evidence or examples.", "published": "2025-04-15 17:35:56", "link": "http://arxiv.org/abs/2504.11420v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning", "abstract": "Hybrid LLM architectures that combine Attention and State Space Models (SSMs)\nachieve state-of-the-art accuracy and runtime performance. Recent work has\ndemonstrated that applying compression and distillation to Attention-only\nmodels yields smaller, more accurate models at a fraction of the training cost.\nIn this work, we explore the effectiveness of compressing Hybrid architectures.\nWe introduce a novel group-aware pruning strategy that preserves the structural\nintegrity of SSM blocks and their sequence modeling capabilities. Furthermore,\nwe demonstrate the necessity of such SSM pruning to achieve improved accuracy\nand inference speed compared to traditional approaches. Our compression recipe\ncombines SSM, FFN, embedding dimension, and layer pruning, followed by\nknowledge distillation-based retraining, similar to the MINITRON technique.\nUsing this approach, we compress the Nemotron-H 8B Hybrid model down to 4B\nparameters with up to 40x fewer training tokens. The resulting model surpasses\nthe accuracy of similarly-sized models while achieving 2x faster inference,\nsignificantly advancing the Pareto frontier.", "published": "2025-04-15 17:26:29", "link": "http://arxiv.org/abs/2504.11409v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DataDecide: How to Predict Best Pretraining Data with Small Experiments", "abstract": "Because large language models are expensive to pretrain on different\ndatasets, using smaller-scale experiments to decide on data is crucial for\nreducing costs. Which benchmarks and methods of making decisions from observed\nperformance at small scale most accurately predict the datasets that yield the\nbest large models? To empower open exploration of this question, we release\nmodels, data, and evaluations in DataDecide -- the most extensive open suite of\nmodels over differences in data and scale. We conduct controlled pretraining\nexperiments across 25 corpora with differing sources, deduplication, and\nfiltering up to 100B tokens, model sizes up to 1B parameters, and 3 random\nseeds. We find that the ranking of models at a single, small size (e.g., 150M\nparameters) is a strong baseline for predicting best models at our larger\ntarget scale (1B) (~80% of com parisons correct). No scaling law methods among\n8 baselines exceed the compute-decision frontier of single-scale predictions,\nbut DataDecide can measure improvement in future scaling laws. We also identify\nthat using continuous likelihood metrics as proxies in small experiments makes\nbenchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable\nat the target 1B scale with just 0.01% of the compute.", "published": "2025-04-15 17:02:15", "link": "http://arxiv.org/abs/2504.11393v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "RankAlign: A Ranking View of the Generator-Validator Gap in Large Language Models", "abstract": "Although large language models (LLMs) have become generally more capable and\naccurate across many tasks, some fundamental sources of unreliability remain in\ntheir behavior. One key limitation is their inconsistency at reporting the the\nsame information when prompts are changed. In this paper, we consider the\ndiscrepancy between a model's generated answer and their own verification of\nthat answer, the generator-validator gap. We define this gap in a more\nstringent way than prior work: we expect correlation of scores from a generator\nand a validator over the entire set of candidate answers. We show that\naccording to this measure, a large gap exists in various settings, including\nquestion answering, lexical semantics tasks, and next-word prediction. We then\npropose RankAlign, a ranking-based training method, and show that it\nsignificantly closes the gap by 31.8% on average, surpassing all baseline\nmethods. Moreover, this approach generalizes well to out-of-domain tasks and\nlexical items.", "published": "2025-04-15 16:53:31", "link": "http://arxiv.org/abs/2504.11381v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cancer-Myth: Evaluating AI Chatbot on Patient Questions with False Presuppositions", "abstract": "Cancer patients are increasingly turning to large language models (LLMs) as a\nnew form of internet search for medical information, making it critical to\nassess how well these models handle complex, personalized questions. However,\ncurrent medical benchmarks focus on medical exams or consumer-searched\nquestions and do not evaluate LLMs on real patient questions with detailed\nclinical contexts. In this paper, we first evaluate LLMs on cancer-related\nquestions drawn from real patients, reviewed by three hematology oncology\nphysicians. While responses are generally accurate, with GPT-4-Turbo scoring\n4.13 out of 5, the models frequently fail to recognize or address false\npresuppositions in the questions-posing risks to safe medical decision-making.\nTo study this limitation systematically, we introduce Cancer-Myth, an\nexpert-verified adversarial dataset of 585 cancer-related questions with false\npresuppositions. On this benchmark, no frontier LLM -- including GPT-4o,\nGemini-1.Pro, and Claude-3.5-Sonnet -- corrects these false presuppositions\nmore than 30% of the time. Even advanced medical agentic methods do not prevent\nLLMs from ignoring false presuppositions. These findings expose a critical gap\nin the clinical reliability of LLMs and underscore the need for more robust\nsafeguards in medical AI systems.", "published": "2025-04-15 16:37:32", "link": "http://arxiv.org/abs/2504.11373v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "OpenTuringBench: An Open-Model-based Benchmark and Framework for Machine-Generated Text Detection and Attribution", "abstract": "Open Large Language Models (OLLMs) are increasingly leveraged in generative\nAI applications, posing new challenges for detecting their outputs. We propose\nOpenTuringBench, a new benchmark based on OLLMs, designed to train and evaluate\nmachine-generated text detectors on the Turing Test and Authorship Attribution\nproblems. OpenTuringBench focuses on a representative set of OLLMs, and\nfeatures a number of challenging evaluation tasks, including\nhuman/machine-manipulated texts, out-of-domain texts, and texts from previously\nunseen models. We also provide OTBDetector, a contrastive learning framework to\ndetect and attribute OLLM-based machine-generated texts. Results highlight the\nrelevance and varying degrees of difficulty of the OpenTuringBench tasks, with\nour detector achieving remarkable capabilities across the various tasks and\noutperforming most existing detectors. Resources are available on the\nOpenTuringBench Hugging Face repository at\nhttps://huggingface.co/datasets/MLNTeam-Unical/OpenTuringBench", "published": "2025-04-15 16:36:14", "link": "http://arxiv.org/abs/2504.11369v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Network Alignment", "abstract": "Complex networks are frequently employed to model physical or virtual complex\nsystems. When certain entities exist across multiple systems simultaneously,\nunveiling their corresponding relationships across the networks becomes\ncrucial. This problem, known as network alignment, holds significant\nimportance. It enhances our understanding of complex system structures and\nbehaviours, facilitates the validation and extension of theoretical physics\nresearch about studying complex systems, and fosters diverse practical\napplications across various fields. However, due to variations in the\nstructure, characteristics, and properties of complex networks across different\nfields, the study of network alignment is often isolated within each domain,\nwith even the terminologies and concepts lacking uniformity. This review\ncomprehensively summarizes the latest advancements in network alignment\nresearch, focusing on analyzing network alignment characteristics and progress\nin various domains such as social network analysis, bioinformatics,\ncomputational linguistics and privacy protection. It provides a detailed\nanalysis of various methods' implementation principles, processes, and\nperformance differences, including structure consistency-based methods, network\nembedding-based methods, and graph neural network-based (GNN-based) methods.\nAdditionally, the methods for network alignment under different conditions,\nsuch as in attributed networks, heterogeneous networks, directed networks, and\ndynamic networks, are presented. Furthermore, the challenges and the open\nissues for future studies are also discussed.", "published": "2025-04-15 16:32:09", "link": "http://arxiv.org/abs/2504.11367v1", "categories": ["physics.soc-ph", "cs.CL"], "primary_category": "physics.soc-ph"}
{"title": "Teaching Large Language Models to Reason through Learning and Forgetting", "abstract": "Leveraging inference-time search in large language models has proven\neffective in further enhancing a trained model's capability to solve complex\nmathematical and reasoning problems. However, this approach significantly\nincreases computational costs and inference time, as the model must generate\nand evaluate multiple candidate solutions to identify a viable reasoning path.\nTo address this, we propose an effective approach that integrates search\ncapabilities directly into the model by fine-tuning it using both successful\n(learning) and failed reasoning paths (forgetting) derived from diverse search\nmethods. While fine-tuning the model with these data might seem\nstraightforward, we identify a critical issue: the model's search capability\ntends to degrade rapidly if fine-tuning is performed naively. We show that this\ndegradation can be substantially mitigated by employing a smaller learning\nrate. Extensive experiments on the challenging Game-of-24 and Countdown\nmathematical reasoning benchmarks show that our approach not only outperforms\nboth standard fine-tuning and inference-time search baselines but also\nsignificantly reduces inference time by 180$\\times$.", "published": "2025-04-15 16:30:02", "link": "http://arxiv.org/abs/2504.11364v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce", "abstract": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning\nlarge language models (LLMs) on complex reasoning tasks. Among recent methods,\nGRPO stands out for its empirical success in training models such as\nDeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In\nthis work, we revisit GRPO from a reinforce-like algorithm perspective and\nanalyze its core components. Surprisingly, we find that a simple rejection\nsampling baseline, RAFT, which trains only on positively rewarded samples,\nyields competitive performance than GRPO and PPO. Our ablation studies reveal\nthat GRPO's main advantage arises from discarding prompts with entirely\nincorrect responses, rather than from its reward normalization. Motivated by\nthis insight, we propose Reinforce-Rej, a minimal extension of policy gradient\nthat filters both entirely incorrect and entirely correct samples.\nReinforce-Rej improves KL efficiency and stability, serving as a lightweight\nyet effective alternative to more complex RL algorithms. We advocate RAFT as a\nrobust and interpretable baseline, and suggest that future advances should\nfocus on more principled designs for incorporating negative samples, rather\nthan relying on them indiscriminately. Our findings provide guidance for future\nwork in reward-based LLM post-training.", "published": "2025-04-15 16:15:02", "link": "http://arxiv.org/abs/2504.11343v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "REWARD CONSISTENCY: Improving Multi-Objective Alignment from a Data-Centric Perspective", "abstract": "Multi-objective preference alignment in language models often encounters a\nchallenging trade-off: optimizing for one human preference (e.g., helpfulness)\nfrequently compromises others (e.g., harmlessness) due to the inherent\nconflicts between competing objectives. While prior work mainly focuses on\nalgorithmic solutions, we explore a novel data-driven approach to uncover the\ntypes of data that can effectively mitigate these conflicts. Specifically, we\npropose the concept of Reward Consistency (RC), which identifies samples that\nalign with multiple preference objectives, thereby reducing conflicts during\ntraining. Through gradient-based analysis, we demonstrate that RC-compliant\nsamples inherently constrain performance degradation during multi-objective\noptimization. Building on these insights, we further develop Reward Consistency\nSampling, a framework that automatically constructs preference datasets that\neffectively mitigate conflicts during multi-objective alignment. Our generated\ndata achieves an average improvement of 13.37% in both the harmless rate and\nhelpfulness win rate when optimizing harmlessness and helpfulness, and can\nconsistently resolve conflicts in varying multi-objective scenarios.", "published": "2025-04-15 16:09:19", "link": "http://arxiv.org/abs/2504.11337v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Looking beyond the next token", "abstract": "The structure of causal language model training assumes that each token can\nbe accurately predicted from the previous context. This contrasts with humans'\nnatural writing and reasoning process, where goals are typically known before\nthe exact argument or phrasings. While this mismatch has been well studied in\nthe literature, the working assumption has been that architectural changes are\nneeded to address this mismatch. We argue that rearranging and processing the\ntraining data sequences can allow models to more accurately imitate the true\ndata-generating process, and does not require any other changes to the\narchitecture or training infrastructure. We demonstrate that this technique,\nTrelawney, and the inference algorithms derived from it allow us to improve\nperformance on several key benchmarks that span planning, algorithmic\nreasoning, and story generation tasks. Finally, our method naturally enables\nthe generation of long-term goals at no additional cost. We investigate how\nusing the model's goal-generation capability can further improve planning and\nreasoning. Additionally, we believe Trelawney could potentially open doors to\nnew capabilities beyond the current language modeling paradigm.", "published": "2025-04-15 16:09:06", "link": "http://arxiv.org/abs/2504.11336v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Dependency Structure Augmented Contextual Scoping Framework for Multimodal Aspect-Based Sentiment Analysis", "abstract": "Multimodal Aspect-Based Sentiment Analysis (MABSA) seeks to extract\nfine-grained information from image-text pairs to identify aspect terms and\ndetermine their sentiment polarity. However, existing approaches often fall\nshort in simultaneously addressing three core challenges: Sentiment Cue\nPerception (SCP), Multimodal Information Misalignment (MIM), and Semantic Noise\nElimination (SNE). To overcome these limitations, we propose DASCO\n(\\textbf{D}ependency Structure \\textbf{A}ugmented \\textbf{Sco}ping Framework),\na fine-grained scope-oriented framework that enhances aspect-level sentiment\nreasoning by leveraging dependency parsing trees. First, we designed a\nmulti-task pretraining strategy for MABSA on our base model, combining\naspect-oriented enhancement, image-text matching, and aspect-level\nsentiment-sensitive cognition. This improved the model's perception of aspect\nterms and sentiment cues while achieving effective image-text alignment,\naddressing key challenges like SCP and MIM. Furthermore, we incorporate\ndependency trees as syntactic branch combining with semantic branch, guiding\nthe model to selectively attend to critical contextual elements within a\ntarget-specific scope while effectively filtering out irrelevant noise for\naddressing SNE problem. Extensive experiments on two benchmark datasets across\nthree subtasks demonstrate that DASCO achieves state-of-the-art performance in\nMABSA, with notable gains in JMASA (+3.1\\% F1 and +5.4\\% precision on\nTwitter2015).", "published": "2025-04-15 16:05:09", "link": "http://arxiv.org/abs/2504.11331v1", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Automated Python Translation", "abstract": "Python is one of the most commonly used programming languages in industry and\neducation. Its English keywords and built-in functions/modules allow it to come\nclose to pseudo-code in terms of its readability and ease of writing. However,\nthose who do not speak English may not experience these advantages. In fact,\nthey may even be hindered in their ability to understand Python code, as the\nEnglish nature of its terms creates an additional layer of overhead. To that\nend, we introduce the task of automatically translating Python's natural\nmodality (keywords, error types, identifiers, etc.) into other human languages.\nThis presents a unique challenge, considering the abbreviated nature of these\nforms, as well as potential untranslatability of advanced\nmathematical/programming concepts across languages. We therefore create an\nautomated pipeline to translate Python into other human languages, comparing\nstrategies using machine translation and large language models. We then use\nthis pipeline to acquire translations from five common Python libraries\n(pytorch, pandas, tensorflow, numpy, and random) in seven languages, and do a\nquality test on a subset of these terms in French, Greek, and Bengali. We hope\nthis will provide a clearer path forward towards creating a universal Python,\naccessible to anyone regardless of nationality or language background.", "published": "2025-04-15 15:30:22", "link": "http://arxiv.org/abs/2504.11290v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Obvious Invisible Threat: LLM-Powered GUI Agents' Vulnerability to Fine-Print Injections", "abstract": "A Large Language Model (LLM) powered GUI agent is a specialized autonomous\nsystem that performs tasks on the user's behalf according to high-level\ninstructions. It does so by perceiving and interpreting the graphical user\ninterfaces (GUIs) of relevant apps, often visually, inferring necessary\nsequences of actions, and then interacting with GUIs by executing the actions\nsuch as clicking, typing, and tapping. To complete real-world tasks, such as\nfilling forms or booking services, GUI agents often need to process and act on\nsensitive user data. However, this autonomy introduces new privacy and security\nrisks. Adversaries can inject malicious content into the GUIs that alters agent\nbehaviors or induces unintended disclosures of private information. These\nattacks often exploit the discrepancy between visual saliency for agents and\nhuman users, or the agent's limited ability to detect violations of contextual\nintegrity in task automation. In this paper, we characterized six types of such\nattacks, and conducted an experimental study to test these attacks with six\nstate-of-the-art GUI agents, 234 adversarial webpages, and 39 human\nparticipants. Our findings suggest that GUI agents are highly vulnerable,\nparticularly to contextually embedded threats. Moreover, human users are also\nsusceptible to many of these attacks, indicating that simple human oversight\nmay not reliably prevent failures. This misalignment highlights the need for\nprivacy-aware agent design. We propose practical defense strategies to inform\nthe development of safer and more reliable GUI agents.", "published": "2025-04-15 15:21:09", "link": "http://arxiv.org/abs/2504.11281v1", "categories": ["cs.HC", "cs.CL", "cs.CR"], "primary_category": "cs.HC"}
{"title": "From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning Method for LLMs", "abstract": "Large language models (LLMs) exhibit excellent performance in natural\nlanguage processing (NLP), but remain highly sensitive to the quality of input\nqueries, especially when these queries contain misleading or inaccurate\ninformation. Existing methods focus on correcting the output, but they often\noverlook the potential of improving the ability of LLMs to detect and correct\nmisleading content in the input itself. In this paper, we propose a novel\nthree-stage fine-tuning method that enhances the ability of LLMs to detect and\ncorrect misleading information in the input, further improving response\naccuracy and reducing hallucinations. Specifically, the three stages include\n(1) training LLMs to identify misleading information, (2) training LLMs to\ncorrect the misleading information using built-in or external knowledge, and\n(3) training LLMs to generate accurate answers based on the corrected queries.\nTo evaluate our method, we conducted experiments on three datasets for the\nhallucination detection task and the question answering (QA) task, as well as\ntwo datasets containing misleading information that we constructed. The\nexperimental results demonstrate that our method significantly improves the\naccuracy and factuality of LLM responses, while also enhancing the ability to\ndetect hallucinations and reducing the generation of hallucinations in the\noutput, particularly when the query contains misleading information. We will\npublicly release our code upon acceptance.", "published": "2025-04-15 15:16:45", "link": "http://arxiv.org/abs/2504.11277v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis", "abstract": "Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation.In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://colmon46.github.io/i2e-bench-leaderboard/", "published": "2025-04-15 14:56:21", "link": "http://arxiv.org/abs/2504.11257v1", "categories": ["cs.HC", "cs.CL", "cs.CV"], "primary_category": "cs.HC"}
{"title": "Towards Automated Safety Requirements Derivation Using Agent-based RAG", "abstract": "We study the automated derivation of safety requirements in a self-driving\nvehicle use case, leveraging LLMs in combination with agent-based\nretrieval-augmented generation. Conventional approaches that utilise\npre-trained LLMs to assist in safety analyses typically lack domain-specific\nknowledge. Existing RAG approaches address this issue, yet their performance\ndeteriorates when handling complex queries and it becomes increasingly harder\nto retrieve the most relevant information. This is particularly relevant for\nsafety-relevant applications. In this paper, we propose the use of agent-based\nRAG to derive safety requirements and show that the retrieved information is\nmore relevant to the queries. We implement an agent-based approach on a\ndocument pool of automotive standards and the Apollo case study, as a\nrepresentative example of an automated driving perception system. Our solution\nis tested on a data set of safety requirement questions and answers, extracted\nfrom the Apollo data. Evaluating a set of selected RAG metrics, we present and\ndiscuss advantages of a agent-based approach compared to default RAG methods.", "published": "2025-04-15 14:43:19", "link": "http://arxiv.org/abs/2504.11243v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling Reasoning Benchmark for LLMs", "abstract": "Reasoning is the fundamental capability of large language models (LLMs). Due\nto the rapid progress of LLMs, there are two main issues of current benchmarks:\ni) these benchmarks can be crushed in a short time (less than 1 year), and ii)\nthese benchmarks may be easily hacked. To handle these issues, we propose the\never-scalingness for building the benchmarks which are uncrushable, unhackable,\nauto-verifiable and general. This paper presents Nondeterministic\nPolynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark\nfor LLMs. Specifically, the NPPC has three main modules: i) npgym, which\nprovides a unified interface of 25 well-known NP-complete problems and can\ngenerate any number of instances with any levels of complexities, ii) npsolver:\nwhich provides a unified interface to evaluate the problem instances with both\nonline and offline models via APIs and local deployments, respectively, and\niii) npeval: which provides the comprehensive and ready-to-use tools to analyze\nthe performances of LLMs over different problems, the number of tokens, the aha\nmoments, the reasoning errors and the solution errors. Extensive experiments\nover widely-used LLMs demonstrate: i) NPPC can successfully decrease the\nperformances of advanced LLMs' performances to below 10%, demonstrating that\nNPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1/o3-mini are the\nmost powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and\no1/o3-mini in most NP-complete problems considered, and iii) the numbers of\ntokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and\nDeepSeek-R1, are observed first to increase and then decrease when the problem\ninstances become more and more difficult. We believe that NPPC is the first\never-scaling reasoning benchmark, serving as the uncrushable and unhackable\ntestbed for LLMs toward artificial general intelligence (AGI).", "published": "2025-04-15 14:40:29", "link": "http://arxiv.org/abs/2504.11239v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Enhancing multimodal analogical reasoning with Logic Augmented Generation", "abstract": "Recent advances in Large Language Models have demonstrated their capabilities\nacross a variety of tasks. However, automatically extracting implicit knowledge\nfrom natural language remains a significant challenge, as machines lack active\nexperience with the physical world. Given this scenario, semantic knowledge\ngraphs can serve as conceptual spaces that guide the automated text generation\nreasoning process to achieve more efficient and explainable results. In this\npaper, we apply a logic-augmented generation (LAG) framework that leverages the\nexplicit representation of a text through a semantic knowledge graph and\napplies it in combination with prompt heuristics to elicit implicit analogical\nconnections. This method generates extended knowledge graph triples\nrepresenting implicit meaning, enabling systems to reason on unlabeled\nmultimodal data regardless of the domain. We validate our work through three\nmetaphor detection and understanding tasks across four datasets, as they\nrequire deep analogical reasoning capabilities. The results show that this\nintegrated approach surpasses current baselines, performs better than humans in\nunderstanding visual metaphors, and enables more explainable reasoning\nprocesses, though still has inherent limitations in metaphor understanding,\nespecially for domain-specific metaphors. Furthermore, we propose a thorough\nerror analysis, discussing issues with metaphorical annotations and current\nevaluation methods.", "published": "2025-04-15 13:47:55", "link": "http://arxiv.org/abs/2504.11190v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Benchmarking Next-Generation Reasoning-Focused Large Language Models in Ophthalmology: A Head-to-Head Evaluation on 5,888 Items", "abstract": "Recent advances in reasoning-focused large language models (LLMs) mark a\nshift from general LLMs toward models designed for complex decision-making, a\ncrucial aspect in medicine. However, their performance in specialized domains\nlike ophthalmology remains underexplored. This study comprehensively evaluated\nand compared the accuracy and reasoning capabilities of four newly developed\nreasoning-focused LLMs, namely DeepSeek-R1, OpenAI o1, o3-mini, and Gemini 2.0\nFlash-Thinking. Each model was assessed using 5,888 multiple-choice\nophthalmology exam questions from the MedMCQA dataset in zero-shot setting.\nQuantitative evaluation included accuracy, Macro-F1, and five text-generation\nmetrics (ROUGE-L, METEOR, BERTScore, BARTScore, and AlignScore), computed\nagainst ground-truth reasonings. Average inference time was recorded for a\nsubset of 100 randomly selected questions. Additionally, two board-certified\nophthalmologists qualitatively assessed clarity, completeness, and reasoning\nstructure of responses to differential diagnosis questions.O1 (0.902) and\nDeepSeek-R1 (0.888) achieved the highest accuracy, with o1 also leading in\nMacro-F1 (0.900). The performance of models across the text-generation metrics\nvaried: O3-mini excelled in ROUGE-L (0.151), o1 in METEOR (0.232), DeepSeek-R1\nand o3-mini tied for BERTScore (0.673), DeepSeek-R1 (-4.105) and Gemini 2.0\nFlash-Thinking (-4.127) performed best in BARTScore, while o3-mini (0.181) and\no1 (0.176) led AlignScore. Inference time across the models varied, with\nDeepSeek-R1 being slowest (40.4 seconds) and Gemini 2.0 Flash-Thinking fastest\n(6.7 seconds). Qualitative evaluation revealed that DeepSeek-R1 and Gemini 2.0\nFlash-Thinking tended to provide detailed and comprehensive intermediate\nreasoning, whereas o1 and o3-mini displayed concise and summarized\njustifications.", "published": "2025-04-15 13:42:34", "link": "http://arxiv.org/abs/2504.11186v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bias Beyond English: Evaluating Social Bias and Debiasing Methods in a Low-Resource Setting", "abstract": "Social bias in language models can potentially exacerbate social\ninequalities. Despite it having garnered wide attention, most research focuses\non English data. In a low-resource scenario, the models often perform worse due\nto insufficient training data. This study aims to leverage high-resource\nlanguage corpora to evaluate bias and experiment with debiasing methods in\nlow-resource languages. We evaluated the performance of recent multilingual\nmodels in five languages: English (\\textsc{eng}), Chinese (\\textsc{zho}),\nRussian (\\textsc{rus}), Indonesian (\\textsc{ind}) and Thai (\\textsc{tha}), and\nanalyzed four bias dimensions: \\textit{gender}, \\textit{religion},\n\\textit{nationality}, and \\textit{race-color}. By constructing multilingual\nbias evaluation datasets, this study allows fair comparisons between models\nacross languages. We have further investigated three debiasing\nmethods-\\texttt{CDA}, \\texttt{Dropout}, \\texttt{SenDeb}-and demonstrated that\ndebiasing methods from high-resource languages can be effectively transferred\nto low-resource ones, providing actionable insights for fairness research in\nmultilingual NLP.", "published": "2025-04-15 13:40:22", "link": "http://arxiv.org/abs/2504.11183v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media Videos", "abstract": "Sexism is generally defined as prejudice and discrimination based on sex or\ngender, affecting every sector of society, from social institutions to\nrelationships and individual behavior. Social media platforms amplify the\nimpact of sexism by conveying discriminatory content not only through text but\nalso across multiple modalities, highlighting the critical need for a\nmultimodal approach to the analysis of sexism online. With the rise of social\nmedia platforms where users share short videos, sexism is increasingly\nspreading through video content. Automatically detecting sexism in videos is a\nchallenging task, as it requires analyzing the combination of verbal, audio,\nand visual elements to identify sexist content. In this study, (1) we introduce\nMuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of\n$\\approx$ 11 hours of videos extracted from TikTok and BitChute; (2) we propose\nan innovative annotation framework for analyzing the contribution of textual\nand multimodal labels in the classification of sexist and non-sexist content;\nand (3) we evaluate a range of large language models (LLMs) and multimodal LLMs\non the task of sexism detection. We find that visual information plays a key\nrole in labeling sexist content for both humans and models. Models effectively\ndetect explicit sexism; however, they struggle with implicit cases, such as\nstereotypes, instances where annotators also show low agreement. This\nhighlights the inherent difficulty of the task, as identifying implicit sexism\ndepends on the social and cultural context.", "published": "2025-04-15 13:16:46", "link": "http://arxiv.org/abs/2504.11169v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Benchmarking Vision Language Models on German Factual Data", "abstract": "Similar to LLMs, the development of vision language models is mainly driven\nby English datasets and models trained in English and Chinese language, whereas\nsupport for other languages, even those considered high-resource languages such\nas German, remains significantly weaker. In this work we present an analysis of\nopen-weight VLMs on factual knowledge in the German and English language. We\ndisentangle the image-related aspects from the textual ones by analyzing\naccu-racy with jury-as-a-judge in both prompt languages and images from German\nand international contexts. We found that for celebrities and sights, VLMs\nstruggle because they are lacking visual cognition of German image contents.\nFor animals and plants, the tested models can often correctly identify the\nimage contents ac-cording to the scientific name or English common name but\nfail in German lan-guage. Cars and supermarket products were identified equally\nwell in English and German images across both prompt languages.", "published": "2025-04-15 11:55:24", "link": "http://arxiv.org/abs/2504.11108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using LLMs as prompt modifier to avoid biases in AI image generators", "abstract": "This study examines how Large Language Models (LLMs) can reduce biases in\ntext-to-image generation systems by modifying user prompts. We define bias as a\nmodel's unfair deviation from population statistics given neutral prompts. Our\nexperiments with Stable Diffusion XL, 3.5 and Flux demonstrate that\nLLM-modified prompts significantly increase image diversity and reduce bias\nwithout the need to change the image generators themselves. While occasionally\nproducing results that diverge from original user intent for elaborate prompts,\nthis approach generally provides more varied interpretations of underspecified\nrequests rather than superficial variations. The method works particularly well\nfor less advanced image generators, though limitations persist for certain\ncontexts like disability representation. All prompts and generated images are\navailable at https://iisys-hof.github.io/llm-prompt-img-gen/", "published": "2025-04-15 11:52:20", "link": "http://arxiv.org/abs/2504.11104v1", "categories": ["cs.CL", "cs.CV", "cs.CY"], "primary_category": "cs.CL"}
{"title": "DeepMLF: Multimodal language model with learnable tokens for deep fusion in sentiment analysis", "abstract": "While multimodal fusion has been extensively studied in Multimodal Sentiment\nAnalysis (MSA), the role of fusion depth and multimodal capacity allocation\nremains underexplored. In this work, we position fusion depth, scalability, and\ndedicated multimodal capacity as primary factors for effective fusion. We\nintroduce DeepMLF, a novel multimodal language model (LM) with learnable tokens\ntailored toward deep fusion. DeepMLF leverages an audiovisual encoder and a\npretrained decoder LM augmented with multimodal information across its layers.\nWe append learnable tokens to the LM that: 1) capture modality interactions in\na controlled fashion and 2) preserve independent information flow for each\nmodality. These fusion tokens gather linguistic information via causal\nself-attention in LM Blocks and integrate with audiovisual information through\ncross-attention MM Blocks. Serving as dedicated multimodal capacity, this\ndesign enables progressive fusion across multiple layers, providing depth in\nthe fusion process. Our training recipe combines modality-specific losses and\nlanguage modelling loss, with the decoder LM tasked to predict ground truth\npolarity. Across three MSA benchmarks with varying dataset characteristics,\nDeepMLF achieves state-of-the-art performance. Our results confirm that deeper\nfusion leads to better performance, with optimal fusion depths (5-7) exceeding\nthose of existing approaches. Additionally, our analysis on the number of\nfusion tokens reveals that small token sets ($\\sim$20) achieve optimal\nperformance. We examine the importance of representation learning order (fusion\ncurriculum) through audiovisual encoder initialization experiments. Our\nablation studies demonstrate the superiority of the proposed fusion design and\ngating while providing a holistic examination of DeepMLF's scalability to LLMs,\nand the impact of each training objective and embedding regularization.", "published": "2025-04-15 11:28:02", "link": "http://arxiv.org/abs/2504.11082v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews", "abstract": "Peer review is a cornerstone of quality control in scientific publishing.\nWith the increasing workload, the unintended use of `quick' heuristics,\nreferred to as lazy thinking, has emerged as a recurring issue compromising\nreview quality. Automated methods to detect such heuristics can help improve\nthe peer-reviewing process. However, there is limited NLP research on this\nissue, and no real-world dataset exists to support the development of detection\ntools. This work introduces LazyReview, a dataset of peer-review sentences\nannotated with fine-grained lazy thinking categories. Our analysis reveals that\nLarge Language Models (LLMs) struggle to detect these instances in a zero-shot\nsetting. However, instruction-based fine-tuning on our dataset significantly\nboosts performance by 10-20 performance points, highlighting the importance of\nhigh-quality training data. Furthermore, a controlled experiment demonstrates\nthat reviews revised with lazy thinking feedback are more comprehensive and\nactionable than those written without such feedback. We will release our\ndataset and the enhanced guidelines that can be used to train junior reviewers\nin the community. (Code available here:\nhttps://github.com/UKPLab/arxiv2025-lazy-review)", "published": "2025-04-15 10:07:33", "link": "http://arxiv.org/abs/2504.11042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Compressing Prompts for Efficient Inference of Large Language Models", "abstract": "Large Language Models (LLMs) have shown outstanding performance across a\nvariety of tasks, partly due to advanced prompting techniques. However, these\ntechniques often require lengthy prompts, which increase computational costs\nand can hinder performance because of the limited context windows of LLMs.\nWhile prompt compression is a straightforward solution, existing methods\nconfront the challenges of retaining essential information, adapting to context\nchanges, and remaining effective across different tasks. To tackle these\nissues, we propose a task-agnostic method called Dynamic Compressing Prompts\n(LLM-DCP). Our method reduces the number of prompt tokens while aiming to\npreserve the performance as much as possible. We model prompt compression as a\nMarkov Decision Process (MDP), enabling the DCP-Agent to sequentially remove\nredundant tokens by adapting to dynamic contexts and retaining crucial content.\nWe develop a reward function for training the DCP-Agent that balances the\ncompression rate, the quality of the LLM output, and the retention of key\ninformation. This allows for prompt token reduction without needing an external\nblack-box LLM. Inspired by the progressive difficulty adjustment in curriculum\nlearning, we introduce a Hierarchical Prompt Compression (HPC) training\nstrategy that gradually increases the compression difficulty, enabling the\nDCP-Agent to learn an effective compression method that maintains information\nintegrity. Experiments demonstrate that our method outperforms state-of-the-art\ntechniques, especially at higher compression rates. The code for our approach\nwill be available at https://github.com/Fhujinwu/DCP.", "published": "2025-04-15 09:20:45", "link": "http://arxiv.org/abs/2504.11004v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ReZero: Enhancing LLM search ability by trying one-more-time", "abstract": "Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM)\nperformance on knowledge-intensive tasks but depends heavily on initial search\nquery quality. Current methods, often using Reinforcement Learning (RL),\ntypically focus on query formulation or reasoning over results, without\nexplicitly encouraging persistence after a failed search. We introduce ReZero\n(Retry-Zero), a novel RL framework that directly rewards the act of retrying a\nsearch query following an initial unsuccessful attempt. This incentivizes the\nLLM to explore alternative queries rather than prematurely halting. ReZero\ndemonstrates significant improvement, achieving 46.88% accuracy compared to a\n25% baseline. By rewarding persistence, ReZero enhances LLM robustness in\ncomplex information-seeking scenarios where initial queries may prove\ninsufficient.", "published": "2025-04-15 09:18:21", "link": "http://arxiv.org/abs/2504.11001v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Role of KG-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs", "abstract": "Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages.", "published": "2025-04-15 08:46:39", "link": "http://arxiv.org/abs/2504.10982v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding LLMs' Cross-Lingual Context Retrieval: How Good It Is And Where It Comes From", "abstract": "The ability of cross-lingual context retrieval is a fundamental aspect of\ncross-lingual alignment of large language models (LLMs), where the model\nextracts context information in one language based on requests in another\nlanguage. Despite its importance in real-life applications, this ability has\nnot been adequately investigated for state-of-the-art models. In this paper, we\nevaluate the cross-lingual context retrieval ability of over 40 LLMs across 12\nlanguages to understand the source of this ability, using cross-lingual machine\nreading comprehension (xMRC) as a representative scenario. Our results show\nthat several small, post-trained open LLMs show strong cross-lingual context\nretrieval ability, comparable to closed-source LLMs such as GPT-4o, and their\nestimated oracle performances greatly improve after post-training. Our\ninterpretability analysis shows that the cross-lingual context retrieval\nprocess can be divided into two main phases: question encoding and answer\nretrieval, which are formed in pre-training and post-training, respectively.\nThe phasing stability correlates with xMRC performance, and the xMRC bottleneck\nlies at the last model layers in the second phase, where the effect of\npost-training can be evidently observed. Our results also indicate that\nlarger-scale pretraining cannot improve the xMRC performance. Instead, larger\nLLMs need further multilingual post-training to fully unlock their\ncross-lingual context retrieval potential. Our code and is available at\nhttps://github.com/NJUNLP/Cross-Lingual-Context-Retrieval", "published": "2025-04-15 06:35:27", "link": "http://arxiv.org/abs/2504.10906v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Reasoning Models: A Survey", "abstract": "Reasoning models have demonstrated remarkable progress in solving complex and\nlogic-intensive tasks by generating extended Chain-of-Thoughts (CoTs) prior to\narriving at a final answer. Yet, the emergence of this \"slow-thinking\"\nparadigm, with numerous tokens generated in sequence, inevitably introduces\nsubstantial computational overhead. To this end, it highlights an urgent need\nfor effective acceleration. This survey aims to provide a comprehensive\noverview of recent advances in efficient reasoning. It categorizes existing\nworks into three key directions: (1) shorter - compressing lengthy CoTs into\nconcise yet effective reasoning chains; (2) smaller - developing compact\nlanguage models with strong reasoning capabilities through techniques such as\nknowledge distillation, other model compression techniques, and reinforcement\nlearning; and (3) faster - designing efficient decoding strategies to\naccelerate inference. A curated collection of papers discussed in this survey\nis available in our GitHub repository.", "published": "2025-04-15 06:28:00", "link": "http://arxiv.org/abs/2504.10903v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ARise: Towards Knowledge-Augmented Reasoning via Risk-Adaptive Search", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities and\nare receiving increasing attention to enhance their reasoning through scaling\ntest--time compute. However, their application in open--ended,\nknowledge--intensive, complex reasoning scenarios is still limited.\nReasoning--oriented methods struggle to generalize to open--ended scenarios due\nto implicit assumptions of complete world knowledge. Meanwhile,\nknowledge--augmented reasoning (KAR) methods fail to address two core\nchallenges: 1) error propagation, where errors in early steps cascade through\nthe chain, and 2) verification bottleneck, where the explore--exploit tradeoff\narises in multi--branch decision processes. To overcome these limitations, we\nintroduce ARise, a novel framework that integrates risk assessment of\nintermediate reasoning states with dynamic retrieval--augmented generation\n(RAG) within a Monte Carlo tree search paradigm. This approach enables\neffective construction and optimization of reasoning plans across multiple\nmaintained hypothesis branches. Experimental results show that ARise\nsignificantly outperforms the state--of--the--art KAR methods by up to 23.10%,\nand the latest RAG-equipped large reasoning models by up to 25.37%.", "published": "2025-04-15 06:06:50", "link": "http://arxiv.org/abs/2504.10893v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Exploring Persona-dependent LLM Alignment for the Moral Machine Experiment", "abstract": "Deploying large language models (LLMs) with agency in real-world applications\nraises critical questions about how these models will behave. In particular,\nhow will their decisions align with humans when faced with moral dilemmas? This\nstudy examines the alignment between LLM-driven decisions and human judgment in\nvarious contexts of the moral machine experiment, including personas reflecting\ndifferent sociodemographics. We find that the moral decisions of LLMs vary\nsubstantially by persona, showing greater shifts in moral decisions for\ncritical tasks than humans. Our data also indicate an interesting partisan\nsorting phenomenon, where political persona predominates the direction and\ndegree of LLM decisions. We discuss the ethical implications and risks\nassociated with deploying these models in applications that involve moral\ndecisions.", "published": "2025-04-15 05:29:51", "link": "http://arxiv.org/abs/2504.10886v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Ai2 Scholar QA: Organized Literature Synthesis with Attribution", "abstract": "Retrieval-augmented generation is increasingly effective in answering\nscientific questions from literature, but many state-of-the-art systems are\nexpensive and closed-source. We introduce Ai2 Scholar QA, a free online\nscientific question answering application. To facilitate research, we make our\nentire pipeline public: as a customizable open-source Python package and\ninteractive web app, along with paper indexes accessible through public APIs\nand downloadable datasets. We describe our system in detail and present\nexperiments analyzing its key design decisions. In an evaluation on a recent\nscientific QA benchmark, we find that Ai2 Scholar QA outperforms competing\nsystems.", "published": "2025-04-15 04:48:18", "link": "http://arxiv.org/abs/2504.10861v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Moving Beyond Next-Token Prediction: Transformers are Context-Sensitive Language Generators", "abstract": "Large Language Models (LLMs), powered by Transformers, have demonstrated\nhuman-like intelligence capabilities, yet their underlying mechanisms remain\npoorly understood. This paper presents a novel framework for interpreting LLMs\nas probabilistic left context-sensitive languages (CSLs) generators. We\nhypothesize that Transformers can be effectively decomposed into three\nfundamental components: context windows, attention mechanisms, and\nautoregressive generation frameworks. This decomposition allows for the\ndevelopment of more flexible and interpretable computational models, moving\nbeyond the traditional view of attention and autoregression as inseparable\nprocesses. We argue that next-token predictions can be understood as\nprobabilistic, dynamic approximations of left CSL production rules, providing\nan intuitive explanation for how simple token predictions can yield human-like\nintelligence outputs. Given that all CSLs are left context-sensitive\n(Penttonen, 1974), we conclude that Transformers stochastically approximate\nCSLs, which are widely recognized as models of human-like intelligence. This\ninterpretation bridges the gap between Formal Language Theory and the observed\ngenerative power of Transformers, laying a foundation for future advancements\nin generative AI theory and applications. Our novel perspective on Transformer\narchitectures will foster a deeper understanding of LLMs and their future\npotentials.", "published": "2025-04-15 04:06:27", "link": "http://arxiv.org/abs/2504.10845v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from Multiple Perspectives", "abstract": "Navigating high-stakes dilemmas involving conflicting values is challenging\neven for humans, let alone for AI. Yet prior work in evaluating the reasoning\ncapabilities of large language models (LLMs) in such situations has been\nlimited to everyday scenarios. To close this gap, this work first introduces\nCLASH (Character perspective-based LLM Assessments in Situations with\nHigh-stakes), a meticulously curated dataset consisting of 345 high-impact\ndilemmas along with 3,795 individual perspectives of diverse values. In\nparticular, we design CLASH in a way to support the study of critical aspects\nof value-based decision-making processes which are missing from prior work,\nincluding understanding decision ambivalence and psychological discomfort as\nwell as capturing the temporal shifts of values in characters' perspectives. By\nbenchmarking 10 open and closed frontier models, we uncover several key\nfindings. (1) Even the strongest models, such as GPT-4o and Claude-Sonnet,\nachieve less than 50% accuracy in identifying situations where the decision\nshould be ambivalent, while they perform significantly better in clear-cut\nscenarios. (2) While LLMs reasonably predict psychological discomfort as marked\nby human, they inadequately comprehend perspectives involving value shifts,\nindicating a need for LLMs to reason over complex values. (3) Our experiments\nalso reveal a significant correlation between LLMs' value preferences and their\nsteerability towards a given value. (4) Finally, LLMs exhibit greater\nsteerability when engaged in value reasoning from a third-party perspective,\ncompared to a first-person setup, though certain value pairs benefit uniquely\nfrom the first-person framing.", "published": "2025-04-15 02:54:16", "link": "http://arxiv.org/abs/2504.10823v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CSPLADE: Learned Sparse Retrieval with Causal Language Models", "abstract": "In recent years, dense retrieval has been the focus of information retrieval\n(IR) research. While effective, dense retrieval produces uninterpretable dense\nvectors, and suffers from the drawback of large index size. Learned sparse\nretrieval (LSR) has emerged as promising alternative, achieving competitive\nretrieval performance while also being able to leverage the classical inverted\nindex data structure for efficient retrieval. However, limited works have\nexplored scaling LSR beyond BERT scale. In this work, we identify two\nchallenges in training large language models (LLM) for LSR: (1) training\ninstability during the early stage of contrastive training; (2) suboptimal\nperformance due to pre-trained LLM's unidirectional attention. To address these\nchallenges, we propose two corresponding techniques: (1) a lightweight\nadaptation training phase to eliminate training instability; (2) two model\nvariants to enable bidirectional information. With these techniques, we are\nable to train LSR models with 8B scale LLM, and achieve competitive retrieval\nperformance with reduced index size. Furthermore, we are among the first to\nanalyze the performance-efficiency tradeoff of LLM-based LSR model through the\nlens of model quantization. Our findings provide insights into adapting LLMs\nfor efficient retrieval modeling.", "published": "2025-04-15 02:31:34", "link": "http://arxiv.org/abs/2504.10816v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Name of Thrones: Evaluating How LLMs Rank Student Names, Race, and Gender in Status Hierarchies", "abstract": "Across cultures, names tell a lot about their bearers as they carry deep\npersonal and cultural significance. Names also serve as powerful signals of\ngender, race, and status in the social hierarchy - a pecking order in which\nindividual positions shape others' expectations on their perceived competence\nand worth. With the widespread adoption of LLMs and as names are often an input\nfor LLMs, it is crucial to evaluate whether LLMs may sort people into status\npositions based on first and last names and, if so, whether it is in an unfair,\nbiased fashion. While prior work has primarily investigated biases in first\nnames, little attention has been paid to last names and even less to the\ncombined effects of first and last names. In this study, we conduct a\nlarge-scale analysis of name variations across 5 ethnicities to examine how AI\nexhibits name biases. Our study investigates three key characteristics of\ninequality and finds that LLMs reflect and reinforce status hierarchies based\non names that signal gender and ethnicity as they encode differential\nexpectations of competence, leadership, and economic potential. Contrary to the\ncommon assumption that AI tends to favor Whites, we show that East and, in some\ncontexts, South Asian names receive higher rankings. We also disaggregate\nAsians, a population projected to be the largest immigrant group in the U.S. by\n2055. Our results challenge the monolithic Asian model minority assumption,\nillustrating a more complex and stratified model of bias. Gender moderates\nbiases, with girls facing unfair disadvantages in certain racial groups.\nAdditionally, spanning cultural categories by adopting Western first names\nimproves AI-perceived status for East and Southeast Asian students,\nparticularly for girls. Our findings underscore the importance of\nintersectional and more nuanced understandings of race, gender, and mixed\nidentities in the evaluation of LLMs.", "published": "2025-04-15 01:47:39", "link": "http://arxiv.org/abs/2504.10797v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "H.5; J.4"], "primary_category": "cs.CL"}
{"title": "GUM-SAGE: A Novel Dataset and Approach for Graded Entity Salience Prediction", "abstract": "Determining and ranking the most salient entities in a text is critical for\nuser-facing systems, especially as users increasingly rely on models to\ninterpret long documents they only partially read. Graded entity salience\naddresses this need by assigning entities scores that reflect their relative\nimportance in a text. Existing approaches fall into two main categories:\nsubjective judgments of salience, which allow for gradient scoring but lack\nconsistency, and summarization-based methods, which define salience as\nmention-worthiness in a summary, promoting explainability but limiting outputs\nto binary labels (entities are either summary-worthy or not). In this paper, we\nintroduce a novel approach for graded entity salience that combines the\nstrengths of both approaches. Using an English dataset spanning 12 spoken and\nwritten genres, we collect 5 summaries per document and calculate each entity's\nsalience score based on its presence across these summaries. Our approach shows\nstronger correlation with scores based on human summaries and alignments, and\noutperforms existing techniques, including LLMs. We release our data and code\nat https://github.com/jl908069/gum_sum_salience to support further research on\ngraded salient entity extraction.", "published": "2025-04-15 01:26:14", "link": "http://arxiv.org/abs/2504.10792v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Art of Audience Engagement: LLM-Based Thin-Slicing of Scientific Talks", "abstract": "This paper examines the thin-slicing approach - the ability to make accurate\njudgments based on minimal information - in the context of scientific\npresentations. Drawing on research from nonverbal communication and personality\npsychology, we show that brief excerpts (thin slices) reliably predict overall\npresentation quality. Using a novel corpus of over one hundred real-life\nscience talks, we employ Large Language Models (LLMs) to evaluate transcripts\nof full presentations and their thin slices. By correlating LLM-based\nevaluations of short excerpts with full-talk assessments, we determine how much\ninformation is needed for accurate predictions. Our results demonstrate that\nLLM-based evaluations align closely with human ratings, proving their validity,\nreliability, and efficiency. Critically, even very short excerpts (less than 10\npercent of a talk) strongly predict overall evaluations. This suggests that the\nfirst moments of a presentation convey relevant information that is used in\nquality evaluations and can shape lasting impressions. The findings are robust\nacross different LLMs and prompting strategies. This work extends thin-slicing\nresearch to public speaking and connects theories of impression formation to\nLLMs and current research on AI communication. We discuss implications for\ncommunication and social cognition research on message reception. Lastly, we\nsuggest an LLM-based thin-slicing framework as a scalable feedback tool to\nenhance human communication.", "published": "2025-04-15 00:08:13", "link": "http://arxiv.org/abs/2504.10768v1", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Elucidating the Design Space of Multimodal Protein Language Models", "abstract": "Multimodal protein language models (PLMs) integrate sequence and token-based\nstructural information, serving as a powerful foundation for protein modeling,\ngeneration, and design. However, the reliance on tokenizing 3D structures into\ndiscrete tokens causes substantial loss of fidelity about fine-grained\nstructural details and correlations. In this paper, we systematically elucidate\nthe design space of multimodal PLMs to overcome their limitations. We identify\ntokenization loss and inaccurate structure token predictions by the PLMs as\nmajor bottlenecks. To address these, our proposed design space covers improved\ngenerative modeling, structure-aware architectures and representation learning,\nand data exploration. Our advancements approach finer-grained supervision,\ndemonstrating that token-based multimodal PLMs can achieve robust structural\nmodeling. The effective design methods dramatically improve the structure\ngeneration diversity, and notably, folding abilities of our 650M model by\nreducing the RMSD from 5.52 to 2.36 on PDB testset, even outperforming 3B\nbaselines and on par with the specialized folding models.", "published": "2025-04-15 17:59:43", "link": "http://arxiv.org/abs/2504.11454v1", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "primary_category": "cs.LG"}
{"title": "A Clean Slate for Offline Reinforcement Learning", "abstract": "Progress in offline reinforcement learning (RL) has been impeded by ambiguous\nproblem definitions and entangled algorithmic designs, resulting in\ninconsistent implementations, insufficient ablations, and unfair evaluations.\nAlthough offline RL explicitly avoids environment interaction, prior methods\nfrequently employ extensive, undocumented online evaluation for hyperparameter\ntuning, complicating method comparisons. Moreover, existing reference\nimplementations differ significantly in boilerplate code, obscuring their core\nalgorithmic contributions. We address these challenges by first introducing a\nrigorous taxonomy and a transparent evaluation protocol that explicitly\nquantifies online tuning budgets. To resolve opaque algorithmic design, we\nprovide clean, minimalistic, single-file implementations of various model-free\nand model-based offline RL methods, significantly enhancing clarity and\nachieving substantial speed-ups. Leveraging these streamlined implementations,\nwe propose Unifloral, a unified algorithm that encapsulates diverse prior\napproaches within a single, comprehensive hyperparameter space, enabling\nalgorithm development in a shared hyperparameter space. Using Unifloral with\nour rigorous evaluation protocol, we develop two novel algorithms - TD3-AWR\n(model-free) and MoBRAC (model-based) - which substantially outperform\nestablished baselines. Our implementation is publicly available at\nhttps://github.com/EmptyJackson/unifloral.", "published": "2025-04-15 17:59:05", "link": "http://arxiv.org/abs/2504.11453v1", "categories": ["cs.LG", "cs.AI", "cs.RO"], "primary_category": "cs.LG"}
{"title": "Greedy Restart Schedules: A Baseline for Dynamic Algorithm Selection on Numerical Black-box Optimization Problems", "abstract": "In many optimization domains, there are multiple different solvers that\ncontribute to the overall state-of-the-art, each performing better on some, and\nworse on other types of problem instances. Meta-algorithmic approaches, such as\ninstance-based algorithm selection, configuration and scheduling, aim to close\nthis gap by extracting the most performance possible from a set of\n(configurable) optimizers. In this context, the best performing individual\nalgorithms are often hand-crafted hybrid heuristics which perform many restarts\nof fast local optimization approaches. However, data-driven techniques to\ncreate optimized restart schedules have not yet been extensively studied.\n  Here, we present a simple scheduling approach that iteratively selects the\nalgorithm performing best on the distribution of unsolved training problems at\ntime of selection, resulting in a problem-independent solver schedule. We\ndemonstrate our approach using well-known optimizers from numerical black-box\noptimization on the BBOB testbed, bridging much of the gap between single and\nvirtual best solver from the original portfolio across various evaluation\nprotocols. Our greedy restart schedule presents a powerful baseline for more\ncomplex dynamic algorithm selection models.", "published": "2025-04-15 17:54:21", "link": "http://arxiv.org/abs/2504.11440v1", "categories": ["math.OC", "cs.AI"], "primary_category": "math.OC"}
{"title": "ADT: Tuning Diffusion Models with Adversarial Supervision", "abstract": "Diffusion models have achieved outstanding image generation by reversing a\nforward noising process to approximate true data distributions. During\ntraining, these models predict diffusion scores from noised versions of true\nsamples in a single forward pass, while inference requires iterative denoising\nstarting from white noise. This training-inference divergences hinder the\nalignment between inference and training data distributions, due to potential\nprediction biases and cumulative error accumulation. To address this problem,\nwe propose an intuitive but effective fine-tuning framework, called Adversarial\nDiffusion Tuning (ADT), by stimulating the inference process during\noptimization and aligning the final outputs with training data by adversarial\nsupervision. Specifically, to achieve robust adversarial training, ADT features\na siamese-network discriminator with a fixed pre-trained backbone and\nlightweight trainable parameters, incorporates an image-to-image sampling\nstrategy to smooth discriminative difficulties, and preserves the original\ndiffusion loss to prevent discriminator hacking. In addition, we carefully\nconstrain the backward-flowing path for back-propagating gradients along the\ninference path without incurring memory overload or gradient explosion.\nFinally, extensive experiments on Stable Diffusion models (v1.5, XL, and v3),\ndemonstrate that ADT significantly improves both distribution alignment and\nimage quality.", "published": "2025-04-15 17:37:50", "link": "http://arxiv.org/abs/2504.11423v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Embodied World Models Emerge from Navigational Task in Open-Ended Environments", "abstract": "Understanding how artificial systems can develop spatial awareness and\nreasoning has long been a challenge in AI research. Traditional models often\nrely on passive observation, but embodied cognition theory suggests that deeper\nunderstanding emerges from active interaction with the environment. This study\ninvestigates whether neural networks can autonomously internalize spatial\nconcepts through interaction, focusing on planar navigation tasks. Using Gated\nRecurrent Units (GRUs) combined with Meta-Reinforcement Learning (Meta-RL), we\nshow that agents can learn to encode spatial properties like direction,\ndistance, and obstacle avoidance. We introduce Hybrid Dynamical Systems (HDS)\nto model the agent-environment interaction as a closed dynamical system,\nrevealing stable limit cycles that correspond to optimal navigation strategies.\nRidge Representation allows us to map navigation paths into a fixed-dimensional\nbehavioral space, enabling comparison with neural states. Canonical Correlation\nAnalysis (CCA) confirms strong alignment between these representations,\nsuggesting that the agent's neural states actively encode spatial knowledge.\nIntervention experiments further show that specific neural dimensions are\ncausally linked to navigation performance. This work provides an approach to\nbridging the gap between action and perception in AI, offering new insights\ninto building adaptive, interpretable models that can generalize across complex\nenvironments. The causal validation of neural representations also opens new\navenues for understanding and controlling the internal mechanisms of AI\nsystems, pushing the boundaries of how machines learn and reason in dynamic,\nreal-world scenarios.", "published": "2025-04-15 17:35:13", "link": "http://arxiv.org/abs/2504.11419v1", "categories": ["cs.AI", "cs.NE"], "primary_category": "cs.AI"}
{"title": "Measures of Variability for Risk-averse Policy Gradient", "abstract": "Risk-averse reinforcement learning (RARL) is critical for decision-making\nunder uncertainty, which is especially valuable in high-stake applications.\nHowever, most existing works focus on risk measures, e.g., conditional\nvalue-at-risk (CVaR), while measures of variability remain underexplored. In\nthis paper, we comprehensively study nine common measures of variability,\nnamely Variance, Gini Deviation, Mean Deviation, Mean-Median Deviation,\nStandard Deviation, Inter-Quantile Range, CVaR Deviation, Semi_Variance, and\nSemi_Standard Deviation. Among them, four metrics have not been previously\nstudied in RARL. We derive policy gradient formulas for these unstudied\nmetrics, improve gradient estimation for Gini Deviation, analyze their gradient\nproperties, and incorporate them with the REINFORCE and PPO frameworks to\npenalize the dispersion of returns.\n  Our empirical study reveals that variance-based metrics lead to unstable\npolicy updates. In contrast, CVaR Deviation and Gini Deviation show consistent\nperformance across different randomness and evaluation domains, achieving high\nreturns while effectively learning risk-averse policies. Mean Deviation and\nSemi_Standard Deviation are also competitive across different scenarios. This\nwork provides a comprehensive overview of variability measures in RARL,\noffering practical insights for risk-aware decision-making and guiding future\nresearch on risk metrics and RARL algorithms.", "published": "2025-04-15 17:28:15", "link": "http://arxiv.org/abs/2504.11412v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Multi-level Cellular Automata for FLIM networks", "abstract": "The necessity of abundant annotated data and complex network architectures\npresents a significant challenge in deep-learning Salient Object Detection\n(deep SOD) and across the broader deep-learning landscape. This challenge is\nparticularly acute in medical applications in developing countries with limited\ncomputational resources. Combining modern and classical techniques offers a\npath to maintaining competitive performance while enabling practical\napplications. Feature Learning from Image Markers (FLIM) methodology empowers\nexperts to design convolutional encoders through user-drawn markers, with\nfilters learned directly from these annotations. Recent findings demonstrate\nthat coupling a FLIM encoder with an adaptive decoder creates a flyweight\nnetwork suitable for SOD, requiring significantly fewer parameters than\nlightweight models and eliminating the need for backpropagation. Cellular\nAutomata (CA) methods have proven successful in data-scarce scenarios but\nrequire proper initialization -- typically through user input, priors, or\nrandomness. We propose a practical intersection of these approaches: using FLIM\nnetworks to initialize CA states with expert knowledge without requiring user\ninteraction for each image. By decoding features from each level of a FLIM\nnetwork, we can initialize multiple CAs simultaneously, creating a multi-level\nframework. Our method leverages the hierarchical knowledge encoded across\ndifferent network layers, merging multiple saliency maps into a high-quality\nfinal output that functions as a CA ensemble. Benchmarks across two challenging\nmedical datasets demonstrate the competitiveness of our multi-level CA approach\ncompared to established models in the deep SOD literature.", "published": "2025-04-15 17:22:24", "link": "http://arxiv.org/abs/2504.11406v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "VideoPanda: Video Panoramic Diffusion with Multi-view Attention", "abstract": "High resolution panoramic video content is paramount for immersive\nexperiences in Virtual Reality, but is non-trivial to collect as it requires\nspecialized equipment and intricate camera setups. In this work, we introduce\nVideoPanda, a novel approach for synthesizing 360$^\\circ$ videos conditioned on\ntext or single-view video data. VideoPanda leverages multi-view attention\nlayers to augment a video diffusion model, enabling it to generate consistent\nmulti-view videos that can be combined into immersive panoramic content.\nVideoPanda is trained jointly using two conditions: text-only and single-view\nvideo, and supports autoregressive generation of long-videos. To overcome the\ncomputational burden of multi-view video generation, we randomly subsample the\nduration and camera views used during training and show that the model is able\nto gracefully generalize to generating more frames during inference. Extensive\nevaluations on both real-world and synthetic video datasets demonstrate that\nVideoPanda generates more realistic and coherent 360$^\\circ$ panoramas across\nall input conditions compared to existing methods. Visit the project website at\nhttps://research-staging.nvidia.com/labs/toronto-ai/VideoPanda/ for results.", "published": "2025-04-15 16:58:15", "link": "http://arxiv.org/abs/2504.11389v1", "categories": ["cs.GR", "cs.AI", "cs.CV"], "primary_category": "cs.GR"}
{"title": "Trajectory Encoding Temporal Graph Networks", "abstract": "Temporal Graph Networks (TGNs) have demonstrated significant success in\ndynamic graph tasks such as link prediction and node classification. Both tasks\ncomprise transductive settings, where the model predicts links among known\nnodes, and in inductive settings, where it generalises learned patterns to\npreviously unseen nodes. Existing TGN designs face a dilemma under these dual\nscenarios. Anonymous TGNs, which rely solely on temporal and structural\ninformation, offer strong inductive generalisation but struggle to distinguish\nknown nodes. In contrast, non-anonymous TGNs leverage node features to excel in\ntransductive tasks yet fail to adapt to new nodes. To address this challenge,\nwe propose Trajectory Encoding TGN (TETGN). Our approach introduces\nautomatically expandable node identifiers (IDs) as learnable temporal\npositional features and performs message passing over these IDs to capture each\nnode's historical context. By integrating this trajectory-aware module with a\nstandard TGN using multi-head attention, TETGN effectively balances\ntransductive accuracy with inductive generalisation. Experimental results on\nthree real-world datasets show that TETGN significantly outperforms strong\nbaselines on both link prediction and node classification tasks, demonstrating\nits ability to unify the advantages of anonymous and non-anonymous models for\ndynamic graph learning.", "published": "2025-04-15 16:57:09", "link": "http://arxiv.org/abs/2504.11386v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "A Winner-Takes-All Mechanism for Event Generation", "abstract": "We present a novel framework for central pattern generator design that\nleverages the intrinsic rebound excitability of neurons in combination with\nwinner-takes-all computation. Our approach unifies decision-making and rhythmic\npattern generation within a simple yet powerful network architecture that\nemploys all-to-all inhibitory connections enhanced by designable excitatory\ninteractions. This design offers significant advantages regarding ease of\nimplementation, adaptability, and robustness. We demonstrate its efficacy\nthrough a ring oscillator model, which exhibits adaptive phase and frequency\nmodulation, making the framework particularly promising for applications in\nneuromorphic systems and robotics.", "published": "2025-04-15 16:40:37", "link": "http://arxiv.org/abs/2504.11374v1", "categories": ["eess.SY", "cs.AI", "cs.SY"], "primary_category": "eess.SY"}
{"title": "DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks", "abstract": "LLM-integrated applications and agents are vulnerable to prompt injection\nattacks, where an attacker injects prompts into their inputs to induce\nattacker-desired outputs. A detection method aims to determine whether a given\ninput is contaminated by an injected prompt. However, existing detection\nmethods have limited effectiveness against state-of-the-art attacks, let alone\nadaptive ones. In this work, we propose DataSentinel, a game-theoretic method\nto detect prompt injection attacks. Specifically, DataSentinel fine-tunes an\nLLM to detect inputs contaminated with injected prompts that are strategically\nadapted to evade detection. We formulate this as a minimax optimization\nproblem, with the objective of fine-tuning the LLM to detect strong adaptive\nattacks. Furthermore, we propose a gradient-based method to solve the minimax\noptimization problem by alternating between the inner max and outer min\nproblems. Our evaluation results on multiple benchmark datasets and LLMs show\nthat DataSentinel effectively detects both existing and adaptive prompt\ninjection attacks.", "published": "2025-04-15 16:26:21", "link": "http://arxiv.org/abs/2504.11358v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "Neural Networks for on-chip Model Predictive Control: a Method to Build Optimized Training Datasets and its application to Type-1 Diabetes", "abstract": "Training Neural Networks (NNs) to behave as Model Predictive Control (MPC)\nalgorithms is an effective way to implement them in constrained embedded\ndevices. By collecting large amounts of input-output data, where inputs\nrepresent system states and outputs are MPC-generated control actions, NNs can\nbe trained to replicate MPC behavior at a fraction of the computational cost.\nHowever, although the composition of the training data critically influences\nthe final NN accuracy, methods for systematically optimizing it remain\nunderexplored. In this paper, we introduce the concept of Optimally-Sampled\nDatasets (OSDs) as ideal training sets and present an efficient algorithm for\ngenerating them. An OSD is a parametrized subset of all the available data that\n(i) preserves existing MPC information up to a certain numerical resolution,\n(ii) avoids duplicate or near-duplicate states, and (iii) becomes saturated or\ncomplete. We demonstrate the effectiveness of OSDs by training NNs to replicate\nthe University of Virginia's MPC algorithm for automated insulin delivery in\nType-1 Diabetes, achieving a four-fold improvement in final accuracy. Notably,\ntwo OSD-trained NNs received regulatory clearance for clinical testing as the\nfirst NN-based control algorithm for direct human insulin dosing. This\nmethodology opens new pathways for implementing advanced optimizations on\nresource-constrained embedded platforms, potentially revolutionizing how\ncomplex algorithms are deployed.", "published": "2025-04-15 16:25:06", "link": "http://arxiv.org/abs/2504.11355v1", "categories": ["eess.SY", "cs.AI", "cs.SY"], "primary_category": "eess.SY"}
{"title": "Kimina-Prover Preview: Towards Large Formal Reasoning Models with Reinforcement Learning", "abstract": "We introduce Kimina-Prover Preview, a large language model that pioneers a\nnovel reasoning-driven exploration paradigm for formal theorem proving, as\nshowcased in this preview release. Trained with a large-scale reinforcement\nlearning pipeline from Qwen2.5-72B, Kimina-Prover demonstrates strong\nperformance in Lean 4 proof generation by employing a structured reasoning\npattern we term \\textit{formal reasoning pattern}. This approach allows the\nmodel to emulate human problem-solving strategies in Lean, iteratively\ngenerating and refining proof steps. Kimina-Prover sets a new state-of-the-art\non the miniF2F benchmark, reaching 80.7% with pass@8192. Beyond improved\nbenchmark performance, our work yields several key insights: (1) Kimina-Prover\nexhibits high sample efficiency, delivering strong results even with minimal\nsampling (pass@1) and scaling effectively with computational budget, stemming\nfrom its unique reasoning pattern and RL training; (2) we demonstrate clear\nperformance scaling with model size, a trend previously unobserved for neural\ntheorem provers in formal mathematics; (3) the learned reasoning style,\ndistinct from traditional search algorithms, shows potential to bridge the gap\nbetween formal verification and informal mathematical intuition. We open source\ndistilled versions with 1.5B and 7B parameters of Kimina-Prover", "published": "2025-04-15 16:23:44", "link": "http://arxiv.org/abs/2504.11354v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Explicit and Implicit Representations in AI-based 3D Reconstruction for Radiology: A systematic literature review", "abstract": "The demand for high-quality medical imaging in clinical practice and assisted\ndiagnosis has made 3D reconstruction in radiological imaging a key research\nfocus. Artificial intelligence (AI) has emerged as a promising approach to\nenhancing reconstruction accuracy while reducing acquisition and processing\ntime, thereby minimizing patient radiation exposure and discomfort and\nultimately benefiting clinical diagnosis. This review explores state-of-the-art\nAI-based 3D reconstruction algorithms in radiological imaging, categorizing\nthem into explicit and implicit approaches based on their underlying\nprinciples. Explicit methods include point-based, volume-based, and Gaussian\nrepresentations, while implicit methods encompass implicit prior embedding and\nneural radiance fields. Additionally, we examine commonly used evaluation\nmetrics and benchmark datasets. Finally, we discuss the current state of\ndevelopment, key challenges, and future research directions in this evolving\nfield. Our project available on: https://github.com/Bean-Young/AI4Med.", "published": "2025-04-15 16:21:47", "link": "http://arxiv.org/abs/2504.11349v1", "categories": ["cs.CV", "cs.AI", "cs.GR", "68T45", "I.4.5"], "primary_category": "cs.CV"}
{"title": "Interpretable Hybrid-Rule Temporal Point Processes", "abstract": "Temporal Point Processes (TPPs) are widely used for modeling event sequences\nin various medical domains, such as disease onset prediction, progression\nanalysis, and clinical decision support. Although TPPs effectively capture\ntemporal dynamics, their lack of interpretability remains a critical challenge.\nRecent advancements have introduced interpretable TPPs. However, these methods\nfail to incorporate numerical features, thereby limiting their ability to\ngenerate precise predictions. To address this issue, we propose Hybrid-Rule\nTemporal Point Processes (HRTPP), a novel framework that integrates temporal\nlogic rules with numerical features, improving both interpretability and\npredictive accuracy in event modeling. HRTPP comprises three key components:\nbasic intensity for intrinsic event likelihood, rule-based intensity for\nstructured temporal dependencies, and numerical feature intensity for dynamic\nprobability modulation. To effectively discover valid rules, we introduce a\ntwo-phase rule mining strategy with Bayesian optimization. To evaluate our\nmethod, we establish a multi-criteria assessment framework, incorporating rule\nvalidity, model fitting, and temporal predictive accuracy. Experimental results\non real-world medical datasets demonstrate that HRTPP outperforms\nstate-of-the-art interpretable TPPs in terms of predictive performance and\nclinical interpretability. In case studies, the rules extracted by HRTPP\nexplain the disease progression, offering valuable contributions to medical\ndiagnosis.", "published": "2025-04-15 16:15:16", "link": "http://arxiv.org/abs/2504.11344v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Transformer-Based Model for Cold Start Mitigation in FaaS Architecture", "abstract": "Serverless architectures, particularly the Function as a Service (FaaS)\nmodel, have become a cornerstone of modern cloud computing due to their ability\nto simplify resource management and enhance application deployment agility.\nHowever, a significant challenge remains: the cold start problem. This\nphenomenon occurs when an idle FaaS function is invoked, requiring a full\ninitialization process, which increases latency and degrades user experience.\nExisting solutions for cold start mitigation are limited in terms of invocation\npattern generalization and implementation complexity. In this study, we propose\nan innovative approach leveraging Transformer models to mitigate the impact of\ncold starts in FaaS architectures. Our solution excels in accurately modeling\nfunction initialization delays and optimizing serverless system performance.\nExperimental evaluation using a public dataset provided by Azure demonstrates a\nsignificant reduction in cold start times, reaching up to 79\\% compared to\nconventional methods.", "published": "2025-04-15 16:12:07", "link": "http://arxiv.org/abs/2504.11338v1", "categories": ["cs.DC", "cs.AI"], "primary_category": "cs.DC"}
{"title": "Code Reborn AI-Driven Legacy Systems Modernization from COBOL to Java", "abstract": "This study investigates AI-driven modernization of legacy COBOL code into\nJava, addressing a critical challenge in aging software systems. Leveraging the\nLegacy COBOL 2024 Corpus -- 50,000 COBOL files from public and enterprise\nsources -- Java parses the code, AI suggests upgrades, and React visualizes\ngains. Achieving 93% accuracy, complexity drops 35% (from 18 to 11.7) and\ncoupling 33% (from 8 to 5.4), surpassing manual efforts (75%) and rule-based\ntools (82%). The approach offers a scalable path to rejuvenate COBOL systems,\nvital for industries like banking and insurance.", "published": "2025-04-15 16:07:54", "link": "http://arxiv.org/abs/2504.11335v1", "categories": ["cs.SE", "cs.AI", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints", "abstract": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints.", "published": "2025-04-15 16:00:21", "link": "http://arxiv.org/abs/2504.11320v1", "categories": ["cs.LG", "cs.AI", "cs.DC", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "CFIS-YOLO: A Lightweight Multi-Scale Fusion Network for Edge-Deployable Wood Defect Detection", "abstract": "Wood defect detection is critical for ensuring quality control in the wood\nprocessing industry. However, current industrial applications face two major\nchallenges: traditional methods are costly, subjective, and labor-intensive,\nwhile mainstream deep learning models often struggle to balance detection\naccuracy and computational efficiency for edge deployment. To address these\nissues, this study proposes CFIS-YOLO, a lightweight object detection model\noptimized for edge devices. The model introduces an enhanced C2f structure, a\ndynamic feature recombination module, and a novel loss function that\nincorporates auxiliary bounding boxes and angular constraints. These\ninnovations improve multi-scale feature fusion and small object localization\nwhile significantly reducing computational overhead. Evaluated on a public wood\ndefect dataset, CFIS-YOLO achieves a mean Average Precision (mAP@0.5) of\n77.5\\%, outperforming the baseline YOLOv10s by 4 percentage points. On SOPHON\nBM1684X edge devices, CFIS-YOLO delivers 135 FPS, reduces power consumption to\n17.3\\% of the original implementation, and incurs only a 0.5 percentage point\ndrop in mAP. These results demonstrate that CFIS-YOLO is a practical and\neffective solution for real-world wood defect detection in resource-constrained\nenvironments.", "published": "2025-04-15 15:45:59", "link": "http://arxiv.org/abs/2504.11305v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Learning to Be A Doctor: Searching for Effective Medical Agent Architectures", "abstract": "Large Language Model (LLM)-based agents have demonstrated strong capabilities\nacross a wide range of tasks, and their application in the medical domain holds\nparticular promise due to the demand for high generalizability and reliance on\ninterdisciplinary knowledge. However, existing medical agent systems often rely\non static, manually crafted workflows that lack the flexibility to accommodate\ndiverse diagnostic requirements and adapt to emerging clinical scenarios.\nMotivated by the success of automated machine learning (AutoML), this paper\nintroduces a novel framework for the automated design of medical agent\narchitectures. Specifically, we define a hierarchical and expressive agent\nsearch space that enables dynamic workflow adaptation through structured\nmodifications at the node, structural, and framework levels. Our framework\nconceptualizes medical agents as graph-based architectures composed of diverse,\nfunctional node types and supports iterative self-improvement guided by\ndiagnostic feedback. Experimental results on skin disease diagnosis tasks\ndemonstrate that the proposed method effectively evolves workflow structures\nand significantly enhances diagnostic accuracy over time. This work represents\nthe first fully automated framework for medical agent architecture design and\noffers a scalable, adaptable foundation for deploying intelligent agents in\nreal-world clinical environments.", "published": "2025-04-15 15:44:21", "link": "http://arxiv.org/abs/2504.11301v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Bipartite Ranking From Multiple Labels: On Loss Versus Label Aggregation", "abstract": "Bipartite ranking is a fundamental supervised learning problem, with the goal\nof learning a ranking over instances with maximal area under the ROC curve\n(AUC) against a single binary target label. However, one may often observe\nmultiple binary target labels, e.g., from distinct human annotators. How can\none synthesize such labels into a single coherent ranking? In this work, we\nformally analyze two approaches to this problem -- loss aggregation and label\naggregation -- by characterizing their Bayes-optimal solutions. Based on this,\nwe show that while both methods can yield Pareto-optimal solutions, loss\naggregation can exhibit label dictatorship: one can inadvertently (and\nundesirably) favor one label over others. This suggests that label aggregation\ncan be preferable to loss aggregation, which we empirically verify.", "published": "2025-04-15 15:25:27", "link": "http://arxiv.org/abs/2504.11284v1", "categories": ["cs.LG", "cs.AI", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Single-Input Multi-Output Model Merging: Leveraging Foundation Models for Dense Multi-Task Learning", "abstract": "Model merging is a flexible and computationally tractable approach to merge\nsingle-task checkpoints into a multi-task model. Prior work has solely focused\non constrained multi-task settings where there is a one-to-one mapping between\na sample and a task, overlooking the paradigm where multiple tasks may operate\non the same sample, e.g., scene understanding. In this paper, we focus on the\nmulti-task setting with single-input-multiple-outputs (SIMO) and show that it\nqualitatively differs from the single-input-single-output model merging\nsettings studied in the literature due to the existence of task-specific\ndecoders and diverse loss objectives. We identify that existing model merging\nmethods lead to significant performance degradation, primarily due to\nrepresentation misalignment between the merged encoder and task-specific\ndecoders. We propose two simple and efficient fixes for the SIMO setting to\nre-align the feature representation after merging. Compared to joint\nfine-tuning, our approach is computationally effective and flexible, and sheds\nlight into identifying task relationships in an offline manner. Experiments on\nNYUv2, Cityscapes, and a subset of the Taskonomy dataset demonstrate: (1) task\narithmetic suffices to enable multi-task capabilities; however, the\nrepresentations generated by the merged encoder has to be re-aligned with the\ntask-specific heads; (2) the proposed architecture rivals traditional\nmulti-task learning in performance but requires fewer samples and training\nsteps by leveraging the existence of task-specific models.", "published": "2025-04-15 15:10:46", "link": "http://arxiv.org/abs/2504.11268v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "DeepSelective: Feature Gating and Representation Matching for Interpretable Clinical Prediction", "abstract": "The rapid accumulation of Electronic Health Records (EHRs) has transformed\nhealthcare by providing valuable data that enhance clinical predictions and\ndiagnoses. While conventional machine learning models have proven effective,\nthey often lack robust representation learning and depend heavily on\nexpert-crafted features. Although deep learning offers powerful solutions, it\nis often criticized for its lack of interpretability. To address these\nchallenges, we propose DeepSelective, a novel end to end deep learning\nframework for predicting patient prognosis using EHR data, with a strong\nemphasis on enhancing model interpretability. DeepSelective combines data\ncompression techniques with an innovative feature selection approach,\nintegrating custom-designed modules that work together to improve both accuracy\nand interpretability. Our experiments demonstrate that DeepSelective not only\nenhances predictive accuracy but also significantly improves interpretability,\nmaking it a valuable tool for clinical decision-making. The source code is\nfreely available at http://www.healthinformaticslab.org/supp/resources.php .", "published": "2025-04-15 15:04:39", "link": "http://arxiv.org/abs/2504.11264v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "A Rollout-Based Algorithm and Reward Function for Efficient Resource Allocation in Business Processes", "abstract": "Resource allocation plays a critical role in minimizing cycle time and\nimproving the efficiency of business processes. Recently, Deep Reinforcement\nLearning (DRL) has emerged as a powerful tool to optimize resource allocation\npolicies in business processes. In the DRL framework, an agent learns a policy\nthrough interaction with the environment, guided solely by reward signals that\nindicate the quality of its decisions. However, existing algorithms are not\nsuitable for dynamic environments such as business processes. Furthermore,\nexisting DRL-based methods rely on engineered reward functions that approximate\nthe desired objective, but a misalignment between reward and objective can lead\nto undesired decisions or suboptimal policies. To address these issues, we\npropose a rollout-based DRL algorithm and a reward function to optimize the\nobjective directly. Our algorithm iteratively improves the policy by evaluating\nexecution trajectories following different actions. Our reward function\ndirectly decomposes the objective function of minimizing the mean cycle time.\nMaximizing our reward function guarantees that the objective function is\nminimized without requiring extensive reward engineering. The results show that\nour method consistently learns the optimal policy in all six evaluated business\nprocesses, outperforming the state-of-the-art algorithm that can only learn the\noptimal policy in two of the evaluated processes.", "published": "2025-04-15 14:46:58", "link": "http://arxiv.org/abs/2504.11250v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Respiratory Inhaler Sound Event Classification Using Self-Supervised Learning", "abstract": "Asthma is a chronic respiratory condition that affects millions of people\nworldwide. While this condition can be managed by administering controller\nmedications through handheld inhalers, clinical studies have shown low\nadherence to the correct inhaler usage technique. Consequently, many patients\nmay not receive the full benefit of their medication. Automated classification\nof inhaler sounds has recently been studied to assess medication adherence.\nHowever, the existing classification models were typically trained using data\nfrom specific inhaler types, and their ability to generalize to sounds from\ndifferent inhalers remains unexplored. In this study, we adapted the wav2vec\n2.0 self-supervised learning model for inhaler sound classification by\npre-training and fine-tuning this model on inhaler sounds. The proposed model\nshows a balanced accuracy of 98% on a dataset collected using a dry powder\ninhaler and smartwatch device. The results also demonstrate that re-finetuning\nthis model on minimal data from a target inhaler is a promising approach to\nadapting a generic inhaler sound classification model to a different inhaler\ndevice and audio capture hardware. This is the first study in the field to\ndemonstrate the potential of smartwatches as assistive technologies for the\npersonalized monitoring of inhaler adherence using machine learning models.", "published": "2025-04-15 14:44:47", "link": "http://arxiv.org/abs/2504.11246v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Influence Maximization in Temporal Social Networks with a Cold-Start Problem: A Supervised Approach", "abstract": "Influence Maximization (IM) in temporal graphs focuses on identifying\ninfluential \"seeds\" that are pivotal for maximizing network expansion. We\nadvocate defining these seeds through Influence Propagation Paths (IPPs), which\nis essential for scaling up the network. Our focus lies in efficiently labeling\nIPPs and accurately predicting these seeds, while addressing the\noften-overlooked cold-start issue prevalent in temporal networks. Our strategy\nintroduces a motif-based labeling method and a tensorized Temporal Graph\nNetwork (TGN) tailored for multi-relational temporal graphs, bolstering\nprediction accuracy and computational efficiency. Moreover, we augment\ncold-start nodes with new neighbors from historical data sharing similar IPPs.\nThe recommendation system within an online team-based gaming environment\npresents subtle impact on the social network, forming multi-relational (i.e.,\nweak and strong) temporal graphs for our empirical IM study. We conduct offline\nexperiments to assess prediction accuracy and model training efficiency,\ncomplemented by online A/B testing to validate practical network growth and the\neffectiveness in addressing the cold-start issue.", "published": "2025-04-15 14:44:30", "link": "http://arxiv.org/abs/2504.11245v1", "categories": ["cs.SI", "cs.AI"], "primary_category": "cs.SI"}
{"title": "Diversity-Driven Learning: Tackling Spurious Correlations and Data Heterogeneity in Federated Models", "abstract": "Federated Learning (FL) enables decentralized training of machine learning\nmodels on distributed data while preserving privacy. However, in real-world FL\nsettings, client data is often non-identically distributed and imbalanced,\nresulting in statistical data heterogeneity which impacts the generalization\ncapabilities of the server's model across clients, slows convergence and\nreduces performance. In this paper, we address this challenge by first\nproposing a characterization of statistical data heterogeneity by means of 6\nmetrics of global and client attribute imbalance, class imbalance, and spurious\ncorrelations. Next, we create and share 7 computer vision datasets for binary\nand multiclass image classification tasks in Federated Learning that cover a\nbroad range of statistical data heterogeneity and hence simulate real-world\nsituations. Finally, we propose FedDiverse, a novel client selection algorithm\nin FL which is designed to manage and leverage data heterogeneity across\nclients by promoting collaboration between clients with complementary data\ndistributions. Experiments on the seven proposed FL datasets demonstrate\nFedDiverse's effectiveness in enhancing the performance and robustness of a\nvariety of FL methods while having low communication and computational\noverhead.", "published": "2025-04-15 14:20:42", "link": "http://arxiv.org/abs/2504.11216v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Mutual Understanding between People and Systems via Neurosymbolic AI and Knowledge Graphs", "abstract": "This chapter investigates the concept of mutual understanding between humans\nand systems, positing that Neuro-symbolic Artificial Intelligence (NeSy AI)\nmethods can significantly enhance this mutual understanding by leveraging\nexplicit symbolic knowledge representations with data-driven learning models.\nWe start by introducing three critical dimensions to characterize mutual\nunderstanding: sharing knowledge, exchanging knowledge, and governing\nknowledge. Sharing knowledge involves aligning the conceptual models of\ndifferent agents to enable a shared understanding of the domain of interest.\nExchanging knowledge relates to ensuring the effective and accurate\ncommunication between agents. Governing knowledge concerns establishing rules\nand processes to regulate the interaction between agents. Then, we present\nseveral different use case scenarios that demonstrate the application of NeSy\nAI and Knowledge Graphs to aid meaningful exchanges between human, artificial,\nand robotic agents. These scenarios highlight both the potential and the\nchallenges of combining top-down symbolic reasoning with bottom-up neural\nlearning, guiding the discussion of the coverage provided by current solutions\nalong the dimensions of sharing, exchanging, and governing knowledge.\nConcurrently, this analysis facilitates the identification of gaps and less\ndeveloped aspects in mutual understanding to address in future research.", "published": "2025-04-15 13:57:09", "link": "http://arxiv.org/abs/2504.11200v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Efficient Distributed Retrieval-Augmented Generation for Enhancing Language Model Performance", "abstract": "Small language models (SLMs) support efficient deployments on\nresource-constrained edge devices, but their limited capacity compromises\ninference performance. Retrieval-augmented generation (RAG) is a promising\nsolution to enhance model performance by integrating external databases,\nwithout requiring intensive on-device model retraining. However, large-scale\npublic databases and user-specific private contextual documents are typically\nlocated on the cloud and the device separately, while existing RAG\nimplementations are primarily centralized. To bridge this gap, we propose\nDRAGON, a distributed RAG framework to enhance on-device SLMs through both\ngeneral and personal knowledge without the risk of leaking document privacy.\nSpecifically, DRAGON decomposes multi-document RAG into multiple parallel token\ngeneration processes performed independently and locally on the cloud and the\ndevice, and employs a newly designed Speculative Aggregation, a dual-side\nspeculative algorithm to avoid frequent output synchronization between the\ncloud and device. A new scheduling algorithm is further introduced to identify\nthe optimal aggregation side based on real-time network conditions. Evaluations\non real-world hardware testbed demonstrate a significant performance\nimprovement of DRAGON-up to 1.9x greater gains over standalone SLM compared to\nthe centralized RAG, substantial reduction in per-token latency, and negligible\nTime to First Token (TTFT) overhead.", "published": "2025-04-15 13:53:08", "link": "http://arxiv.org/abs/2504.11197v1", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Exploring Backdoor Attack and Defense for LLM-empowered Recommendations", "abstract": "The fusion of Large Language Models (LLMs) with recommender systems (RecSys)\nhas dramatically advanced personalized recommendations and drawn extensive\nattention. Despite the impressive progress, the safety of LLM-based RecSys\nagainst backdoor attacks remains largely under-explored. In this paper, we\nraise a new problem: Can a backdoor with a specific trigger be injected into\nLLM-based Recsys, leading to the manipulation of the recommendation responses\nwhen the backdoor trigger is appended to an item's title? To investigate the\nvulnerabilities of LLM-based RecSys under backdoor attacks, we propose a new\nattack framework termed Backdoor Injection Poisoning for RecSys (BadRec).\nBadRec perturbs the items' titles with triggers and employs several fake users\nto interact with these items, effectively poisoning the training set and\ninjecting backdoors into LLM-based RecSys. Comprehensive experiments reveal\nthat poisoning just 1% of the training data with adversarial examples is\nsufficient to successfully implant backdoors, enabling manipulation of\nrecommendations. To further mitigate such a security threat, we propose a\nuniversal defense strategy called Poison Scanner (P-Scanner). Specifically, we\nintroduce an LLM-based poison scanner to detect the poisoned items by\nleveraging the powerful language understanding and rich knowledge of LLMs. A\ntrigger augmentation agent is employed to generate diverse synthetic triggers\nto guide the poison scanner in learning domain-specific knowledge of the\npoisoned item detection task. Extensive experiments on three real-world\ndatasets validate the effectiveness of the proposed P-Scanner.", "published": "2025-04-15 13:37:38", "link": "http://arxiv.org/abs/2504.11182v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "TerraMind: Large-Scale Generative Multimodality for Earth Observation", "abstract": "We present TerraMind, the first any-to-any generative, multimodal foundation\nmodel for Earth observation (EO). Unlike other multimodal models, TerraMind is\npretrained on dual-scale representations combining both token-level and\npixel-level data across modalities. On a token level, TerraMind encodes\nhigh-level contextual information to learn cross-modal relationships, while on\na pixel level, TerraMind leverages fine-grained representations to capture\ncritical spatial nuances. We pretrained TerraMind on nine geospatial modalities\nof a global, large-scale dataset. In this paper, we demonstrate that (i)\nTerraMind's dual-scale early fusion approach unlocks a range of zero-shot and\nfew-shot applications for Earth observation, (ii) TerraMind introduces\n\"Thinking-in-Modalities\" (TiM) -- the capability of generating additional\nartificial data during finetuning and inference to improve the model output --\nand (iii) TerraMind achieves beyond state-of-the-art performance in\ncommunity-standard benchmarks for EO like PANGAEA. The pretraining dataset, the\nmodel weights, and our code is open-sourced under a permissive license.", "published": "2025-04-15 13:17:39", "link": "http://arxiv.org/abs/2504.11171v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails", "abstract": "Large Language Models (LLMs) guardrail systems are designed to protect\nagainst prompt injection and jailbreak attacks. However, they remain vulnerable\nto evasion techniques. We demonstrate two approaches for bypassing LLM prompt\ninjection and jailbreak detection systems via traditional character injection\nmethods and algorithmic Adversarial Machine Learning (AML) evasion techniques.\nThrough testing against six prominent protection systems, including Microsoft's\nAzure Prompt Shield and Meta's Prompt Guard, we show that both methods can be\nused to evade detection while maintaining adversarial utility achieving in some\ninstances up to 100% evasion success. Furthermore, we demonstrate that\nadversaries can enhance Attack Success Rates (ASR) against black-box targets by\nleveraging word importance ranking computed by offline white-box models. Our\nfindings reveal vulnerabilities within current LLM protection mechanisms and\nhighlight the need for more robust guardrail systems.", "published": "2025-04-15 13:16:02", "link": "http://arxiv.org/abs/2504.11168v1", "categories": ["cs.CR", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CR"}
{"title": "DMAGaze: Gaze Estimation Based on Feature Disentanglement and Multi-Scale Attention", "abstract": "Gaze estimation, which predicts gaze direction, commonly faces the challenge\nof interference from complex gaze-irrelevant information in face images. In\nthis work, we propose DMAGaze, a novel gaze estimation framework that exploits\ninformation from facial images in three aspects: gaze-relevant global features\n(disentangled from facial image), local eye features (extracted from cropped\neye patch), and head pose estimation features, to improve overall performance.\nFirstly, we design a new continuous mask-based Disentangler to accurately\ndisentangle gaze-relevant and gaze-irrelevant information in facial images by\nachieving the dual-branch disentanglement goal through separately\nreconstructing the eye and non-eye regions. Furthermore, we introduce a new\ncascaded attention module named Multi-Scale Global Local Attention Module\n(MS-GLAM). Through a customized cascaded attention structure, it effectively\nfocuses on global and local information at multiple scales, further enhancing\nthe information from the Disentangler. Finally, the global gaze-relevant\nfeatures disentangled by the upper face branch, combined with head pose and\nlocal eye features, are passed through the detection head for high-precision\ngaze estimation. Our proposed DMAGaze has been extensively validated on two\nmainstream public datasets, achieving state-of-the-art performance.", "published": "2025-04-15 13:08:43", "link": "http://arxiv.org/abs/2504.11160v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "C-SHAP for time series: An approach to high-level temporal explanations", "abstract": "Time series are ubiquitous in domains such as energy forecasting, healthcare,\nand industry. Using AI systems, some tasks within these domains can be\nefficiently handled. Explainable AI (XAI) aims to increase the reliability of\nAI solutions by explaining model reasoning. For time series, many XAI methods\nprovide point- or sequence-based attribution maps. These methods explain model\nreasoning in terms of low-level patterns. However, they do not capture\nhigh-level patterns that may also influence model reasoning. We propose a\nconcept-based method to provide explanations in terms of these high-level\npatterns. In this paper, we present C-SHAP for time series, an approach which\ndetermines the contribution of concepts to a model outcome. We provide a\ngeneral definition of C-SHAP and present an example implementation using time\nseries decomposition. Additionally, we demonstrate the effectiveness of the\nmethodology through a use case from the energy domain.", "published": "2025-04-15 13:06:32", "link": "http://arxiv.org/abs/2504.11159v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Divergence of Empirical Neural Tangent Kernel in Classification Problems", "abstract": "This paper demonstrates that in classification problems, fully connected\nneural networks (FCNs) and residual neural networks (ResNets) cannot be\napproximated by kernel logistic regression based on the Neural Tangent Kernel\n(NTK) under overtraining (i.e., when training time approaches infinity).\nSpecifically, when using the cross-entropy loss, regardless of how large the\nnetwork width is (as long as it is finite), the empirical NTK diverges from the\nNTK on the training samples as training time increases. To establish this\nresult, we first demonstrate the strictly positive definiteness of the NTKs for\nmulti-layer FCNs and ResNets. Then, we prove that during training, % with the\ncross-entropy loss, the neural network parameters diverge if the smallest\neigenvalue of the empirical NTK matrix (Gram matrix) with respect to training\nsamples is bounded below by a positive constant. This behavior contrasts\nsharply with the lazy training regime commonly observed in regression problems.\nConsequently, using a proof by contradiction, we show that the empirical NTK\ndoes not uniformly converge to the NTK across all times on the training samples\nas the network width increases. We validate our theoretical results through\nexperiments on both synthetic data and the MNIST classification task. This\nfinding implies that NTK theory is not applicable in this context, with\nsignificant theoretical implications for understanding neural networks in\nclassification problems.", "published": "2025-04-15 12:30:21", "link": "http://arxiv.org/abs/2504.11130v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Fine-Tuning Large Language Models on Quantum Optimization Problems for Circuit Generation", "abstract": "Large language models (LLM) have achieved remarkable outcomes in addressing\ncomplex problems, including math, coding, and analyzing large amounts of\nscientific reports. Yet few works have explored the potential of LLM in quantum\ncomputing. The most challenging problem is how to leverage LLMs to\nautomatically generate quantum circuits at a large scale. In this paper, we\naddress such a challenge by fine-tuning LLMs and injecting the domain-specific\nknowledge of quantum computing. In particular, we investigate the mechanisms to\ngenerate training data sets and construct the end-to-end pipeline to fine-tune\npre-trained LLMs that produce parameterized quantum circuits for optimization\nproblems. We have prepared 14,000 quantum circuits covering a substantial part\nof the quantum optimization landscape: 12 optimization problem instances and\ntheir optimized QAOA, VQE, and adaptive VQE circuits. The fine-tuned LLMs can\nconstruct syntactically correct parametrized quantum circuits in the most\nrecent OpenQASM 3.0. We have evaluated the quality of the parameters by\ncomparing them to the optimized expectation values and distributions. Our\nevaluation shows that the fine-tuned LLM outperforms state-of-the-art models\nand that the parameters are better than random. The LLM-generated parametrized\ncircuits and initial parameters can be used as a starting point for further\noptimization, \\emph{e.g.,} templates in quantum machine learning and the\nbenchmark for compilers and hardware.", "published": "2025-04-15 11:56:54", "link": "http://arxiv.org/abs/2504.11109v1", "categories": ["quant-ph", "cs.AI"], "primary_category": "quant-ph"}
{"title": "AI-guided Antibiotic Discovery Pipeline from Target Selection to Compound Identification", "abstract": "Antibiotic resistance presents a growing global health crisis, demanding new\ntherapeutic strategies that target novel bacterial mechanisms. Recent advances\nin protein structure prediction and machine learning-driven molecule generation\noffer a promising opportunity to accelerate drug discovery. However, practical\nguidance on selecting and integrating these models into real-world pipelines\nremains limited. In this study, we develop an end-to-end, artificial\nintelligence-guided antibiotic discovery pipeline that spans target\nidentification to compound realization. We leverage structure-based clustering\nacross predicted proteomes of multiple pathogens to identify conserved,\nessential, and non-human-homologous targets. We then systematically evaluate\nsix leading 3D-structure-aware generative models$\\unicode{x2014}$spanning\ndiffusion, autoregressive, graph neural network, and language model\narchitectures$\\unicode{x2014}$on their usability, chemical validity, and\nbiological relevance. Rigorous post-processing filters and commercial analogue\nsearches reduce over 100 000 generated compounds to a focused, synthesizable\nset. Our results highlight DeepBlock and TamGen as top performers across\ndiverse criteria, while also revealing critical trade-offs between model\ncomplexity, usability, and output quality. This work provides a comparative\nbenchmark and blueprint for deploying artificial intelligence in early-stage\nantibiotic development.", "published": "2025-04-15 11:36:27", "link": "http://arxiv.org/abs/2504.11091v1", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "primary_category": "q-bio.BM"}
{"title": "QAMA: Quantum annealing multi-head attention operator with classical deep learning framework", "abstract": "As large language models scale up, the conventional attention mechanism faces\ncritical challenges of exponential growth in memory consumption and energy\ncosts. Quantum annealing computing, with its inherent advantages in\ncomputational efficiency and low energy consumption, offers an innovative\ndirection for constructing novel deep learning architectures. This study\nproposes the first Quantum Annealing-based Multi-head Attention (QAMA)\nmechanism, achieving seamless compatibility with classical attention\narchitectures through quadratic unconstrained binary optimization (QUBO)\nmodeling of forward propagation and energy-based backpropagation. The method\ninnovatively leverages the quantum bit interaction characteristics of Ising\nmodels to optimize the conventional $O(n^2)$ spatiotemporal complexity into\nlinear resource consumption. Integrated with the optical computing advantages\nof coherent Ising machines (CIM), the system maintains millisecond-level\nreal-time responsiveness while significantly reducing energy consumption. Our\nkey contributions include: Theoretical proofs establish QAMA mathematical\nequivalence to classical attention mechanisms; Dual optimization of multi-head\nspecificity and long-range information capture via QUBO constraints; Explicit\ngradient proofs for the Ising energy equation are utilized to implement\ngradient conduction as the only path in the computational graph as a layer;\nProposed soft selection mechanism overcoming traditional binary attention\nlimitations to approximate continuous weights. Experiments on QBoson CPQC\nquantum computer show QAMA achieves comparable accuracy to classical operators\nwhile reducing inference time to millisecond level and improving solution\nquality. This work pioneers architectural-level integration of quantum\ncomputing and deep learning, applicable to any attention-based model, driving\nparadigm innovation in AI foundational computing.", "published": "2025-04-15 11:29:09", "link": "http://arxiv.org/abs/2504.11083v1", "categories": ["quant-ph", "cs.AI"], "primary_category": "quant-ph"}
{"title": "Emergence of Goal-Directed Behaviors via Active Inference with Self-Prior", "abstract": "Infants often exhibit goal-directed behaviors, such as reaching for a sensory\nstimulus, even when no external reward criterion is provided. These\nintrinsically motivated behaviors facilitate spontaneous exploration and\nlearning of the body and environment during early developmental stages.\nAlthough computational modeling can offer insight into the mechanisms\nunderlying such behaviors, many existing studies on intrinsic motivation focus\nprimarily on how exploration contributes to acquiring external rewards. In this\npaper, we propose a novel density model for an agent's own multimodal sensory\nexperiences, called the \"self-prior,\" and investigate whether it can\nautonomously induce goal-directed behavior. Integrated within an active\ninference framework based on the free energy principle, the self-prior\ngenerates behavioral references purely from an intrinsic process that minimizes\nmismatches between average past sensory experiences and current observations.\nThis mechanism is also analogous to the acquisition and utilization of a body\nschema through continuous interaction with the environment. We examine this\napproach in a simulated environment and confirm that the agent spontaneously\nreaches toward a tactile stimulus. Our study implements intrinsically motivated\nbehavior shaped by the agent's own sensory experiences, demonstrating the\nspontaneous emergence of intentional behavior during early development.", "published": "2025-04-15 11:16:27", "link": "http://arxiv.org/abs/2504.11075v1", "categories": ["cs.AI", "68T05, 68T40, 68T42", "I.2.0; I.2.6; I.2.9"], "primary_category": "cs.AI"}
{"title": "Dynamical errors in machine learning forecasts", "abstract": "In machine learning forecasting, standard error metrics such as mean absolute\nerror (MAE) and mean squared error (MSE) quantify discrepancies between\npredictions and target values. However, these metrics do not directly evaluate\nthe physical and/or dynamical consistency of forecasts, an increasingly\ncritical concern in scientific and engineering applications.\n  Indeed, a fundamental yet often overlooked question is whether machine\nlearning forecasts preserve the dynamical behavior of the underlying system.\nAddressing this issue is essential for assessing the fidelity of machine\nlearning models and identifying potential failure modes, particularly in\napplications where maintaining correct dynamical behavior is crucial.\n  In this work, we investigate the relationship between standard forecasting\nerror metrics, such as MAE and MSE, and the dynamical properties of the\nunderlying system. To achieve this goal, we use two recently developed\ndynamical indices: the instantaneous dimension ($d$), and the inverse\npersistence ($\\theta$). Our results indicate that larger forecast errors --\ne.g., higher MSE -- tend to occur in states with higher $d$ (higher complexity)\nand higher $\\theta$ (lower persistence). To further assess dynamical\nconsistency, we propose error metrics based on the dynamical indices that\nmeasure the discrepancy of the forecasted $d$ and $\\theta$ versus their correct\nvalues. Leveraging these dynamical indices-based metrics, we analyze direct and\nrecursive forecasting strategies for three canonical datasets -- Lorenz,\nKuramoto-Sivashinsky equation, and Kolmogorov flow -- as well as a real-world\nweather forecasting task. Our findings reveal substantial distortions in\ndynamical properties in ML forecasts, especially for long forecast lead times\nor long recursive simulations, providing complementary information on ML\nforecast fidelity that can be used to improve ML models.", "published": "2025-04-15 11:16:13", "link": "http://arxiv.org/abs/2504.11074v1", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "primary_category": "cs.LG"}
{"title": "Neural Control Barrier Functions from Physics Informed Neural Networks", "abstract": "As autonomous systems become increasingly prevalent in daily life, ensuring\ntheir safety is paramount. Control Barrier Functions (CBFs) have emerged as an\neffective tool for guaranteeing safety; however, manually designing them for\nspecific applications remains a significant challenge. With the advent of deep\nlearning techniques, recent research has explored synthesizing CBFs using\nneural networks-commonly referred to as neural CBFs. This paper introduces a\nnovel class of neural CBFs that leverages a physics-inspired neural network\nframework by incorporating Zubov's Partial Differential Equation (PDE) within\nthe context of safety. This approach provides a scalable methodology for\nsynthesizing neural CBFs applicable to high-dimensional systems. Furthermore,\nby utilizing reciprocal CBFs instead of zeroing CBFs, the proposed framework\nallows for the specification of flexible, user-defined safe regions. To\nvalidate the effectiveness of the approach, we present case studies on three\ndifferent systems: an inverted pendulum, autonomous ground navigation, and\naerial navigation in obstacle-laden environments.", "published": "2025-04-15 10:13:30", "link": "http://arxiv.org/abs/2504.11045v1", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "cs.RO"}
{"title": "QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models", "abstract": "In typical multimodal tasks, such as Visual Question Answering (VQA),\nadversarial attacks targeting a specific image and question can lead large\nvision-language models (LVLMs) to provide incorrect answers. However, it is\ncommon for a single image to be associated with multiple questions, and LVLMs\nmay still answer other questions correctly even for an adversarial image\nattacked by a specific question. To address this, we introduce the\nquery-agnostic visual attack (QAVA), which aims to create robust adversarial\nexamples that generate incorrect responses to unspecified and unknown\nquestions. Compared to traditional adversarial attacks focused on specific\nimages and questions, QAVA significantly enhances the effectiveness and\nefficiency of attacks on images when the question is unknown, achieving\nperformance comparable to attacks on known target questions. Our research\nbroadens the scope of visual adversarial attacks on LVLMs in practical\nsettings, uncovering previously overlooked vulnerabilities, particularly in the\ncontext of visual adversarial threats. The code is available at\nhttps://github.com/btzyd/qava.", "published": "2025-04-15 10:00:01", "link": "http://arxiv.org/abs/2504.11038v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "\"Even explanations will not help in trusting [this] fundamentally biased system\": A Predictive Policing Case-Study", "abstract": "In today's society, where Artificial Intelligence (AI) has gained a vital\nrole, concerns regarding user's trust have garnered significant attention. The\nuse of AI systems in high-risk domains have often led users to either\nunder-trust it, potentially causing inadequate reliance or over-trust it,\nresulting in over-compliance. Therefore, users must maintain an appropriate\nlevel of trust. Past research has indicated that explanations provided by AI\nsystems can enhance user understanding of when to trust or not trust the\nsystem. However, the utility of presentation of different explanations forms\nstill remains to be explored especially in high-risk domains. Therefore, this\nstudy explores the impact of different explanation types (text, visual, and\nhybrid) and user expertise (retired police officers and lay users) on\nestablishing appropriate trust in AI-based predictive policing. While we\nobserved that the hybrid form of explanations increased the subjective trust in\nAI for expert users, it did not led to better decision-making. Furthermore, no\nform of explanations helped build appropriate trust. The findings of our study\nemphasize the importance of re-evaluating the use of explanations to build\n[appropriate] trust in AI based systems especially when the system's use is\nquestionable. Finally, we synthesize potential challenges and policy\nrecommendations based on our results to design for appropriate trust in\nhigh-risk based AI-based systems.", "published": "2025-04-15 09:43:48", "link": "http://arxiv.org/abs/2504.11020v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC"}
{"title": "GATE3D: Generalized Attention-based Task-synergized Estimation in 3D*", "abstract": "The emerging trend in computer vision emphasizes developing universal models\ncapable of simultaneously addressing multiple diverse tasks. Such universality\ntypically requires joint training across multi-domain datasets to ensure\neffective generalization. However, monocular 3D object detection presents\nunique challenges in multi-domain training due to the scarcity of datasets\nannotated with accurate 3D ground-truth labels, especially beyond typical\nroad-based autonomous driving contexts. To address this challenge, we introduce\na novel weakly supervised framework leveraging pseudo-labels. Current\npretrained models often struggle to accurately detect pedestrians in non-road\nenvironments due to inherent dataset biases. Unlike generalized image-based 2D\nobject detection models, achieving similar generalization in monocular 3D\ndetection remains largely unexplored. In this paper, we propose GATE3D, a novel\nframework designed specifically for generalized monocular 3D object detection\nvia weak supervision. GATE3D effectively bridges domain gaps by employing\nconsistency losses between 2D and 3D predictions. Remarkably, our model\nachieves competitive performance on the KITTI benchmark as well as on an\nindoor-office dataset collected by us to evaluate the generalization\ncapabilities of our framework. Our results demonstrate that GATE3D\nsignificantly accelerates learning from limited annotated data through\neffective pre-training strategies, highlighting substantial potential for\nbroader impacts in robotics, augmented reality, and virtual reality\napplications. Project page: https://ies0411.github.io/GATE3D/", "published": "2025-04-15 09:37:54", "link": "http://arxiv.org/abs/2504.11014v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Document Quality Scoring for Web Crawling", "abstract": "The internet contains large amounts of low-quality content, yet users expect\nweb search engines to deliver high-quality, relevant results. The abundant\npresence of low-quality pages can negatively impact retrieval and crawling\nprocesses by wasting resources on these documents. Therefore, search engines\ncan greatly benefit from techniques that leverage efficient quality estimation\nmethods to mitigate these negative impacts. Quality scoring methods for web\npages are useful for many processes typical for web search systems, including\nstatic index pruning, index tiering, and crawling. Building on work by Chang et\nal.~\\cite{chang2024neural}, who proposed using neural estimators of semantic\nquality for static index pruning, we extend their approach and apply their\nneural quality scorers to assess the semantic quality of web pages in crawling\nprioritisation tasks. In our experimental analysis, we found that prioritising\nsemantically high-quality pages over low-quality ones can improve downstream\nsearch effectiveness. Our software contribution consists of a Docker container\nthat computes an effective quality score for a given web page, allowing the\nquality scorer to be easily included and used in other components of web search\nsystems.", "published": "2025-04-15 09:32:57", "link": "http://arxiv.org/abs/2504.11011v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "MediSee: Reasoning-based Pixel-level Perception in Medical Images", "abstract": "Despite remarkable advancements in pixel-level medical image perception,\nexisting methods are either limited to specific tasks or heavily rely on\naccurate bounding boxes or text labels as input prompts. However, the medical\nknowledge required for input is a huge obstacle for general public, which\ngreatly reduces the universality of these methods. Compared with these\ndomain-specialized auxiliary information, general users tend to rely on oral\nqueries that require logical reasoning. In this paper, we introduce a novel\nmedical vision task: Medical Reasoning Segmentation and Detection (MedSD),\nwhich aims to comprehend implicit queries about medical images and generate the\ncorresponding segmentation mask and bounding box for the target object. To\naccomplish this task, we first introduce a Multi-perspective, Logic-driven\nMedical Reasoning Segmentation and Detection (MLMR-SD) dataset, which\nencompasses a substantial collection of medical entity targets along with their\ncorresponding reasoning. Furthermore, we propose MediSee, an effective baseline\nmodel designed for medical reasoning segmentation and detection. The\nexperimental results indicate that the proposed method can effectively address\nMedSD with implicit colloquial queries and outperform traditional medical\nreferring segmentation methods.", "published": "2025-04-15 09:28:53", "link": "http://arxiv.org/abs/2504.11008v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "TMCIR: Token Merge Benefits Composed Image Retrieval", "abstract": "Composed Image Retrieval (CIR) retrieves target images using a multi-modal\nquery that combines a reference image with text describing desired\nmodifications. The primary challenge is effectively fusing this visual and\ntextual information. Current cross-modal feature fusion approaches for CIR\nexhibit an inherent bias in intention interpretation. These methods tend to\ndisproportionately emphasize either the reference image features\n(visual-dominant fusion) or the textual modification intent (text-dominant\nfusion through image-to-text conversion). Such an imbalanced representation\noften fails to accurately capture and reflect the actual search intent of the\nuser in the retrieval results. To address this challenge, we propose TMCIR, a\nnovel framework that advances composed image retrieval through two key\ninnovations: 1) Intent-Aware Cross-Modal Alignment. We first fine-tune CLIP\nencoders contrastively using intent-reflecting pseudo-target images,\nsynthesized from reference images and textual descriptions via a diffusion\nmodel. This step enhances the encoder ability of text to capture nuanced\nintents in textual descriptions. 2) Adaptive Token Fusion. We further fine-tune\nall encoders contrastively by comparing adaptive token-fusion features with the\ntarget image. This mechanism dynamically balances visual and textual\nrepresentations within the contrastive learning pipeline, optimizing the\ncomposed feature for retrieval. Extensive experiments on Fashion-IQ and CIRR\ndatasets demonstrate that TMCIR significantly outperforms state-of-the-art\nmethods, particularly in capturing nuanced user intent.", "published": "2025-04-15 09:14:04", "link": "http://arxiv.org/abs/2504.10995v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "ProtFlow: Fast Protein Sequence Design via Flow Matching on Compressed Protein Language Model Embeddings", "abstract": "The design of protein sequences with desired functionalities is a fundamental\ntask in protein engineering. Deep generative methods, such as autoregressive\nmodels and diffusion models, have greatly accelerated the discovery of novel\nprotein sequences. However, these methods mainly focus on local or shallow\nresidual semantics and suffer from low inference efficiency, large modeling\nspace and high training cost. To address these challenges, we introduce\nProtFlow, a fast flow matching-based protein sequence design framework that\noperates on embeddings derived from semantically meaningful latent space of\nprotein language models. By compressing and smoothing the latent space,\nProtFlow enhances performance while training on limited computational\nresources. Leveraging reflow techniques, ProtFlow enables high-quality\nsingle-step sequence generation. Additionally, we develop a joint design\npipeline for the design scene of multichain proteins. We evaluate ProtFlow\nacross diverse protein design tasks, including general peptides and long-chain\nproteins, antimicrobial peptides, and antibodies. Experimental results\ndemonstrate that ProtFlow outperforms task-specific methods in these\napplications, underscoring its potential and broad applicability in\ncomputational protein sequence design and analysis.", "published": "2025-04-15 08:46:53", "link": "http://arxiv.org/abs/2504.10983v1", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "primary_category": "cs.LG"}
{"title": "Evaluating Trust in AI, Human, and Co-produced Feedback Among Undergraduate Students", "abstract": "As generative AI transforms educational feedback practices, understanding\nstudents' perceptions of different feedback providers becomes crucial for\neffective implementation. This study addresses a critical gap by comparing\nundergraduate students' trust in AI-generated, human-created, and human-AI\nco-produced feedback, informing how institutions can adapt feedback practices\nin this new era. Through a within-subject experiment with 91 participants, we\ninvestigated factors predicting students' ability to distinguish between\nfeedback types, perception of feedback quality, and potential biases to AI\ninvolvement. Findings revealed that students generally preferred AI and\nco-produced feedback over human feedback in terms of perceived usefulness and\nobjectivity. Only AI feedback suffered a decline in perceived genuineness when\nfeedback sources were revealed, while co-produced feedback maintained its\npositive perception. Educational AI experience improved students' ability to\nidentify AI feedback and increased their trust in all feedback types, while\ngeneral AI experience decreased perceived usefulness and credibility. Male\nstudents consistently rated all feedback types as less valuable than their\nfemale and non-binary counterparts. These insights inform evidence-based\nguidelines for integrating AI into higher education feedback systems while\naddressing trust concerns and fostering AI literacy among students.", "published": "2025-04-15 08:06:36", "link": "http://arxiv.org/abs/2504.10961v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC"}
{"title": "BEACON: A Benchmark for Efficient and Accurate Counting of Subgraphs", "abstract": "Subgraph counting the task of determining the number of instances of a query\npattern within a large graph lies at the heart of many critical applications,\nfrom analyzing financial networks and transportation systems to understanding\nbiological interactions. Despite decades of work yielding efficient algorithmic\n(AL) solutions and, more recently, machine learning (ML) approaches, a clear\ncomparative understanding is elusive. This gap stems from the absence of a\nunified evaluation framework, standardized datasets, and accessible ground\ntruths, all of which hinder systematic analysis and fair benchmarking. To\novercome these barriers, we introduce BEACON: a comprehensive benchmark\ndesigned to rigorously evaluate both AL and ML-based subgraph counting methods.\nBEACON provides a standardized dataset with verified ground truths, an\nintegrated evaluation environment, and a public leaderboard, enabling\nreproducible and transparent comparisons across diverse approaches. Our\nextensive experiments reveal that while AL methods excel in efficiently\ncounting subgraphs on very large graphs, they struggle with complex patterns\n(e.g., those exceeding six nodes). In contrast, ML methods are capable of\nhandling larger patterns but demand massive graph data inputs and often yield\nsuboptimal accuracy on small, dense graphs. These insights not only highlight\nthe unique strengths and limitations of each approach but also pave the way for\nfuture advancements in subgraph counting techniques. Overall, BEACON represents\na significant step towards unifying and accelerating research in subgraph\ncounting, encouraging innovative solutions and fostering a deeper understanding\nof the trade-offs between algorithmic and machine learning paradigms.", "published": "2025-04-15 07:53:47", "link": "http://arxiv.org/abs/2504.10948v1", "categories": ["cs.DS", "cs.AI", "cs.DB", "cs.SI"], "primary_category": "cs.DS"}
{"title": "Can LLMs Leverage Observational Data? Towards Data-Driven Causal Discovery with LLMs", "abstract": "Causal discovery traditionally relies on statistical methods applied to\nobservational data, often requiring large datasets and assumptions about\nunderlying causal structures. Recent advancements in Large Language Models\n(LLMs) have introduced new possibilities for causal discovery by providing\ndomain expert knowledge. However, it remains unclear whether LLMs can\neffectively process observational data for causal discovery. In this work, we\nexplore the potential of LLMs for data-driven causal discovery by integrating\nobservational data for LLM-based reasoning. Specifically, we examine whether\nLLMs can effectively utilize observational data through two prompting\nstrategies: pairwise prompting and breadth first search (BFS)-based prompting.\nIn both approaches, we incorporate the observational data directly into the\nprompt to assess LLMs' ability to infer causal relationships from such data.\nExperiments on benchmark datasets show that incorporating observational data\nenhances causal discovery, boosting F1 scores by up to 0.11 point using both\npairwise and BFS LLM-based prompting, while outperforming traditional\nstatistical causal discovery baseline by up to 0.52 points. Our findings\nhighlight the potential and limitations of LLMs for data-driven causal\ndiscovery, demonstrating their ability to move beyond textual metadata and\neffectively interpret and utilize observational data for more informed causal\nreasoning. Our studies lays the groundwork for future advancements toward fully\nLLM-driven causal discovery.", "published": "2025-04-15 07:32:35", "link": "http://arxiv.org/abs/2504.10936v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Transfer Learning for Temporal Link Prediction", "abstract": "Link prediction on graphs has applications spanning from recommender systems\nto drug discovery. Temporal link prediction (TLP) refers to predicting future\nlinks in a temporally evolving graph and adds additional complexity related to\nthe dynamic nature of graphs. State-of-the-art TLP models incorporate memory\nmodules alongside graph neural networks to learn both the temporal mechanisms\nof incoming nodes and the evolving graph topology. However, memory modules only\nstore information about nodes seen at train time, and hence such models cannot\nbe directly transferred to entirely new graphs at test time and deployment. In\nthis work, we study a new transfer learning task for temporal link prediction,\nand develop transfer-effective methods for memory-laden models. Specifically,\nmotivated by work showing the informativeness of structural signals for the TLP\ntask, we augment a structural mapping module to the existing TLP model\narchitectures, which learns a mapping from graph structural (topological)\nfeatures to memory embeddings. Our work paves the way for a memory-free\nfoundation model for TLP.", "published": "2025-04-15 07:12:00", "link": "http://arxiv.org/abs/2504.10925v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Towards A Universal Graph Structural Encoder", "abstract": "Recent advancements in large-scale pre-training have shown the potential to\nlearn generalizable representations for downstream tasks. In the graph domain,\nhowever, capturing and transferring structural information across different\ngraph domains remains challenging, primarily due to the inherent differences in\ntopological patterns across various contexts. Additionally, most existing\nmodels struggle to capture the complexity of rich graph structures, leading to\ninadequate exploration of the embedding space. To address these challenges, we\npropose GFSE, a universal graph structural encoder designed to capture\ntransferable structural patterns across diverse domains such as molecular\ngraphs, social networks, and citation networks. GFSE is the first cross-domain\ngraph structural encoder pre-trained with multiple self-supervised learning\nobjectives. Built on a Graph Transformer, GFSE incorporates attention\nmechanisms informed by graph inductive bias, enabling it to encode intricate\nmulti-level and fine-grained topological features. The pre-trained GFSE\nproduces generic and theoretically expressive positional and structural\nencoding for graphs, which can be seamlessly integrated with various downstream\ngraph feature encoders, including graph neural networks for vectorized features\nand Large Language Models for text-attributed graphs. Comprehensive experiments\non synthetic and real-world datasets demonstrate GFSE's capability to\nsignificantly enhance the model's performance while requiring substantially\nless task-specific fine-tuning. Notably, GFSE achieves state-of-the-art\nperformance in 81.6% evaluated cases, spanning diverse graph models and\ndatasets, highlighting its potential as a powerful and versatile encoder for\ngraph-structured data.", "published": "2025-04-15 06:57:26", "link": "http://arxiv.org/abs/2504.10917v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "LOKA Protocol: A Decentralized Framework for Trustworthy and Ethical AI Agent Ecosystems", "abstract": "The rise of autonomous AI agents, capable of perceiving, reasoning, and\nacting independently, signals a profound shift in how digital ecosystems\noperate, govern, and evolve. As these agents proliferate beyond centralized\ninfrastructures, they expose foundational gaps in identity, accountability, and\nethical alignment. Three critical questions emerge: Identity: Who or what is\nthe agent? Accountability: Can its actions be verified, audited, and trusted?\nEthical Consensus: Can autonomous systems reliably align with human values and\nprevent harmful emergent behaviors? We present the novel LOKA Protocol (Layered\nOrchestration for Knowledgeful Agents), a unified, systems-level architecture\nfor building ethically governed, interoperable AI agent ecosystems. LOKA\nintroduces a proposed Universal Agent Identity Layer (UAIL) for decentralized,\nverifiable identity; intent-centric communication protocols for semantic\ncoordination across diverse agents; and a Decentralized Ethical Consensus\nProtocol (DECP) that enables agents to make context-aware decisions grounded in\nshared ethical baselines. Anchored in emerging standards such as Decentralized\nIdentifiers (DIDs), Verifiable Credentials (VCs), and post-quantum\ncryptography, LOKA offers a scalable, future-resilient blueprint for\nmulti-agent AI governance. By embedding identity, trust, and ethics into the\nprotocol layer itself, LOKA establishes the foundation for a new era of\nresponsible, transparent, and autonomous AI ecosystems operating across digital\nand physical domains.", "published": "2025-04-15 06:51:35", "link": "http://arxiv.org/abs/2504.10915v1", "categories": ["cs.MA", "cs.AI", "cs.CY"], "primary_category": "cs.MA"}
{"title": "Bridging Distribution Gaps in Time Series Foundation Model Pretraining with Prototype-Guided Normalization", "abstract": "Foundation models have achieved remarkable success across diverse\nmachine-learning domains through large-scale pretraining on large, diverse\ndatasets. However, pretraining on such datasets introduces significant\nchallenges due to substantial mismatches in data distributions, a problem\nparticularly pronounced with time series data. In this paper, we tackle this\nissue by proposing a domain-aware adaptive normalization strategy within the\nTransformer architecture. Specifically, we replace the traditional LayerNorm\nwith a prototype-guided dynamic normalization mechanism (ProtoNorm), where\nlearned prototypes encapsulate distinct data distributions, and\nsample-to-prototype affinity determines the appropriate normalization layer.\nThis mechanism effectively captures the heterogeneity of time series\ncharacteristics, aligning pretrained representations with downstream tasks.\nThrough comprehensive empirical evaluation, we demonstrate that our method\nsignificantly outperforms conventional pretraining techniques across both\nclassification and forecasting tasks, while effectively mitigating the adverse\neffects of distribution shifts during pretraining. Incorporating ProtoNorm is\nas simple as replacing a single line of code. Extensive experiments on diverse\nreal-world time series benchmarks validate the robustness and generalizability\nof our approach, advancing the development of more versatile time series\nfoundation models.", "published": "2025-04-15 06:23:00", "link": "http://arxiv.org/abs/2504.10900v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Xpose: Bi-directional Engineering for Hidden Query Extraction", "abstract": "Query reverse engineering (QRE) aims to synthesize a SQL query to connect a\ngiven database and result instance. A recent variation of QRE is where an\nadditional input, an opaque executable containing a ground-truth query, is\nprovided, and the goal is to non-invasively extract this specific query through\nonly input-output examples. This variant, called Hidden Query Extraction (HQE),\nhas a spectrum of industrial use-cases including query recovery, database\nsecurity, and vendor migration. The reverse engineering (RE) tools developed\nfor HQE, which are based on database mutation and generation techniques, can\nonly extract flat queries with key-based equi joins and conjunctive arithmetic\nfilter predicates, making them limited wrt both query structure and query\noperators. In this paper, we present Xpose, a HQE solution that elevates the\nextraction scope to realistic complex queries, such as those found in the TPCH\nbenchmark. A two-pronged approach is taken: (1) The existing RE scope is\nsubstantially extended to incorporate union connectors, algebraic filter\npredicates, and disjunctions for both values and predicates. (2) The predictive\npower of LLMs is leveraged to convert business descriptions of the opaque\napplication into extraction guidance, representing ``forward engineering\" (FE).\nThe FE module recognizes common constructs, such as nesting of sub-queries,\nouter joins, and scalar functions. In essence, FE establishes the broad query\ncontours, while RE fleshes out the fine-grained details. We have evaluated\nXpose on (a) E-TPCH, a query suite comprising the complete TPCH benchmark\nextended with queries featuring unions, diverse join types, and sub-queries;\nand (b) the real-world STACK benchmark. The experimental results demonstrate\nthat its bi-directional engineering approach accurately extracts these complex\nqueries, representing a significant step forward with regard to HQE coverage.", "published": "2025-04-15 06:17:58", "link": "http://arxiv.org/abs/2504.10898v1", "categories": ["cs.DB", "cs.AI", "H.2.8"], "primary_category": "cs.DB"}
{"title": "CDUPatch: Color-Driven Universal Adversarial Patch Attack for Dual-Modal Visible-Infrared Detectors", "abstract": "Adversarial patches are widely used to evaluate the robustness of object\ndetection systems in real-world scenarios. These patches were initially\ndesigned to deceive single-modal detectors (e.g., visible or infrared) and have\nrecently been extended to target visible-infrared dual-modal detectors.\nHowever, existing dual-modal adversarial patch attacks have limited attack\neffectiveness across diverse physical scenarios. To address this, we propose\nCDUPatch, a universal cross-modal patch attack against visible-infrared object\ndetectors across scales, views, and scenarios. Specifically, we observe that\ncolor variations lead to different levels of thermal absorption, resulting in\ntemperature differences in infrared imaging. Leveraging this property, we\npropose an RGB-to-infrared adapter that maps RGB patches to infrared patches,\nenabling unified optimization of cross-modal patches. By learning an optimal\ncolor distribution on the adversarial patch, we can manipulate its thermal\nresponse and generate an adversarial infrared texture. Additionally, we\nintroduce a multi-scale clipping strategy and construct a new visible-infrared\ndataset, MSDrone, which contains aerial vehicle images in varying scales and\nperspectives. These data augmentation strategies enhance the robustness of our\npatch in real-world conditions. Experiments on four benchmark datasets (e.g.,\nDroneVehicle, LLVIP, VisDrone, MSDrone) show that our method outperforms\nexisting patch attacks in the digital domain. Extensive physical tests further\nconfirm strong transferability across scales, views, and scenarios.", "published": "2025-04-15 05:46:00", "link": "http://arxiv.org/abs/2504.10888v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "PuzzleBench: A Fully Dynamic Evaluation Framework for Large Multimodal Models on Puzzle Solving", "abstract": "Large Multimodal Models (LMMs) have demonstrated impressive capabilities\nacross a wide range of multimodal tasks, achieving ever-increasing performance\non various evaluation benchmarks. However, existing benchmarks are typically\nstatic and often overlap with pre-training datasets, leading to fixed\ncomplexity constraints and substantial data contamination issues. Meanwhile,\nmanually annotated datasets are labor-intensive, time-consuming, and subject to\nhuman bias and inconsistency, leading to reliability and reproducibility\nissues. To address these problems, we propose a fully dynamic multimodal\nevaluation framework, named Open-ended Visual Puzzle Generation (OVPG), which\naims to generate fresh, diverse, and verifiable evaluation data automatically\nin puzzle-solving tasks. Specifically, the OVPG pipeline consists of a raw\nmaterial sampling module, a visual content generation module, and a puzzle rule\ndesign module, which ensures that each evaluation instance is primitive, highly\nrandomized, and uniquely solvable, enabling continual adaptation to the\nevolving capabilities of LMMs. Built upon OVPG, we construct PuzzleBench, a\ndynamic and scalable benchmark comprising 11,840 VQA samples. It features six\ncarefully designed puzzle tasks targeting three core LMM competencies, visual\nrecognition, logical reasoning, and context understanding. PuzzleBench differs\nfrom static benchmarks that quickly become outdated. It enables ongoing dataset\nrefreshing through OVPG and a rich set of open-ended puzzle designs, allowing\nseamless adaptation to the evolving capabilities of LMMs.", "published": "2025-04-15 05:29:31", "link": "http://arxiv.org/abs/2504.10885v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Bringing together invertible UNets with invertible attention modules for memory-efficient diffusion models", "abstract": "Diffusion models have recently gained state of the art performance on many\nimage generation tasks. However, most models require significant computational\nresources to achieve this. This becomes apparent in the application of medical\nimage synthesis due to the 3D nature of medical datasets like CT-scans, MRIs,\nelectron microscope, etc. In this paper we propose a novel architecture for a\nsingle GPU memory-efficient training for diffusion models for high dimensional\nmedical datasets. The proposed model is built by using an invertible UNet\narchitecture with invertible attention modules. This leads to the following two\ncontributions: 1. denoising diffusion models and thus enabling memory usage to\nbe independent of the dimensionality of the dataset, and 2. reducing the energy\nusage during training. While this new model can be applied to a multitude of\nimage generation tasks, we showcase its memory-efficiency on the 3D BraTS2020\ndataset leading to up to 15\\% decrease in peak memory consumption during\ntraining with comparable results to SOTA while maintaining the image quality.", "published": "2025-04-15 05:26:42", "link": "http://arxiv.org/abs/2504.10883v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Large Language Model-Informed Feature Discovery Improves Prediction and Interpretation of Credibility Perceptions of Visual Content", "abstract": "In today's visually dominated social media landscape, predicting the\nperceived credibility of visual content and understanding what drives human\njudgment are crucial for countering misinformation. However, these tasks are\nchallenging due to the diversity and richness of visual features. We introduce\na Large Language Model (LLM)-informed feature discovery framework that\nleverages multimodal LLMs, such as GPT-4o, to evaluate content credibility and\nexplain its reasoning. We extract and quantify interpretable features using\ntargeted prompts and integrate them into machine learning models to improve\ncredibility predictions. We tested this approach on 4,191 visual social media\nposts across eight topics in science, health, and politics, using credibility\nratings from 5,355 crowdsourced workers. Our method outperformed zero-shot\nGPT-based predictions by 13 percent in R2, and revealed key features like\ninformation concreteness and image format. We discuss the implications for\nmisinformation mitigation, visual credibility, and the role of LLMs in social\nscience.", "published": "2025-04-15 05:11:40", "link": "http://arxiv.org/abs/2504.10878v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.4.9; J.4"], "primary_category": "cs.CV"}
{"title": "Can Vision-Language Models Understand and Interpret Dynamic Gestures from Pedestrians? Pilot Datasets and Exploration Towards Instructive Nonverbal Commands for Cooperative Autonomous Vehicles", "abstract": "In autonomous driving, it is crucial to correctly interpret traffic gestures\n(TGs), such as those of an authority figure providing orders or instructions,\nor a pedestrian signaling the driver, to ensure a safe and pleasant traffic\nenvironment for all road users. This study investigates the capabilities of\nstate-of-the-art vision-language models (VLMs) in zero-shot interpretation,\nfocusing on their ability to caption and classify human gestures in traffic\ncontexts. We create and publicly share two custom datasets with varying formal\nand informal TGs, such as 'Stop', 'Reverse', 'Hail', etc. The datasets are\n\"Acted TG (ATG)\" and \"Instructive TG In-The-Wild (ITGI)\". They are annotated\nwith natural language, describing the pedestrian's body position and gesture.\nWe evaluate models using three methods utilizing expert-generated captions as\nbaseline and control: (1) caption similarity, (2) gesture classification, and\n(3) pose sequence reconstruction similarity. Results show that current VLMs\nstruggle with gesture understanding: sentence similarity averages below 0.59,\nand classification F1 scores reach only 0.14-0.39, well below the expert\nbaseline of 0.70. While pose reconstruction shows potential, it requires more\ndata and refined metrics to be reliable. Our findings reveal that although some\nSOTA VLMs can interpret zero-shot human traffic gestures, none are accurate and\nrobust enough to be trustworthy, emphasizing the need for further research in\nthis domain.", "published": "2025-04-15 05:04:25", "link": "http://arxiv.org/abs/2504.10873v1", "categories": ["cs.CV", "cs.AI", "cs.HC"], "primary_category": "cs.CV"}
{"title": "Understanding the theoretical properties of projected Bellman equation, linear Q-learning, and approximate value iteration", "abstract": "In this paper, we study the theoretical properties of the projected Bellman\nequation (PBE) and two algorithms to solve this equation: linear Q-learning and\napproximate value iteration (AVI). We consider two sufficient conditions for\nthe existence of a solution to PBE : strictly negatively row dominating\ndiagonal (SNRDD) assumption and a condition motivated by the convergence of\nAVI. The SNRDD assumption also ensures the convergence of linear Q-learning,\nand its relationship with the convergence of AVI is examined. Lastly, several\ninteresting observations on the solution of PBE are provided when using\n$\\epsilon$-greedy policy.", "published": "2025-04-15 04:56:33", "link": "http://arxiv.org/abs/2504.10865v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Rethinking Theory of Mind Benchmarks for LLMs: Towards A User-Centered Perspective", "abstract": "The last couple of years have witnessed emerging research that appropriates\nTheory-of-Mind (ToM) tasks designed for humans to benchmark LLM's ToM\ncapabilities as an indication of LLM's social intelligence. However, this\napproach has a number of limitations. Drawing on existing psychology and AI\nliterature, we summarize the theoretical, methodological, and evaluation\nlimitations by pointing out that certain issues are inherently present in the\noriginal ToM tasks used to evaluate human's ToM, which continues to persist and\nexacerbated when appropriated to benchmark LLM's ToM. Taking a human-computer\ninteraction (HCI) perspective, these limitations prompt us to rethink the\ndefinition and criteria of ToM in ToM benchmarks in a more dynamic,\ninteractional approach that accounts for user preferences, needs, and\nexperiences with LLMs in such evaluations. We conclude by outlining potential\nopportunities and challenges towards this direction.", "published": "2025-04-15 03:44:43", "link": "http://arxiv.org/abs/2504.10839v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC"}
{"title": "Uplink Assisted Joint Channel Estimation and CSI Feedback: An Approach Based on Deep Joint Source-Channel Coding", "abstract": "In frequency division duplex (FDD) multiple-input multiple-output (MIMO)\nwireless communication systems, the acquisition of downlink channel state\ninformation (CSI) is essential for maximizing spatial resource utilization and\nimproving system spectral efficiency. The separate design of modules in\nAI-based CSI feedback architectures under traditional modular communication\nframeworks, including channel estimation (CE), CSI compression and feedback,\nleads to sub-optimal performance. In this paper, we propose an uplink assisted\njoint CE and and CSI feedback approach via deep learning for downlink CSI\nacquisition, which mitigates performance degradation caused by distribution\nbias across separately trained modules in traditional modular communication\nframeworks. The proposed network adopts a deep joint source-channel coding\n(DJSCC) architecture to mitigate the cliff effect encountered in the\nconventional separate source-channel coding. Furthermore, we exploit the uplink\nCSI as auxiliary information to enhance CSI reconstruction accuracy by\nleveraging the partial reciprocity between the uplink and downlink channels in\nFDD systems, without introducing additional overhead. The effectiveness of\nuplink CSI as assisted information and the necessity of an end-toend\nmulti-module joint training architecture is validated through comprehensive\nablation and scalability experiments.", "published": "2025-04-15 03:29:24", "link": "http://arxiv.org/abs/2504.10836v1", "categories": ["eess.SP", "cs.AI"], "primary_category": "eess.SP"}
{"title": "Towards Spatially-Aware and Optimally Faithful Concept-Based Explanations", "abstract": "Post-hoc, unsupervised concept-based explanation methods (U-CBEMs) are a\npromising tool for generating semantic explanations of the decision-making\nprocesses in deep neural networks, having applications in both model\nimprovement and understanding. It is vital that the explanation is accurate, or\nfaithful, to the model, yet we identify several limitations of prior\nfaithfulness metrics that inhibit an accurate evaluation; most notably, prior\nmetrics involve only the set of concepts present, ignoring how they may be\nspatially distributed. We address these limitations with Surrogate Faithfulness\n(SF), an evaluation method that introduces a spatially-aware surrogate and two\nnovel faithfulness metrics. Using SF, we produce Optimally Faithful (OF)\nexplanations, where concepts are found that maximize faithfulness. Our\nexperiments show that (1) adding spatial-awareness to prior U-CBEMs increases\nfaithfulness in all cases; (2) OF produces significantly more faithful\nexplanations than prior U-CBEMs (30% or higher improvement in error); (3) OF's\nlearned concepts generalize well to out-of-domain data and are more robust to\nadversarial examples, where prior U-CBEMs struggle.", "published": "2025-04-15 03:24:13", "link": "http://arxiv.org/abs/2504.10833v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Hallucination-Aware Generative Pretrained Transformer for Cooperative Aerial Mobility Control", "abstract": "This paper proposes SafeGPT, a two-tiered framework that integrates\ngenerative pretrained transformers (GPTs) with reinforcement learning (RL) for\nefficient and reliable unmanned aerial vehicle (UAV) last-mile deliveries. In\nthe proposed design, a Global GPT module assigns high-level tasks such as\nsector allocation, while an On-Device GPT manages real-time local route\nplanning. An RL-based safety filter monitors each GPT decision and overrides\nunsafe actions that could lead to battery depletion or duplicate visits,\neffectively mitigating hallucinations. Furthermore, a dual replay buffer\nmechanism helps both the GPT modules and the RL agent refine their strategies\nover time. Simulation results demonstrate that SafeGPT achieves higher delivery\nsuccess rates compared to a GPT-only baseline, while substantially reducing\nbattery consumption and travel distance. These findings validate the efficacy\nof combining GPT-based semantic reasoning with formal safety guarantees,\ncontributing a viable solution for robust and energy-efficient UAV logistics.", "published": "2025-04-15 03:21:08", "link": "http://arxiv.org/abs/2504.10831v1", "categories": ["cs.AI", "cs.RO", "68T05"], "primary_category": "cs.AI"}
{"title": "Progressive Rock Music Classification", "abstract": "This study investigates the classification of progressive rock music, a genre\ncharacterized by complex compositions and diverse instrumentation, distinct\nfrom other musical styles. Addressing this Music Information Retrieval (MIR)\ntask, we extracted comprehensive audio features, including spectrograms,\nMel-Frequency Cepstral Coefficients (MFCCs), chromagrams, and beat positions\nfrom song snippets using the Librosa library. A winner-take-all voting strategy\nwas employed to aggregate snippet-level predictions into final song\nclassifications. We conducted a comparative analysis of various machine\nlearning techniques. Ensemble methods, encompassing Bagging (Random Forest,\nExtraTrees, Bagging Classifier) and Boosting (XGBoost, Gradient Boosting), were\nexplored, utilizing Principal Component Analysis (PCA) for dimensionality\nreduction to manage computational constraints with high-dimensional feature\nsets. Additionally, deep learning approaches were investigated, including the\ndevelopment of custom 1D Convolutional Neural Network (1D CNN) architectures\n(named \"Zuck\" and \"Satya\") featuring specific layer configurations,\nnormalization, and activation functions. Furthermore, we fine-tuned a\nstate-of-the-art Audio Spectrogram Transformer (AST) model, leveraging its\nattention-based mechanisms for audio classification. Performance evaluation on\nvalidation and test sets revealed varying effectiveness across models, with\nensemble methods like Extra Trees achieving test accuracies up to 76.38%. This\nresearch provides insights into the application and relative performance of\ndiverse machine learning paradigms for the nuanced task of progressive rock\ngenre classification.", "published": "2025-04-15 02:48:52", "link": "http://arxiv.org/abs/2504.10821v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FHBench: Towards Efficient and Personalized Federated Learning for Multimodal Healthcare", "abstract": "Federated Learning (FL) has emerged as an effective solution for\nmulti-institutional collaborations without sharing patient data, offering a\nrange of methods tailored for diverse applications. However, real-world medical\ndatasets are often multimodal, and computational resources are limited, posing\nsignificant challenges for existing FL approaches. Recognizing these\nlimitations, we developed the Federated Healthcare Benchmark(FHBench), a\nbenchmark specifically designed from datasets derived from real-world\nhealthcare applications. FHBench encompasses critical diagnostic tasks across\ndomains such as the nervous, cardiovascular, and respiratory systems and\ngeneral pathology, providing comprehensive support for multimodal healthcare\nevaluations and filling a significant gap in existing benchmarks. Building on\nFHBench, we introduced Efficient Personalized Federated Learning with Adaptive\nLoRA(EPFL), a personalized FL framework that demonstrates superior efficiency\nand effectiveness across various healthcare modalities. Our results highlight\nthe robustness of FHBench as a benchmarking tool and the potential of EPFL as\nan innovative approach to advancing healthcare-focused FL, addressing key\nlimitations of existing methods.", "published": "2025-04-15 02:38:00", "link": "http://arxiv.org/abs/2504.10817v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "E2E Parking Dataset: An Open Benchmark for End-to-End Autonomous Parking", "abstract": "End-to-end learning has shown great potential in autonomous parking, yet the\nlack of publicly available datasets limits reproducibility and benchmarking.\nWhile prior work introduced a visual-based parking model and a pipeline for\ndata generation, training, and close-loop test, the dataset itself was not\nreleased. To bridge this gap, we create and open-source a high-quality dataset\nfor end-to-end autonomous parking. Using the original model, we achieve an\noverall success rate of 85.16% with lower average position and orientation\nerrors (0.24 meters and 0.34 degrees).", "published": "2025-04-15 02:21:09", "link": "http://arxiv.org/abs/2504.10812v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "PatrolVision: Automated License Plate Recognition in the wild", "abstract": "Adoption of AI driven techniques in public services remains low due to\nchallenges related to accuracy and speed of information at population scale.\nComputer vision techniques for traffic monitoring have not gained much\npopularity despite their relative strength in areas such as autonomous driving.\nDespite large number of academic methods for Automatic License Plate\nRecognition (ALPR) systems, very few provide an end to end solution for\npatrolling in the city. This paper presents a novel prototype for a low power\nGPU based patrolling system to be deployed in an urban environment on\nsurveillance vehicles for automated vehicle detection, recognition and\ntracking. In this work, we propose a complete ALPR system for Singapore license\nplates having both single and double line creating our own YOLO based network.\nWe focus on unconstrained capture scenarios as would be the case in real world\napplication, where the license plate (LP) might be considerably distorted due\nto oblique views. In this work, we first detect the license plate from the full\nimage using RFB-Net and rectify multiple distorted license plates in a single\nimage. After that, the detected license plate image is fed to our network for\ncharacter recognition. We evaluate the performance of our proposed system on a\nnewly built dataset covering more than 16,000 images. The system was able to\ncorrectly detect license plates with 86\\% precision and recognize characters of\na license plate in 67\\% of the test set, and 89\\% accuracy with one incorrect\ncharacter (partial match). We also test latency of our system and achieve 64FPS\non Tesla P4 GPU", "published": "2025-04-15 02:10:43", "link": "http://arxiv.org/abs/2504.10810v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Visual Language Models show widespread visual deficits on neuropsychological tests", "abstract": "Visual Language Models (VLMs) show remarkable performance in visual reasoning\ntasks, successfully tackling college-level challenges that require high-level\nunderstanding of images. However, some recent reports of VLMs struggling to\nreason about elemental visual concepts like orientation, position, continuity,\nand occlusion suggest a potential gulf between human and VLM vision. Here we\nuse the toolkit of neuropsychology to systematically assess the capabilities of\nthree state-of-the-art VLMs across visual domains. Using 51 tests drawn from\nsix clinical and experimental batteries, we characterise the visual abilities\nof leading VLMs relative to normative performance in healthy adults. While the\nmodels excel in straightforward object recognition tasks, we find widespread\ndeficits in low- and mid-level visual abilities that would be considered\nclinically significant in humans. These selective deficits, profiled through\nvalidated test batteries, suggest that an artificial system can achieve complex\nobject recognition without developing foundational visual concepts that in\nhumans require no explicit training.", "published": "2025-04-15 01:04:56", "link": "http://arxiv.org/abs/2504.10786v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.0; I.2.10"], "primary_category": "cs.CV"}
{"title": "ATLASv2: LLM-Guided Adaptive Landmark Acquisition and Navigation on the Edge", "abstract": "Autonomous systems deployed on edge devices face significant challenges,\nincluding resource constraints, real-time processing demands, and adapting to\ndynamic environments. This work introduces ATLASv2, a novel system that\nintegrates a fine-tuned TinyLLM, real-time object detection, and efficient path\nplanning to enable hierarchical, multi-task navigation and manipulation all on\nthe edge device, Jetson Nano. ATLASv2 dynamically expands its navigable\nlandmarks by detecting and localizing objects in the environment which are\nsaved to its internal knowledge base to be used for future task execution. We\nevaluate ATLASv2 in real-world environments, including a handcrafted home and\noffice setting constructed with diverse objects and landmarks. Results show\nthat ATLASv2 effectively interprets natural language instructions, decomposes\nthem into low-level actions, and executes tasks with high success rates. By\nleveraging generative AI in a fully on-board framework, ATLASv2 achieves\noptimized resource utilization with minimal prompting latency and power\nconsumption, bridging the gap between simulated environments and real-world\napplications.", "published": "2025-04-15 00:55:57", "link": "http://arxiv.org/abs/2504.10784v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "Neural Network Emulation of the Classical Limit in Quantum Systems via Learned Observable Mappings", "abstract": "The classical limit of quantum mechanics, formally investigated through\nframeworks like strict deformation quantization, remains a profound area of\ninquiry in the philosophy of physics. This paper explores a computational\napproach employing a neural network to emulate the emergence of classical\nbehavior from the quantum harmonic oscillator as Planck's constant $\\hbar$\napproaches zero. We develop and train a neural network architecture to learn\nthe mapping from initial expectation values and $\\hbar$ to the time evolution\nof the expectation value of position. By analyzing the network's predictions\nacross different regimes of hbar, we aim to provide computational insights into\nthe nature of the quantum-classical transition. This work demonstrates the\npotential of machine learning as a complementary tool for exploring\nfoundational questions in quantum mechanics and its classical limit.", "published": "2025-04-15 00:48:36", "link": "http://arxiv.org/abs/2504.10781v1", "categories": ["quant-ph", "cs.AI"], "primary_category": "quant-ph"}
{"title": "Aligning Generative Denoising with Discriminative Objectives Unleashes Diffusion for Visual Perception", "abstract": "With the success of image generation, generative diffusion models are\nincreasingly adopted for discriminative tasks, as pixel generation provides a\nunified perception interface. However, directly repurposing the generative\ndenoising process for discriminative objectives reveals critical gaps rarely\naddressed previously. Generative models tolerate intermediate sampling errors\nif the final distribution remains plausible, but discriminative tasks require\nrigorous accuracy throughout, as evidenced in challenging multi-modal tasks\nlike referring image segmentation. Motivated by this gap, we analyze and\nenhance alignment between generative diffusion processes and perception tasks,\nfocusing on how perception quality evolves during denoising. We find: (1)\nearlier denoising steps contribute disproportionately to perception quality,\nprompting us to propose tailored learning objectives reflecting varying\ntimestep contributions; (2) later denoising steps show unexpected perception\ndegradation, highlighting sensitivity to training-denoising distribution\nshifts, addressed by our diffusion-tailored data augmentation; and (3)\ngenerative processes uniquely enable interactivity, serving as controllable\nuser interfaces adaptable to correctional prompts in multi-round interactions.\nOur insights significantly improve diffusion-based perception models without\narchitectural changes, achieving state-of-the-art performance on depth\nestimation, referring image segmentation, and generalist perception tasks. Code\navailable at https://github.com/ziqipang/ADDP.", "published": "2025-04-15 17:59:54", "link": "http://arxiv.org/abs/2504.11457v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SimpleAR: Pushing the Frontier of Autoregressive Visual Generation through Pretraining, SFT, and RL", "abstract": "This work presents SimpleAR, a vanilla autoregressive visual generation\nframework without complex architecure modifications. Through careful\nexploration of training and inference optimization, we demonstrate that: 1)\nwith only 0.5B parameters, our model can generate 1024x1024 resolution images\nwith high fidelity, and achieve competitive results on challenging\ntext-to-image benchmarks, e.g., 0.59 on GenEval and 79.66 on DPG; 2) both\nsupervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO)\ntraining could lead to significant improvements on generation aesthectics and\nprompt alignment; and 3) when optimized with inference acceleraton techniques\nlike vLLM, the time for SimpleAR to generate an 1024x1024 image could be\nreduced to around 14 seconds. By sharing these findings and open-sourcing the\ncode, we hope to reveal the potential of autoregressive visual generation and\nencourage more participation in this research field. Code is available at\nhttps://github.com/wdrink/SimpleAR.", "published": "2025-04-15 17:59:46", "link": "http://arxiv.org/abs/2504.11455v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "PARTFIELD: Learning 3D Feature Fields for Part Segmentation and Beyond", "abstract": "We propose PartField, a feedforward approach for learning part-based 3D\nfeatures, which captures the general concept of parts and their hierarchy\nwithout relying on predefined templates or text-based names, and can be applied\nto open-world 3D shapes across various modalities. PartField requires only a 3D\nfeedforward pass at inference time, significantly improving runtime and\nrobustness compared to prior approaches. Our model is trained by distilling 2D\nand 3D part proposals from a mix of labeled datasets and image segmentations on\nlarge unsupervised datasets, via a contrastive learning formulation. It\nproduces a continuous feature field which can be clustered to yield a\nhierarchical part decomposition. Comparisons show that PartField is up to 20%\nmore accurate and often orders of magnitude faster than other recent\nclass-agnostic part-segmentation methods. Beyond single-shape part\ndecomposition, consistency in the learned field emerges across shapes, enabling\ntasks such as co-segmentation and correspondence, which we demonstrate in\nseveral applications of these general-purpose, hierarchical, and consistent 3D\nfeature fields. Check our Webpage!\nhttps://research.nvidia.com/labs/toronto-ai/partfield-release/", "published": "2025-04-15 17:58:16", "link": "http://arxiv.org/abs/2504.11451v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Diffusion Distillation With Direct Preference Optimization For Efficient 3D LiDAR Scene Completion", "abstract": "The application of diffusion models in 3D LiDAR scene completion is limited\ndue to diffusion's slow sampling speed. Score distillation accelerates\ndiffusion sampling but with performance degradation, while post-training with\ndirect policy optimization (DPO) boosts performance using preference data. This\npaper proposes Distillation-DPO, a novel diffusion distillation framework for\nLiDAR scene completion with preference aligment. First, the student model\ngenerates paired completion scenes with different initial noises. Second, using\nLiDAR scene evaluation metrics as preference, we construct winning and losing\nsample pairs. Such construction is reasonable, since most LiDAR scene metrics\nare informative but non-differentiable to be optimized directly. Third,\nDistillation-DPO optimizes the student model by exploiting the difference in\nscore functions between the teacher and student models on the paired completion\nscenes. Such procedure is repeated until convergence. Extensive experiments\ndemonstrate that, compared to state-of-the-art LiDAR scene completion diffusion\nmodels, Distillation-DPO achieves higher-quality scene completion while\naccelerating the completion speed by more than 5-fold. Our method is the first\nto explore adopting preference learning in distillation to the best of our\nknowledge and provide insights into preference-aligned distillation. Our code\nis public available on https://github.com/happyw1nd/DistillationDPO.", "published": "2025-04-15 17:57:13", "link": "http://arxiv.org/abs/2504.11447v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Mamba-Based Ensemble learning for White Blood Cell Classification", "abstract": "White blood cell (WBC) classification assists in assessing immune health and\ndiagnosing various diseases, yet manual classification is labor-intensive and\nprone to inconsistencies. Recent advancements in deep learning have shown\npromise over traditional methods; however, challenges such as data imbalance\nand the computational demands of modern technologies, such as Transformer-based\nmodels which do not scale well with input size, limit their practical\napplication. This paper introduces a novel framework that leverages Mamba\nmodels integrated with ensemble learning to improve WBC classification. Mamba\nmodels, known for their linear complexity, provide a scalable alternative to\nTransformer-based approaches, making them suitable for deployment in\nresource-constrained environments. Additionally, we introduce a new WBC\ndataset, Chula-WBC-8, for benchmarking. Our approach not only validates the\neffectiveness of Mamba models in this domain but also demonstrates their\npotential to significantly enhance classification efficiency without\ncompromising accuracy. The source code can be found at\nhttps://github.com/LewisClifton/Mamba-WBC-Classification.", "published": "2025-04-15 17:53:18", "link": "http://arxiv.org/abs/2504.11438v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Enhancing Out-of-Distribution Detection with Extended Logit Normalization", "abstract": "Out-of-distribution (OOD) detection is essential for the safe deployment of\nmachine learning models. Recent advances have explored improved classification\nlosses and representation learning strategies to enhance OOD detection.\nHowever, these methods are often tailored to specific post-hoc detection\ntechniques, limiting their generalizability. In this work, we identify a\ncritical issue in Logit Normalization (LogitNorm), which inhibits its\neffectiveness in improving certain post-hoc OOD detection methods. To address\nthis, we propose Extended Logit Normalization ($\\textbf{ELogitNorm}$), a novel\nhyperparameter-free formulation that significantly benefits a wide range of\npost-hoc detection methods. By incorporating feature distance-awareness to\nLogitNorm, $\\textbf{ELogitNorm}$ shows more robust OOD separability and\nin-distribution (ID) confidence calibration than its predecessor. Extensive\nexperiments across standard benchmarks demonstrate that our approach\noutperforms state-of-the-art training-time methods in OOD detection while\nmaintaining strong ID classification accuracy.", "published": "2025-04-15 17:51:35", "link": "http://arxiv.org/abs/2504.11434v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "NormalCrafter: Learning Temporally Consistent Normals from Video Diffusion Priors", "abstract": "Surface normal estimation serves as a cornerstone for a spectrum of computer\nvision applications. While numerous efforts have been devoted to static image\nscenarios, ensuring temporal coherence in video-based normal estimation remains\na formidable challenge. Instead of merely augmenting existing methods with\ntemporal components, we present NormalCrafter to leverage the inherent temporal\npriors of video diffusion models. To secure high-fidelity normal estimation\nacross sequences, we propose Semantic Feature Regularization (SFR), which\naligns diffusion features with semantic cues, encouraging the model to\nconcentrate on the intrinsic semantics of the scene. Moreover, we introduce a\ntwo-stage training protocol that leverages both latent and pixel space learning\nto preserve spatial accuracy while maintaining long temporal context. Extensive\nevaluations demonstrate the efficacy of our method, showcasing a superior\nperformance in generating temporally consistent normal sequences with intricate\ndetails from diverse videos.", "published": "2025-04-15 17:39:07", "link": "http://arxiv.org/abs/2504.11427v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Leveraging Point Transformers for Detecting Anatomical Landmarks in Digital Dentistry", "abstract": "The increasing availability of intraoral scanning devices has heightened\ntheir importance in modern clinical orthodontics. Clinicians utilize advanced\nComputer-Aided Design techniques to create patient-specific treatment plans\nthat include laboriously identifying crucial landmarks such as cusps,\nmesial-distal locations, facial axis points, and tooth-gingiva boundaries.\nDetecting such landmarks automatically presents challenges, including limited\ndataset sizes, significant anatomical variability among subjects, and the\ngeometric nature of the data. We present our experiments from the 3DTeethLand\nGrand Challenge at MICCAI 2024. Our method leverages recent advancements in\npoint cloud learning through transformer architectures. We designed a Point\nTransformer v3 inspired module to capture meaningful geometric and anatomical\nfeatures, which are processed by a lightweight decoder to predict per-point\ndistances, further processed by graph-based non-minima suppression. We report\npromising results and discuss insights on learned feature interpretability.", "published": "2025-04-15 17:34:56", "link": "http://arxiv.org/abs/2504.11418v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Deep Learning-based Bathymetry Retrieval without In-situ Depths using Remote Sensing Imagery and SfM-MVS DSMs with Data Gaps", "abstract": "Accurate, detailed, and high-frequent bathymetry is crucial for shallow\nseabed areas facing intense climatological and anthropogenic pressures. Current\nmethods utilizing airborne or satellite optical imagery to derive bathymetry\nprimarily rely on either SfM-MVS with refraction correction or Spectrally\nDerived Bathymetry (SDB). However, SDB methods often require extensive manual\nfieldwork or costly reference data, while SfM-MVS approaches face challenges\neven after refraction correction. These include depth data gaps and noise in\nenvironments with homogeneous visual textures, which hinder the creation of\naccurate and complete Digital Surface Models (DSMs) of the seabed. To address\nthese challenges, this work introduces a methodology that combines the\nhigh-fidelity 3D reconstruction capabilities of the SfM-MVS methods with\nstate-of-the-art refraction correction techniques, along with the spectral\nanalysis capabilities of a new deep learning-based method for bathymetry\nprediction. This integration enables a synergistic approach where SfM-MVS\nderived DSMs with data gaps are used as training data to generate complete\nbathymetric maps. In this context, we propose Swin-BathyUNet that combines\nU-Net with Swin Transformer self-attention layers and a cross-attention\nmechanism, specifically tailored for SDB. Swin-BathyUNet is designed to improve\nbathymetric accuracy by capturing long-range spatial relationships and can also\nfunction as a standalone solution for standard SDB with various training depth\ndata, independent of the SfM-MVS output. Experimental results in two completely\ndifferent test sites in the Mediterranean and Baltic Seas demonstrate the\neffectiveness of the proposed approach through extensive experiments that\ndemonstrate improvements in bathymetric accuracy, detail, coverage, and noise\nreduction in the predicted DSM. The code is available at\nhttps://github.com/pagraf/Swin-BathyUNet.", "published": "2025-04-15 17:31:48", "link": "http://arxiv.org/abs/2504.11416v1", "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Robustness and sex differences in skin cancer detection: logistic regression vs CNNs", "abstract": "Deep learning has been reported to achieve high performances in the detection\nof skin cancer, yet many challenges regarding the reproducibility of results\nand biases remain. This study is a replication (different data, same analysis)\nof a study on Alzheimer's disease [28] which studied robustness of logistic\nregression (LR) and convolutional neural networks (CNN) across patient sexes.\nWe explore sex bias in skin cancer detection, using the PAD-UFES-20 dataset\nwith LR trained on handcrafted features reflecting dermatological guidelines\n(ABCDE and the 7-point checklist), and a pre-trained ResNet-50 model. We\nevaluate these models in alignment with [28]: across multiple training datasets\nwith varied sex composition to determine their robustness. Our results show\nthat both the LR and the CNN were robust to the sex distributions, but the\nresults also revealed that the CNN had a significantly higher accuracy (ACC)\nand area under the receiver operating characteristics (AUROC) for male patients\nthan for female patients. We hope these findings to contribute to the growing\nfield of investigating potential bias in popular medical machine learning\nmethods. The data and relevant scripts to reproduce our results can be found in\nour Github.", "published": "2025-04-15 17:31:46", "link": "http://arxiv.org/abs/2504.11415v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Omni$^2$: Unifying Omnidirectional Image Generation and Editing in an Omni Model", "abstract": "$360^{\\circ}$ omnidirectional images (ODIs) have gained considerable\nattention recently, and are widely used in various virtual reality (VR) and\naugmented reality (AR) applications. However, capturing such images is\nexpensive and requires specialized equipment, making ODI synthesis increasingly\nimportant. While common 2D image generation and editing methods are rapidly\nadvancing, these models struggle to deliver satisfactory results when\ngenerating or editing ODIs due to the unique format and broad 360$^{\\circ}$\nField-of-View (FoV) of ODIs. To bridge this gap, we construct\n\\textbf{\\textit{Any2Omni}}, the first comprehensive ODI generation-editing\ndataset comprises 60,000+ training data covering diverse input conditions and\nup to 9 ODI generation and editing tasks. Built upon Any2Omni, we propose an\n\\textbf{\\underline{Omni}} model for \\textbf{\\underline{Omni}}-directional image\ngeneration and editing (\\textbf{\\textit{Omni$^2$}}), with the capability of\nhandling various ODI generation and editing tasks under diverse input\nconditions using one model. Extensive experiments demonstrate the superiority\nand effectiveness of the proposed Omni$^2$ model for both the ODI generation\nand editing tasks.", "published": "2025-04-15 16:53:11", "link": "http://arxiv.org/abs/2504.11379v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "From Gaze to Insight: Bridging Human Visual Attention and Vision Language Model Explanation for Weakly-Supervised Medical Image Segmentation", "abstract": "Medical image segmentation remains challenging due to the high cost of\npixel-level annotations for training. In the context of weak supervision,\nclinician gaze data captures regions of diagnostic interest; however, its\nsparsity limits its use for segmentation. In contrast, vision-language models\n(VLMs) provide semantic context through textual descriptions but lack the\nexplanation precision required. Recognizing that neither source alone suffices,\nwe propose a teacher-student framework that integrates both gaze and language\nsupervision, leveraging their complementary strengths. Our key insight is that\ngaze data indicates where clinicians focus during diagnosis, while VLMs explain\nwhy those regions are significant. To implement this, the teacher model first\nlearns from gaze points enhanced by VLM-generated descriptions of lesion\nmorphology, establishing a foundation for guiding the student model. The\nteacher then directs the student through three strategies: (1) Multi-scale\nfeature alignment to fuse visual cues with textual semantics; (2)\nConfidence-weighted consistency constraints to focus on reliable predictions;\n(3) Adaptive masking to limit error propagation in uncertain areas. Experiments\non the Kvasir-SEG, NCI-ISBI, and ISIC datasets show that our method achieves\nDice scores of 80.78%, 80.53%, and 84.22%, respectively-improving 3-5% over\ngaze baselines without increasing the annotation burden. By preserving\ncorrelations among predictions, gaze data, and lesion descriptions, our\nframework also maintains clinical interpretability. This work illustrates how\nintegrating human visual attention with AI-generated semantic context can\neffectively overcome the limitations of individual weak supervision signals,\nthereby advancing the development of deployable, annotation-efficient medical\nAI systems. Code is available at: https://github.com/jingkunchen/FGI.git.", "published": "2025-04-15 16:32:15", "link": "http://arxiv.org/abs/2504.11368v1", "categories": ["cs.CV", "68T45", "I.2.10; I.4.8"], "primary_category": "cs.CV"}
{"title": "A Decade of Wheat Mapping for Lebanon", "abstract": "Wheat accounts for approximately 20% of the world's caloric intake, making it\na vital component of global food security. Given this importance, mapping wheat\nfields plays a crucial role in enabling various stakeholders, including policy\nmakers, researchers, and agricultural organizations, to make informed decisions\nregarding food security, supply chain management, and resource allocation. In\nthis paper, we tackle the problem of accurately mapping wheat fields out of\nsatellite images by introducing an improved pipeline for winter wheat\nsegmentation, as well as presenting a case study on a decade-long analysis of\nwheat mapping in Lebanon. We integrate a Temporal Spatial Vision Transformer\n(TSViT) with Parameter-Efficient Fine Tuning (PEFT) and a novel post-processing\npipeline based on the Fields of The World (FTW) framework. Our proposed\npipeline addresses key challenges encountered in existing approaches, such as\nthe clustering of small agricultural parcels in a single large field. By\nmerging wheat segmentation with precise field boundary extraction, our method\nproduces geometrically coherent and semantically rich maps that enable us to\nperform in-depth analysis such as tracking crop rotation pattern over years.\nExtensive evaluations demonstrate improved boundary delineation and field-level\nprecision, establishing the potential of the proposed framework in operational\nagricultural monitoring and historical trend analysis. By allowing for accurate\nmapping of wheat fields, this work lays the foundation for a range of critical\nstudies and future advances, including crop monitoring and yield estimation.", "published": "2025-04-15 16:31:54", "link": "http://arxiv.org/abs/2504.11366v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DeepWheel: Generating a 3D Synthetic Wheel Dataset for Design and Performance Evaluation", "abstract": "Data-driven design is emerging as a powerful strategy to accelerate\nengineering innovation. However, its application to vehicle wheel design\nremains limited due to the lack of large-scale, high-quality datasets that\ninclude 3D geometry and physical performance metrics. To address this gap, this\nstudy proposes a synthetic design-performance dataset generation framework\nusing generative AI. The proposed framework first generates 2D rendered images\nusing Stable Diffusion, and then reconstructs the 3D geometry through 2.5D\ndepth estimation. Structural simulations are subsequently performed to extract\nengineering performance data. To further expand the design and performance\nspace, topology optimization is applied, enabling the generation of a more\ndiverse set of wheel designs. The final dataset, named DeepWheel, consists of\nover 6,000 photo-realistic images and 900 structurally analyzed 3D models. This\nmulti-modal dataset serves as a valuable resource for surrogate model training,\ndata-driven inverse design, and design space exploration. The proposed\nmethodology is also applicable to other complex design domains. The dataset is\nreleased under the Creative Commons Attribution-NonCommercial 4.0\nInternational(CC BY-NC 4.0) and is available on the\nhttps://www.smartdesignlab.org/datasets", "published": "2025-04-15 16:20:00", "link": "http://arxiv.org/abs/2504.11347v1", "categories": ["cs.CV", "physics.app-ph", "68T07"], "primary_category": "cs.CV"}
{"title": "Seedream 3.0 Technical Report", "abstract": "We present Seedream 3.0, a high-performance Chinese-English bilingual image\ngeneration foundation model. We develop several technical improvements to\naddress existing challenges in Seedream 2.0, including alignment with\ncomplicated prompts, fine-grained typography generation, suboptimal visual\naesthetics and fidelity, and limited image resolutions. Specifically, the\nadvancements of Seedream 3.0 stem from improvements across the entire pipeline,\nfrom data construction to model deployment. At the data stratum, we double the\ndataset using a defect-aware training paradigm and a dual-axis collaborative\ndata-sampling framework. Furthermore, we adopt several effective techniques\nsuch as mixed-resolution training, cross-modality RoPE, representation\nalignment loss, and resolution-aware timestep sampling in the pre-training\nphase. During the post-training stage, we utilize diversified aesthetic\ncaptions in SFT, and a VLM-based reward model with scaling, thereby achieving\noutputs that well align with human preferences. Furthermore, Seedream 3.0\npioneers a novel acceleration paradigm. By employing consistent noise\nexpectation and importance-aware timestep sampling, we achieve a 4 to 8 times\nspeedup while maintaining image quality. Seedream 3.0 demonstrates significant\nimprovements over Seedream 2.0: it enhances overall capabilities, in particular\nfor text-rendering in complicated Chinese characters which is important to\nprofessional typography generation. In addition, it provides native\nhigh-resolution output (up to 2K), allowing it to generate images with high\nvisual quality.", "published": "2025-04-15 16:19:07", "link": "http://arxiv.org/abs/2504.11346v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of Complex Videos in the Wild", "abstract": "This report provides a comprehensive overview of the 4th Pixel-level Video\nUnderstanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025.\nIt summarizes the challenge outcomes, participating methodologies, and future\nresearch directions. The challenge features two tracks: MOSE, which focuses on\ncomplex scene video object segmentation, and MeViS, which targets\nmotion-guided, language-based video segmentation. Both tracks introduce new,\nmore challenging datasets designed to better reflect real-world scenarios.\nThrough detailed evaluation and analysis, the challenge offers valuable\ninsights into the current state-of-the-art and emerging trends in complex video\nsegmentation. More information can be found on the workshop website:\nhttps://pvuw.github.io/.", "published": "2025-04-15 16:02:47", "link": "http://arxiv.org/abs/2504.11326v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Big Brother is Watching: Proactive Deepfake Detection via Learnable Hidden Face", "abstract": "As deepfake technologies continue to advance, passive detection methods\nstruggle to generalize with various forgery manipulations and datasets.\nProactive defense techniques have been actively studied with the primary aim of\npreventing deepfake operation effectively working. In this paper, we aim to\nbridge the gap between passive detection and proactive defense, and seek to\nsolve the detection problem utilizing a proactive methodology. Inspired by\nseveral watermarking-based forensic methods, we explore a novel detection\nframework based on the concept of ``hiding a learnable face within a face''.\nSpecifically, relying on a semi-fragile invertible steganography network, a\nsecret template image is embedded into a host image imperceptibly, acting as an\nindicator monitoring for any malicious image forgery when being restored by the\ninverse steganography process. Instead of being manually specified, the secret\ntemplate is optimized during training to resemble a neutral facial appearance,\njust like a ``big brother'' hidden in the image to be protected. By\nincorporating a self-blending mechanism and robustness learning strategy with a\nsimulative transmission channel, a robust detector is built to accurately\ndistinguish if the steganographic image is maliciously tampered or benignly\nprocessed. Finally, extensive experiments conducted on multiple datasets\ndemonstrate the superiority of the proposed approach over competing passive and\nproactive detection methods.", "published": "2025-04-15 15:50:54", "link": "http://arxiv.org/abs/2504.11309v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Uncertainty Estimation for Trust Attribution to Speed-of-Sound Reconstruction with Variational Networks", "abstract": "Speed-of-sound (SoS) is a biomechanical characteristic of tissue, and its\nimaging can provide a promising biomarker for diagnosis. Reconstructing SoS\nimages from ultrasound acquisitions can be cast as a limited-angle\ncomputed-tomography problem, with Variational Networks being a promising\nmodel-based deep learning solution. Some acquired data frames may, however, get\ncorrupted by noise due to, e.g., motion, lack of contact, and acoustic shadows,\nwhich in turn negatively affects the resulting SoS reconstructions. We propose\nto use the uncertainty in SoS reconstructions to attribute trust to each\nindividual acquired frame. Given multiple acquisitions, we then use an\nuncertainty based automatic selection among these retrospectively, to improve\ndiagnostic decisions. We investigate uncertainty estimation based on Monte\nCarlo Dropout and Bayesian Variational Inference. We assess our automatic frame\nselection method for differential diagnosis of breast cancer, distinguishing\nbetween benign fibroadenoma and malignant carcinoma. We evaluate 21 lesions\nclassified as BI-RADS~4, which represents suspicious cases for probable\nmalignancy. The most trustworthy frame among four acquisitions of each lesion\nwas identified using uncertainty based criteria. Selecting a frame informed by\nuncertainty achieved an area under curve of 76% and 80% for Monte Carlo Dropout\nand Bayesian Variational Inference, respectively, superior to any\nuncertainty-uninformed baselines with the best one achieving 64%. A novel use\nof uncertainty estimation is proposed for selecting one of multiple data\nacquisitions for further processing and decision making.", "published": "2025-04-15 15:48:51", "link": "http://arxiv.org/abs/2504.11307v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Context-Aware Palmprint Recognition via a Relative Similarity Metric", "abstract": "We propose a new approach to matching mechanism for palmprint recognition by\nintroducing a Relative Similarity Metric (RSM) that enhances the robustness and\ndiscriminability of existing matching frameworks. While conventional systems\nrely on direct pairwise similarity measures, such as cosine or Euclidean\ndistances, these metrics fail to capture how a pairwise similarity compares\nwithin the context of the entire dataset. Our method addresses this by\nevaluating the relative consistency of similarity scores across up to all\nidentities, allowing for better suppression of false positives and negatives.\nApplied atop the CCNet architecture, our method achieves a new state-of-the-art\n0.000036% Equal Error Rate (EER) on the Tongji dataset, outperforming previous\nmethods and demonstrating the efficacy of incorporating relational structure\ninto the palmprint matching process.", "published": "2025-04-15 15:46:17", "link": "http://arxiv.org/abs/2504.11306v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Autoregressive Distillation of Diffusion Transformers", "abstract": "Diffusion models with transformer architectures have demonstrated promising\ncapabilities in generating high-fidelity images and scalability for high\nresolution. However, iterative sampling process required for synthesis is very\nresource-intensive. A line of work has focused on distilling solutions to\nprobability flow ODEs into few-step student models. Nevertheless, existing\nmethods have been limited by their reliance on the most recent denoised samples\nas input, rendering them susceptible to exposure bias. To address this\nlimitation, we propose AutoRegressive Distillation (ARD), a novel approach that\nleverages the historical trajectory of the ODE to predict future steps. ARD\noffers two key benefits: 1) it mitigates exposure bias by utilizing a predicted\nhistorical trajectory that is less susceptible to accumulated errors, and 2) it\nleverages the previous history of the ODE trajectory as a more effective source\nof coarse-grained information. ARD modifies the teacher transformer\narchitecture by adding token-wise time embedding to mark each input from the\ntrajectory history and employs a block-wise causal attention mask for training.\nFurthermore, incorporating historical inputs only in lower transformer layers\nenhances performance and efficiency. We validate the effectiveness of ARD in a\nclass-conditioned generation on ImageNet and T2I synthesis. Our model achieves\na $5\\times$ reduction in FID degradation compared to the baseline methods while\nrequiring only 1.1\\% extra FLOPs on ImageNet-256. Moreover, ARD reaches FID of\n1.84 on ImageNet-256 in merely 4 steps and outperforms the publicly available\n1024p text-to-image distilled models in prompt adherence score with a minimal\ndrop in FID compared to the teacher. Project page:\nhttps://github.com/alsdudrla10/ARD.", "published": "2025-04-15 15:33:49", "link": "http://arxiv.org/abs/2504.11295v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "UniAnimate-DiT: Human Image Animation with Large-Scale Video Diffusion Transformer", "abstract": "This report presents UniAnimate-DiT, an advanced project that leverages the\ncutting-edge and powerful capabilities of the open-source Wan2.1 model for\nconsistent human image animation. Specifically, to preserve the robust\ngenerative capabilities of the original Wan2.1 model, we implement Low-Rank\nAdaptation (LoRA) technique to fine-tune a minimal set of parameters,\nsignificantly reducing training memory overhead. A lightweight pose encoder\nconsisting of multiple stacked 3D convolutional layers is designed to encode\nmotion information of driving poses. Furthermore, we adopt a simple\nconcatenation operation to integrate the reference appearance into the model\nand incorporate the pose information of the reference image for enhanced pose\nalignment. Experimental results show that our approach achieves visually\nappearing and temporally consistent high-fidelity animations. Trained on 480p\n(832x480) videos, UniAnimate-DiT demonstrates strong generalization\ncapabilities to seamlessly upscale to 720P (1280x720) during inference. The\ntraining and inference code is publicly available at\nhttps://github.com/ali-vilab/UniAnimate-DiT.", "published": "2025-04-15 15:29:11", "link": "http://arxiv.org/abs/2504.11289v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Efficient Medical Image Restoration via Reliability Guided Learning in Frequency Domain", "abstract": "Medical image restoration tasks aim to recover high-quality images from\ndegraded observations, exhibiting emergent desires in many clinical scenarios,\nsuch as low-dose CT image denoising, MRI super-resolution, and MRI artifact\nremoval. Despite the success achieved by existing deep learning-based\nrestoration methods with sophisticated modules, they struggle with rendering\ncomputationally-efficient reconstruction results. Moreover, they usually ignore\nthe reliability of the restoration results, which is much more urgent in\nmedical systems. To alleviate these issues, we present LRformer, a Lightweight\nTransformer-based method via Reliability-guided learning in the frequency\ndomain. Specifically, inspired by the uncertainty quantification in Bayesian\nneural networks (BNNs), we develop a Reliable Lesion-Semantic Prior Producer\n(RLPP). RLPP leverages Monte Carlo (MC) estimators with stochastic sampling\noperations to generate sufficiently-reliable priors by performing multiple\ninferences on the foundational medical image segmentation model, MedSAM.\nAdditionally, instead of directly incorporating the priors in the spatial\ndomain, we decompose the cross-attention (CA) mechanism into real symmetric and\nimaginary anti-symmetric parts via fast Fourier transform (FFT), resulting in\nthe design of the Guided Frequency Cross-Attention (GFCA) solver. By leveraging\nthe conjugated symmetric property of FFT, GFCA reduces the computational\ncomplexity of naive CA by nearly half. Extensive experimental results in\nvarious tasks demonstrate the superiority of the proposed LRformer in both\neffectiveness and efficiency.", "published": "2025-04-15 15:26:28", "link": "http://arxiv.org/abs/2504.11286v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Distillation-Supervised Convolutional Low-Rank Adaptation for Efficient Image Super-Resolution", "abstract": "Convolutional neural networks (CNNs) have been widely used in efficient image\nsuper-resolution. However, for CNN-based methods, performance gains often\nrequire deeper networks and larger feature maps, which increase complexity and\ninference costs. Inspired by LoRA's success in fine-tuning large language\nmodels, we explore its application to lightweight models and propose\nDistillation-Supervised Convolutional Low-Rank Adaptation (DSCLoRA), which\nimproves model performance without increasing architectural complexity or\ninference costs. Specifically, we integrate ConvLoRA into the efficient SR\nnetwork SPAN by replacing the SPAB module with the proposed SConvLB module and\nincorporating ConvLoRA layers into both the pixel shuffle block and its\npreceding convolutional layer. DSCLoRA leverages low-rank decomposition for\nparameter updates and employs a spatial feature affinity-based knowledge\ndistillation strategy to transfer second-order statistical information from\nteacher models (pre-trained SPAN) to student models (ours). This method\npreserves the core knowledge of lightweight models and facilitates optimal\nsolution discovery under certain conditions. Experiments on benchmark datasets\nshow that DSCLoRA improves PSNR and SSIM over SPAN while maintaining its\nefficiency and competitive image quality. Notably, DSCLoRA ranked first in the\nOverall Performance Track of the NTIRE 2025 Efficient Super-Resolution\nChallenge. Our code and models are made publicly available at\nhttps://github.com/Yaozzz666/DSCF-SR.", "published": "2025-04-15 15:12:57", "link": "http://arxiv.org/abs/2504.11271v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Enhanced Small Target Detection via Multi-Modal Fusion and Attention Mechanisms: A YOLOv5 Approach", "abstract": "With the rapid development of information technology, modern warfare\nincreasingly relies on intelligence, making small target detection critical in\nmilitary applications. The growing demand for efficient, real-time detection\nhas created challenges in identifying small targets in complex environments due\nto interference. To address this, we propose a small target detection method\nbased on multi-modal image fusion and attention mechanisms. This method\nleverages YOLOv5, integrating infrared and visible light data along with a\nconvolutional attention module to enhance detection performance. The process\nbegins with multi-modal dataset registration using feature point matching,\nensuring accurate network training. By combining infrared and visible light\nfeatures with attention mechanisms, the model improves detection accuracy and\nrobustness. Experimental results on anti-UAV and Visdrone datasets demonstrate\nthe effectiveness and practicality of our approach, achieving superior\ndetection results for small and dim targets.", "published": "2025-04-15 15:02:10", "link": "http://arxiv.org/abs/2504.11262v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Cryo-em images are intrinsically low dimensional", "abstract": "Simulation-based inference provides a powerful framework for cryo-electron\nmicroscopy, employing neural networks in methods like CryoSBI to infer\nbiomolecular conformations via learned latent representations. This latent\nspace represents a rich opportunity, encoding valuable information about the\nphysical system and the inference process. Harnessing this potential hinges on\nunderstanding the underlying geometric structure of these representations. We\ninvestigate this structure by applying manifold learning techniques to CryoSBI\nrepresentations of hemagglutinin (simulated and experimental). We reveal that\nthese high-dimensional data inherently populate low-dimensional, smooth\nmanifolds, with simulated data effectively covering the experimental\ncounterpart. By characterizing the manifold's geometry using Diffusion Maps and\nidentifying its principal axes of variation via coordinate interpretation\nmethods, we establish a direct link between the latent structure and key\nphysical parameters. Discovering this intrinsic low-dimensionality and\ninterpretable geometric organization not only validates the CryoSBI approach\nbut enables us to learn more from the data structure and provides opportunities\nfor improving future inference strategies by exploiting this revealed manifold\ngeometry.", "published": "2025-04-15 14:46:25", "link": "http://arxiv.org/abs/2504.11249v1", "categories": ["q-bio.QM", "cs.CV", "cs.LG", "q-bio.BM", "stat.ML"], "primary_category": "q-bio.QM"}
{"title": "Next-Future: Sample-Efficient Policy Learning for Robotic-Arm Tasks", "abstract": "Hindsight Experience Replay (HER) is widely regarded as the state-of-the-art\nalgorithm for achieving sample-efficient multi-goal reinforcement learning (RL)\nin robotic manipulation tasks with binary rewards. HER facilitates learning\nfrom failed attempts by replaying trajectories with redefined goals. However,\nit relies on a heuristic-based replay method that lacks a principled framework.\nTo address this limitation, we introduce a novel replay strategy,\n\"Next-Future\", which focuses on rewarding single-step transitions. This\napproach significantly enhances sample efficiency and accuracy in learning\nmulti-goal Markov decision processes (MDPs), particularly under stringent\naccuracy requirements -- a critical aspect for performing complex and precise\nrobotic-arm tasks. We demonstrate the efficacy of our method by highlighting\nhow single-step learning enables improved value approximation within the\nmulti-goal RL framework. The performance of the proposed replay strategy is\nevaluated across eight challenging robotic manipulation tasks, using ten random\nseeds for training. Our results indicate substantial improvements in sample\nefficiency for seven out of eight tasks and higher success rates in six tasks.\nFurthermore, real-world experiments validate the practical feasibility of the\nlearned policies, demonstrating the potential of \"Next-Future\" in solving\ncomplex robotic-arm tasks.", "published": "2025-04-15 14:45:51", "link": "http://arxiv.org/abs/2504.11247v1", "categories": ["cs.RO", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Leveraging multimodal explanatory annotations for video interpretation with Modality Specific Dataset", "abstract": "We examine the impact of concept-informed supervision on multimodal video\ninterpretation models using MOByGaze, a dataset containing human-annotated\nexplanatory concepts. We introduce Concept Modality Specific Datasets (CMSDs),\nwhich consist of data subsets categorized by the modality (visual, textual, or\naudio) of annotated concepts. Models trained on CMSDs outperform those using\ntraditional legacy training in both early and late fusion approaches. Notably,\nthis approach enables late fusion models to achieve performance close to that\nof early fusion models. These findings underscore the importance of\nmodality-specific annotations in developing robust, self-explainable video\nmodels and contribute to advancing interpretable multimodal learning in complex\nvideo analysis.", "published": "2025-04-15 14:33:25", "link": "http://arxiv.org/abs/2504.11232v1", "categories": ["cs.CV", "cs.MM"], "primary_category": "cs.CV"}
{"title": "CAP-Net: A Unified Network for 6D Pose and Size Estimation of Categorical Articulated Parts from a Single RGB-D Image", "abstract": "This paper tackles category-level pose estimation of articulated objects in\nrobotic manipulation tasks and introduces a new benchmark dataset. While recent\nmethods estimate part poses and sizes at the category level, they often rely on\ngeometric cues and complex multi-stage pipelines that first segment parts from\nthe point cloud, followed by Normalized Part Coordinate Space (NPCS) estimation\nfor 6D poses. These approaches overlook dense semantic cues from RGB images,\nleading to suboptimal accuracy, particularly for objects with small parts. To\naddress these limitations, we propose a single-stage Network, CAP-Net, for\nestimating the 6D poses and sizes of Categorical Articulated Parts. This method\ncombines RGB-D features to generate instance segmentation and NPCS\nrepresentations for each part in an end-to-end manner. CAP-Net uses a unified\nnetwork to simultaneously predict point-wise class labels, centroid offsets,\nand NPCS maps. A clustering algorithm then groups points of the same predicted\nclass based on their estimated centroid distances to isolate each part.\nFinally, the NPCS region of each part is aligned with the point cloud to\nrecover its final pose and size. To bridge the sim-to-real domain gap, we\nintroduce the RGBD-Art dataset, the largest RGB-D articulated dataset to date,\nfeaturing photorealistic RGB images and depth noise simulated from real\nsensors. Experimental evaluations on the RGBD-Art dataset demonstrate that our\nmethod significantly outperforms the state-of-the-art approach. Real-world\ndeployments of our model in robotic tasks underscore its robustness and\nexceptional sim-to-real transfer capabilities, confirming its substantial\npractical utility. Our dataset, code and pre-trained models are available on\nthe project page.", "published": "2025-04-15 14:30:26", "link": "http://arxiv.org/abs/2504.11230v1", "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV"}
{"title": "3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians", "abstract": "3D affordance reasoning is essential in associating human instructions with\nthe functional regions of 3D objects, facilitating precise, task-oriented\nmanipulations in embodied AI. However, current methods, which predominantly\ndepend on sparse 3D point clouds, exhibit limited generalizability and\nrobustness due to their sensitivity to coordinate variations and the inherent\nsparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers\nhigh-fidelity, real-time rendering with minimal computational overhead by\nrepresenting scenes as dense, continuous distributions. This positions 3DGS as\na highly effective approach for capturing fine-grained affordance details and\nimproving recognition accuracy. Nevertheless, its full potential remains\nlargely untapped due to the absence of large-scale, 3DGS-specific affordance\ndatasets. To overcome these limitations, we present 3DAffordSplat, the first\nlarge-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning.\nThis dataset includes 23,677 Gaussian instances, 8,354 point cloud instances,\nand 6,631 manually annotated affordance labels, encompassing 21 object\ncategories and 18 affordance types. Building upon this dataset, we introduce\nAffordSplatNet, a novel model specifically designed for affordance reasoning\nusing 3DGS representations. AffordSplatNet features an innovative cross-modal\nstructure alignment module that exploits structural consistency priors to align\n3D point cloud and 3DGS representations, resulting in enhanced affordance\nrecognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat\ndataset significantly advances affordance learning within the 3DGS domain,\nwhile AffordSplatNet consistently outperforms existing methods across both seen\nand unseen settings, highlighting its robust generalization capabilities.", "published": "2025-04-15 14:21:47", "link": "http://arxiv.org/abs/2504.11218v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Focal Split: Untethered Snapshot Depth from Differential Defocus", "abstract": "We introduce Focal Split, a handheld, snapshot depth camera with fully\nonboard power and computing based on depth-from-differential-defocus (DfDD).\nFocal Split is passive, avoiding power consumption of light sources. Its\nachromatic optical system simultaneously forms two differentially defocused\nimages of the scene, which can be independently captured using two photosensors\nin a snapshot. The data processing is based on the DfDD theory, which\nefficiently computes a depth and a confidence value for each pixel with only\n500 floating point operations (FLOPs) per pixel from the camera measurements.\nWe demonstrate a Focal Split prototype, which comprises a handheld custom\ncamera system connected to a Raspberry Pi 5 for real-time data processing. The\nsystem consumes 4.9 W and is powered on a 5 V, 10,000 mAh battery. The\nprototype can measure objects with distances from 0.4 m to 1.2 m, outputting\n480$\\times$360 sparse depth maps at 2.1 frames per second (FPS) using\nunoptimized Python scripts. Focal Split is DIY friendly. A comprehensive guide\nto building your own Focal Split depth camera, code, and additional data can be\nfound at https://focal-split.qiguo.org.", "published": "2025-04-15 14:01:36", "link": "http://arxiv.org/abs/2504.11202v1", "categories": ["cs.CV", "eess.IV", "eess.SP", "68U10", "I.4.8"], "primary_category": "cs.CV"}
{"title": "Video Summarization with Large Language Models", "abstract": "The exponential increase in video content poses significant challenges in\nterms of efficient navigation, search, and retrieval, thus requiring advanced\nvideo summarization techniques. Existing video summarization methods, which\nheavily rely on visual features and temporal dynamics, often fail to capture\nthe semantics of video content, resulting in incomplete or incoherent\nsummaries. To tackle the challenge, we propose a new video summarization\nframework that leverages the capabilities of recent Large Language Models\n(LLMs), expecting that the knowledge learned from massive data enables LLMs to\nevaluate video frames in a manner that better aligns with diverse semantics and\nhuman judgments, effectively addressing the inherent subjectivity in defining\nkeyframes. Our method, dubbed LLM-based Video Summarization (LLMVS), translates\nvideo frames into a sequence of captions using a Muti-modal Large Language\nModel (M-LLM) and then assesses the importance of each frame using an LLM,\nbased on the captions in its local context. These local importance scores are\nrefined through a global attention mechanism in the entire context of video\ncaptions, ensuring that our summaries effectively reflect both the details and\nthe overarching narrative. Our experimental results demonstrate the superiority\nof the proposed method over existing ones in standard benchmarks, highlighting\nthe potential of LLMs in the processing of multimedia content.", "published": "2025-04-15 13:56:14", "link": "http://arxiv.org/abs/2504.11199v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning", "abstract": "Vision-language models (VLMs), such as CLIP, have gained significant\npopularity as foundation models, with numerous fine-tuning methods developed to\nenhance performance on downstream tasks. However, due to their inherent\nvulnerability and the common practice of selecting from a limited set of\nopen-source models, VLMs suffer from a higher risk of adversarial attacks than\ntraditional vision models. Existing defense techniques typically rely on\nadversarial fine-tuning during training, which requires labeled data and lacks\nof flexibility for downstream tasks. To address these limitations, we propose\nrobust test-time prompt tuning (R-TPT), which mitigates the impact of\nadversarial attacks during the inference stage. We first reformulate the\nclassic marginal entropy objective by eliminating the term that introduces\nconflicts under adversarial conditions, retaining only the pointwise entropy\nminimization. Furthermore, we introduce a plug-and-play reliability-based\nweighted ensembling strategy, which aggregates useful information from reliable\naugmented views to strengthen the defense. R-TPT enhances defense against\nadversarial attacks without requiring labeled training data while offering high\nflexibility for inference tasks. Extensive experiments on widely used\nbenchmarks with various attacks demonstrate the effectiveness of R-TPT. The\ncode is available in https://github.com/TomSheng21/R-TPT.", "published": "2025-04-15 13:49:31", "link": "http://arxiv.org/abs/2504.11195v1", "categories": ["cs.LG", "cs.CR", "cs.CV"], "primary_category": "cs.LG"}
{"title": "TerraMesh: A Planetary Mosaic of Multimodal Earth Observation Data", "abstract": "Large-scale foundation models in Earth Observation can learn versatile,\nlabel-efficient representations by leveraging massive amounts of unlabeled\ndata. However, existing public datasets are often limited in scale, geographic\ncoverage, or sensor variety. We introduce TerraMesh, a new globally diverse,\nmultimodal dataset combining optical, synthetic aperture radar, elevation, and\nland-cover modalities in an Analysis-Ready Data format. TerraMesh includes over\n9 million samples with eight spatiotemporal aligned modalities, enabling\nlarge-scale pre-training and fostering robust cross-modal correlation learning.\nWe provide detailed data processing steps, comprehensive statistics, and\nempirical evidence demonstrating improved model performance when pre-trained on\nTerraMesh. The dataset will be made publicly available with a permissive\nlicense.", "published": "2025-04-15 13:20:35", "link": "http://arxiv.org/abs/2504.11172v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "YOLO-RS: Remote Sensing Enhanced Crop Detection Methods", "abstract": "With the rapid development of remote sensing technology, crop classification\nand health detection based on deep learning have gradually become a research\nhotspot. However, the existing target detection methods show poor performance\nwhen dealing with small targets in remote sensing images, especially in the\ncase of complex background and image mixing, which is difficult to meet the\npractical application requirementsite. To address this problem, a novel target\ndetection model YOLO-RS is proposed in this paper. The model is based on the\nlatest Yolov11 which significantly enhances the detection of small targets by\nintroducing the Context Anchor Attention (CAA) mechanism and an efficient\nmulti-field multi-scale feature fusion network. YOLO-RS adopts a bidirectional\nfeature fusion strategy in the feature fusion process, which effectively\nenhances the model's performance in the detection of small targets. Small\ntarget detection. Meanwhile, the ACmix module at the end of the model backbone\nnetwork solves the category imbalance problem by adaptively adjusting the\ncontrast and sample mixing, thus enhancing the detection accuracy in complex\nscenes. In the experiments on the PDT remote sensing crop health detection\ndataset and the CWC crop classification dataset, YOLO-RS improves both the\nrecall and the mean average precision (mAP) by about 2-3\\% or so compared with\nthe existing state-of-the-art methods, while the F1-score is also significantly\nimproved. Moreover, the computational complexity of the model only increases by\nabout 5.2 GFLOPs, indicating its significant advantages in both performance and\nefficiency. The experimental results validate the effectiveness and application\npotential of YOLO-RS in the task of detecting small targets in remote sensing\nimages.", "published": "2025-04-15 13:13:22", "link": "http://arxiv.org/abs/2504.11165v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "TSAL: Few-shot Text Segmentation Based on Attribute Learning", "abstract": "Recently supervised learning rapidly develops in scene text segmentation.\nHowever, the lack of high-quality datasets and the high cost of pixel\nannotation greatly limit the development of them. Considering the\nwell-performed few-shot learning methods for downstream tasks, we investigate\nthe application of the few-shot learning method to scene text segmentation. We\npropose TSAL, which leverages CLIP's prior knowledge to learn text attributes\nfor segmentation. To fully utilize the semantic and texture information in the\nimage, a visual-guided branch is proposed to separately extract text and\nbackground features. To reduce data dependency and improve text detection\naccuracy, the adaptive prompt-guided branch employs effective adaptive prompt\ntemplates to capture various text attributes. To enable adaptive prompts\ncapture distinctive text features and complex background distribution, we\npropose Adaptive Feature Alignment module(AFA). By aligning learnable tokens of\ndifferent attributes with visual features and prompt prototypes, AFA enables\nadaptive prompts to capture both general and distinctive attribute information.\nTSAL can capture the unique attributes of text and achieve precise segmentation\nusing only few images. Experiments demonstrate that our method achieves SOTA\nperformance on multiple text segmentation datasets under few-shot settings and\nshow great potential in text-related domains.", "published": "2025-04-15 13:12:42", "link": "http://arxiv.org/abs/2504.11164v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SAR-to-RGB Translation with Latent Diffusion for Earth Observation", "abstract": "Earth observation satellites like Sentinel-1 (S1) and Sentinel-2 (S2) provide\ncomplementary remote sensing (RS) data, but S2 images are often unavailable due\nto cloud cover or data gaps. To address this, we propose a diffusion model\n(DM)-based approach for SAR-to-RGB translation, generating synthetic optical\nimages from SAR inputs. We explore three different setups: two using Standard\nDiffusion, which reconstruct S2 images by adding and removing noise (one\nwithout and one with class conditioning), and one using Cold Diffusion, which\nblends S2 with S1 before removing the SAR signal. We evaluate the generated\nimages in downstream tasks, including land cover classification and cloud\nremoval. While generated images may not perfectly replicate real S2 data, they\nstill provide valuable information. Our results show that class conditioning\nimproves classification accuracy, while cloud removal performance remains\ncompetitive despite our approach not being optimized for it. Interestingly,\ndespite exhibiting lower perceptual quality, the Cold Diffusion setup performs\nwell in land cover classification, suggesting that traditional quantitative\nevaluation metrics may not fully reflect the practical utility of generated\nimages. Our findings highlight the potential of DMs for SAR-to-RGB translation\nin RS applications where RGB images are missing.", "published": "2025-04-15 12:58:30", "link": "http://arxiv.org/abs/2504.11154v1", "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV"}
{"title": "GC-GAT: Multimodal Vehicular Trajectory Prediction using Graph Goal Conditioning and Cross-context Attention", "abstract": "Predicting future trajectories of surrounding vehicles heavily relies on what\ncontextual information is given to a motion prediction model. The context\nitself can be static (lanes, regulatory elements, etc) or dynamic (traffic\nparticipants). This paper presents a lane graph-based motion prediction model\nthat first predicts graph-based goal proposals and later fuses them with cross\nattention over multiple contextual elements. We follow the famous\nencoder-interactor-decoder architecture where the encoder encodes scene context\nusing lightweight Gated Recurrent Units, the interactor applies cross-context\nattention over encoded scene features and graph goal proposals, and the decoder\nregresses multimodal trajectories via Laplacian Mixture Density Network from\nthe aggregated encodings. Using cross-attention over graph-based goal proposals\ngives robust trajectory estimates since the model learns to attend to future\ngoal-relevant scene elements for the intended agent. We evaluate our work on\nnuScenes motion prediction dataset, achieving state-of-the-art results.", "published": "2025-04-15 12:53:07", "link": "http://arxiv.org/abs/2504.11150v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Taming Consistency Distillation for Accelerated Human Image Animation", "abstract": "Recent advancements in human image animation have been propelled by video\ndiffusion models, yet their reliance on numerous iterative denoising steps\nresults in high inference costs and slow speeds. An intuitive solution involves\nadopting consistency models, which serve as an effective acceleration paradigm\nthrough consistency distillation. However, simply employing this strategy in\nhuman image animation often leads to quality decline, including visual\nblurring, motion degradation, and facial distortion, particularly in dynamic\nregions. In this paper, we propose the DanceLCM approach complemented by\nseveral enhancements to improve visual quality and motion continuity at\nlow-step regime: (1) segmented consistency distillation with an auxiliary\nlight-weight head to incorporate supervision from real video latents,\nmitigating cumulative errors resulting from single full-trajectory generation;\n(2) a motion-focused loss to centre on motion regions, and explicit injection\nof facial fidelity features to improve face authenticity. Extensive qualitative\nand quantitative experiments demonstrate that DanceLCM achieves results\ncomparable to state-of-the-art video diffusion models with a mere 2-4 inference\nsteps, significantly reducing the inference burden without compromising video\nquality. The code and models will be made publicly available.", "published": "2025-04-15 12:44:53", "link": "http://arxiv.org/abs/2504.11143v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Visual Re-Ranking with Non-Visual Side Information", "abstract": "The standard approach for visual place recognition is to use global image\ndescriptors to retrieve the most similar database images for a given query\nimage. The results can then be further improved with re-ranking methods that\nre-order the top scoring images. However, existing methods focus on re-ranking\nbased on the same image descriptors that were used for the initial retrieval,\nwhich we argue provides limited additional signal.\n  In this work we propose Generalized Contextual Similarity Aggregation (GCSA),\nwhich is a graph neural network-based re-ranking method that, in addition to\nthe visual descriptors, can leverage other types of available side information.\nThis can for example be other sensor data (such as signal strength of nearby\nWiFi or BlueTooth endpoints) or geometric properties such as camera poses for\ndatabase images. In many applications this information is already present or\ncan be acquired with low effort. Our architecture leverages the concept of\naffinity vectors to allow for a shared encoding of the heterogeneous\nmulti-modal input. Two large-scale datasets, covering both outdoor and indoor\nlocalization scenarios, are utilized for training and evaluation. In\nexperiments we show significant improvement not only on image retrieval\nmetrics, but also for the downstream visual localization task.", "published": "2025-04-15 12:37:16", "link": "http://arxiv.org/abs/2504.11134v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "K-means Enhanced Density Gradient Analysis for Urban and Transport Metrics Using Multi-Modal Satellite Imagery", "abstract": "This paper presents a novel computational approach for evaluating urban\nmetrics through density gradient analysis using multi-modal satellite imagery,\nwith applications including public transport and other urban systems. By\ncombining optical and Synthetic Aperture Radar (SAR) data, we develop a method\nto segment urban areas, identify urban centers, and quantify density gradients.\nOur approach calculates two key metrics: the density gradient coefficient\n($\\alpha$) and the minimum effective distance (LD) at which density reaches a\ntarget threshold. We further employ machine learning techniques, specifically\nK-means clustering, to objectively identify uniform and high-variability\nregions within density gradient plots. We demonstrate that these metrics\nprovide an effective screening tool for public transport analyses by revealing\nthe underlying urban structure. Through comparative analysis of two\nrepresentative cities with contrasting urban morphologies (monocentric vs\npolycentric), we establish relationships between density gradient\ncharacteristics and public transport network topologies. Cities with clear\ndensity peaks in their gradient plots indicate distinct urban centers requiring\ndifferent transport strategies than those with more uniform density\ndistributions. This methodology offers urban planners a cost-effective,\nglobally applicable approach to preliminary public transport assessment using\nfreely available satellite data. The complete implementation, with additional\nexamples and documentation, is available in an open-source repository under the\nMIT license at https://github.com/nexri/Satellite-Imagery-Urban-Analysis.", "published": "2025-04-15 12:25:42", "link": "http://arxiv.org/abs/2504.11128v1", "categories": ["cs.CV", "cs.LG", "eess.IV", "I.4.6, I.4.7, I.4.3"], "primary_category": "cs.CV"}
{"title": "Revealing Covert Attention by Analyzing Human and Reinforcement Learning Agent Gameplay", "abstract": "This study introduces a novel method for revealing human covert attention\npatterns using gameplay data alone, utilizing offline attention techniques from\nreinforcement learning (RL). We propose the contextualized, task-relevant (CTR)\nattention network, which generates attention maps from both human and RL agent\ngameplay in Atari environments. These maps are sparse yet retain the necessary\ninformation for the current player's decision making. We compare the\nCTR-derived attention maps with a temporally integrated overt attention (TIOA)\nmodel based on eye-tracking data, serving as a point of comparison and\ndiscussion. Visual inspection reveals distinct attention patterns: human CTR\nmaps focus on the player and rather nearby opponents, occasionally shifting\nbetween stronger focus and broader views - sometimes even attending to empty\nspace ahead. In contrast, agent maps maintain a consistent broad focus on most\nobjects, including distant ones and the player. Quantitative analysis further\ndemonstrates that human CTR maps align more closely with TIOA than agent maps\ndo. Our findings indicate that the CTR attention network can effectively reveal\nhuman covert attention patterns from gameplay alone, without the need for\nadditional data like brain activity recordings. This work contributes to\nunderstanding human-agent attention differences and enables the development of\nRL agents augmented with human covert attention.", "published": "2025-04-15 12:07:14", "link": "http://arxiv.org/abs/2504.11118v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Flyweight FLIM Networks for Salient Object Detection in Biomedical Images", "abstract": "Salient Object Detection (SOD) with deep learning often requires substantial\ncomputational resources and large annotated datasets, making it impractical for\nresource-constrained applications. Lightweight models address computational\ndemands but typically strive in complex and scarce labeled-data scenarios.\nFeature Learning from Image Markers (FLIM) learns an encoder's convolutional\nkernels among image patches extracted from discriminative regions marked on a\nfew representative images, dismissing large annotated datasets, pretraining,\nand backpropagation. Such a methodology exploits information redundancy\ncommonly found in biomedical image applications. This study presents methods to\nlearn dilated-separable convolutional kernels and multi-dilation layers without\nbackpropagation for FLIM networks. It also proposes a novel network\nsimplification method to reduce kernel redundancy and encoder size. By\ncombining a FLIM encoder with an adaptive decoder, a concept recently\nintroduced to estimate a pointwise convolution per image, this study presents\nvery efficient (named flyweight) SOD models for biomedical images. Experimental\nresults in challenging datasets demonstrate superior efficiency and\neffectiveness to lightweight models. By requiring significantly fewer\nparameters and floating-point operations, the results show competitive\neffectiveness to heavyweight models. These advances highlight the potential of\nFLIM networks for data-limited and resource-constrained applications with\ninformation redundancy.", "published": "2025-04-15 11:57:40", "link": "http://arxiv.org/abs/2504.11112v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "S$^2$Teacher: Step-by-step Teacher for Sparsely Annotated Oriented Object Detection", "abstract": "Although fully-supervised oriented object detection has made significant\nprogress in multimodal remote sensing image understanding, it comes at the cost\nof labor-intensive annotation. Recent studies have explored weakly and\nsemi-supervised learning to alleviate this burden. However, these methods\noverlook the difficulties posed by dense annotations in complex remote sensing\nscenes. In this paper, we introduce a novel setting called sparsely annotated\noriented object detection (SAOOD), which only labels partial instances, and\npropose a solution to address its challenges. Specifically, we focus on two key\nissues in the setting: (1) sparse labeling leading to overfitting on limited\nforeground representations, and (2) unlabeled objects (false negatives)\nconfusing feature learning. To this end, we propose the S$^2$Teacher, a novel\nmethod that progressively mines pseudo-labels for unlabeled objects, from easy\nto hard, to enhance foreground representations. Additionally, it reweights the\nloss of unlabeled objects to mitigate their impact during training. Extensive\nexperiments demonstrate that S$^2$Teacher not only significantly improves\ndetector performance across different sparse annotation levels but also\nachieves near-fully-supervised performance on the DOTA dataset with only 10%\nannotation instances, effectively balancing detection accuracy with annotation\nefficiency. The code will be public.", "published": "2025-04-15 11:57:00", "link": "http://arxiv.org/abs/2504.11111v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Token-Level Constraint Boundary Search for Jailbreaking Text-to-Image Models", "abstract": "Recent advancements in Text-to-Image (T2I) generation have significantly\nenhanced the realism and creativity of generated images. However, such powerful\ngenerative capabilities pose risks related to the production of inappropriate\nor harmful content. Existing defense mechanisms, including prompt checkers and\npost-hoc image checkers, are vulnerable to sophisticated adversarial attacks.\nIn this work, we propose TCBS-Attack, a novel query-based black-box jailbreak\nattack that searches for tokens located near the decision boundaries defined by\ntext and image checkers. By iteratively optimizing tokens near these\nboundaries, TCBS-Attack generates semantically coherent adversarial prompts\ncapable of bypassing multiple defensive layers in T2I models. Extensive\nexperiments demonstrate that our method consistently outperforms\nstate-of-the-art jailbreak attacks across various T2I models, including\nsecurely trained open-source models and commercial online services like DALL-E\n3. TCBS-Attack achieves an ASR-4 of 45\\% and an ASR-1 of 21\\% on jailbreaking\nfull-chain T2I models, significantly surpassing baseline methods.", "published": "2025-04-15 11:53:40", "link": "http://arxiv.org/abs/2504.11106v1", "categories": ["cs.CV", "cs.CR"], "primary_category": "cs.CV"}
{"title": "Consensus Entropy: Harnessing Multi-VLM Agreement for Self-Verifying and Self-Improving OCR", "abstract": "The Optical Character Recognition (OCR) task is important for evaluating\nVision-Language Models (VLMs) and providing high-quality data sources for LLM\ntraining data. While state-of-the-art VLMs show improved average OCR accuracy,\nthey still struggle with sample-level quality degradation and lack reliable\nautomatic detection of low-quality outputs. We introduce Consensus Entropy\n(CE), a training-free post-inference method that quantifies OCR uncertainty by\naggregating outputs from multiple VLMs. Our approach exploits a key insight:\ncorrect VLM OCR predictions converge in output space while errors diverge. We\ndevelop a lightweight multi-model framework that effectively identifies\nproblematic samples, selects the best outputs and combines model strengths.\nExperiments across multiple OCR benchmarks and VLMs demonstrate that CE\noutperforms VLM-as-judge approaches and single-model baselines at the same cost\nand achieves state-of-the-art results across multiple metrics. For instance,\nour solution demonstrates: achieving 15.2\\% higher F1 scores than VLM-as-judge\nmethods in quality verification, delivering 6.0\\% accuracy gains on\nmathematical calculation tasks, and requiring rephrasing only 7.3\\% of inputs\nwhile maintaining overall performance. Notably, the entire process requires\nneither training nor supervision while maintaining plug-and-play functionality\nthroughout.", "published": "2025-04-15 11:51:18", "link": "http://arxiv.org/abs/2504.11101v1", "categories": ["cs.CV", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Vivid4D: Improving 4D Reconstruction from Monocular Video by Video Inpainting", "abstract": "Reconstructing 4D dynamic scenes from casually captured monocular videos is\nvaluable but highly challenging, as each timestamp is observed from a single\nviewpoint. We introduce Vivid4D, a novel approach that enhances 4D monocular\nvideo synthesis by augmenting observation views - synthesizing multi-view\nvideos from a monocular input. Unlike existing methods that either solely\nleverage geometric priors for supervision or use generative priors while\noverlooking geometry, we integrate both. This reformulates view augmentation as\na video inpainting task, where observed views are warped into new viewpoints\nbased on monocular depth priors. To achieve this, we train a video inpainting\nmodel on unposed web videos with synthetically generated masks that mimic\nwarping occlusions, ensuring spatially and temporally consistent completion of\nmissing regions. To further mitigate inaccuracies in monocular depth priors, we\nintroduce an iterative view augmentation strategy and a robust reconstruction\nloss. Experiments demonstrate that our method effectively improves monocular 4D\nscene reconstruction and completion.", "published": "2025-04-15 11:38:14", "link": "http://arxiv.org/abs/2504.11092v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "InfoClus: Informative Clustering of High-dimensional Data Embeddings", "abstract": "Developing an understanding of high-dimensional data can be facilitated by\nvisualizing that data using dimensionality reduction. However, the\nlow-dimensional embeddings are often difficult to interpret. To facilitate the\nexploration and interpretation of low-dimensional embeddings, we introduce a\nnew concept named partitioning with explanations. The idea is to partition the\ndata shown through the embedding into groups, each of which is given a sparse\nexplanation using the original high-dimensional attributes. We introduce an\nobjective function that quantifies how much we can learn through observing the\nexplanations of the data partitioning, using information theory, and also how\ncomplex the explanations are. Through parameterization of the complexity, we\ncan tune the solutions towards the desired granularity. We propose InfoClus,\nwhich optimizes the partitioning and explanations jointly, through greedy\nsearch constrained over a hierarchical clustering. We conduct a qualitative and\nquantitative analysis of InfoClus on three data sets. We contrast the results\non the Cytometry data with published manual analysis results, and compare with\ntwo other recent methods for explaining embeddings (RVX and VERA). These\ncomparisons highlight that InfoClus has distinct advantages over existing\nprocedures and methods. We find that InfoClus can automatically create good\nstarting points for the analysis of dimensionality-reduction-based scatter\nplots.", "published": "2025-04-15 11:34:03", "link": "http://arxiv.org/abs/2504.11089v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Change State Space Models for Remote Sensing Change Detection", "abstract": "Despite their frequent use for change detection, both ConvNets and Vision\ntransformers (ViT) exhibit well-known limitations, namely the former struggle\nto model long-range dependencies while the latter are computationally\ninefficient, rendering them challenging to train on large-scale datasets.\nVision Mamba, an architecture based on State Space Models has emerged as an\nalternative addressing the aforementioned deficiencies and has been already\napplied to remote sensing change detection, though mostly as a feature\nextracting backbone. In this article the Change State Space Model is\nintroduced, that has been specifically designed for change detection by\nfocusing on the relevant changes between bi-temporal images, effectively\nfiltering out irrelevant information. By concentrating solely on the changed\nfeatures, the number of network parameters is reduced, enhancing significantly\ncomputational efficiency while maintaining high detection performance and\nrobustness against input degradation. The proposed model has been evaluated via\nthree benchmark datasets, where it outperformed ConvNets, ViTs, and Mamba-based\ncounterparts at a fraction of their computational complexity. The\nimplementation will be made available at https://github.com/Elman295/CSSM upon\nacceptance.", "published": "2025-04-15 11:25:10", "link": "http://arxiv.org/abs/2504.11080v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Improving fingerprint presentation attack detection by an approach integrated into the personal verification stage", "abstract": "Presentation Attack Detection (PAD) systems are usually designed\nindependently of the fingerprint verification system. While this can be\nacceptable for use cases where specific user templates are not predetermined,\nit represents a missed opportunity to enhance security in scenarios where\nintegrating PAD with the fingerprint verification system could significantly\nleverage users' templates, which are the real target of a potential\npresentation attack. This does not mean that a PAD should be specifically\ndesigned for such users; that would imply the availability of many enrolled\nusers' PAI and, consequently, complexity, time, and cost increase. On the\ncontrary, we propose to equip a basic PAD, designed according to the state of\nthe art, with an innovative add-on module called the Closeness Binary Code (CC)\nmodule. The term \"closeness\" refers to a peculiar property of the bona\nfide-related features: in an Euclidean feature space, genuine fingerprints tend\nto cluster in a specific pattern. First, samples from the same finger are close\nto each other, then samples from other fingers of the same user and finally,\nsamples from fingers of other users. This property is statistically verified in\nour previous publication, and further confirmed in this paper. It is\nindependent of the user population and the feature set class, which can be\nhandcrafted or deep network-based (embeddings). Therefore, the add-on can be\ndesigned without the need for the targeted user samples; moreover, it exploits\nher/his samples' \"closeness\" property during the verification stage. Extensive\nexperiments on benchmark datasets and state-of-the-art PAD methods confirm the\nbenefits of the proposed add-on, which can be easily coupled with the main PAD\nmodule integrated into the fingerprint verification system.", "published": "2025-04-15 11:01:06", "link": "http://arxiv.org/abs/2504.11066v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "UKDM: Underwater keypoint detection and matching using underwater image enhancement techniques", "abstract": "The purpose of this paper is to explore the use of underwater image\nenhancement techniques to improve keypoint detection and matching. By applying\nadvanced deep learning models, including generative adversarial networks and\nconvolutional neural networks, we aim to find the best method which improves\nthe accuracy of keypoint detection and the robustness of matching algorithms.\nWe evaluate the performance of these techniques on various underwater datasets,\ndemonstrating significant improvements over traditional methods.", "published": "2025-04-15 10:52:19", "link": "http://arxiv.org/abs/2504.11063v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Crane: Context-Guided Prompt Learning and Attention Refinement for Zero-Shot Anomaly Detections", "abstract": "Anomaly Detection (AD) involves identifying deviations from normal data\ndistributions and is critical in fields such as medical diagnostics and\nindustrial defect detection. Traditional AD methods typically require the\navailability of normal training samples; however, this assumption is not always\nfeasible, as collecting such data can be impractical. Additionally, these\nmethods often struggle to generalize across different domains. Recent\nadvancements, such as AnomalyCLIP and AdaCLIP, utilize the zero-shot\ngeneralization capabilities of CLIP but still face a performance gap between\nimage-level and pixel-level anomaly detection. To address this gap, we propose\na novel approach that conditions the prompts of the text encoder based on image\ncontext extracted from the vision encoder. Also, to capture fine-grained\nvariations more effectively, we have modified the CLIP vision encoder and\naltered the extraction of dense features. These changes ensure that the\nfeatures retain richer spatial and structural information for both normal and\nanomalous prompts. Our method achieves state-of-the-art performance, improving\nperformance by 2% to 29% across different metrics on 14 datasets. This\ndemonstrates its effectiveness in both image-level and pixel-level anomaly\ndetection.", "published": "2025-04-15 10:42:25", "link": "http://arxiv.org/abs/2504.11055v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Leveraging LLMs and attention-mechanism for automatic annotation of historical maps", "abstract": "Historical maps are essential resources that provide insights into the\ngeographical landscapes of the past. They serve as valuable tools for\nresearchers across disciplines such as history, geography, and urban studies,\nfacilitating the reconstruction of historical environments and the analysis of\nspatial transformations over time. However, when constrained to analogue or\nscanned formats, their interpretation is limited to humans and therefore not\nscalable. Recent advancements in machine learning, particularly in computer\nvision and large language models (LLMs), have opened new avenues for automating\nthe recognition and classification of features and objects in historical maps.\nIn this paper, we propose a novel distillation method that leverages LLMs and\nattention mechanisms for the automatic annotation of historical maps. LLMs are\nemployed to generate coarse classification labels for low-resolution historical\nimage patches, while attention mechanisms are utilized to refine these labels\nto higher resolutions. Experimental results demonstrate that the refined labels\nachieve a high recall of more than 90%. Additionally, the intersection over\nunion (IoU) scores--84.2% for Wood and 72.0% for Settlement--along with\nprecision scores of 87.1% and 79.5%, respectively, indicate that most labels\nare well-aligned with ground-truth annotations. Notably, these results were\nachieved without the use of fine-grained manual labels during training,\nunderscoring the potential of our approach for efficient and scalable\nhistorical map analysis.", "published": "2025-04-15 10:34:23", "link": "http://arxiv.org/abs/2504.11050v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Defending Against Frequency-Based Attacks with Diffusion Models", "abstract": "Adversarial training is a common strategy for enhancing model robustness\nagainst adversarial attacks. However, it is typically tailored to the specific\nattack types it is trained on, limiting its ability to generalize to unseen\nthreat models. Adversarial purification offers an alternative by leveraging a\ngenerative model to remove perturbations before classification. Since the\npurifier is trained independently of both the classifier and the threat models,\nit is better equipped to handle previously unseen attack scenarios. Diffusion\nmodels have proven highly effective for noise purification, not only in\ncountering pixel-wise adversarial perturbations but also in addressing\nnon-adversarial data shifts. In this study, we broaden the focus beyond\npixel-wise robustness to explore the extent to which purification can mitigate\nboth spectral and spatial adversarial attacks. Our findings highlight its\neffectiveness in handling diverse distortion patterns across low- to\nhigh-frequency regions.", "published": "2025-04-15 09:57:17", "link": "http://arxiv.org/abs/2504.11034v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Acquisition of high-quality images for camera calibration in robotics applications via speech prompts", "abstract": "Accurate intrinsic and extrinsic camera calibration can be an important\nprerequisite for robotic applications that rely on vision as input. While there\nis ongoing research on enabling camera calibration using natural images, many\nsystems in practice still rely on using designated calibration targets with\ne.g. checkerboard patterns or April tag grids. Once calibration images from\ndifferent perspectives have been acquired and feature descriptors detected,\nthose are typically used in an optimization process to minimize the geometric\nreprojection error. For this optimization to converge, input images need to be\nof sufficient quality and particularly sharpness; they should neither contain\nmotion blur nor rolling-shutter artifacts that can arise when the calibration\nboard was not static during image capture. In this work, we present a novel\ncalibration image acquisition technique controlled via voice commands recorded\nwith a clip-on microphone, that can be more robust and user-friendly than e.g.\ntriggering capture with a remote control, or filtering out blurry frames from a\nvideo sequence in postprocessing. To achieve this, we use a state-of-the-art\nspeech-to-text transcription model with accurate per-word timestamping to\ncapture trigger words with precise temporal alignment. Our experiments show\nthat the proposed method improves user experience by being fast and efficient,\nallowing us to successfully calibrate complex multi-camera setups.", "published": "2025-04-15 09:54:43", "link": "http://arxiv.org/abs/2504.11031v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Easy3D: A Simple Yet Effective Method for 3D Interactive Segmentation", "abstract": "The increasing availability of digital 3D environments, whether through\nimage-based 3D reconstruction, generation, or scans obtained by robots, is\ndriving innovation across various applications. These come with a significant\ndemand for 3D interaction, such as 3D Interactive Segmentation, which is useful\nfor tasks like object selection and manipulation. Additionally, there is a\npersistent need for solutions that are efficient, precise, and performing well\nacross diverse settings, particularly in unseen environments and with\nunfamiliar objects. In this work, we introduce a 3D interactive segmentation\nmethod that consistently surpasses previous state-of-the-art techniques on both\nin-domain and out-of-domain datasets. Our simple approach integrates a\nvoxel-based sparse encoder with a lightweight transformer-based decoder that\nimplements implicit click fusion, achieving superior performance and maximizing\nefficiency. Our method demonstrates substantial improvements on benchmark\ndatasets, including ScanNet, ScanNet++, S3DIS, and KITTI-360, and also on\nunseen geometric distributions such as the ones obtained by Gaussian Splatting.\nThe project web-page is available at https://simonelli-andrea.github.io/easy3d.", "published": "2025-04-15 09:49:51", "link": "http://arxiv.org/abs/2504.11024v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Meta-learning For Few-Shot Time Series Crop Type Classification: A Benchmark On The EuroCropsML Dataset", "abstract": "Spatial imbalances in crop type data pose significant challenges for accurate\nclassification in remote sensing applications. Algorithms aiming at\ntransferring knowledge from data-rich to data-scarce tasks have thus surged in\npopularity. However, despite their effectiveness in previous evaluations, their\nperformance in challenging real-world applications is unclear and needs to be\nevaluated. This study benchmarks transfer learning and several meta-learning\nalgorithms, including (First-Order) Model-Agnostic Meta-Learning ((FO)-MAML),\nAlmost No Inner Loop (ANIL), and Task-Informed Meta-Learning (TIML), on the\nreal-world EuroCropsML time series dataset, which combines farmer-reported crop\ndata with Sentinel-2 satellite observations from Estonia, Latvia, and Portugal.\nOur findings indicate that MAML-based meta-learning algorithms achieve slightly\nhigher accuracy compared to simpler transfer learning methods when applied to\ncrop type classification tasks in Estonia after pre-training on data from\nLatvia. However, this improvement comes at the cost of increased computational\ndemands and training time. Moreover, we find that the transfer of knowledge\nbetween geographically disparate regions, such as Estonia and Portugal, poses\nsignificant challenges to all investigated algorithms. These insights\nunderscore the trade-offs between accuracy and computational resource\nrequirements in selecting machine learning methods for real-world crop type\nclassification tasks and highlight the difficulties of transferring knowledge\nbetween different regions of the Earth. To facilitate future research in this\ndomain, we present the first comprehensive benchmark for evaluating transfer\nand meta-learning methods for crop type classification under real-world\nconditions. The corresponding code is publicly available at\nhttps://github.com/dida-do/eurocrops-meta-learning.", "published": "2025-04-15 09:47:57", "link": "http://arxiv.org/abs/2504.11022v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "DRIFT open dataset: A drone-derived intelligence for traffic analysis in urban environmen", "abstract": "Reliable traffic data are essential for understanding urban mobility and\ndeveloping effective traffic management strategies. This study introduces the\nDRone-derived Intelligence For Traffic analysis (DRIFT) dataset, a large-scale\nurban traffic dataset collected systematically from synchronized drone videos\nat approximately 250 meters altitude, covering nine interconnected\nintersections in Daejeon, South Korea. DRIFT provides high-resolution vehicle\ntrajectories that include directional information, processed through video\nsynchronization and orthomap alignment, resulting in a comprehensive dataset of\n81,699 vehicle trajectories. Through our DRIFT dataset, researchers can\nsimultaneously analyze traffic at multiple scales - from individual vehicle\nmaneuvers like lane-changes and safety metrics such as time-to-collision to\naggregate network flow dynamics across interconnected urban intersections. The\nDRIFT dataset is structured to enable immediate use without additional\npreprocessing, complemented by open-source models for object detection and\ntrajectory extraction, as well as associated analytical tools. DRIFT is\nexpected to significantly contribute to academic research and practical\napplications, such as traffic flow analysis and simulation studies. The dataset\nand related resources are publicly accessible at\nhttps://github.com/AIxMobility/The-DRIFT.", "published": "2025-04-15 09:43:13", "link": "http://arxiv.org/abs/2504.11019v1", "categories": ["cs.CV", "I.2.10; I.4.8; H.2.8; J.7"], "primary_category": "cs.CV"}
{"title": "AnimeDL-2M: Million-Scale AI-Generated Anime Image Detection and Localization in Diffusion Era", "abstract": "Recent advances in image generation, particularly diffusion models, have\nsignificantly lowered the barrier for creating sophisticated forgeries, making\nimage manipulation detection and localization (IMDL) increasingly challenging.\nWhile prior work in IMDL has focused largely on natural images, the anime\ndomain remains underexplored-despite its growing vulnerability to AI-generated\nforgeries. Misrepresentations of AI-generated images as hand-drawn artwork,\ncopyright violations, and inappropriate content modifications pose serious\nthreats to the anime community and industry. To address this gap, we propose\nAnimeDL-2M, the first large-scale benchmark for anime IMDL with comprehensive\nannotations. It comprises over two million images including real, partially\nmanipulated, and fully AI-generated samples. Experiments indicate that models\ntrained on existing IMDL datasets of natural images perform poorly when applied\nto anime images, highlighting a clear domain gap between anime and natural\nimages. To better handle IMDL tasks in anime domain, we further propose\nAniXplore, a novel model tailored to the visual characteristics of anime\nimagery. Extensive evaluations demonstrate that AniXplore achieves superior\nperformance compared to existing methods. Dataset and code can be found in\nhttps://flytweety.github.io/AnimeDL2M/.", "published": "2025-04-15 09:41:08", "link": "http://arxiv.org/abs/2504.11015v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "PraNet-V2: Dual-Supervised Reverse Attention for Medical Image Segmentation", "abstract": "Accurate medical image segmentation is essential for effective diagnosis and\ntreatment. Previously, PraNet-V1 was proposed to enhance polyp segmentation by\nintroducing a reverse attention (RA) module that utilizes background\ninformation. However, PraNet-V1 struggles with multi-class segmentation tasks.\nTo address this limitation, we propose PraNet-V2, which, compared to PraNet-V1,\neffectively performs a broader range of tasks including multi-class\nsegmentation. At the core of PraNet-V2 is the Dual-Supervised Reverse Attention\n(DSRA) module, which incorporates explicit background supervision, independent\nbackground modeling, and semantically enriched attention fusion. Our PraNet-V2\nframework demonstrates strong performance on four polyp segmentation datasets.\nAdditionally, by integrating DSRA to iteratively enhance foreground\nsegmentation results in three state-of-the-art semantic segmentation models, we\nachieve up to a 1.36% improvement in mean Dice score. Code is available at:\nhttps://github.com/ai4colonoscopy/PraNet-V2/tree/main/binary_seg/jittor.", "published": "2025-04-15 08:49:29", "link": "http://arxiv.org/abs/2504.10986v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DMPT: Decoupled Modality-aware Prompt Tuning for Multi-modal Object Re-identification", "abstract": "Current multi-modal object re-identification approaches based on large-scale\npre-trained backbones (i.e., ViT) have displayed remarkable progress and\nachieved excellent performance. However, these methods usually adopt the\nstandard full fine-tuning paradigm, which requires the optimization of\nconsiderable backbone parameters, causing extensive computational and storage\nrequirements. In this work, we propose an efficient prompt-tuning framework\ntailored for multi-modal object re-identification, dubbed DMPT, which freezes\nthe main backbone and only optimizes several newly added decoupled\nmodality-aware parameters. Specifically, we explicitly decouple the visual\nprompts into modality-specific prompts which leverage prior modality knowledge\nfrom a powerful text encoder and modality-independent semantic prompts which\nextract semantic information from multi-modal inputs, such as visible,\nnear-infrared, and thermal-infrared. Built upon the extracted features, we\nfurther design a Prompt Inverse Bind (PromptIBind) strategy that employs bind\nprompts as a medium to connect the semantic prompt tokens of different\nmodalities and facilitates the exchange of complementary multi-modal\ninformation, boosting final re-identification results. Experimental results on\nmultiple common benchmarks demonstrate that our DMPT can achieve competitive\nresults to existing state-of-the-art methods while requiring only 6.5%\nfine-tuning of the backbone parameters.", "published": "2025-04-15 08:48:41", "link": "http://arxiv.org/abs/2504.10985v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Seeing like a Cephalopod: Colour Vision with a Monochrome Event Camera", "abstract": "Cephalopods exhibit unique colour discrimination capabilities despite having\none type of photoreceptor, relying instead on chromatic aberration induced by\ntheir ocular optics and pupil shapes to perceive spectral information. We took\ninspiration from this biological mechanism to design a spectral imaging system\nthat combines a ball lens with an event-based camera. Our approach relies on a\nmotorised system that shifts the focal position, mirroring the adaptive lens\nmotion in cephalopods. This approach has enabled us to achieve\nwavelength-dependent focusing across the visible light and near-infrared\nspectrum, making the event a spectral sensor. We characterise chromatic\naberration effects, using both event-based and conventional frame-based\nsensors, validating the effectiveness of bio-inspired spectral discrimination\nboth in simulation and in a real setup as well as assessing the spectral\ndiscrimination performance. Our proposed approach provides a robust spectral\nsensing capability without conventional colour filters or computational\ndemosaicing. This approach opens new pathways toward new spectral sensing\nsystems inspired by nature's evolutionary solutions. Code and analysis are\navailable at: https://samiarja.github.io/neuromorphic_octopus_eye/", "published": "2025-04-15 08:47:11", "link": "http://arxiv.org/abs/2504.10984v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Deep Learning in Concealed Dense Prediction", "abstract": "Deep learning is developing rapidly and handling common computer vision tasks\nwell. It is time to pay attention to more complex vision tasks, as model size,\nknowledge, and reasoning capabilities continue to improve. In this paper, we\nintroduce and review a family of complex tasks, termed Concealed Dense\nPrediction (CDP), which has great value in agriculture, industry, etc. CDP's\nintrinsic trait is that the targets are concealed in their surroundings, thus\nfully perceiving them requires fine-grained representations, prior knowledge,\nauxiliary reasoning, etc. The contributions of this review are three-fold: (i)\nWe introduce the scope, characteristics, and challenges specific to CDP tasks\nand emphasize their essential differences from generic vision tasks. (ii) We\ndevelop a taxonomy based on concealment counteracting to summarize deep\nlearning efforts in CDP through experiments on three tasks. We compare 25\nstate-of-the-art methods across 12 widely used concealed datasets. (iii) We\ndiscuss the potential applications of CDP in the large model era and summarize\n6 potential research directions. We offer perspectives for the future\ndevelopment of CDP by constructing a large-scale multimodal instruction\nfine-tuning dataset, CvpINST, and a concealed visual perception agent,\nCvpAgent.", "published": "2025-04-15 08:44:42", "link": "http://arxiv.org/abs/2504.10979v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "AgentPolyp: Accurate Polyp Segmentation via Image Enhancement Agent", "abstract": "Since human and environmental factors interfere, captured polyp images\nusually suffer from issues such as dim lighting, blur, and overexposure, which\npose challenges for downstream polyp segmentation tasks. To address the\nchallenges of noise-induced degradation in polyp images, we present AgentPolyp,\na novel framework integrating CLIP-based semantic guidance and dynamic image\nenhancement with a lightweight neural network for segmentation. The agent first\nevaluates image quality using CLIP-driven semantic analysis (e.g., identifying\n``low-contrast polyps with vascular textures\") and adapts reinforcement\nlearning strategies to dynamically apply multi-modal enhancement operations\n(e.g., denoising, contrast adjustment). A quality assessment feedback loop\noptimizes pixel-level enhancement and segmentation focus in a collaborative\nmanner, ensuring robust preprocessing before neural network segmentation. This\nmodular architecture supports plug-and-play extensions for various enhancement\nalgorithms and segmentation networks, meeting deployment requirements for\nendoscopic devices.", "published": "2025-04-15 08:39:35", "link": "http://arxiv.org/abs/2504.10978v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Adaptive Decision Boundary for Few-Shot Class-Incremental Learning", "abstract": "Few-Shot Class-Incremental Learning (FSCIL) aims to continuously learn new\nclasses from a limited set of training samples without forgetting knowledge of\npreviously learned classes. Conventional FSCIL methods typically build a robust\nfeature extractor during the base training session with abundant training\nsamples and subsequently freeze this extractor, only fine-tuning the classifier\nin subsequent incremental phases. However, current strategies primarily focus\non preventing catastrophic forgetting, considering only the relationship\nbetween novel and base classes, without paying attention to the specific\ndecision spaces of each class. To address this challenge, we propose a\nplug-and-play Adaptive Decision Boundary Strategy (ADBS), which is compatible\nwith most FSCIL methods. Specifically, we assign a specific decision boundary\nto each class and adaptively adjust these boundaries during training to\noptimally refine the decision spaces for the classes in each session.\nFurthermore, to amplify the distinctiveness between classes, we employ a novel\ninter-class constraint loss that optimizes the decision boundaries and\nprototypes for each class. Extensive experiments on three benchmarks, namely\nCIFAR100, miniImageNet, and CUB200, demonstrate that incorporating our ADBS\nmethod with existing FSCIL techniques significantly improves performance,\nachieving overall state-of-the-art results.", "published": "2025-04-15 08:37:24", "link": "http://arxiv.org/abs/2504.10976v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Self-Supervised Enhancement of Forward-Looking Sonar Images: Bridging Cross-Modal Degradation Gaps through Feature Space Transformation and Multi-Frame Fusion", "abstract": "Enhancing forward-looking sonar images is critical for accurate underwater\ntarget detection. Current deep learning methods mainly rely on supervised\ntraining with simulated data, but the difficulty in obtaining high-quality\nreal-world paired data limits their practical use and generalization. Although\nself-supervised approaches from remote sensing partially alleviate data\nshortages, they neglect the cross-modal degradation gap between sonar and\nremote sensing images. Directly transferring pretrained weights often leads to\noverly smooth sonar images, detail loss, and insufficient brightness. To\naddress this, we propose a feature-space transformation that maps sonar images\nfrom the pixel domain to a robust feature domain, effectively bridging the\ndegradation gap. Additionally, our self-supervised multi-frame fusion strategy\nleverages complementary inter-frame information to naturally remove speckle\nnoise and enhance target-region brightness. Experiments on three self-collected\nreal-world forward-looking sonar datasets show that our method significantly\noutperforms existing approaches, effectively suppressing noise, preserving\ndetailed edges, and substantially improving brightness, demonstrating strong\npotential for underwater target detection applications.", "published": "2025-04-15 08:34:56", "link": "http://arxiv.org/abs/2504.10974v1", "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV"}
{"title": "AFiRe: Anatomy-Driven Self-Supervised Learning for Fine-Grained Representation in Radiographic Images", "abstract": "Current self-supervised methods, such as contrastive learning, predominantly\nfocus on global discrimination, neglecting the critical fine-grained anatomical\ndetails required for accurate radiographic analysis. To address this challenge,\nwe propose an Anatomy-driven self-supervised framework for enhancing\nFine-grained Representation in radiographic image analysis (AFiRe). The core\nidea of AFiRe is to align the anatomical consistency with the unique\ntoken-processing characteristics of Vision Transformer. Specifically, AFiRe\nsynergistically performs two self-supervised schemes: (i) Token-wise\nanatomy-guided contrastive learning, which aligns image tokens based on\nstructural and categorical consistency, thereby enhancing fine-grained\nspatial-anatomical discrimination; (ii) Pixel-level anomaly-removal\nrestoration, which particularly focuses on local anomalies, thereby refining\nthe learned discrimination with detailed geometrical information. Additionally,\nwe propose Synthetic Lesion Mask to enhance anatomical diversity while\npreserving intra-consistency, which is typically corrupted by traditional data\naugmentations, such as Cropping and Affine transformations. Experimental\nresults show that AFiRe: (i) provides robust anatomical discrimination,\nachieving more cohesive feature clusters compared to state-of-the-art\ncontrastive learning methods; (ii) demonstrates superior generalization,\nsurpassing 7 radiography-specific self-supervised methods in multi-label\nclassification tasks with limited labeling; and (iii) integrates fine-grained\ninformation, enabling precise anomaly detection using only image-level\nannotations.", "published": "2025-04-15 08:29:54", "link": "http://arxiv.org/abs/2504.10972v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "An Efficient and Mixed Heterogeneous Model for Image Restoration", "abstract": "Image restoration~(IR), as a fundamental multimedia data processing task, has\na significant impact on downstream visual applications. In recent years,\nresearchers have focused on developing general-purpose IR models capable of\nhandling diverse degradation types, thereby reducing the cost and complexity of\nmodel development. Current mainstream approaches are based on three\narchitectural paradigms: CNNs, Transformers, and Mambas. CNNs excel in\nefficient inference, whereas Transformers and Mamba excel at capturing\nlong-range dependencies and modeling global contexts. While each architecture\nhas demonstrated success in specialized, single-task settings, limited efforts\nhave been made to effectively integrate heterogeneous architectures to jointly\naddress diverse IR challenges. To bridge this gap, we propose RestorMixer, an\nefficient and general-purpose IR model based on mixed-architecture fusion.\nRestorMixer adopts a three-stage encoder-decoder structure, where each stage is\ntailored to the resolution and feature characteristics of the input. In the\ninitial high-resolution stage, CNN-based blocks are employed to rapidly extract\nshallow local features. In the subsequent stages, we integrate a refined\nmulti-directional scanning Mamba module with a multi-scale window-based\nself-attention mechanism. This hierarchical and adaptive design enables the\nmodel to leverage the strengths of CNNs in local feature extraction, Mamba in\nglobal context modeling, and attention mechanisms in dynamic feature\nrefinement. Extensive experimental results demonstrate that RestorMixer\nachieves leading performance across multiple IR tasks while maintaining high\ninference efficiency. The official code can be accessed at\nhttps://github.com/ClimBin/RestorMixer.", "published": "2025-04-15 08:19:12", "link": "http://arxiv.org/abs/2504.10967v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Recognition of Geometrical Shapes by Dictionary Learning", "abstract": "Dictionary learning is a versatile method to produce an overcomplete set of\nvectors, called atoms, to represent a given input with only a few atoms. In the\nliterature, it has been used primarily for tasks that explore its powerful\nrepresentation capabilities, such as for image reconstruction. In this work, we\npresent a first approach to make dictionary learning work for shape\nrecognition, considering specifically geometrical shapes. As we demonstrate,\nthe choice of the underlying optimization method has a significant impact on\nrecognition quality. Experimental results confirm that dictionary learning may\nbe an interesting method for shape recognition tasks.", "published": "2025-04-15 08:05:16", "link": "http://arxiv.org/abs/2504.10958v1", "categories": ["cs.CV", "cs.LG", "00A69, 68U05, 68T05"], "primary_category": "cs.CV"}
{"title": "Cross-Frequency Implicit Neural Representation with Self-Evolving Parameters", "abstract": "Implicit neural representation (INR) has emerged as a powerful paradigm for\nvisual data representation. However, classical INR methods represent data in\nthe original space mixed with different frequency components, and several\nfeature encoding parameters (e.g., the frequency parameter $\\omega$ or the rank\n$R$) need manual configurations. In this work, we propose a self-evolving\ncross-frequency INR using the Haar wavelet transform (termed CF-INR), which\ndecouples data into four frequency components and employs INRs in the wavelet\nspace. CF-INR allows the characterization of different frequency components\nseparately, thus enabling higher accuracy for data representation. To more\nprecisely characterize cross-frequency components, we propose a cross-frequency\ntensor decomposition paradigm for CF-INR with self-evolving parameters, which\nautomatically updates the rank parameter $R$ and the frequency parameter\n$\\omega$ for each frequency component through self-evolving optimization. This\nself-evolution paradigm eliminates the laborious manual tuning of these\nparameters, and learns a customized cross-frequency feature encoding\nconfiguration for each dataset. We evaluate CF-INR on a variety of visual data\nrepresentation and recovery tasks, including image regression, inpainting,\ndenoising, and cloud removal. Extensive experiments demonstrate that CF-INR\noutperforms state-of-the-art methods in each case.", "published": "2025-04-15 07:14:35", "link": "http://arxiv.org/abs/2504.10929v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Towards Efficient Partially Relevant Video Retrieval with Active Moment Discovering", "abstract": "Partially relevant video retrieval (PRVR) is a practical yet challenging task\nin text-to-video retrieval, where videos are untrimmed and contain much\nbackground content. The pursuit here is of both effective and efficient\nsolutions to capture the partial correspondence between text queries and\nuntrimmed videos. Existing PRVR methods, which typically focus on modeling\nmulti-scale clip representations, however, suffer from content independence and\ninformation redundancy, impairing retrieval performance. To overcome these\nlimitations, we propose a simple yet effective approach with active moment\ndiscovering (AMDNet). We are committed to discovering video moments that are\nsemantically consistent with their queries. By using learnable span anchors to\ncapture distinct moments and applying masked multi-moment attention to\nemphasize salient moments while suppressing redundant backgrounds, we achieve\nmore compact and informative video representations. To further enhance moment\nmodeling, we introduce a moment diversity loss to encourage different moments\nof distinct regions and a moment relevance loss to promote semantically\nquery-relevant moments, which cooperate with a partially relevant retrieval\nloss for end-to-end optimization. Extensive experiments on two large-scale\nvideo datasets (\\ie, TVR and ActivityNet Captions) demonstrate the superiority\nand efficiency of our AMDNet. In particular, AMDNet is about 15.5 times smaller\n(\\#parameters) while 6.0 points higher (SumR) than the up-to-date method\nGMMFormer on TVR.", "published": "2025-04-15 07:00:18", "link": "http://arxiv.org/abs/2504.10920v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Embedding Radiomics into Vision Transformers for Multimodal Medical Image Classification", "abstract": "Background: Deep learning has significantly advanced medical image analysis,\nwith Vision Transformers (ViTs) offering a powerful alternative to\nconvolutional models by modeling long-range dependencies through\nself-attention. However, ViTs are inherently data-intensive and lack\ndomain-specific inductive biases, limiting their applicability in medical\nimaging. In contrast, radiomics provides interpretable, handcrafted descriptors\nof tissue heterogeneity but suffers from limited scalability and integration\ninto end-to-end learning frameworks. In this work, we propose the\nRadiomics-Embedded Vision Transformer (RE-ViT) that combines radiomic features\nwith data-driven visual embeddings within a ViT backbone.\n  Purpose: To develop a hybrid RE-ViT framework that integrates radiomics and\npatch-wise ViT embeddings through early fusion, enhancing robustness and\nperformance in medical image classification.\n  Methods: Following the standard ViT pipeline, images were divided into\npatches. For each patch, handcrafted radiomic features were extracted and fused\nwith linearly projected pixel embeddings. The fused representations were\nnormalized, positionally encoded, and passed to the ViT encoder. A learnable\n[CLS] token aggregated patch-level information for classification. We evaluated\nRE-ViT on three public datasets (including BUSI, ChestXray2017, and Retinal\nOCT) using accuracy, macro AUC, sensitivity, and specificity. RE-ViT was\nbenchmarked against CNN-based (VGG-16, ResNet) and hybrid (TransMed) models.\n  Results: RE-ViT achieved state-of-the-art results: on BUSI,\nAUC=0.950+/-0.011; on ChestXray2017, AUC=0.989+/-0.004; on Retinal OCT,\nAUC=0.986+/-0.001, which outperforms other comparison models.\n  Conclusions: The RE-ViT framework effectively integrates radiomics with ViT\narchitectures, demonstrating improved performance and generalizability across\nmultimodal medical image classification tasks.", "published": "2025-04-15 06:55:58", "link": "http://arxiv.org/abs/2504.10916v1", "categories": ["physics.med-ph", "cs.CV"], "primary_category": "physics.med-ph"}
{"title": "InterAnimate: Taming Region-aware Diffusion Model for Realistic Human Interaction Animation", "abstract": "Recent video generation research has focused heavily on isolated actions,\nleaving interactive motions-such as hand-face interactions-largely unexamined.\nThese interactions are essential for emerging biometric authentication systems,\nwhich rely on interactive motion-based anti-spoofing approaches. From a\nsecurity perspective, there is a growing need for large-scale, high-quality\ninteractive videos to train and strengthen authentication models. In this work,\nwe introduce a novel paradigm for animating realistic hand-face interactions.\nOur approach simultaneously learns spatio-temporal contact dynamics and\nbiomechanically plausible deformation effects, enabling natural interactions\nwhere hand movements induce anatomically accurate facial deformations while\nmaintaining collision-free contact. To facilitate this research, we present\nInterHF, a large-scale hand-face interaction dataset featuring 18 interaction\npatterns and 90,000 annotated videos. Additionally, we propose InterAnimate, a\nregion-aware diffusion model designed specifically for interaction animation.\nInterAnimate leverages learnable spatial and temporal latents to effectively\ncapture dynamic interaction priors and integrates a region-aware interaction\nmechanism that injects these priors into the denoising process. To the best of\nour knowledge, this work represents the first large-scale effort to\nsystematically study human hand-face interactions. Qualitative and quantitative\nresults show InterAnimate produces highly realistic animations, setting a new\nbenchmark. Code and data will be made public to advance research.", "published": "2025-04-15 06:32:45", "link": "http://arxiv.org/abs/2504.10905v1", "categories": ["cs.CV", "cs.HC"], "primary_category": "cs.CV"}
{"title": "Fine-Grained Rib Fracture Diagnosis with Hyperbolic Embeddings: A Detailed Annotation Framework and Multi-Label Classification Model", "abstract": "Accurate rib fracture identification and classification are essential for\ntreatment planning. However, existing datasets often lack fine-grained\nannotations, particularly regarding rib fracture characterization, type, and\nprecise anatomical location on individual ribs. To address this, we introduce a\nnovel rib fracture annotation protocol tailored for fracture classification.\nFurther, we enhance fracture classification by leveraging cross-modal\nembeddings that bridge radiological images and clinical descriptions. Our\napproach employs hyperbolic embeddings to capture the hierarchical nature of\nfracture, mapping visual features and textual descriptions into a shared\nnon-Euclidean manifold. This framework enables more nuanced similarity\ncomputations between imaging characteristics and clinical descriptions,\naccounting for the inherent hierarchical relationships in fracture taxonomy.\nExperimental results demonstrate that our approach outperforms existing methods\nacross multiple classification tasks, with average recall improvements of 6% on\nthe AirRib dataset and 17.5% on the public RibFrac dataset.", "published": "2025-04-15 05:47:09", "link": "http://arxiv.org/abs/2504.10889v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Safe-Construct: Redefining Construction Safety Violation Recognition as 3D Multi-View Engagement Task", "abstract": "Recognizing safety violations in construction environments is critical yet\nremains underexplored in computer vision. Existing models predominantly rely on\n2D object detection, which fails to capture the complexities of real-world\nviolations due to: (i) an oversimplified task formulation treating violation\nrecognition merely as object detection, (ii) inadequate validation under\nrealistic conditions, (iii) absence of standardized baselines, and (iv) limited\nscalability from the unavailability of synthetic dataset generators for diverse\nconstruction scenarios. To address these challenges, we introduce\nSafe-Construct, the first framework that reformulates violation recognition as\na 3D multi-view engagement task, leveraging scene-level worker-object context\nand 3D spatial understanding. We also propose the Synthetic Indoor Construction\nSite Generator (SICSG) to create diverse, scalable training data, overcoming\ndata limitations. Safe-Construct achieves a 7.6% improvement over\nstate-of-the-art methods across four violation types. We rigorously evaluate\nour approach in near-realistic settings, incorporating four violations, four\nworkers, 14 objects, and challenging conditions like occlusions (worker-object,\nworker-worker) and variable illumination (back-lighting, overexposure,\nsunlight). By integrating 3D multi-view spatial understanding and synthetic\ndata generation, Safe-Construct sets a new benchmark for scalable and robust\nsafety monitoring in high-risk industries. Project Website:\nhttps://Safe-Construct.github.io/Safe-Construct", "published": "2025-04-15 05:21:09", "link": "http://arxiv.org/abs/2504.10880v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Weather-Aware Object Detection Transformer for Domain Adaptation", "abstract": "RT-DETRs have shown strong performance across various computer vision tasks\nbut are known to degrade under challenging weather conditions such as fog. In\nthis work, we investigate three novel approaches to enhance RT-DETR robustness\nin foggy environments: (1) Domain Adaptation via Perceptual Loss, which\ndistills domain-invariant features from a teacher network to a student using\nperceptual supervision; (2) Weather Adaptive Attention, which augments the\nattention mechanism with fog-sensitive scaling by introducing an auxiliary\nfoggy image stream; and (3) Weather Fusion Encoder, which integrates a\ndual-stream encoder architecture that fuses clear and foggy image features via\nmulti-head self and cross-attention. Despite the architectural innovations,\nnone of the proposed methods consistently outperform the baseline RT-DETR. We\nanalyze the limitations and potential causes, offering insights for future\nresearch in weather-aware object detection.", "published": "2025-04-15 05:11:18", "link": "http://arxiv.org/abs/2504.10877v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DAAF:Degradation-Aware Adaptive Fusion Framework for Robust Infrared and Visible Images Fusion", "abstract": "Existing infrared and visible image fusion(IVIF) algorithms often prioritize\nhigh-quality images, neglecting image degradation such as low light and noise,\nwhich limits the practical potential. This paper propose Degradation-Aware\nAdaptive image Fusion (DAAF), which achieves unified modeling of adaptive\ndegradation optimization and image fusion. Specifically, DAAF comprises an\nauxiliary Adaptive Degradation Optimization Network (ADON) and a Feature\nInteractive Local-Global Fusion (FILGF) Network. Firstly, ADON includes\ninfrared and visible-light branches. Within the infrared branch,\nfrequency-domain feature decomposition and extraction are employed to isolate\nGaussian and stripe noise. In the visible-light branch, Retinex decomposition\nis applied to extract illumination and reflectance components, enabling\ncomplementary enhancement of detail and illumination distribution.\nSubsequently, FILGF performs interactive multi-scale local-global feature\nfusion. Local feature fusion consists of intra-inter model feature complement,\nwhile global feature fusion is achieved through a interactive cross-model\nattention. Extensive experiments have shown that DAAF outperforms current IVIF\nalgorithms in normal and complex degradation scenarios.", "published": "2025-04-15 05:02:49", "link": "http://arxiv.org/abs/2504.10871v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ZeroGrasp: Zero-Shot Shape Reconstruction Enabled Robotic Grasping", "abstract": "Robotic grasping is a cornerstone capability of embodied systems. Many\nmethods directly output grasps from partial information without modeling the\ngeometry of the scene, leading to suboptimal motion and even collisions. To\naddress these issues, we introduce ZeroGrasp, a novel framework that\nsimultaneously performs 3D reconstruction and grasp pose prediction in near\nreal-time. A key insight of our method is that occlusion reasoning and modeling\nthe spatial relationships between objects is beneficial for both accurate\nreconstruction and grasping. We couple our method with a novel large-scale\nsynthetic dataset, which comprises 1M photo-realistic images, high-resolution\n3D reconstructions and 11.3B physically-valid grasp pose annotations for 12K\nobjects from the Objaverse-LVIS dataset. We evaluate ZeroGrasp on the\nGraspNet-1B benchmark as well as through real-world robot experiments.\nZeroGrasp achieves state-of-the-art performance and generalizes to novel\nreal-world objects by leveraging synthetic data.", "published": "2025-04-15 04:37:39", "link": "http://arxiv.org/abs/2504.10857v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "LVLM_CSP: Accelerating Large Vision Language Models via Clustering, Scattering, and Pruning for Reasoning Segmentation", "abstract": "Large Vision Language Models (LVLMs) have been widely adopted to guide vision\nfoundation models in performing reasoning segmentation tasks, achieving\nimpressive performance. However, the substantial computational overhead\nassociated with LVLMs presents a new challenge. The primary source of this\ncomputational cost arises from processing hundreds of image tokens. Therefore,\nan effective strategy to mitigate such overhead is to reduce the number of\nimage tokens, a process known as image token pruning. Previous studies on image\ntoken pruning for LVLMs have primarily focused on high level visual\nunderstanding tasks, such as visual question answering and image captioning. In\ncontrast, guiding vision foundation models to generate accurate visual masks\nbased on textual queries demands precise semantic and spatial reasoning\ncapabilities. Consequently, pruning methods must carefully control individual\nimage tokens throughout the LVLM reasoning process. Our empirical analysis\nreveals that existing methods struggle to adequately balance reductions in\ncomputational overhead with the necessity to maintain high segmentation\naccuracy. In this work, we propose LVLM_CSP, a novel training free visual token\npruning method specifically designed for LVLM based reasoning segmentation\ntasks. LVLM_CSP consists of three stages: clustering, scattering, and pruning.\nInitially, the LVLM performs coarse-grained visual reasoning using a subset of\nselected image tokens. Next, fine grained reasoning is conducted, and finally,\nmost visual tokens are pruned in the last stage. Extensive experiments\ndemonstrate that LVLM_CSP achieves a 65% reduction in image token inference\nFLOPs with virtually no accuracy degradation, and a 70% reduction with only a\nminor 1% drop in accuracy on the 7B LVLM.", "published": "2025-04-15 04:27:15", "link": "http://arxiv.org/abs/2504.10854v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Enhancing Features in Long-tailed Data Using Large Vision Mode", "abstract": "Language-based foundation models, such as large language models (LLMs) or\nlarge vision-language models (LVLMs), have been widely studied in long-tailed\nrecognition. However, the need for linguistic data is not applicable to all\npractical tasks. In this study, we aim to explore using large vision models\n(LVMs) or visual foundation models (VFMs) to enhance long-tailed data features\nwithout any language information. Specifically, we extract features from the\nLVM and fuse them with features in the baseline network's map and latent space\nto obtain the augmented features. Moreover, we design several prototype-based\nlosses in the latent space to further exploit the potential of the augmented\nfeatures. In the experimental section, we validate our approach on two\nbenchmark datasets: ImageNet-LT and iNaturalist2018.", "published": "2025-04-15 04:21:50", "link": "http://arxiv.org/abs/2504.10852v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "A comprehensive review of remote sensing in wetland classification and mapping", "abstract": "Wetlands constitute critical ecosystems that support both biodiversity and\nhuman well-being; however, they have experienced a significant decline since\nthe 20th century. Back in the 1970s, researchers began to employ remote sensing\ntechnologies for wetland classification and mapping to elucidate the extent and\nvariations of wetlands. Although some review articles summarized the\ndevelopment of this field, there is a lack of a thorough and in-depth\nunderstanding of wetland classification and mapping: (1) the scientific\nimportance of wetlands, (2) major data, methods used in wetland classification\nand mapping, (3) driving factors of wetland changes, (4) current research\nparadigm and limitations, (5) challenges and opportunities in wetland\nclassification and mapping under the context of technological innovation and\nglobal environmental change. In this review, we aim to provide a comprehensive\nperspective and new insights into wetland classification and mapping for\nreaders to answer these questions. First, we conduct a meta-analysis of over\n1,200 papers, encompassing wetland types, methods, sensor types, and study\nsites, examining prevailing trends in wetland classification and mapping. Next,\nwe review and synthesize the wetland features and existing data and methods in\nwetland classification and mapping. We also summarize typical wetland mapping\nproducts and explore the intrinsic driving factors of wetland changes across\nmultiple spatial and temporal scales. Finally, we discuss current limitations\nand propose future directions in response to global environmental change and\ntechnological innovation. This review consolidates our understanding of wetland\nremote sensing and offers scientific recommendations that foster transformative\nprogress in wetland science.", "published": "2025-04-15 03:59:36", "link": "http://arxiv.org/abs/2504.10842v1", "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Optimal Hardness of Online Algorithms for Large Independent Sets", "abstract": "We study the algorithmic problem of finding a large independent set in an\nErd\\\"{o}s-R\\'{e}nyi random graph $\\mathbb{G}(n,p)$. For constant $p$ and\n$b=1/(1-p)$, the largest independent set has size $2\\log_b n$, while a simple\ngreedy algorithm revealing vertices sequentially and making decisions based\nonly on previously seen vertices finds an independent set of size $\\log_b n$.\nIn his seminal 1976 paper, Karp challenged to either improve this guarantee or\nestablish its hardness. Decades later, this problem remains open, one of the\nmost prominent algorithmic problems in the theory of random graphs.\n  In this paper, we establish that a broad class of online algorithms fails to\nfind an independent set of size $(1+\\epsilon)\\log_b n$ any constant\n$\\epsilon>0$ w.h.p. This class includes Karp's algorithm as a special case, and\nextends it by allowing the algorithm to query exceptional edges not yet 'seen'\nby the algorithm. Our lower bound holds for all $p\\in [d/n,1-n^{-1/d}]$, where\n$d$ is a large constant. In the dense regime (constant $p$), we further prove\nthat our result is asymptotically tight with respect to the number of\nexceptional edges queried, by designing an online algorithm which beats the\nhalf-optimality threshold when the number of exceptional edges slightly exceeds\nour bound.\n  Our result provides evidence for the algorithmic hardness of Karp's problem\nby supporting the conjectured optimality of the aforementioned greedy algorithm\nand establishing it within the class of online algorithms. Our proof relies on\na refined analysis of the geometric structure of tuples of large independent\nsets, establishing a variant of the Overlap Gap Property (OGP) commonly used as\na barrier for classes of algorithms. While OGP has predominantly served as a\nbarrier to stable algorithms, online algorithms are not stable and our\napplication of OGP-based techniques to online setting is novel.", "published": "2025-04-15 17:58:08", "link": "http://arxiv.org/abs/2504.11450v1", "categories": ["cs.DS", "cs.CC", "cs.DM", "math.CO", "math.PR"], "primary_category": "cs.DS"}
{"title": "Evaluation Report on MCP Servers", "abstract": "With the rise of LLMs, a large number of Model Context Protocol (MCP)\nservices have emerged since the end of 2024. However, the effectiveness and\nefficiency of MCP servers have not been well studied. To study these questions,\nwe propose an evaluation framework, called MCPBench. We selected several widely\nused MCP server and conducted an experimental evaluation on their accuracy,\ntime, and token usage. Our experiments showed that the most effective MCP, Bing\nWeb Search, achieved an accuracy of 64%. Importantly, we found that the\naccuracy of MCP servers can be substantially enhanced by involving declarative\ninterface. This research paves the way for further investigations into\noptimized MCP implementations, ultimately leading to better AI-driven\napplications and data retrieval solutions.", "published": "2025-04-15 11:40:12", "link": "http://arxiv.org/abs/2504.11094v1", "categories": ["cs.IR", "cs.DB"], "primary_category": "cs.IR"}
{"title": "Why am I seeing this? Towards recognizing social media recommender systems with missing recommendations", "abstract": "Social media plays a crucial role in shaping society, often amplifying\npolarization and spreading misinformation. These effects stem from complex\ndynamics involving user interactions, individual traits, and recommender\nalgorithms driving content selection. Recommender systems, which significantly\nshape the content users see and decisions they make, offer an opportunity for\nintervention and regulation. However, assessing their impact is challenging due\nto algorithmic opacity and limited data availability. To effectively model user\ndecision-making, it is crucial to recognize the recommender system adopted by\nthe platform.\n  This work introduces a method for Automatic Recommender Recognition using\nGraph Neural Networks (GNNs), based solely on network structure and observed\nbehavior. To infer the hidden recommender, we first train a Recommender Neutral\nUser model (RNU) using a GNN and an adapted hindsight academic network\nrecommender, aiming to reduce reliance on the actual recommender in the data.\nWe then generate several Recommender Hypothesis-specific Synthetic Datasets\n(RHSD) by combining the RNU with different known recommenders, producing ground\ntruths for testing. Finally, we train Recommender Hypothesis-specific User\nmodels (RHU) under various hypotheses and compare each candidate with the\noriginal used to generate the RHSD.\n  Our approach enables accurate detection of hidden recommenders and their\ninfluence on user behavior. Unlike audit-based methods, it captures system\nbehavior directly, without ad hoc experiments that often fail to reflect real\nplatforms. This study provides insights into how recommenders shape behavior,\naiding efforts to reduce polarization and misinformation.", "published": "2025-04-15 09:16:17", "link": "http://arxiv.org/abs/2504.11000v1", "categories": ["cs.IR", "cs.SI"], "primary_category": "cs.IR"}
{"title": "MSCRS: Multi-modal Semantic Graph Prompt Learning Framework for Conversational Recommender Systems", "abstract": "Conversational Recommender Systems (CRSs) aim to provide personalized\nrecommendations by interacting with users through conversations. Most existing\nstudies of CRS focus on extracting user preferences from conversational\ncontexts. However, due to the short and sparse nature of conversational\ncontexts, it is difficult to fully capture user preferences by conversational\ncontexts only. We argue that multi-modal semantic information can enrich user\npreference expressions from diverse dimensions (e.g., a user preference for a\ncertain movie may stem from its magnificent visual effects and compelling\nstoryline). In this paper, we propose a multi-modal semantic graph prompt\nlearning framework for CRS, named MSCRS. First, we extract textual and image\nfeatures of items mentioned in the conversational contexts. Second, we capture\nhigher-order semantic associations within different semantic modalities\n(collaborative, textual, and image) by constructing modality-specific graph\nstructures. Finally, we propose an innovative integration of multi-modal\nsemantic graphs with prompt learning, harnessing the power of large language\nmodels to comprehensively explore high-dimensional semantic relationships.\nExperimental results demonstrate that our proposed method significantly\nimproves accuracy in item recommendation, as well as generates more natural and\ncontextually relevant content in response generation. We have released the code\nand the expanded multi-modal CRS datasets to facilitate further exploration in\nrelated research\\footnote{https://github.com/BIAOBIAO12138/MSCRS-main}.", "published": "2025-04-15 07:05:22", "link": "http://arxiv.org/abs/2504.10921v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Full-Diversity Construction-D Lattices: Design and Decoding Perspective on Block-Fading Channels", "abstract": "This paper introduces a novel framework for constructing algebraic lattices\nbased on Construction-D, leveraging nested linear codes and prime ideals from\nalgebraic number fields. We focus on the application of these lattices in\nblock-fading (BF) channels, which are characterized by piecewise-constant\nfading across blocks of transmitted symbols. This approach results in a\nsemi-systematic generator matrix, providing a structured foundation for\nhigh-dimensional lattice design for BF channels. The proposed Construction-D\nlattices exhibit the full diversity property, making them highly effective for\nerror performance improvement. To address this, we develop an efficient\ndecoding algorithm designed specifically for full-diversity Construction-D\nlattices.\n  Simulations indicate that the proposed lattices notably enhance error\nperformance compared to full-diversity Construction-A lattices in diversity-2\ncases. Interestingly, unlike AWGN channels, the expected performance\nenhancement of Construction-D over Construction-A, resulting from an increased\nnumber of nested code levels, was observed only in the two-level and\ndiversity-2 cases. This phenomenon is likely attributed to the intensified\neffects of error propagation that occur during successive cancellation at\nhigher levels, as well as the higher diversity orders.\n  These findings highlight the promise of Construction-D lattices as an\neffective coding strategy for enhancing communication reliability in BF\nchannels.", "published": "2025-04-15 17:57:56", "link": "http://arxiv.org/abs/2504.11448v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Fault Tolerant Quantum Simulation via Symplectic Transvections", "abstract": "Conventional approaches to fault-tolerant quantum computing realize logical\ncircuits gate-by-gate, synthesizing each gate independently on one or more code\nblocks. This incurs excess overhead and doesn't leverage common structures in\nquantum algorithms. In contrast, we propose a framework that enables the\nexecution of entire logical circuit blocks at once, preserving their global\nstructure. This whole-block approach allows for the direct implementation of\nlogical Trotter circuits - of arbitrary rotation angles - on any stabilizer\ncode, providing a powerful new method for fault tolerant Hamiltonian simulation\nwithin a single code block. At the heart of our approach lies a deep structural\ncorrespondence between symplectic transvections and Trotter circuits. This\nconnection enables both logical and physical circuits to share the Trotter\nstructure while preserving stabilizer centralization and circuit symmetry even\nin the presence of non-Clifford rotations. We discuss potential approaches to\nfault tolerance via biased noise and code concatenation. While we illustrate\nthe key principles using a $[[8,3,3]]$ code, our simulations show that the\nframework applies to Hamiltonian simulation on even good quantum LDPC codes.\nThese results open the door to new algorithm-tailored, block-level strategies\nfor fault tolerant circuit design, especially in quantum simulation.", "published": "2025-04-15 17:56:07", "link": "http://arxiv.org/abs/2504.11444v1", "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "quant-ph"}
{"title": "Breaking the TDD Flow for Over-the-Air Phase Synchronization in Distributed Antenna Systems", "abstract": "Phase synchronization between distributed antenna arrays requires\nmeasurements that break the standard time-division duplex (TDD) operation. We\npresent a feasibility study on implementing such synchronization and analyze\nits impact on the quality of service. Considering two antenna arrays with\nindependent local oscillators (LOs), we propose a modified TDD flow to\naccommodate the transmission of phase synchronization signals, formulate the\nphase estimation and compensation problem, and derive the achievable downlink\nspectral efficiency (SE). Numerical results show that frequent re-estimation of\nthe interarray phase disparity is essential for maximizing SE in systems with\nlow-quality LOs. Furthermore, applying a Kalman filter for phase tracking\nsubstantially improves the SE, especially if phase estimation errors are large\ncompared to LOs phase drifts.", "published": "2025-04-15 17:26:42", "link": "http://arxiv.org/abs/2504.11411v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Property Inheritance for Subtensors in Tensor Train Decompositions", "abstract": "Tensor dimensionality reduction is one of the fundamental tools for modern\ndata science. To address the high computational overhead, fiber-wise sampled\nsubtensors that preserve the original tensor rank are often used in designing\nefficient and scalable tensor dimensionality reduction. However, the theory of\nproperty inheritance for subtensors is still underdevelopment, that is, how the\nessential properties of the original tensor will be passed to its subtensors.\nThis paper theoretically studies the property inheritance of the two key tensor\nproperties, namely incoherence and condition number, under the tensor train\nsetting. We also show how tensor train rank is preserved through fiber-wise\nsampling. The key parameters introduced in theorems are numerically evaluated\nunder various settings. The results show that the properties of interest can be\nwell preserved to the subtensors formed via fiber-wise sampling. Overall, this\npaper provides several handy analytic tools for developing efficient tensor\nanalysis", "published": "2025-04-15 17:10:38", "link": "http://arxiv.org/abs/2504.11396v1", "categories": ["cs.IT", "math.IT", "stat.ML"], "primary_category": "cs.IT"}
{"title": "Taxonomy of Prediction", "abstract": "A prediction makes a claim about a system's future given knowledge of its\npast. A retrodiction makes a claim about its past given knowledge of its\nfuture. We introduce the ambidextrous hidden Markov chain that does both\noptimally -- the bidirectional machine whose state structure makes explicit all\nstatistical correlations in a stochastic process. We introduce an informational\ntaxonomy to profile these correlations via a suite of multivariate information\nmeasures. While prior results laid out the different kinds of information\ncontained in isolated measurements, in addition to being limited to single\nmeasurements the associated informations were challenging to calculate\nexplicitly. Overcoming these via bidirectional machine states, we expand that\nanalysis to information embedded across sequential measurements. The result\nhighlights fourteen new interpretable and calculable information measures that\nfully characterize a process' informational structure. Additionally, we\nintroduce a labeling and indexing scheme that systematizes\ninformation-theoretic analyses of highly complex multivariate systems.\nOperationalizing this, we provide algorithms to directly calculate all of these\nquantities in closed form for finitely-modeled processes.", "published": "2025-04-15 16:36:20", "link": "http://arxiv.org/abs/2504.11371v1", "categories": ["cond-mat.stat-mech", "cs.IT", "math.IT", "nlin.AO"], "primary_category": "cond-mat.stat-mech"}
{"title": "A Mathematical Framework of Semantic Communication based on Category Theory", "abstract": "While semantic communication (SemCom) has recently demonstrated great\npotential to enhance transmission efficiency and reliability by leveraging\nmachine learning (ML) and knowledge base (KB), there is a lack of mathematical\nmodeling to rigorously characterize SemCom system and quantify the performance\ngain obtained from ML and KB. In this paper, we develop a mathematical\nframework for SemCom based on category theory, rigorously model the concepts of\nsemantic entities and semantic probability space. Within this framework,\nsemantic entropy is introduced to quantify the uncertainty of semantic\nentities. We theoretically prove that semantic entropy can be effectively\nreduced by exploiting KB, which capture semantic dependencies. Specifically,\nsemantic entities can be combined based on semantic ambiguity, and are encoded\nbased on contextual relationships among them. Then we refine semantic channel\ncapacity modeling, which considers the mutual information contained in KB to\nbetter reflect SemCom efficiency. Numerical simulations validate the\neffectiveness of the proposed framework, showing that SemCom with KB\nintegration outperforms traditional communication in both entropy reduction and\ncoding efficiency.", "published": "2025-04-15 16:07:33", "link": "http://arxiv.org/abs/2504.11334v1", "categories": ["cs.NI", "cs.IT", "math.IT"], "primary_category": "cs.NI"}
{"title": "Easy repair via codes with simplex locality", "abstract": "In the context of distributed storage systems, locally repairable codes have\nbecome important. In this paper we focus on codes that allow for multi-erasure\npattern decoding with low computational effort. Different optimality\nrequirements, measured by the code's rate, minimum distance, locality,\navailability as well as field size, influence each other and can not all be\nmaximized at the same time. We focus on the notion of easy repair, more\nspecifically on the construction of codes that can repair correctable erasure\npatterns with minimal computational effort. In particular, we introduce the\neasy repair property and then present codes of different rates that possess\nthis property. The presented codes are all in some way related to simplex codes\nand comprise block codes as well as unit-memory convolutional codes. We also\nformulate conditions under which the easy repairs can be performed in parallel,\nthus improving access speed of the distributed storage system.", "published": "2025-04-15 14:47:22", "link": "http://arxiv.org/abs/2504.11251v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Scalable Transceiver Design for Multi-User Communication in FDD Massive MIMO Systems via Deep Learning", "abstract": "This paper addresses the joint transceiver design, including pilot\ntransmission, channel feature extraction and feedback, as well as precoding,\nfor low-overhead downlink massive multiple-input multiple-output (MIMO)\ncommunication in frequency-division duplex (FDD) systems. Although deep\nlearning (DL) has shown great potential in tackling this problem, existing\nmethods often suffer from poor scalability in practical systems, as the\nsolution obtained in the training phase merely works for a fixed feedback\ncapacity and a fixed number of users in the deployment phase. To address this\nlimitation, we propose a novel DL-based framework comprised of choreographed\nneural networks, which can utilize one training phase to generate all the\ntransceiver solutions used in the deployment phase with varying sizes of\nfeedback codebooks and numbers of users. The proposed framework includes a\nresidual vector-quantized variational autoencoder (RVQ-VAE) for efficient\nchannel feedback and an edge graph attention network (EGAT) for robust\nmultiuser precoding. It can adapt to different feedback capacities by flexibly\nadjusting the RVQ codebook sizes using the hierarchical codebook structure, and\nscale with the number of users through a feedback module sharing scheme and the\ninherent scalability of EGAT. Moreover, a progressive training strategy is\nproposed to further enhance data transmission performance and generalization\ncapability. Numerical results on a real-world dataset demonstrate the superior\nscalability and performance of our approach over existing methods.", "published": "2025-04-15 13:11:26", "link": "http://arxiv.org/abs/2504.11162v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Helper-Friendly Latency-Bounded Mitigation Strategies against Reactive Jamming Adversaries", "abstract": "Due to the recent developments in the field of full-duplex radios and\ncognitive radios, a new class of reactive jamming attacks has gained attention\nwherein an adversary transmits jamming energy over the victim's frequency band\nand also monitors various energy statistics in the network so as to detect\ncountermeasures, thereby trapping the victim. Although cooperative mitigation\nstrategies against such security threats exist, they are known to incur\nspectral-efficiency loss on the helper node, and are also not robust to\nvariable latency-constraints on victim's messages. Identifying these research\ngaps in existing countermeasures against reactive jamming attacks, we propose a\nfamily of helper-friendly cooperative mitigation strategies that are applicable\nfor a wide-range of latency-requirements on the victim's messages as well as\npractical radio hardware at the helper nodes. The proposed strategies are\ndesigned to facilitate reliable communication for the victim, without\ncompromising the helper's spectral efficiency and also minimally disturbing the\nvarious energy statistics in the network. For theoretical guarantees on their\nefficacy, interesting optimization problems are formulated on the choice of the\nunderlying parameters, followed by extensive mathematical analyses on their\nerror-performance and covertness. Experimental results indicate that the\nproposed strategies should be preferred over the state-of-the-art methods when\nthe helper node is unwilling to compromise on its error performance for\nassisting the victim.", "published": "2025-04-15 11:56:57", "link": "http://arxiv.org/abs/2504.11110v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "New Constructions of Binary Cyclic Codes with Both Relatively Large Minimum Distance and Dual Distance", "abstract": "Binary cyclic codes are worth studying due to their applications and\ntheoretical importance. It is an important problem to construct an infinite\nfamily of cyclic codes with large minimum distance $d$ and dual distance\n$d^{\\perp}$. In recent years, much research has been devoted to improving the\nlower bound on $d$, some of which have exceeded the square-root bound. The\nconstructions presented recently seem to indicate that when the minimum\ndistance increases, the minimum distance of its dual code decreases. In this\npaper, we focus on the new constructions of binary cyclic codes with length\n$n=2^m-1$, dimension near $n/2$, and both relatively large minimum distance and\ndual distance. For $m$ is even, we construct a family of binary cyclic codes\nwith parameters $[2^m-1,2^{m-1}\\pm1,d]$, where $d\\ge 2^{m/2}-1$ and\n$d^\\perp\\ge2^{m/2}$. Both the minimum distance and the dual distance are\nsignificantly better than the previous results. When $m$ is the product of two\ndistinct primes, we construct some cyclic codes with dimensions $k=(n+1)/2$ and\n$d>\\frac{n}{\\log_2n},$ where the lower bound on the minimum distance is much\nlarger than the square-root bound. For $m$ is odd, we present two families of\nbinary $[2^m-1,2^{m-1},d]$ cyclic codes with $d\\ge2^{(m+1)/2}-1$,\n$d^\\perp\\ge2^{(m+1)/2}$ and $d\\ge2^{(m+3)/2}-15$, $d^\\perp\\ge2^{(m-1)/2}$\nrespectively, which leads that $d\\cdot d^\\perp$ can reach $2n$ asymptotically.\nTo the best of our knowledge, except for the punctured binary Reed-Muller\ncodes, there is no other construction of binary cyclic codes that reaches this\nbound.", "published": "2025-04-15 09:29:41", "link": "http://arxiv.org/abs/2504.11010v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "A Primer on Orthogonal Delay-Doppler Division Multiplexing (ODDM)", "abstract": "As a new type of multicarrier (MC) scheme built upon the recently discovered\ndelay-Doppler domain orthogonal pulse (DDOP), orthogonal delay-Doppler division\nmultiplexing (ODDM) aims to address the challenges of waveform design in linear\ntime-varying channels. In this paper, we explore the design principles of ODDM\nand clarify the key ideas underlying the DDOP. We then derive an alternative\nrepresentation of the DDOP and highlight the fundamental differences between\nODDM and conventional MC schemes. Finally, we discuss and compare two\nimplementation methods for ODDM.", "published": "2025-04-15 07:56:44", "link": "http://arxiv.org/abs/2504.10949v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Low-Overhead Channel Estimation Framework for Beyond Diagonal Reconfigurable Intelligent Surface Assisted Multi-User MIMO Communication", "abstract": "Beyond diagonal reconfigurable intelligent surface (BD-RIS) refers to a\nfamily of RIS architectures characterized by scattering matrices not limited to\nbeing diagonal and enables higher wave manipulation flexibility and large\nperformance gains over conventional (diagonal) RIS. To achieve those promising\ngains, accurate channel state information (CSI) needs to be acquired in BD-RIS\nassisted communication systems. However, the number of coefficients in the\ncascaded channels to be estimated in BD-RIS assisted systems is significantly\nlarger than that in conventional RIS assisted systems, because the channels\nassociated with the off-diagonal elements of the scattering matrix have to be\nestimated as well. Surprisingly, for the first time in the literature, this\npaper rigorously shows that the uplink channel estimation overhead in BD-RIS\nassisted systems is actually of the same order as that in the conventional RIS\nassisted systems. This amazing result stems from a key observation: for each\nuser antenna, its cascaded channel matrix associated with one reference BD-RIS\nelement is a scaled version of that associated with any other BD-RIS element\ndue to the common RIS-base station (BS) channel. In other words, the number of\nindependent unknown variables is far less than it would seem at first glance.\nBuilding upon this property, this paper manages to characterize the minimum\noverhead to perfectly estimate all the channels in the ideal case without noise\nat the BS, and propose a twophase estimation framework for the practical case\nwith noise at the BS. Numerical results demonstrate outstanding channel\nestimation overhead reduction over existing schemes in BD-RIS assisted systems.", "published": "2025-04-15 06:48:08", "link": "http://arxiv.org/abs/2504.10911v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Radiation Footprint Control in Cell-Free Cooperative ISAC: Optimal Joint BS Activation and Beamforming Coordination", "abstract": "Coordinated beamforming across distributed base stations (BSs) in cell-free\narchitectures can efficiently support integrated sensing and communication\n(ISAC) users by improving resource sharing and reducing conflicts in the\nspatial domain. However, coordinating numerous BSs within the ISAC network\nposes risks of generating substantial interference for other networks sharing\nthe spectrum, while also increasing operational costs from power consumption\nand signaling overhead. Therefore, in this paper, we propose an\ninterference-suppressed and cost-optimized cell-free ISAC network by\nopportunistically and cooperatively orchestrating distributed radio resources\nto address competing sensing and communication (S\\&C) demands. Specifically, we\nconceive a radiation footprint control mechanism that autonomously suppresses\ninterference across the entire signal propagation space to safeguard other\nnetworks without exchanging signaling. Then, we propose joint BS activation and\nbeamforming coordination to dynamically activate appropriate BSs and\norchestrate their spatial beams for service provisioning. Building upon this\nframework, we formulate a cost-efficient utility maximization problem that\nconsiders individual S\\&C demands and location-dependent radiation footprint\nconstraints. Since this results in a non-convex optimization problem, we\ndevelop a monotonic optimization embedded branch-and-bound (MO-BRB) algorithm\nto find the optimal solution. Additionally, we apply a low-complexity iterative\nmethod to obtain near-optimal solutions. Finally, simulation results validate\nthe effectiveness of the proposed algorithms.", "published": "2025-04-15 03:14:05", "link": "http://arxiv.org/abs/2504.10830v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "AdapCsiNet: Environment-Adaptive CSI Feedback via Scene Graph-Aided Deep Learning", "abstract": "Accurate channel state information (CSI) is critical for realizing the full\npotential of multiple-antenna wireless communication systems. While deep\nlearning (DL)-based CSI feedback methods have shown promise in reducing\nfeedback overhead, their generalization capability across varying propagation\nenvironments remains limited due to their data-driven nature. Existing\nsolutions based on online training improve adaptability but impose significant\noverhead in terms of data collection and computational resources. In this work,\nwe propose AdapCsiNet, an environment-adaptive DL-based CSI feedback framework\nthat eliminates the need for online training. By integrating environmental\ninformation -- represented as a scene graph -- into a hypernetwork-guided CSI\nreconstruction process, AdapCsiNet dynamically adapts to diverse channel\nconditions. A two-step training strategy is introduced to ensure baseline\nreconstruction performance and effective environment-aware adaptation.\nSimulation results demonstrate that AdapCsiNet achieves up to 46.4% improvement\nin CSI reconstruction accuracy and matches the performance of online learning\nmethods without incurring additional runtime overhead.", "published": "2025-04-15 01:51:15", "link": "http://arxiv.org/abs/2504.10798v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Early Impacts of M365 Copilot", "abstract": "Advances in generative AI have rapidly expanded the potential of computers to\nperform or assist in a wide array of tasks traditionally performed by humans.\nWe analyze a large, real-world randomized experiment of over 6,000 workers at\n56 firms to present some of the earliest evidence on how these technologies are\nchanging the way knowledge workers do their jobs. We find substantial time\nsavings on common core tasks across a wide range of industries and occupations:\nworkers who make use of this technology spent half an hour less reading email\neach week and completed documents 12% faster. Despite the newness of the\ntechnology, nearly 40% of workers who were given access to the tool used it\nregularly in their work throughout the 6-month study.", "published": "2025-04-15 17:55:32", "link": "http://arxiv.org/abs/2504.11443v1", "categories": ["econ.GN", "cs.LG", "q-fin.EC"], "primary_category": "econ.GN"}
{"title": "Shifting Work Patterns with Generative AI", "abstract": "We present evidence on how generative AI changes the work patterns of\nknowledge workers using data from a 6-month-long, cross-industry, randomized\nfield experiment. Half of the 6,000 workers in the study received access to a\ngenerative AI tool integrated into the applications they already used for\nemails, document creation, and meetings. We find that access to the AI tool\nduring the first year of its release primarily impacted behaviors that could be\nchanged independently and not behaviors that required coordination to change:\nworkers who used the tool spent 3 fewer hours, or 25% less time on email each\nweek (intent to treat estimate is 1.4 hours) and seemed to complete documents\nmoderately faster, but did not significantly change time spent in meetings.", "published": "2025-04-15 17:52:00", "link": "http://arxiv.org/abs/2504.11436v1", "categories": ["econ.GN", "cs.LG", "q-fin.EC"], "primary_category": "econ.GN"}
{"title": "Predicting Wave Dynamics using Deep Learning with Multistep Integration Inspired Attention and Physics-Based Loss Decomposition", "abstract": "In this paper, we present a physics-based deep learning framework for\ndata-driven prediction of wave propagation in fluid media. The proposed\napproach, termed Multistep Integration-Inspired Attention (MI2A), combines a\ndenoising-based convolutional autoencoder for reduced latent representation\nwith an attention-based recurrent neural network with long-short-term memory\ncells for time evolution of reduced coordinates. This proposed architecture\ndraws inspiration from classical linear multistep methods to enhance stability\nand long-horizon accuracy in latent-time integration. Despite the efficiency of\nhybrid neural architectures in modeling wave dynamics, autoregressive\npredictions are often prone to accumulating phase and amplitude errors over\ntime. To mitigate this issue within the MI2A framework, we introduce a novel\nloss decomposition strategy that explicitly separates the training loss\nfunction into distinct phase and amplitude components. We assess the\nperformance of MI2A against two baseline reduced-order models trained with\nstandard mean-squared error loss: a sequence-to-sequence recurrent neural\nnetwork and a variant using Luong-style attention. To demonstrate the\neffectiveness of the MI2A model, we consider three benchmark wave propagation\nproblems of increasing complexity, namely one-dimensional linear convection,\nthe nonlinear viscous Burgers equation, and the two-dimensional Saint-Venant\nshallow water system. Our results demonstrate that the MI2A framework\nsignificantly improves the accuracy and stability of long-term predictions,\naccurately preserving wave amplitude and phase characteristics. Compared to the\nstandard long-short term memory and attention-based models, MI2A-based deep\nlearning exhibits superior generalization and temporal accuracy, making it a\npromising tool for real-time wave modeling.", "published": "2025-04-15 17:47:20", "link": "http://arxiv.org/abs/2504.11433v1", "categories": ["cs.LG", "cs.NA", "math.NA", "physics.flu-dyn"], "primary_category": "cs.LG"}
{"title": "MLPs and KANs for data-driven learning in physical problems: A performance comparison", "abstract": "There is increasing interest in solving partial differential equations (PDEs)\nby casting them as machine learning problems. Recently, there has been a spike\nin exploring Kolmogorov-Arnold Networks (KANs) as an alternative to traditional\nneural networks represented by Multi-Layer Perceptrons (MLPs). While showing\npromise, their performance advantages in physics-based problems remain largely\nunexplored. Several critical questions persist: Can KANs capture complex\nphysical dynamics and under what conditions might they outperform traditional\narchitectures? In this work, we present a comparative study of KANs and MLPs\nfor learning physical systems governed by PDEs. We assess their performance\nwhen applied in deep operator networks (DeepONet) and graph network-based\nsimulators (GNS), and test them on physical problems that vary significantly in\nscale and complexity. Drawing inspiration from the Kolmogorov Representation\nTheorem, we examine the behavior of KANs and MLPs across shallow and deep\nnetwork architectures. Our results reveal that although KANs do not\nconsistently outperform MLPs when configured as deep neural networks, they\ndemonstrate superior expressiveness in shallow network settings, significantly\noutpacing MLPs in accuracy over our test cases. This suggests that KANs are a\npromising choice, offering a balance of efficiency and accuracy in applications\ninvolving physical systems.", "published": "2025-04-15 17:13:42", "link": "http://arxiv.org/abs/2504.11397v1", "categories": ["cs.LG", "physics.comp-ph"], "primary_category": "cs.LG"}
{"title": "Accelerating Multiscale Modeling with Hybrid Solvers: Coupling FEM and Neural Operators with Domain Decomposition", "abstract": "Numerical solvers for partial differential equations (PDEs) face challenges\nbalancing computational cost and accuracy, especially in multiscale and dynamic\nsystems. Neural operators can significantly speed up simulations; however, they\noften face challenges such as error accumulation and limited generalization in\nmultiphysics problems. This work introduces a novel hybrid framework that\nintegrates physics-informed DeepONet with FEM through domain decomposition. The\ncore innovation lies in adaptively coupling FEM and DeepONet subdomains via a\nSchwarz alternating method. This methodology strategically allocates\ncomputationally demanding regions to a pre-trained Deep Operator Network, while\nthe remaining computational domain is solved through FEM. To address dynamic\nsystems, we integrate the Newmark time-stepping scheme directly into the\nDeepONet, significantly mitigating error accumulation in long-term simulations.\nFurthermore, an adaptive subdomain evolution enables the ML-resolved region to\nexpand dynamically, capturing emerging fine-scale features without remeshing.\nThe framework's efficacy has been validated across a range of solid mechanics\nproblems, including static, quasi-static, and dynamic regimes, demonstrating\naccelerated convergence rates (up to 20% improvement compared to FE-FE\napproaches), while preserving solution fidelity with error < 1%. Our case\nstudies show that our proposed hybrid solver: (1) maintains solution continuity\nacross subdomain interfaces, (2) reduces computational costs by eliminating\nfine mesh requirements, (3) mitigates error accumulation in time-dependent\nsimulations, and (4) enables automatic adaptation to evolving physical\nphenomena. This work bridges the gap between numerical methods and AI-driven\nsurrogates, offering a scalable pathway for high-fidelity simulations in\nengineering and scientific applications.", "published": "2025-04-15 16:54:04", "link": "http://arxiv.org/abs/2504.11383v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "An Adaptive Dropout Approach for High-Dimensional Bayesian Optimization", "abstract": "Bayesian optimization (BO) is a widely used algorithm for solving expensive\nblack-box optimization problems. However, its performance decreases\nsignificantly on high-dimensional problems due to the inherent\nhigh-dimensionality of the acquisition function. In the proposed algorithm, we\nadaptively dropout the variables of the acquisition function along the\niterations. By gradually reducing the dimension of the acquisition function,\nthe proposed approach has less and less difficulty to optimize the acquisition\nfunction. Numerical experiments demonstrate that AdaDropout effectively tackle\nhigh-dimensional challenges and improve solution quality where standard\nBayesian optimization methods often struggle. Moreover, it achieves superior\nresults when compared with state-of-the-art high-dimensional Bayesian\noptimization approaches. This work provides a simple yet efficient solution for\nhigh-dimensional expensive optimization.", "published": "2025-04-15 16:23:25", "link": "http://arxiv.org/abs/2504.11353v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Erzeugunsgrad, VC-Dimension and Neural Networks with rational activation function", "abstract": "The notion of Erzeugungsgrad was introduced by Joos Heintz in 1983 to bound\nthe number of non-empty cells occurring after a process of quantifier\nelimination. We extend this notion and the combinatorial bounds of Theorem 2 in\nHeintz (1983) using the degree for constructible sets defined in\nPardo-Sebasti\\'an (2022). We show that the Erzeugungsgrad is the key ingredient\nto connect affine Intersection Theory over algebraically closed fields and the\nVC-Theory of Computational Learning Theory for families of classifiers given by\nparameterized families of constructible sets. In particular, we prove that the\nVC-dimension and the Krull dimension are linearly related up to logarithmic\nfactors based on Intersection Theory. Using this relation, we study the density\nof correct test sequences in evasive varieties. We apply these ideas to analyze\nparameterized families of neural networks with rational activation function.", "published": "2025-04-15 16:16:38", "link": "http://arxiv.org/abs/2504.11345v1", "categories": ["cs.LG", "math.AG", "13F20, 14A10, 68T07"], "primary_category": "cs.LG"}
{"title": "Subset-Contrastive Multi-Omics Network Embedding", "abstract": "Motivation: Network-based analyses of omics data are widely used, and while\nmany of these methods have been adapted to single-cell scenarios, they often\nremain memory- and space-intensive. As a result, they are better suited to\nbatch data or smaller datasets. Furthermore, the application of network-based\nmethods in multi-omics often relies on similarity-based networks, which lack\nstructurally-discrete topologies. This limitation may reduce the effectiveness\nof graph-based methods that were initially designed for topologies with better\ndefined structures. Results: We propose Subset-Contrastive multi-Omics Network\nEmbedding (SCONE), a method that employs contrastive learning techniques on\nlarge datasets through a scalable subgraph contrastive approach. By exploiting\nthe pairwise similarity basis of many network-based omics methods, we\ntransformed this characteristic into a strength, developing an approach that\naims to achieve scalable and effective analysis. Our method demonstrates\nsynergistic omics integration for cell type clustering in single-cell data.\nAdditionally, we evaluate its performance in a bulk multi-omics integration\nscenario, where SCONE performs comparable to the state-of-the-art despite\nutilising limited views of the original data. We anticipate that our findings\nwill motivate further research into the use of subset contrastive methods for\nomics data.", "published": "2025-04-15 16:01:39", "link": "http://arxiv.org/abs/2504.11321v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Mildly-Interacting Fermionic Unitaries are Efficiently Learnable", "abstract": "Recent work has shown that one can efficiently learn fermionic Gaussian\nunitaries, also commonly known as nearest-neighbor matchcircuits or\nnon-interacting fermionic unitaries. However, one could ask a similar question\nabout unitaries that are near Gaussian: for example, unitaries prepared with a\nsmall number of non-Gaussian circuit elements. These operators find\nsignificance in quantum chemistry and many-body physics, yet no algorithm\nexists to learn them.\n  We give the first such result by devising an algorithm which makes queries to\na $n$-mode fermionic unitary $U$ prepared by at most $O(t)$ non-Gaussian gates\nand returns a circuit approximating $U$ to diamond distance $\\varepsilon$ in\ntime $\\textrm{poly}(n,2^t,1/\\varepsilon)$. This resolves a central open\nquestion of Mele and Herasymenko under the strongest distance metric. In fact,\nour algorithm is much more general: we define a property of unitary Gaussianity\nknown as unitary Gaussian dimension and show that our algorithm can learn\n$n$-mode unitaries of Gaussian dimension at least $2n - O(t)$ in time\n$\\textrm{poly}(n,2^t,1/\\varepsilon)$. Indeed, this class subsumes unitaries\nprepared by at most $O(t)$ non-Gaussian gates but also includes several\nunitaries that require up to $2^{O(t)}$ non-Gaussian gates to construct.\n  In addition, we give a $\\textrm{poly}(n,1/\\varepsilon)$-time algorithm to\ndistinguish whether an $n$-mode unitary is of Gaussian dimension at least $k$\nor $\\varepsilon$-far from all such unitaries in Frobenius distance, promised\nthat one is the case. Along the way, we prove structural results about\nnear-Gaussian fermionic unitaries that are likely to be of independent\ninterest.", "published": "2025-04-15 15:59:32", "link": "http://arxiv.org/abs/2504.11318v1", "categories": ["quant-ph", "cs.DS", "cs.LG"], "primary_category": "quant-ph"}
{"title": "Differentially Private Geodesic and Linear Regression", "abstract": "In statistical applications it has become increasingly common to encounter\ndata structures that live on non-linear spaces such as manifolds. Classical\nlinear regression, one of the most fundamental methodologies of statistical\nlearning, captures the relationship between an independent variable and a\nresponse variable which both are assumed to live in Euclidean space. Thus,\ngeodesic regression emerged as an extension where the response variable lives\non a Riemannian manifold. The parameters of geodesic regression, as with linear\nregression, capture the relationship of sensitive data and hence one should\nconsider the privacy protection practices of said parameters. We consider\nreleasing Differentially Private (DP) parameters of geodesic regression via the\nK-Norm Gradient (KNG) mechanism for Riemannian manifolds. We derive theoretical\nbounds for the sensitivity of the parameters showing they are tied to their\nrespective Jacobi fields and hence the curvature of the space. This\ncorroborates recent findings of differential privacy for the Fr\\'echet mean. We\ndemonstrate the efficacy of our methodology on the sphere,\n$\\mbS^2\\subset\\mbR^3$ and, since it is general to Riemannian manifolds, the\nmanifold of Euclidean space which simplifies geodesic regression to a case of\nlinear regression. Our methodology is general to any Riemannian manifold and\nthus it is suitable for data in domains such as medical imaging and computer\nvision.", "published": "2025-04-15 15:45:48", "link": "http://arxiv.org/abs/2504.11304v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Limits of Discrete Energy of Families of Increasing Sets", "abstract": "The Hausdorff dimension of a set can be detected using the Riesz energy.\nHere, we consider situations where a sequence of points, $\\{x_n\\}$, ``fills\nin'' a set $E \\subset \\mathbb{R}^d$ in an appropriate sense and investigate the\ndegree to which the discrete analog to the Riesz energy of these sets can be\nused to bound the Hausdorff dimension of $E$. We also discuss applications to\ndata science and Erd\\H{o}s/Falconer type problems.", "published": "2025-04-15 15:45:14", "link": "http://arxiv.org/abs/2504.11302v1", "categories": ["math.CA", "cs.LG", "math.MG"], "primary_category": "math.CA"}
{"title": "Efficient and Stable Multi-Dimensional Kolmogorov-Smirnov Distance", "abstract": "We revisit extending the Kolmogorov-Smirnov distance between probability\ndistributions to the multidimensional setting and make new arguments about the\nproper way to approach this generalization. Our proposed formulation maximizes\nthe difference over orthogonal dominating rectangular ranges (d-sided\nrectangles in R^d), and is an integral probability metric. We also prove that\nthe distance between a distribution and a sample from the distribution\nconverges to 0 as the sample size grows, and bound this rate. Moreover, we show\nthat one can, up to this same approximation error, compute the distance\nefficiently in 4 or fewer dimensions; specifically the runtime is near-linear\nin the size of the sample needed for that error. With this, we derive a\ndelta-precision two-sample hypothesis test using this distance. Finally, we\nshow these metric and approximation properties do not hold for other popular\nvariants.", "published": "2025-04-15 15:42:49", "link": "http://arxiv.org/abs/2504.11299v1", "categories": ["stat.CO", "cs.CG", "cs.LG"], "primary_category": "stat.CO"}
{"title": "Multi-Agent Reinforcement Learning for Greenhouse Gas Offset Credit Markets", "abstract": "Climate change is a major threat to the future of humanity, and its impacts\nare being intensified by excess man-made greenhouse gas emissions. One method\ngovernments can employ to control these emissions is to provide firms with\nemission limits and penalize any excess emissions above the limit. Excess\nemissions may also be offset by firms who choose to invest in carbon reducing\nand capturing projects. These projects generate offset credits which can be\nsubmitted to a regulating agency to offset a firm's excess emissions, or they\ncan be traded with other firms. In this work, we characterize the finite-agent\nNash equilibrium for offset credit markets. As computing Nash equilibria is an\nNP-hard problem, we utilize the modern reinforcement learning technique\nNash-DQN to efficiently estimate the market's Nash equilibria. We demonstrate\nnot only the validity of employing reinforcement learning methods applied to\nclimate themed financial markets, but also the significant financial savings\nemitting firms may achieve when abiding by the Nash equilibria through\nnumerical experiments.", "published": "2025-04-15 14:56:42", "link": "http://arxiv.org/abs/2504.11258v1", "categories": ["q-fin.MF", "cs.LG"], "primary_category": "q-fin.MF"}
{"title": "Reconstructing Fine-Grained Network Data using Autoencoder Architectures with Domain Knowledge Penalties", "abstract": "The ability to reconstruct fine-grained network session data, including\nindividual packets, from coarse-grained feature vectors is crucial for\nimproving network security models. However, the large-scale collection and\nstorage of raw network traffic pose significant challenges, particularly for\ncapturing rare cyberattack samples. These challenges hinder the ability to\nretain comprehensive datasets for model training and future threat detection.\nTo address this, we propose a machine learning approach guided by formal\nmethods to encode and reconstruct network data. Our method employs autoencoder\nmodels with domain-informed penalties to impute PCAP session headers from\nstructured feature representations. Experimental results demonstrate that\nincorporating domain knowledge through constraint-based loss terms\nsignificantly improves reconstruction accuracy, particularly for categorical\nfeatures with session-level encodings. By enabling efficient reconstruction of\ndetailed network sessions, our approach facilitates data-efficient model\ntraining while preserving privacy and storage efficiency.", "published": "2025-04-15 14:51:44", "link": "http://arxiv.org/abs/2504.11255v1", "categories": ["cs.LG", "cs.NI"], "primary_category": "cs.LG"}
{"title": "The Forward-Forward Algorithm: Characterizing Training Behavior", "abstract": "The Forward-Forward algorithm is an alternative learning method which\nconsists of two forward passes rather than a forward and backward pass employed\nby backpropagation. Forward-Forward networks employ layer local loss functions\nwhich are optimized based on the layer activation for each forward pass rather\nthan a single global objective function. This work explores the dynamics of\nmodel and layer accuracy changes in Forward-Forward networks as training\nprogresses in pursuit of a mechanistic understanding of their internal\nbehavior. Treatments to various system characteristics are applied to\ninvestigate changes in layer and overall model accuracy as training progresses,\nhow accuracy is impacted by layer depth, and how strongly individual layer\naccuracy is correlated with overall model accuracy. The empirical results\npresented suggest that layers deeper within Forward-Forward networks experience\na delay in accuracy improvement relative to shallower layers and that shallower\nlayer accuracy is strongly correlated with overall model accuracy.", "published": "2025-04-15 14:30:18", "link": "http://arxiv.org/abs/2504.11229v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "VEXP: A Low-Cost RISC-V ISA Extension for Accelerated Softmax Computation in Transformers", "abstract": "While Transformers are dominated by Floating-Point (FP)\nMatrix-Multiplications, their aggressive acceleration through dedicated\nhardware or many-core programmable systems has shifted the performance\nbottleneck to non-linear functions like Softmax. Accelerating Softmax is\nchallenging due to its non-pointwise, non-linear nature, with exponentiation as\nthe most demanding step. To address this, we design a custom arithmetic block\nfor Bfloat16 exponentiation leveraging a novel approximation algorithm based on\nSchraudolph's method, and we integrate it into the Floating-Point Unit (FPU) of\nthe RISC-V cores of a compute cluster, through custom Instruction Set\nArchitecture (ISA) extensions, with a negligible area overhead of 1\\%. By\noptimizing the software kernels to leverage the extension, we execute Softmax\nwith 162.7$\\times$ less latency and 74.3$\\times$ less energy compared to the\nbaseline cluster, achieving an 8.2$\\times$ performance improvement and\n4.1$\\times$ higher energy efficiency for the FlashAttention-2 kernel in GPT-2\nconfiguration. Moreover, the proposed approach enables a multi-cluster system\nto efficiently execute end-to-end inference of pre-trained Transformer models,\nsuch as GPT-2, GPT-3 and ViT, achieving up to 5.8$\\times$ and 3.6$\\times$\nreduction in latency and energy consumption, respectively, without requiring\nre-training and with negligible accuracy loss.", "published": "2025-04-15 14:28:48", "link": "http://arxiv.org/abs/2504.11227v1", "categories": ["cs.AR", "cs.LG"], "primary_category": "cs.AR"}
{"title": "SDFs from Unoriented Point Clouds using Neural Variational Heat Distances", "abstract": "We propose a novel variational approach for computing neural Signed Distance\nFields (SDF) from unoriented point clouds. To this end, we replace the commonly\nused eikonal equation with the heat method, carrying over to the neural domain\nwhat has long been standard practice for computing distances on discrete\nsurfaces. This yields two convex optimization problems for whose solution we\nemploy neural networks: We first compute a neural approximation of the\ngradients of the unsigned distance field through a small time step of heat flow\nwith weighted point cloud densities as initial data. Then we use it to compute\na neural approximation of the SDF. We prove that the underlying variational\nproblems are well-posed. Through numerical experiments, we demonstrate that our\nmethod provides state-of-the-art surface reconstruction and consistent SDF\ngradients. Furthermore, we show in a proof-of-concept that it is accurate\nenough for solving a PDE on the zero-level set.", "published": "2025-04-15 14:13:54", "link": "http://arxiv.org/abs/2504.11212v1", "categories": ["math.NA", "cs.LG", "cs.NA", "65K10, 68T07, 65D18, 49J45"], "primary_category": "math.NA"}
{"title": "A Real-time Anomaly Detection Method for Robots based on a Flexible and Sparse Latent Space", "abstract": "The growing demand for robots to operate effectively in diverse environments\nnecessitates the need for robust real-time anomaly detection techniques during\nrobotic operations. However, deep learning-based models in robotics face\nsignificant challenges due to limited training data and highly noisy signal\nfeatures. In this paper, we present Sparse Masked Autoregressive Flow-based\nAdversarial AutoEncoders model to address these problems. This approach\nintegrates Masked Autoregressive Flow model into Adversarial AutoEncoders to\nconstruct a flexible latent space and utilize Sparse autoencoder to efficiently\nfocus on important features, even in scenarios with limited feature space. Our\nexperiments demonstrate that the proposed model achieves a 4.96% to 9.75%\nhigher area under the receiver operating characteristic curve for\npick-and-place robotic operations with randomly placed cans, compared to\nexisting state-of-the-art methods. Notably, it showed up to 19.67% better\nperformance in scenarios involving collisions with lightweight objects.\nAdditionally, unlike the existing state-of-the-art model, our model performs\ninferences within 1 millisecond, ensuring real-time anomaly detection. These\ncapabilities make our model highly applicable to machine learning-based robotic\nsafety systems in dynamic environments. The code will be made publicly\navailable after acceptance.", "published": "2025-04-15 13:17:14", "link": "http://arxiv.org/abs/2504.11170v1", "categories": ["cs.RO", "cs.LG"], "primary_category": "cs.RO"}
{"title": "TD-Suite: All Batteries Included Framework for Technical Debt Classification", "abstract": "Recognizing that technical debt is a persistent and significant challenge\nrequiring sophisticated management tools, TD-Suite offers a comprehensive\nsoftware framework specifically engineered to automate the complex task of its\nclassification within software projects. It leverages the advanced natural\nlanguage understanding of state-of-the-art transformer models to analyze\ntextual artifacts, such as developer discussions in issue reports, where subtle\nindicators of debt often lie hidden.\n  TD-Suite provides a seamless end-to-end pipeline, managing everything from\ninitial data ingestion and rigorous preprocessing to model training, thorough\nevaluation, and final inference. This allows it to support both straightforward\nbinary classification (debt or no debt) and more valuable, identifying specific\ncategories like code, design, or documentation debt, thus enabling more\ntargeted management strategies.\n  To ensure the generated models are robust and perform reliably on real-world,\noften imbalanced, datasets, TD-Suite incorporates critical training\nmethodologies: k-fold cross-validation assesses generalization capability,\nearly stopping mechanisms prevent overfitting to the training data, and class\nweighting strategies effectively address skewed data distributions. Beyond core\nfunctionality, and acknowledging the growing importance of sustainability, the\nframework integrates tracking and reporting of carbon emissions associated with\nthe computationally intensive model training process.\n  It also features a user-friendly Gradio web interface in a Docker container\nsetup, simplifying model interaction, evaluation, and inference.", "published": "2025-04-15 11:31:17", "link": "http://arxiv.org/abs/2504.11085v1", "categories": ["cs.SE", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Scalability and Maintainability Challenges and Solutions in Machine Learning: Systematic Literature Review", "abstract": "This systematic literature review examines the critical challenges and\nsolutions related to scalability and maintainability in Machine Learning (ML)\nsystems. As ML applications become increasingly complex and widespread across\nindustries, the need to balance system scalability with long-term\nmaintainability has emerged as a significant concern. This review synthesizes\ncurrent research and practices addressing these dual challenges across the\nentire ML life-cycle, from data engineering to model deployment in production.\nWe analyzed 124 papers to identify and categorize 41 maintainability challenges\nand 13 scalability challenges, along with their corresponding solutions. Our\nfindings reveal intricate inter dependencies between scalability and\nmaintainability, where improvements in one often impact the other.\n  The review is structured around six primary research questions, examining\nmaintainability and scalability challenges in data engineering, model\nengineering, and ML system development. We explore how these challenges\nmanifest differently across various stages of the ML life-cycle.\n  This comprehensive overview offers valuable insights for both researchers and\npractitioners in the field of ML systems. It aims to guide future research\ndirections, inform best practices, and contribute to the development of more\nrobust, efficient, and sustainable ML applications across various domains.", "published": "2025-04-15 11:24:43", "link": "http://arxiv.org/abs/2504.11079v1", "categories": ["cs.SE", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Using Time Structure to Estimate Causal Effects", "abstract": "There exist several approaches for estimating causal effects in time series\nwhen latent confounding is present. Many of these approaches rely on additional\nauxiliary observed variables or time series such as instruments, negative\ncontrols or time series that satisfy the front- or backdoor criterion in\ncertain graphs. In this paper, we present a novel approach for estimating\ndirect (and via Wright's path rule total) causal effects in a time series setup\nwhich does not rely on additional auxiliary observed variables or time series.\nThis approach assumes that the underlying time series is a Structural Vector\nAutoregressive (SVAR) process and estimates direct causal effects by solving\ncertain linear equation systems made up of different covariances and model\nparameters. We state sufficient graphical criteria in terms of the so-called\nfull time graph under which these linear equations systems are uniquely\nsolvable and under which their solutions contain the to-be-identified direct\ncausal effects as components. We also state sufficient lag-based criteria under\nwhich the previously mentioned graphical conditions are satisfied and, thus,\nunder which direct causal effects are identifiable. Several numerical\nexperiments underline the correctness and applicability of our results.", "published": "2025-04-15 11:21:37", "link": "http://arxiv.org/abs/2504.11076v1", "categories": ["stat.ME", "cs.LG", "62M10", "G.3"], "primary_category": "stat.ME"}
{"title": "Morphing-based Compression for Data-centric ML Pipelines", "abstract": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours.", "published": "2025-04-15 11:02:34", "link": "http://arxiv.org/abs/2504.11067v1", "categories": ["cs.DB", "cs.DC", "cs.LG"], "primary_category": "cs.DB"}
{"title": "Zero-Shot Whole-Body Humanoid Control via Behavioral Foundation Models", "abstract": "Unsupervised reinforcement learning (RL) aims at pre-training agents that can\nsolve a wide range of downstream tasks in complex environments. Despite recent\nadvancements, existing approaches suffer from several limitations: they may\nrequire running an RL process on each downstream task to achieve a satisfactory\nperformance, they may need access to datasets with good coverage or\nwell-curated task-specific samples, or they may pre-train policies with\nunsupervised losses that are poorly correlated with the downstream tasks of\ninterest. In this paper, we introduce a novel algorithm regularizing\nunsupervised RL towards imitating trajectories from unlabeled behavior\ndatasets. The key technical novelty of our method, called Forward-Backward\nRepresentations with Conditional-Policy Regularization, is to train\nforward-backward representations to embed the unlabeled trajectories to the\nsame latent space used to represent states, rewards, and policies, and use a\nlatent-conditional discriminator to encourage policies to ``cover'' the states\nin the unlabeled behavior dataset. As a result, we can learn policies that are\nwell aligned with the behaviors in the dataset, while retaining zero-shot\ngeneralization capabilities for reward-based and imitation tasks. We\ndemonstrate the effectiveness of this new approach in a challenging humanoid\ncontrol problem: leveraging observation-only motion capture datasets, we train\nMeta Motivo, the first humanoid behavioral foundation model that can be\nprompted to solve a variety of whole-body tasks, including motion tracking,\ngoal reaching, and reward optimization. The resulting model is capable of\nexpressing human-like behaviors and it achieves competitive performance with\ntask-specific methods while outperforming state-of-the-art unsupervised RL and\nmodel-based baselines.", "published": "2025-04-15 10:41:11", "link": "http://arxiv.org/abs/2504.11054v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "QualiTagger: Automating software quality detection in issue trackers", "abstract": "A systems quality is a major concern for development teams when it evolve.\nUnderstanding the effects of a loss of quality in the codebase is crucial to\navoid side effects like the appearance of technical debt. Although the\nidentification of these qualities in software requirements described in natural\nlanguage has been investigated, most of the results are often not applicable in\npractice, and rely on having been validated on small datasets and limited\namount of projects. For many years, machine learning (ML) techniques have been\nproved as a valid technique to identify and tag terms described in natural\nlanguage. In order to advance previous works, in this research we use cutting\nedge models like Transformers, together with a vast dataset mined and curated\nfrom GitHub, to identify what text is usually associated with different quality\nproperties. We also study the distribution of such qualities in issue trackers\nfrom openly accessible software repositories, and we evaluate our approach both\nwith students from a software engineering course and with its application to\nrecognize security labels in industry.", "published": "2025-04-15 10:40:40", "link": "http://arxiv.org/abs/2504.11053v1", "categories": ["cs.SE", "cs.LG"], "primary_category": "cs.SE"}
{"title": "A PyTorch-Compatible Spike Encoding Framework for Energy-Efficient Neuromorphic Applications", "abstract": "Spiking Neural Networks (SNNs) offer promising energy efficiency advantages,\nparticularly when processing sparse spike trains. However, their\nincompatibility with traditional datasets, which consist of batches of input\nvectors rather than spike trains, necessitates the development of efficient\nencoding methods. This paper introduces a novel, open-source PyTorch-compatible\nPython framework for spike encoding, designed for neuromorphic applications in\nmachine learning and reinforcement learning. The framework supports a range of\nencoding algorithms, including Leaky Integrate-and-Fire (LIF), Step Forward\n(SF), Pulse Width Modulation (PWM), and Ben's Spiker Algorithm (BSA), as well\nas specialized encoding strategies covering population coding and reinforcement\nlearning scenarios. Furthermore, we investigate the performance trade-offs of\neach method on embedded hardware using C/C++ implementations, considering\nenergy consumption, computation time, spike sparsity, and reconstruction\naccuracy. Our findings indicate that SF typically achieves the lowest\nreconstruction error and offers the highest energy efficiency and fastest\nencoding speed, achieving the second-best spike sparsity. At the same time,\nother methods demonstrate particular strengths depending on the signal\ncharacteristics. This framework and the accompanying empirical analysis provide\nvaluable resources for selecting optimal encoding strategies for\nenergy-efficient SNN applications.", "published": "2025-04-15 09:50:03", "link": "http://arxiv.org/abs/2504.11026v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Leveraging Vertical Public-Private Split for Improved Synthetic Data Generation", "abstract": "Differentially Private Synthetic Data Generation (DP-SDG) is a key enabler of\nprivate and secure tabular-data sharing, producing artificial data that carries\nthrough the underlying statistical properties of the input data. This typically\ninvolves adding carefully calibrated statistical noise to guarantee individual\nprivacy, at the cost of synthetic data quality. Recent literature has explored\nscenarios where a small amount of public data is used to help enhance the\nquality of synthetic data. These methods study a horizontal public-private\npartitioning which assumes access to a small number of public rows that can be\nused for model initialization, providing a small utility gain. However,\nrealistic datasets often naturally consist of public and private attributes,\nmaking a vertical public-private partitioning relevant for practical synthetic\ndata deployments. We propose a novel framework that adapts horizontal\npublic-assisted methods into the vertical setting. We compare this framework\nagainst our alternative approach that uses conditional generation, highlighting\ninitial limitations of public-data assisted methods and proposing future\nresearch directions to address these challenges.", "published": "2025-04-15 08:59:03", "link": "http://arxiv.org/abs/2504.10987v1", "categories": ["cs.LG", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Early Detection of Cognitive Impairment in Elderly using a Passive FPVS-EEG BCI and Machine Learning -- Extended Version", "abstract": "Early dementia diagnosis requires biomarkers sensitive to both structural and\nfunctional brain changes. While structural neuroimaging biomarkers have\nprogressed significantly, objective functional biomarkers of early cognitive\ndecline remain a critical unmet need. Current cognitive assessments often rely\non behavioral responses, making them susceptible to factors like effort,\npractice effects, and educational background, thereby hindering early and\naccurate detection. This work introduces a novel approach, leveraging a\nlightweight convolutional neural network (CNN) to infer cognitive impairment\nlevels directly from electroencephalography (EEG) data. Critically, this method\nemploys a passive fast periodic visual stimulation (FPVS) paradigm, eliminating\nthe need for explicit behavioral responses or task comprehension from the\nparticipant. This passive approach provides an objective measure of working\nmemory function, independent of confounding factors inherent in active\ncognitive tasks, and offers a promising new avenue for early and unbiased\ndetection of cognitive decline.", "published": "2025-04-15 08:34:13", "link": "http://arxiv.org/abs/2504.10973v1", "categories": ["q-bio.NC", "cs.HC", "cs.LG", "J.3"], "primary_category": "q-bio.NC"}
{"title": "Learning-Based User Association for MmWave Vehicular Networks With Kernelized Contextual Bandits", "abstract": "Vehicles require timely channel conditions to determine the base station (BS)\nto communicate with, but it is costly to estimate the fast-fading mmWave\nchannels frequently. Without additional channel estimations, the proposed\nDistributed Kernelized Upper Confidence Bound (DK-UCB) algorithm estimates the\ncurrent instantaneous transmission rates utilizing past contexts, such as the\nvehicle's location and velocity, along with past instantaneous transmission\nrates. To capture the nonlinear mapping from a context to the instantaneous\ntransmission rate, DK-UCB maps a context into the reproducing kernel Hilbert\nspace (RKHS) where a linear mapping becomes observable. To improve estimation\naccuracy, we propose a novel kernel function in RKHS which incorporates the\npropagation characteristics of the mmWave signals. Moreover, DK-UCB encourages\na vehicle to share necessary information when it has conducted significant\nexplorations, which speeds up the learning process while maintaining affordable\ncommunication costs.", "published": "2025-04-15 08:05:27", "link": "http://arxiv.org/abs/2504.10959v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers", "abstract": "Task arithmetic refers to editing the pre-trained model by adding a weighted\nsum of task vectors, each of which is the weight update from the pre-trained\nmodel to fine-tuned models for certain tasks. This approach recently gained\nattention as a computationally efficient inference method for model editing,\ne.g., multi-task learning, forgetting, and out-of-domain generalization\ncapabilities. However, the theoretical understanding of why task vectors can\nexecute various conceptual operations remains limited, due to the highly\nnon-convexity of training Transformer-based models. To the best of our\nknowledge, this paper provides the first theoretical characterization of the\ngeneralization guarantees of task vector methods on nonlinear Transformers. We\nconsider a conceptual learning setting, where each task is a binary\nclassification problem based on a discriminative pattern. We theoretically\nprove the effectiveness of task addition in simultaneously learning a set of\nirrelevant or aligned tasks, as well as the success of task negation in\nunlearning one task from irrelevant or contradictory tasks. Moreover, we prove\nthe proper selection of linear coefficients for task arithmetic to achieve\nguaranteed generalization to out-of-domain tasks. All of our theoretical\nresults hold for both dense-weight parameters and their low-rank\napproximations. Although established in a conceptual setting, our theoretical\nfindings were validated on a practical machine unlearning task using the large\nlanguage model Phi-1.5 (1.3B).", "published": "2025-04-15 08:04:39", "link": "http://arxiv.org/abs/2504.10957v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Multi-scale DeepOnet (Mscale-DeepOnet) for Mitigating Spectral Bias in Learning High Frequency Operators of Oscillatory Functions", "abstract": "In this paper, a multi-scale DeepOnet (Mscale-DeepOnet) is proposed to reduce\nthe spectral bias of the DeepOnet in learning high-frequency mapping between\nhighly oscillatory functions, with an application to the nonlinear mapping\nbetween the coefficient of the Helmholtz equation and its solution. The\nMscale-DeepOnet introduces the multiscale neural network in the branch and\ntrunk networks of the original DeepOnet, the resulting Mscale-DeepOnet is shown\nto be able to capture various high-frequency components of the mapping itself\nand its image. Numerical results demonstrate the substantial improvement of the\nMscale-DeepOnet for the problem of wave scattering in the high-frequency regime\nover the normal DeepOnet with a similar number of network parameters.", "published": "2025-04-15 07:19:54", "link": "http://arxiv.org/abs/2504.10932v1", "categories": ["cs.LG", "cs.NA", "math.NA"], "primary_category": "cs.LG"}
{"title": "Fast-Powerformer: A Memory-Efficient Transformer for Accurate Mid-Term Wind Power Forecasting", "abstract": "Wind power forecasting (WPF), as a significant research topic within\nrenewable energy, plays a crucial role in enhancing the security, stability,\nand economic operation of power grids. However, due to the high stochasticity\nof meteorological factors (e.g., wind speed) and significant fluctuations in\nwind power output, mid-term wind power forecasting faces a dual challenge of\nmaintaining high accuracy and computational efficiency. To address these\nissues, this paper proposes an efficient and lightweight mid-term wind power\nforecasting model, termed Fast-Powerformer. The proposed model is built upon\nthe Reformer architecture, incorporating structural enhancements such as a\nlightweight Long Short-Term Memory (LSTM) embedding module, an input\ntransposition mechanism, and a Frequency Enhanced Channel Attention Mechanism\n(FECAM). These improvements enable the model to strengthen temporal feature\nextraction, optimize dependency modeling across variables, significantly reduce\ncomputational complexity, and enhance sensitivity to periodic patterns and\ndominant frequency components. Experimental results conducted on multiple\nreal-world wind farm datasets demonstrate that the proposed Fast-Powerformer\nachieves superior prediction accuracy and operational efficiency compared to\nmainstream forecasting approaches. Furthermore, the model exhibits fast\ninference speed and low memory consumption, highlighting its considerable\npractical value for real-world deployment scenarios.", "published": "2025-04-15 07:09:54", "link": "http://arxiv.org/abs/2504.10923v1", "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Leveraging Submodule Linearity Enhances Task Arithmetic Performance in LLMs", "abstract": "Task arithmetic is a straightforward yet highly effective strategy for model\nmerging, enabling the resultant model to exhibit multi-task capabilities.\nRecent research indicates that models demonstrating linearity enhance the\nperformance of task arithmetic. In contrast to existing methods that rely on\nthe global linearization of the model, we argue that this linearity already\nexists within the model's submodules. In particular, we present a statistical\nanalysis and show that submodules (e.g., layers, self-attentions, and MLPs)\nexhibit significantly higher linearity than the overall model. Based on these\nfindings, we propose an innovative model merging strategy that independently\nmerges these submodules. Especially, we derive a closed-form solution for\noptimal merging weights grounded in the linear properties of these submodules.\nExperimental results demonstrate that our method consistently outperforms the\nstandard task arithmetic approach and other established baselines across\ndifferent model scales and various tasks. This result highlights the benefits\nof leveraging the linearity of submodules and provides a new perspective for\nexploring solutions for effective and practical multi-task model merging.", "published": "2025-04-15 06:23:24", "link": "http://arxiv.org/abs/2504.10902v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "ICAFS: Inter-Client-Aware Feature Selection for Vertical Federated Learning", "abstract": "Vertical federated learning (VFL) enables a paradigm for vertically\npartitioned data across clients to collaboratively train machine learning\nmodels. Feature selection (FS) plays a crucial role in Vertical Federated\nLearning (VFL) due to the unique nature that data are distributed across\nmultiple clients. In VFL, different clients possess distinct subsets of\nfeatures for overlapping data samples, making the process of identifying and\nselecting the most relevant features a complex yet essential task. Previous FS\nefforts have primarily revolved around intra-client feature selection,\noverlooking vital feature interaction across clients, leading to subpar model\noutcomes. We introduce ICAFS, a novel multi-stage ensemble approach for\neffective FS in VFL by considering inter-client interactions. By employing\nconditional feature synthesis alongside multiple learnable feature selectors,\nICAFS facilitates ensemble FS over these selectors using synthetic embeddings.\nThis method bypasses the limitations of private gradient sharing and allows for\nmodel training using real data with refined embeddings. Experiments on multiple\nreal-world datasets demonstrate that ICAFS surpasses current state-of-the-art\nmethods in prediction accuracy.", "published": "2025-04-15 04:19:04", "link": "http://arxiv.org/abs/2504.10851v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "How to Enhance Downstream Adversarial Robustness (almost) without Touching the Pre-Trained Foundation Model?", "abstract": "With the rise of powerful foundation models, a pre-training-fine-tuning\nparadigm becomes increasingly popular these days: A foundation model is\npre-trained using a huge amount of data from various sources, and then the\ndownstream users only need to fine-tune and adapt it to specific downstream\ntasks. However, due to the high computation complexity of adversarial training,\nit is not feasible to fine-tune the foundation model to improve its robustness\non the downstream task. Observing the above challenge, we want to improve the\ndownstream robustness without updating/accessing the weights in the foundation\nmodel. Inspired from existing literature in robustness inheritance (Kim et al.,\n2020), through theoretical investigation, we identify a close relationship\nbetween robust contrastive learning with the adversarial robustness of\nsupervised learning. To further validate and utilize this theoretical insight,\nwe design a simple-yet-effective robust auto-encoder as a data pre-processing\nmethod before feeding the data into the foundation model. The proposed approach\nhas zero access to the foundation model when training the robust auto-encoder.\nExtensive experiments demonstrate the effectiveness of the proposed method in\nimproving the robustness of downstream tasks, verifying the connection between\nthe feature robustness (implied by small adversarial contrastive loss) and the\nrobustness of the downstream task.", "published": "2025-04-15 04:17:37", "link": "http://arxiv.org/abs/2504.10850v1", "categories": ["cs.LG", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Tabular foundation model to detect empathy from visual cues", "abstract": "Detecting empathy from video interactions is an emerging area of research.\nVideo datasets, however, are often released as extracted features (i.e.,\ntabular data) rather than raw footage due to privacy and ethical concerns.\nPrior research on such tabular datasets established tree-based classical\nmachine learning approaches as the best-performing models. Motivated by the\nrecent success of textual foundation models (i.e., large language models), we\nexplore the use of tabular foundation models in empathy detection from tabular\nvisual features. We experiment with two recent tabular foundation models $-$\nTabPFN v2 and TabICL $-$ through in-context learning and fine-tuning setups.\nOur experiments on a public human-robot interaction benchmark demonstrate a\nsignificant boost in cross-subject empathy detection accuracy over several\nstrong baselines (accuracy: $0.590 \\rightarrow 0.730$; AUC: $0.564 \\rightarrow\n0.669$). In addition to performance improvement, we contribute novel insights\nand an evaluation setup to ensure generalisation on unseen subjects in this\npublic benchmark. As the practice of releasing video features as tabular\ndatasets is likely to persist due to privacy constraints, our findings will be\nwidely applicable to future empathy detection video datasets as well.", "published": "2025-04-15 02:06:05", "link": "http://arxiv.org/abs/2504.10808v1", "categories": ["cs.CV", "cs.HC", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Power-scaled Bayesian Inference with Score-based Generative mModels", "abstract": "We propose a score-based generative algorithm for sampling from power-scaled\npriors and likelihoods within the Bayesian inference framework. Our algorithm\nenables flexible control over prior-likelihood influence without requiring\nretraining for different power-scaling configurations. Specifically, we focus\non synthesizing seismic velocity models conditioned on imaged seismic. Our\nmethod enables sensitivity analysis by sampling from intermediate power\nposteriors, allowing us to assess the relative influence of the prior and\nlikelihood on samples of the posterior distribution. Through a comprehensive\nset of experiments, we evaluate the effects of varying the power parameter in\ndifferent settings: applying it solely to the prior, to the likelihood of a\nBayesian formulation, and to both simultaneously. The results show that\nincreasing the power of the likelihood up to a certain threshold improves the\nfidelity of posterior samples to the conditioning data (e.g., seismic images),\nwhile decreasing the prior power promotes greater structural diversity among\nsamples. Moreover, we find that moderate scaling of the likelihood leads to a\nreduced shot data residual, confirming its utility in posterior refinement.", "published": "2025-04-15 02:06:04", "link": "http://arxiv.org/abs/2504.10807v1", "categories": ["cs.LG", "cs.CV", "physics.geo-ph"], "primary_category": "cs.LG"}
{"title": "Wasserstein Distributionally Regret Optimization", "abstract": "Distributionally Robust Optimization (DRO) is a popular framework for\ndecision-making under uncertainty, but its adversarial nature can lead to\noverly conservative solutions. To address this, we study ex-ante\nDistributionally Robust Regret Optimization (DRRO), focusing on\nWasserstein-based ambiguity sets which are popular due to their links to\nregularization and machine learning. We provide a systematic analysis of\nWasserstein DRRO, paralleling known results for Wasserstein DRO. Under\nsmoothness and regularity conditions, we show that Wasserstein DRRO coincides\nwith Empirical Risk Minimization (ERM) up to first-order terms, and exactly so\nin convex quadratic settings. We revisit the Wasserstein DRRO newsvendor\nproblem, where the loss is the maximum of two linear functions of demand and\ndecision. Extending [25], we show that the regret can be computed by maximizing\ntwo one-dimensional concave functions. For more general loss functions\ninvolving the maximum of multiple linear terms in multivariate random variables\nand decision vectors, we prove that computing the regret and thus also the DRRO\npolicy is NP-hard. We then propose a convex relaxation for these more general\nWasserstein DRRO problems and demonstrate its strong empirical performance.\nFinally, we provide an upper bound on the optimality gap of our relaxation and\nshow it improves over recent alternatives.", "published": "2025-04-15 01:47:11", "link": "http://arxiv.org/abs/2504.10796v1", "categories": ["math.OC", "cs.LG"], "primary_category": "math.OC"}
{"title": "SonicSieve: Bringing Directional Speech Extraction to Smartphones Using Acoustic Microstructures", "abstract": "Imagine placing your smartphone on a table in a noisy restaurant and clearly\ncapturing the voices of friends seated around you, or recording a lecturer's\nvoice with clarity in a reverberant auditorium. We introduce SonicSieve, the\nfirst intelligent directional speech extraction system for smartphones using a\nbio-inspired acoustic microstructure. Our passive design embeds directional\ncues onto incoming speech without any additional electronics. It attaches to\nthe in-line mic of low-cost wired earphones which can be attached to\nsmartphones. We present an end-to-end neural network that processes the raw\naudio mixtures in real-time on mobile devices. Our results show that SonicSieve\nachieves a signal quality improvement of 5.0 dB when focusing on a 30{\\deg}\nangular region. Additionally, the performance of our system based on only two\nmicrophones exceeds that of conventional 5-microphone arrays.", "published": "2025-04-15 01:30:48", "link": "http://arxiv.org/abs/2504.10793v1", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AtlasD: Automatic Local Symmetry Discovery", "abstract": "Existing symmetry discovery methods predominantly focus on global\ntransformations across the entire system or space, but they fail to consider\nthe symmetries in local neighborhoods. This may result in the reported symmetry\ngroup being a misrepresentation of the true symmetry. In this paper, we\nformalize the notion of local symmetry as atlas equivariance. Our proposed\npipeline, automatic local symmetry discovery (AtlasD), recovers the local\nsymmetries of a function by training local predictor networks and then learning\na Lie group basis to which the predictors are equivariant. We demonstrate\nAtlasD is capable of discovering local symmetry groups with multiple connected\ncomponents in top-quark tagging and partial differential equation experiments.\nThe discovered local symmetry is shown to be a useful inductive bias that\nimproves the performance of downstream tasks in climate segmentation and vision\ntasks.", "published": "2025-04-15 00:41:55", "link": "http://arxiv.org/abs/2504.10777v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Collaborative Bayesian Optimization via Wasserstein Barycenters", "abstract": "Motivated by the growing need for black-box optimization and data privacy, we\nintroduce a collaborative Bayesian optimization (BO) framework that addresses\nboth of these challenges. In this framework agents work collaboratively to\noptimize a function they only have oracle access to. In order to mitigate\nagainst communication and privacy constraints, agents are not allowed to share\ntheir data but can share their Gaussian process (GP) surrogate models. To\nenable collaboration under these constraints, we construct a central model to\napproximate the objective function by leveraging the concept of Wasserstein\nbarycenters of GPs. This central model integrates the shared models without\naccessing the underlying data. A key aspect of our approach is a collaborative\nacquisition function that balances exploration and exploitation, allowing for\nthe optimization of decision variables collaboratively in each iteration. We\nprove that our proposed algorithm is asymptotically consistent and that its\nimplementation via Monte Carlo methods is numerically accurate. Through\nnumerical experiments, we demonstrate that our approach outperforms other\nbaseline collaborative frameworks and is competitive with centralized\napproaches that do not consider data privacy.", "published": "2025-04-15 00:15:09", "link": "http://arxiv.org/abs/2504.10770v1", "categories": ["cs.LG", "math.OC"], "primary_category": "cs.LG"}
{"title": "A Multi-UAV Formation Obstacle Avoidance Method Combined Improved Simulated Annealing and Adaptive Artificial Potential Field", "abstract": "The traditional Artificial Potential Field (APF) method exhibits limitations\nin its force distribution: excessive attraction when UAVs are far from the\ntarget may cause collisions with obstacles, while insufficient attraction near\nthe goal often results in failure to reach the target. Furthermore, APF is\nhighly susceptible to local minima, compromising motion reliability in complex\nenvironments. To address these challenges, this paper presents a novel hybrid\nobstacle avoidance algorithm-Deflected Simulated Annealing-Adaptive Artificial\nPotential Field (DSA-AAPF)-which combines an improved simulated annealing\nmechanism with an enhanced APF model. The proposed approach integrates a\nLeader-Follower distributed formation strategy with the APF framework, where\nthe resultant force formulation is redefined to smooth UAV trajectories. An\nadaptive gravitational gain function is introduced to dynamically adjust UAV\nvelocity based on environmental context, and a fast-converging controller\nensures accurate and efficient convergence to the target. Moreover, a\ndirectional deflection mechanism is embedded within the simulated annealing\nprocess, enabling UAVs to escape local minima caused by semi-enclosed obstacles\nthrough continuous rotational motion. The simulation results, covering\nformation reconfiguration, complex obstacle avoidance, and entrapment escape,\ndemonstrate the feasibility, robustness, and superiority of the proposed\nDSA-AAPF algorithm.", "published": "2025-04-15 10:53:51", "link": "http://arxiv.org/abs/2504.11064v1", "categories": ["cs.MA", "cs.RO", "cs.SY", "eess.SY"], "primary_category": "cs.MA"}
{"title": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces via Generalized Winding Numbers", "abstract": "Efficient and accurate evaluation of containment queries for regions bound by\ntrimmed NURBS surfaces is important in many graphics and engineering\napplications. However, the algebraic complexity of surface-surface\nintersections makes gaps and overlaps between surfaces difficult to avoid for\nin-the-wild surface models. By considering this problem through the lens of the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape, we can define a\ncontainment query that is robust to model watertightness. Applying contemporary\ntechniques for the 3D GWN on arbitrary curved surfaces would require some form\nof geometric discretization, potentially inducing containment\nmisclassifications near boundary components. In contrast, our proposed method\ncomputes an accurate GWN directly on the curved geometry of the input model. We\naccomplish this using a novel reformulation of the relevant surface integral\nusing Stokes' theorem, which in turn permits an efficient adaptive quadrature\ncalculation on the boundary and trimming curves of the model. While this is\nsufficient for \"far-field\" query points that are distant from the surface, we\naugment this approach for \"near-field\" query points (i.e., within a bounding\nbox) and even those coincident to the surface patches via a strategy that\ndirectly identifies and accounts for the jump discontinuity in the scalar\nfield. We demonstrate that our method of evaluating the GWN field is robust to\ncomplex trimming geometry in a CAD model, and is accurate up to arbitrary\nprecision at arbitrary distances from the surface. Furthermore, the derived\ncontainment query is robust to non-watertightness while respecting all curved\nfeatures of the input shape.", "published": "2025-04-15 17:51:39", "link": "http://arxiv.org/abs/2504.11435v1", "categories": ["cs.GR", "cs.CG", "cs.NA", "math.NA", "68U05", "I.3.5"], "primary_category": "cs.GR"}
{"title": "Optimal and Scalable Augmented Lagrangian preconditioners for Fictitious Domain problems", "abstract": "We present optimal and scalable preconditioning techniques to solve linear\nsystems of equations with a block two-by-two and three-by-three structure\narising from fictitious domain problems and from finite element discretizations\nof immersed boundary methods. In particular, we propose two augmented\nLagrangian-based preconditioners to accelerate the convergence of iterative\nsolvers for these two classes of linear. We consider two relevant examples to\nillustrate the performance of these preconditioners when used in conjunction\nwith flexible GMRES: the Poisson and the Stokes fictitious domain problems. A\nspectral analysis is established for both exact and inexact versions of these\npreconditioners. We show the effectiveness of the proposed approach and the\nrobustness of our preconditioning strategy through extensive numerical tests in\nboth two and three dimensions.", "published": "2025-04-15 16:12:51", "link": "http://arxiv.org/abs/2504.11339v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Implicit dual time-stepping positivity-preserving entropy-stable schemes for the compressible Navier-Stokes equations", "abstract": "We generalize the explicit high-order positivity-preserving entropy-stable\nspectral collocation schemes developed in [30, 34] for the three-dimensional\n(3D) compressible Navier Stokes equations to a time implicit formulation. The\ntime derivative terms are discretized by using the first- and second-order\nimplicit backward difference formulas (BDF1 and BDF2) that are well suited for\nsolving steady-state and time-dependent viscous flows at high Reynolds numbers,\nrespectively. The nonlinear system of discrete equations at each physical\ntimestep is solved by using a dual time-stepping technique. The proposed scheme\nis provably entropy-stable and positivity-preserving and provides unconditional\nstability properties in the physical time. Numerical results demonstrating\naccuracy and positivity-preserving properties of the new dual time-stepping\nscheme are presented for supersonic viscous flows with strong shock waves and\ncontact discontinuities.", "published": "2025-04-15 16:07:05", "link": "http://arxiv.org/abs/2504.11333v1", "categories": ["math.NA", "cs.NA", "65L06, 65M12, 65M70, 76J20, 76M25"], "primary_category": "math.NA"}
{"title": "Optimal finite element approximations of monotone semilinear elliptic PDE with subcritical nonlinearities", "abstract": "We study iterative finite element approximations for the numerical\napproximation of semilinear elliptic boundary value problems with monotone\nnonlinear reactions of subcritical growth. The focus of our contribution is on\nan optimal a priori error estimate for a contractive Picard type iteration\nscheme on meshes that are locally refined towards possible corner singularities\nin polygonal domains. Our analysis involves, in particular, an elliptic\nregularity result in weighted Sobolev spaces and the use of the Trudinger\ninequality, which is instrumental in dealing with subcritically growing\nnonlinearities. A series of numerical experiments confirm the accuracy and\nefficiency of our method.", "published": "2025-04-15 15:32:26", "link": "http://arxiv.org/abs/2504.11292v1", "categories": ["math.NA", "cs.NA", "47J25, 65J15, 65N30"], "primary_category": "math.NA"}
{"title": "Characterizing High Schmidt Number Witnesses in Arbitrary Dimensions System", "abstract": "A profound comprehension of quantum entanglement is crucial for the\nprogression of quantum technologies. The degree of entanglement can be assessed\nby enumerating the entangled degrees of freedom, leading to the determination\nof a parameter known as the Schmidt number. In this paper, we develop an\nefficient analytical tool for characterizing high Schmidt number witnesses for\nbipartite quantum states in arbitrary dimensions. Our methods not only offer\nviable mathematical methods for constructing high-dimensional Schmidt number\nwitnesses in theory but also simplify the quantification of entanglement and\ndimensionality. Most notably, we develop high-dimensional Schmidt number\nwitnesses within arbitrary-dimensional systems, with our Schmidt witness\ncoefficients relying solely on the operator Schmidt coefficient. Subsequently,\nwe demonstrate our theoretical advancements and computational superiority by\nconstructing Schmidt number witnesses in arbitrary dimensional bipartite\nquantum systems with Schmidt numbers four and five.", "published": "2025-04-15 14:15:16", "link": "http://arxiv.org/abs/2504.11213v1", "categories": ["quant-ph", "cs.NA", "math-ph", "math.MP", "math.NA", "math.SP"], "primary_category": "quant-ph"}
{"title": "Low-Rank SPIKE Framework for Solving Large Sparse Linear Systems with Applications", "abstract": "The SPIKE family of linear system solvers provides parallelism using a block\ntridiagonal partitioning. Typically SPIKE-based solvers are applied to banded\nsystems, resulting in structured off-diagonal blocks with non-zeros elements\nrestricted to relatively small submatrices comprising the band of the original\nmatrix. In this work, a low-rank SVD based approximation of the off-diagonal\nblocks is investigated. This produces a representation which more effectively\nhandles matrices with large, sparse bands. A set of flexible distributed\nsolvers, the LR-SPIKE variants, are implemented. There are applicable to a wide\nrange of applications -- from use as a \"black-box\" preconditioner which\nstraightforwardly improves upon the classic Block Jacobi preconditioner, to use\nas a specialized \"approximate direct solver.\" An investigation of the\neffectiveness of the new preconditioners for a selection of SuiteSparse\nmatrices is performed, particularly focusing on matrices derived from 3D finite\nelement simulations. In addition, the SPIKE approximate linear system solvers\nare also paired with the FEAST eigenvalue solver, where they are shown to be\nparticularly effective due to the former's rapid convergence, and the latter's\nacceptance of loose linear system solver convergence, resulting in a\ncombination which requires very few solver iterations.", "published": "2025-04-15 13:15:00", "link": "http://arxiv.org/abs/2504.11167v1", "categories": ["math.NA", "cs.MS", "cs.NA"], "primary_category": "math.NA"}
{"title": "An Unsupervised Network Architecture Search Method for Solving Partial Differential Equations", "abstract": "Solving partial differential equations (PDEs) has been indispensable in\nscientific and engineering applications. Recently, deep learning methods have\nbeen widely used to solve high-dimensional problems, one of which is the\nphysics-informed neural network (PINN). Typically, a deep learning method has\nthree main components: a neural network, a loss function, and an optimizer.\nWhile the construction of the loss function is rooted in the definition of\nsolution space, how to choose a optimal neural network is somewhat ad hoc,\nleaving much room for improvement. In the framework of PINN, we propose an\nunsupervised network architecture search method for solving PDEs, termed\nPINN-DARTS, which applies the differentiable architecture search (DARTS) to\nfind the optimal network architecture structure in a given set of neural\nnetworks. In this set, the number of layers and the number of neurons in each\nlayer can change. In the searching phase, both network and architecture\nparameters are updated simultaneously, so the running time is close to that of\nPINN with a pre-determined network structure. Unlike available works, our\napproach is unsupervised and purely based on the PDE residual without any prior\nusage of solutions. PINN-DARTS outputs the optimal network structure as well as\nthe associated numerical solution. The performance of PINN-DARTS is verified on\nseveral benchmark PDEs, including elliptic, parabolic, wave, and Burgers'\nequations. Compared to traditional architecture search methods, PINN-DARTS\nachieves significantly higher architectural accuracy. Another interesting\nobservation is that both the solution complexity and the PDE type have a\nprominent impact on the optimal network architecture. Our study suggests that\narchitectures with uneven widths from layer to layer may have superior\nperformance across different solution complexities and different PDE types.", "published": "2025-04-15 12:42:32", "link": "http://arxiv.org/abs/2504.11140v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A fully variational numerical method for structural topology optimization based on a Cahn-Hilliard model", "abstract": "We formulate a novel numerical method suitable for the solution of topology\noptimization problems in solid mechanics. The most salient feature of the new\napproach is that the space and time discrete equations of the numerical method\ncan be obtained as the optimality conditions of a single incremental potential.\nThe governing equations define a gradient flow of the mass in the domain that\nmaximizes the stiffness of the proposed solid, while exactly preserving the\nmass of the allocated material. Moreover, we propose a change of variables in\nthe model equations that constrains the value of the density within admissible\nbounds and a continuation strategy that speeds up the evolution of the flow.\nThe proposed strategy results in a robust and efficient topology optimization\nmethod that is exactly mass-preserving, does not employ Lagrange multipliers,\nand is fully variational.", "published": "2025-04-15 11:42:54", "link": "http://arxiv.org/abs/2504.11096v1", "categories": ["math.NA", "cs.NA", "math-ph", "math.MP"], "primary_category": "math.NA"}
{"title": "A study of troubled-cell indicators applied to finite volume methods using a novel monotonicity parameter", "abstract": "We adapt a troubled-cell indicator developed for discontinuous Galerkin (DG)\nmethods to the finite volume method (FVM) framework for solving hyperbolic\nconservation laws. This indicator depends solely on the cell-average data of\nthe target cell and its immediate neighbours. Once the troubled-cells are\nidentified, we apply the limiter only in these cells instead of applying in all\ncomputational cells. We introduce a novel technique to quantify the quality of\nthe solution in the neighbourhood of the shock by defining a monotonicity\nparameter $\\mu$. Numerical results from various two-dimensional simulations on\nthe hyperbolic systems of Euler equations using a finite volume solver\nemploying MUSCL reconstruction validate the performance of the troubled-cell\nindicator and the approach of limiting only in the troubled-cells. These\nresults show that limiting only in the troubled-cells is preferable to limiting\neverywhere as it improves convergence without compromising on the solution\naccuracy.", "published": "2025-04-15 10:44:16", "link": "http://arxiv.org/abs/2504.11056v1", "categories": ["math.NA", "cs.CE", "cs.NA"], "primary_category": "math.NA"}
{"title": "A broken Hardy inequality on finite element space and application to strain gradient elasticity", "abstract": "We illustrate a broken Hardy inequality on discontinuous finite element\nspaces, blowing up with a logarithmic factor with respect to the meshes size.\nThis is motivated by numerical analysis for the strain gradient elasticity with\nnatural boundary conditions. A mixed finite element pair is employed to solve\nthis model with nearly incompressible materials. This pair is quasi-stable with\na logarithmic factor, which is not significant in the approximation error, and\nconverges robustly in the incompressible limit and uniformly in the microscopic\nmaterial parameter. Numerical results back up that the theoretical predictions\nare nearly optimal. Moreover, the regularity estimates for the model over a\nsmooth domain have been proved with the aid of the Agmon-Douglis-Nirenberg\ntheory.", "published": "2025-04-15 09:08:57", "link": "http://arxiv.org/abs/2504.10993v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Convergence rate for a semidiscrete approximation of scalar conservation laws", "abstract": "We propose a semidiscrete scheme for approximation of entropy solutions of\none-dimensional scalar conservation laws with nonnegative initial data. The\nscheme is based on the concept of particle paths for conservation laws and can\nbe interpreted as a finite-particle discretization. A convergence rate of order\n$1/2$ with respect to initial particle spacing is proved. As a special case,\nthis covers the convergence of the Follow--the--Leader model to the\nLighthill--Whitham--Richards model for traffic flow.", "published": "2025-04-15 07:58:28", "link": "http://arxiv.org/abs/2504.10951v1", "categories": ["math.AP", "cs.NA", "math.NA", "65M12, 35L65, 90B20"], "primary_category": "math.AP"}
{"title": "Breaking the Dimensional Barrier: A Pontryagin-Guided Direct Policy Optimization for Continuous-Time Multi-Asset Portfolio", "abstract": "Solving large-scale, continuous-time portfolio optimization problems\ninvolving numerous assets and state-dependent dynamics has long been challenged\nby the curse of dimensionality. Traditional dynamic programming and PDE-based\nmethods, while rigorous, typically become computationally intractable beyond a\nsmall number of state variables (often limited to ~3-6 in prior numerical\nstudies). To overcome this critical barrier, we introduce the\n\\emph{Pontryagin-Guided Direct Policy Optimization} (PG-DPO) framework. PG-DPO\nleverages Pontryagin's Maximum Principle to directly guide neural network\npolicies via backpropagation-through-time, naturally incorporating exogenous\nstate processes without requiring dense state grids. Crucially, our\ncomputationally efficient ``Two-Stage'' variant exploits rapidly stabilizing\ncostate estimates derived from BPTT, converting them into near-optimal\nclosed-form Pontryagin controls after only a short warm-up, significantly\nreducing training overhead. This enables a breakthrough in scalability:\nnumerical experiments demonstrate that PG-DPO successfully tackles problems\nwith dimensions previously considered far out of reach, optimizing portfolios\nwith up to 50 assets and 10 state variables. The framework delivers\nnear-optimal policies, offering a practical and powerful alternative for\nhigh-dimensional continuous-time portfolio choice.", "published": "2025-04-15 12:03:14", "link": "http://arxiv.org/abs/2504.11116v1", "categories": ["q-fin.PM", "q-fin.CP"], "primary_category": "q-fin.PM"}
{"title": "Can Large Language Models Trade? Testing Financial Theories with LLM Agents in Market Simulations", "abstract": "This paper presents a realistic simulated stock market where large language\nmodels (LLMs) act as heterogeneous competing trading agents. The open-source\nframework incorporates a persistent order book with market and limit orders,\npartial fills, dividends, and equilibrium clearing alongside agents with varied\nstrategies, information sets, and endowments. Agents submit standardized\ndecisions using structured outputs and function calls while expressing their\nreasoning in natural language. Three findings emerge: First, LLMs demonstrate\nconsistent strategy adherence and can function as value investors, momentum\ntraders, or market makers per their instructions. Second, market dynamics\nexhibit features of real financial markets, including price discovery, bubbles,\nunderreaction, and strategic liquidity provision. Third, the framework enables\nanalysis of LLMs' responses to varying market conditions, similar to partial\ndependence plots in machine-learning interpretability. The framework allows\nsimulating financial theories without closed-form solutions, creating\nexperimental designs that would be costly with human participants, and\nestablishing how prompts can generate correlated behaviors affecting market\nstability.", "published": "2025-04-15 01:18:36", "link": "http://arxiv.org/abs/2504.10789v1", "categories": ["q-fin.CP", "econ.GN", "q-fin.EC", "q-fin.GN", "q-fin.TR"], "primary_category": "q-fin.CP"}
{"title": "Simulation-based inference for stochastic nonlinear mixed-effects models with applications in systems biology", "abstract": "The analysis of data from multiple experiments, such as observations of\nseveral individuals, is commonly approached using mixed-effects models, which\naccount for variation between individuals through hierarchical representations.\nThis makes mixed-effects models widely applied in fields such as biology,\npharmacokinetics, and sociology. In this work, we propose a novel methodology\nfor scalable Bayesian inference in hierarchical mixed-effects models. Our\nframework first constructs amortized approximations of the likelihood and the\nposterior distribution, which are then rapidly refined for each individual\ndataset, to ultimately approximate the parameters posterior across many\nindividuals. The framework is easily trainable, as it uses mixtures of experts\nbut without neural networks, leading to parsimonious yet expressive surrogate\nmodels of the likelihood and the posterior. We demonstrate the effectiveness of\nour methodology using challenging stochastic models, such as mixed-effects\nstochastic differential equations emerging in systems biology-driven problems.\nHowever, the approach is broadly applicable and can accommodate both stochastic\nand deterministic models. We show that our approach can seamlessly handle\ninference for many parameters. Additionally, we applied our method to a\nreal-data case study of mRNA transfection. When compared to exact\npseudomarginal Bayesian inference, our approach proved to be both fast and\ncompetitive in terms of statistical accuracy.", "published": "2025-04-15 15:18:58", "link": "http://arxiv.org/abs/2504.11279v1", "categories": ["stat.CO", "stat.ME", "stat.ML"], "primary_category": "stat.CO"}
{"title": "Hessian stability and convergence rates for entropic and Sinkhorn potentials via semiconcavity", "abstract": "In this paper we determine quantitative stability bounds for the Hessian of\nentropic potentials, i.e., the dual solution to the entropic optimal transport\nproblem. Up to authors' knowledge this is the first work addressing this\nsecond-order quantitative stability estimate in general unbounded settings. Our\nproof strategy relies on semiconcavity properties of entropic potentials and on\nthe representation of entropic transport plans as laws of forward and backward\ndiffusion processes, known as Schr\\\"odinger bridges. Moreover, our approach\nallows to deduce a stochastic proof of quantitative stability entropic\nestimates and integrated gradient estimates as well. Finally, as a direct\nconsequence of these stability bounds, we deduce exponential convergence rates\nfor gradient and Hessian of Sinkhorn iterates along Sinkhorn's algorithm, a\nproblem that was still open in unbounded settings. Our rates have a polynomial\ndependence on the regularization parameter.", "published": "2025-04-15 12:34:09", "link": "http://arxiv.org/abs/2504.11133v1", "categories": ["math.PR", "math.AP", "math.OC", "stat.ML", "49Q22, 49L12, 39B62, 60J60, 68Q87, 68W40"], "primary_category": "math.PR"}
{"title": "On relative universality, regression operator, and conditional independence", "abstract": "The notion of relative universality with respect to a {\\sigma}-field was\nintroduced to establish the unbiasedness and Fisher consistency of an estimator\nin nonlinear sufficient dimension reduction. However, there is a gap in the\nproof of this result in the existing literature. The existing definition of\nrelative universality seems to be too strong for the proof to be valid. In this\nnote we modify the definition of relative universality using the concept of\n\\k{o}-measurability, and rigorously establish the mentioned unbiasedness and\nFisher consistency. The significance of this result is beyond its original\ncontext of sufficient dimension reduction, because relative universality allows\nus to use the regression operator to fully characterize conditional\nindependence, a crucially important statistical relation that sits at the core\nof many areas and methodologies in statistics and machine learning, such as\ndimension reduction, graphical models, probability embedding, causal inference,\nand Bayesian estimation.", "published": "2025-04-15 10:12:26", "link": "http://arxiv.org/abs/2504.11044v1", "categories": ["math.ST", "stat.ME", "stat.ML", "stat.TH", "62", "G.3"], "primary_category": "math.ST"}
{"title": "Dopamine Audiobook: A Training-free MLLM Agent for Emotional and Human-like Audiobook Generation", "abstract": "Audiobook generation, which creates vivid and emotion-rich audio works, faces\nchallenges in conveying complex emotions, achieving human-like qualities, and\naligning evaluations with human preferences. Existing text-to-speech (TTS)\nmethods are often limited to specific scenarios, struggle with emotional\ntransitions, and lack automatic human-aligned evaluation benchmarks, instead\nrelying on either misaligned automated metrics or costly human assessments. To\naddress these issues, we propose Dopamine Audiobook, a new unified\ntraining-free system leveraging a multimodal large language model (MLLM) as an\nAI agent for emotional and human-like audiobook generation and evaluation.\nSpecifically, we first design a flow-based emotion-enhanced framework that\ndecomposes complex emotional speech synthesis into controllable sub-tasks.\nThen, we propose an adaptive model selection module that dynamically selects\nthe most suitable TTS methods from a set of existing state-of-the-art (SOTA)\nTTS methods for diverse scenarios. We further enhance emotional expressiveness\nthrough paralinguistic augmentation and prosody retrieval at word and utterance\nlevels. For evaluation, we propose a novel GPT-based evaluation framework\nincorporating self-critique, perspective-taking, and psychological MagicEmo\nprompts to ensure human-aligned and self-aligned assessments. Experiments show\nthat our method generates long speech with superior emotional expression to\nSOTA TTS models in various metrics. Importantly, our evaluation framework\ndemonstrates better alignment with human preferences and transferability across\naudio tasks. Project website with audio samples can be found at\nhttps://dopamine-audiobook.github.io.", "published": "2025-04-15 09:19:44", "link": "http://arxiv.org/abs/2504.11002v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Real-Time Word-Level Temporal Segmentation in Streaming Speech Recognition", "abstract": "Rich-text captions are essential to help communication for Deaf and\nhard-of-hearing (DHH) people, second-language learners, and those with autism\nspectrum disorder (ASD). They also preserve nuances when converting speech to\ntext, enhancing the realism of presentation scripts and conversation or speech\nlogs. However, current real-time captioning systems lack the capability to\nalter text attributes (ex. capitalization, sizes, and fonts) at the word level,\nhindering the accurate conveyance of speaker intent that is expressed in the\ntones or intonations of the speech. For example, ''YOU should do this'' tends\nto be considered as indicating ''You'' as the focus of the sentence, whereas\n''You should do THIS'' tends to be ''This'' as the focus. This paper proposes a\nsolution that changes the text decorations at the word level in real time. As a\nprototype, we developed an application that adjusts word size based on the\nloudness of each spoken word. Feedback from users implies that this system\nhelped to convey the speaker's intent, offering a more engaging and accessible\ncaptioning experience.", "published": "2025-04-15 04:17:08", "link": "http://arxiv.org/abs/2504.10849v1", "categories": ["cs.HC", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "SteerMusic: Enhanced Musical Consistency for Zero-shot Text-Guided and Personalized Music Editing", "abstract": "Music editing is an important step in music production, which has broad\napplications, including game development and film production. Most existing\nzero-shot text-guided methods rely on pretrained diffusion models by involving\nforward-backward diffusion processes for editing. However, these methods often\nstruggle to maintain the music content consistency. Additionally, text\ninstructions alone usually fail to accurately describe the desired music. In\nthis paper, we propose two music editing methods that enhance the consistency\nbetween the original and edited music by leveraging score distillation. The\nfirst method, SteerMusic, is a coarse-grained zero-shot editing approach using\ndelta denoising score. The second method, SteerMusic+, enables fine-grained\npersonalized music editing by manipulating a concept token that represents a\nuser-defined musical style. SteerMusic+ allows for the editing of music into\nany user-defined musical styles that cannot be achieved by the text\ninstructions alone. Experimental results show that our methods outperform\nexisting approaches in preserving both music content consistency and editing\nfidelity. User studies further validate that our methods achieve superior music\nediting quality. Audio examples are available on https://steermusic.pages.dev/.", "published": "2025-04-15 03:08:09", "link": "http://arxiv.org/abs/2504.10826v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generalized Audio Deepfake Detection Using Frame-level Latent Information Entropy", "abstract": "Generalizability, the capacity of a robust model to perform effectively on\nunseen data, is crucial for audio deepfake detection due to the rapid evolution\nof text-to-speech (TTS) and voice conversion (VC) technologies. A promising\napproach to differentiate between bonafide and spoof samples lies in\nidentifying intrinsic disparities to enhance model generalizability. From an\ninformation-theoretic perspective, we hypothesize the information content is\none of the intrinsic differences: bonafide sample represents a dense,\ninformation-rich sampling of the real world, whereas spoof sample is typically\nderived from lower-dimensional, less informative representations. To implement\nthis, we introduce frame-level latent information entropy detector(f-InfoED), a\nframework that extracts distinctive information entropy from latent\nrepresentations at the frame level to identify audio deepfakes. Furthermore, we\npresent AdaLAM, which extends large pre-trained audio models with trainable\nadapters for enhanced feature extraction. To facilitate comprehensive\nevaluation, the audio deepfake forensics 2024 (ADFF 2024) dataset was built by\nthe latest TTS and VC methods. Extensive experiments demonstrate that our\nproposed approach achieves state-of-the-art performance and exhibits remarkable\ngeneralization capabilities. Further analytical studies confirms the efficacy\nof AdaLAM in extracting discriminative audio features and f-InfoED in\nleveraging latent entropy information for more generalized deepfake detection.", "published": "2025-04-15 02:39:46", "link": "http://arxiv.org/abs/2504.10819v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Audio Watermarks are Shallow: Limitations of Post-Hoc Watermarking Techniques for Speech", "abstract": "In the audio modality, state-of-the-art watermarking methods leverage deep\nneural networks to allow the embedding of human-imperceptible signatures in\ngenerated audio. The ideal is to embed signatures that can be detected with\nhigh accuracy when the watermarked audio is altered via compression, filtering,\nor other transformations. Existing audio watermarking techniques operate in a\npost-hoc manner, manipulating \"low-level\" features of audio recordings after\ngeneration (e.g. through the addition of a low-magnitude watermark signal). We\nshow that this post-hoc formulation makes existing audio watermarks vulnerable\nto transformation-based removal attacks. Focusing on speech audio, we (1) unify\nand extend existing evaluations of the effect of audio transformations on\nwatermark detectability, and (2) demonstrate that state-of-the-art post-hoc\naudio watermarks can be removed with no knowledge of the watermarking scheme\nand minimal degradation in audio quality.", "published": "2025-04-15 00:52:01", "link": "http://arxiv.org/abs/2504.10782v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Orbiter Continuous Lunar Beaming", "abstract": "In this work, free-space optics-based continuous wireless power transmission\nbetween multiple low lunar orbit satellites and a solar panel on the lunar\nrover located at the lunar south pole are investigated based on the\ntime-varying harvested power and overall system efficiency metrics. The\nperformances are compared between a solar panel with the tracking ability and a\nfixed solar panel that induces \\textit{the cosine effect} due to the\ntime-dependent angle of incidence (AoI). In our work, the Systems Tool Kit\n(STK) high-precision orbit propagator, which calculates the ephemeris data\nprecisely, is utilized. Interestingly, orbiter deployments in constellations\nchange significantly during a Moon revolution; thus, short-duration simulations\ncannot be used straightforwardly. In our work, many satellite configurations\nare assessed to be able to find a Cislunar constellation that establishes a\ncontinuous line-of-sight (LoS) between the solar panel and at least a single\nLLO satellite. It is found that 40-satellite schemes enable the establishment\nof a continuous WPT system model. Besides, a satellite selection method (SSM)\nis introduced so that only the best LoS link among multiple simultaneous links\nfrom multiple satellites will be active for optimum efficiency. Our benchmark\nsystem of a 40-satellite quadruple orbit scheme is compared with 30-satellite\nand a single satellite schemes based on the average harvested powers and\noverall system efficiencies 27.3 days so the trade-off options can be assessed\nfrom the multiple Cislunar models. The outcomes show that the average system\nefficiencies of single, 30-satellite, and 40-satellite schemes are 2.84%,\n32.33%, and 33.29%, respectively, for the tracking panel and 0.97%, 18.33%, and\n20.44%, respectively, for the fixed solar panel case.", "published": "2025-04-15 15:43:39", "link": "http://arxiv.org/abs/2504.11300v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Physics-Aware Initialization Refinement in Code-Aided EM for Blind Channel Estimation", "abstract": "This paper addresses the well-known local maximum problem of the\nexpectation-maximization (EM) algorithm in blind intersymbol interference (ISI)\nchannel estimation. This problem primarily results from phase and shift\nambiguity during initialization, which blind estimation is inherently unable to\ndistinguish. We propose an effective initialization refinement algorithm that\nutilizes the decoder output as a model selection metric, incorporating a\ntechnique to detect phase and shift ambiguity. Our results show that the\nproposed algorithm significantly reduces the number of local maximum cases to\nnearly one-third for a 3-tap ISI channel under highly uncertain initial\nconditions. The improvement becomes more pronounced as initial errors increase\nand the channel memory grows. When used in a turbo equalizer, the proposed\nalgorithm is required only in the first turbo iteration, which limits any\ncomplexity increase with subsequent iterations.", "published": "2025-04-15 14:42:47", "link": "http://arxiv.org/abs/2504.11241v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Guided Wave-Based Structural Awareness Under Varying Operating States via Manifold Representations", "abstract": "Guided wave-based structural health monitoring (SHM) remains a powerful\nstrategy for identifying early-stage defects and safeguarding vital aerospace\nstructures. Yet, its practical use is often hindered by the enormous,\nhigh-dimensional data streams produced by sensor arrays operating at megahertz\nsampling rates, coupled with the added complexity of shifts in environmental\nand operational conditions (EOCs). Studies have explored various\ndata-compression approaches that retain critical diagnostic details in a\nlower-dimensional latent space. While conventional techniques can streamline\ndimensionality to some extent, they do not always capture the nonlinear\ninteractions typical of guided waves. Manifold learning, as illustrated by\nDiffusion Maps, tackles these nonlinearities by deriving low-dimensional\nembeddings directly from wave signals, minimizing the need for manual feature\nextraction. In parallel, developments in deep learning -- particularly\nautoencoders -- provide an encoder-decoder model for both data compression and\nreconstruction. Convolutional autoencoders (CAEs) and variational autoencoders\n(VAEs) have been particularly effective for guided wave applications. However,\ncurrent methods can still struggle to maintain accurate state estimation under\nchanging EOCs, and they are often limited to a single task. In response, the\nproposed framework adopts a two-fold strategy: it compresses high-dimensional\nsignals into lower-dimensional representations and then leverages those\nrepresentations to both estimate structural states and reconstruct the original\ndata, even as conditions vary. Applied to two real-world SHM use-cases, this\nintegrated method has proven its ability to preserve and retrieve key damage\nsignatures under noise, shifting operational parameters, and other complicating\nfactors.", "published": "2025-04-15 14:38:30", "link": "http://arxiv.org/abs/2504.11235v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Fully Asynchronous Unsourced Random Access Scheme", "abstract": "We investigate fully asynchronous unsourced random access (URA), and propose\na high-performing scheme that employs on-off division multiple access (ODMA).\nIn this scheme, active users distribute their data over the transmit block\nbased on a sparse transmission pattern without any limitations on the starting\ntime. At the receiver side, we adopt a double sliding-window decoding approach,\nutilizing a smaller inner decoding window of two block lengths within a larger\nouter window to enhance the interference cancellation process. Within the inner\nwindow, the receiver iteratively applies preamble-free joint starting time and\npattern detection, single-user decoding, and successive interference\ncancellation operations. A notable feature of the proposed scheme is its\nelimination of the need for a preamble for starting time detection; this is\nachieved using ODMA transmission patterns. Numerical results demonstrate that\nthe proposed asynchronous URA scheme outperforms existing alternatives.", "published": "2025-04-15 12:31:24", "link": "http://arxiv.org/abs/2504.11131v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Unified Hardware Accelerator for Fast Fourier Transform and Number Theoretic Transform", "abstract": "The Number Theoretic Transform (NTT) is an indispensable tool for computing\nefficient polynomial multiplications in post-quantum lattice-based\ncryptography. It has strong resemblance with the Fast Fourier Transform (FFT),\nwhich is the most widely used algorithm in digital signal processing. In this\nwork, we demonstrate a unified hardware accelerator supporting both 512-point\ncomplex FFT as well as 256-point NTT for the recently standardized NIST\npost-quantum key encapsulation and digital signature algorithms ML-KEM and\nML-DSA respectively. Our proposed architecture effectively utilizes the\narithmetic circuitry required for complex FFT, and the only additional circuits\nrequired are for modular reduction along with modifications in the control\nlogic. Our implementation achieves performance comparable to state-of-the-art\nML-KEM / ML-DSA NTT accelerators on FPGA, thus demonstrating how an FFT\naccelerator can be augmented to support NTT and the unified hardware can be\nused for both digital signal processing and post-quantum lattice-based\ncryptography applications.", "published": "2025-04-15 12:13:05", "link": "http://arxiv.org/abs/2504.11124v1", "categories": ["cs.CR", "eess.SP"], "primary_category": "cs.CR"}
{"title": "Continuous Aperture Array (CAPA)-Based Secure Wireless Communications", "abstract": "A continuous aperture array (CAPA)-based secure communication system is\ninvestigated, where a base station equipped with a CAPA transmits signals to a\nlegitimate user under the existence of an eavesdropper. For improving the\nsecrecy performance, the artificial noise (AN) is employed at the BS for the\njamming purpose. We aim at maximizing the secrecy rate by jointly optimizing\nthe information-bearing and AN source current patterns, subject to the maximum\ntransmit power constraint. To solve the resultant non-convex integral-based\nfunctional programming problem, a channel subspace-based approach is first\nproposed via exploiting the result that the optimal current patterns always lie\nwithin the subspace spanned by all users' channel responses. Then, the\nintractable CAPA continuous source current pattern design problem with an\ninfinite number of optimization variables is equivalently transformed into the\nchannel-subspace weighting factor optimization problem with a finite number of\noptimization variables. A penalty-based successive convex approximation method\nis developed for iteratively optimizing the finite-size weighting vectors. To\nfurther reduce the computational complexity, we propose a two-stage source\ncurrent patterns design scheme. Specifically, the information-bearing and AN\npatterns are first designed using the maximal ration transmission and\nzero-forcing transmission, respectively. Then, the remaining power allocation\nis addressed via the one-dimensional search method. Numerical results unveil\nthat 1) the CAPA brings in significant secrecy rate gain compared to the\nconventional discrete multiple-input multiple-output; 2) the proposed channel\nsubspace-based algorithm outperforms the conventional Fourier-based approach,\nwhile sustaining much lower computational complexity; and 3) the two-stage\nZF-MRT approach has negligible performance loss for the large transmit power\nregime.", "published": "2025-04-15 12:02:03", "link": "http://arxiv.org/abs/2504.11114v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Signal Matrix-Based Local Flaw Detection Framework for Steel Wire Ropes Using Convolutional Neural Networks", "abstract": "Steel wire ropes (SWRs) are critical load-bearing components in industrial\napplications, yet their structural integrity is often compromised by local\nflaws (LFs). Magnetic Flux Leakage (MFL) is a widely used non-destructive\ntesting method that detects defects by measuring perturbations in magnetic\nfields. Traditional MFL detection methods suffer from critical limitations:\none-dimensional approaches fail to capture spatial relationships across sensor\nchannels, while multi-dimensional image-based techniques introduce\ninterpolation artifacts and computational inefficiencies. This paper proposes a\nnovel detection framework based on signal matrices, directly processing raw\nmulti-channel MFL signals using a specialized Convolutional Neural Network for\nsignal matrix as input (SM-CNN). The architecture incorporates stripe pooling\nto preserve channel-wise features and symmetric padding to improve boundary\ndefect detection. Our model achieves state-of-the-art performance with 98.74%\naccuracy and 97.85% recall. Additionally, it demonstrates exceptional\ncomputational efficiency, processing at 87.72 frames per second (FPS) with a\nlow inference latency of 2.6ms and preprocessing time of 8.8ms. With only 1.48\nmillion parameters, this lightweight design supports real-time processing,\nestablishing a new benchmark for SWR inspection in industrial settings.", "published": "2025-04-15 07:59:04", "link": "http://arxiv.org/abs/2504.10952v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "ACSNet: A Deep Neural Network for Compound GNSS Jamming Signal Classification", "abstract": "In the global navigation satellite system (GNSS), identifying not only single\nbut also compound jamming signals is crucial for ensuring reliable navigation\nand positioning, particularly in future wireless communication scenarios such\nas the space-air-ground integrated network (SAGIN). However, conventional\ntechniques often struggle with low recognition accuracy and high computational\ncomplexity, especially under low jamming-to-noise ratio (JNR) conditions. To\novercome the challenge of accurately identifying compound jamming signals\nembedded within GNSS signals, we propose ACSNet, a novel convolutional neural\nnetwork designed specifically for this purpose. Unlike traditional methods that\ntend to exhibit lower accuracy and higher computational demands, particularly\nin low JNR environments, ACSNet addresses these issues by integrating\nasymmetric convolution blocks, which enhance its sensitivity to subtle signal\nvariations. Simulations demonstrate that ACSNet significantly improves accuracy\nin low JNR regions and shows robust resilience to power ratio (PR) variations,\nconfirming its effectiveness and efficiency for practical GNSS interference\nmanagement applications.", "published": "2025-04-15 02:05:30", "link": "http://arxiv.org/abs/2504.10806v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Generative and Explainable AI for High-Dimensional Channel Estimation", "abstract": "In this paper, we propose a new adversarial training framework to address\nhigh-dimensional instantaneous channel estimation in wireless communications.\nSpecifically, we train a generative adversarial network to predict a channel\nrealization in the time-frequency-space domain, in which the generator exploits\nthe third-order moment of the input in its loss function and applies a new\nreparameterization method for latent distribution learning to minimize the\nWasserstein distance between the true and estimated channel distributions.\nNext, we propose an explainable artificial intelligence mechanism to examine\nhow the critic discriminates the generated channel. We demonstrate that our\nproposed framework is superior to existing methods in terms of minimizing\nestimation errors. Additionally, we find that the critic's attention focuses on\nthe high-power portion of the channel's time-frequency representation.", "published": "2025-04-15 00:29:40", "link": "http://arxiv.org/abs/2504.10775v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Automated Python Translation", "abstract": "Python is one of the most commonly used programming languages in industry and\neducation. Its English keywords and built-in functions/modules allow it to come\nclose to pseudo-code in terms of its readability and ease of writing. However,\nthose who do not speak English may not experience these advantages. In fact,\nthey may even be hindered in their ability to understand Python code, as the\nEnglish nature of its terms creates an additional layer of overhead. To that\nend, we introduce the task of automatically translating Python's natural\nmodality (keywords, error types, identifiers, etc.) into other human languages.\nThis presents a unique challenge, considering the abbreviated nature of these\nforms, as well as potential untranslatability of advanced\nmathematical/programming concepts across languages. We therefore create an\nautomated pipeline to translate Python into other human languages, comparing\nstrategies using machine translation and large language models. We then use\nthis pipeline to acquire translations from five common Python libraries\n(pytorch, pandas, tensorflow, numpy, and random) in seven languages, and do a\nquality test on a subset of these terms in French, Greek, and Bengali. We hope\nthis will provide a clearer path forward towards creating a universal Python,\naccessible to anyone regardless of nationality or language background.", "published": "2025-04-15 15:30:22", "link": "http://arxiv.org/abs/2504.11290v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis", "abstract": "Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation. In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://colmon46.github.io/i2e-bench-leaderboard/ .", "published": "2025-04-15 14:56:21", "link": "http://arxiv.org/abs/2504.11257v2", "categories": ["cs.HC", "cs.CL", "cs.CV"], "primary_category": "cs.HC"}
{"title": "Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs", "abstract": "Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages.", "published": "2025-04-15 08:46:39", "link": "http://arxiv.org/abs/2504.10982v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Elucidating the Design Space of Multimodal Protein Language Models", "abstract": "Multimodal protein language models (PLMs) integrate sequence and token-based\nstructural information, serving as a powerful foundation for protein modeling,\ngeneration, and design. However, the reliance on tokenizing 3D structures into\ndiscrete tokens causes substantial loss of fidelity about fine-grained\nstructural details and correlations. In this paper, we systematically elucidate\nthe design space of multimodal PLMs to overcome their limitations. We identify\ntokenization loss and inaccurate structure token predictions by the PLMs as\nmajor bottlenecks. To address these, our proposed design space covers improved\ngenerative modeling, structure-aware architectures and representation learning,\nand data exploration. Our advancements approach finer-grained supervision,\ndemonstrating that token-based multimodal PLMs can achieve robust structural\nmodeling. The effective design methods dramatically improve the structure\ngeneration diversity, and notably, folding abilities of our 650M model by\nreducing the RMSD from 5.52 to 2.36 on PDB testset, even outperforming 3B\nbaselines and on par with the specialized folding models.", "published": "2025-04-15 17:59:43", "link": "http://arxiv.org/abs/2504.11454v2", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "primary_category": "cs.LG"}
{"title": "Efficient Distributed Retrieval-Augmented Generation for Enhancing Language Model Performance", "abstract": "Small language models (SLMs) support efficient deployments on\nresource-constrained edge devices, but their limited capacity compromises\ninference performance. Retrieval-augmented generation (RAG) is a promising\nsolution to enhance model performance by integrating external databases,\nwithout requiring intensive on-device model retraining. However, large-scale\npublic databases and user-specific private contextual documents are typically\nlocated on the cloud and the device separately, while existing RAG\nimplementations are primarily centralized. To bridge this gap, we propose\nDRAGON, a distributed RAG framework to enhance on-device SLMs through both\ngeneral and personal knowledge without the risk of leaking document privacy.\nSpecifically, DRAGON decomposes multi-document RAG into multiple parallel token\ngeneration processes performed independently and locally on the cloud and the\ndevice, and employs a newly designed Speculative Aggregation, a dual-side\nspeculative algorithm to avoid frequent output synchronization between the\ncloud and device. A new scheduling algorithm is further introduced to identify\nthe optimal aggregation side based on real-time network conditions. Evaluations\non real-world hardware testbed demonstrate a significant performance\nimprovement of DRAGON-up to 1.9x greater gains over standalone SLM compared to\nthe centralized RAG, substantial reduction in per-token latency, and negligible\nTime to First Token (TTFT) overhead.", "published": "2025-04-15 13:53:08", "link": "http://arxiv.org/abs/2504.11197v2", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails", "abstract": "Large Language Models (LLMs) guardrail systems are designed to protect\nagainst prompt injection and jailbreak attacks. However, they remain vulnerable\nto evasion techniques. We demonstrate two approaches for bypassing LLM prompt\ninjection and jailbreak detection systems via traditional character injection\nmethods and algorithmic Adversarial Machine Learning (AML) evasion techniques.\nThrough testing against six prominent protection systems, including Microsoft's\nAzure Prompt Shield and Meta's Prompt Guard, we show that both methods can be\nused to evade detection while maintaining adversarial utility achieving in some\ninstances up to 100% evasion success. Furthermore, we demonstrate that\nadversaries can enhance Attack Success Rates (ASR) against black-box targets by\nleveraging word importance ranking computed by offline white-box models. Our\nfindings reveal vulnerabilities within current LLM protection mechanisms and\nhighlight the need for more robust guardrail systems.", "published": "2025-04-15 13:16:02", "link": "http://arxiv.org/abs/2504.11168v2", "categories": ["cs.CR", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CR"}
{"title": "Dynamical errors in machine learning forecasts", "abstract": "In machine learning forecasting, standard error metrics such as mean absolute\nerror (MAE) and mean squared error (MSE) quantify discrepancies between\npredictions and target values. However, these metrics do not directly evaluate\nthe physical and/or dynamical consistency of forecasts, an increasingly\ncritical concern in scientific and engineering applications.\n  Indeed, a fundamental yet often overlooked question is whether machine\nlearning forecasts preserve the dynamical behavior of the underlying system.\nAddressing this issue is essential for assessing the fidelity of machine\nlearning models and identifying potential failure modes, particularly in\napplications where maintaining correct dynamical behavior is crucial.\n  In this work, we investigate the relationship between standard forecasting\nerror metrics, such as MAE and MSE, and the dynamical properties of the\nunderlying system. To achieve this goal, we use two recently developed\ndynamical indices: the instantaneous dimension ($d$), and the inverse\npersistence ($\\theta$). Our results indicate that larger forecast errors --\ne.g., higher MSE -- tend to occur in states with higher $d$ (higher complexity)\nand higher $\\theta$ (lower persistence). To further assess dynamical\nconsistency, we propose error metrics based on the dynamical indices that\nmeasure the discrepancy of the forecasted $d$ and $\\theta$ versus their correct\nvalues. Leveraging these dynamical indices-based metrics, we analyze direct and\nrecursive forecasting strategies for three canonical datasets -- Lorenz,\nKuramoto-Sivashinsky equation, and Kolmogorov flow -- as well as a real-world\nweather forecasting task. Our findings reveal substantial distortions in\ndynamical properties in ML forecasts, especially for long forecast lead times\nor long recursive simulations, providing complementary information on ML\nforecast fidelity that can be used to improve ML models.", "published": "2025-04-15 11:16:13", "link": "http://arxiv.org/abs/2504.11074v2", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "primary_category": "cs.LG"}
{"title": "GATE3D: Generalized Attention-based Task-synergized Estimation in 3D*", "abstract": "The emerging trend in computer vision emphasizes developing universal models\ncapable of simultaneously addressing multiple diverse tasks. Such universality\ntypically requires joint training across multi-domain datasets to ensure\neffective generalization. However, monocular 3D object detection presents\nunique challenges in multi-domain training due to the scarcity of datasets\nannotated with accurate 3D ground-truth labels, especially beyond typical\nroad-based autonomous driving contexts. To address this challenge, we introduce\na novel weakly supervised framework leveraging pseudo-labels. Current\npretrained models often struggle to accurately detect pedestrians in non-road\nenvironments due to inherent dataset biases. Unlike generalized image-based 2D\nobject detection models, achieving similar generalization in monocular 3D\ndetection remains largely unexplored. In this paper, we propose GATE3D, a novel\nframework designed specifically for generalized monocular 3D object detection\nvia weak supervision. GATE3D effectively bridges domain gaps by employing\nconsistency losses between 2D and 3D predictions. Remarkably, our model\nachieves competitive performance on the KITTI benchmark as well as on an\nindoor-office dataset collected by us to evaluate the generalization\ncapabilities of our framework. Our results demonstrate that GATE3D\nsignificantly accelerates learning from limited annotated data through\neffective pre-training strategies, highlighting substantial potential for\nbroader impacts in robotics, augmented reality, and virtual reality\napplications. Project page: https://ies0411.github.io/GATE3D/", "published": "2025-04-15 09:37:54", "link": "http://arxiv.org/abs/2504.11014v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Visual Language Models show widespread visual deficits on neuropsychological tests", "abstract": "Visual Language Models (VLMs) show remarkable performance in visual reasoning\ntasks, successfully tackling college-level challenges that require high-level\nunderstanding of images. However, some recent reports of VLMs struggling to\nreason about elemental visual concepts like orientation, position, continuity,\nand occlusion suggest a potential gulf between human and VLM vision. Here we\nuse the toolkit of neuropsychology to systematically assess the capabilities of\nthree state-of-the-art VLMs across visual domains. Using 51 tests drawn from\nsix clinical and experimental batteries, we characterise the visual abilities\nof leading VLMs relative to normative performance in healthy adults. While the\nmodels excel in straightforward object recognition tasks, we find widespread\ndeficits in low- and mid-level visual abilities that would be considered\nclinically significant in humans. These selective deficits, profiled through\nvalidated test batteries, suggest that an artificial system can achieve complex\nobject recognition without developing foundational visual concepts that in\nhumans require no explicit training.", "published": "2025-04-15 01:04:56", "link": "http://arxiv.org/abs/2504.10786v2", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.0; I.2.10"], "primary_category": "cs.CV"}
{"title": "Diffusion Distillation With Direct Preference Optimization For Efficient 3D LiDAR Scene Completion", "abstract": "The application of diffusion models in 3D LiDAR scene completion is limited\ndue to diffusion's slow sampling speed. Score distillation accelerates\ndiffusion sampling but with performance degradation, while post-training with\ndirect policy optimization (DPO) boosts performance using preference data. This\npaper proposes Distillation-DPO, a novel diffusion distillation framework for\nLiDAR scene completion with preference aligment. First, the student model\ngenerates paired completion scenes with different initial noises. Second, using\nLiDAR scene evaluation metrics as preference, we construct winning and losing\nsample pairs. Such construction is reasonable, since most LiDAR scene metrics\nare informative but non-differentiable to be optimized directly. Third,\nDistillation-DPO optimizes the student model by exploiting the difference in\nscore functions between the teacher and student models on the paired completion\nscenes. Such procedure is repeated until convergence. Extensive experiments\ndemonstrate that, compared to state-of-the-art LiDAR scene completion diffusion\nmodels, Distillation-DPO achieves higher-quality scene completion while\naccelerating the completion speed by more than 5-fold. Our method is the first\nto explore adopting preference learning in distillation to the best of our\nknowledge and provide insights into preference-aligned distillation. Our code\nis public available on https://github.com/happyw1nd/DistillationDPO.", "published": "2025-04-15 17:57:13", "link": "http://arxiv.org/abs/2504.11447v2", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DeepWheel: Generating a 3D Synthetic Wheel Dataset for Design and Performance Evaluation", "abstract": "Data-driven design is emerging as a powerful strategy to accelerate\nengineering innovation. However, its application to vehicle wheel design\nremains limited due to the lack of large-scale, high-quality datasets that\ninclude 3D geometry and physical performance metrics. To address this gap, this\nstudy proposes a synthetic design-performance dataset generation framework\nusing generative AI. The proposed framework first generates 2D rendered images\nusing Stable Diffusion, and then reconstructs the 3D geometry through 2.5D\ndepth estimation. Structural simulations are subsequently performed to extract\nengineering performance data. To further expand the design and performance\nspace, topology optimization is applied, enabling the generation of a more\ndiverse set of wheel designs. The final dataset, named DeepWheel, consists of\nover 6,000 photo-realistic images and 900 structurally analyzed 3D models. This\nmulti-modal dataset serves as a valuable resource for surrogate model training,\ndata-driven inverse design, and design space exploration. The proposed\nmethodology is also applicable to other complex design domains. The dataset is\nreleased under the Creative Commons Attribution-NonCommercial 4.0\nInternational(CC BY-NC 4.0) and is available on the\nhttps://www.smartdesignlab.org/datasets", "published": "2025-04-15 16:20:00", "link": "http://arxiv.org/abs/2504.11347v2", "categories": ["cs.CV", "physics.app-ph", "68T07"], "primary_category": "cs.CV"}
{"title": "Seedream 3.0 Technical Report", "abstract": "We present Seedream 3.0, a high-performance Chinese-English bilingual image\ngeneration foundation model. We develop several technical improvements to\naddress existing challenges in Seedream 2.0, including alignment with\ncomplicated prompts, fine-grained typography generation, suboptimal visual\naesthetics and fidelity, and limited image resolutions. Specifically, the\nadvancements of Seedream 3.0 stem from improvements across the entire pipeline,\nfrom data construction to model deployment. At the data stratum, we double the\ndataset using a defect-aware training paradigm and a dual-axis collaborative\ndata-sampling framework. Furthermore, we adopt several effective techniques\nsuch as mixed-resolution training, cross-modality RoPE, representation\nalignment loss, and resolution-aware timestep sampling in the pre-training\nphase. During the post-training stage, we utilize diversified aesthetic\ncaptions in SFT, and a VLM-based reward model with scaling, thereby achieving\noutputs that well align with human preferences. Furthermore, Seedream 3.0\npioneers a novel acceleration paradigm. By employing consistent noise\nexpectation and importance-aware timestep sampling, we achieve a 4 to 8 times\nspeedup while maintaining image quality. Seedream 3.0 demonstrates significant\nimprovements over Seedream 2.0: it enhances overall capabilities, in particular\nfor text-rendering in complicated Chinese characters which is important to\nprofessional typography generation. In addition, it provides native\nhigh-resolution output (up to 2K), allowing it to generate images with high\nvisual quality.", "published": "2025-04-15 16:19:07", "link": "http://arxiv.org/abs/2504.11346v2", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians", "abstract": "3D affordance reasoning is essential in associating human instructions with\nthe functional regions of 3D objects, facilitating precise, task-oriented\nmanipulations in embodied AI. However, current methods, which predominantly\ndepend on sparse 3D point clouds, exhibit limited generalizability and\nrobustness due to their sensitivity to coordinate variations and the inherent\nsparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers\nhigh-fidelity, real-time rendering with minimal computational overhead by\nrepresenting scenes as dense, continuous distributions. This positions 3DGS as\na highly effective approach for capturing fine-grained affordance details and\nimproving recognition accuracy. Nevertheless, its full potential remains\nlargely untapped due to the absence of large-scale, 3DGS-specific affordance\ndatasets. To overcome these limitations, we present 3DAffordSplat, the first\nlarge-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning.\nThis dataset includes 23,677 Gaussian instances, 8,354 point cloud instances,\nand 6,631 manually annotated affordance labels, encompassing 21 object\ncategories and 18 affordance types. Building upon this dataset, we introduce\nAffordSplatNet, a novel model specifically designed for affordance reasoning\nusing 3DGS representations. AffordSplatNet features an innovative cross-modal\nstructure alignment module that exploits structural consistency priors to align\n3D point cloud and 3DGS representations, resulting in enhanced affordance\nrecognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat\ndataset significantly advances affordance learning within the 3DGS domain,\nwhile AffordSplatNet consistently outperforms existing methods across both seen\nand unseen settings, highlighting its robust generalization capabilities.", "published": "2025-04-15 14:21:47", "link": "http://arxiv.org/abs/2504.11218v2", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Consensus Entropy: Harnessing Multi-VLM Agreement for Self-Verifying and Self-Improving OCR", "abstract": "The Optical Character Recognition (OCR) task is important for evaluating\nVision-Language Models (VLMs) and providing high-quality data sources for LLM\ntraining data. While state-of-the-art VLMs show improved average OCR accuracy,\nthey still struggle with sample-level quality degradation and lack reliable\nautomatic detection of low-quality outputs. We introduce Consensus Entropy\n(CE), a training-free post-inference method that quantifies OCR uncertainty by\naggregating outputs from multiple VLMs. Our approach exploits a key insight:\ncorrect VLM OCR predictions converge in output space while errors diverge. We\ndevelop a lightweight multi-model framework that effectively identifies\nproblematic samples, selects the best outputs and combines model strengths.\nExperiments across multiple OCR benchmarks and VLMs demonstrate that CE\noutperforms VLM-as-judge approaches and single-model baselines at the same cost\nand achieves state-of-the-art results across multiple metrics. For instance,\nour solution demonstrates: achieving 15.2% higher F1 scores than VLM-as-judge\nmethods in quality verification, delivering 6.0% accuracy gains on mathematical\ncalculation tasks, and requiring rephrasing only 7.3% of inputs while\nmaintaining overall performance. Notably, the entire process requires neither\ntraining nor supervision while maintaining plug-and-play functionality\nthroughout.", "published": "2025-04-15 11:51:18", "link": "http://arxiv.org/abs/2504.11101v2", "categories": ["cs.CV", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Self-Supervised Enhancement of Forward-Looking Sonar Images: Bridging Cross-Modal Degradation Gaps through Feature Space Transformation and Multi-Frame Fusion", "abstract": "Enhancing forward-looking sonar images is critical for accurate underwater\ntarget detection. Current deep learning methods mainly rely on supervised\ntraining with simulated data, but the difficulty in obtaining high-quality\nreal-world paired data limits their practical use and generalization. Although\nself-supervised approaches from remote sensing partially alleviate data\nshortages, they neglect the cross-modal degradation gap between sonar and\nremote sensing images. Directly transferring pretrained weights often leads to\noverly smooth sonar images, detail loss, and insufficient brightness. To\naddress this, we propose a feature-space transformation that maps sonar images\nfrom the pixel domain to a robust feature domain, effectively bridging the\ndegradation gap. Additionally, our self-supervised multi-frame fusion strategy\nleverages complementary inter-frame information to naturally remove speckle\nnoise and enhance target-region brightness. Experiments on three self-collected\nreal-world forward-looking sonar datasets show that our method significantly\noutperforms existing approaches, effectively suppressing noise, preserving\ndetailed edges, and substantially improving brightness, demonstrating strong\npotential for underwater target detection applications.", "published": "2025-04-15 08:34:56", "link": "http://arxiv.org/abs/2504.10974v2", "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Fine-Grained Rib Fracture Diagnosis with Hyperbolic Embeddings: A Detailed Annotation Framework and Multi-Label Classification Model", "abstract": "Accurate rib fracture identification and classification are essential for\ntreatment planning. However, existing datasets often lack fine-grained\nannotations, particularly regarding rib fracture characterization, type, and\nprecise anatomical location on individual ribs. To address this, we introduce a\nnovel rib fracture annotation protocol tailored for fracture classification.\nFurther, we enhance fracture classification by leveraging cross-modal\nembeddings that bridge radiological images and clinical descriptions. Our\napproach employs hyperbolic embeddings to capture the hierarchical nature of\nfracture, mapping visual features and textual descriptions into a shared\nnon-Euclidean manifold. This framework enables more nuanced similarity\ncomputations between imaging characteristics and clinical descriptions,\naccounting for the inherent hierarchical relationships in fracture taxonomy.\nExperimental results demonstrate that our approach outperforms existing methods\nacross multiple classification tasks, with average recall improvements of 6% on\nthe AirRib dataset and 17.5% on the public RibFrac dataset.", "published": "2025-04-15 05:47:09", "link": "http://arxiv.org/abs/2504.10889v2", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Accelerating Multiscale Modeling with Hybrid Solvers: Coupling FEM and Neural Operators with Domain Decomposition", "abstract": "Numerical solvers for partial differential equations (PDEs) face challenges\nbalancing computational cost and accuracy, especially in multiscale and dynamic\nsystems. Neural operators can significantly speed up simulations; however, they\noften face challenges such as error accumulation and limited generalization in\nmultiphysics problems. This work introduces a novel hybrid framework that\nintegrates physics-informed DeepONet with FEM through domain decomposition. The\ncore innovation lies in adaptively coupling FEM and DeepONet subdomains via a\nSchwarz alternating method. This methodology strategically allocates\ncomputationally demanding regions to a pre-trained Deep Operator Network, while\nthe remaining computational domain is solved through FEM. To address dynamic\nsystems, we integrate the Newmark time-stepping scheme directly into the\nDeepONet, significantly mitigating error accumulation in long-term simulations.\nFurthermore, an adaptive subdomain evolution enables the ML-resolved region to\nexpand dynamically, capturing emerging fine-scale features without remeshing.\nThe framework's efficacy has been validated across a range of solid mechanics\nproblems, including static, quasi-static, and dynamic regimes, demonstrating\naccelerated convergence rates (up to 20% improvement compared to FE-FE\napproaches), while preserving solution fidelity with error < 1%. Our case\nstudies show that our proposed hybrid solver: (1) maintains solution continuity\nacross subdomain interfaces, (2) reduces computational costs by eliminating\nfine mesh requirements, (3) mitigates error accumulation in time-dependent\nsimulations, and (4) enables automatic adaptation to evolving physical\nphenomena. This work bridges the gap between numerical methods and AI-driven\nsurrogates, offering a scalable pathway for high-fidelity simulations in\nengineering and scientific applications.", "published": "2025-04-15 16:54:04", "link": "http://arxiv.org/abs/2504.11383v2", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "A Real-time Anomaly Detection Method for Robots based on a Flexible and Sparse Latent Space", "abstract": "The growing demand for robots to operate effectively in diverse environments\nnecessitates the need for robust real-time anomaly detection techniques during\nrobotic operations. However, deep learning-based models in robotics face\nsignificant challenges due to limited training data and highly noisy signal\nfeatures. In this paper, we present Sparse Masked Autoregressive Flow-based\nAdversarial AutoEncoders model to address these problems. This approach\nintegrates Masked Autoregressive Flow model into Adversarial AutoEncoders to\nconstruct a flexible latent space and utilize Sparse autoencoder to efficiently\nfocus on important features, even in scenarios with limited feature space. Our\nexperiments demonstrate that the proposed model achieves a 4.96% to 9.75%\nhigher area under the receiver operating characteristic curve for\npick-and-place robotic operations with randomly placed cans, compared to\nexisting state-of-the-art methods. Notably, it showed up to 19.67% better\nperformance in scenarios involving collisions with lightweight objects.\nAdditionally, unlike the existing state-of-the-art model, our model performs\ninferences within 1 millisecond, ensuring real-time anomaly detection. These\ncapabilities make our model highly applicable to machine learning-based robotic\nsafety systems in dynamic environments. The code will be made publicly\navailable after acceptance.", "published": "2025-04-15 13:17:14", "link": "http://arxiv.org/abs/2504.11170v2", "categories": ["cs.RO", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Wasserstein Distributionally Robust Regret Optimization", "abstract": "Distributionally Robust Optimization (DRO) is a popular framework for\ndecision-making under uncertainty, but its adversarial nature can lead to\noverly conservative solutions. To address this, we study ex-ante\nDistributionally Robust Regret Optimization (DRRO), focusing on\nWasserstein-based ambiguity sets which are popular due to their links to\nregularization and machine learning. We provide a systematic analysis of\nWasserstein DRRO, paralleling known results for Wasserstein DRO. Under\nsmoothness and regularity conditions, we show that Wasserstein DRRO coincides\nwith Empirical Risk Minimization (ERM) up to first-order terms, and exactly so\nin convex quadratic settings. We revisit the Wasserstein DRRO newsvendor\nproblem, where the loss is the maximum of two linear functions of demand and\ndecision. Extending [25], we show that the regret can be computed by maximizing\ntwo one-dimensional concave functions. For more general loss functions\ninvolving the maximum of multiple linear terms in multivariate random variables\nand decision vectors, we prove that computing the regret and thus also the DRRO\npolicy is NP-hard. We then propose a convex relaxation for these more general\nWasserstein DRRO problems and demonstrate its strong empirical performance.\nFinally, we provide an upper bound on the optimality gap of our relaxation and\nshow it improves over recent alternatives.", "published": "2025-04-15 01:47:11", "link": "http://arxiv.org/abs/2504.10796v2", "categories": ["math.OC", "cs.LG"], "primary_category": "math.OC"}
{"title": "Improving Instruct Models for Free: A Study on Partial Adaptation", "abstract": "Instruct models, obtained from various instruction tuning or post-training\nsteps, are commonly deemed superior and more usable than their base\ncounterpart. While the model gains instruction following ability, instruction\ntuning may lead to forgetting the knowledge from pre-training or it may\nencourage the model being overly conversational or verbose. This, in turn, can\nlead to degradation of in-context few-shot learning performance. In this work,\nwe study the performance trajectory between base and instruct models by scaling\ndown the strength of instruction-tuning via the partial adaption method. We\nshow that, across several model families and model sizes, reducing the strength\nof instruction-tuning results in material improvement on a few-shot in-context\nlearning benchmark covering a variety of classic natural language tasks. This\ncomes at the cost of losing some degree of instruction following ability as\nmeasured by AlpacaEval. Our study shines light on the potential trade-off\nbetween in-context learning and instruction following abilities that is worth\nconsidering in practice.", "published": "2025-04-15 21:35:09", "link": "http://arxiv.org/abs/2504.11626v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AskQE: Question Answering as Automatic Evaluation for Machine Translation", "abstract": "How can a monolingual English speaker determine whether an automatic\ntranslation in French is good enough to be shared? Existing MT error detection\nand quality estimation (QE) techniques do not address this practical scenario.\nWe introduce AskQE, a question generation and answering framework designed to\ndetect critical MT errors and provide actionable feedback, helping users decide\nwhether to accept or reject MT outputs even without the knowledge of the target\nlanguage. Using ContraTICO, a dataset of contrastive synthetic MT errors in the\nCOVID-19 domain, we explore design choices for AskQE and develop an optimized\nversion relying on LLaMA-3 70B and entailed facts to guide question generation.\nWe evaluate the resulting system on the BioMQM dataset of naturally occurring\nMT errors, where AskQE has higher Kendall's Tau correlation and decision\naccuracy with human ratings compared to other QE metrics.", "published": "2025-04-15 19:57:42", "link": "http://arxiv.org/abs/2504.11582v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GraphicBench: A Planning Benchmark for Graphic Design with Language Agents", "abstract": "Large Language Model (LLM)-powered agents have unlocked new possibilities for\nautomating human tasks. While prior work has focused on well-defined tasks with\nspecified goals, the capabilities of agents in creative design tasks with\nopen-ended goals remain underexplored. We introduce GraphicBench, a new\nplanning benchmark for graphic design that covers 1,079 user queries and input\nimages across four design types. We further present GraphicTown, an LLM agent\nframework with three design experts and 46 actions (tools) to choose from for\nexecuting each step of the planned workflows in web environments. Experiments\nwith six LLMs demonstrate their ability to generate workflows that integrate\nboth explicit design constraints from user queries and implicit commonsense\nconstraints. However, these workflows often do not lead to successful execution\noutcomes, primarily due to challenges in: (1) reasoning about spatial\nrelationships, (2) coordinating global dependencies across experts, and (3)\nretrieving the most appropriate action per step. We envision GraphicBench as a\nchallenging yet valuable testbed for advancing LLM-agent planning and execution\nin creative design tasks.", "published": "2025-04-15 19:26:59", "link": "http://arxiv.org/abs/2504.11571v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs", "abstract": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement\nlearning (RL), excel in textual reasoning, they struggle in scenarios requiring\nstructured problem-solving, such as geometric reasoning, concise computation,\nor complex equation solving-areas where computational tools like code\ninterpreters (CI) demonstrate distinct advantages. To bridge this gap, we\npropose ReTool, which enhances long-form reasoning with tool-integrated\nlearning, including two key features: (1) dynamic interleaving of real-time\ncode execution within natural language reasoning processes, and (2) an\nautomated RL paradigm that allows policy rollouts with multi-turn real-time\ncode execution and teaches the model in learning when and how to invoke tools\nbased on outcome feedback. ReTool employs a systematic training framework,\nbeginning with synthetic cold-start data generation to produce code-augmented\nlong-form reasoning traces for fine-tuning base models. Subsequent RL training\nleverages task outcomes as rewards to iteratively refine the model's tool use\nstrategy, enabling autonomous discovery of optimal tool invocation patterns\nwithout human priors. Experiments on the challenging MATH Olympiad benchmark\nAIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with\n400 training steps, outperforming text-based RL baseline (40% accuracy, 1080\nsteps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%\naccuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further\nanalysis reveals emergent behaviors such as code self-correction, signaling an\n''aha moment'' in which the model autonomously masters adaptive tool use. These\nfindings highlight the promise of outcome-driven tool integration for advancing\ncomplex mathematical reasoning and offer new insights into hybrid\nneuro-symbolic systems.", "published": "2025-04-15 18:10:22", "link": "http://arxiv.org/abs/2504.11536v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation", "abstract": "There is growing interest in hypothesis generation with large language models\n(LLMs). However, fundamental questions remain: what makes a good hypothesis,\nand how can we systematically evaluate methods for hypothesis generation? To\naddress this, we introduce HypoBench, a novel benchmark designed to evaluate\nLLMs and hypothesis generation methods across multiple aspects, including\npractical utility, generalizability, and hypothesis discovery rate. HypoBench\nincludes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets.\nWe evaluate four state-of-the-art LLMs combined with six existing\nhypothesis-generation methods. Overall, our results suggest that existing\nmethods are capable of discovering valid and novel patterns in the data.\nHowever, the results from synthetic datasets indicate that there is still\nsignificant room for improvement, as current hypothesis generation methods do\nnot fully uncover all relevant or meaningful patterns. Specifically, in\nsynthetic settings, as task difficulty increases, performance significantly\ndrops, with best models and methods only recovering 38.8% of the ground-truth\nhypotheses. These findings highlight challenges in hypothesis generation and\ndemonstrate that HypoBench serves as a valuable resource for improving AI\nsystems designed to assist scientific discovery.", "published": "2025-04-15 18:00:00", "link": "http://arxiv.org/abs/2504.11524v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Graph-Driven Multimodal Feature Learning Framework for Apparent Personality Assessment", "abstract": "Predicting personality traits automatically has become a challenging problem\nin computer vision. This paper introduces an innovative multimodal feature\nlearning framework for personality analysis in short video clips. For visual\nprocessing, we construct a facial graph and design a Geo-based two-stream\nnetwork incorporating an attention mechanism, leveraging both Graph\nConvolutional Networks (GCN) and Convolutional Neural Networks (CNN) to capture\nstatic facial expressions. Additionally, ResNet18 and VGGFace networks are\nemployed to extract global scene and facial appearance features at the frame\nlevel. To capture dynamic temporal information, we integrate a BiGRU with a\ntemporal attention module for extracting salient frame representations. To\nenhance the model's robustness, we incorporate the VGGish CNN for audio-based\nfeatures and XLM-Roberta for text-based features. Finally, a multimodal channel\nattention mechanism is introduced to integrate different modalities, and a\nMulti-Layer Perceptron (MLP) regression model is used to predict personality\ntraits. Experimental results confirm that our proposed framework surpasses\nexisting state-of-the-art approaches in performance.", "published": "2025-04-15 14:26:12", "link": "http://arxiv.org/abs/2504.11515v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Improving LLM Interpretability and Performance via Guided Embedding Refinement for Sequential Recommendation", "abstract": "The fast development of Large Language Models (LLMs) offers growing\nopportunities to further improve sequential recommendation systems. Yet for\nsome practitioners, integrating LLMs to their existing base recommendation\nsystems raises questions about model interpretability, transparency and related\nsafety. To partly alleviate challenges from these questions, we propose guided\nembedding refinement, a method that carries out a guided and interpretable\nusage of LLM to enhance the embeddings associated with the base recommendation\nsystem. Instead of directly using LLMs as the backbone of sequential\nrecommendation systems, we utilize them as auxiliary tools to emulate the sales\nlogic of recommendation and generate guided embeddings that capture\ndomain-relevant semantic information on interpretable attributes. Benefiting\nfrom the strong generalization capabilities of the guided embedding, we\nconstruct refined embedding by using the guided embedding and reduced-dimension\nversion of the base embedding. We then integrate the refined embedding into the\nrecommendation module for training and inference. A range of numerical\nexperiments demonstrate that guided embedding is adaptable to various given\nexisting base embedding models, and generalizes well across different\nrecommendation tasks. The numerical results show that the refined embedding not\nonly improves recommendation performance, achieving approximately $10\\%$ to\n$50\\%$ gains in Mean Reciprocal Rank (MRR), Recall rate, and Normalized\nDiscounted Cumulative Gain (NDCG), but also enhances interpretability, as\nevidenced by case studies.", "published": "2025-04-15 23:03:53", "link": "http://arxiv.org/abs/2504.11658v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "Data driven approach towards more efficient Newton-Raphson power flow calculation for distribution grids", "abstract": "Power flow (PF) calculations are fundamental to power system analysis to\nensure stable and reliable grid operation. The Newton-Raphson (NR) method is\ncommonly used for PF analysis due to its rapid convergence when initialized\nproperly. However, as power grids operate closer to their capacity limits,\nill-conditioned cases and convergence issues pose significant challenges. This\nwork, therefore, addresses these challenges by proposing strategies to improve\nNR initialization, hence minimizing iterations and avoiding divergence. We\nexplore three approaches: (i) an analytical method that estimates the basin of\nattraction using mathematical bounds on voltages, (ii) Two data-driven models\nleveraging supervised learning or physics-informed neural networks (PINNs) to\npredict optimal initial guesses, and (iii) a reinforcement learning (RL)\napproach that incrementally adjusts voltages to accelerate convergence. These\nmethods are tested on benchmark systems. This research is particularly relevant\nfor modern power systems, where high penetration of renewables and\ndecentralized generation require robust and scalable PF solutions. In\nexperiments, all three proposed methods demonstrate a strong ability to provide\nan initial guess for Newton-Raphson method to converge with fewer steps. The\nfindings provide a pathway for more efficient real-time grid operations, which,\nin turn, support the transition toward smarter and more resilient electricity\nnetworks.", "published": "2025-04-15 22:37:55", "link": "http://arxiv.org/abs/2504.11650v1", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.NA", "cs.SY", "math.NA", "I.2.8"], "primary_category": "eess.SY"}
{"title": "Achieving Tighter Finite-Time Rates for Heterogeneous Federated Stochastic Approximation under Markovian Sampling", "abstract": "Motivated by collaborative reinforcement learning (RL) and optimization with\ntime-correlated data, we study a generic federated stochastic approximation\nproblem involving $M$ agents, where each agent is characterized by an\nagent-specific (potentially nonlinear) local operator. The goal is for the\nagents to communicate intermittently via a server to find the root of the\naverage of the agents' local operators. The generality of our setting stems\nfrom allowing for (i) Markovian data at each agent and (ii) heterogeneity in\nthe roots of the agents' local operators. The limited recent work that has\naccounted for both these features in a federated setting fails to guarantee\nconvergence to the desired point or to show any benefit of collaboration;\nfurthermore, they rely on projection steps in their algorithms to guarantee\nbounded iterates. Our work overcomes each of these limitations. We develop a\nnovel algorithm titled \\texttt{FedHSA}, and prove that it guarantees\nconvergence to the correct point, while enjoying an $M$-fold linear speedup in\nsample-complexity due to collaboration. To our knowledge, \\emph{this is the\nfirst finite-time result of its kind}, and establishing it (without relying on\na projection step) entails a fairly intricate argument that accounts for the\ninterplay between complex temporal correlations due to Markovian sampling,\nmultiple local steps to save communication, and the drift-effects induced by\nheterogeneous local operators. Our results have implications for a broad class\nof heterogeneous federated RL problems (e.g., policy evaluation and control)\nwith function approximation, where the agents' Markov decision processes can\ndiffer in their probability transition kernels and reward functions.", "published": "2025-04-15 22:13:55", "link": "http://arxiv.org/abs/2504.11645v1", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "math.OC"], "primary_category": "cs.LG"}
{"title": "Possibility for Proactive Anomaly Detection", "abstract": "Time-series anomaly detection, which detects errors and failures in a\nworkflow, is one of the most important topics in real-world applications. The\npurpose of time-series anomaly detection is to reduce potential damages or\nlosses. However, existing anomaly detection models detect anomalies through the\nerror between the model output and the ground truth (observed) value, which\nmakes them impractical. In this work, we present a \\textit{proactive} approach\nfor time-series anomaly detection based on a time-series forecasting model\nspecialized for anomaly detection and a data-driven anomaly detection model.\nOur proactive approach establishes an anomaly threshold from training data with\na data-driven anomaly detection model, and anomalies are subsequently detected\nby identifying predicted values that exceed the anomaly threshold. In addition,\nwe extensively evaluated the model using four anomaly detection benchmarks and\nanalyzed both predictable and unpredictable anomalies. We attached the source\ncode as supplementary material.", "published": "2025-04-15 21:25:02", "link": "http://arxiv.org/abs/2504.11623v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Towards Interpretable Deep Generative Models via Causal Representation Learning", "abstract": "Recent developments in generative artificial intelligence (AI) rely on\nmachine learning techniques such as deep learning and generative modeling to\nachieve state-of-the-art performance across wide-ranging domains. These\nmethods' surprising performance is due in part to their ability to learn\nimplicit \"representations'' of complex, multi-modal data. Unfortunately, deep\nneural networks are notoriously black boxes that obscure these representations,\nmaking them difficult to interpret or analyze. To resolve these difficulties,\none approach is to build new interpretable neural network models from the\nground up. This is the goal of the emerging field of causal representation\nlearning (CRL) that uses causality as a vector for building flexible,\ninterpretable, and transferable generative AI. CRL can be seen as a culmination\nof three intrinsically statistical problems: (i) latent variable models such as\nfactor analysis; (ii) causal graphical models with latent variables; and (iii)\nnonparametric statistics and deep learning. This paper reviews recent progress\nin CRL from a statistical perspective, focusing on connections to classical\nmodels and statistical and causal identifiablity results. This review also\nhighlights key application areas, implementation strategies, and open\nstatistical questions in CRL.", "published": "2025-04-15 20:46:42", "link": "http://arxiv.org/abs/2504.11609v1", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Deep Learning Approaches for Medical Imaging Under Varying Degrees of Label Availability: A Comprehensive Survey", "abstract": "Deep learning has achieved significant breakthroughs in medical imaging, but\nthese advancements are often dependent on large, well-annotated datasets.\nHowever, obtaining such datasets poses a significant challenge, as it requires\ntime-consuming and labor-intensive annotations from medical experts.\nConsequently, there is growing interest in learning paradigms such as\nincomplete, inexact, and absent supervision, which are designed to operate\nunder limited, inexact, or missing labels. This survey categorizes and reviews\nthe evolving research in these areas, analyzing around 600 notable\ncontributions since 2018. It covers tasks such as image classification,\nsegmentation, and detection across various medical application areas, including\nbut not limited to brain, chest, and cardiac imaging. We attempt to establish\nthe relationships among existing research studies in related areas. We provide\nformal definitions of different learning paradigms and offer a comprehensive\nsummary and interpretation of various learning mechanisms and strategies,\naiding readers in better understanding the current research landscape and\nideas. We also discuss potential future research challenges.", "published": "2025-04-15 20:06:43", "link": "http://arxiv.org/abs/2504.11588v1", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 92C50, 92C55", "I.2.10; I.4.5; I.4.6; I.4.9; J.3"], "primary_category": "cs.CV"}
{"title": "MULTI-LF: A Unified Continuous Learning Framework for Real-Time DDoS Detection in Multi-Environment Networks", "abstract": "Detecting Distributed Denial of Service (DDoS) attacks in Multi-Environment\n(M-En) networks presents significant challenges due to diverse malicious\ntraffic patterns and the evolving nature of cyber threats. Existing AI-based\ndetection systems struggle to adapt to new attack strategies and lack real-time\nattack detection capabilities with high accuracy and efficiency. This study\nproposes an online, continuous learning methodology for DDoS detection in M-En\nnetworks, enabling continuous model updates and real-time adaptation to\nemerging threats, including zero-day attacks. First, we develop a unique M-En\nnetwork dataset by setting up a realistic, real-time simulation using the NS-3\ntool, incorporating both victim and bot devices. DDoS attacks with varying\npacket sizes are simulated using the DDoSim application across IoT and\ntraditional IP-based environments under M-En network criteria. Our approach\nemploys a multi-level framework (MULTI-LF) featuring two machine learning\nmodels: a lightweight Model 1 (M1) trained on a selective, critical packet\ndataset for fast and efficient initial detection, and a more complex, highly\naccurate Model 2 (M2) trained on extensive data. When M1 exhibits low\nconfidence in its predictions, the decision is escalated to M2 for verification\nand potential fine-tuning of M1 using insights from M2. If both models\ndemonstrate low confidence, the system flags the incident for human\nintervention, facilitating model updates with human-verified categories to\nenhance adaptability to unseen attack patterns. We validate the MULTI-LF\nthrough real-world simulations, demonstrating superior classification accuracy\nof 0.999 and low prediction latency of 0.866 seconds compared to established\nbaselines. Furthermore, we evaluate performance in terms of memory usage (3.632\nMB) and CPU utilization (10.05%) in real-time scenarios.", "published": "2025-04-15 19:44:53", "link": "http://arxiv.org/abs/2504.11575v1", "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Perceptions of Agentic AI in Organizations: Implications for Responsible AI and ROI", "abstract": "As artificial intelligence (AI) systems rapidly gain autonomy, the need for\nrobust responsible AI frameworks becomes paramount. This paper investigates how\norganizations perceive and adapt such frameworks amidst the emerging landscape\nof increasingly sophisticated agentic AI. Employing an interpretive qualitative\napproach, the study explores the lived experiences of AI professionals.\nFindings highlight that the inherent complexity of agentic AI systems and their\nresponsible implementation, rooted in the intricate interconnectedness of\nresponsible AI dimensions and the thematic framework (an analytical structure\ndeveloped from the data), combined with the novelty of agentic AI, contribute\nto significant challenges in organizational adaptation, characterized by\nknowledge gaps, a limited emphasis on stakeholder engagement, and a strong\nfocus on control. These factors, by hindering effective adaptation and\nimplementation, ultimately compromise the potential for responsible AI and the\nrealization of ROI.", "published": "2025-04-15 19:15:06", "link": "http://arxiv.org/abs/2504.11564v1", "categories": ["cs.CY", "cs.AI", "68T99 (Primary), 91D25 (Secondary)", "K.4; I.2; K.4.2; K.4.3"], "primary_category": "cs.CY"}
{"title": "Error Broadcast and Decorrelation as a Potential Artificial and Natural Learning Mechanism", "abstract": "We introduce the Error Broadcast and Decorrelation (EBD) algorithm, a novel\nlearning framework that addresses the credit assignment problem in neural\nnetworks by directly broadcasting output error to individual layers. Leveraging\nthe stochastic orthogonality property of the optimal minimum mean square error\n(MMSE) estimator, EBD defines layerwise loss functions to penalize correlations\nbetween layer activations and output errors, offering a principled approach to\nerror broadcasting without the need for weight transport. The optimization\nframework naturally leads to the experimentally observed three-factor learning\nrule and integrates with biologically plausible frameworks to enhance\nperformance and plausibility. Numerical experiments demonstrate that EBD\nachieves performance comparable to or better than known error-broadcast methods\non benchmark datasets. While the scalability of EBD to very large or complex\ndatasets remains to be further explored, our findings suggest it provides a\nbiologically plausible, efficient, and adaptable alternative for neural network\ntraining. This approach could inform future advancements in artificial and\nnatural learning paradigms.", "published": "2025-04-15 19:00:53", "link": "http://arxiv.org/abs/2504.11558v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Probabilistic causal graphs as categorical data synthesizers: Do they do better than Gaussian Copulas and Conditional Tabular GANs?", "abstract": "This study investigates the generation of high-quality synthetic categorical\ndata, such as survey data, using causal graph models. Generating synthetic data\naims not only to create a variety of data for training the models but also to\npreserve privacy while capturing relationships between the data. The research\nemploys Structural Equation Modeling (SEM) followed by Bayesian Networks (BN).\nWe used the categorical data that are based on the survey of accessibility to\nservices for people with disabilities. We created both SEM and BN models to\nrepresent causal relationships and to capture joint distributions between\nvariables. In our case studies, such variables include, in particular,\ndemographics, types of disability, types of accessibility barriers and\nfrequencies of encountering those barriers.\n  The study compared the SEM-based BN method with alternative approaches,\nincluding the probabilistic Gaussian copula technique and generative models\nlike the Conditional Tabular Generative Adversarial Network (CTGAN). The\nproposed method outperformed others in statistical metrics, including the\nChi-square test, Kullback-Leibler divergence, and Total Variation Distance\n(TVD). In particular, the BN model demonstrated superior performance, achieving\nthe highest TVD, indicating alignment with the original data. The Gaussian\nCopula ranked second, while CTGAN exhibited moderate performance. These\nanalyses confirmed the ability of the SEM-based BN to produce synthetic data\nthat maintain statistical and relational validity while maintaining\nconfidentiality. This approach is particularly beneficial for research on\nsensitive data, such as accessibility and disability studies.", "published": "2025-04-15 18:41:54", "link": "http://arxiv.org/abs/2504.11547v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes", "abstract": "Retrieval-augmented generation (RAG) empowers large language models to access\nexternal and private corpus, enabling factually consistent responses in\nspecific domains. By exploiting the inherent structure of the corpus,\ngraph-based RAG methods further enrich this process by building a knowledge\ngraph index and leveraging the structural nature of graphs. However, current\ngraph-based RAG approaches seldom prioritize the design of graph structures.\nInadequately designed graph not only impede the seamless integration of diverse\ngraph algorithms but also result in workflow inconsistencies and degraded\nperformance. To further unleash the potential of graph for RAG, we propose\nNodeRAG, a graph-centric framework introducing heterogeneous graph structures\nthat enable the seamless and holistic integration of graph-based methodologies\ninto the RAG workflow. By aligning closely with the capabilities of LLMs, this\nframework ensures a fully cohesive and efficient end-to-end process. Through\nextensive experiments, we demonstrate that NodeRAG exhibits performance\nadvantages over previous methods, including GraphRAG and LightRAG, not only in\nindexing time, query time, and storage efficiency but also in delivering\nsuperior question-answering performance on multi-hop benchmarks and open-ended\nhead-to-head evaluations with minimal retrieval tokens. Our GitHub repository\ncould be seen at https://github.com/Terry-Xu-666/NodeRAG.", "published": "2025-04-15 18:24:00", "link": "http://arxiv.org/abs/2504.11544v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites", "abstract": "We introduce REAL, a benchmark and framework for multi-turn agent evaluations\non deterministic simulations of real-world websites. REAL comprises\nhigh-fidelity, deterministic replicas of 11 widely-used websites across domains\nsuch as e-commerce, travel, communication, and professional networking. We also\nrelease a benchmark consisting of 112 practical tasks that mirror everyday\ncomplex user interactions requiring both accurate information retrieval and\nstate-changing actions. All interactions occur within this fully controlled\nsetting, eliminating safety risks and enabling robust, reproducible evaluation\nof agent capability and reliability. Our novel evaluation framework combines\nprogrammatic checks of website state for action-based tasks with rubric-guided\nLLM-based judgments for information retrieval. The framework supports both\nopen-source and proprietary agent systems through a flexible evaluation harness\nthat accommodates black-box commands within browser environments, allowing\nresearch labs to test agentic systems without modification. Our empirical\nresults show that frontier language models achieve at most a 41% success rate\non REAL, highlighting critical gaps in autonomous web navigation and task\ncompletion capabilities. Our framework supports easy integration of new tasks,\nreproducible evaluation, and scalable data generation for training web agents.\nThe websites, framework, and leaderboard are available at https://realevals.xyz\nand https://github.com/agi-inc/REAL.", "published": "2025-04-15 18:22:55", "link": "http://arxiv.org/abs/2504.11543v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Co-STAR: Collaborative Curriculum Self-Training with Adaptive Regularization for Source-Free Video Domain Adaptation", "abstract": "Recent advances in Source-Free Unsupervised Video Domain Adaptation (SFUVDA)\nleverage vision-language models to enhance pseudo-label generation. However,\nchallenges such as noisy pseudo-labels and over-confident predictions limit\ntheir effectiveness in adapting well across domains. We propose Co-STAR, a\nnovel framework that integrates curriculum learning with collaborative\nself-training between a source-trained teacher and a contrastive\nvision-language model (CLIP). Our curriculum learning approach employs a\nreliability-based weight function that measures bidirectional prediction\nalignment between the teacher and CLIP, balancing between confident and\nuncertain predictions. This function preserves uncertainty for difficult\nsamples, while prioritizing reliable pseudo-labels when the predictions from\nboth models closely align. To further improve adaptation, we propose Adaptive\nCurriculum Regularization, which modifies the learning priority of samples in a\nprobabilistic, adaptive manner based on their confidence scores and prediction\nstability, mitigating overfitting to noisy and over-confident samples.\nExtensive experiments across multiple video domain adaptation benchmarks\ndemonstrate that Co-STAR consistently outperforms state-of-the-art SFUVDA\nmethods. Code is available at: https://github.com/Plrbear/Co-Star", "published": "2025-04-15 23:47:35", "link": "http://arxiv.org/abs/2504.11669v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Real-time Object and Event Detection Service through Computer Vision and Edge Computing", "abstract": "The World Health Organization suggests that road traffic crashes cost\napproximately 518 billion dollars globally each year, which accounts for 3% of\nthe gross domestic product for most countries. Most fatal road accidents in\nurban areas involve Vulnerable Road Users (VRUs). Smart cities environments\npresent innovative approaches to combat accidents involving cutting-edge\ntechnologies, that include advanced sensors, extensive datasets, Machine\nLearning (ML) models, communication systems, and edge computing. This paper\nproposes a strategy and an implementation of a system for road monitoring and\nsafety for smart cities, based on Computer Vision (CV) and edge computing.\nPromising results were obtained by implementing vision algorithms and tracking\nusing surveillance cameras, that are part of a Smart City testbed, the Aveiro\nTech City Living Lab (ATCLL). The algorithm accurately detects and tracks cars,\npedestrians, and bicycles, while predicting the road state, the distance\nbetween moving objects, and inferring on collision events to prevent\ncollisions, in near real-time.", "published": "2025-04-15 23:11:42", "link": "http://arxiv.org/abs/2504.11662v1", "categories": ["cs.CV", "68T45"], "primary_category": "cs.CV"}
{"title": "DamageCAT: A Deep Learning Transformer Framework for Typology-Based Post-Disaster Building Damage Categorization", "abstract": "Natural disasters increasingly threaten communities worldwide, creating an\nurgent need for rapid, reliable building damage assessment to guide emergency\nresponse and recovery efforts. Current methods typically classify damage in\nbinary (damaged/undamaged) or ordinal severity terms, limiting their practical\nutility. In fact, the determination of damage typology is crucial for response\nand recovery efforts. To address this important gap, this paper introduces\nDamageCAT, a novel framework that provides typology-based categorical damage\ndescriptions rather than simple severity ratings. Accordingly, this study\npresents two key contributions: (1) the BD-TypoSAT dataset containing satellite\nimage triplets (pre-disaster, post-disaster, and damage masks) from Hurricane\nIda with four damage categories (partial roof damage, total roof damage,\npartial structural collapse, and total structural collapse), and (2) a\nhierarchical U-Net-based transformer architecture that effectively processes\npre-post disaster image pairs to identify and categorize building damage.\nDespite significant class imbalances in the training data, our model achieved\nrobust performance with overall metrics of 0.7921 Intersection over Union (IoU)\nand 0.8835 F1 scores across all categories. The model's capability to recognize\nintricate damage typology in less common categories is especially remarkable.\nThe DamageCAT framework advances automated damage assessment by providing\nactionable, typological information that better supports disaster response\ndecision-making and resource allocation compared to traditional severity-based\napproaches.", "published": "2025-04-15 21:53:59", "link": "http://arxiv.org/abs/2504.11637v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "RAID: An In-Training Defense against Attribute Inference Attacks in Recommender Systems", "abstract": "In various networks and mobile applications, users are highly susceptible to\nattribute inference attacks, with particularly prevalent occurrences in\nrecommender systems. Attackers exploit partially exposed user profiles in\nrecommendation models, such as user embeddings, to infer private attributes of\ntarget users, such as gender and political views. The goal of defenders is to\nmitigate the effectiveness of these attacks while maintaining recommendation\nperformance. Most existing defense methods, such as differential privacy and\nattribute unlearning, focus on post-training settings, which limits their\ncapability of utilizing training data to preserve recommendation performance.\nAlthough adversarial training extends defenses to in-training settings, it\noften struggles with convergence due to unstable training processes. In this\npaper, we propose RAID, an in-training defense method against attribute\ninference attacks in recommender systems. In addition to the recommendation\nobjective, we define a defensive objective to ensure that the distribution of\nprotected attributes becomes independent of class labels, making users\nindistinguishable from attribute inference attacks. Specifically, this\ndefensive objective aims to solve a constrained Wasserstein barycenter problem\nto identify the centroid distribution that makes the attribute\nindistinguishable while complying with recommendation performance constraints.\nTo optimize our proposed objective, we use optimal transport to align users\nwith the centroid distribution. We conduct extensive experiments on four\nreal-world datasets to evaluate RAID. The experimental results validate the\neffectiveness of RAID and demonstrate its significant superiority over existing\nmethods in multiple aspects.", "published": "2025-04-15 10:24:37", "link": "http://arxiv.org/abs/2504.11510v1", "categories": ["cs.IR", "cs.AI", "cs.CR", "cs.CY", "cs.LG"], "primary_category": "cs.IR"}
{"title": "PATFinger: Prompt-Adapted Transferable Fingerprinting against Unauthorized Multimodal Dataset Usage", "abstract": "The multimodal datasets can be leveraged to pre-train large-scale\nvision-language models by providing cross-modal semantics. Current endeavors\nfor determining the usage of datasets mainly focus on single-modal dataset\nownership verification through intrusive methods and non-intrusive techniques,\nwhile cross-modal approaches remain under-explored. Intrusive methods can adapt\nto multimodal datasets but degrade model accuracy, while non-intrusive methods\nrely on label-driven decision boundaries that fail to guarantee stable\nbehaviors for verification. To address these issues, we propose a novel\nprompt-adapted transferable fingerprinting scheme from a training-free\nperspective, called PATFinger, which incorporates the global optimal\nperturbation (GOP) and the adaptive prompts to capture dataset-specific\ndistribution characteristics. Our scheme utilizes inherent dataset attributes\nas fingerprints instead of compelling the model to learn triggers. The GOP is\nderived from the sample distribution to maximize embedding drifts between\ndifferent modalities. Subsequently, our PATFinger re-aligns the adaptive prompt\nwith GOP samples to capture the cross-modal interactions on the carefully\ncrafted surrogate model. This allows the dataset owner to check the usage of\ndatasets by observing specific prediction behaviors linked to the PATFinger\nduring retrieval queries. Extensive experiments demonstrate the effectiveness\nof our scheme against unauthorized multimodal dataset usage on various\ncross-modal retrieval architectures by 30% over state-of-the-art baselines.", "published": "2025-04-15 09:53:02", "link": "http://arxiv.org/abs/2504.11509v1", "categories": ["cs.IR", "cs.CV"], "primary_category": "cs.IR"}
{"title": "Adaptive Error Correction for Entanglement Distillation", "abstract": "Quantum network applications impose a variety of requirements on entanglement\nresources in terms of rate, fidelity, latency, and more. The repeaters in the\nquantum network must combine good methods for entanglement generation,\neffective entanglement distillation, and smart routing protocols to satisfy\nthese application requirements. In this work, we focus on quantum error\ncorrection-based entanglement distillation in a linear chain of quantum\nrepeaters. While conventional approaches reuse the same distillation scheme\nover multiple hop lengths after entanglement swaps, we propose a novel adaptive\nerror correction scheme that boosts end-to-end metrics. Specifically, depending\non the network operation point, we adapt the code used in distillation over\nsuccessive rounds to monotonically increase the rate while also improving\nfidelity. We demonstrate the effectiveness of this strategy using three codes:\n[[9,1,3]], [[9,2,3]], [[9,3,3]]. We compare the performance of four different\nprotocols that combine the codes in different ways, where we define a new\nperformance metric, efficiency, that incorporates both overall rate and\nfidelity. While we highlight our innovation under minimal assumptions on noise,\nthe method can be easily generalized to realistic network settings. By\ncombining our approach with good entanglement generation methods and smart\nrouting protocols, we can achieve application requirements in a systematic,\nresource-efficient, way.", "published": "2025-04-15 23:52:25", "link": "http://arxiv.org/abs/2504.11670v1", "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "quant-ph"}
{"title": "Transformer-Driven Neural Beamforming with Imperfect CSI in Urban Macro Wireless Channels", "abstract": "The literature is abundant with methodologies focusing on using transformer\narchitectures due to their prominence in wireless signal processing and their\ncapability to capture long-range dependencies via attention mechanisms. In\nparticular, depthwise separable convolutions enhance parameter efficiency for\nthe process of high-dimensional data characteristics of MIMO systems. In this\nwork, we introduce a novel unsupervised deep learning framework that integrates\ndepthwise separable convolutions and transformers to generate beamforming\nweights under imperfect channel state information (CSI) for a multi-user\nsingle-input multiple-output (MU-SIMO) system in dense urban environments. The\nprimary goal is to enhance throughput by maximizing sum-rate while ensuring\nreliable communication. Spectral efficiency and block error rate (BLER) are\nconsidered as performance metrics. Experiments are carried out under various\nconditions to compare the performance of the proposed NNBF framework against\nbaseline methods zero-forcing beamforming (ZFBF) and minimum mean square error\n(MMSE) beamforming. Experimental results demonstrate the superiority of the\nproposed framework over the baseline techniques.", "published": "2025-04-15 23:41:24", "link": "http://arxiv.org/abs/2504.11667v1", "categories": ["cs.IT", "cs.LG", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float", "abstract": "Large Language Models (LLMs) have grown rapidly in size, creating significant\nchallenges for efficient deployment on resource-constrained hardware. In this\npaper, we introduce Dynamic-Length Float (DFloat11), a lossless compression\nframework that reduces LLM size by 30% while preserving outputs that are\nbit-for-bit identical to the original model. DFloat11 is motivated by the low\nentropy in the BFloat16 weight representation of LLMs, which reveals\nsignificant inefficiency in existing storage format. By applying entropy\ncoding, DFloat11 assigns dynamic-length encodings to weights based on\nfrequency, achieving near information-optimal compression without any loss of\nprecision. To facilitate efficient inference with dynamic-length encodings, we\ndevelop a custom GPU kernel for fast online decompression. Our design\nincorporates the following: (i) decomposition of memory-intensive lookup tables\n(LUTs) into compact LUTs that fit in GPU SRAM, (ii) a two-phase kernel for\ncoordinating thread read/write positions using lightweight auxiliary variables,\nand (iii) transformer-block-level decompression to minimize latency.\nExperiments on recent models, including Llama-3.1, Qwen-2.5, and Gemma-3,\nvalidates our hypothesis that DFloat11 achieves around 30% model size reduction\nwhile preserving bit-for-bit exact outputs. Compared to a potential alternative\nof offloading parts of an uncompressed model to the CPU to meet memory\nconstraints, DFloat11 achieves 1.9-38.8x higher throughput in token generation.\nWith a fixed GPU memory budget, DFloat11 enables 5.3-13.17x longer context\nlengths than uncompressed models. Notably, our method enables lossless\ninference of Llama-3.1-405B, an 810GB model, on a single node equipped with\n8x80GB GPUs. Our code and models are available at\nhttps://github.com/LeanModels/DFloat11.", "published": "2025-04-15 22:38:38", "link": "http://arxiv.org/abs/2504.11651v1", "categories": ["cs.LG", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Robust Markov stability for community detection at a scale learned based on the structure", "abstract": "Community detection, the unsupervised task of clustering nodes of a graph,\nfinds applications across various fields. The common approaches for community\ndetection involve optimizing an objective function to partition the nodes into\ncommunities at a single scale of granularity. However, the single-scale\napproaches often fall short of producing partitions that are robust and at a\nsuitable scale. The existing algorithm, PyGenStability, returns multiple robust\npartitions for a network by optimizing the multi-scale Markov stability\nfunction. However, in cases where the suitable scale is not known or assumed by\nthe user, there is no principled method to select a single robust partition at\na suitable scale from the multiple partitions that PyGenStability produces. Our\nproposed method combines the Markov stability framework with a pre-trained\nmachine learning model for scale selection to obtain one robust partition at a\nscale that is learned based on the graph structure. This automatic scale\nselection involves using a gradient boosting model pre-trained on hand-crafted\nand embedding-based network features from a labeled dataset of 10k benchmark\nnetworks. This model was trained to predicts the scale value that maximizes the\nsimilarity of the output partition to the planted partition of the benchmark\nnetwork. Combining our scale selection algorithm with the PyGenStability\nalgorithm results in PyGenStabilityOne (PO): a hyperparameter-free multi-scale\ncommunity detection algorithm that returns one robust partition at a suitable\nscale without the need for any assumptions, input, or tweaking from the user.\nWe compare the performance of PO against 29 algorithms and show that it\noutperforms 25 other algorithms by statistically meaningful margins. Our\nresults facilitate choosing between community detection algorithms, among which\nPO stands out as the accurate, robust, and hyperparameter-free method.", "published": "2025-04-15 21:16:14", "link": "http://arxiv.org/abs/2504.11621v1", "categories": ["cs.SI", "cond-mat.stat-mech", "cs.LG", "90C90, 90C10, 90C57, 90C59, 90C35, 05C15, 65K05", "I.2.6; G.2.2"], "primary_category": "cs.SI"}
{"title": "Generalized probabilistic canonical correlation analysis for multi-modal data integration with full or partial observations", "abstract": "Background: The integration and analysis of multi-modal data are increasingly\nessential across various domains including bioinformatics. As the volume and\ncomplexity of such data grow, there is a pressing need for computational models\nthat not only integrate diverse modalities but also leverage their\ncomplementary information to improve clustering accuracy and insights,\nespecially when dealing with partial observations with missing data. Results:\nWe propose Generalized Probabilistic Canonical Correlation Analysis (GPCCA), an\nunsupervised method for the integration and joint dimensionality reduction of\nmulti-modal data. GPCCA addresses key challenges in multi-modal data analysis\nby handling missing values within the model, enabling the integration of more\nthan two modalities, and identifying informative features while accounting for\ncorrelations within individual modalities. The model demonstrates robustness to\nvarious missing data patterns and provides low-dimensional embeddings that\nfacilitate downstream clustering and analysis. In a range of simulation\nsettings, GPCCA outperforms existing methods in capturing essential patterns\nacross modalities. Additionally, we demonstrate its applicability to\nmulti-omics data from TCGA cancer datasets and a multi-view image dataset.\nConclusion: GPCCA offers a useful framework for multi-modal data integration,\neffectively handling missing data and providing informative low-dimensional\nembeddings. Its performance across cancer genomics and multi-view image data\nhighlights its robustness and potential for broad application. To make the\nmethod accessible to the wider research community, we have released an R\npackage, GPCCA, which is available at https://github.com/Kaversoniano/GPCCA.", "published": "2025-04-15 20:49:31", "link": "http://arxiv.org/abs/2504.11610v1", "categories": ["stat.ML", "cs.LG", "q-bio.QM"], "primary_category": "stat.ML"}
{"title": "Dueling Deep Reinforcement Learning for Financial Time Series", "abstract": "Reinforcement learning (RL) has emerged as a powerful paradigm for solving\ndecision-making problems in dynamic environments. In this research, we explore\nthe application of Double DQN (DDQN) and Dueling Network Architectures, to\nfinancial trading tasks using historical SP500 index data. Our focus is\ntraining agents capable of optimizing trading strategies while accounting for\npractical constraints such as transaction costs. The study evaluates the model\nperformance across scenarios with and without commissions, highlighting the\nimpact of cost-sensitive environments on reward dynamics. Despite computational\nlimitations and the inherent complexity of financial time series data, the\nagent successfully learned meaningful trading policies. The findings confirm\nthat RL agents, even when trained on limited datasets, can outperform random\nstrategies by leveraging advanced architectures such as DDQN and Dueling\nNetworks. However, significant challenges persist, particularly with a\nsub-optimal policy due to the complexity of data source.", "published": "2025-04-15 20:30:34", "link": "http://arxiv.org/abs/2504.11601v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Towards a Universal Vibration Analysis Dataset: A Framework for Transfer Learning in Predictive Maintenance and Structural Health Monitoring", "abstract": "ImageNet has become a reputable resource for transfer learning, allowing the\ndevelopment of efficient ML models with reduced training time and data\nrequirements. However, vibration analysis in predictive maintenance, structural\nhealth monitoring, and fault diagnosis, lacks a comparable large-scale,\nannotated dataset to facilitate similar advancements. To address this, a\ndataset framework is proposed that begins with bearing vibration data as an\ninitial step towards creating a universal dataset for vibration-based\nspectrogram analysis for all machinery. The initial framework includes a\ncollection of bearing vibration signals from various publicly available\ndatasets. To demonstrate the advantages of this framework, experiments were\nconducted using a deep learning architecture, showing improvements in model\nperformance when pre-trained on bearing vibration data and fine-tuned on a\nsmaller, domain-specific dataset. These findings highlight the potential to\nparallel the success of ImageNet in visual computing but for vibration\nanalysis. For future work, this research will include a broader range of\nvibration signals from multiple types of machinery, emphasizing\nspectrogram-based representations of the data. Each sample will be labeled\naccording to machinery type, operational status, and the presence or type of\nfaults, ensuring its utility for supervised and unsupervised learning tasks.\nAdditionally, a framework for data preprocessing, feature extraction, and model\ntraining specific to vibration data will be developed. This framework will\nstandardize methodologies across the research community, allowing for\ncollaboration and accelerating progress in predictive maintenance, structural\nhealth monitoring, and related fields. By mirroring the success of ImageNet in\nvisual computing, this dataset has the potential to improve the development of\nintelligent systems in industrial applications.", "published": "2025-04-15 19:57:26", "link": "http://arxiv.org/abs/2504.11581v1", "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Traffic Adaptive Moving-window Service Patrolling for Real-time Incident Management during High-impact Events", "abstract": "This paper presents the Traffic Adaptive Moving-window Patrolling Algorithm\n(TAMPA), designed to improve real-time incident management during major events\nlike sports tournaments and concerts. Such events significantly stress\ntransportation networks, requiring efficient and adaptive patrol solutions.\nTAMPA integrates predictive traffic modeling and real-time complaint\nestimation, dynamically optimizing patrol deployment. Using dynamic\nprogramming, the algorithm continuously adjusts patrol strategies within short\nplanning windows, effectively balancing immediate response and efficient\nrouting. Leveraging the Dvoretzky-Kiefer-Wolfowitz inequality, TAMPA detects\nsignificant shifts in complaint patterns, triggering proactive adjustments in\npatrol routes. Theoretical analyses ensure performance remains closely aligned\nwith optimal solutions. Simulation results from an urban traffic network\ndemonstrate TAMPA's superior performance, showing improvements of approximately\n87.5\\% over stationary methods and 114.2\\% over random strategies. Future work\nincludes enhancing adaptability and incorporating digital twin technology for\nimproved predictive accuracy, particularly relevant for events like the 2026\nFIFA World Cup at MetLife Stadium.", "published": "2025-04-15 19:25:50", "link": "http://arxiv.org/abs/2504.11570v1", "categories": ["math.OC", "cs.LG"], "primary_category": "math.OC"}
{"title": "Sub-optimality of the Separation Principle for Quadratic Control from Bilinear Observations", "abstract": "We consider the problem of controlling a linear dynamical system from\nbilinear observations with minimal quadratic cost. Despite the similarity of\nthis problem to standard linear quadratic Gaussian (LQG) control, we show that\nwhen the observation model is bilinear, neither does the Separation Principle\nhold, nor is the optimal controller affine in the estimated state. Moreover,\nthe cost-to-go is non-convex in the control input. Hence, finding an analytical\nexpression for the optimal feedback controller is difficult in general. Under\ncertain settings, we show that the standard LQG controller locally maximizes\nthe cost instead of minimizing it. Furthermore, the optimal controllers\n(derived analytically) are not unique and are nonlinear in the estimated state.\nWe also introduce a notion of input-dependent observability and derive\nconditions under which the Kalman filter covariance remains bounded. We\nillustrate our theoretical results through numerical experiments in multiple\nsynthetic settings.", "published": "2025-04-15 18:53:51", "link": "http://arxiv.org/abs/2504.11555v1", "categories": ["math.OC", "cs.LG", "cs.SY", "eess.SY", "stat.ML"], "primary_category": "math.OC"}
{"title": "Normalizing Flow Regression for Bayesian Inference with Offline Likelihood Evaluations", "abstract": "Bayesian inference with computationally expensive likelihood evaluations\nremains a significant challenge in many scientific domains. We propose\nnormalizing flow regression (NFR), a novel offline inference method for\napproximating posterior distributions. Unlike traditional surrogate approaches\nthat require additional sampling or inference steps, NFR directly yields a\ntractable posterior approximation through regression on existing log-density\nevaluations. We introduce training techniques specifically for flow regression,\nsuch as tailored priors and likelihood functions, to achieve robust posterior\nand model evidence estimation. We demonstrate NFR's effectiveness on synthetic\nbenchmarks and real-world applications from neuroscience and biology, showing\nsuperior or comparable performance to existing methods. NFR represents a\npromising approach for Bayesian inference when standard methods are\ncomputationally prohibitive or existing model evaluations can be recycled.", "published": "2025-04-15 18:52:33", "link": "http://arxiv.org/abs/2504.11554v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "LANGTRAJ: Diffusion Model and Dataset for Language-Conditioned Trajectory Simulation", "abstract": "Evaluating autonomous vehicles with controllability enables scalable testing\nin counterfactual or structured settings, enhancing both efficiency and safety.\nWe introduce LangTraj, a language-conditioned scene-diffusion model that\nsimulates the joint behavior of all agents in traffic scenarios. By\nconditioning on natural language inputs, LangTraj provides flexible and\nintuitive control over interactive behaviors, generating nuanced and realistic\nscenarios. Unlike prior approaches that depend on domain-specific guidance\nfunctions, LangTraj incorporates language conditioning during training,\nfacilitating more intuitive traffic simulation control. We propose a novel\nclosed-loop training strategy for diffusion models, explicitly tailored to\nenhance stability and realism during closed-loop simulation. To support\nlanguage-conditioned simulation, we develop Inter-Drive, a large-scale dataset\nwith diverse and interactive labels for training language-conditioned diffusion\nmodels. Our dataset is built upon a scalable pipeline for annotating\nagent-agent interactions and single-agent behaviors, ensuring rich and varied\nsupervision. Validated on the Waymo Motion Dataset, LangTraj demonstrates\nstrong performance in realism, language controllability, and\nlanguage-conditioned safety-critical simulation, establishing a new paradigm\nfor flexible and scalable autonomous vehicle testing.", "published": "2025-04-15 17:14:06", "link": "http://arxiv.org/abs/2504.11521v1", "categories": ["cs.LG", "cs.RO", "I.2.9; I.2.6"], "primary_category": "cs.LG"}
{"title": "Strengthening Anomaly Awareness", "abstract": "We present a refined version of the Anomaly Awareness framework for enhancing\nunsupervised anomaly detection. Our approach introduces minimal supervision\ninto Variational Autoencoders (VAEs) through a two-stage training strategy: the\nmodel is first trained in an unsupervised manner on background data, and then\nfine-tuned using a small sample of labeled anomalies to encourage larger\nreconstruction errors for anomalous samples.\n  We validate the method across diverse domains, including the MNIST dataset\nwith synthetic anomalies, network intrusion data from the CICIDS benchmark,\ncollider physics data from the LHCO2020 dataset, and simulated events from the\nStandard Model Effective Field Theory (SMEFT). The latter provides a realistic\nexample of subtle kinematic deviations in Higgs boson production. In all cases,\nthe model demonstrates improved sensitivity to unseen anomalies, achieving\nbetter separation between normal and anomalous samples. These results indicate\nthat even limited anomaly information, when incorporated through targeted\nfine-tuning, can substantially improve the generalization and performance of\nunsupervised models for anomaly detection.", "published": "2025-04-15 16:52:22", "link": "http://arxiv.org/abs/2504.11520v1", "categories": ["hep-ph", "cs.LG"], "primary_category": "hep-ph"}
{"title": "Uncertainty Quantification in Multiscale Modeling of Polymer Composite Materials Using Physically Recurrent Neural Networks", "abstract": "This study investigates whether Physically Recurrent Neural Networks (PRNNs),\na recent surrogate model for heterogeneous materials, trained on a micromodel\nwith fixed material parameters, can maintain accuracy for varying material\nproperties without retraining, and propagate uncertainty in a multiscale\nframework. Unlike conventional RNNs, where parameter changes require training\nor explicit inclusion of material properties as extra input features, PRNNs\nembeds material models in their material layer that allow for modification of\nmaterial parameters after training. When adjusting material properties\ndynamically according to the input during testing, PRNN shows high accuracy\nacross a wide range of parameters. Therefore the surrogate can be applied to\nmultiscale uncertainty quantification (UQ). Compared to the full-order\nsimulations on an overly coarse mesh, the PRNN-driven model reduces simulation\ntime by over 7000 times while accurately capturing highly nonlinear evolution\nof the probability density for the macroscopic response as a result of a given\ndistribution for microscale material parameters. A PRNN-driven UQ is\ndemonstrated on a more accurate finer mesh that would be computationally\ninfeasible with the full-order model.", "published": "2025-04-15 21:33:56", "link": "http://arxiv.org/abs/2504.11625v1", "categories": ["cond-mat.dis-nn", "cond-mat.mtrl-sci", "cs.NA", "math.NA"], "primary_category": "cond-mat.dis-nn"}
{"title": "Computing the tropical Abel--Jacobi Transform and tropical distances for metric graphs", "abstract": "Metric graphs are important models for capturing the structure of complex\ndata across various domains. While much effort has been devoted to extracting\ngeometric and topological features from graph data, computational aspects of\nmetric graphs as abstract tropical curves remains unexplored. In this paper, we\npresent the first computational and machine learning-driven study of metric\ngraphs from the perspective of tropical algebraic geometry. Specifically, we\nstudy the tropical Abel--Jacobi transform, a vectorization of points on a\nmetric graph via the tropical Abel--Jacobi map into its associated flat torus,\nthe tropical Jacobian. We develop algorithms to compute this transform and\ninvestigate how the resulting embeddings depend on different combinatorial\nmodels of the same metric graph.\n  Once embedded, we compute pairwise distances between points in the tropical\nJacobian under two natural metrics: the tropical polarization distance and the\nFoster--Zhang distance. Computing these distances are generally NP-hard as they\nturn out to be linked to classical lattice problems in computational\ncomplexity, however, we identify a class of metric graphs where fast and\nexplicit computations are feasible. For the general case, we propose practical\nalgorithms for both exact and approximate distance matrix computations using\nlattice basis reduction and mixed-integer programming solvers. Our work lays\nthe groundwork for future applications of tropical geometry and the tropical\nAbel--Jacobi transform in machine learning and data analysis.", "published": "2025-04-15 21:15:32", "link": "http://arxiv.org/abs/2504.11619v1", "categories": ["math.AG", "cs.NA", "math.MG", "math.NA"], "primary_category": "math.AG"}
{"title": "Effective dimensionality reduction for Greeks computation using Randomized QMC", "abstract": "Global sensitivity analysis is employed to evaluate the effective dimension\nreduction achieved through Chebyshev interpolation and the conditional pathwise\nmethod for Greek estimation of discretely monitored barrier options and\narithmetic average Asian options. We compare results from finite difference and\nMonte Carlo methods with those obtained by using randomized Quasi Monte Carlo\ncombined with Brownian bridge discretization. Additionally, we investigate the\nbenefits of incorporating importance sampling with either the finite difference\nor Chebyshev interpolation methods. Our findings demonstrate that the reduced\neffective dimensionality identified through global sensitivity analysis\nexplains the performance advantages of one approach over another. Specifically,\nthe increased smoothness provided by Chebyshev or conditional pathwise methods\nenhances the convergence rate of randomized Quasi Monte Carlo integration,\nleading to the significant increase of accuracy and reduced computational\ncosts.", "published": "2025-04-15 19:51:07", "link": "http://arxiv.org/abs/2504.11576v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "FEAT: Free energy Estimators with Adaptive Transport", "abstract": "We present Free energy Estimators with Adaptive Transport (FEAT), a novel\nframework for free energy estimation -- a critical challenge across scientific\ndomains. FEAT leverages learned transports implemented via stochastic\ninterpolants and provides consistent, minimum-variance estimators based on\nescorted Jarzynski equality and controlled Crooks theorem, alongside\nvariational upper and lower bounds on free energy differences. Unifying\nequilibrium and non-equilibrium methods under a single theoretical framework,\nFEAT establishes a principled foundation for neural free energy calculations.\nExperimental validation on toy examples, molecular simulations, and quantum\nfield theory demonstrates improvements over existing learning-based methods.", "published": "2025-04-15 15:16:18", "link": "http://arxiv.org/abs/2504.11516v1", "categories": ["stat.ML", "cs.LG", "physics.chem-ph", "physics.comp-ph"], "primary_category": "stat.ML"}
{"title": "Making Acoustic Side-Channel Attacks on Noisy Keyboards Viable with LLM-Assisted Spectrograms' \"Typo\" Correction", "abstract": "The large integration of microphones into devices increases the opportunities\nfor Acoustic Side-Channel Attacks (ASCAs), as these can be used to capture\nkeystrokes' audio signals that might reveal sensitive information. However, the\ncurrent State-Of-The-Art (SOTA) models for ASCAs, including Convolutional\nNeural Networks (CNNs) and hybrid models, such as CoAtNet, still exhibit\nlimited robustness under realistic noisy conditions. Solving this problem\nrequires either: (i) an increased model's capacity to infer contextual\ninformation from longer sequences, allowing the model to learn that an\ninitially noisily typed word is the same as a futurely collected non-noisy\nword, or (ii) an approach to fix misidentified information from the contexts,\nas one does not type random words, but the ones that best fit the conversation\ncontext. In this paper, we demonstrate that both strategies are viable and\ncomplementary solutions for making ASCAs practical. We observed that no\nexisting solution leverages advanced transformer architectures' power for these\ntasks and propose that: (i) Visual Transformers (VTs) are the candidate\nsolutions for capturing long-term contextual information and (ii)\ntransformer-powered Large Language Models (LLMs) are the candidate solutions to\nfix the ``typos'' (mispredictions) the model might make. Thus, we here present\nthe first-of-its-kind approach that integrates VTs and LLMs for ASCAs.\n  We first show that VTs achieve SOTA performance in classifying keystrokes\nwhen compared to the previous CNN benchmark. Second, we demonstrate that LLMs\ncan mitigate the impact of real-world noise. Evaluations on the natural\nsentences revealed that: (i) incorporating LLMs (e.g., GPT-4o) in our ASCA\npipeline boosts the performance of error-correction tasks; and (ii) the\ncomparable performance can be attained by a lightweight, fine-tuned smaller LLM\n(67 times smaller than GPT-4o), using...", "published": "2025-04-15 21:23:25", "link": "http://arxiv.org/abs/2504.11622v1", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "An Energy-efficient Ordered Transmission-based Sequential Estimation", "abstract": "Estimation problems in wireless sensor networks typically involve gathering\nand processing data from distributed sensors to infer the state of an\nenvironment at the fusion center. However, not all measurements contribute\nsignificantly to improving estimation accuracy. The ordered transmission\nprotocol, a promising approach for enhancing energy efficiency in wireless\nnetworks, allows for the selection of measurements from different sensors to\nensure the desired estimation quality. In this work, we use the idea of ordered\ntransmission to reduce the number of transmissions required for sequential\nestimation within a network, thereby achieving energy-efficient estimation. We\nderive a new stopping rule that minimizes the number of transmissions while\nmaintaining estimation accuracy similar to general sequential estimation with\nunordered transmissions. Moreover, we derive the expected number of\ntransmissions required for both general sequential estimation with unordered\ntransmissions and proposed sequential estimation with ordered transmissions and\nmake a comparison between the two systems. Simulation results indicate that our\nproposed scheme can efficiently reduce transmissions while still ensuring the\nquality of estimation.", "published": "2025-04-15 21:54:15", "link": "http://arxiv.org/abs/2504.11638v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Accelerated Recovery with RIS: Designing Wireless Resilience in Mission-Critical Environments", "abstract": "As 6G and beyond redefine connectivity, wireless networks become the\nfoundation of critical operations, making resilience more essential than ever.\nWith this shift, wireless systems cannot only take on vital services previously\nhandled by wired infrastructures but also enable novel innovative applications\nthat would not be possible with wired systems. As a result, there is a pressing\ndemand for strategies that can adapt to dynamic channel conditions,\ninterference, and unforeseen disruptions, ensuring seamless and reliable\nperformance in an increasingly complex environment. Despite considerable\nresearch, existing resilience assessments lack comprehensive key performance\nindicators (KPIs), especially those quantifying its adaptability, which are\nvital for identifying a system's capacity to rapidly adapt and reallocate\nresources. In this work, we bridge this gap by proposing a novel framework that\nexplicitly quantifies the adaption performance by augmenting the gradient of\nthe system's rate function. To further enhance the network resilience, we\nintegrate Reconfigurable Intelligent Surfaces (RISs) into our framework due to\ntheir capability to dynamically reshape the propagation environment while\nproviding alternative channel paths. Numerical results show that gradient\naugmentation enhances resilience by improving adaptability under adverse\nconditions while proactively preparing for future disruptions.", "published": "2025-04-15 20:09:04", "link": "http://arxiv.org/abs/2504.11589v1", "categories": ["eess.SP", "cs.SY", "eess.SY"], "primary_category": "eess.SP"}
{"title": "Beam Misalignment in 3GPP mmWave NR", "abstract": "This paper presents an analytical framework for evaluating beam misalignment\nin 3GPP mmWave NR systems implementing analog beamforming. Our approach\ncaptures the interaction between user mobility, beam sweeping mechanisms, and\ndeployment configurations, focusing on long-term average performance metrics.\nSpecifically, we model the beam misalignment rates at both the base station\n(BS) and user equipment (UE) as Poisson processes and derive expressions for\nthe expected misalignment duration, misalignment fraction, and overall\nbeamforming gain. The framework accounts for practical constraints in NR such\nas Synchronization Signal Blocks (SSB) periodicity, TDD frame structures, and\nSSB overhead. Through numerical evaluation based on 3GPP mmWave parameters, we\nidentify key trade-offs between beam counts, user mobility, and SSB timing,\nproviding actionable design insights for robust and efficient beam management\nin future high-frequency networks.", "published": "2025-04-15 19:15:26", "link": "http://arxiv.org/abs/2504.11565v1", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI"}
{"title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs", "abstract": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement\nlearning (RL), excel in textual reasoning, they struggle in scenarios requiring\nstructured problem-solving, such as geometric reasoning, concise computation,\nor complex equation solving-areas where computational tools like code\ninterpreters (CI) demonstrate distinct advantages. To bridge this gap, we\npropose ReTool, which enhances long-form reasoning with tool-integrated\nlearning, including two key features: (1) dynamic interleaving of real-time\ncode execution within natural language reasoning processes, and (2) an\nautomated RL paradigm that allows policy rollouts with multi-turn real-time\ncode execution and teaches the model in learning when and how to invoke tools\nbased on outcome feedback. ReTool employs a systematic training framework,\nbeginning with synthetic cold-start data generation to produce code-augmented\nlong-form reasoning traces for fine-tuning base models. Subsequent RL training\nleverages task outcomes as rewards to iteratively refine the model's tool use\nstrategy, enabling autonomous discovery of optimal tool invocation patterns\nwithout human priors. Experiments on the challenging MATH Olympiad benchmark\nAIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with\n400 training steps, outperforming text-based RL baseline (40% accuracy, 1080\nsteps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%\naccuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further\nanalysis reveals emergent behaviors such as code self-correction, signaling an\n''aha moment'' in which the model autonomously masters adaptive tool use. These\nfindings highlight the promise of outcome-driven tool integration for advancing\ncomplex mathematical reasoning and offer new insights into hybrid\nneuro-symbolic systems.", "published": "2025-04-15 18:10:22", "link": "http://arxiv.org/abs/2504.11536v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis", "abstract": "Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation. In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://colmon46.github.io/i2e-bench-leaderboard/ .", "published": "2025-04-15 14:56:21", "link": "http://arxiv.org/abs/2504.11257v3", "categories": ["cs.HC", "cs.CL", "cs.CV"], "primary_category": "cs.HC"}
{"title": "Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs", "abstract": "Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages.", "published": "2025-04-15 08:46:39", "link": "http://arxiv.org/abs/2504.10982v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CSPLADE: Learned Sparse Retrieval with Causal Language Models", "abstract": "In recent years, dense retrieval has been the focus of information retrieval\n(IR) research. While effective, dense retrieval produces uninterpretable dense\nvectors, and suffers from the drawback of large index size. Learned sparse\nretrieval (LSR) has emerged as promising alternative, achieving competitive\nretrieval performance while also being able to leverage the classical inverted\nindex data structure for efficient retrieval. However, limited works have\nexplored scaling LSR beyond BERT scale. In this work, we identify two\nchallenges in training large language models (LLM) for LSR: (1) training\ninstability during the early stage of contrastive training; (2) suboptimal\nperformance due to pre-trained LLM's unidirectional attention. To address these\nchallenges, we propose two corresponding techniques: (1) a lightweight\nadaptation training phase to eliminate training instability; (2) two model\nvariants to enable bidirectional information. With these techniques, we are\nable to train LSR models with 8B scale LLM, and achieve competitive retrieval\nperformance with reduced index size. Furthermore, we are among the first to\nanalyze the performance-efficiency tradeoff of LLM-based LSR model through the\nlens of model quantization. Our findings provide insights into adapting LLMs\nfor efficient retrieval modeling.", "published": "2025-04-15 02:31:34", "link": "http://arxiv.org/abs/2504.10816v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites", "abstract": "We introduce REAL, a benchmark and framework for multi-turn agent evaluations\non deterministic simulations of real-world websites. REAL comprises\nhigh-fidelity, deterministic replicas of 11 widely-used websites across domains\nsuch as e-commerce, travel, communication, and professional networking. We also\nrelease a benchmark consisting of 112 practical tasks that mirror everyday\ncomplex user interactions requiring both accurate information retrieval and\nstate-changing actions. All interactions occur within this fully controlled\nsetting, eliminating safety risks and enabling robust, reproducible evaluation\nof agent capability and reliability. Our novel evaluation framework combines\nprogrammatic checks of website state for action-based tasks with rubric-guided\nLLM-based judgments for information retrieval. The framework supports both\nopen-source and proprietary agent systems through a flexible evaluation harness\nthat accommodates black-box commands within browser environments, allowing\nresearch labs to test agentic systems without modification. Our empirical\nresults show that frontier language models achieve at most a 41% success rate\non REAL, highlighting critical gaps in autonomous web navigation and task\ncompletion capabilities. Our framework supports easy integration of new tasks,\nreproducible evaluation, and scalable post-training data generation, marking a\nsignificant step forward in evaluating and advancing agent capabilities.", "published": "2025-04-15 18:22:55", "link": "http://arxiv.org/abs/2504.11543v2", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "PATFinger: Prompt-Adapted Transferable Fingerprinting against Unauthorized Multimodal Dataset Usage", "abstract": "The multimodal datasets can be leveraged to pre-train large-scale\nvision-language models by providing cross-modal semantics. Current endeavors\nfor determining the usage of datasets mainly focus on single-modal dataset\nownership verification through intrusive methods and non-intrusive techniques,\nwhile cross-modal approaches remain under-explored. Intrusive methods can adapt\nto multimodal datasets but degrade model accuracy, while non-intrusive methods\nrely on label-driven decision boundaries that fail to guarantee stable\nbehaviors for verification. To address these issues, we propose a novel\nprompt-adapted transferable fingerprinting scheme from a training-free\nperspective, called PATFinger, which incorporates the global optimal\nperturbation (GOP) and the adaptive prompts to capture dataset-specific\ndistribution characteristics. Our scheme utilizes inherent dataset attributes\nas fingerprints instead of compelling the model to learn triggers. The GOP is\nderived from the sample distribution to maximize embedding drifts between\ndifferent modalities. Subsequently, our PATFinger re-aligns the adaptive prompt\nwith GOP samples to capture the cross-modal interactions on the carefully\ncrafted surrogate model. This allows the dataset owner to check the usage of\ndatasets by observing specific prediction behaviors linked to the PATFinger\nduring retrieval queries. Extensive experiments demonstrate the effectiveness\nof our scheme against unauthorized multimodal dataset usage on various\ncross-modal retrieval architectures by 30% over state-of-the-art baselines.", "published": "2025-04-15 09:53:02", "link": "http://arxiv.org/abs/2504.11509v2", "categories": ["cs.IR", "cs.CV"], "primary_category": "cs.IR"}
{"title": "Computing the Tropical Abel--Jacobi Transform and Tropical Distances for Metric Graphs", "abstract": "Metric graphs are important models for capturing the structure of complex\ndata across various domains. While much effort has been devoted to extracting\ngeometric and topological features from graph data, computational aspects of\nmetric graphs as abstract tropical curves remains unexplored. In this paper, we\npresent the first computational and machine learning-driven study of metric\ngraphs from the perspective of tropical algebraic geometry. Specifically, we\nstudy the tropical Abel--Jacobi transform, a vectorization of points on a\nmetric graph via the tropical Abel--Jacobi map into its associated flat torus,\nthe tropical Jacobian. We develop algorithms to compute this transform and\ninvestigate how the resulting embeddings depend on different combinatorial\nmodels of the same metric graph.\n  Once embedded, we compute pairwise distances between points in the tropical\nJacobian under two natural metrics: the tropical polarization distance and the\nFoster--Zhang distance. Computing these distances are generally NP-hard as they\nturn out to be linked to classical lattice problems in computational\ncomplexity, however, we identify a class of metric graphs where fast and\nexplicit computations are feasible. For the general case, we propose practical\nalgorithms for both exact and approximate distance matrix computations using\nlattice basis reduction and mixed-integer programming solvers. Our work lays\nthe groundwork for future applications of tropical geometry and the tropical\nAbel--Jacobi transform in machine learning and data analysis.", "published": "2025-04-15 21:15:32", "link": "http://arxiv.org/abs/2504.11619v2", "categories": ["math.AG", "cs.NA", "math.MG", "math.NA"], "primary_category": "math.AG"}
{"title": "Reimagining Urban Science: Scaling Causal Inference with Large Language Models", "abstract": "Urban causal research is essential for understanding the complex dynamics of\ncities and informing evidence-based policies. However, it is challenged by the\ninefficiency and bias of hypothesis generation, barriers to multimodal data\ncomplexity, and the methodological fragility of causal experimentation. Recent\nadvances in large language models (LLMs) present an opportunity to rethink how\nurban causal analysis is conducted. This Perspective examines current urban\ncausal research by analyzing taxonomies that categorize research topics, data\nsources, and methodological approaches to identify structural gaps. We then\nintroduce an LLM-driven conceptual framework, AutoUrbanCI, composed of four\ndistinct modular agents responsible for hypothesis generation, data\nengineering, experiment design and execution, and results interpretation with\npolicy recommendations. We propose evaluation criteria for rigor and\ntransparency and reflect on implications for human-AI collaboration, equity,\nand accountability. We call for a new research agenda that embraces\nAI-augmented workflows not as replacements for human expertise but as tools to\nbroaden participation, improve reproducibility, and unlock more inclusive forms\nof urban causal reasoning.", "published": "2025-04-15 16:58:11", "link": "http://arxiv.org/abs/2504.12345v1", "categories": ["cs.CL", "cs.CY", "cs.MA"], "primary_category": "cs.CL"}
{"title": "TransST: Transfer Learning Embedded Spatial Factor Modeling of Spatial Transcriptomics Data", "abstract": "Background: Spatial transcriptomics have emerged as a powerful tool in\nbiomedical research because of its ability to capture both the spatial contexts\nand abundance of the complete RNA transcript profile in organs of interest.\nHowever, limitations of the technology such as the relatively low resolution\nand comparatively insufficient sequencing depth make it difficult to reliably\nextract real biological signals from these data. To alleviate this challenge,\nwe propose a novel transfer learning framework, referred to as TransST, to\nadaptively leverage the cell-labeled information from external sources in\ninferring cell-level heterogeneity of a target spatial transcriptomics data.\n  Results: Applications in several real studies as well as a number of\nsimulation settings show that our approach significantly improves existing\ntechniques. For example, in the breast cancer study, TransST successfully\nidentifies five biologically meaningful cell clusters, including the two\nsubgroups of cancer in situ and invasive cancer; in addition, only TransST is\nable to separate the adipose tissues from the connective issues among all the\nstudied methods.\n  Conclusions: In summary, the proposed method TransST is both effective and\nrobust in identifying cell subclusters and detecting corresponding driving\nbiomarkers in spatial transcriptomics data.", "published": "2025-04-15 22:03:38", "link": "http://arxiv.org/abs/2504.12353v1", "categories": ["q-bio.GN", "cs.LG", "stat.AP", "stat.ML"], "primary_category": "q-bio.GN"}
{"title": "GOAT-TTS: LLM-based Text-To-Speech Generation Optimized via A Dual-Branch Architecture", "abstract": "While large language models (LLMs) have revolutionized text-to-speech (TTS)\nsynthesis through discrete tokenization paradigms, current architectures\nexhibit fundamental tensions between three critical dimensions: 1) irreversible\nloss of acoustic characteristics caused by quantization of speech prompts; 2)\nstringent dependence on precisely aligned prompt speech-text pairs that limit\nreal-world deployment; and 3) catastrophic forgetting of the LLM's native text\ncomprehension during optimization for speech token generation. To address these\nchallenges, we propose an LLM-based text-to-speech Generation approach\nOptimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework\nintroduces two key innovations: (1) The modality-alignment branch combines a\nspeech encoder and projector to capture continuous acoustic embeddings,\nenabling bidirectional correlation between paralinguistic features (language,\ntimbre, emotion) and semantic text representations without transcript\ndependency; (2) The speech-generation branch employs modular fine-tuning on\ntop-k layers of an LLM for speech token prediction while freezing the bottom-k\nlayers to preserve foundational linguistic knowledge. Moreover, multi-token\nprediction is introduced to support real-time streaming TTS synthesis.\nExperimental results demonstrate that our GOAT-TTS achieves performance\ncomparable to state-of-the-art TTS models while validating the efficacy of\nsynthesized dialect speech data.", "published": "2025-04-15 01:44:56", "link": "http://arxiv.org/abs/2504.12339v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
