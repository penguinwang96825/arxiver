{"title": "Deep Learning applied to NLP", "abstract": "Convolutional Neural Network (CNNs) are typically associated with Computer\nVision. CNNs are responsible for major breakthroughs in Image Classification\nand are the core of most Computer Vision systems today. More recently CNNs have\nbeen applied to problems in Natural Language Processing and gotten some\ninteresting results. In this paper, we will try to explain the basics of CNNs,\nits different variations and how they have been applied to NLP.", "published": "2017-03-09 01:04:07", "link": "http://arxiv.org/abs/1703.03091v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Sockpuppets in Deceptive Opinion Spam", "abstract": "This paper explores the problem of sockpuppet detection in deceptive opinion\nspam using authorship attribution and verification approaches. Two methods are\nexplored. The first is a feature subsampling scheme that uses the KL-Divergence\non stylistic language models of an author to find discriminative features. The\nsecond is a transduction scheme, spy induction that leverages the diversity of\nauthors in the unlabeled test set by sending a set of spies (positive samples)\nfrom the training set to retrieve hidden samples in the unlabeled test set\nusing nearest and farthest neighbors. Experiments using ground truth sockpuppet\ndata show the effectiveness of the proposed schemes.", "published": "2017-03-09 06:20:49", "link": "http://arxiv.org/abs/1703.03149v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Turkish PoS Tagging by Reducing Sparsity with Morpheme Tags in Small\n  Datasets", "abstract": "Sparsity is one of the major problems in natural language processing. The\nproblem becomes even more severe in agglutinating languages that are highly\nprone to be inflected. We deal with sparsity in Turkish by adopting\nmorphological features for part-of-speech tagging. We learn inflectional and\nderivational morpheme tags in Turkish by using conditional random fields (CRF)\nand we employ the morpheme tags in part-of-speech (PoS) tagging by using hidden\nMarkov models (HMMs) to mitigate sparsity. Results show that using morpheme\ntags in PoS tagging helps alleviate the sparsity in emission probabilities. Our\nmodel outperforms other hidden Markov model based PoS tagging models for small\ntraining datasets in Turkish. We obtain an accuracy of 94.1% in morpheme\ntagging and 89.2% in PoS tagging on a 5K training dataset.", "published": "2017-03-09 09:46:56", "link": "http://arxiv.org/abs/1703.03200v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Information Extraction in Illicit Domains", "abstract": "Extracting useful entities and attribute values from illicit domains such as\nhuman trafficking is a challenging problem with the potential for widespread\nsocial impact. Such domains employ atypical language models, have `long tails'\nand suffer from the problem of concept drift. In this paper, we propose a\nlightweight, feature-agnostic Information Extraction (IE) paradigm specifically\ndesigned for such domains. Our approach uses raw, unlabeled text from an\ninitial corpus, and a few (12-120) seed annotations per domain-specific\nattribute, to learn robust IE models for unobserved pages and websites.\nEmpirically, we demonstrate that our approach can outperform feature-centric\nConditional Random Field baselines by over 18\\% F-Measure on five annotated\nsets of real-world human trafficking datasets in both low-supervision and\nhigh-supervision settings. We also show that our approach is demonstrably\nrobust to concept drift, and can be efficiently bootstrapped even in a serial\ncomputing environment.", "published": "2017-03-09 01:28:00", "link": "http://arxiv.org/abs/1703.03097v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Loyalty in Online Communities", "abstract": "Loyalty is an essential component of multi-community engagement. When users\nhave the choice to engage with a variety of different communities, they often\nbecome loyal to just one, focusing on that community at the expense of others.\nHowever, it is unclear how loyalty is manifested in user behavior, or whether\nloyalty is encouraged by certain community characteristics.\n  In this paper we operationalize loyalty as a user-community relation: users\nloyal to a community consistently prefer it over all others; loyal communities\nretain their loyal users over time. By exploring this relation using a large\ndataset of discussion communities from Reddit, we reveal that loyalty is\nmanifested in remarkably consistent behaviors across a wide spectrum of\ncommunities. Loyal users employ language that signals collective identity and\nengage with more esoteric, less popular content, indicating they may play a\ncurational role in surfacing new material. Loyal communities have denser\nuser-user interaction networks and lower rates of triadic closure, suggesting\nthat community-level loyalty is associated with more cohesive interactions and\nless fragmentation into subgroups. We exploit these general patterns to predict\nfuture rates of loyalty. Our results show that a user's propensity to become\nloyal is apparent from their first interactions with a community, suggesting\nthat some users are intrinsically loyal from the very beginning.", "published": "2017-03-09 18:37:50", "link": "http://arxiv.org/abs/1703.03386v3", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "What can you do with a rock? Affordance extraction via word embeddings", "abstract": "Autonomous agents must often detect affordances: the set of behaviors enabled\nby a situation. Affordance detection is particularly helpful in domains with\nlarge action spaces, allowing the agent to prune its search space by avoiding\nfutile behaviors. This paper presents a method for affordance extraction via\nword embeddings trained on a Wikipedia corpus. The resulting word vectors are\ntreated as a common knowledge database which can be queried using linear\nalgebra. We apply this method to a reinforcement learning agent in a text-only\nenvironment and show that affordance-based action selection improves\nperformance most of the time. Our method increases the computational complexity\nof each learning step but significantly reduces the total number of steps\nneeded. In addition, the agent's action selections begin to resemble those a\nhuman would choose.", "published": "2017-03-09 19:16:14", "link": "http://arxiv.org/abs/1703.03429v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "The cognitive roots of regularization in language", "abstract": "Regularization occurs when the output a learner produces is less variable\nthan the linguistic data they observed. In an artificial language learning\nexperiment, we show that there exist at least two independent sources of\nregularization bias in cognition: a domain-general source based on cognitive\nload and a domain-specific source triggered by linguistic stimuli. Both of\nthese factors modulate how frequency information is encoded and produced, but\nonly the production-side modulations result in regularization (i.e. cause\nlearners to eliminate variation from the observed input). We formalize the\ndefinition of regularization as the reduction of entropy and find that entropy\nmeasures are better at identifying regularization behavior than frequency-based\nanalyses. Using our experimental data and a model of cultural transmission, we\ngenerate predictions for the amount of regularity that would develop in each\nexperimental condition if the artificial language were transmitted over several\ngenerations of learners. Here we find that the effect of cognitive constraints\ncan become more complex when put into the context of cultural evolution:\nalthough learning biases certainly carry information about the course of\nlanguage evolution, we should not expect a one-to-one correspondence between\nthe micro-level processes that regularize linguistic datasets and the\nmacro-level evolution of linguistic regularity.", "published": "2017-03-09 19:50:00", "link": "http://arxiv.org/abs/1703.03442v2", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "A Structured Self-attentive Sentence Embedding", "abstract": "This paper proposes a new model for extracting an interpretable sentence\nembedding by introducing self-attention. Instead of using a vector, we use a\n2-D matrix to represent the embedding, with each row of the matrix attending on\na different part of the sentence. We also propose a self-attention mechanism\nand a special regularization term for the model. As a side effect, the\nembedding comes with an easy way of visualizing what specific parts of the\nsentence are encoded into the embedding. We evaluate our model on 3 different\ntasks: author profiling, sentiment classification, and textual entailment.\nResults show that our model yields a significant performance gain compared to\nother sentence embedding methods in all of the 3 tasks.", "published": "2017-03-09 04:42:30", "link": "http://arxiv.org/abs/1703.03130v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
