{"title": "Unsupervised Stemming based Language Model for Telugu Broadcast News\n  Transcription", "abstract": "In Indian Languages , native speakers are able to understand new words formed\nby either combining or modifying root words with tense and / or gender. Due to\ndata insufficiency, Automatic Speech Recognition system (ASR) may not\naccommodate all the words in the language model irrespective of the size of the\ntext corpus. It also becomes computationally challenging if the volume of the\ndata increases exponentially due to morphological changes to the root word. In\nthis paper a new unsupervised method is proposed for a Indian language: Telugu,\nbased on the unsupervised method for Hindi, to generate the Out of Vocabulary\n(OOV) words in the language model. By using techniques like smoothing and\ninterpolation of pre-processed data with supervised and unsupervised stemming,\ndifferent issues in language model for Indian language: Telugu has been\naddressed. We observe that the smoothing techniques Witten-Bell and Kneser-Ney\nperform well when compared to other techniques on pre-processed data from\nsupervised learning. The ASRs accuracy is improved by 0.76% and 0.94% with\nsupervised and unsupervised stemming respectively.", "published": "2019-08-10 11:39:17", "link": "http://arxiv.org/abs/1908.03734v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multi-modality Latent Interaction Network for Visual Question Answering", "abstract": "Exploiting relationships between visual regions and question words have\nachieved great success in learning multi-modality features for Visual Question\nAnswering (VQA). However, we argue that existing methods mostly model relations\nbetween individual visual regions and words, which are not enough to correctly\nanswer the question. From humans' perspective, answering a visual question\nrequires understanding the summarizations of visual and language information.\nIn this paper, we proposed the Multi-modality Latent Interaction module (MLI)\nto tackle this problem. The proposed module learns the cross-modality\nrelationships between latent visual and language summarizations, which\nsummarize visual regions and question into a small number of latent\nrepresentations to avoid modeling uninformative individual region-word\nrelations. The cross-modality information between the latent summarizations are\npropagated to fuse valuable information from both modalities and are used to\nupdate the visual and word features. Such MLI modules can be stacked for\nseveral stages to model complex and latent relations between the two modalities\nand achieves highly competitive performance on public VQA benchmarks, VQA v2.0\nand TDIUC . In addition, we show that the performance of our methods could be\nsignificantly improved by combining with pre-trained language model BERT.", "published": "2019-08-10 05:57:01", "link": "http://arxiv.org/abs/1908.04289v1", "categories": ["cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
