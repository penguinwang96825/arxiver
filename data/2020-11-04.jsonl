{"title": "Probing Multilingual BERT for Genetic and Typological Signals", "abstract": "We probe the layers in multilingual BERT (mBERT) for phylogenetic and\ngeographic language signals across 100 languages and compute language distances\nbased on the mBERT representations. We 1) employ the language distances to\ninfer and evaluate language trees, finding that they are close to the reference\nfamily tree in terms of quartet tree distance, 2) perform distance matrix\nregression analysis, finding that the language distances can be best explained\nby phylogenetic and worst by structural factors and 3) present a novel measure\nfor measuring diachronic meaning stability (based on cross-lingual\nrepresentation variability) which correlates significantly with published\nranked lists based on linguistic approaches. Our results contribute to the\nnascent field of typological interpretability of cross-lingual text\nrepresentations.", "published": "2020-11-04 00:03:04", "link": "http://arxiv.org/abs/2011.02070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chinese Grammatical Correction Using BERT-based Pre-trained Model", "abstract": "In recent years, pre-trained models have been extensively studied, and\nseveral downstream tasks have benefited from their utilization. In this study,\nwe verify the effectiveness of two methods that incorporate a BERT-based\npre-trained model developed by Cui et al. (2020) into an encoder-decoder model\non Chinese grammatical error correction tasks. We also analyze the error type\nand conclude that sentence-level errors are yet to be addressed.", "published": "2020-11-04 01:23:30", "link": "http://arxiv.org/abs/2011.02093v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PheMT: A Phenomenon-wise Dataset for Machine Translation Robustness on\n  User-Generated Contents", "abstract": "Neural Machine Translation (NMT) has shown drastic improvement in its quality\nwhen translating clean input, such as text from the news domain. However,\nexisting studies suggest that NMT still struggles with certain kinds of input\nwith considerable noise, such as User-Generated Contents (UGC) on the Internet.\nTo make better use of NMT for cross-cultural communication, one of the most\npromising directions is to develop a model that correctly handles these\nexpressions. Though its importance has been recognized, it is still not clear\nas to what creates the great gap in performance between the translation of\nclean input and that of UGC. To answer the question, we present a new dataset,\nPheMT, for evaluating the robustness of MT systems against specific linguistic\nphenomena in Japanese-English translation. Our experiments with the created\ndataset revealed that not only our in-house models but even widely used\noff-the-shelf systems are greatly disturbed by the presence of certain\nphenomena.", "published": "2020-11-04 04:44:47", "link": "http://arxiv.org/abs/2011.02121v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural text normalization leveraging similarities of strings and sounds", "abstract": "We propose neural models that can normalize text by considering the\nsimilarities of word strings and sounds. We experimentally compared a model\nthat considers the similarities of both word strings and sounds, a model that\nconsiders only the similarity of word strings or of sounds, and a model without\nthe similarities as a baseline. Results showed that leveraging the word string\nsimilarity succeeded in dealing with misspellings and abbreviations, and taking\ninto account the sound similarity succeeded in dealing with phonetic\nsubstitutions and emphasized characters. So that the proposed models achieved\nhigher F$_1$ scores than the baseline.", "published": "2020-11-04 08:24:05", "link": "http://arxiv.org/abs/2011.02173v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Chemical-Protein Interactions via Calibrated Deep Neural\n  Network and Self-training", "abstract": "The extraction of interactions between chemicals and proteins from several\nbiomedical articles is important in many fields of biomedical research such as\ndrug development and prediction of drug side effects. Several natural language\nprocessing methods, including deep neural network (DNN) models, have been\napplied to address this problem. However, these methods were trained with\nhard-labeled data, which tend to become over-confident, leading to degradation\nof the model reliability. To estimate the data uncertainty and improve the\nreliability, \"calibration\" techniques have been applied to deep learning\nmodels. In this study, to extract chemical--protein interactions, we propose a\nDNN-based approach incorporating uncertainty information and calibration\ntechniques. Our model first encodes the input sequence using a pre-trained\nlanguage-understanding model, following which it is trained using two\ncalibration methods: mixup training and addition of a confidence penalty loss.\nFinally, the model is re-trained with augmented data that are extracted using\nthe estimated uncertainties. Our approach has achieved state-of-the-art\nperformance with regard to the Biocreative VI ChemProt task, while preserving\nhigher calibration abilities than those of previous approaches. Furthermore,\nour approach also presents the possibilities of using uncertainty estimation\nfor performance improvement.", "published": "2020-11-04 10:14:31", "link": "http://arxiv.org/abs/2011.02207v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A BERT-based Dual Embedding Model for Chinese Idiom Prediction", "abstract": "Chinese idioms are special fixed phrases usually derived from ancient\nstories, whose meanings are oftentimes highly idiomatic and non-compositional.\nThe Chinese idiom prediction task is to select the correct idiom from a set of\ncandidate idioms given a context with a blank. We propose a BERT-based dual\nembedding model to encode the contextual words as well as to learn dual\nembeddings of the idioms. Specifically, we first match the embedding of each\ncandidate idiom with the hidden representation corresponding to the blank in\nthe context. We then match the embedding of each candidate idiom with the\nhidden representations of all the tokens in the context thorough context\npooling. We further propose to use two separate idiom embeddings for the two\nkinds of matching. Experiments on a recently released Chinese idiom cloze test\ndataset show that our proposed method performs better than the existing state\nof the art. Ablation experiments also show that both context pooling and dual\nembedding contribute to the improvement of performance.", "published": "2020-11-04 16:12:39", "link": "http://arxiv.org/abs/2011.02378v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MTLB-STRUCT @PARSEME 2020: Capturing Unseen Multiword Expressions Using\n  Multi-task Learning and Pre-trained Masked Language Models", "abstract": "This paper describes a semi-supervised system that jointly learns verbal\nmultiword expressions (VMWEs) and dependency parse trees as an auxiliary task.\nThe model benefits from pre-trained multilingual BERT. BERT hidden layers are\nshared among the two tasks and we introduce an additional linear layer to\nretrieve VMWE tags. The dependency parse tree prediction is modelled by a\nlinear layer and a bilinear one plus a tree CRF on top of BERT. The system has\nparticipated in the open track of the PARSEME shared task 2020 and ranked first\nin terms of F1-score in identifying unseen VMWEs as well as VMWEs in general,\naveraged across all 14 languages.", "published": "2020-11-04 21:16:48", "link": "http://arxiv.org/abs/2011.02541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MK-SQuIT: Synthesizing Questions using Iterative Template-filling", "abstract": "The aim of this work is to create a framework for synthetically generating\nquestion/query pairs with as little human input as possible. These datasets can\nbe used to train machine translation systems to convert natural language\nquestions into queries, a useful tool that could allow for more natural access\nto database information. Existing methods of dataset generation require human\ninput that scales linearly with the size of the dataset, resulting in small\ndatasets. Aside from a short initial configuration task, no human input is\nrequired during the query generation process of our system. We leverage\nWikiData, a knowledge base of RDF triples, as a source for generating the main\ncontent of questions and queries. Using multiple layers of question templating\nwe are able to sidestep some of the most challenging parts of query generation\nthat have been handled by humans in previous methods; humans never have to\nmodify, aggregate, inspect, annotate, or generate any questions or queries at\nany step in the process. Our system is easily configurable to multiple domains\nand can be modified to generate queries in natural languages other than\nEnglish. We also present an example dataset of 110,000 question/query pairs\nacross four WikiData domains. We then present a baseline model that we train\nusing the dataset which shows promise in a commercial QA setting.", "published": "2020-11-04 22:33:05", "link": "http://arxiv.org/abs/2011.02566v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paralinguistic Privacy Protection at the Edge", "abstract": "Voice user interfaces and digital assistants are rapidly entering our lives\nand becoming singular touch points spanning our devices. These always-on\nservices capture and transmit our audio data to powerful cloud services for\nfurther processing and subsequent actions. Our voices and raw audio signals\ncollected through these devices contain a host of sensitive paralinguistic\ninformation that is transmitted to service providers regardless of deliberate\nor false triggers. As our emotional patterns and sensitive attributes like our\nidentity, gender, well-being, are easily inferred using deep acoustic models,\nwe encounter a new generation of privacy risks by using these services. One\napproach to mitigate the risk of paralinguistic-based privacy breaches is to\nexploit a combination of cloud-based processing with privacy-preserving,\non-device paralinguistic information learning and filtering before transmitting\nvoice data. In this paper we introduce EDGY, a configurable, lightweight,\ndisentangled representation learning framework that transforms and filters\nhigh-dimensional voice data to identify and contain sensitive attributes at the\nedge prior to offloading to the cloud. We evaluate EDGY's on-device performance\nand explore optimization techniques, including model quantization and knowledge\ndistillation, to enable private, accurate and efficient representation learning\non resource-constrained devices. Our results show that EDGY runs in tens of\nmilliseconds with 0.2% relative improvement in \"zero-shot\" ABX score or minimal\nperformance penalties of approximately 5.95% word error rate (WER) in learning\nlinguistic representations from raw voice signals, using a CPU and a\nsingle-core ARM processor without specialized hardware.", "published": "2020-11-04 14:11:35", "link": "http://arxiv.org/abs/2011.02930v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Answer Identification in Collaborative Organizational Group Chat", "abstract": "We present a simple unsupervised approach for answer identification in\norganizational group chat. In recent years, organizational group chat is on the\nrise enabling asynchronous text-based collaboration between co-workers in\ndifferent locations and time zones. Finding answers to questions is often\ncritical for work efficiency. However, group chat is characterized by\nintertwined conversations and 'always on' availability, making it hard for\nusers to pinpoint answers to questions they care about in real-time or search\nfor answers in retrospective. In addition, structural and lexical\ncharacteristics differ between chat groups, making it hard to find a 'one model\nfits all' approach. Our Kernel Density Estimation (KDE) based clustering\napproach termed Ans-Chat implicitly learns discussion patterns as a means for\nanswer identification, thus eliminating the need to channel-specific tagging.\nEmpirical evaluation shows that this solution outperforms other approached.", "published": "2020-11-04 09:42:54", "link": "http://arxiv.org/abs/2011.08074v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Augmentation for End-to-end Code-switching Speech Recognition", "abstract": "Training a code-switching end-to-end automatic speech recognition (ASR) model\nnormally requires a large amount of data, while code-switching data is often\nlimited. In this paper, three novel approaches are proposed for code-switching\ndata augmentation. Specifically, they are audio splicing with the existing\ncode-switching data, and TTS with new code-switching texts generated by word\ntranslation or word insertion. Our experiments on 200 hours Mandarin-English\ncode-switching dataset show that all the three proposed approaches yield\nsignificant improvements on code-switching ASR individually. Moreover, all the\nproposed approaches can be combined with recent popular SpecAugment, and an\naddition gain can be obtained. WER is significantly reduced by relative 24.0%\ncompared to the system without any data augmentation, and still relative 13.0%\ngain compared to the system with only SpecAugment", "published": "2020-11-04 07:12:44", "link": "http://arxiv.org/abs/2011.02160v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "An Improved Attention for Visual Question Answering", "abstract": "We consider the problem of Visual Question Answering (VQA). Given an image\nand a free-form, open-ended, question, expressed in natural language, the goal\nof VQA system is to provide accurate answer to this question with respect to\nthe image. The task is challenging because it requires simultaneous and\nintricate understanding of both visual and textual information. Attention,\nwhich captures intra- and inter-modal dependencies, has emerged as perhaps the\nmost widely used mechanism for addressing these challenges. In this paper, we\npropose an improved attention-based architecture to solve VQA. We incorporate\nan Attention on Attention (AoA) module within encoder-decoder framework, which\nis able to determine the relation between attention results and queries.\nAttention module generates weighted average for each query. On the other hand,\nAoA module first generates an information vector and an attention gate using\nattention results and current context; and then adds another attention to\ngenerate final attended information by multiplying the two. We also propose\nmultimodal fusion module to combine both visual and textual information. The\ngoal of this fusion module is to dynamically decide how much information should\nbe considered from each modality. Extensive experiments on VQA-v2 benchmark\ndataset show that our method achieves the state-of-the-art performance.", "published": "2020-11-04 07:34:54", "link": "http://arxiv.org/abs/2011.02164v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Diversity Aware Relevance Learning for Argument Search", "abstract": "In this work, we focus on the problem of retrieving relevant arguments for a\nquery claim covering diverse aspects. State-of-the-art methods rely on explicit\nmappings between claims and premises, and thus are unable to utilize large\navailable collections of premises without laborious and costly manual\nannotation. Their diversity approach relies on removing duplicates via\nclustering which does not directly ensure that the selected premises cover all\naspects. This work introduces a new multi-step approach for the argument\nretrieval problem. Rather than relying on ground-truth assignments, our\napproach employs a machine learning model to capture semantic relationships\nbetween arguments. Beyond that, it aims to cover diverse facets of the query,\ninstead of trying to identify duplicates explicitly. Our empirical evaluation\ndemonstrates that our approach leads to a significant improvement in the\nargument retrieval task even though it requires less data.", "published": "2020-11-04 08:37:44", "link": "http://arxiv.org/abs/2011.02177v4", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Hybrid Supervised Reinforced Model for Dialogue Systems", "abstract": "This paper presents a recurrent hybrid model and training procedure for\ntask-oriented dialogue systems based on Deep Recurrent Q-Networks (DRQN). The\nmodel copes with both tasks required for Dialogue Management: State Tracking\nand Decision Making. It is based on modeling Human-Machine interaction into a\nlatent representation embedding an interaction context to guide the discussion.\nThe model achieves greater performance, learning speed and robustness than a\nnon-recurrent baseline. Moreover, results allow interpreting and validating the\npolicy evolution and the latent representations information-wise.", "published": "2020-11-04 12:03:12", "link": "http://arxiv.org/abs/2011.02243v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Optimizing Transformer for Low-Resource Neural Machine Translation", "abstract": "Language pairs with limited amounts of parallel data, also known as\nlow-resource languages, remain a challenge for neural machine translation.\nWhile the Transformer model has achieved significant improvements for many\nlanguage pairs and has become the de facto mainstream architecture, its\ncapability under low-resource conditions has not been fully investigated yet.\nOur experiments on different subsets of the IWSLT14 training data show that the\neffectiveness of Transformer under low-resource conditions is highly dependent\non the hyper-parameter settings. Our experiments show that using an optimized\nTransformer for low-resource conditions improves the translation quality up to\n7.3 BLEU points compared to using the Transformer default settings.", "published": "2020-11-04 13:12:29", "link": "http://arxiv.org/abs/2011.02266v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "PCP Theorems, SETH and More: Towards Proving Sub-linear Time\n  Inapproximability", "abstract": "In this paper we propose the PCP-like theorem for sub-linear time\ninapproximability. Abboud et al. have devised the distributed PCP framework for\nsub-quadratic time inapproximability. We show that the distributed PCP theorem\ncan be generalized for proving arbitrary polynomial time inapproximability, but\nfails in the linear case. We prove the sub-linear PCP theorem by adapting from\nan MA-protocol for the Set Containment problem, and show how to use the theorem\nto prove both existing and new inapproximability results, exhibiting the power\nof the sub-linear PCP theorem. Considering the emerging research works on\nsub-linear time algorithms, the sub-linear PCP theorem is important in guiding\nthe research in sub-linear time approximation algorithms.", "published": "2020-11-04 14:39:41", "link": "http://arxiv.org/abs/2011.02320v4", "categories": ["cs.CC", "cs.CL"], "primary_category": "cs.CC"}
{"title": "Indic-Transformers: An Analysis of Transformer Language Models for\n  Indian Languages", "abstract": "Language models based on the Transformer architecture have achieved\nstate-of-the-art performance on a wide range of NLP tasks such as text\nclassification, question-answering, and token classification. However, this\nperformance is usually tested and reported on high-resource languages, like\nEnglish, French, Spanish, and German. Indian languages, on the other hand, are\nunderrepresented in such benchmarks. Despite some Indian languages being\nincluded in training multilingual Transformer models, they have not been the\nprimary focus of such work. In order to evaluate the performance on Indian\nlanguages specifically, we analyze these language models through extensive\nexperiments on multiple downstream tasks in Hindi, Bengali, and Telugu\nlanguage. Here, we compare the efficacy of fine-tuning model parameters of\npre-trained models against that of training a language model from scratch.\nMoreover, we empirically argue against the strict dependency between the\ndataset size and model performance, but rather encourage task-specific model\nand method selection. We achieve state-of-the-art performance on Hindi and\nBengali languages for text classification task. Finally, we present effective\nstrategies for handling the modeling of Indian languages and we release our\nmodel checkpoints for the community :\nhttps://huggingface.co/neuralspace-reverie.", "published": "2020-11-04 14:43:43", "link": "http://arxiv.org/abs/2011.02323v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigating Novel Verb Learning in BERT: Selectional Preference\n  Classes and Alternation-Based Syntactic Generalization", "abstract": "Previous studies investigating the syntactic abilities of deep learning\nmodels have not targeted the relationship between the strength of the\ngrammatical generalization and the amount of evidence to which the model is\nexposed during training. We address this issue by deploying a novel\nword-learning paradigm to test BERT's few-shot learning capabilities for two\naspects of English verbs: alternations and classes of selectional preferences.\nFor the former, we fine-tune BERT on a single frame in a verbal-alternation\npair and ask whether the model expects the novel verb to occur in its sister\nframe. For the latter, we fine-tune BERT on an incomplete selectional network\nof verbal objects and ask whether it expects unattested but plausible\nverb/object pairs. We find that BERT makes robust grammatical generalizations\nafter just one or two instances of a novel word in fine-tuning. For the verbal\nalternation tests, we find that the model displays behavior that is consistent\nwith a transitivity bias: verbs seen few times are expected to take direct\nobjects, but verbs seen with direct objects are not expected to occur\nintransitively.", "published": "2020-11-04 17:17:49", "link": "http://arxiv.org/abs/2011.02417v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Offline Reinforcement Learning from Human Feedback in Real-World\n  Sequence-to-Sequence Tasks", "abstract": "Large volumes of interaction logs can be collected from NLP systems that are\ndeployed in the real world. How can this wealth of information be leveraged?\nUsing such interaction logs in an offline reinforcement learning (RL) setting\nis a promising approach. However, due to the nature of NLP tasks and the\nconstraints of production systems, a series of challenges arise. We present a\nconcise overview of these challenges and discuss possible solutions.", "published": "2020-11-04 19:30:46", "link": "http://arxiv.org/abs/2011.02511v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Augmenting Images for ASR and TTS through Single-loop and Dual-loop\n  Multimodal Chain Framework", "abstract": "Previous research has proposed a machine speech chain to enable automatic\nspeech recognition (ASR) and text-to-speech synthesis (TTS) to assist each\nother in semi-supervised learning and to avoid the need for a large amount of\npaired speech and text data. However, that framework still requires a large\namount of unpaired (speech or text) data. A prototype multimodal machine chain\nwas then explored to further reduce the need for a large amount of unpaired\ndata, which could improve ASR or TTS even when no more speech or text data were\navailable. Unfortunately, this framework relied on the image retrieval (IR)\nmodel, and thus it was limited to handling only those images that were already\nknown during training. Furthermore, the performance of this framework was only\ninvestigated with single-speaker artificial speech data. In this study, we\nrevamp the multimodal machine chain framework with image generation (IG) and\ninvestigate the possibility of augmenting image data for ASR and TTS using\nsingle-loop and dual-loop architectures on multispeaker natural speech data.\nExperimental results revealed that both single-loop and dual-loop multimodal\nchain frameworks enabled ASR and TTS to improve their performance using an\nimage-only dataset.", "published": "2020-11-04 02:26:02", "link": "http://arxiv.org/abs/2011.02099v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Incremental Machine Speech Chain Towards Enabling Listening while\n  Speaking in Real-time", "abstract": "Inspired by a human speech chain mechanism, a machine speech chain framework\nbased on deep learning was recently proposed for the semi-supervised\ndevelopment of automatic speech recognition (ASR) and text-to-speech synthesis\nTTS) systems. However, the mechanism to listen while speaking can be done only\nafter receiving entire input sequences. Thus, there is a significant delay when\nencountering long utterances. By contrast, humans can listen to what hey speak\nin real-time, and if there is a delay in hearing, they won't be able to\ncontinue speaking. In this work, we propose an incremental machine speech chain\ntowards enabling machine to listen while speaking in real-time. Specifically,\nwe construct incremental ASR (ISR) and incremental TTS (ITTS) by letting both\nsystems improve together through a short-term loop. Our experimental results\nreveal that our proposed framework is able to reduce delays due to long\nutterances while keeping a comparable performance to the non-incremental basic\nmachine speech chain.", "published": "2020-11-04 04:59:38", "link": "http://arxiv.org/abs/2011.02126v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Sequence-to-Sequence Learning via Attention Transfer for Incremental\n  Speech Recognition", "abstract": "Attention-based sequence-to-sequence automatic speech recognition (ASR)\nrequires a significant delay to recognize long utterances because the output is\ngenerated after receiving entire input sequences. Although several studies\nrecently proposed sequence mechanisms for incremental speech recognition (ISR),\nusing different frameworks and learning algorithms is more complicated than the\nstandard ASR model. One main reason is because the model needs to decide the\nincremental steps and learn the transcription that aligns with the current\nshort speech segment. In this work, we investigate whether it is possible to\nemploy the original architecture of attention-based ASR for ISR tasks by\ntreating a full-utterance ASR as the teacher model and the ISR as the student\nmodel. We design an alternative student network that, instead of using a\nthinner or a shallower model, keeps the original architecture of the teacher\nmodel but with shorter sequences (few encoder and decoder states). Using\nattention transfer, the student network learns to mimic the same alignment\nbetween the current input short speech segments and the transcription. Our\nexperiments show that by delaying the starting time of recognition process with\nabout 1.7 sec, we can achieve comparable performance to one that needs to wait\nuntil the end.", "published": "2020-11-04 05:06:01", "link": "http://arxiv.org/abs/2011.02127v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Machine Speech Chain for Javanese, Sundanese, Balinese,\n  and Bataks Speech Recognition and Synthesis", "abstract": "Even though over seven hundred ethnic languages are spoken in Indonesia, the\navailable technology remains limited that could support communication within\nindigenous communities as well as with people outside the villages. As a\nresult, indigenous communities still face isolation due to cultural barriers;\nlanguages continue to disappear. To accelerate communication, speech-to-speech\ntranslation (S2ST) technology is one approach that can overcome language\nbarriers. However, S2ST systems require machine translation (MT), speech\nrecognition (ASR), and synthesis (TTS) that rely heavily on supervised training\nand a broad set of language resources that can be difficult to collect from\nethnic communities. Recently, a machine speech chain mechanism was proposed to\nenable ASR and TTS to assist each other in semi-supervised learning. The\nframework was initially implemented only for monolingual languages. In this\nstudy, we focus on developing speech recognition and synthesis for these\nIndonesian ethnic languages: Javanese, Sundanese, Balinese, and Bataks. We\nfirst separately train ASR and TTS of standard Indonesian in supervised\ntraining. We then develop ASR and TTS of ethnic languages by utilizing\nIndonesian ASR and TTS in a cross-lingual machine speech chain framework with\nonly text or only speech data removing the need for paired speech-text data of\nthose ethnic languages.", "published": "2020-11-04 05:13:32", "link": "http://arxiv.org/abs/2011.02128v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Prosodic Representation Learning and Contextual Sampling for Neural\n  Text-to-Speech", "abstract": "In this paper, we introduce Kathaka, a model trained with a novel two-stage\ntraining process for neural speech synthesis with contextually appropriate\nprosody. In Stage I, we learn a prosodic distribution at the sentence level\nfrom mel-spectrograms available during training. In Stage II, we propose a\nnovel method to sample from this learnt prosodic distribution using the\ncontextual information available in text. To do this, we use BERT on text, and\ngraph-attention networks on parse trees extracted from text. We show a\nstatistically significant relative improvement of $13.2\\%$ in naturalness over\na strong baseline when compared to recordings. We also conduct an ablation\nstudy on variations of our sampling technique, and show a statistically\nsignificant improvement over the baseline in each case.", "published": "2020-11-04 12:20:21", "link": "http://arxiv.org/abs/2011.02252v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Query Expansion System for the VoxCeleb Speaker Recognition Challenge\n  2020", "abstract": "In this report, we describe our submission to the VoxCeleb Speaker\nRecognition Challenge (VoxSRC) 2020. Two approaches are adopted. One is to\napply query expansion on speaker verification, which shows significant progress\ncompared to baseline in the study. Another is to use Kaldi extract x-vector and\nto combine its Probabilistic Linear Discriminant Analysis (PLDA) score with\nResNet score.", "published": "2020-11-04 05:24:18", "link": "http://arxiv.org/abs/2011.02882v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "JNLP Team: Deep Learning for Legal Processing in COLIEE 2020", "abstract": "We propose deep learning based methods for automatic systems of legal\nretrieval and legal question-answering in COLIEE 2020. These systems are all\ncharacterized by being pre-trained on large amounts of data before being\nfinetuned for the specified tasks. This approach helps to overcome the data\nscarcity and achieve good performance, thus can be useful for tackling related\nproblems in information retrieval, and decision support in the legal domain.\nBesides, the approach can be explored to deal with other domain specific\nproblems.", "published": "2020-11-04 06:14:11", "link": "http://arxiv.org/abs/2011.08071v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tweet Sentiment Quantification: An Experimental Re-Evaluation", "abstract": "Sentiment quantification is the task of training, by means of supervised\nlearning, estimators of the relative frequency (also called ``prevalence'') of\nsentiment-related classes (such as \\textsf{Positive}, \\textsf{Neutral},\n\\textsf{Negative}) in a sample of unlabelled texts. This task is especially\nimportant when these texts are tweets, since the final goal of most sentiment\nclassification efforts carried out on Twitter data is actually quantification\n(and not the classification of individual tweets). It is well-known that\nsolving quantification by means of ``classify and count'' (i.e., by classifying\nall unlabelled items by means of a standard classifier and counting the items\nthat have been assigned to a given class) is less than optimal in terms of\naccuracy, and that more accurate quantification methods exist. Gao and\nSebastiani (2016) carried out a systematic comparison of quantification methods\non the task of tweet sentiment quantification. In hindsight, we observe that\nthe experimental protocol followed in that work was weak, and that the\nreliability of the conclusions that were drawn from the results is thus\nquestionable. We now re-evaluate those quantification methods (plus a few more\nmodern ones) on exactly the same same datasets, this time following a now\nconsolidated and much more robust experimental protocol (which also involves\nsimulating the presence, in the test data, of class prevalence values very\ndifferent from those of the training set). This experimental protocol (even\nwithout counting the newly added methods) involves a number of experiments\n5,775 times larger than that of the original study. The results of our\nexperiments are dramatically different from those obtained by Gao and\nSebastiani, and they provide a different, much more solid understanding of the\nrelative strengths and weaknesses of different sentiment quantification\nmethods.", "published": "2020-11-04 21:41:34", "link": "http://arxiv.org/abs/2011.08091v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robust Speaker Extraction Network Based on Iterative Refined Adaptation", "abstract": "Speaker extraction aims to extract target speech signal from a multi-talker\nenvironment with interference speakers and surrounding noise, given the target\nspeaker's reference information. Most speaker extraction systems achieve\nsatisfactory performance on the premise that the test speakers have been\nencountered during training time. Such systems suffer from performance\ndegradation given unseen target speakers and/or mismatched reference voiceprint\ninformation. In this paper we propose a novel strategy named Iterative Refined\nAdaptation (IRA) to improve the robustness and generalization capability of\nspeaker extraction systems in the aforementioned scenarios. Given an initial\nspeaker embedding encoded by an auxiliary network, the extraction network can\nobtain a latent representation of the target speaker, which is fed back to the\nauxiliary network to get a refined embedding to provide more accurate guidance\nfor the extraction network. Experiments on WSJ0-2mix-extr and WHAM! dataset\nconfirm the superior performance of the proposed method over the network\nwithout IRA in terms of SI-SDR and PESQ improvement.", "published": "2020-11-04 02:45:21", "link": "http://arxiv.org/abs/2011.02102v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Deep Multi-task Network for Delay Estimation and Echo Cancellation", "abstract": "Echo path delay (or ref-delay) estimation is a big challenge in acoustic echo\ncancellation. Different devices may introduce various ref-delay in practice.\nRef-delay inconsistency slows down the convergence of adaptive filters, and\nalso degrades the performance of deep learning models due to 'unseen'\nref-delays in the training set. In this paper, a multi-task network is proposed\nto address both ref-delay estimation and echo cancellation tasks. The proposed\narchitecture consists of two convolutional recurrent networks (CRNNs) to\nestimate the echo and enhanced signals separately, as well as a fully-connected\n(FC) network to estimate the echo path delay. Echo signal is first predicted,\nand then is combined with reference signal together for delay estimation. At\nthe end, delay compensated reference and microphone signals are used to predict\nthe enhanced target signal. Experimental results suggest that the proposed\nmethod makes reliable delay estimation and outperforms the existing\nstate-of-the-art solutions in inconsistent echo path delay scenarios, in terms\nof echo return loss enhancement (ERLE) and perceptual evaluation of speech\nquality (PESQ). Furthermore, a data augmentation method is studied to evaluate\nthe model performance on different portion of synthetical data with\nartificially introduced ref-delay.", "published": "2020-11-04 03:31:13", "link": "http://arxiv.org/abs/2011.02109v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Multi-Modal Transformers Utterance-Level Code-Switching Detection", "abstract": "An utterance that contains speech from multiple languages is known as a\ncode-switched sentence. In this work, we propose a novel technique to predict\nwhether given audio is mono-lingual or code-switched. We propose a multi-modal\nlearning approach by utilising the phoneme information along with audio\nfeatures for code-switch detection. Our model consists of a Phoneme Network\nthat processes phoneme sequence and Audio Network(AN), which processes the mfcc\nfeatures. We fuse representation learned from both the Networks to predict if\nthe utterance is code-switched or not. The Audio Network and Phonetic Network\nconsist of initial convolution, Bi-LSTM, and transformer encoder layers. The\ntransformer encoder layer helps in selecting important and relevant features\nfor better classification by using self-attention. We show that utilising the\nphoneme sequence of the utterance along with the mfcc features improves the\nperformance of code-switch detection significantly. We train and evaluate our\nmodel on Microsoft code-switching challenge datasets for Telugu, Tamil, and\nGujarati languages. Our experiments show that the multi-modal learning approach\nsignificantly improved accuracy over the uni-modal approaches for\nTelugu-English, Gujarati-English, and Tamil-English datasets. We also study the\nsystem performance using different neural layers and show that the transformers\nhelp obtain better performance.", "published": "2020-11-04 05:26:09", "link": "http://arxiv.org/abs/2011.02132v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Learning in your voice: Non-parallel voice conversion based on speaker\n  consistency loss", "abstract": "In this paper, we propose a novel voice conversion strategy to resolve the\nmismatch between the training and conversion scenarios when parallel speech\ncorpus is unavailable for training. Based on auto-encoder and disentanglement\nframeworks, we design the proposed model to extract identity and content\nrepresentations while reconstructing the input speech signal itself. Since we\nuse other speaker's identity information in the training process, the training\nphilosophy is naturally matched with the objective of voice conversion process.\nIn addition, we effectively design the disentanglement framework to reliably\npreserve linguistic information and to enhance the quality of converted speech\nsignals. The superiority of the proposed method is shown in subjective\nlistening tests as well as objective measures.", "published": "2020-11-04 07:58:35", "link": "http://arxiv.org/abs/2011.02168v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "One-shot conditional audio filtering of arbitrary sounds", "abstract": "We consider the problem of separating a particular sound source from a\nsingle-channel mixture, based on only a short sample of the target source.\nUsing SoundFilter, a wave-to-wave neural network architecture, we can train a\nmodel without using any sound class labels. Using a conditioning encoder model\nwhich is learned jointly with the source separation network, the trained model\ncan be \"configured\" to filter arbitrary sound sources, even ones that it has\nnot seen during training. Evaluated on the FSD50k dataset, our model obtains an\nSI-SDR improvement of 9.6 dB for mixtures of two sounds. When trained on\nLibrispeech, our model achieves an SI-SDR improvement of 14.0 dB when\nseparating one voice from a mixture of two speakers. Moreover, we show that the\nrepresentation learned by the conditioning encoder clusters acoustically\nsimilar sounds together in the embedding space, even though it is trained\nwithout using any labels.", "published": "2020-11-04 17:20:42", "link": "http://arxiv.org/abs/2011.02421v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Frustratingly Easy Noise-aware Training of Acoustic Models", "abstract": "Environmental noises and reverberation have a detrimental effect on the\nperformance of automatic speech recognition (ASR) systems. Multi-condition\ntraining of neural network-based acoustic models is used to deal with this\nproblem, but it requires many-folds data augmentation, resulting in increased\ntraining time. In this paper, we propose utterance-level noise vectors for\nnoise-aware training of acoustic models in hybrid ASR. Our noise vectors are\nobtained by combining the means of speech frames and silence frames in the\nutterance, where the speech/silence labels may be obtained from a GMM-HMM model\ntrained for ASR alignments, such that no extra computation is required beyond\naveraging of feature vectors. We show through experiments on AMI and Aurora-4\nthat this simple adaptation technique can result in 6-7% relative WER\nimprovement. We implement several embedding-based adaptation baselines proposed\nin literature, and show that our method outperforms them on both the datasets.\nFinally, we extend our method to the online ASR setting by using frame-level\nmaximum likelihood for the mean estimation.", "published": "2020-11-04 01:20:00", "link": "http://arxiv.org/abs/2011.02090v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DESNet: A Multi-channel Network for Simultaneous Speech Dereverberation,\n  Enhancement and Separation", "abstract": "In this paper, we propose a multi-channel network for simultaneous speech\ndereverberation, enhancement and separation (DESNet). To enable gradient\npropagation and joint optimization, we adopt the attentional selection\nmechanism of the multi-channel features, which is originally proposed in\nend-to-end unmixing, fixed-beamforming and extraction (E2E-UFE) structure.\nFurthermore, the novel deep complex convolutional recurrent network (DCCRN) is\nused as the structure of the speech unmixing and the neural network based\nweighted prediction error (WPE) is cascaded beforehand for speech\ndereverberation. We also introduce the staged SNR strategy and symphonic loss\nfor the training of the network to further improve the final performance.\nExperiments show that in non-dereverberated case, the proposed DESNet\noutperforms DCCRN and most state-of-the-art structures in speech enhancement\nand separation, while in dereverberated scenario, DESNet also shows\nimprovements over the cascaded WPE-DCCRN networks.", "published": "2020-11-04 05:25:10", "link": "http://arxiv.org/abs/2011.02131v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "IEEE SLT 2021 Alpha-mini Speech Challenge: Open Datasets, Tracks, Rules\n  and Baselines", "abstract": "The IEEE Spoken Language Technology Workshop (SLT) 2021 Alpha-mini Speech\nChallenge (ASC) is intended to improve research on keyword spotting (KWS) and\nsound source location (SSL) on humanoid robots. Many publications report\nsignificant improvements in deep learning based KWS and SSL on open source\ndatasets in recent years. For deep learning model training, it is necessary to\nexpand the data coverage to improve the robustness of model. Thus, simulating\nmulti-channel noisy and reverberant data from single-channel speech, noise,\necho and room impulsive response (RIR) is widely adopted. However, this\napproach may generate mismatch between simulated data and recorded data in real\napplication scenarios, especially echo data. In this challenge, we open source\na sizable speech, keyword, echo and noise corpus for promoting data-driven\nmethods, particularly deep-learning approaches on KWS and SSL. We also choose\nAlpha-mini, a humanoid robot produced by UBTECH equipped with a built-in\nfour-microphone array on its head, to record development and evaluation sets\nunder the actual Alpha-mini robot application scenario, including noise as well\nas echo and mechanical noise generated by the robot itself for model\nevaluation. Furthermore, we illustrate the rules, evaluation methods and\nbaselines for researchers to quickly assess their achievements and optimize\ntheir models.", "published": "2020-11-04 09:42:13", "link": "http://arxiv.org/abs/2011.02198v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Multi-Channel Temporal Attention Convolutional Neural Network Model\n  for Environmental Sound Classification", "abstract": "Recently, many attention-based deep neural networks have emerged and achieved\nstate-of-the-art performance in environmental sound classification. The essence\nof attention mechanism is assigning contribution weights on different parts of\nfeatures, namely channels, spectral or spatial contents, and temporal frames.\nIn this paper, we propose an effective convolutional neural network structure\nwith a multi-channel temporal attention (MCTA) block, which applies a temporal\nattention mechanism within each channel of the embedded features to extract\nchannel-wise relevant temporal information. This multi-channel temporal\nattention structure will result in a distinct attention vector for each\nchannel, which enables the network to fully exploit the relevant temporal\ninformation in different channels. The datasets used to test our model include\nESC-50 and its subset ESC-10, along with development sets of DCASE 2018 and\n2019. In our experiments, MCTA performed better than the single-channel\ntemporal attention model and the non-attention model with the same number of\nparameters. Furthermore, we compared our model with some successful\nattention-based models and obtained competitive results with a relatively\nlighter network.", "published": "2020-11-04 22:05:57", "link": "http://arxiv.org/abs/2011.02561v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Can We Trust Deep Speech Prior?", "abstract": "Recently, speech enhancement (SE) based on deep speech prior has attracted\nmuch attention, such as the variational auto-encoder with non-negative matrix\nfactorization (VAE-NMF) architecture. Compared to conventional approaches that\nrepresent clean speech by shallow models such as Gaussians with a low-rank\ncovariance, the new approach employs deep generative models to represent the\nclean speech, which often provides a better prior. Despite the clear advantage\nin theory, we argue that deep priors must be used with much caution, since the\nlikelihood produced by a deep generative model does not always coincide with\nthe speech quality. We designed a comprehensive study on this issue and\ndemonstrated that based on deep speech priors, a reasonable SE performance can\nbe achieved, but the results might be suboptimal. A careful analysis showed\nthat this problem is deeply rooted in the disharmony between the flexibility of\ndeep generative models and the nature of the maximum-likelihood (ML) training.", "published": "2020-11-04 03:35:21", "link": "http://arxiv.org/abs/2011.02110v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Correlation based Multi-phasal models for improved imagined speech EEG\n  recognition", "abstract": "Translation of imagined speech electroencephalogram(EEG) into human\nunderstandable commands greatly facilitates the design of naturalistic brain\ncomputer interfaces. To achieve improved imagined speech unit classification,\nthis work aims to profit from the parallel information contained in\nmulti-phasal EEG data recorded while speaking, imagining and performing\narticulatory movements corresponding to specific speech units. A bi-phase\ncommon representation learning module using neural networks is designed to\nmodel the correlation and reproducibility between an analysis phase and a\nsupport phase. The trained Correlation Network is then employed to extract\ndiscriminative features of the analysis phase. These features are further\nclassified into five binary phonological categories using machine learning\nmodels such as Gaussian mixture based hidden Markov model and deep neural\nnetworks. The proposed approach further handles the non-availability of\nmulti-phasal data during decoding. Topographic visualizations along with\nresult-based inferences suggest that the multi-phasal correlation modelling\napproach proposed in the paper enhances imagined-speech EEG recognition\nperformance.", "published": "2020-11-04 09:39:53", "link": "http://arxiv.org/abs/2011.02195v1", "categories": ["eess.SP", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Single channel voice separation for unknown number of speakers under\n  reverberant and noisy settings", "abstract": "We present a unified network for voice separation of an unknown number of\nspeakers. The proposed approach is composed of several separation heads\noptimized together with a speaker classification branch. The separation is\ncarried out in the time domain, together with parameter sharing between all\nseparation heads. The classification branch estimates the number of speakers\nwhile each head is specialized in separating a different number of speakers. We\nevaluate the proposed model under both clean and noisy reverberant set-tings.\nResults suggest that the proposed approach is superior to the baseline model by\na significant margin. Additionally, we present a new noisy and reverberant\ndataset of up to five different speakers speaking simultaneously.", "published": "2020-11-04 14:59:14", "link": "http://arxiv.org/abs/2011.02329v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Influence of Event Duration on Automatic Wheeze Classification", "abstract": "Patients with respiratory conditions typically exhibit adventitious\nrespiratory sounds, such as wheezes. Wheeze events have variable duration. In\nthis work we studied the influence of event duration on wheeze classification,\nnamely how the creation of the non-wheeze class affected the classifiers'\nperformance. First, we evaluated several classifiers on an open access\nrespiratory sound database, with the best one reaching sensitivity and\nspecificity values of 98% and 95%, respectively. Then, by changing one\nparameter in the design of the non-wheeze class, i.e., event duration, the best\nclassifier only reached sensitivity and specificity values of 55% and 76%,\nrespectively. These results demonstrate the importance of experimental design\non the assessment of wheeze classification algorithms' performance.", "published": "2020-11-04 11:03:25", "link": "http://arxiv.org/abs/2011.02874v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Hierarchical Subspace Model for Language-Attuned Acoustic Unit\n  Discovery", "abstract": "In this work, we propose a hierarchical subspace model for acoustic unit\ndiscovery. In this approach, we frame the task as one of learning embeddings on\na low-dimensional phonetic subspace, and simultaneously specify the subspace\nitself as an embedding on a hyper-subspace. We train the hyper-subspace on a\nset of transcribed languages and transfer it to the target language. In the\ntarget language, we infer both the language and unit embeddings in an\nunsupervised manner, and in so doing, we simultaneously learn a subspace of\nunits specific to that language and the units that dwell on it. We conduct our\nexperiments on TIMIT and two low-resource languages: Mboshi and Yoruba. Results\nshow that our model outperforms major acoustic unit discovery techniques, both\nin terms of clustering quality and segmentation accuracy.", "published": "2020-11-04 16:34:19", "link": "http://arxiv.org/abs/2011.03115v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
