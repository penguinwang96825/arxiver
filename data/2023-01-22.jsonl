{"title": "Ensemble Transfer Learning for Multilingual Coreference Resolution", "abstract": "Entity coreference resolution is an important research problem with many\napplications, including information extraction and question answering.\nCoreference resolution for English has been studied extensively. However, there\nis relatively little work for other languages. A problem that frequently occurs\nwhen working with a non-English language is the scarcity of annotated training\ndata. To overcome this challenge, we design a simple but effective\nensemble-based framework that combines various transfer learning (TL)\ntechniques. We first train several models using different TL methods. Then,\nduring inference, we compute the unweighted average scores of the models'\npredictions to extract the final set of predicted clusters. Furthermore, we\nalso propose a low-cost TL method that bootstraps coreference resolution models\nby utilizing Wikipedia anchor texts. Leveraging the idea that the coreferential\nlinks naturally exist between anchor texts pointing to the same article, our\nmethod builds a sizeable distantly-supervised dataset for the target language\nthat consists of tens of thousands of documents. We can pre-train a model on\nthe pseudo-labeled dataset before finetuning it on the final target dataset.\nExperimental results on two benchmark datasets, OntoNotes and SemEval, confirm\nthe effectiveness of our methods. Our best ensembles consistently outperform\nthe baseline approach of simple training by up to 7.68% in the F1 score. These\nensembles also achieve new state-of-the-art results for three languages:\nArabic, Dutch, and Spanish.", "published": "2023-01-22 18:22:55", "link": "http://arxiv.org/abs/2301.09175v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Representing Interlingual Meaning in Lexical Databases", "abstract": "In today's multilingual lexical databases, the majority of the world's\nlanguages are under-represented. Beyond a mere issue of resource\nincompleteness, we show that existing lexical databases have structural\nlimitations that result in a reduced expressivity on culturally-specific words\nand in mapping them across languages. In particular, the lexical meaning space\nof dominant languages, such as English, is represented more accurately while\nlinguistically or culturally diverse languages are mapped in an approximate\nmanner. Our paper assesses state-of-the-art multilingual lexical databases and\nevaluates their strengths and limitations with respect to their expressivity on\nlexical phenomena of linguistic diversity.", "published": "2023-01-22 17:41:29", "link": "http://arxiv.org/abs/2301.09169v1", "categories": ["cs.CL", "cs.AI", "I.2.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "Summarize the Past to Predict the Future: Natural Language Descriptions\n  of Context Boost Multimodal Object Interaction Anticipation", "abstract": "We study object interaction anticipation in egocentric videos. This task\nrequires an understanding of the spatio-temporal context formed by past actions\non objects, coined action context. We propose TransFusion, a multimodal\ntransformer-based architecture. It exploits the representational power of\nlanguage by summarizing the action context. TransFusion leverages pre-trained\nimage captioning and vision-language models to extract the action context from\npast video frames. This action context together with the next video frame is\nprocessed by the multimodal fusion module to forecast the next object\ninteraction. Our model enables more efficient end-to-end learning. The large\npre-trained language models add common sense and a generalisation capability.\nExperiments on Ego4D and EPIC-KITCHENS-100 show the effectiveness of our\nmultimodal fusion model. They also highlight the benefits of using\nlanguage-based context summaries in a task where vision seems to suffice. Our\nmethod outperforms state-of-the-art approaches by 40.4% in relative terms in\noverall mAP on the Ego4D test set. We validate the effectiveness of TransFusion\nvia experiments on EPIC-KITCHENS-100. Video and code are available at\nhttps://eth-ait.github.io/transfusion-proj/.", "published": "2023-01-22 21:30:12", "link": "http://arxiv.org/abs/2301.09209v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "An Empirical Study of Metrics to Measure Representational Harms in\n  Pre-Trained Language Models", "abstract": "Large-scale Pre-Trained Language Models (PTLMs) capture knowledge from\nmassive human-written data which contains latent societal biases and toxic\ncontents. In this paper, we leverage the primary task of PTLMs, i.e., language\nmodeling, and propose a new metric to quantify manifested implicit\nrepresentational harms in PTLMs towards 13 marginalized demographics. Using\nthis metric, we conducted an empirical analysis of 24 widely used PTLMs. Our\nanalysis provides insights into the correlation between the proposed metric in\nthis work and other related metrics for representational harm. We observe that\nour metric correlates with most of the gender-specific metrics in the\nliterature. Through extensive experiments, we explore the connections between\nPTLMs architectures and representational harms across two dimensions: depth and\nwidth of the networks. We found that prioritizing depth over width, mitigates\nrepresentational harms in some PTLMs. Our code and data can be found at\nhttps://github.com/microsoft/SafeNLP.", "published": "2023-01-22 21:47:26", "link": "http://arxiv.org/abs/2301.09211v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Interpretability in Activation Space Analysis of Transformers: A Focused\n  Survey", "abstract": "The field of natural language processing has reached breakthroughs with the\nadvent of transformers. They have remained state-of-the-art since then, and\nthere also has been much research in analyzing, interpreting, and evaluating\nthe attention layers and the underlying embedding space. In addition to the\nself-attention layers, the feed-forward layers in the transformer are a\nprominent architectural component. From extensive research, we observe that its\nrole is under-explored. We focus on the latent space, known as the Activation\nSpace, that consists of the neuron activations from these feed-forward layers.\nIn this survey paper, we review interpretability methods that examine the\nlearnings that occurred in this activation space. Since there exists only\nlimited research in this direction, we conduct a detailed examination of each\nwork and point out potential future directions of research. We hope our work\nprovides a step towards strengthening activation space analysis.", "published": "2023-01-22 18:03:20", "link": "http://arxiv.org/abs/2302.09304v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a\n  Case Study", "abstract": "Several high-resource Text to Speech (TTS) systems currently produce natural,\nwell-established human-like speech. In contrast, low-resource languages,\nincluding Arabic, have very limited TTS systems due to the lack of resources.\nWe propose a fully unsupervised method for building TTS, including automatic\ndata selection and pre-training/fine-tuning strategies for TTS training, using\nbroadcast news as a case study. We show how careful selection of data, yet\nsmaller amounts, can improve the efficiency of TTS system in generating more\nnatural speech than a system trained on a bigger dataset. We adopt to propose\ndifferent approaches for the: 1) data: we applied automatic annotations using\nDNSMOS, automatic vowelization, and automatic speech recognition (ASR) for\nfixing transcriptions' errors; 2) model: we used transfer learning from\nhigh-resource language in TTS model and fine-tuned it with one hour broadcast\nrecording then we used this model to guide a FastSpeech2-based Conformer model\nfor duration. Our objective evaluation shows 3.9% character error rate (CER),\nwhile the groundtruth has 1.3% CER. As for the subjective evaluation, where 1\nis bad and 5 is excellent, our FastSpeech2-based Conformer model achieved a\nmean opinion score (MOS) of 4.4 for intelligibility and 4.2 for naturalness,\nwhere many annotators recognized the voice of the broadcaster, which proves the\neffectiveness of our proposed unsupervised method.", "published": "2023-01-22 10:41:58", "link": "http://arxiv.org/abs/2301.09099v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Differentially Private Natural Language Models: Recent Advances and\n  Future Directions", "abstract": "Recent developments in deep learning have led to great success in various\nnatural language processing (NLP) tasks. However, these applications may\ninvolve data that contain sensitive information. Therefore, how to achieve good\nperformance while also protecting the privacy of sensitive data is a crucial\nchallenge in NLP. To preserve privacy, Differential Privacy (DP), which can\nprevent reconstruction attacks and protect against potential side knowledge, is\nbecoming a de facto technique for private data analysis. In recent years, NLP\nin DP models (DP-NLP) has been studied from different perspectives, which\ndeserves a comprehensive review. In this paper, we provide the first systematic\nreview of recent advances in DP deep learning models in NLP. In particular, we\nfirst discuss some differences and additional challenges of DP-NLP compared\nwith the standard DP deep learning. Then, we investigate some existing work on\nDP-NLP and present its recent developments from three aspects: gradient\nperturbation based methods, embedding vector perturbation based methods, and\nensemble model based methods. We also discuss some challenges and future\ndirections.", "published": "2023-01-22 12:29:03", "link": "http://arxiv.org/abs/2301.09112v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Speaker Embeddings with Adversarial Multi-task Learning for\n  Age Group Classification", "abstract": "Recently, researchers have utilized neural network-based speaker embedding\ntechniques in speaker-recognition tasks to identify speakers accurately.\nHowever, speaker-discriminative embeddings do not always represent speech\nfeatures such as age group well. In an embedding model that has been highly\ntrained to capture speaker traits, the task of age group classification is\ncloser to speech information leakage. Hence, to improve age group\nclassification performance, we consider the use of speaker-discriminative\nembeddings derived from adversarial multi-task learning to align features and\nreduce the domain discrepancy in age subgroups. In addition, we investigated\ndifferent types of speaker embeddings to learn and generalize the\ndomain-invariant representations for age groups. Experimental results on the\nVoxCeleb Enrichment dataset verify the effectiveness of our proposed adaptive\nadversarial network in multi-objective scenarios and leveraging speaker\nembeddings for the domain adaptation task.", "published": "2023-01-22 05:01:13", "link": "http://arxiv.org/abs/2301.09058v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Estimation of Source and Receiver Positions, Room Geometry and\n  Reflection Coefficients From a Single Room Impulse Response", "abstract": "We propose an algorithm to estimate source and receiver positions, room\ngeometry and reflection coefficients from a single room impulse response\nsimultaneously. It is based on a symmetry analysis of the room impulse\nresponse. The proposed method utilizes the times of arrivals of the direct\npath, first order reflections and second order reflections. The proposed method\nis robust to erroneous pulses and non-specular reflections. It can be applied\nto any room with parallel walls as long as the required arrival times of\nreflections are available. In contrast to the state-of-art method, we do not\nrestrict the location of source and receiver.", "published": "2023-01-22 20:46:10", "link": "http://arxiv.org/abs/2301.09198v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cellular Network Speech Enhancement: Removing Background and\n  Transmission Noise", "abstract": "The primary objective of speech enhancement is to reduce background noise\nwhile preserving the target's speech. A common dilemma occurs when a speaker is\nconfined to a noisy environment and receives a call with high background and\ntransmission noise. To address this problem, the Deep Noise Suppression (DNS)\nChallenge focuses on removing the background noise with the next-generation\ndeep learning models to enhance the target's speech; however, researchers fail\nto consider Voice Over IP (VoIP) applications their transmission noise.\nFocusing on Google Meet and its cellular application, our work achieves\nstate-of-the-art performance on the Google Meet To Phone Track of the VoIP DNS\nChallenge. This paper demonstrates how to beat industrial performance and\nachieve 1.92 PESQ and 0.88 STOI, as well as superior acoustic fidelity,\nperceptual quality, and intelligibility in various metrics.", "published": "2023-01-22 00:18:10", "link": "http://arxiv.org/abs/2301.09027v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dance2MIDI: Dance-driven multi-instruments music generation", "abstract": "Dance-driven music generation aims to generate musical pieces conditioned on\ndance videos. Previous works focus on monophonic or raw audio generation, while\nthe multi-instruments scenario is under-explored. The challenges associated\nwith the dance-driven multi-instrument music (MIDI) generation are twofold: 1)\nno publicly available multi-instruments MIDI and video paired dataset and 2)\nthe weak correlation between music and video. To tackle these challenges, we\nbuild the first multi-instruments MIDI and dance paired dataset (D2MIDI). Based\non our proposed dataset, we introduce a multi-instruments MIDI generation\nframework (Dance2MIDI) conditioned on dance video. Specifically, 1) to capture\nthe relationship between dance and music, we employ the Graph Convolutional\nNetwork to encode the dance motion. This allows us to extract features related\nto dance movement and dance style, 2) to generate a harmonious rhythm, we\nutilize a Transformer model to decode the drum track sequence, leveraging a\ncross-attention mechanism, and 3) we model the task of generating the remaining\ntracks based on the drum track as a sequence understanding and completion task.\nA BERT-like model is employed to comprehend the context of the entire music\npiece through self-supervised learning. We evaluate the generated music of our\nframework trained on the D2MIDI dataset and demonstrate that our method\nachieves State-of-the-Art performance.", "published": "2023-01-22 08:35:51", "link": "http://arxiv.org/abs/2301.09080v7", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
