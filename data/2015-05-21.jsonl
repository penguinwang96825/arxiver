{"title": "Translation Memory Retrieval Methods", "abstract": "Translation Memory (TM) systems are one of the most widely used translation\ntechnologies. An important part of TM systems is the matching algorithm that\ndetermines what translations get retrieved from the bank of available\ntranslations to assist the human translator. Although detailed accounts of the\nmatching algorithms used in commercial systems can't be found in the\nliterature, it is widely believed that edit distance algorithms are used. This\npaper investigates and evaluates the use of several matching algorithms,\nincluding the edit distance algorithm that is believed to be at the heart of\nmost modern commercial TM systems. This paper presents results showing how well\nvarious matching algorithms correlate with human judgments of helpfulness\n(collected via crowdsourcing with Amazon's Mechanical Turk). A new algorithm\nbased on weighted n-gram precision that can be adjusted for translator length\npreferences consistently returns translations judged to be most helpful by\ntranslators for multiple domains and language pairs.", "published": "2015-05-21 18:57:34", "link": "http://arxiv.org/abs/1505.05841v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "The IBM 2015 English Conversational Telephone Speech Recognition System", "abstract": "We describe the latest improvements to the IBM English conversational\ntelephone speech recognition system. Some of the techniques that were found\nbeneficial are: maxout networks with annealed dropout rates; networks with a\nvery large number of outputs trained on 2000 hours of data; joint modeling of\npartially unfolded recurrent neural networks and convolutional nets by\ncombining the bottleneck and output layers and retraining the resulting model;\nand lastly, sophisticated language model rescoring with exponential and neural\nnetwork LMs. These techniques result in an 8.0% word error rate on the\nSwitchboard part of the Hub5-2000 evaluation test set which is 23% relative\nbetter than our previous best published result.", "published": "2015-05-21 20:49:32", "link": "http://arxiv.org/abs/1505.05899v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are You Talking to a Machine? Dataset and Methods for Multilingual Image\n  Question Answering", "abstract": "In this paper, we present the mQA model, which is able to answer questions\nabout the content of an image. The answer can be a sentence, a phrase or a\nsingle word. Our model contains four components: a Long Short-Term Memory\n(LSTM) to extract the question representation, a Convolutional Neural Network\n(CNN) to extract the visual representation, an LSTM for storing the linguistic\ncontext in an answer, and a fusing component to combine the information from\nthe first three components and generate the answer. We construct a Freestyle\nMultilingual Image Question Answering (FM-IQA) dataset to train and evaluate\nour mQA model. It contains over 150,000 images and 310,000 freestyle Chinese\nquestion-answer pairs and their English translations. The quality of the\ngenerated answers of our mQA model on this dataset is evaluated by human judges\nthrough a Turing Test. Specifically, we mix the answers provided by humans and\nour model. The human judges need to distinguish our model from the human. They\nwill also provide a score (i.e. 0, 1, 2, the larger the better) indicating the\nquality of the answer. We propose strategies to monitor the quality of this\nevaluation process. The experiments show that in 64.7% of cases, the human\njudges cannot distinguish our model from humans. The average score is 1.454\n(1.918 for human). The details of this work, including the FM-IQA dataset, can\nbe found on the project page: http://idl.baidu.com/FM-IQA.html", "published": "2015-05-21 06:09:36", "link": "http://arxiv.org/abs/1505.05612v3", "categories": ["cs.CV", "cs.CL", "cs.LG", "I.2.6; I.2.7; I.2.10"], "primary_category": "cs.CV"}
{"title": "A Re-ranking Model for Dependency Parser with Recursive Convolutional\n  Neural Network", "abstract": "In this work, we address the problem to model all the nodes (words or\nphrases) in a dependency tree with the dense representations. We propose a\nrecursive convolutional neural network (RCNN) architecture to capture syntactic\nand compositional-semantic representations of phrases and words in a dependency\ntree. Different with the original recursive neural network, we introduce the\nconvolution and pooling layers, which can model a variety of compositions by\nthe feature maps and choose the most informative compositions by the pooling\nlayers. Based on RCNN, we use a discriminative model to re-rank a $k$-best list\nof candidate dependency parsing trees. The experiments show that RCNN is very\neffective to improve the state-of-the-art dependency parsing on both English\nand Chinese datasets.", "published": "2015-05-21 10:23:10", "link": "http://arxiv.org/abs/1505.05667v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
