{"title": "AutoExtend: Extending Word Embeddings to Embeddings for Synsets and\n  Lexemes", "abstract": "We present \\textit{AutoExtend}, a system to learn embeddings for synsets and\nlexemes. It is flexible in that it can take any word embeddings as input and\ndoes not need an additional training corpus. The synset/lexeme embeddings\nobtained live in the same vector space as the word embeddings. A sparse tensor\nformalization guarantees efficiency and parallelizability. We use WordNet as a\nlexical resource, but AutoExtend can be easily applied to other resources like\nFreebase. AutoExtend achieves state-of-the-art performance on word similarity\nand word sense disambiguation tasks.", "published": "2015-07-04 16:59:30", "link": "http://arxiv.org/abs/1507.01127v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Describing Multimedia Content using Attention-based Encoder--Decoder\n  Networks", "abstract": "Whereas deep neural networks were first mostly used for classification tasks,\nthey are rapidly expanding in the realm of structured output problems, where\nthe observed target is composed of multiple random variables that have a rich\njoint distribution, given the input. We focus in this paper on the case where\nthe input also has a rich structure and the input and output structures are\nsomehow related. We describe systems that learn to attend to different places\nin the input, for each element of the output, for a variety of tasks: machine\ntranslation, image caption generation, video clip description and speech\nrecognition. All these systems are based on a shared set of building blocks:\ngated recurrent neural networks and convolutional neural networks, along with\ntrained attention mechanisms. We report on experimental results with these\nsystems, showing impressively good performance and the advantage of the\nattention mechanism.", "published": "2015-07-04 01:06:16", "link": "http://arxiv.org/abs/1507.01053v1", "categories": ["cs.NE", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.NE"}
