{"title": "Word Embedding Transformation for Robust Unsupervised Bilingual Lexicon\n  Induction", "abstract": "Great progress has been made in unsupervised bilingual lexicon induction\n(UBLI) by aligning the source and target word embeddings independently trained\non monolingual corpora. The common assumption of most UBLI models is that the\nembedding spaces of two languages are approximately isomorphic. Therefore the\nperformance is bound by the degree of isomorphism, especially on etymologically\nand typologically distant languages. To address this problem, we propose a\ntransformation-based method to increase the isomorphism. Embeddings of two\nlanguages are made to match with each other by rotating and scaling. The method\ndoes not require any form of supervision and can be applied to any language\npair. On a benchmark data set of bilingual lexicon induction, our approach can\nachieve competitive or superior performance compared to state-of-the-art\nmethods, with particularly strong results being found on distant languages.", "published": "2021-05-26 02:09:58", "link": "http://arxiv.org/abs/2105.12297v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SGPT: Semantic Graphs based Pre-training for Aspect-based Sentiment\n  Analysis", "abstract": "Previous studies show effective of pre-trained language models for sentiment\nanalysis. However, most of these studies ignore the importance of sentimental\ninformation for pre-trained models.Therefore, we fully investigate the\nsentimental information for pre-trained models and enhance pre-trained language\nmodels with semantic graphs for sentiment analysis.In particular, we introduce\nSemantic Graphs based Pre-training(SGPT) using semantic graphs to obtain\nsynonym knowledge for aspect-sentiment pairs and similar aspect/sentiment\nterms.We then optimize the pre-trained language model with the semantic\ngraphs.Empirical studies on several downstream tasks show that proposed model\noutperforms strong pre-trained baselines. The results also show the\neffectiveness of proposed semantic graphs for pre-trained model.", "published": "2021-05-26 02:32:50", "link": "http://arxiv.org/abs/2105.12305v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Read, Listen, and See: Leveraging Multimodal Information Helps Chinese\n  Spell Checking", "abstract": "Chinese Spell Checking (CSC) aims to detect and correct erroneous characters\nfor user-generated text in the Chinese language. Most of the Chinese spelling\nerrors are misused semantically, phonetically or graphically similar\ncharacters. Previous attempts noticed this phenomenon and try to use the\nsimilarity for this task. However, these methods use either heuristics or\nhandcrafted confusion sets to predict the correct character. In this paper, we\npropose a Chinese spell checker called ReaLiSe, by directly leveraging the\nmultimodal information of the Chinese characters. The ReaLiSe model tackles the\nCSC task by (1) capturing the semantic, phonetic and graphic information of the\ninput characters, and (2) selectively mixing the information in these\nmodalities to predict the correct output. Experiments on the SIGHAN benchmarks\nshow that the proposed model outperforms strong baselines by a large margin.", "published": "2021-05-26 02:38:11", "link": "http://arxiv.org/abs/2105.12306v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Pronoun Resolution via Masked Noun-Phrase Prediction", "abstract": "In this work, we propose Masked Noun-Phrase Prediction (MNPP), a pre-training\nstrategy to tackle pronoun resolution in a fully unsupervised setting. Firstly,\nWe evaluate our pre-trained model on various pronoun resolution datasets\nwithout any finetuning. Our method outperforms all previous unsupervised\nmethods on all datasets by large margins. Secondly, we proceed to a few-shot\nsetting where we finetune our pre-trained model on WinoGrande-S and XS\nseparately. Our method outperforms RoBERTa-large baseline with large margins,\nmeanwhile, achieving a higher AUC score after further finetuning on the\nremaining three official splits of WinoGrande.", "published": "2021-05-26 08:30:18", "link": "http://arxiv.org/abs/2105.12392v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SentEmojiBot: Empathising Conversations Generation with Emojis", "abstract": "The increasing use of dialogue agents makes it extremely desirable for them\nto understand and acknowledge the implied emotions to respond like humans with\nempathy. Chatbots using traditional techniques analyze emotions based on the\ncontext and meaning of the text and lack the understanding of emotions\nexpressed through face. Emojis representing facial expressions present a\npromising way to express emotions. However, none of the AI systems utilizes\nemojis for empathetic conversation generation. We propose, SentEmojiBot, based\non the SentEmoji dataset, to generate empathetic conversations with a\ncombination of emojis and text. Evaluation metrics show that the BERT-based\nmodel outperforms the vanilla transformer model. A user study indicates that\nthe dialogues generated by our model were understandable and adding emojis\nimproved empathetic traits in conversations by 9.8%", "published": "2021-05-26 08:51:44", "link": "http://arxiv.org/abs/2105.12399v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Optimization of Tokenization and Downstream Model", "abstract": "Since traditional tokenizers are isolated from a downstream task and model,\nthey cannot output an appropriate tokenization depending on the task and model,\nalthough recent studies imply that the appropriate tokenization improves the\nperformance. In this paper, we propose a novel method to find an appropriate\ntokenization to a given downstream model by jointly optimizing a tokenizer and\nthe model. The proposed method has no restriction except for using loss values\ncomputed by the downstream model to train the tokenizer, and thus, we can apply\nthe proposed method to any NLP task. Moreover, the proposed method can be used\nto explore the appropriate tokenization for an already trained model as\npost-processing. Therefore, the proposed method is applicable to various\nsituations. We evaluated whether our method contributes to improving\nperformance on text classification in three languages and machine translation\nin eight language pairs. Experimental results show that our proposed method\nimproves the performance by determining appropriate tokenizations.", "published": "2021-05-26 09:05:10", "link": "http://arxiv.org/abs/2105.12410v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Morphology Dataset and Models for Multiple Languages, from the\n  Large to the Endangered", "abstract": "We train neural models for morphological analysis, generation and\nlemmatization for morphologically rich languages. We present a method for\nautomatically extracting substantially large amount of training data from FSTs\nfor 22 languages, out of which 17 are endangered. The neural models follow the\nsame tagset as the FSTs in order to make it possible to use them as fallback\nsystems together with the FSTs. The source code, models and datasets have been\nreleased on Zenodo.", "published": "2021-05-26 09:35:38", "link": "http://arxiv.org/abs/2105.12428v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The statistical advantage of automatic NLG metrics at the system level", "abstract": "Estimating the expected output quality of generation systems is central to\nNLG. This paper qualifies the notion that automatic metrics are not as good as\nhumans in estimating system-level quality. Statistically, humans are unbiased,\nhigh variance estimators, while metrics are biased, low variance estimators. We\ncompare these estimators by their error in pairwise prediction (which\ngeneration system is better?) using the bootstrap. Measuring this error is\ncomplicated: predictions are evaluated against noisy, human predicted labels\ninstead of the ground truth, and metric predictions fluctuate based on the test\nsets they were calculated on. By applying a bias-variance-noise decomposition,\nwe adjust this error to a noise-free, infinite test set setting. Our analysis\ncompares the adjusted error of metrics to humans and a derived, perfect\nsegment-level annotator, both of which are unbiased estimators dependent on the\nnumber of judgments collected. In MT, we identify two settings where metrics\noutperform humans due to a statistical advantage in variance: when the number\nof human judgments used is small, and when the quality difference between\ncompared systems is small. The data and code to reproduce our analyses are\navailable at https://github.com/johntzwei/metric-statistical-advantage .", "published": "2021-05-26 09:53:57", "link": "http://arxiv.org/abs/2105.12437v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bilingual Mutual Information Based Adaptive Training for Neural Machine\n  Translation", "abstract": "Recently, token-level adaptive training has achieved promising improvement in\nmachine translation, where the cross-entropy loss function is adjusted by\nassigning different training weights to different tokens, in order to alleviate\nthe token imbalance problem. However, previous approaches only use static word\nfrequency information in the target language without considering the source\nlanguage, which is insufficient for bilingual tasks like machine translation.\nIn this paper, we propose a novel bilingual mutual information (BMI) based\nadaptive objective, which measures the learning difficulty for each target\ntoken from the perspective of bilingualism, and assigns an adaptive weight\naccordingly to improve token-level adaptive training. This method assigns\nlarger training weights to tokens with higher BMI, so that easy tokens are\nupdated with coarse granularity while difficult tokens are updated with fine\ngranularity. Experimental results on WMT14 English-to-German and WMT19\nChinese-to-English demonstrate the superiority of our approach compared with\nthe Transformer baseline and previous token-level adaptive training approaches.\nFurther analyses confirm that our method can improve the lexical diversity.", "published": "2021-05-26 12:54:24", "link": "http://arxiv.org/abs/2105.12523v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deception detection in text and its relation to the cultural dimension\n  of individualism/collectivism", "abstract": "Deception detection is a task with many applications both in direct physical\nand in computer-mediated communication. Our focus is on automatic deception\ndetection in text across cultures. We view culture through the prism of the\nindividualism/collectivism dimension and we approximate culture by using\ncountry as a proxy. Having as a starting point recent conclusions drawn from\nthe social psychology discipline, we explore if differences in the usage of\nspecific linguistic features of deception across cultures can be confirmed and\nattributed to norms in respect to the individualism/collectivism divide. We\nalso investigate if a universal feature set for cross-cultural text deception\ndetection tasks exists. We evaluate the predictive power of different feature\nsets and approaches. We create culture/language-aware classifiers by\nexperimenting with a wide range of n-gram features based on phonology,\nmorphology and syntax, other linguistic cues like word and phoneme counts,\npronouns use, etc., and token embeddings. We conducted our experiments over 11\ndatasets from 5 languages i.e., English, Dutch, Russian, Spanish and Romanian,\nfrom six countries (US, Belgium, India, Russia, Mexico and Romania), and we\napplied two classification methods i.e, logistic regression and fine-tuned BERT\nmodels. The results showed that our task is fairly complex and demanding. There\nare indications that some linguistic cues of deception have cultural origins,\nand are consistent in the context of diverse domains and dataset settings for\nthe same language. This is more evident for the usage of pronouns and the\nexpression of sentiment in deceptive language. The results of this work show\nthat the automatic deception detection across cultures and languages cannot be\nhandled in a unified manner, and that such approaches should be augmented with\nknowledge about cultural differences and the domains of interest.", "published": "2021-05-26 13:09:47", "link": "http://arxiv.org/abs/2105.12530v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Model as an Annotator: Exploring DialoGPT for Dialogue\n  Summarization", "abstract": "Current dialogue summarization systems usually encode the text with a number\nof general semantic features (e.g., keywords and topics) to gain more powerful\ndialogue modeling capabilities. However, these features are obtained via\nopen-domain toolkits that are dialog-agnostic or heavily relied on human\nannotations. In this paper, we show how DialoGPT, a pre-trained model for\nconversational response generation, can be developed as an unsupervised\ndialogue annotator, which takes advantage of dialogue background knowledge\nencoded in DialoGPT. We apply DialoGPT to label three types of features on two\ndialogue summarization datasets, SAMSum and AMI, and employ pre-trained and non\npre-trained models as our summarizes. Experimental results show that our\nproposed method can obtain remarkable improvements on both datasets and\nachieves new state-of-the-art performance on the SAMSum dataset.", "published": "2021-05-26 13:50:13", "link": "http://arxiv.org/abs/2105.12544v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prosodic segmentation for parsing spoken dialogue", "abstract": "Parsing spoken dialogue poses unique difficulties, including disfluencies and\nunmarked boundaries between sentence-like units. Previous work has shown that\nprosody can help with parsing disfluent speech (Tran et al. 2018), but has\nassumed that the input to the parser is already segmented into sentence-like\nunits (SUs), which isn't true in existing speech applications. We investigate\nhow prosody affects a parser that receives an entire dialogue turn as input (a\nturn-based model), instead of gold standard pre-segmented SUs (an SU-based\nmodel). In experiments on the English Switchboard corpus, we find that when\nusing transcripts alone, the turn-based model has trouble segmenting SUs,\nleading to worse parse performance than the SU-based model. However, prosody\ncan effectively replace gold standard SU boundaries: with prosody, the\nturn-based model performs as well as the SU-based model (90.79 vs. 90.65 F1\nscore, respectively), despite performing two tasks (SU segmentation and\nparsing) rather than one (parsing alone). Analysis shows that pitch and\nintensity features are the most important for this corpus, since they allow the\nmodel to correctly distinguish an SU boundary from a speech disfluency -- a\ndistinction that the model otherwise struggles to make.", "published": "2021-05-26 16:30:16", "link": "http://arxiv.org/abs/2105.12667v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TexRel: a Green Family of Datasets for Emergent Communications on\n  Relations", "abstract": "We propose a new dataset TexRel as a playground for the study of emergent\ncommunications, in particular for relations. By comparison with other relations\ndatasets, TexRel provides rapid training and experimentation, whilst being\nsufficiently large to avoid overfitting in the context of emergent\ncommunications. By comparison with using symbolic inputs, TexRel provides a\nmore realistic alternative whilst remaining efficient and fast to learn. We\ncompare the performance of TexRel with a related relations dataset Shapeworld.\nWe provide baseline performance results on TexRel for sender architectures,\nreceiver architectures and end-to-end architectures. We examine the effect of\nmultitask learning in the context of shapes, colors and relations on accuracy,\ntopological similarity and clustering precision. We investigate whether\nincreasing the size of the latent meaning space improves metrics of\ncompositionality. We carry out a case-study on using TexRel to reproduce the\nresults of an experiment in a recent paper that used symbolic inputs, but using\nour own non-symbolic inputs, from TexRel, instead.", "published": "2021-05-26 19:45:33", "link": "http://arxiv.org/abs/2105.12804v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERTifying the Hidden Markov Model for Multi-Source Weakly Supervised\n  Named Entity Recognition", "abstract": "We study the problem of learning a named entity recognition (NER) tagger\nusing noisy labels from multiple weak supervision sources. Though cheap to\nobtain, the labels from weak supervision sources are often incomplete,\ninaccurate, and contradictory, making it difficult to learn an accurate NER\nmodel. To address this challenge, we propose a conditional hidden Markov model\n(CHMM), which can effectively infer true labels from multi-source noisy labels\nin an unsupervised way. CHMM enhances the classic hidden Markov model with the\ncontextual representation power of pre-trained language models. Specifically,\nCHMM learns token-wise transition and emission probabilities from the BERT\nembeddings of the input tokens to infer the latent true labels from noisy\nobservations. We further refine CHMM with an alternate-training approach\n(CHMM-ALT). It fine-tunes a BERT-NER model with the labels inferred by CHMM,\nand this BERT-NER's output is regarded as an additional weak source to train\nthe CHMM in return. Experiments on four NER benchmarks from various domains\nshow that our method outperforms state-of-the-art weakly supervised NER models\nby wide margins.", "published": "2021-05-26 21:18:48", "link": "http://arxiv.org/abs/2105.12848v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Information Spreading Mechanisms During COVID-19 Pandemic\n  by Analyzing the Impact of Tweet Text and User Features for Retweet\n  Prediction", "abstract": "COVID-19 has affected the world economy and the daily life routine of almost\neveryone. It has been a hot topic on social media platforms such as Twitter,\nFacebook, etc. These social media platforms enable users to share information\nwith other users who can reshare this information, thus causing this\ninformation to spread. Twitter's retweet functionality allows users to share\nthe existing content with other users without altering the original content.\nAnalysis of social media platforms can help in detecting emergencies during\npandemics that lead to taking preventive measures. One such type of analysis is\npredicting the number of retweets for a given COVID-19 related tweet. Recently,\nCIKM organized a retweet prediction challenge for COVID-19 tweets focusing on\nusing numeric features only. However, our hypothesis is, tweet text may play a\nvital role in an accurate retweet prediction. In this paper, we combine numeric\nand text features for COVID-19 related retweet predictions. For this purpose,\nwe propose two CNN and RNN based models and evaluate the performance of these\nmodels on a publicly available TweetsCOV19 dataset using seven different\nevaluation metrics. Our evaluation results show that combining tweet text with\nnumeric features improves the performance of retweet prediction significantly.", "published": "2021-05-26 15:55:58", "link": "http://arxiv.org/abs/2106.07344v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Improving Sign Language Translation with Monolingual Data by Sign\n  Back-Translation", "abstract": "Despite existing pioneering works on sign language translation (SLT), there\nis a non-trivial obstacle, i.e., the limited quantity of parallel sign-text\ndata. To tackle this parallel data bottleneck, we propose a sign\nback-translation (SignBT) approach, which incorporates massive spoken language\ntexts into SLT training. With a text-to-gloss translation model, we first\nback-translate the monolingual text to its gloss sequence. Then, the paired\nsign sequence is generated by splicing pieces from an estimated gloss-to-sign\nbank at the feature level. Finally, the synthetic parallel data serves as a\nstrong supplement for the end-to-end training of the encoder-decoder SLT\nframework.\n  To promote the SLT research, we further contribute CSL-Daily, a large-scale\ncontinuous SLT dataset. It provides both spoken language translations and\ngloss-level annotations. The topic revolves around people's daily lives (e.g.,\ntravel, shopping, medical care), the most likely SLT application scenario.\nExtensive experimental results and analysis of SLT methods are reported on\nCSL-Daily. With the proposed sign back-translation method, we obtain a\nsubstantial improvement over previous state-of-the-art SLT methods.", "published": "2021-05-26 08:49:30", "link": "http://arxiv.org/abs/2105.12397v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger", "abstract": "Backdoor attacks are a kind of insidious security threat against machine\nlearning models. After being injected with a backdoor in training, the victim\nmodel will produce adversary-specified outputs on the inputs embedded with\npredesigned triggers but behave properly on normal inputs during inference. As\na sort of emergent attack, backdoor attacks in natural language processing\n(NLP) are investigated insufficiently. As far as we know, almost all existing\ntextual backdoor attack methods insert additional contents into normal samples\nas triggers, which causes the trigger-embedded samples to be detected and the\nbackdoor attacks to be blocked without much effort. In this paper, we propose\nto use the syntactic structure as the trigger in textual backdoor attacks. We\nconduct extensive experiments to demonstrate that the syntactic trigger-based\nattack method can achieve comparable attack performance (almost 100% success\nrate) to the insertion-based methods but possesses much higher invisibility and\nstronger resistance to defenses. These results also reveal the significant\ninsidiousness and harmfulness of textual backdoor attacks. All the code and\ndata of this paper can be obtained at https://github.com/thunlp/HiddenKiller.", "published": "2021-05-26 08:54:19", "link": "http://arxiv.org/abs/2105.12400v2", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "LMMS Reloaded: Transformer-based Sense Embeddings for Disambiguation and\n  Beyond", "abstract": "Distributional semantics based on neural approaches is a cornerstone of\nNatural Language Processing, with surprising connections to human meaning\nrepresentation as well. Recent Transformer-based Language Models have proven\ncapable of producing contextual word representations that reliably convey\nsense-specific information, simply as a product of self-supervision. Prior work\nhas shown that these contextual representations can be used to accurately\nrepresent large sense inventories as sense embeddings, to the extent that a\ndistance-based solution to Word Sense Disambiguation (WSD) tasks outperforms\nmodels trained specifically for the task. Still, there remains much to\nunderstand on how to use these Neural Language Models (NLMs) to produce sense\nembeddings that can better harness each NLM's meaning representation abilities.\nIn this work we introduce a more principled approach to leverage information\nfrom all layers of NLMs, informed by a probing analysis on 14 NLM variants. We\nalso emphasize the versatility of these sense embeddings in contrast to\ntask-specific models, applying them on several sense-related tasks, besides\nWSD, while demonstrating improved performance using our proposed approach over\nprior work focused on sense embeddings. Finally, we discuss unexpected findings\nregarding layer and model performance variations, and potential applications\nfor downstream tasks.", "published": "2021-05-26 10:14:22", "link": "http://arxiv.org/abs/2105.12449v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Construction of Sememe Knowledge Bases via Dictionaries", "abstract": "A sememe is defined as the minimum semantic unit in linguistics. Sememe\nknowledge bases (SKBs), which comprise words annotated with sememes, enable\nsememes to be applied to natural language processing. So far a large body of\nresearch has showcased the unique advantages and effectiveness of SKBs in\nvarious tasks. However, most languages have no SKBs, and manual construction of\nSKBs is time-consuming and labor-intensive. To tackle this challenge, we\npropose a simple and fully automatic method of building an SKB via an existing\ndictionary. We use this method to build an English SKB and a French SKB, and\nconduct comprehensive evaluations from both intrinsic and extrinsic\nperspectives. Experimental results demonstrate that the automatically built\nEnglish SKB is even superior to HowNet, the most widely used SKB that takes\ndecades to build manually. And both the English and French SKBs can bring\nobvious performance enhancement in multiple downstream tasks. All the code and\ndata of this paper (except the copyrighted dictionaries) can be obtained at\nhttps://github.com/thunlp/DictSKB.", "published": "2021-05-26 14:41:01", "link": "http://arxiv.org/abs/2105.12585v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero-shot Medical Entity Retrieval without Annotation: Learning From\n  Rich Knowledge Graph Semantics", "abstract": "Medical entity retrieval is an integral component for understanding and\ncommunicating information across various health systems. Current approaches\ntend to work well on specific medical domains but generalize poorly to unseen\nsub-specialties. This is of increasing concern under a public health crisis as\nnew medical conditions and drug treatments come to light frequently. Zero-shot\nretrieval is challenging due to the high degree of ambiguity and variability in\nmedical corpora, making it difficult to build an accurate similarity measure\nbetween mentions and concepts. Medical knowledge graphs (KG), however, contain\nrich semantics including large numbers of synonyms as well as its curated\ngraphical structures. To take advantage of this valuable information, we\npropose a suite of learning tasks designed for training efficient zero-shot\nentity retrieval models. Without requiring any human annotation, our knowledge\ngraph enriched architecture significantly outperforms common zero-shot\nbenchmarks including BM25 and Clinical BERT with 7% to 30% higher recall across\nmultiple major medical ontologies, such as UMLS, SNOMED, and ICD-10.", "published": "2021-05-26 16:53:48", "link": "http://arxiv.org/abs/2105.12682v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Quantifying and Avoiding Unfair Qualification Labour in Crowdsourcing", "abstract": "Extensive work has argued in favour of paying crowd workers a wage that is at\nleast equivalent to the U.S. federal minimum wage. Meanwhile, research on\ncollecting high quality annotations suggests using a qualification that\nrequires workers to have previously completed a certain number of tasks. If\nmost requesters who pay fairly require workers to have completed a large number\nof tasks already then workers need to complete a substantial amount of poorly\npaid work before they can earn a fair wage. Through analysis of worker\ndiscussions and guidance for researchers, we estimate that workers spend\napproximately 2.25 months of full time effort on poorly paid tasks in order to\nget the qualifications needed for better paid tasks. We discuss alternatives to\nthis qualification and conduct a study of the correlation between\nqualifications and work quality on two NLP tasks. We find that it is possible\nto reduce the burden on workers while still collecting high quality data.", "published": "2021-05-26 18:02:39", "link": "http://arxiv.org/abs/2105.12762v1", "categories": ["cs.CL", "cs.HC", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Predict then Interpolate: A Simple Algorithm to Learn Stable Classifiers", "abstract": "We propose Predict then Interpolate (PI), a simple algorithm for learning\ncorrelations that are stable across environments. The algorithm follows from\nthe intuition that when using a classifier trained on one environment to make\npredictions on examples from another environment, its mistakes are informative\nas to which correlations are unstable. In this work, we prove that by\ninterpolating the distributions of the correct predictions and the wrong\npredictions, we can uncover an oracle distribution where the unstable\ncorrelation vanishes. Since the oracle interpolation coefficients are not\naccessible, we use group distributionally robust optimization to minimize the\nworst-case risk across all such interpolations. We evaluate our method on both\ntext classification and image classification. Empirical results demonstrate\nthat our algorithm is able to learn robust classifiers (outperforms IRM by\n23.85% on synthetic environments and 12.41% on natural environments). Our code\nand data are available at https://github.com/YujiaBao/Predict-then-Interpolate.", "published": "2021-05-26 15:37:48", "link": "http://arxiv.org/abs/2105.12628v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "It is rotating leaders who build the swarm: social network determinants\n  of growth for healthcare virtual communities of practice", "abstract": "Purpose: The purpose of this paper is to identify the factors influencing the\ngrowth of healthcare virtual communities of practice (VCoPs) through a\nseven-year longitudinal study conducted using metrics from social-network and\nsemantic analysis. By studying online communication along the three dimensions\nof social interactions (connectivity, interactivity and language use), the\nauthors aim to provide VCoP managers with valuable insights to improve the\nsuccess of their communities. Design/methodology/approach: Communications over\na period of seven years (April 2008 to April 2015) and between 14,000 members\nof 16 different healthcare VCoPs coexisting on the same web platform were\nanalysed. Multilevel regression models were used to reveal the main\ndeterminants of community growth over time. Independent variables were derived\nfrom social network and semantic analysis measures. Findings: Results show that\nstructural and content-based variables predict the growth of the community.\nProgressively, more people will join a community if its structure is more\ncentralised, leaders are more dynamic (they rotate more) and the language used\nin the posts is less complex. Research limitations/implications: The available\ndata set included one Web platform and a limited number of control variables.\nTo consolidate the findings of the present study, the experiment should be\nreplicated on other healthcare VCoPs. Originality/value: The study provides\nuseful recommendations for setting up and nurturing the growth of professional\ncommunities, considering, at the same time, the interaction patterns among the\ncommunity members, the dynamic evolution of these interactions and the use of\nlanguage. New analytical tools are presented, together with the use of\ninnovative interaction metrics, that can significantly influence community\ngrowth, such as rotating leadership.", "published": "2021-05-26 16:15:31", "link": "http://arxiv.org/abs/2105.12659v1", "categories": ["cs.SI", "cs.CL", "physics.soc-ph", "J.4; I.2.7; H.4.0"], "primary_category": "cs.SI"}
{"title": "Multitask Learning for Grapheme-to-Phoneme Conversion of Anglicisms in\n  German Speech Recognition", "abstract": "Anglicisms are a challenge in German speech recognition. Due to their\nirregular pronunciation compared to native German words, automatically\ngenerated pronunciation dictionaries often include faulty phoneme sequences for\nAnglicisms. In this work, we propose a multitask sequence-to-sequence approach\nfor grapheme-to-phoneme conversion to improve the phonetization of Anglicisms.\nWe extended a grapheme-to-phoneme model with a classifier to distinguish\nAnglicisms from native German words. With this approach, the model learns to\ngenerate pronunciations differently depending on the classification result. We\nused our model to create supplementary Anglicism pronunciation dictionaries\nthat are added to an existing German speech recognition model. Tested on a\ndedicated Anglicism evaluation set, we improved the recognition of Anglicisms\ncompared to a baseline model, reducing the word error rate by 1 % and the\nAnglicism error rate by 3 %. We show that multitask learning can help solving\nthe challenge of Anglicisms in German speech recognition.", "published": "2021-05-26 17:42:13", "link": "http://arxiv.org/abs/2105.12708v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Trade the Event: Corporate Events Detection for News-Based Event-Driven\n  Trading", "abstract": "In this paper, we introduce an event-driven trading strategy that predicts\nstock movements by detecting corporate events from news articles. Unlike\nexisting models that utilize textual features (e.g., bag-of-words) and\nsentiments to directly make stock predictions, we consider corporate events as\nthe driving force behind stock movements and aim to profit from the temporary\nstock mispricing that may occur when corporate events take place. The core of\nthe proposed strategy is a bi-level event detection model. The low-level event\ndetector identifies events' existences from each token, while the high-level\nevent detector incorporates the entire article's representation and the\nlow-level detected results to discover events at the article-level. We also\ndevelop an elaborately-annotated dataset EDT for corporate event detection and\nnews-based stock prediction benchmark. EDT includes 9721 news articles with\ntoken-level event labels as well as 303893 news articles with minute-level\ntimestamps and comprehensive stock price labels. Experiments on EDT indicate\nthat the proposed strategy outperforms all the baselines in winning rate,\nexcess returns over the market, and the average return on each transaction.", "published": "2021-05-26 20:39:40", "link": "http://arxiv.org/abs/2105.12825v2", "categories": ["cs.CL", "q-fin.CP", "q-fin.TR"], "primary_category": "cs.CL"}
{"title": "Compensating class imbalance for acoustic chimpanzee detection with\n  convolutional recurrent neural networks", "abstract": "Automatic detection systems are important in passive acoustic monitoring\n(PAM) systems, as these record large amounts of audio data which are infeasible\nfor humans to evaluate manually. In this paper we evaluated methods for\ncompensating class imbalance for deep-learning based automatic detection of\nacoustic chimpanzee calls. The prevalence of chimpanzee calls in natural\nhabitats is very rare, i.e. databases feature a heavy imbalance between\nbackground and target calls. Such imbalances can have negative effects on\nclassifier performances. We employed a state-of-the-art detection approach\nbased on convolutional recurrent neural networks (CRNNs). We extended the\ndetection pipeline through various stages for compensating class imbalance.\nThese included (1) spectrogram denoising, (2) alternative loss functions, and\n(3) resampling. Our key findings are: (1) spectrogram denoising operations\nsignificantly improved performance for both target classes, (2) standard binary\ncross entropy reached the highest performance, and (3) manipulating relative\nclass imbalance through resampling either decreased or maintained performance\ndepending on the target class. Finally, we reached detection performances of\n33% for drumming and 5% for vocalization, which is a >7 fold increase compared\nto previously published results. We conclude that supporting the network to\nlearn decoupling noise conditions from foreground classes is of primary\nimportance for increasing performance.", "published": "2021-05-26 12:03:26", "link": "http://arxiv.org/abs/2105.12502v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploiting Temporal Dependencies for Cross-Modal Music Piece\n  Identification", "abstract": "This paper addresses the problem of cross-modal musical piece identification\nand retrieval: finding the appropriate recording(s) from a database given a\nsheet music query, and vice versa, working directly with audio and scanned\nsheet music images. The fundamental approach to this is to learn a cross-modal\nembedding space with a suitable similarity structure for audio and sheet image\nsnippets, using a deep neural network, and identifying candidate pieces by\ncross-modal near neighbour search in this space. However, this method is\noblivious of temporal aspects of music. In this paper, we introduce two\nstrategies that address this shortcoming. First, we present a strategy that\naligns sequences of embeddings learned from sheet music scans and audio\nsnippets. A series of experiments on whole piece and fragment-level retrieval\non 24 hours worth of classical piano recordings demonstrates significant\nimprovement. Second, we show that the retrieval can be further improved by\nintroducing an attention mechanism to the embedding learning model that reduces\nthe effects of tempo variations in music. To conclude, we assess the\nscalability of our method and discuss potential measures to make it suitable\nfor truly large-scale applications.", "published": "2021-05-26 13:21:47", "link": "http://arxiv.org/abs/2105.12536v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-attending RNN for Speech Enhancement to Improve Cross-corpus\n  Generalization", "abstract": "Deep neural networks (DNNs) represent the mainstream methodology for\nsupervised speech enhancement, primarily due to their capability to model\ncomplex functions using hierarchical representations. However, a recent study\nrevealed that DNNs trained on a single corpus fail to generalize to untrained\ncorpora, especially in low signal-to-noise ratio (SNR) conditions. Developing a\nnoise, speaker, and corpus independent speech enhancement algorithm is\nessential for real-world applications. In this study, we propose a\nself-attending recurrent neural network, or attentive recurrent network (ARN),\nfor time-domain speech enhancement to improve cross-corpus generalization. ARN\ncomprises of recurrent neural networks (RNNs) augmented with self-attention\nblocks and feedforward blocks. We evaluate ARN on different corpora with\nnonstationary noises in low SNR conditions. Experimental results demonstrate\nthat ARN substantially outperforms competitive approaches to time-domain speech\nenhancement, such as RNNs and dual-path ARNs. Additionally, we report an\nimportant finding that the two popular approaches to speech enhancement:\ncomplex spectral mapping and time-domain enhancement, obtain similar results\nfor RNN and ARN with large-scale training. We also provide a challenging subset\nof the test set used in this study for evaluating future algorithms and\nfacilitating direct comparisons.", "published": "2021-05-26 20:48:02", "link": "http://arxiv.org/abs/2105.12831v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Training Speech Enhancement Systems with Noisy Speech Datasets", "abstract": "Recently, deep neural network (DNN)-based speech enhancement (SE) systems\nhave been used with great success. During training, such systems require clean\nspeech data - ideally, in large quantity with a variety of acoustic conditions,\nmany different speaker characteristics and for a given sampling rate (e.g.,\n48kHz for fullband SE). However, obtaining such clean speech data is not\nstraightforward - especially, if only considering publicly available datasets.\nAt the same time, a lot of material for automatic speech recognition (ASR) with\nthe desired acoustic/speaker/sampling rate characteristics is publicly\navailable except being clean, i.e., it also contains background noise as this\nis even often desired in order to have ASR systems that are noise-robust.\nHence, using such data to train SE systems is not straightforward. In this\npaper, we propose two improvements to train SE systems on noisy speech data.\nFirst, we propose several modifications of the loss functions, which make them\nrobust against noisy speech targets. In particular, computing the median over\nthe sample axis before averaging over time-frequency bins allows to use such\ndata. Furthermore, we propose a noise augmentation scheme for mixture-invariant\ntraining (MixIT), which allows using it also in such scenarios. For our\nexperiments, we use the Mozilla Common Voice dataset and we show that using our\nrobust loss function improves PESQ by up to 0.19 compared to a system trained\nin the traditional way. Similarly, for MixIT we can see an improvement of up to\n0.27 in PESQ when using our proposed noise augmentation.", "published": "2021-05-26 03:32:39", "link": "http://arxiv.org/abs/2105.12315v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Receptive Field Regularization Techniques for Audio Classification and\n  Tagging with Deep Convolutional Neural Networks", "abstract": "In this paper, we study the performance of variants of well-known\nConvolutional Neural Network (CNN) architectures on different audio tasks. We\nshow that tuning the Receptive Field (RF) of CNNs is crucial to their\ngeneralization. An insufficient RF limits the CNN's ability to fit the training\ndata. In contrast, CNNs with an excessive RF tend to over-fit the training data\nand fail to generalize to unseen testing data. As state-of-the-art CNN\narchitectures-in computer vision and other domains-tend to go deeper in terms\nof number of layers, their RF size increases and therefore they degrade in\nperformance in several audio classification and tagging tasks. We study\nwell-known CNN architectures and how their building blocks affect their\nreceptive field. We propose several systematic approaches to control the RF of\nCNNs and systematically test the resulting architectures on different audio\nclassification and tagging tasks and datasets. The experiments show that\nregularizing the RF of CNNs using our proposed approaches can drastically\nimprove the generalization of models, out-performing complex architectures and\npre-trained models on larger datasets. The proposed CNNs achieve\nstate-of-the-art results in multiple tasks, from acoustic scene classification\nto emotion and theme detection in music to instrument recognition, as\ndemonstrated by top ranks in several pertinent challenges (DCASE, MediaEval).", "published": "2021-05-26 08:36:29", "link": "http://arxiv.org/abs/2105.12395v1", "categories": ["cs.SD", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
