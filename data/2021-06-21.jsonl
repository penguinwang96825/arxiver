{"title": "Out of Context: A New Clue for Context Modeling of Aspect-based\n  Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) aims to predict the sentiment\nexpressed in a review with respect to a given aspect. The core of ABSA is to\nmodel the interaction between the context and given aspect to extract the\naspect-related information. In prior work, attention mechanisms and dependency\ngraph networks are commonly adopted to capture the relations between the\ncontext and given aspect. And the weighted sum of context hidden states is used\nas the final representation fed to the classifier. However, the information\nrelated to the given aspect may be already discarded and adverse information\nmay be retained in the context modeling processes of existing models. This\nproblem cannot be solved by subsequent modules and there are two reasons:\nfirst, their operations are conducted on the encoder-generated context hidden\nstates, whose value cannot change after the encoder; second, existing encoders\nonly consider the context while not the given aspect. To address this problem,\nwe argue the given aspect should be considered as a new clue out of context in\nthe context modeling process. As for solutions, we design several aspect-aware\ncontext encoders based on different backbones: an aspect-aware LSTM and three\naspect-aware BERTs. They are dedicated to generate aspect-aware hidden states\nwhich are tailored for ABSA task. In these aspect-aware context encoders, the\nsemantics of the given aspect is used to regulate the information flow.\nConsequently, the aspect-related information can be retained and\naspect-irrelevant information can be excluded in the generated hidden states.\nWe conduct extensive experiments on several benchmark datasets with empirical\nanalysis, demonstrating the efficacies and advantages of our proposed\naspect-aware context encoders.", "published": "2021-06-21 02:26:03", "link": "http://arxiv.org/abs/2106.10816v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empower Distantly Supervised Relation Extraction with Collaborative\n  Adversarial Training", "abstract": "With recent advances in distantly supervised (DS) relation extraction (RE),\nconsiderable attention is attracted to leverage multi-instance learning (MIL)\nto distill high-quality supervision from the noisy DS. Here, we go beyond label\nnoise and identify the key bottleneck of DS-MIL to be its low data utilization:\nas high-quality supervision being refined by MIL, MIL abandons a large amount\nof training instances, which leads to a low data utilization and hinders model\ntraining from having abundant supervision. In this paper, we propose\ncollaborative adversarial training to improve the data utilization, which\ncoordinates virtual adversarial training (VAT) and adversarial training (AT) at\ndifferent levels. Specifically, since VAT is label-free, we employ the\ninstance-level VAT to recycle instances abandoned by MIL. Besides, we deploy AT\nat the bag-level to unleash the full potential of the high-quality supervision\ngot by MIL. Our proposed method brings consistent improvements (~ 5 absolute\nAUC score) to the previous state of the art, which verifies the importance of\nthe data utilization issue and the effectiveness of our method.", "published": "2021-06-21 03:54:02", "link": "http://arxiv.org/abs/2106.10835v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CIL: Contrastive Instance Learning Framework for Distantly Supervised\n  Relation Extraction", "abstract": "The journey of reducing noise from distant supervision (DS) generated\ntraining data has been started since the DS was first introduced into the\nrelation extraction (RE) task. For the past decade, researchers apply the\nmulti-instance learning (MIL) framework to find the most reliable feature from\na bag of sentences. Although the pattern of MIL bags can greatly reduce DS\nnoise, it fails to represent many other useful sentence features in the\ndatasets. In many cases, these sentence features can only be acquired by extra\nsentence-level human annotation with heavy costs. Therefore, the performance of\ndistantly supervised RE models is bounded. In this paper, we go beyond typical\nMIL framework and propose a novel contrastive instance learning (CIL)\nframework. Specifically, we regard the initial MIL as the relational triple\nencoder and constraint positive pairs against negative pairs for each instance.\nExperiments demonstrate the effectiveness of our proposed framework, with\nsignificant improvements over the previous methods on NYT10, GDS and KBP.", "published": "2021-06-21 04:51:59", "link": "http://arxiv.org/abs/2106.10855v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ArgFuse: A Weakly-Supervised Framework for Document-Level Event Argument\n  Aggregation", "abstract": "Most of the existing information extraction frameworks (Wadden et al., 2019;\nVeysehet al., 2020) focus on sentence-level tasks and are hardly able to\ncapture the consolidated information from a given document. In our endeavour to\ngenerate precise document-level information frames from lengthy textual\nrecords, we introduce the task of Information Aggregation or Argument\nAggregation. More specifically, our aim is to filter irrelevant and redundant\nargument mentions that were extracted at a sentence level and render a document\nlevel information frame. Majority of the existing works have been observed to\nresolve related tasks of document-level event argument extraction (Yang et al.,\n2018a; Zheng et al., 2019a) and salient entity identification (Jain et\nal.,2020) using supervised techniques. To remove dependency from large amounts\nof labelled data, we explore the task of information aggregation using\nweakly-supervised techniques. In particular, we present an extractive algorithm\nwith multiple sieves which adopts active learning strategies to work\nefficiently in low-resource settings. For this task, we have annotated our own\ntest dataset comprising of 131 document information frames and have released\nthe code and dataset to further research prospects in this new domain. To the\nbest of our knowledge, we are the first to establish baseline results for this\ntask in English. Our data and code are publicly available at\nhttps://github.com/DebanjanaKar/ArgFuse.", "published": "2021-06-21 05:21:27", "link": "http://arxiv.org/abs/2106.10862v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Ad Text Classification with Transformer-Based Natural Language\n  Processing Methods", "abstract": "In this study, a natural language processing-based (NLP-based) method is\nproposed for the sector-wise automatic classification of ad texts created on\nonline advertising platforms. Our data set consists of approximately 21,000\nlabeled advertising texts from 12 different sectors. In the study, the\nBidirectional Encoder Representations from Transformers (BERT) model, which is\na transformer-based language model that is recently used in fields such as text\nclassification in the natural language processing literature, was used. The\nclassification efficiencies obtained using a pre-trained BERT model for the\nTurkish language are shown in detail.", "published": "2021-06-21 07:38:31", "link": "http://arxiv.org/abs/2106.10899v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explicit Interaction Network for Aspect Sentiment Triplet Extraction", "abstract": "Aspect Sentiment Triplet Extraction (ASTE) aims to recognize targets, their\nsentiment polarities and opinions explaining the sentiment from a sentence.\nASTE could be naturally divided into 3 atom subtasks, namely target detection,\nopinion detection and sentiment classification. We argue that the proper\nsubtask combination, compositional feature extraction for target-opinion pairs,\nand interaction between subtasks would be the key to success. Prior work,\nhowever, may fail on `one-to-many' or `many-to-one' situations or derive\nnon-existent sentiment triplets due to defective subtask formulation,\nsub-optimal feature representation or the lack of subtask interaction. In this\npaper, we divide ASTE into target-opinion joint detection and sentiment\nclassification subtasks, which is in line with human cognition, and\ncorrespondingly utilize sequence encoder and table encoder to handle them.\nTable encoder extracts sentiment at token-pair level, so that the compositional\nfeature between targets and opinions can be easily captured. To establish\nexplicit interaction between subtasks, we utilize the table representation to\nguide the sequence encoding, and inject the sequence features back into the\ntable encoder. Experiments show that our model outperforms state-of-the-art\nmethods on six popular ASTE datasets.", "published": "2021-06-21 14:36:38", "link": "http://arxiv.org/abs/2106.11148v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Calibrating Neural-Probabilistic Model for Authorship Verification\n  Under Covariate Shift", "abstract": "We are addressing two fundamental problems in authorship verification (AV):\nTopic variability and miscalibration. Variations in the topic of two disputed\ntexts are a major cause of error for most AV systems. In addition, it is\nobserved that the underlying probability estimates produced by deep learning AV\nmechanisms oftentimes do not match the actual case counts in the respective\ntraining data. As such, probability estimates are poorly calibrated. We are\nexpanding our framework from PAN 2020 to include Bayes factor scoring (BFS) and\nan uncertainty adaptation layer (UAL) to address both problems. Experiments\nwith the 2020/21 PAN AV shared task data show that the proposed method\nsignificantly reduces sensitivities to topical variations and significantly\nimproves the system's calibration.", "published": "2021-06-21 15:33:48", "link": "http://arxiv.org/abs/2106.11196v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Race, Racism, and Anti-Racism in NLP", "abstract": "Despite inextricable ties between race and language, little work has\nconsidered race in NLP research and development. In this work, we survey 79\npapers from the ACL anthology that mention race. These papers reveal various\ntypes of race-related bias in all stages of NLP model development, highlighting\nthe need for proactive consideration of how NLP systems can uphold racial\nhierarchies. However, persistent gaps in research on race and NLP remain: race\nhas been siloed as a niche topic and remains ignored in many NLP tasks; most\nwork operationalizes race as a fixed single-dimensional variable with a\nground-truth label, which risks reinforcing differences produced by historical\nracism; and the voices of historically marginalized people are nearly absent in\nNLP literature. By identifying where and how NLP literature has and has not\nconsidered race, especially in comparison to related fields, our work calls for\ninclusion and racial justice in NLP research practices.", "published": "2021-06-21 20:59:06", "link": "http://arxiv.org/abs/2106.11410v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ROPE: Reading Order Equivariant Positional Encoding for Graph-based\n  Document Information Extraction", "abstract": "Natural reading orders of words are crucial for information extraction from\nform-like documents. Despite recent advances in Graph Convolutional Networks\n(GCNs) on modeling spatial layout patterns of documents, they have limited\nability to capture reading orders of given word-level node representations in a\ngraph. We propose Reading Order Equivariant Positional Encoding (ROPE), a new\npositional encoding technique designed to apprehend the sequential presentation\nof words in documents. ROPE generates unique reading order codes for\nneighboring words relative to the target word given a word-level graph\nconnectivity. We study two fundamental document entity extraction tasks\nincluding word labeling and word grouping on the public FUNSD dataset and a\nlarge-scale payment dataset. We show that ROPE consistently improves existing\nGCNs with a margin up to 8.4% F1-score.", "published": "2021-06-21 00:48:04", "link": "http://arxiv.org/abs/2106.10786v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Does Robustness Improve Fairness? Approaching Fairness with Word\n  Substitution Robustness Methods for Text Classification", "abstract": "Existing bias mitigation methods to reduce disparities in model outcomes\nacross cohorts have focused on data augmentation, debiasing model embeddings,\nor adding fairness-based optimization objectives during training. Separately,\ncertified word substitution robustness methods have been developed to decrease\nthe impact of spurious features and synonym substitutions on model predictions.\nWhile their end goals are different, they both aim to encourage models to make\nthe same prediction for certain changes in the input. In this paper, we\ninvestigate the utility of certified word substitution robustness methods to\nimprove equality of odds and equality of opportunity on multiple text\nclassification tasks. We observe that certified robustness methods improve\nfairness, and using both robustness and bias mitigation methods in training\nresults in an improvement in both fronts", "published": "2021-06-21 03:20:44", "link": "http://arxiv.org/abs/2106.10826v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Pay Better Attention to Attention: Head Selection in Multilingual and\n  Multi-Domain Sequence Modeling", "abstract": "Multi-head attention has each of the attention heads collect salient\ninformation from different parts of an input sequence, making it a powerful\nmechanism for sequence modeling. Multilingual and multi-domain learning are\ncommon scenarios for sequence modeling, where the key challenge is to maximize\npositive transfer and mitigate negative transfer across languages and domains.\nIn this paper, we find that non-selective attention sharing is sub-optimal for\nachieving good generalization across all languages and domains. We further\npropose attention sharing strategies to facilitate parameter sharing and\nspecialization in multilingual and multi-domain sequence modeling. Our approach\nautomatically learns shared and specialized attention heads for different\nlanguages and domains to mitigate their interference. Evaluated in various\ntasks including speech recognition, text-to-text and speech-to-text\ntranslation, the proposed attention sharing strategies consistently bring gains\nto sequence models built upon multi-head attention. For speech-to-text\ntranslation, our approach yields an average of $+2.0$ BLEU over $13$ language\ndirections in multilingual setting and $+2.0$ BLEU over $3$ domains in\nmulti-domain setting.", "published": "2021-06-21 04:03:23", "link": "http://arxiv.org/abs/2106.10840v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Software-Based Dialogue Systems: Survey, Taxonomy and Challenges", "abstract": "The use of natural language interfaces in the field of human-computer\ninteraction is undergoing intense study through dedicated scientific and\nindustrial research. The latest contributions in the field, including deep\nlearning approaches like recurrent neural networks, the potential of\ncontext-aware strategies and user-centred design approaches, have brought back\nthe attention of the community to software-based dialogue systems, generally\nknown as conversational agents or chatbots. Nonetheless, and given the novelty\nof the field, a generic, context-independent overview on the current state of\nresearch of conversational agents covering all research perspectives involved\nis missing. Motivated by this context, this paper reports a survey of the\ncurrent state of research of conversational agents through a systematic\nliterature review of secondary studies. The conducted research is designed to\ndevelop an exhaustive perspective through a clear presentation of the\naggregated knowledge published by recent literature within a variety of\ndomains, research focuses and contexts. As a result, this research proposes a\nholistic taxonomy of the different dimensions involved in the conversational\nagents' field, which is expected to help researchers and to lay the groundwork\nfor future research in the field of natural language interfaces.", "published": "2021-06-21 07:41:44", "link": "http://arxiv.org/abs/2106.10901v2", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "TCIC: Theme Concepts Learning Cross Language and Vision for Image\n  Captioning", "abstract": "Existing research for image captioning usually represents an image using a\nscene graph with low-level facts (objects and relations) and fails to capture\nthe high-level semantics. In this paper, we propose a Theme Concepts extended\nImage Captioning (TCIC) framework that incorporates theme concepts to represent\nhigh-level cross-modality semantics. In practice, we model theme concepts as\nmemory vectors and propose Transformer with Theme Nodes (TTN) to incorporate\nthose vectors for image captioning. Considering that theme concepts can be\nlearned from both images and captions, we propose two settings for their\nrepresentations learning based on TTN. On the vision side, TTN is configured to\ntake both scene graph based features and theme concepts as input for visual\nrepresentation learning. On the language side, TTN is configured to take both\ncaptions and theme concepts as input for text representation re-construction.\nBoth settings aim to generate target captions with the same transformer-based\ndecoder. During the training, we further align representations of theme\nconcepts learned from images and corresponding captions to enforce the\ncross-modality learning. Experimental results on MS COCO show the effectiveness\nof our approach compared to some state-of-the-art models.", "published": "2021-06-21 09:12:55", "link": "http://arxiv.org/abs/2106.10936v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Extractive approach for text summarisation using graphs", "abstract": "Natural language processing is an important discipline with the aim of\nunderstanding text by its digital representation, that due to the diverse way\nwe write and speak, is often not accurate enough. Our paper explores different\ngraph-related algorithms that can be used in solving the text summarization\nproblem using an extractive approach. We consider two metrics: sentence overlap\nand edit distance for measuring sentence similarity.", "published": "2021-06-21 10:03:34", "link": "http://arxiv.org/abs/2106.10955v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interventional Video Grounding with Dual Contrastive Learning", "abstract": "Video grounding aims to localize a moment from an untrimmed video for a given\ntextual query. Existing approaches focus more on the alignment of visual and\nlanguage stimuli with various likelihood-based matching or regression\nstrategies, i.e., P(Y|X). Consequently, these models may suffer from spurious\ncorrelations between the language and video features due to the selection bias\nof the dataset. 1) To uncover the causality behind the model and data, we first\npropose a novel paradigm from the perspective of the causal inference, i.e.,\ninterventional video grounding (IVG) that leverages backdoor adjustment to\ndeconfound the selection bias based on structured causal model (SCM) and\ndo-calculus P(Y|do(X)). Then, we present a simple yet effective method to\napproximate the unobserved confounder as it cannot be directly sampled from the\ndataset. 2) Meanwhile, we introduce a dual contrastive learning approach (DCL)\nto better align the text and video by maximizing the mutual information (MI)\nbetween query and video clips, and the MI between start/end frames of a target\nmoment and the others within a video to learn more informative visual\nrepresentations. Experiments on three standard benchmarks show the\neffectiveness of our approaches. Our code is available on GitHub:\nhttps://github.com/nanguoshun/IVG.", "published": "2021-06-21 12:11:28", "link": "http://arxiv.org/abs/2106.11013v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Learning to Rank Question Answer Pairs with Bilateral Contrastive Data\n  Augmentation", "abstract": "In this work, we propose a novel and easy-to-apply data augmentation\nstrategy, namely Bilateral Generation (BiG), with a contrastive training\nobjective for improving the performance of ranking question answer pairs with\nexisting labeled data. In specific, we synthesize pseudo-positive QA pairs in\ncontrast to the original negative QA pairs with two pre-trained generation\nmodels, one for question generation, the other for answer generation, which are\nfine-tuned on the limited positive QA pairs from the original dataset. With the\naugmented dataset, we design a contrastive training objective for learning to\nrank question answer pairs. Experimental results on three benchmark datasets\nshow that our method significantly improves the performance of ranking models\nby making full use of existing labeled data and can be easily applied to\ndifferent ranking models.", "published": "2021-06-21 13:29:43", "link": "http://arxiv.org/abs/2106.11096v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Iterative Network Pruning with Uncertainty Regularization for Lifelong\n  Sentiment Classification", "abstract": "Lifelong learning capabilities are crucial for sentiment classifiers to\nprocess continuous streams of opinioned information on the Web. However,\nperforming lifelong learning is non-trivial for deep neural networks as\ncontinually training of incrementally available information inevitably results\nin catastrophic forgetting or interference. In this paper, we propose a novel\niterative network pruning with uncertainty regularization method for lifelong\nsentiment classification (IPRLS), which leverages the principles of network\npruning and weight regularization. By performing network pruning with\nuncertainty regularization in an iterative manner, IPRLS can adapta single BERT\nmodel to work with continuously arriving data from multiple domains while\navoiding catastrophic forgetting and interference. Specifically, we leverage an\niterative pruning method to remove redundant parameters in large deep networks\nso that the freed-up space can then be employed to learn new tasks, tackling\nthe catastrophic forgetting problem. Instead of keeping the old-tasks fixed\nwhen learning new tasks, we also use an uncertainty regularization based on the\nBayesian online learning framework to constrain the update of old tasks weights\nin BERT, which enables positive backward transfer, i.e. learning new tasks\nimproves performance on past tasks while protecting old knowledge from being\nlost. In addition, we propose a task-specific low-dimensional residual function\nin parallel to each layer of BERT, which makes IPRLS less prone to losing the\nknowledge saved in the base BERT network when learning a new task. Extensive\nexperiments on 16 popular review corpora demonstrate that the proposed IPRLS\nmethod sig-nificantly outperforms the strong baselines for lifelong sentiment\nclassification. For reproducibility, we submit the code and data\nat:https://github.com/siat-nlp/IPRLS.", "published": "2021-06-21 15:34:13", "link": "http://arxiv.org/abs/2106.11197v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Discriminative Entity-Aware Language Model for Virtual Assistants", "abstract": "High-quality automatic speech recognition (ASR) is essential for virtual\nassistants (VAs) to work well. However, ASR often performs poorly on VA\nrequests containing named entities. In this work, we start from the observation\nthat many ASR errors on named entities are inconsistent with real-world\nknowledge. We extend previous discriminative n-gram language modeling\napproaches to incorporate real-world knowledge from a Knowledge Graph (KG),\nusing features that capture entity type-entity and entity-entity relationships.\nWe apply our model through an efficient lattice rescoring process, achieving\nrelative sentence error rate reductions of more than 25% on some synthesized\ntest sets covering less popular entities, with minimal degradation on a\nuniformly sampled VA test set.", "published": "2021-06-21 17:50:28", "link": "http://arxiv.org/abs/2106.11292v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Phrase-level Active Learning for Neural Machine Translation", "abstract": "Neural machine translation (NMT) is sensitive to domain shift. In this paper,\nwe address this problem in an active learning setting where we can spend a\ngiven budget on translating in-domain data, and gradually fine-tune a\npre-trained out-of-domain NMT model on the newly translated data. Existing\nactive learning methods for NMT usually select sentences based on uncertainty\nscores, but these methods require costly translation of full sentences even\nwhen only one or two key phrases within the sentence are informative. To\naddress this limitation, we re-examine previous work from the phrase-based\nmachine translation (PBMT) era that selected not full sentences, but rather\nindividual phrases. However, while incorporating these phrases into PBMT\nsystems was relatively simple, it is less trivial for NMT systems, which need\nto be trained on full sequences to capture larger structural properties of\nsentences unique to the new domain. To overcome these hurdles, we propose to\nselect both full sentences and individual phrases from unlabelled data in the\nnew domain for routing to human translators. In a German-English translation\ntask, our active learning approach achieves consistent improvements over\nuncertainty-based sentence selection methods, improving up to 1.2 BLEU score\nover strong active learning baselines.", "published": "2021-06-21 19:20:42", "link": "http://arxiv.org/abs/2106.11375v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How well do you know your summarization datasets?", "abstract": "State-of-the-art summarization systems are trained and evaluated on massive\ndatasets scraped from the web. Despite their prevalence, we know very little\nabout the underlying characteristics (data noise, summarization complexity,\netc.) of these datasets, and how these affect system performance and the\nreliability of automatic metrics like ROUGE. In this study, we manually analyze\n600 samples from three popular summarization datasets. Our study is driven by a\nsix-class typology which captures different noise types (missing facts,\nentities) and degrees of summarization difficulty (extractive, abstractive). We\nfollow with a thorough analysis of 27 state-of-the-art summarization models and\n5 popular metrics, and report our key insights: (1) Datasets have distinct data\nquality and complexity distributions, which can be traced back to their\ncollection process. (2) The performance of models and reliability of metrics is\ndependent on sample complexity. (3) Faithful summaries often receive low scores\nbecause of the poor diversity of references. We release the code, annotated\ndata and model outputs.", "published": "2021-06-21 19:44:06", "link": "http://arxiv.org/abs/2106.11388v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Learning Models in Detection of Dietary Supplement Adverse Event\n  Signals from Twitter", "abstract": "Objective: The objective of this study is to develop a deep learning pipeline\nto detect signals on dietary supplement-related adverse events (DS AEs) from\nTwitter. Material and Methods: We obtained 247,807 tweets ranging from 2012 to\n2018 that mentioned both DS and AE. We annotated biomedical entities and\nrelations on 2,000 randomly selected tweets. For the concept extraction task,\nwe compared the performance of traditional word embeddings with SVM, CRF and\nLSTM-CRF classifiers to BERT models. For the relation extraction task, we\ncompared GloVe vectors with CNN classifiers to BERT models. We chose the best\nperforming models in each task to assemble an end-to-end deep learning pipeline\nto detect DS AE signals and compared the results to the known DS AEs from a DS\nknowledge base (i.e., iDISK). Results: In both tasks, the BERT-based models\noutperformed traditional word embeddings. The best performing concept\nextraction model is the BioBERT model that can identify supplement, symptom,\nand body organ entities with F1-scores of 0.8646, 0.8497, and 0.7104,\nrespectively. The best performing relation extraction model is the BERT model\nthat can identify purpose and AE relations with F1-scores of 0.8335 and 0.7538,\nrespectively. The end-to-end pipeline was able to extract DS indication and DS\nAEs with an F1-score of 0.7459 and 0,7414, respectively. Comparing to the\niDISK, we could find both known and novel DS-AEs. Conclusion: We have\ndemonstrated the feasibility of detecting DS AE signals from Twitter with a\nBioBERT-based deep learning pipeline.", "published": "2021-06-21 20:35:01", "link": "http://arxiv.org/abs/2106.11403v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Non-native English lexicon creation for bilingual speech synthesis", "abstract": "Bilingual English speakers speak English as one of their languages. Their\nEnglish is of a non-native kind, and their conversations are of a code-mixed\nfashion. The intelligibility of a bilingual text-to-speech (TTS) system for\nsuch non-native English speakers depends on a lexicon that captures the phoneme\nsequence used by non-native speakers. However, due to the lack of non-native\nEnglish lexicon, existing bilingual TTS systems employ native English lexicons\nthat are widely available, in addition to their native language lexicon. Due to\nthe inconsistency between the non-native English pronunciation in the audio and\nnative English lexicon in the text, the intelligibility of synthesized speech\nin such TTS systems is significantly reduced.\n  This paper is motivated by the knowledge that the native language of the\nspeaker highly influences non-native English pronunciation. We propose a\ngeneric approach to obtain rules based on letter to phoneme alignment to map\nnative English lexicon to their non-native version. The effectiveness of such\nmapping is studied by comparing bilingual (Indian English and Hindi) TTS\nsystems trained with and without the proposed rules. The subjective evaluation\nshows that the bilingual TTS system trained with the proposed non-native\nEnglish lexicon rules obtains a 6% absolute improvement in preference.", "published": "2021-06-21 06:07:14", "link": "http://arxiv.org/abs/2106.10870v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "STEP-EZ: Syntax Tree guided semantic ExPlanation for Explainable\n  Zero-shot modeling of clinical depression symptoms from text", "abstract": "We focus on exploring various approaches of Zero-Shot Learning (ZSL) and\ntheir explainability for a challenging yet important supervised learning task\nnotorious for training data scarcity, i.e. Depression Symptoms Detection (DSD)\nfrom text. We start with a comprehensive synthesis of different components of\nour ZSL modeling and analysis of our ground truth samples and Depression\nsymptom clues curation process with the help of a practicing clinician. We next\nanalyze the accuracy of various state-of-the-art ZSL models and their potential\nenhancements for our task. Further, we sketch a framework for the use of ZSL\nfor hierarchical text-based explanation mechanism, which we call, Syntax\nTree-Guided Semantic Explanation (STEP). Finally, we summarize experiments from\nwhich we conclude that we can use ZSL models and achieve reasonable accuracy\nand explainability, measured by a proposed Explainability Index (EI). This work\nis, to our knowledge, the first work to exhaustively explore the efficacy of\nZSL models for DSD task, both in terms of accuracy and explainability.", "published": "2021-06-21 08:57:22", "link": "http://arxiv.org/abs/2106.10928v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Abstract Geometrical Computation 11: Slanted Firing Squad\n  Synchronisation on Signal Machines", "abstract": "Firing Squad Synchronisation on Cellular Automata is the dynamical\nsynchronisation of finitely many cells without any prior knowledge of their\nrange. This can be conceived as a signal with an infinite speed. Most of the\nproposed constructions naturally translate to the continuous setting of signal\nmachines and generate fractal figures with an accumulation on a horizontal\nline, i.e. synchronously, in the space-time diagram. Signal machines are\nstudied in a series of articles named Abstract Geometrical Computation.\n  In the present article, we design a signal machine that is able to\nsynchronise/accumulate on any non-infinite slope. The slope is encoded in the\ninitial configuration. This is done by constructing an infinite tree such that\neach node computes the way the tree expands.\n  The interest of Abstract Geometrical computation is to do away with the\nconstraint of discrete space, while tackling new difficulties from continuous\nspace. The interest of this paper in particular is to provide basic tools for\nfurther study of computable accumulation lines in the signal machine model.", "published": "2021-06-21 15:15:01", "link": "http://arxiv.org/abs/2106.11176v1", "categories": ["cs.DM", "cs.CL", "math.DS"], "primary_category": "cs.DM"}
{"title": "Dive into Deep Learning", "abstract": "This open-source book represents our attempt to make deep learning\napproachable, teaching readers the concepts, the context, and the code. The\nentire book is drafted in Jupyter notebooks, seamlessly integrating exposition\nfigures, math, and interactive examples with self-contained code. Our goal is\nto offer a resource that could (i) be freely available for everyone; (ii) offer\nsufficient technical depth to provide a starting point on the path to actually\nbecoming an applied machine learning scientist; (iii) include runnable code,\nshowing readers how to solve problems in practice; (iv) allow for rapid\nupdates, both by us and also by the community at large; (v) be complemented by\na forum for interactive discussion of technical details and to answer\nquestions.", "published": "2021-06-21 18:19:46", "link": "http://arxiv.org/abs/2106.11342v5", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Membership Inference on Word Embedding and Beyond", "abstract": "In the text processing context, most ML models are built on word embeddings.\nThese embeddings are themselves trained on some datasets, potentially\ncontaining sensitive data. In some cases this training is done independently,\nin other cases, it occurs as part of training a larger, task-specific model. In\neither case, it is of interest to consider membership inference attacks based\non the embedding layer as a way of understanding sensitive information leakage.\nBut, somewhat surprisingly, membership inference attacks on word embeddings and\ntheir effect in other natural language processing (NLP) tasks that use these\nembeddings, have remained relatively unexplored.\n  In this work, we show that word embeddings are vulnerable to black-box\nmembership inference attacks under realistic assumptions. Furthermore, we show\nthat this leakage persists through two other major NLP applications:\nclassification and text-generation, even when the embedding layer is not\nexposed to the attacker. We show that our MI attack achieves high attack\naccuracy against a classifier model and an LSTM-based language model. Indeed,\nour attack is a cheaper membership inference attack on text-generative models,\nwhich does not require the knowledge of the target model or any expensive\ntraining of text-generative models as shadow models.", "published": "2021-06-21 19:37:06", "link": "http://arxiv.org/abs/2106.11384v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Incremental Deep Neural Network Learning using Classification Confidence\n  Thresholding", "abstract": "Most modern neural networks for classification fail to take into account the\nconcept of the unknown. Trained neural networks are usually tested in an\nunrealistic scenario with only examples from a closed set of known classes. In\nan attempt to develop a more realistic model, the concept of working in an open\nset environment has been introduced. This in turn leads to the concept of\nincremental learning where a model with its own architecture and initial\ntrained set of data can identify unknown classes during the testing phase and\nautonomously update itself if evidence of a new class is detected. Some\nproblems that arise in incremental learning are inefficient use of resources to\nretrain the classifier repeatedly and the decrease of classification accuracy\nas multiple classes are added over time. This process of instantiating new\nclasses is repeated as many times as necessary, accruing errors. To address\nthese problems, this paper proposes the Classification Confidence Threshold\napproach to prime neural networks for incremental learning to keep accuracies\nhigh by limiting forgetting. A lean method is also used to reduce resources\nused in the retraining of the neural network. The proposed method is based on\nthe idea that a network is able to incrementally learn a new class even when\nexposed to a limited number samples associated with the new class. This method\ncan be applied to most existing neural networks with minimal changes to network\narchitecture.", "published": "2021-06-21 22:46:28", "link": "http://arxiv.org/abs/2106.11437v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "MeshRIR: A Dataset of Room Impulse Responses on Meshed Grid Points For\n  Evaluating Sound Field Analysis and Synthesis Methods", "abstract": "A new impulse response (IR) dataset called \"MeshRIR\" is introduced. Currently\navailable datasets usually include IRs at an array of microphones from several\nsource positions under various room conditions, which are basically designed\nfor evaluating speech enhancement and distant speech recognition methods. On\nthe other hand, methods of estimating or controlling spatial sound fields have\nbeen extensively investigated in recent years; however, the current IR datasets\nare not applicable to validating and comparing these methods because of the low\nspatial resolution of measurement points. MeshRIR consists of IRs measured at\npositions obtained by finely discretizing a spatial region. Two subdatasets are\ncurrently available: one consists of IRs in a three-dimensional cuboidal region\nfrom a single source, and the other consists of IRs in a two-dimensional square\nregion from an array of 32 sources. Therefore, MeshRIR is suitable for\nevaluating sound field analysis and synthesis methods. This dataset is freely\navailable at https://sh01k.github.io/MeshRIR/ with some codes of sample\napplications.", "published": "2021-06-21 01:35:14", "link": "http://arxiv.org/abs/2106.10801v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Ensemble of ACCDOA- and EINV2-based Systems with D3Nets and Impulse\n  Response Simulation for Sound Event Localization and Detection", "abstract": "This report describes our systems submitted to the DCASE2021 challenge task\n3: sound event localization and detection (SELD) with directional interference.\nOur previous system based on activity-coupled Cartesian direction of arrival\n(ACCDOA) representation enables us to solve a SELD task with a single target.\nThis ACCDOA-based system with efficient network architecture called RD3Net and\ndata augmentation techniques outperformed state-of-the-art SELD systems in\nterms of localization and location-dependent detection. Using the ACCDOA-based\nsystem as a base, we perform model ensembles by averaging outputs of several\nsystems trained with different conditions such as input features, training\nfolds, and model architectures. We also use the event independent network v2\n(EINV2)-based system to increase the diversity of the model ensembles. To\ngeneralize the models, we further propose impulse response simulation (IRS),\nwhich generates simulated multi-channel signals by convolving simulated room\nimpulse responses (RIRs) with source signals extracted from the original\ndataset. Our systems significantly improved over the baseline system on the\ndevelopment dataset.", "published": "2021-06-21 01:52:45", "link": "http://arxiv.org/abs/2106.10806v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Controllable Context-aware Conversational Speech Synthesis", "abstract": "In spoken conversations, spontaneous behaviors like filled pause and\nprolongations always happen. Conversational partner tends to align features of\ntheir speech with their interlocutor which is known as entrainment. To produce\nhuman-like conversations, we propose a unified controllable spontaneous\nconversational speech synthesis framework to model the above two phenomena.\nSpecifically, we use explicit labels to represent two typical spontaneous\nbehaviors filled-pause and prolongation in the acoustic model and develop a\nneural network based predictor to predict the occurrences of the two behaviors\nfrom text. We subsequently develop an algorithm based on the predictor to\ncontrol the occurrence frequency of the behaviors, making the synthesized\nspeech vary from less disfluent to more disfluent. To model the speech\nentrainment at acoustic level, we utilize a context acoustic encoder to extract\na global style embedding from the previous speech conditioning on the\nsynthesizing of current speech. Furthermore, since the current and previous\nutterances belong to the different speakers in a conversation, we add a domain\nadversarial training module to eliminate the speaker-related information in the\nacoustic encoder while maintaining the style-related information. Experiments\nshow that our proposed approach can synthesize realistic conversations and\ncontrol the occurrences of the spontaneous behaviors naturally.", "published": "2021-06-21 03:36:14", "link": "http://arxiv.org/abs/2106.10828v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Glow-WaveGAN: Learning Speech Representations from GAN-based Variational\n  Auto-Encoder For High Fidelity Flow-based Speech Synthesis", "abstract": "Current two-stage TTS framework typically integrates an acoustic model with a\nvocoder -- the acoustic model predicts a low resolution intermediate\nrepresentation such as Mel-spectrum while the vocoder generates waveform from\nthe intermediate representation. Although the intermediate representation is\nserved as a bridge, there still exists critical mismatch between the acoustic\nmodel and the vocoder as they are commonly separately learned and work on\ndifferent distributions of representation, leading to inevitable artifacts in\nthe synthesized speech. In this work, different from using pre-designed\nintermediate representation in most previous studies, we propose to use VAE\ncombining with GAN to learn a latent representation directly from speech and\nthen utilize a flow-based acoustic model to model the distribution of the\nlatent representation from text. In this way, the mismatch problem is migrated\nas the two stages work on the same distribution. Results demonstrate that the\nflow-based acoustic model can exactly model the distribution of our learned\nspeech representation and the proposed TTS framework, namely Glow-WaveGAN, can\nproduce high fidelity speech outperforming the state-of-the-art GAN-based\nmodel.", "published": "2021-06-21 03:44:51", "link": "http://arxiv.org/abs/2106.10831v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech prosody and remote experiments: a technical report", "abstract": "The aim of this paper is twofold. First, we present a review of different\nrecording options for gathering prosodic data in the event that fieldwork is\nimpracticable (e.g. due to pandemics). Under this light, we mimic a\nlong-distance reading task experiment using different software and hardware\nsynchronously. In order to evaluate the employed methodologies, we extract\nnoise levels and frequency manipulation of the recordings. Subsequently, we\nexamine the impact of the different recordings onto linguistic variables, such\nas the pitch curves and values. We also include a discussion on experimental\npracticalities. After balancing these factors, we decree an online platform,\nZencastr, as the most affordable and practical for acoustic data collection.\nSecondly, we want to open up a debate on the most optimal remote methodology\nthat researchers on speech prosody can deploy.", "published": "2021-06-21 08:26:47", "link": "http://arxiv.org/abs/2106.10915v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards sound based testing of COVID-19 -- Summary of the first\n  Diagnostics of COVID-19 using Acoustics (DiCOVA) Challenge", "abstract": "The technology development for point-of-care tests (POCTs) targeting\nrespiratory diseases has witnessed a growing demand in the recent past.\nInvestigating the presence of acoustic biomarkers in modalities such as cough,\nbreathing and speech sounds, and using them for building POCTs can offer fast,\ncontactless and inexpensive testing. In view of this, over the past year, we\nlaunched the ``Coswara'' project to collect cough, breathing and speech sound\nrecordings via worldwide crowdsourcing. With this data, a call for development\nof diagnostic tools was announced in the Interspeech 2021 as a special session\ntitled ``Diagnostics of COVID-19 using Acoustics (DiCOVA) Challenge''. The goal\nwas to bring together researchers and practitioners interested in developing\nacoustics-based COVID-19 POCTs by enabling them to work on the same set of\ndevelopment and test datasets. As part of the challenge, datasets with\nbreathing, cough, and speech sound samples from COVID-19 and non-COVID-19\nindividuals were released to the participants. The challenge consisted of two\ntracks. The Track-1 focused only on cough sounds, and participants competed in\na leaderboard setting. In Track-2, breathing and speech samples were provided\nfor the participants, without a competitive leaderboard. The challenge\nattracted 85 plus registrations with 29 final submissions for Track-1. This\npaper describes the challenge (datasets, tasks, baseline system), and presents\na focused summary of the various systems submitted by the participating teams.\nAn analysis of the results from the top four teams showed that a fusion of the\nscores from these teams yields an area-under-the-curve of 95.1% on the blind\ntest data. By summarizing the lessons learned, we foresee the challenge\noverview in this paper to help accelerate technology for acoustic-based POCTs.", "published": "2021-06-21 11:30:05", "link": "http://arxiv.org/abs/2106.10997v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Attention-based cross-modal fusion for audio-visual voice activity\n  detection in musical video streams", "abstract": "Many previous audio-visual voice-related works focus on speech, ignoring the\nsinging voice in the growing number of musical video streams on the Internet.\nFor processing diverse musical video data, voice activity detection is a\nnecessary step. This paper attempts to detect the speech and singing voices of\ntarget performers in musical video streams using audiovisual information. To\nintegrate information of audio and visual modalities, a multi-branch network is\nproposed to learn audio and image representations, and the representations are\nfused by attention based on semantic similarity to shape the acoustic\nrepresentations through the probability of anchor vocalization. Experiments\nshow the proposed audio-visual multi-branch network far outperforms the\naudio-only model in challenging acoustic environments, indicating the\ncross-modal information fusion based on semantic correlation is sensible and\nsuccessful.", "published": "2021-06-21 20:59:26", "link": "http://arxiv.org/abs/2106.11411v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Computational Pronunciation Analysis in Sung Utterances", "abstract": "Recent automatic lyrics transcription (ALT) approaches focus on building\nstronger acoustic models or in-domain language models, while the pronunciation\naspect is seldom touched upon. This paper applies a novel computational\nanalysis on the pronunciation variances in sung utterances and further proposes\na new pronunciation model adapted for singing. The singing-adapted model is\ntested on multiple public datasets via word recognition experiments. It\nperforms better than the standard speech dictionary in all settings reporting\nthe best results on ALT in a capella recordings using n-gram language models.\nFor reproducibility, we share the sentence-level annotations used in testing,\nproviding a new benchmark evaluation set for ALT.", "published": "2021-06-21 10:53:47", "link": "http://arxiv.org/abs/2106.10977v1", "categories": ["cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "EML Online Speech Activity Detection for the Fearless Steps Challenge\n  Phase-III", "abstract": "Speech Activity Detection (SAD), locating speech segments within an audio\nrecording, is a main part of most speech technology applications. Robust SAD is\nusually more difficult in noisy conditions with varying signal-to-noise ratios\n(SNR). The Fearless Steps challenge has recently provided such data from the\nNASA Apollo-11 mission for different speech processing tasks including SAD.\nMost audio recordings are degraded by different kinds and levels of noise\nvarying within and between channels. This paper describes the EML online\nalgorithm for the most recent phase of this challenge. The proposed algorithm\ncan be trained both in a supervised and unsupervised manner and assigns speech\nand non-speech labels at runtime approximately every 0.1 sec. The experimental\nresults show a competitive accuracy on both development and evaluation datasets\nwith a real-time factor of about 0.002 using a single CPU machine.", "published": "2021-06-21 12:55:51", "link": "http://arxiv.org/abs/2106.11075v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Affinity Mixup for Weakly Supervised Sound Event Detection", "abstract": "The weakly supervised sound event detection problem is the task of predicting\nthe presence of sound events and their corresponding starting and ending points\nin a weakly labeled dataset. A weak dataset associates each training sample (a\nshort recording) to one or more present sources. Networks that solely rely on\nconvolutional and recurrent layers cannot directly relate multiple frames in a\nrecording. Motivated by attention and graph neural networks, we introduce the\nconcept of an affinity mixup to incorporate time-level similarities and make a\nconnection between frames. This regularization technique mixes up features in\ndifferent layers using an adaptive affinity matrix. Our proposed affinity mixup\nnetwork improves over state-of-the-art techniques event-F1 scores by $8.2\\%$.", "published": "2021-06-21 16:25:09", "link": "http://arxiv.org/abs/2106.11233v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Do sound event representations generalize to other audio tasks? A case\n  study in audio transfer learning", "abstract": "Transfer learning is critical for efficient information transfer across\nmultiple related learning problems. A simple, yet effective transfer learning\napproach utilizes deep neural networks trained on a large-scale task for\nfeature extraction. Such representations are then used to learn related\ndownstream tasks. In this paper, we investigate transfer learning capacity of\naudio representations obtained from neural networks trained on a large-scale\nsound event detection dataset. We build and evaluate these representations\nacross a wide range of other audio tasks, via a simple linear classifier\ntransfer mechanism. We show that such simple linear transfer is already\npowerful enough to achieve high performance on the downstream tasks. We also\nprovide insights into the attributes of sound event representations that enable\nsuch efficient information transfer.", "published": "2021-06-21 18:04:59", "link": "http://arxiv.org/abs/2106.11335v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "UniTTS: Residual Learning of Unified Embedding Space for Speech Style\n  Control", "abstract": "We propose a novel high-fidelity expressive speech synthesis model, UniTTS,\nthat learns and controls overlapping style attributes avoiding interference.\nUniTTS represents multiple style attributes in a single unified embedding space\nby the residuals between the phoneme embeddings before and after applying the\nattributes. The proposed method is especially effective in controlling multiple\nattributes that are difficult to separate cleanly, such as speaker ID and\nemotion, because it minimizes redundancy when adding variance in speaker ID and\nemotion, and additionally, predicts duration, pitch, and energy based on the\nspeaker ID and emotion. In experiments, the visualization results exhibit that\nthe proposed methods learned multiple attributes harmoniously in a manner that\ncan be easily separated again. As well, UniTTS synthesized high-fidelity speech\nsignals controlling multiple style attributes. The synthesized speech samples\nare presented at\nhttps://anonymous-authors2022.github.io/paper_works/UniTTS/demos/.", "published": "2021-06-21 15:07:09", "link": "http://arxiv.org/abs/2106.11171v3", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
