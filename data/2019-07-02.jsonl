{"title": "Neural Machine Reading Comprehension: Methods and Trends", "abstract": "Machine reading comprehension (MRC), which requires a machine to answer\nquestions based on a given context, has attracted increasing attention with the\nincorporation of various deep-learning techniques over the past few years.\nAlthough research on MRC based on deep learning is flourishing, there remains a\nlack of a comprehensive survey summarizing existing approaches and recent\ntrends, which motivated the work presented in this article. Specifically, we\ngive a thorough review of this research field, covering different aspects\nincluding (1) typical MRC tasks: their definitions, differences, and\nrepresentative datasets; (2) the general architecture of neural MRC: the main\nmodules and prevalent approaches to each; and (3) new trends: some emerging\nareas in neural MRC as well as the corresponding challenges. Finally,\nconsidering what has been achieved so far, the survey also envisages what the\nfuture may hold by discussing the open issues left to be addressed.", "published": "2019-07-02 01:14:13", "link": "http://arxiv.org/abs/1907.01118v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue\n  Systems", "abstract": "Developing Video-Grounded Dialogue Systems (VGDS), where a dialogue is\nconducted based on visual and audio aspects of a given video, is significantly\nmore challenging than traditional image or text-grounded dialogue systems\nbecause (1) feature space of videos span across multiple picture frames, making\nit difficult to obtain semantic information; and (2) a dialogue agent must\nperceive and process information from different modalities (audio, video,\ncaption, etc.) to obtain a comprehensive understanding. Most existing work is\nbased on RNNs and sequence-to-sequence architectures, which are not very\neffective for capturing complex long-term dependencies (like in videos). To\novercome this, we propose Multimodal Transformer Networks (MTN) to encode\nvideos and incorporate information from different modalities. We also propose\nquery-aware attention through an auto-encoder to extract query-aware features\nfrom non-text modalities. We develop a training procedure to simulate\ntoken-level decoding to improve the quality of generated responses during\ninference. We get state of the art performance on Dialogue System Technology\nChallenge 7 (DSTC7). Our model also generalizes to another multimodal\nvisual-grounded dialogue task, and obtains promising performance. We\nimplemented our models using PyTorch and the code is released at\nhttps://github.com/henryhungle/MTN.", "published": "2019-07-02 04:54:17", "link": "http://arxiv.org/abs/1907.01166v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discourse Understanding and Factual Consistency in Abstractive\n  Summarization", "abstract": "We introduce a general framework for abstractive summarization with factual\nconsistency and distinct modeling of the narrative flow in an output summary.\nOur work addresses current limitations of models for abstractive summarization\nthat often hallucinate information or generate summaries with coherence issues.\n  To generate abstractive summaries with factual consistency and narrative\nflow, we propose Cooperative Generator -- Discriminator Networks (Co-opNet), a\nnovel transformer-based framework where a generator works with a discriminator\narchitecture to compose coherent long-form summaries. We explore four different\ndiscriminator objectives which each capture a different aspect of coherence,\nincluding whether salient spans of generated abstracts are hallucinated or\nappear in the input context, and the likelihood of sentence adjacency in\ngenerated abstracts. We measure the ability of Co-opNet to learn these\nobjectives with arXiv scientific papers, using the abstracts as a proxy for\ngold long-form scientific article summaries. Empirical results from automatic\nand human evaluations demonstrate that Co-opNet learns to summarize with\nconsiderably improved global coherence compared to competitive baselines.", "published": "2019-07-02 10:06:33", "link": "http://arxiv.org/abs/1907.01272v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Robustness in Real-World Neural Machine Translation Engines", "abstract": "As a commercial provider of machine translation, we are constantly training\nengines for a variety of uses, languages, and content types. In each case,\nthere can be many variables, such as the amount of training data available, and\nthe quality requirements of the end user. These variables can have an impact on\nthe robustness of Neural MT engines. On the whole, Neural MT cures many ills of\nother MT paradigms, but at the same time, it has introduced a new set of\nchallenges to address. In this paper, we describe some of the specific issues\nwith practical NMT and the approaches we take to improve model robustness in\nreal-world scenarios.", "published": "2019-07-02 10:11:23", "link": "http://arxiv.org/abs/1907.01279v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Latent Dirichlet Allocation Based Acoustic Data Selection for Automatic\n  Speech Recognition", "abstract": "Selecting in-domain data from a large pool of diverse and out-of-domain data\nis a non-trivial problem. In most cases simply using all of the available data\nwill lead to sub-optimal and in some cases even worse performance compared to\ncarefully selecting a matching set. This is true even for data-inefficient\nneural models. Acoustic Latent Dirichlet Allocation (aLDA) is shown to be\nuseful in a variety of speech technology related tasks, including domain\nadaptation of acoustic models for automatic speech recognition and entity\nlabeling for information retrieval. In this paper we propose to use aLDA as a\ndata similarity criterion in a data selection framework. Given a large pool of\nout-of-domain and potentially mismatched data, the task is to select the\nbest-matching training data to a set of representative utterances sampled from\na target domain. Our target data consists of around 32 hours of meeting data\n(both far-field and close-talk) and the pool contains 2k hours of meeting,\ntalks, voice search, dictation, command-and-control, audio books, lectures,\ngeneric media and telephony speech data. The proposed technique for training\ndata selection, significantly outperforms random selection, posterior-based\nselection as well as using all of the available data.", "published": "2019-07-02 11:33:52", "link": "http://arxiv.org/abs/1907.01302v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing large scale biomedical knowledge bases from scratch with\n  rapid annotation of interpretable patterns", "abstract": "Knowledge base construction is crucial for summarising, understanding and\ninferring relationships between biomedical entities. However, for many\npractical applications such as drug discovery, the scarcity of relevant facts\n(e.g. gene X is therapeutic target for disease Y) severely limits a domain\nexpert's ability to create a usable knowledge base, either directly or by\ntraining a relation extraction model.\n  In this paper, we present a simple and effective method of extracting new\nfacts with a pre-specified binary relationship type from the biomedical\nliterature, without requiring any training data or hand-crafted rules. Our\nsystem discovers, ranks and presents the most salient patterns to domain\nexperts in an interpretable form. By marking patterns as compatible with the\ndesired relationship type, experts indirectly batch-annotate candidate pairs\nwhose relationship is expressed with such patterns in the literature. Even with\na complete absence of seed data, experts are able to discover thousands of\nhigh-quality pairs with the desired relationship within minutes. When a small\nnumber of relevant pairs do exist - even when their relationship is more\ngeneral (e.g. gene X is biologically associated with disease Y) than the\nrelationship of interest - our system leverages them in order to i) learn a\nbetter ranking of the patterns to be annotated or ii) generate weakly labelled\npairs in a fully automated manner.\n  We evaluate our method both intrinsically and via a downstream knowledge base\ncompletion task, and show that it is an effective way of constructing knowledge\nbases when few or no relevant facts are already available.", "published": "2019-07-02 14:53:30", "link": "http://arxiv.org/abs/1907.01417v2", "categories": ["cs.CL", "H.3.1; J.3"], "primary_category": "cs.CL"}
{"title": "How we do things with words: Analyzing text as social and cultural data", "abstract": "In this article we describe our experiences with computational text analysis.\nWe hope to achieve three primary goals. First, we aim to shed light on thorny\nissues not always at the forefront of discussions about computational text\nanalysis methods. Second, we hope to provide a set of best practices for\nworking with thick social and cultural concepts. Our guidance is based on our\nown experiences and is therefore inherently imperfect. Still, given our\ndiversity of disciplinary backgrounds and research practices, we hope to\ncapture a range of ideas and identify commonalities that will resonate for\nmany. And this leads to our final goal: to help promote interdisciplinary\ncollaborations. Interdisciplinary insights and partnerships are essential for\nrealizing the full potential of any computational text analysis that involves\nsocial and cultural concepts, and the more we are able to bridge these divides,\nthe more fruitful we believe our work will be.", "published": "2019-07-02 15:55:42", "link": "http://arxiv.org/abs/1907.01468v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data mining Mandarin tone contour shapes", "abstract": "In spontaneous speech, Mandarin tones that belong to the same tone category\nmay exhibit many different contour shapes. We explore the use of data mining\nand NLP techniques for understanding the variability of tones in a large corpus\nof Mandarin newscast speech. First, we adapt a graph-based approach to\ncharacterize the clusters (fuzzy types) of tone contour shapes observed in each\ntone n-gram category. Second, we show correlations between these realized\ncontour shape types and a bag of automatically extracted linguistic features.\nWe discuss the implications of the current study within the context of\nphonological and information theory.", "published": "2019-07-02 22:29:53", "link": "http://arxiv.org/abs/1907.01668v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language2Pose: Natural Language Grounded Pose Forecasting", "abstract": "Generating animations from natural language sentences finds its applications\nin a a number of domains such as movie script visualization, virtual human\nanimation and, robot motion planning. These sentences can describe different\nkinds of actions, speeds and direction of these actions, and possibly a target\ndestination. The core modeling challenge in this language-to-pose application\nis how to map linguistic concepts to motion animations.\n  In this paper, we address this multimodal problem by introducing a neural\narchitecture called Joint Language to Pose (or JL2P), which learns a joint\nembedding of language and pose. This joint embedding space is learned\nend-to-end using a curriculum learning approach which emphasizes shorter and\neasier sequences first before moving to longer and harder ones. We evaluate our\nproposed model on a publicly available corpus of 3D pose data and\nhuman-annotated sentences. Both objective metrics and human judgment evaluation\nconfirm that our proposed approach is able to generate more accurate animations\nand are deemed visually more representative by humans than other data driven\napproaches.", "published": "2019-07-02 00:38:44", "link": "http://arxiv.org/abs/1907.01108v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Neural Semantic Parsing with Anonymization for Command Understanding in\n  General-Purpose Service Robots", "abstract": "Service robots are envisioned to undertake a wide range of tasks at the\nrequest of users. Semantic parsing is one way to convert natural language\ncommands given to these robots into executable representations. Methods for\ncreating semantic parsers, however, rely either on large amounts of data or on\nengineered lexical features and parsing rules, which has limited their\napplication in robotics. To address this challenge, we propose an approach that\nleverages neural semantic parsing methods in combination with contextual word\nembeddings to enable the training of a semantic parser with little data and\nwithout domain specific parser engineering. Key to our approach is the use of\nan anonymized target representation which is more easily learned by the parser.\nIn most cases, this simplified representation can trivially be transformed into\nan executable format, and in others the parse can be completed through further\ninteraction with the user. We evaluate this approach in the context of the\nRoboCup@Home General Purpose Service Robot task, where we have collected a\ncorpus of paraphrased versions of commands from the standardized command\ngenerator. Our results show that neural semantic parsers can predict the\nlogical form of unseen commands with 89% accuracy. We release our data and the\ndetails of our models to encourage further development from the RoboCup and\nservice robotics communities.", "published": "2019-07-02 01:09:35", "link": "http://arxiv.org/abs/1907.01115v1", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "A Neural Grammatical Error Correction System Built On Better\n  Pre-training and Sequential Transfer Learning", "abstract": "Grammatical error correction can be viewed as a low-resource\nsequence-to-sequence task, because publicly available parallel corpora are\nlimited. To tackle this challenge, we first generate erroneous versions of\nlarge unannotated corpora using a realistic noising function. The resulting\nparallel corpora are subsequently used to pre-train Transformer models. Then,\nby sequentially applying transfer learning, we adapt these models to the domain\nand style of the test set. Combined with a context-aware neural spellchecker,\nour system achieves competitive results in both restricted and low resource\ntracks in ACL 2019 BEA Shared Task. We release all of our code and materials\nfor reproducibility.", "published": "2019-07-02 09:33:36", "link": "http://arxiv.org/abs/1907.01256v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Danish Stance Classification and Rumour Resolution", "abstract": "The Internet is rife with flourishing rumours that spread through microblogs\nand social media. Recent work has shown that analysing the stance of the crowd\ntowards a rumour is a good indicator for its veracity. One state-of-the-art\nsystem uses an LSTM neural network to automatically classify stance for posts\non Twitter by considering the context of a whole branch, while another, more\nsimple Decision Tree classifier, performs at least as well by performing\ncareful feature engineering. One approach to predict the veracity of a rumour\nis to use stance as the only feature for a Hidden Markov Model (HMM). This\nthesis generates a stance-annotated Reddit dataset for the Danish language, and\nimplements various models for stance classification. Out of these, a Linear\nSupport Vector Machine provides the best results with an accuracy of 0.76 and\nmacro F1 score of 0.42. Furthermore, experiments show that stance labels can be\nused across languages and platforms with a HMM to predict the veracity of\nrumours, achieving an accuracy of 0.82 and F1 score of 0.67. Even higher scores\nare achieved by relying only on the Danish dataset. In this case veracity\nprediction scores an accuracy of 0.83 and an F1 of 0.68. Finally, when using\nautomatic stance labels for the HMM, only a small drop in performance is\nobserved, showing that the implemented system can have practical applications.", "published": "2019-07-02 11:44:31", "link": "http://arxiv.org/abs/1907.01304v1", "categories": ["cs.CL", "cs.SI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Sequence Labeling Parsing by Learning Across Representations", "abstract": "We use parsing as sequence labeling as a common framework to learn across\nconstituency and dependency syntactic abstractions. To do so, we cast the\nproblem as multitask learning (MTL). First, we show that adding a parsing\nparadigm as an auxiliary loss consistently improves the performance on the\nother paradigm. Secondly, we explore an MTL sequence labeling model that parses\nboth representations, at almost no cost in terms of performance and speed. The\nresults across the board show that on average MTL models with auxiliary losses\nfor constituency parsing outperform single-task ones by 1.14 F1 points, and for\ndependency parsing by 0.62 UAS points.", "published": "2019-07-02 13:13:13", "link": "http://arxiv.org/abs/1907.01339v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CS563-QA: A Collection for Evaluating Question Answering Systems", "abstract": "Question Answering (QA) is a challenging topic since it requires tackling the\nvarious difficulties of natural language understanding. Since evaluation is\nimportant not only for identifying the strong and weak points of the various\ntechniques for QA, but also for facilitating the inception of new methods and\ntechniques, in this paper we present a collection for evaluating QA methods\nover free text that we have created. Although it is a small collection, it\ncontains cases of increasing difficulty, therefore it has an educational value\nand it can be used for rapid evaluation of QA systems.", "published": "2019-07-02 20:00:29", "link": "http://arxiv.org/abs/1907.01611v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "MultiWOZ 2.1: A Consolidated Multi-Domain Dialogue Dataset with State\n  Corrections and State Tracking Baselines", "abstract": "MultiWOZ 2.0 (Budzianowski et al., 2018) is a recently released multi-domain\ndialogue dataset spanning 7 distinct domains and containing over 10,000\ndialogues. Though immensely useful and one of the largest resources of its kind\nto-date, MultiWOZ 2.0 has a few shortcomings. Firstly, there is substantial\nnoise in the dialogue state annotations and dialogue utterances which\nnegatively impact the performance of state-tracking models. Secondly, follow-up\nwork (Lee et al., 2019) has augmented the original dataset with user dialogue\nacts. This leads to multiple co-existent versions of the same dataset with\nminor modifications. In this work we tackle the aforementioned issues by\nintroducing MultiWOZ 2.1. To fix the noisy state annotations, we use\ncrowdsourced workers to re-annotate state and utterances based on the original\nutterances in the dataset. This correction process results in changes to over\n32% of state annotations across 40% of the dialogue turns. In addition, we fix\n146 dialogue utterances by canonicalizing slot values in the utterances to the\nvalues in the dataset ontology. To address the second problem, we combined the\ncontributions of the follow-up works into MultiWOZ 2.1. Hence, our dataset also\nincludes user dialogue acts as well as multiple slot descriptions per dialogue\nstate slot. We then benchmark a number of state-of-the-art dialogue state\ntracking models on the MultiWOZ 2.1 dataset and show the joint state tracking\nperformance on the corrected state annotations. We are publicly releasing\nMultiWOZ 2.1 to the community, hoping that this dataset resource will allow for\nmore effective models across various dialogue subproblems to be built in the\nfuture.", "published": "2019-07-02 22:30:31", "link": "http://arxiv.org/abs/1907.01669v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scalable Multi Corpora Neural Language Models for ASR", "abstract": "Neural language models (NLM) have been shown to outperform conventional\nn-gram language models by a substantial margin in Automatic Speech Recognition\n(ASR) and other tasks. There are, however, a number of challenges that need to\nbe addressed for an NLM to be used in a practical large-scale ASR system. In\nthis paper, we present solutions to some of the challenges, including training\nNLM from heterogenous corpora, limiting latency impact and handling\npersonalized bias in the second-pass rescorer. Overall, we show that we can\nachieve a 6.2% relative WER reduction using neural LM in a second-pass n-best\nrescoring framework with a minimal increase in latency.", "published": "2019-07-02 23:28:52", "link": "http://arxiv.org/abs/1907.01677v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Predicting Retrosynthetic Reaction using Self-Corrected Transformer\n  Neural Networks", "abstract": "Synthesis planning is the process of recursively decomposing target molecules\ninto available precursors. Computer-aided retrosynthesis can potentially assist\nchemists in designing synthetic routes, but at present it is cumbersome and\nprovides results of dissatisfactory quality. In this study, we develop a\ntemplate-free self-corrected retrosynthesis predictor (SCROP) to perform a\nretrosynthesis prediction task trained by using the Transformer neural network\narchitecture. In the method, the retrosynthesis planning is converted as a\nmachine translation problem between molecular linear notations of reactants and\nthe products. Coupled with a neural network-based syntax corrector, our method\nachieves an accuracy of 59.0% on a standard benchmark dataset, which increases\n>21% over other deep learning methods, and >6% over template-based methods.\nMore importantly, our method shows an accuracy 1.7 times higher than other\nstate-of-the-art methods for compounds not appearing in the training set.", "published": "2019-07-02 13:35:37", "link": "http://arxiv.org/abs/1907.01356v2", "categories": ["physics.chem-ph", "cs.CL", "cs.LG"], "primary_category": "physics.chem-ph"}
{"title": "Augmenting Self-attention with Persistent Memory", "abstract": "Transformer networks have lead to important progress in language modeling and\nmachine translation. These models include two consecutive modules, a\nfeed-forward layer and a self-attention layer. The latter allows the network to\ncapture long term dependencies and are often regarded as the key ingredient in\nthe success of Transformers. Building upon this intuition, we propose a new\nmodel that solely consists of attention layers. More precisely, we augment the\nself-attention layers with persistent memory vectors that play a similar role\nas the feed-forward layer. Thanks to these vectors, we can remove the\nfeed-forward layer without degrading the performance of a transformer. Our\nevaluation shows the benefits brought by our model on standard character and\nword level language modeling benchmarks.", "published": "2019-07-02 15:56:20", "link": "http://arxiv.org/abs/1907.01470v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Neural Image Captioning", "abstract": "In recent years, the biggest advances in major Computer Vision tasks, such as\nobject recognition, handwritten-digit identification, facial recognition, and\nmany others., have all come through the use of Convolutional Neural Networks\n(CNNs). Similarly, in the domain of Natural Language Processing, Recurrent\nNeural Networks (RNNs), and Long Short Term Memory networks (LSTMs) in\nparticular, have been crucial to some of the biggest breakthroughs in\nperformance for tasks such as machine translation, part-of-speech tagging,\nsentiment analysis, and many others. These individual advances have greatly\nbenefited tasks even at the intersection of NLP and Computer Vision, and\ninspired by this success, we studied some existing neural image captioning\nmodels that have proven to work well. In this work, we study some existing\ncaptioning models that provide near state-of-the-art performances, and try to\nenhance one such model. We also present a simple image captioning model that\nmakes use of a CNN, an LSTM, and the beam search1 algorithm, and study its\nperformance based on various qualitative and quantitative metrics.", "published": "2019-07-02 22:49:25", "link": "http://arxiv.org/abs/1907.02065v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WHAM!: Extending Speech Separation to Noisy Environments", "abstract": "Recent progress in separating the speech signals from multiple overlapping\nspeakers using a single audio channel has brought us closer to solving the\ncocktail party problem. However, most studies in this area use a constrained\nproblem setup, comparing performance when speakers overlap almost completely,\nat artificially low sampling rates, and with no external background noise. In\nthis paper, we strive to move the field towards more realistic and challenging\nscenarios. To that end, we created the WSJ0 Hipster Ambient Mixtures (WHAM!)\ndataset, consisting of two speaker mixtures from the wsj0-2mix dataset combined\nwith real ambient noise samples. The samples were collected in coffee shops,\nrestaurants, and bars in the San Francisco Bay Area, and are made publicly\navailable. We benchmark various speech separation architectures and objective\nfunctions to evaluate their robustness to noise. While separation performance\ndecreases as a result of noise, we still observe substantial gains relative to\nthe noisy signals for most approaches.", "published": "2019-07-02 04:27:55", "link": "http://arxiv.org/abs/1907.01160v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Attention model for articulatory features detection", "abstract": "Articulatory distinctive features, as well as phonetic transcription, play\nimportant role in speech-related tasks: computer-assisted pronunciation\ntraining, text-to-speech conversion (TTS), studying speech production\nmechanisms, speech recognition for low-resourced languages. End-to-end\napproaches to speech-related tasks got a lot of traction in recent years. We\napply Listen, Attend and Spell~(LAS)~\\cite{Chan-LAS2016} architecture to phones\nrecognition on a small small training set, like TIMIT~\\cite{TIMIT-1992}. Also,\nwe introduce a novel decoding technique that allows to train manners and places\nof articulation detectors end-to-end using attention models. We also explore\njoint phones recognition and articulatory features detection in multitask\nlearning setting.", "published": "2019-07-02 10:30:27", "link": "http://arxiv.org/abs/1907.01914v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML", "68T10", "I.2.7"], "primary_category": "eess.AS"}
{"title": "Sub-band Convolutional Neural Networks for Small-footprint Spoken Term\n  Classification", "abstract": "This paper proposes a Sub-band Convolutional Neural Network for spoken term\nclassification. Convolutional neural networks (CNNs) have proven to be very\neffective in acoustic applications such as spoken term classification, keyword\nspotting, speaker identification, acoustic event detection, etc. Unlike\napplications in computer vision, the spatial invariance property of 2D\nconvolutional kernels does not fit acoustic applications well since the meaning\nof a specific 2D kernel varies a lot along the feature axis in an input feature\nmap. We propose a sub-band CNN architecture to apply different convolutional\nkernels on each feature sub-band, which makes the overall computation more\nefficient. Experimental results show that the computational efficiency brought\nby sub-band CNN is more beneficial for small-footprint models. Compared to a\nbaseline full band CNN for spoken term classification on a publicly available\nSpeech Commands dataset, the proposed sub-band CNN architecture reduces the\ncomputation by 39.7% on commands classification, and 49.3% on digits\nclassification with accuracy maintained.", "published": "2019-07-02 15:29:23", "link": "http://arxiv.org/abs/1907.01448v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adaptive Music Composition for Games", "abstract": "The generation of music that adapts dynamically to content and actions has an\nimportant role in building more immersive, memorable and emotive game\nexperiences. To date, the development of adaptive music systems for video games\nis limited by both the nature of algorithms used for real-time music generation\nand the limited modelling of player action, game world context and emotion in\ncurrent games. We propose that these issues must be addressed in tandem for the\nquality and flexibility of adaptive game music to significantly improve.\nCognitive models of knowledge organisation and emotional affect are integrated\nwith multi-modal, multi-agent composition techniques to produce a novel\nAdaptive Music System (AMS). The system is integrated into two stylistically\ndistinct games. Gamers reported an overall higher immersion and correlation of\nmusic with game-world concepts with the AMS than with the original game\nsoundtracks in both games.", "published": "2019-07-02 04:10:02", "link": "http://arxiv.org/abs/1907.01154v1", "categories": ["cs.MM", "cs.AI", "cs.SD", "eess.AS", "I.2.1; H.5.5; D.2.11"], "primary_category": "cs.MM"}
{"title": "Learning to Traverse Latent Spaces for Musical Score Inpainting", "abstract": "Music Inpainting is the task of filling in missing or lost information in a\npiece of music. We investigate this task from an interactive music creation\nperspective. To this end, a novel deep learning-based approach for musical\nscore inpainting is proposed. The designed model takes both past and future\nmusical context into account and is capable of suggesting ways to connect them\nin a musically meaningful manner. To achieve this, we leverage the\nrepresentational power of the latent space of a Variational Auto-Encoder and\ntrain a Recurrent Neural Network which learns to traverse this latent space\nconditioned on the past and future musical contexts. Consequently, the designed\nmodel is capable of generating several measures of music to connect two musical\nexcerpts. The capabilities and performance of the model are showcased by\ncomparison with competitive baselines using several objective and subjective\nevaluation methods. The results show that the model generates meaningful\ninpaintings and can be used in interactive music creation applications.\nOverall, the method demonstrates the merit of learning complex trajectories in\nthe latent spaces of deep generative models.", "published": "2019-07-02 04:39:05", "link": "http://arxiv.org/abs/1907.01164v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Can a Robot Hear the Shape and Dimensions of a Room?", "abstract": "Knowing the geometry of a space is desirable for many applications, e.g.\nsound source localization, sound field reproduction or auralization. In\ncircumstances where only acoustic signals can be obtained, estimating the\ngeometry of a room is a challenging proposition. Existing methods have been\nproposed to reconstruct a room from the room impulse responses (RIRs). However,\nthe sound source and microphones must be deployed in a feasible region of the\nroom for it to work, which is impractical when the room is unknown. This work\npropose to employ a robot equipped with a sound source and four acoustic\nsensors, to follow a proposed path planning strategy to moves around the room\nto collect first image sources for room geometry estimation. The strategy can\neffectively drives the robot from a random initial location through the room so\nthat the room geometry is guaranteed to be revealed. Effectiveness of the\nproposed approach is extensively validated in a synthetic environment, where\nthe results obtained are highly promising.", "published": "2019-07-02 05:08:27", "link": "http://arxiv.org/abs/1907.01169v1", "categories": ["cs.SD", "cs.RO", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Kite: Automatic speech recognition for unmanned aerial vehicles", "abstract": "This paper addresses the problem of building a speech recognition system\nattuned to the control of unmanned aerial vehicles (UAVs). Even though UAVs are\nbecoming widespread, the task of creating voice interfaces for them is largely\nunaddressed. To this end, we introduce a multi-modal evaluation dataset for UAV\ncontrol, consisting of spoken commands and associated images, which represent\nthe visual context of what the UAV \"sees\" when the pilot utters the command. We\nprovide baseline results and address two research directions: (i) how robust\nthe language models are, given an incomplete list of commands at train time;\n(ii) how to incorporate visual information in the language model. We find that\nrecurrent neural networks (RNNs) are a solution to both tasks: they can be\nsuccessfully adapted using a small number of commands and they can be extended\nto use visual cues. Our results show that the image-based RNN outperforms its\ntext-only counterpart even if the command-image training associations are\nautomatically generated and inherently imperfect. The dataset and our code are\navailable at http://kite.speed.pub.ro.", "published": "2019-07-02 06:50:24", "link": "http://arxiv.org/abs/1907.01195v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Conditioned-U-Net: Introducing a Control Mechanism in the U-Net for\n  Multiple Source Separations", "abstract": "Data-driven models for audio source separation such as U-Net or Wave-U-Net\nare usually models dedicated to and specifically trained for a single task,\ne.g. a particular instrument isolation. Training them for various tasks at once\ncommonly results in worse performances than training them for a single\nspecialized task. In this work, we introduce the Conditioned-U-Net (C-U-Net)\nwhich adds a control mechanism to the standard U-Net. The control mechanism\nallows us to train a unique and generic U-Net to perform the separation of\nvarious instruments. The C-U-Net decides the instrument to isolate according to\na one-hot-encoding input vector. The input vector is embedded to obtain the\nparameters that control Feature-wise Linear Modulation (FiLM) layers. FiLM\nlayers modify the U-Net feature maps in order to separate the desired\ninstrument via affine transformations. The C-U-Net performs different\ninstrument separations, all with a single model achieving the same performances\nas the dedicated ones at a lower cost.", "published": "2019-07-02 10:10:53", "link": "http://arxiv.org/abs/1907.01277v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MIDI-Sandwich: Multi-model Multi-task Hierarchical Conditional VAE-GAN\n  networks for Symbolic Single-track Music Generation", "abstract": "Most existing neural network models for music generation explore how to\ngenerate music bars, then directly splice the music bars into a song. However,\nthese methods do not explore the relationship between the bars, and the\nconnected song as a whole has no musical form structure and sense of musical\ndirection. To address this issue, we propose a Multi-model Multi-task\nHierarchical Conditional VAE-GAN (Variational Autoencoder-Generative\nadversarial networks) networks, named MIDI-Sandwich, which combines musical\nknowledge, such as musical form, tonic, and melodic motion. The MIDI-Sandwich\nhas two submodels: Hierarchical Conditional Variational Autoencoder (HCVAE) and\nHierarchical Conditional Generative Adversarial Network (HCGAN). The HCVAE uses\nhierarchical structure. The underlying layer of HCVAE uses Local Conditional\nVariational Autoencoder (L-CVAE) to generate a music bar which is pre-specified\nby the First and Last Notes (FLN). The upper layer of HCVAE uses Global\nVariational Autoencoder(G-VAE) to analyze the latent vector sequence generated\nby the L-CVAE encoder, to explore the musical relationship between the bars,\nand to produce the song pieced together by multiple music bars generated by the\nL-CVAE decoder, which makes the song both have musical structure and sense of\ndirection. At the same time, the HCVAE shares a part of itself with the HCGAN\nto further improve the performance of the generated music. The MIDI-Sandwich is\nvalidated on the Nottingham dataset and is able to generate a single-track\nmelody sequence (17x8 beats), which is superior to the length of most of the\ngenerated models (8 to 32 beats). Meanwhile, by referring to the experimental\nmethods of many classical kinds of literature, the quality evaluation of the\ngenerated music is performed. The above experiments prove the validity of the\nmodel.", "published": "2019-07-02 19:55:33", "link": "http://arxiv.org/abs/1907.01607v2", "categories": ["eess.AS", "cs.LG", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
