{"title": "JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction", "abstract": "We present a new parallel corpus, JHU FLuency-Extended GUG corpus (JFLEG) for\ndeveloping and evaluating grammatical error correction (GEC). Unlike other\ncorpora, it represents a broad range of language proficiency levels and uses\nholistic fluency edits to not only correct grammatical errors but also make the\noriginal text more native sounding. We describe the types of corrections made\nand benchmark four leading GEC systems on this corpus, identifying specific\nareas in which they do well and how they can improve. JFLEG fulfills the need\nfor a new gold standard to properly assess the current state of GEC.", "published": "2017-02-14 03:47:34", "link": "http://arxiv.org/abs/1702.04066v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Relevance of Auditory-Based Gabor Features for Deep Learning in\n  Automatic Speech Recognition", "abstract": "Previous studies support the idea of merging auditory-based Gabor features\nwith deep learning architectures to achieve robust automatic speech\nrecognition, however, the cause behind the gain of such combination is still\nunknown. We believe these representations provide the deep learning decoder\nwith more discriminable cues. Our aim with this paper is to validate this\nhypothesis by performing experiments with three different recognition tasks\n(Aurora 4, CHiME 2 and CHiME 3) and assess the discriminability of the\ninformation encoded by Gabor filterbank features. Additionally, to identify the\ncontribution of low, medium and high temporal modulation frequencies subsets of\nthe Gabor filterbank were used as features (dubbed LTM, MTM and HTM\nrespectively). With temporal modulation frequencies between 16 and 25 Hz, HTM\nconsistently outperformed the remaining ones in every condition, highlighting\nthe robustness of these representations against channel distortions, low\nsignal-to-noise ratios and acoustically challenging real-life scenarios with\nrelative improvements from 11 to 56% against a Mel-filterbank-DNN baseline. To\nexplain the results, a measure of similarity between phoneme classes from DNN\nactivations is proposed and linked to their acoustic properties. We find this\nmeasure to be consistent with the observed error rates and highlight specific\ndifferences on phoneme level to pinpoint the benefit of the proposed features.", "published": "2017-02-14 18:46:47", "link": "http://arxiv.org/abs/1702.04333v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A case study on using speech-to-translation alignments for language\n  documentation", "abstract": "For many low-resource or endangered languages, spoken language resources are\nmore likely to be annotated with translations than with transcriptions. Recent\nwork exploits such annotations to produce speech-to-translation alignments,\nwithout access to any text transcriptions. We investigate whether providing\nsuch information can aid in producing better (mismatched) crowdsourced\ntranscriptions, which in turn could be valuable for training speech recognition\nsystems, and show that they can indeed be beneficial through a small-scale case\nstudy as a proof-of-concept. We also present a simple phonetically aware string\naveraging technique that produces transcriptions of higher quality.", "published": "2017-02-14 20:06:15", "link": "http://arxiv.org/abs/1702.04372v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
