{"title": "One Question Answering Model for Many Languages with Cross-lingual Dense\n  Passage Retrieval", "abstract": "We present Cross-lingual Open-Retrieval Answer Generation (CORA), the first\nunified many-to-many question answering (QA) model that can answer questions\nacross many languages, even for ones without language-specific annotated data\nor knowledge sources. We introduce a new dense passage retrieval algorithm that\nis trained to retrieve documents across languages for a question. Combined with\na multilingual autoregressive generation model, CORA answers directly in the\ntarget language without any translation or in-language retrieval modules as\nused in prior work. We propose an iterative training method that automatically\nextends annotated data available only in high-resource languages to\nlow-resource ones. Our results show that CORA substantially outperforms the\nprevious state of the art on multilingual open QA benchmarks across 26\nlanguages, 9 of which are unseen during training. Our analyses show the\nsignificance of cross-lingual retrieval and generation in many languages,\nparticularly under low-resource settings.", "published": "2021-07-26 06:02:54", "link": "http://arxiv.org/abs/2107.11976v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Coreference Resolution with Harmonized Annotations", "abstract": "In this paper, we present coreference resolution experiments with a newly\ncreated multilingual corpus CorefUD. We focus on the following languages:\nCzech, Russian, Polish, German, Spanish, and Catalan. In addition to\nmonolingual experiments, we combine the training data in multilingual\nexperiments and train two joined models -- for Slavic languages and for all the\nlanguages together. We rely on an end-to-end deep learning model that we\nslightly adapted for the CorefUD corpus. Our results show that we can profit\nfrom harmonized annotations, and using joined models helps significantly for\nthe languages with smaller training data.", "published": "2021-07-26 10:11:06", "link": "http://arxiv.org/abs/2107.12088v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Negation in Neural Machine Translation", "abstract": "In this paper, we evaluate the translation of negation both automatically and\nmanually, in English--German (EN--DE) and English--Chinese (EN--ZH). We show\nthat the ability of neural machine translation (NMT) models to translate\nnegation has improved with deeper and more advanced networks, although the\nperformance varies between language pairs and translation directions. The\naccuracy of manual evaluation in EN-DE, DE-EN, EN-ZH, and ZH-EN is 95.7%,\n94.8%, 93.4%, and 91.7%, respectively. In addition, we show that\nunder-translation is the most significant error type in NMT, which contrasts\nwith the more diverse error profile previously observed for statistical machine\ntranslation. To better understand the root of the under-translation of\nnegation, we study the model's information flow and training data. While our\ninformation flow analysis does not reveal any deficiencies that could be used\nto detect or fix the under-translation of negation, we find that negation is\noften rephrased during training, which could make it more difficult for the\nmodel to learn a reliable link between source and target negation. We finally\nconduct intrinsic analysis and extrinsic probing tasks on negation, showing\nthat NMT models can distinguish negation and non-negation tokens very well and\nencode a lot of information about negation in hidden states but nevertheless\nleave room for improvement.", "published": "2021-07-26 13:19:57", "link": "http://arxiv.org/abs/2107.12203v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Span-Level Interactions for Aspect Sentiment Triplet Extraction", "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is the most recent subtask of ABSA\nwhich outputs triplets of an aspect target, its associated sentiment, and the\ncorresponding opinion term. Recent models perform the triplet extraction in an\nend-to-end manner but heavily rely on the interactions between each target word\nand opinion word. Thereby, they cannot perform well on targets and opinions\nwhich contain multiple words. Our proposed span-level approach explicitly\nconsiders the interaction between the whole spans of targets and opinions when\npredicting their sentiment relation. Thus, it can make predictions with the\nsemantics of whole spans, ensuring better sentiment consistency. To ease the\nhigh computational cost caused by span enumeration, we propose a dual-channel\nspan pruning strategy by incorporating supervision from the Aspect Term\nExtraction (ATE) and Opinion Term Extraction (OTE) tasks. This strategy not\nonly improves computational efficiency but also distinguishes the opinion and\ntarget spans more properly. Our framework simultaneously achieves strong\nperformance for the ASTE as well as ATE and OTE tasks. In particular, our\nanalysis shows that our span-level approach achieves more significant\nimprovements over the baselines on triplets with multi-word targets or\nopinions.", "published": "2021-07-26 13:47:31", "link": "http://arxiv.org/abs/2107.12214v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Preliminary Steps Towards Federated Sentiment Classification", "abstract": "Automatically mining sentiment tendency contained in natural language is a\nfundamental research to some artificial intelligent applications, where\nsolutions alternate with challenges. Transfer learning and multi-task learning\ntechniques have been leveraged to mitigate the supervision sparsity and\ncollaborate multiple heterogeneous domains correspondingly. Recent years, the\nsensitive nature of users' private data raises another challenge for sentiment\nclassification, i.e., data privacy protection. In this paper, we resort to\nfederated learning for multiple domain sentiment classification under the\nconstraint that the corpora must be stored on decentralized devices. In view of\nthe heterogeneous semantics across multiple parties and the peculiarities of\nword embedding, we pertinently provide corresponding solutions. First, we\npropose a Knowledge Transfer Enhanced Private-Shared (KTEPS) framework for\nbetter model aggregation and personalization in federated sentiment\nclassification. Second, we propose KTEPS$^\\star$ with the consideration of the\nrich semantic and huge embedding size properties of word vectors, utilizing\nProjection-based Dimension Reduction (PDR) methods for privacy protection and\nefficient transmission simultaneously. We propose two federated sentiment\nclassification scenes based on public benchmarks, and verify the superiorities\nof our proposed methods with abundant experimental investigations.", "published": "2021-07-26 04:57:49", "link": "http://arxiv.org/abs/2107.11956v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Knowledge Graph and Attention Help? A Quantitative Analysis into\n  Bag-level Relation Extraction", "abstract": "Knowledge Graph (KG) and attention mechanism have been demonstrated effective\nin introducing and selecting useful information for weakly supervised methods.\nHowever, only qualitative analysis and ablation study are provided as evidence.\nIn this paper, we contribute a dataset and propose a paradigm to quantitatively\nevaluate the effect of attention and KG on bag-level relation extraction (RE).\nWe find that (1) higher attention accuracy may lead to worse performance as it\nmay harm the model's ability to extract entity mention features; (2) the\nperformance of attention is largely influenced by various noise distribution\npatterns, which is closely related to real-world datasets; (3) KG-enhanced\nattention indeed improves RE performance, while not through enhanced attention\nbut by incorporating entity prior; and (4) attention mechanism may exacerbate\nthe issue of insufficient training data. Based on these findings, we show that\na straightforward variant of RE model can achieve significant improvements (6%\nAUC on average) on two real-world datasets as compared with three\nstate-of-the-art baselines. Our codes and datasets are available at\nhttps://github.com/zig-kwin-hu/how-KG-ATT-help.", "published": "2021-07-26 09:38:28", "link": "http://arxiv.org/abs/2107.12064v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Emotion Prediction by Modeling Emotion Definitions", "abstract": "In this paper, we propose a new framework for fine-grained emotion prediction\nin the text through emotion definition modeling. Our approach involves a\nmulti-task learning framework that models definitions of emotions as an\nauxiliary task while being trained on the primary task of emotion prediction.\nWe model definitions using masked language modeling and class definition\nprediction tasks. Our models outperform existing state-of-the-art for\nfine-grained emotion dataset GoEmotions. We further show that this trained\nmodel can be used for transfer learning on other benchmark datasets in emotion\nprediction with varying emotion label sets, domains, and sizes. The proposed\nmodels outperform the baselines on transfer learning experiments demonstrating\nthe generalization capability of the models.", "published": "2021-07-26 12:11:18", "link": "http://arxiv.org/abs/2107.12135v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploiting Language Model for Efficient Linguistic Steganalysis", "abstract": "Recent advances in linguistic steganalysis have successively applied CNN,\nRNN, GNN and other efficient deep models for detecting secret information in\ngenerative texts. These methods tend to seek stronger feature extractors to\nachieve higher steganalysis effects. However, we have found through experiments\nthat there actually exists significant difference between automatically\ngenerated stego texts and carrier texts in terms of the conditional probability\ndistribution of individual words. Such kind of difference can be naturally\ncaptured by the language model used for generating stego texts. Through further\nexperiments, we conclude that this ability can be transplanted to a text\nclassifier by pre-training and fine-tuning to improve the detection\nperformance. Motivated by this insight, we propose two methods for efficient\nlinguistic steganalysis. One is to pre-train a language model based on RNN, and\nthe other is to pre-train a sequence autoencoder. The results indicate that the\ntwo methods have different degrees of performance gain compared to the randomly\ninitialized RNN, and the convergence speed is significantly accelerated.\nMoreover, our methods achieved the best performance compared to related works,\nwhile providing a solution for real-world scenario where there are more cover\ntexts than stego texts.", "published": "2021-07-26 12:37:18", "link": "http://arxiv.org/abs/2107.12168v3", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "DYPLODOC: Dynamic Plots for Document Classification", "abstract": "Narrative generation and analysis are still on the fringe of modern natural\nlanguage processing yet are crucial in a variety of applications. This paper\nproposes a feature extraction method for plot dynamics. We present a dataset\nthat consists of the plot descriptions for thirteen thousand TV shows alongside\nmeta-information on their genres and dynamic plots extracted from them. We\nvalidate the proposed tool for plot dynamics extraction and discuss possible\napplications of this method to the tasks of narrative analysis and generation.", "published": "2021-07-26 14:12:45", "link": "http://arxiv.org/abs/2107.12226v1", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Meta-Learning Adversarial Domain Adaptation Network for Few-Shot Text\n  Classification", "abstract": "Meta-learning has emerged as a trending technique to tackle few-shot text\nclassification and achieved state-of-the-art performance. However, existing\nsolutions heavily rely on the exploitation of lexical features and their\ndistributional signatures on training data, while neglecting to strengthen the\nmodel's ability to adapt to new tasks. In this paper, we propose a novel\nmeta-learning framework integrated with an adversarial domain adaptation\nnetwork, aiming to improve the adaptive ability of the model and generate\nhigh-quality text embedding for new classes. Extensive experiments are\nconducted on four benchmark datasets and our method demonstrates clear\nsuperiority over the state-of-the-art models in all the datasets. In\nparticular, the accuracy of 1-shot and 5-shot classification on the dataset of\n20 Newsgroups is boosted from 52.1% to 59.6%, and from 68.3% to 77.8%,\nrespectively.", "published": "2021-07-26 15:09:40", "link": "http://arxiv.org/abs/2107.12262v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Argumentative Dialogue System for COVID-19 Vaccine Information", "abstract": "Dialogue systems are widely used in AI to support timely and interactive\ncommunication with users. We propose a general-purpose dialogue system\narchitecture that leverages computational argumentation to perform reasoning\nand provide consistent and explainable answers. We illustrate the system using\na COVID-19 vaccine information case study.", "published": "2021-07-26 09:58:39", "link": "http://arxiv.org/abs/2107.12079v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "I.2.1; I.2.4"], "primary_category": "cs.CL"}
{"title": "Thought Flow Nets: From Single Predictions to Trains of Model Thought", "abstract": "When humans solve complex problems, they typically create a sequence of ideas\n(involving an intuitive decision, reflection, error correction, etc.) in order\nto reach a conclusive decision. Contrary to this, today's models are mostly\ntrained to map an input to one single and fixed output. In this paper, we\ninvestigate how we can give models the opportunity of a second, third and\n$k$-th thought. Taking inspiration from Hegel's dialectics, we propose the\nconcept of a thought flow which creates a sequence of predictions. We present a\nself-correction mechanism that is trained to estimate the model's correctness\nand performs iterative prediction updates based on the correctness prediction's\ngradient. We introduce our method at the example of question answering and\nconduct extensive experiments that demonstrate (i) our method's ability to\ncorrect its own predictions and (ii) its potential to notably improve model\nperformances. In addition, we conduct a qualitative analysis of thought flow\ncorrection patterns and explore how thought flow predictions affect human users\nwithin a crowdsourcing study. We find that (iii) thought flows enable improved\nuser performance and are perceived as more natural, correct, and intelligent as\nsingle and/or top-3 predictions.", "published": "2021-07-26 13:56:37", "link": "http://arxiv.org/abs/2107.12220v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Improving Word Recognition in Speech Transcriptions by Decision-level\n  Fusion of Stemming and Two-way Phoneme Pruning", "abstract": "We introduce an unsupervised approach for correcting highly imperfect speech\ntranscriptions based on a decision-level fusion of stemming and two-way phoneme\npruning. Transcripts are acquired from videos by extracting audio using Ffmpeg\nframework and further converting audio to text transcript using Google API. In\nthe benchmark LRW dataset, there are 500 word categories, and 50 videos per\nclass in mp4 format. All videos consist of 29 frames (each 1.16 s long) and the\nword appears in the middle of the video. In our approach we tried to improve\nthe baseline accuracy from 9.34% by using stemming, phoneme extraction,\nfiltering and pruning. After applying the stemming algorithm to the text\ntranscript and evaluating the results, we achieved 23.34% accuracy in word\nrecognition. To convert words to phonemes we used the Carnegie Mellon\nUniversity (CMU) pronouncing dictionary that provides a phonetic mapping of\nEnglish words to their pronunciations. A two-way phoneme pruning is proposed\nthat comprises of the two non-sequential steps: 1) filtering and pruning the\nphonemes containing vowels and plosives 2) filtering and pruning the phonemes\ncontaining vowels and fricatives. After obtaining results of stemming and\ntwo-way phoneme pruning, we applied decision-level fusion and that led to an\nimprovement of word recognition rate upto 32.96%.", "published": "2021-07-26 18:44:24", "link": "http://arxiv.org/abs/2107.12428v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "An Adapter Based Pre-Training for Efficient and Scalable Self-Supervised\n  Speech Representation Learning", "abstract": "We present a method for transferring pre-trained self-supervised (SSL) speech\nrepresentations to multiple languages. There is an abundance of unannotated\nspeech, so creating self-supervised representations from raw audio and\nfine-tuning on small annotated datasets is a promising direction to build\nspeech recognition systems. SSL models generally perform SSL on raw audio in a\npre-training phase and then fine-tune on a small fraction of annotated data.\nSuch models have produced state of the art results for ASR. However, these\nmodels are very expensive to pre-train. We use an existing wav2vec 2.0 model\nand tackle the problem of learning new language representations while utilizing\nexisting model knowledge. Crucially we do so without catastrophic forgetting of\nthe existing language representation. We use adapter modules to speed up\npre-training a new language task. Our model can decrease pre-training times by\n32% when learning a new language task, and learn this new audio-language\nrepresentation without forgetting previous language representation. We evaluate\nby applying these language representations to automatic speech recognition.", "published": "2021-07-26 10:39:03", "link": "http://arxiv.org/abs/2107.13530v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Language Grounding with 3D Objects", "abstract": "Seemingly simple natural language requests to a robot are generally\nunderspecified, for example \"Can you bring me the wireless mouse?\" Flat images\nof candidate mice may not provide the discriminative information needed for\n\"wireless.\" The world, and objects in it, are not flat images but complex 3D\nshapes. If a human requests an object based on any of its basic properties,\nsuch as color, shape, or texture, robots should perform the necessary\nexploration to accomplish the task. In particular, while substantial effort and\nprogress has been made on understanding explicitly visual attributes like color\nand category, comparatively little progress has been made on understanding\nlanguage about shapes and contours. In this work, we introduce a novel\nreasoning task that targets both visual and non-visual language about 3D\nobjects. Our new benchmark, ShapeNet Annotated with Referring Expressions\n(SNARE) requires a model to choose which of two objects is being referenced by\na natural language description. We introduce several CLIP-based models for\ndistinguishing objects and demonstrate that while recent advances in jointly\nmodeling vision and language are useful for robotic language understanding, it\nis still the case that these image-based models are weaker at understanding the\n3D nature of objects -- properties which play a key role in manipulation. We\nfind that adding view estimation to language grounding models improves accuracy\non both SNARE and when identifying objects referred to in language on a robot\nplatform, but note that a large gap remains between these models and human\nperformance.", "published": "2021-07-26 23:35:58", "link": "http://arxiv.org/abs/2107.12514v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Inplace Gated Convolutional Recurrent Neural Network For Dual-channel\n  Speech Enhancement", "abstract": "For dual-channel speech enhancement, it is a promising idea to design an\nend-to-end model based on the traditional array signal processing guideline and\nthe manifold space of multi-channel signals. We found that the idea above can\nbe effectively implemented by the classical convolutional recurrent neural\nnetworks (CRN) architecture. We propose a very compact in place gated\nconvolutional recurrent neural network (inplace GCRN) for end-to-end\nmulti-channel speech enhancement, which utilizes inplace-convolution for\nfrequency pattern extraction and reconstruction. The inplace characteristics\nefficiently preserve spatial cues in each frequency bin for channel-wise long\nshort-term memory neural networks (LSTM) tracing the spatial source. In\naddition, we come up with a new spectrum recovery method by predict amplitude\nmask, mapping, and phase, which effectively improves the speech quality.", "published": "2021-07-26 05:49:49", "link": "http://arxiv.org/abs/2107.11968v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Crowdsourcing strong labels for sound event detection", "abstract": "Strong labels are a necessity for evaluation of sound event detection\nmethods, but often scarcely available due to the high resources required by the\nannotation task. We present a method for estimating strong labels using\ncrowdsourced weak labels, through a process that divides the annotation task\ninto simple unit tasks. Based on estimations of annotators' competence,\naggregation and processing of the weak labels results in a set of objective\nstrong labels. The experiment uses synthetic audio in order to verify the\nquality of the resulting annotations through comparison with ground truth. The\nproposed method produces labels with high precision, though not all event\ninstances are recalled. Detection metrics comparing the produced annotations\nwith the ground truth show 80% F-score in 1 s segments, and up to 89.5%\nintersection-based F1-score calculated according to the polyphonic sound\ndetection score metrics.", "published": "2021-07-26 10:11:24", "link": "http://arxiv.org/abs/2107.12089v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Raw Differentiable Architecture Search for Speech Deepfake and Spoofing\n  Detection", "abstract": "End-to-end approaches to anti-spoofing, especially those which operate\ndirectly upon the raw signal, are starting to be competitive with their more\ntraditional counterparts. Until recently, all such approaches consider only the\nlearning of network parameters; the network architecture is still hand crafted.\nThis too, however, can also be learned. Described in this paper is our attempt\nto learn automatically the network architecture of a speech deepfake and\nspoofing detection solution, while jointly optimising other network components\nand parameters, such as the first convolutional layer which operates on raw\nsignal inputs. The resulting raw differentiable architecture search system\ndelivers a tandem detection cost function score of 0.0517 for the ASVspoof 2019\nlogical access database, a result which is among the best single-system results\nreported to date.", "published": "2021-07-26 13:36:14", "link": "http://arxiv.org/abs/2107.12212v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "UR Channel-Robust Synthetic Speech Detection System for ASVspoof 2021", "abstract": "In this paper, we present UR-AIR system submission to the logical access (LA)\nand the speech deepfake (DF) tracks of the ASVspoof 2021 Challenge. The LA and\nDF tasks focus on synthetic speech detection (SSD), i.e. detecting\ntext-to-speech and voice conversion as spoofing attacks. Different from\nprevious ASVspoof challenges, the LA task this year presents codec and\ntransmission channel variability, while the new task DF presents general audio\ncompression. Built upon our previous research work on improving the robustness\nof the SSD systems to channel effects, we propose a channel-robust synthetic\nspeech detection system for the challenge. To mitigate the channel variability\nissue, we use an acoustic simulator to apply transmission codec, compression\ncodec, and convolutional impulse responses to augmenting the original datasets.\nFor the neural network backbone, we propose to use Emphasized Channel\nAttention, Propagation and Aggregation Time Delay Neural Networks (ECAPA-TDNN)\nas our primary model. We also incorporate one-class learning with\nchannel-robust training strategies to further learn a channel-invariant speech\nrepresentation. Our submission achieved EER 20.33% in the DF task; EER 5.46%\nand min-tDCF 0.3094 in the LA task.", "published": "2021-07-26 08:15:24", "link": "http://arxiv.org/abs/2107.12018v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Facetron: A Multi-speaker Face-to-Speech Model based on Cross-modal\n  Latent Representations", "abstract": "In this paper, we propose a multi-speaker face-to-speech waveform generation\nmodel that also works for unseen speaker conditions. Using a generative\nadversarial network (GAN) with linguistic and speaker characteristic features\nas auxiliary conditions, our method directly converts face images into speech\nwaveforms under an end-to-end training framework. The linguistic features are\nextracted from lip movements using a lip-reading model, and the speaker\ncharacteristic features are predicted from face images using cross-modal\nlearning with a pre-trained acoustic model. Since these two features are\nuncorrelated and controlled independently, we can flexibly synthesize speech\nwaveforms whose speaker characteristics vary depending on the input face\nimages. We show the superiority of our proposed model over conventional methods\nin terms of objective and subjective evaluation results. Specifically, we\nevaluate the performances of linguistic features by measuring their accuracy on\nan automatic speech recognition task. In addition, we estimate speaker and\ngender similarity for multi-speaker and unseen conditions, respectively. We\nalso evaluate the aturalness of the synthesized speech waveforms using a mean\nopinion score (MOS) test and non-intrusive objective speech quality assessment\n(NISQA).The demo samples of the proposed and other models are available at\nhttps://sam-0927.github.io/", "published": "2021-07-26 07:36:02", "link": "http://arxiv.org/abs/2107.12003v3", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Joint Direction and Proximity Classification of Overlapping Sound Events\n  from Binaural Audio", "abstract": "Sound source proximity and distance estimation are of great interest in many\npractical applications, since they provide significant information for acoustic\nscene analysis. As both tasks share complementary qualities, ensuring efficient\ninteraction between these two is crucial for a complete picture of an aural\nenvironment. In this paper, we aim to investigate several ways of performing\njoint proximity and direction estimation from binaural recordings, both defined\nas coarse classification problems based on Deep Neural Networks (DNNs).\nConsidering the limitations of binaural audio, we propose two methods of\nsplitting the sphere into angular areas in order to obtain a set of directional\nclasses. For each method we study different model types to acquire information\nabout the direction-of-arrival (DoA). Finally, we propose various ways of\ncombining the proximity and direction estimation problems into a joint task\nproviding temporal information about the onsets and offsets of the appearing\nsources. Experiments are performed for a synthetic reverberant binaural dataset\nconsisting of up to two overlapping sound events.", "published": "2021-07-26 08:48:46", "link": "http://arxiv.org/abs/2107.12033v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SVEva Fair: A Framework for Evaluating Fairness in Speaker Verification", "abstract": "Despite the success of deep neural networks (DNNs) in enabling on-device\nvoice assistants, increasing evidence of bias and discrimination in machine\nlearning is raising the urgency of investigating the fairness of these systems.\nSpeaker verification is a form of biometric identification that gives access to\nvoice assistants. Due to a lack of fairness metrics and evaluation frameworks\nthat are appropriate for testing the fairness of speaker verification\ncomponents, little is known about how model performance varies across\nsubgroups, and what factors influence performance variation. To tackle this\nemerging challenge, we design and develop SVEva Fair, an accessible, actionable\nand model-agnostic framework for evaluating the fairness of speaker\nverification components. The framework provides evaluation measures and\nvisualisations to interrogate model performance across speaker subgroups and\ncompare fairness between models. We demonstrate SVEva Fair in a case study with\nend-to-end DNNs trained on the VoxCeleb datasets to reveal potential bias in\nexisting embedded speech recognition systems based on the demographic\nattributes of speakers. Our evaluation shows that publicly accessible benchmark\nmodels are not fair and consistently produce worse predictions for some\nnationalities, and for female speakers of most nationalities. To pave the way\nfor fair and reliable embedded speaker verification, SVEva Fair has been\nimplemented as an open-source python library and can be integrated into the\nembedded ML development pipeline to facilitate developers and researchers in\ntroubleshooting unreliable speaker verification performance, and selecting high\nimpact approaches for mitigating fairness challenges", "published": "2021-07-26 09:15:46", "link": "http://arxiv.org/abs/2107.12049v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adaptation of Tacotron2-based Text-To-Speech for\n  Articulatory-to-Acoustic Mapping using Ultrasound Tongue Imaging", "abstract": "For articulatory-to-acoustic mapping, typically only limited parallel\ntraining data is available, making it impossible to apply fully end-to-end\nsolutions like Tacotron2. In this paper, we experimented with transfer learning\nand adaptation of a Tacotron2 text-to-speech model to improve the final\nsynthesis quality of ultrasound-based articulatory-to-acoustic mapping with a\nlimited database. We use a multi-speaker pre-trained Tacotron2 TTS model and a\npre-trained WaveGlow neural vocoder. The articulatory-to-acoustic conversion\ncontains three steps: 1) from a sequence of ultrasound tongue image recordings,\na 3D convolutional neural network predicts the inputs of the pre-trained\nTacotron2 model, 2) the Tacotron2 model converts this intermediate\nrepresentation to an 80-dimensional mel-spectrogram, and 3) the WaveGlow model\nis applied for final inference. This generated speech contains the timing of\nthe original articulatory data from the ultrasound recording, but the F0\ncontour and the spectral information is predicted by the Tacotron2 model. The\nF0 values are independent of the original ultrasound images, but represent the\ntarget speaker, as they are inferred from the pre-trained Tacotron2 model. In\nour experiments, we demonstrated that the synthesized speech quality is more\nnatural with the proposed solutions than with our earlier model.", "published": "2021-07-26 09:19:20", "link": "http://arxiv.org/abs/2107.12051v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Beyond Voice Identity Conversion: Manipulating Voice Attributes by\n  Adversarial Learning of Structured Disentangled Representations", "abstract": "Voice conversion (VC) consists of digitally altering the voice of an\nindividual to manipulate part of its content, primarily its identity, while\nmaintaining the rest unchanged. Research in neural VC has accomplished\nconsiderable breakthroughs with the capacity to falsify a voice identity using\na small amount of data with a highly realistic rendering. This paper goes\nbeyond voice identity and presents a neural architecture that allows the\nmanipulation of voice attributes (e.g., gender and age). Leveraging the latest\nadvances on adversarial learning of structured speech representation, a novel\nstructured neural network is proposed in which multiple auto-encoders are used\nto encode speech as a set of idealistically independent linguistic and\nextra-linguistic representations, which are learned adversariarly and can be\nmanipulated during VC. Moreover, the proposed architecture is time-synchronized\nso that the original voice timing is preserved during conversion which allows\nlip-sync applications. Applied to voice gender conversion on the real-world\nVCTK dataset, our proposed architecture can learn successfully\ngender-independent representation and convert the voice gender with a very high\nefficiency and naturalness.", "published": "2021-07-26 17:40:43", "link": "http://arxiv.org/abs/2107.12346v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TaikoNation: Patterning-focused Chart Generation for Rhythm Action Games", "abstract": "Generating rhythm game charts from songs via machine learning has been a\nproblem of increasing interest in recent years. However, all existing systems\nstruggle to replicate human-like patterning: the placement of game objects in\nrelation to each other to form congruent patterns based on events in the song.\nPatterning is a key identifier of high quality rhythm game content, seen as a\nnecessary component in human rankings. We establish a new approach for chart\ngeneration that produces charts with more congruent, human-like patterning than\nseen in prior work.", "published": "2021-07-26 22:55:57", "link": "http://arxiv.org/abs/2107.12506v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Vowel-based Meeteilon dialect identification using a Random Forest\n  classifier", "abstract": "This paper presents a vowel-based dialect identification system for\nMeeteilon. For this work, a vowel dataset is created by using Meeteilon Speech\nCorpora available at Linguistic Data Consortium for Indian Languages (LDC-IL).\nSpectral features such as formant frequencies (F1, F1 and F3) and prosodic\nfeatures such as pitch (F0), energy, intensity and segment duration values are\nextracted from monophthong vowel sounds. Random forest classifier, a decision\ntree-based ensemble algorithm is used for classification of three major\ndialects of Meeteilon namely, Imphal, Kakching and Sekmai. Model has shown an\naverage dialect identification performance in terms of accuracy of around\n61.57%. The role of spectral and prosodic features are found to be significant\nin Meeteilon dialect classification.", "published": "2021-07-26 04:09:00", "link": "http://arxiv.org/abs/2107.13419v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
