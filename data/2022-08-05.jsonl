{"title": "Towards No.1 in CLUE Semantic Matching Challenge: Pre-trained Language\n  Model Erlangshen with Propensity-Corrected Loss", "abstract": "This report describes a pre-trained language model Erlangshen with\npropensity-corrected loss, the No.1 in CLUE Semantic Matching Challenge. In the\npre-training stage, we construct a dynamic masking strategy based on knowledge\nin Masked Language Modeling (MLM) with whole word masking. Furthermore, by\nobserving the specific structure of the dataset, the pre-trained Erlangshen\napplies propensity-corrected loss (PCL) in the fine-tuning phase. Overall, we\nachieve 72.54 points in F1 Score and 78.90 points in Accuracy on the test set.\nOur code is publicly available at:\nhttps://github.com/IDEA-CCNL/Fengshenbang-LM/tree/hf-ds/fengshen/examples/clue_sim.", "published": "2022-08-05 02:52:29", "link": "http://arxiv.org/abs/2208.02959v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Phrase translation using a bilingual dictionary and n-gram data: A case\n  study from Vietnamese to English", "abstract": "Past approaches to translate a phrase in a language L1 to a language L2 using\na dictionary-based approach require grammar rules to restructure initial\ntranslations. This paper introduces a novel method without using any grammar\nrules to translate a given phrase in L1, which does not exist in the\ndictionary, to L2. We require at least one L1-L2 bilingual dictionary and\nn-gram data in L2. The average manual evaluation score of our translations is\n4.29/5.00, which implies very high quality.", "published": "2022-08-05 07:16:25", "link": "http://arxiv.org/abs/2208.03018v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-Resource Dense Retrieval for Open-Domain Question Answering: A\n  Comprehensive Survey", "abstract": "Dense retrieval (DR) approaches based on powerful pre-trained language models\n(PLMs) achieved significant advances and have become a key component for modern\nopen-domain question-answering systems. However, they require large amounts of\nmanual annotations to perform competitively, which is infeasible to scale. To\naddress this, a growing body of research works have recently focused on\nimproving DR performance under low-resource scenarios. These works differ in\nwhat resources they require for training and employ a diverse set of\ntechniques. Understanding such differences is crucial for choosing the right\ntechnique under a specific low-resource scenario. To facilitate this\nunderstanding, we provide a thorough structured overview of mainstream\ntechniques for low-resource DR. Based on their required resources, we divide\nthe techniques into three main categories: (1) only documents are needed; (2)\ndocuments and questions are needed; and (3) documents and question-answer pairs\nare needed. For every technique, we introduce its general-form algorithm,\nhighlight the open issues and pros and cons. Promising directions are outlined\nfor future research.", "published": "2022-08-05 14:35:03", "link": "http://arxiv.org/abs/2208.03197v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Construction of English Resume Corpus and Test with Pre-trained Language\n  Models", "abstract": "Information extraction(IE) has always been one of the essential tasks of NLP.\nMoreover, one of the most critical application scenarios of information\nextraction is the information extraction of resumes. Constructed text is\nobtained by classifying each part of the resume. It is convenient to store\nthese texts for later search and analysis. Furthermore, the constructed resume\ndata can also be used in the AI resume screening system. Significantly reduce\nthe labor cost of HR. This study aims to transform the information extraction\ntask of resumes into a simple sentence classification task. Based on the\nEnglish resume dataset produced by the prior study. The classification rules\nare improved to create a larger and more fine-grained classification dataset of\nresumes. This corpus is also used to test some current mainstream Pre-training\nlanguage models (PLMs) performance.Furthermore, in order to explore the\nrelationship between the number of training samples and the correctness rate of\nthe resume dataset, we also performed comparison experiments with training sets\nof different train set sizes.The final multiple experimental results show that\nthe resume dataset with improved annotation rules and increased sample size of\nthe dataset improves the accuracy of the original resume dataset.", "published": "2022-08-05 15:07:23", "link": "http://arxiv.org/abs/2208.03219v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Task Generalization via Unified Schema Prompt", "abstract": "Task generalization has been a long standing challenge in Natural Language\nProcessing (NLP). Recent research attempts to improve the task generalization\nability of pre-trained language models by mapping NLP tasks into human-readable\nprompted forms. However, these approaches require laborious and inflexible\nmanual collection of prompts, and different prompts on the same downstream task\nmay receive unstable performance. We propose Unified Schema Prompt, a flexible\nand extensible prompting method, which automatically customizes the learnable\nprompts for each task according to the task input schema. It models the shared\nknowledge between tasks, while keeping the characteristics of different task\nschema, and thus enhances task generalization ability. The schema prompt takes\nthe explicit data structure of each task to formulate prompts so that little\nhuman effort is involved. To test the task generalization ability of schema\nprompt at scale, we conduct schema prompt-based multitask pre-training on a\nwide variety of general NLP tasks. The framework achieves strong zero-shot and\nfew-shot generalization performance on 16 unseen downstream tasks from 8 task\ntypes (e.g., QA, NLI, etc). Furthermore, comprehensive analyses demonstrate the\neffectiveness of each component in the schema prompt, its flexibility in task\ncompositionality, and its ability to improve performance under a full-data\nfine-tuning setting.", "published": "2022-08-05 15:26:36", "link": "http://arxiv.org/abs/2208.03229v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Atlas: Few-shot Learning with Retrieval Augmented Language Models", "abstract": "Large language models have shown impressive few-shot results on a wide range\nof tasks. However, when knowledge is key for such results, as is the case for\ntasks such as question answering and fact checking, massive parameter counts to\nstore knowledge seem to be needed. Retrieval augmented models are known to\nexcel at knowledge intensive tasks without the need for as many parameters, but\nit is unclear whether they work in few-shot settings. In this work we present\nAtlas, a carefully designed and pre-trained retrieval augmented language model\nable to learn knowledge intensive tasks with very few training examples. We\nperform evaluations on a wide range of tasks, including MMLU, KILT and\nNaturalQuestions, and study the impact of the content of the document index,\nshowing that it can easily be updated. Notably, Atlas reaches over 42% accuracy\non Natural Questions using only 64 examples, outperforming a 540B parameters\nmodel by 3% despite having 50x fewer parameters.", "published": "2022-08-05 17:39:22", "link": "http://arxiv.org/abs/2208.03299v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language\n  Models", "abstract": "We present Branch-Train-Merge (BTM), a communication-efficient algorithm for\nembarrassingly parallel training of large language models (LLMs). We show it is\npossible to independently train subparts of a new class of LLMs on different\nsubsets of the data, eliminating the massive multi-node synchronization\ncurrently required to train LLMs. BTM learns a set of independent expert LMs\n(ELMs), each specialized to a different textual domain, such as scientific or\nlegal text. These ELMs can be added and removed to update data coverage,\nensembled to generalize to new domains, or averaged to collapse back to a\nsingle LM for efficient inference. New ELMs are learned by branching from\n(mixtures of) ELMs in the current set, further training the parameters on data\nfor the new domain, and then merging the resulting model back into the set for\nfuture use. Experiments show that BTM improves in- and out-of-domain\nperplexities as compared to GPT-style Transformer LMs, when controlling for\ntraining cost. Through extensive analysis, we show that these results are\nrobust to different ELM initialization schemes, but require expert domain\nspecialization; LM ensembles with random data splits do not perform well. We\nalso present a study of scaling BTM into a new corpus of 64 domains (192B\nwhitespace-separated tokens in total); the resulting LM (22.4B total\nparameters) performs as well as a Transformer LM trained with 2.5 times more\ncompute. These gains grow with the number of domains, suggesting more\naggressive parallelism could be used to efficiently train larger models in\nfuture work.", "published": "2022-08-05 17:46:38", "link": "http://arxiv.org/abs/2208.03306v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Noise-Robust Loss for Unlabeled Entity Problem in Named Entity\n  Recognition", "abstract": "Named Entity Recognition (NER) is an important task in natural language\nprocessing. However, traditional supervised NER requires large-scale annotated\ndatasets. Distantly supervision is proposed to alleviate the massive demand for\ndatasets, but datasets constructed in this way are extremely noisy and have a\nserious unlabeled entity problem. The cross entropy (CE) loss function is\nhighly sensitive to unlabeled data, leading to severe performance degradation.\nAs an alternative, we propose a new loss function called NRCES to cope with\nthis problem. A sigmoid term is used to mitigate the negative impact of noise.\nIn addition, we balance the convergence and noise tolerance of the model\naccording to samples and the training process. Experiments on synthetic and\nreal-world datasets demonstrate that our approach shows strong robustness in\nthe case of severe unlabeled entity problem, achieving new state-of-the-art on\nreal-world datasets.", "published": "2022-08-05 00:02:13", "link": "http://arxiv.org/abs/2208.02934v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ZLPR: A Novel Loss for Multi-label Classification", "abstract": "In the era of deep learning, loss functions determine the range of tasks\navailable to models and algorithms. To support the application of deep learning\nin multi-label classification (MLC) tasks, we propose the ZLPR (zero-bounded\nlog-sum-exp \\& pairwise rank-based) loss in this paper. Compared to other\nrank-based losses for MLC, ZLPR can handel problems that the number of target\nlabels is uncertain, which, in this point of view, makes it equally capable\nwith the other two strategies often used in MLC, namely the binary relevance\n(BR) and the label powerset (LP). Additionally, ZLPR takes the corelation\nbetween labels into consideration, which makes it more comprehensive than the\nBR methods. In terms of computational complexity, ZLPR can compete with the BR\nmethods because its prediction is also label-independent, which makes it take\nless time and memory than the LP methods. Our experiments demonstrate the\neffectiveness of ZLPR on multiple benchmark datasets and multiple evaluation\nmetrics. Moreover, we propose the soft version and the corresponding\nKL-divergency calculation method of ZLPR, which makes it possible to apply some\nregularization tricks such as label smoothing to enhance the generalization of\nmodels.", "published": "2022-08-05 02:36:16", "link": "http://arxiv.org/abs/2208.02955v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Meaning without reference in large language models", "abstract": "The widespread success of large language models (LLMs) has been met with\nskepticism that they possess anything like human concepts or meanings. Contrary\nto claims that LLMs possess no meaning whatsoever, we argue that they likely\ncapture important aspects of meaning, and moreover work in a way that\napproximates a compelling account of human cognition in which meaning arises\nfrom conceptual role. Because conceptual role is defined by the relationships\nbetween internal representational states, meaning cannot be determined from a\nmodel's architecture, training data, or objective function, but only by\nexamination of how its internal states relate to each other. This approach may\nclarify why and how LLMs are so successful and suggest how they can be made\nmore human-like.", "published": "2022-08-05 02:48:26", "link": "http://arxiv.org/abs/2208.02957v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChiQA: A Large Scale Image-based Real-World Question Answering Dataset\n  for Multi-Modal Understanding", "abstract": "Visual question answering is an important task in both natural language and\nvision understanding. However, in most of the public visual question answering\ndatasets such as VQA, CLEVR, the questions are human generated that specific to\nthe given image, such as `What color are her eyes?'. The human generated\ncrowdsourcing questions are relatively simple and sometimes have the bias\ntoward certain entities or attributes. In this paper, we introduce a new\nquestion answering dataset based on image-ChiQA. It contains the real-world\nqueries issued by internet users, combined with several related open-domain\nimages. The system should determine whether the image could answer the question\nor not. Different from previous VQA datasets, the questions are real-world\nimage-independent queries that are more various and unbiased. Compared with\nprevious image-retrieval or image-caption datasets, the ChiQA not only measures\nthe relatedness but also measures the answerability, which demands more\nfine-grained vision and language reasoning. ChiQA contains more than 40K\nquestions and more than 200K question-images pairs. A three-level 2/1/0 label\nis assigned to each pair indicating perfect answer, partially answer and\nirrelevant. Data analysis shows ChiQA requires a deep understanding of both\nlanguage and vision, including grounding, comparisons, and reading. We evaluate\nseveral state-of-the-art visual-language models such as ALBEF, demonstrating\nthat there is still a large room for improvements on ChiQA.", "published": "2022-08-05 07:55:28", "link": "http://arxiv.org/abs/2208.03030v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Knowledge Authoring with Factual English", "abstract": "Knowledge representation and reasoning (KRR) systems represent knowledge as\ncollections of facts and rules. Like databases, KRR systems contain information\nabout domains of human activities like industrial enterprises, science, and\nbusiness. KRRs can represent complex concepts and relations, and they can query\nand manipulate information in sophisticated ways. Unfortunately, the KRR\ntechnology has been hindered by the fact that specifying the requisite\nknowledge requires skills that most domain experts do not have, and\nprofessional knowledge engineers are hard to find. One solution could be to\nextract knowledge from English text, and a number of works have attempted to do\nso (OpenSesame, Google's Sling, etc.). Unfortunately, at present, extraction of\nlogical facts from unrestricted natural language is still too inaccurate to be\nused for reasoning, while restricting the grammar of the language (so-called\ncontrolled natural language, or CNL) is hard for the users to learn and use.\nNevertheless, some recent CNL-based approaches, such as the Knowledge Authoring\nLogic Machine (KALM), have shown to have very high accuracy compared to others,\nand a natural question is to what extent the CNL restrictions can be lifted. In\nthis paper, we address this issue by transplanting the KALM framework to a\nneural natural language parser, mStanza. Here we limit our attention to\nauthoring facts and queries and therefore our focus is what we call factual\nEnglish statements. Authoring other types of knowledge, such as rules, will be\nconsidered in our followup work. As it turns out, neural network based parsers\nhave problems of their own and the mistakes they make range from part-of-speech\ntagging to lemmatization to dependency errors. We present a number of\ntechniques for combating these problems and test the new system, KALMFL (i.e.,\nKALM for factual language), on a number of benchmarks, which show KALMFL\nachieves correctness in excess of 95%.", "published": "2022-08-05 10:49:41", "link": "http://arxiv.org/abs/2208.03094v1", "categories": ["cs.LO", "cs.CL"], "primary_category": "cs.LO"}
{"title": "BlenderBot 3: a deployed conversational agent that continually learns to\n  responsibly engage", "abstract": "We present BlenderBot 3, a 175B parameter dialogue model capable of\nopen-domain conversation with access to the internet and a long-term memory,\nand having been trained on a large number of user defined tasks. We release\nboth the model weights and code, and have also deployed the model on a public\nweb page to interact with organic users. This technical report describes how\nthe model was built (architecture, model and training scheme), and details of\nits deployment, including safety mechanisms. Human evaluations show its\nsuperiority to existing open-domain dialogue agents, including its predecessors\n(Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for\ncontinual learning using the data collected from deployment, which will also be\npublicly released. The goal of this research program is thus to enable the\ncommunity to study ever-improving responsible agents that learn through\ninteraction.", "published": "2022-08-05 14:20:46", "link": "http://arxiv.org/abs/2208.03188v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning New Skills after Deployment: Improving open-domain\n  internet-driven dialogue with human feedback", "abstract": "Frozen models trained to mimic static datasets can never improve their\nperformance. Models that can employ internet-retrieval for up-to-date\ninformation and obtain feedback from humans during deployment provide the\npromise of both adapting to new information, and improving their performance.\nIn this work we study how to improve internet-driven conversational skills in\nsuch a learning framework. We collect deployment data, which we make publicly\navailable, of human interactions, and collect various types of human feedback\n-- including binary quality measurements, free-form text feedback, and\nfine-grained reasons for failure. We then study various algorithms for\nimproving from such feedback, including standard supervised learning, rejection\nsampling, model-guiding and reward-based learning, in order to make\nrecommendations on which type of feedback and algorithms work best. We find the\nrecently introduced Director model (Arora et al., '22) shows significant\nimprovements over other existing approaches.", "published": "2022-08-05 16:41:46", "link": "http://arxiv.org/abs/2208.03270v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Holistic Approach to Undesired Content Detection in the Real World", "abstract": "We present a holistic approach to building a robust and useful natural\nlanguage classification system for real-world content moderation. The success\nof such a system relies on a chain of carefully designed and executed steps,\nincluding the design of content taxonomies and labeling instructions, data\nquality control, an active learning pipeline to capture rare events, and a\nvariety of methods to make the model robust and to avoid overfitting. Our\nmoderation system is trained to detect a broad set of categories of undesired\ncontent, including sexual content, hateful content, violence, self-harm, and\nharassment. This approach generalizes to a wide range of different content\ntaxonomies and can be used to create high-quality content classifiers that\noutperform off-the-shelf models.", "published": "2022-08-05 16:47:23", "link": "http://arxiv.org/abs/2208.03274v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning from data in the mixed adversarial non-adversarial case:\n  Finding the helpers and ignoring the trolls", "abstract": "The promise of interaction between intelligent conversational agents and\nhumans is that models can learn from such feedback in order to improve.\nUnfortunately, such exchanges in the wild will not always involve human\nutterances that are benign or of high quality, and will include a mixture of\nengaged (helpers) and unengaged or even malicious users (trolls). In this work\nwe study how to perform robust learning in such an environment. We introduce a\nbenchmark evaluation, SafetyMix, which can evaluate methods that learn safe vs.\ntoxic language in a variety of adversarial settings to test their robustness.\nWe propose and analyze several mitigating learning algorithms that identify\ntrolls either at the example or at the user level. Our main finding is that\nuser-based methods, that take into account that troll users will exhibit\nadversarial behavior across multiple examples, work best in a variety of\nsettings on our benchmark. We then test these methods in a further real-life\nsetting of conversations collected during deployment, with similar results.", "published": "2022-08-05 17:33:33", "link": "http://arxiv.org/abs/2208.03295v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Going Beyond Approximation: Encoding Constraints for Explainable\n  Multi-hop Inference via Differentiable Combinatorial Solvers", "abstract": "Integer Linear Programming (ILP) provides a viable mechanism to encode\nexplicit and controllable assumptions about explainable multi-hop inference\nwith natural language. However, an ILP formulation is non-differentiable and\ncannot be integrated into broader deep learning architectures. Recently,\nThayaparan et al. (2021a) proposed a novel methodology to integrate ILP with\nTransformers to achieve end-to-end differentiability for complex multi-hop\ninference. While this hybrid framework has been demonstrated to deliver better\nanswer and explanation selection than transformer-based and existing ILP\nsolvers, the neuro-symbolic integration still relies on a convex relaxation of\nthe ILP formulation, which can produce sub-optimal solutions. To improve these\nlimitations, we propose Diff-Comb Explainer, a novel neuro-symbolic\narchitecture based on Differentiable BlackBox Combinatorial solvers (DBCS)\n(Pogan\\v{c}i\\'c et al., 2019). Unlike existing differentiable solvers, the\npresented model does not require the transformation and relaxation of the\nexplicit semantic constraints, allowing for direct and more efficient\nintegration of ILP formulations. Diff-Comb Explainer demonstrates improved\naccuracy and explainability over non-differentiable solvers, Transformers and\nexisting differentiable constraint-based multi-hop inference frameworks.", "published": "2022-08-05 18:07:53", "link": "http://arxiv.org/abs/2208.03339v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Covariant-Contravariant Refinement Modal $\u03bc$-calculus", "abstract": "The notion of covariant-contravariant refinement (CC-refinement, for short)\nis a generalization of the notions of bisimulation, simulation and refinement.\nThis paper introduces CC-refinement modal $\\mu$-calculus (CCRML$^{\\mu}$)\nobtained from the modal $\\mu$-calculus system K$^{\\mu}$ by adding CC-refinement\nquantifiers, establishes an axiom system for CCRML$^{\\mu}$ and explores the\nimportant properties: soundness, completeness and decidability of this axiom\nsystem. The language of CCRML$^{\\mu}$ may be considered as a specification\nlanguage for describing the properties of a system referring to reactive and\ngenerative actions. It may be used to formalize some interesting problems in\nthe field of formal methods.", "published": "2022-08-05 05:24:30", "link": "http://arxiv.org/abs/2208.02989v1", "categories": ["cs.LO", "cs.CL", "cs.FL"], "primary_category": "cs.LO"}
{"title": "Global Pointer: Novel Efficient Span-based Approach for Named Entity\n  Recognition", "abstract": "Named entity recognition (NER) task aims at identifying entities from a piece\nof text that belong to predefined semantic types such as person, location,\norganization, etc. The state-of-the-art solutions for flat entities NER\ncommonly suffer from capturing the fine-grained semantic information in\nunderlying texts. The existing span-based approaches overcome this limitation,\nbut the computation time is still a concern. In this work, we propose a novel\nspan-based NER framework, namely Global Pointer (GP), that leverages the\nrelative positions through a multiplicative attention mechanism. The ultimate\ngoal is to enable a global view that considers the beginning and the end\npositions to predict the entity. To this end, we design two modules to identify\nthe head and the tail of a given entity to enable the inconsistency between the\ntraining and inference processes. Moreover, we introduce a novel classification\nloss function to address the imbalance label problem. In terms of parameters,\nwe introduce a simple but effective approximate method to reduce the training\nparameters. We extensively evaluate GP on various benchmark datasets. Our\nextensive experiments demonstrate that GP can outperform the existing solution.\nMoreover, the experimental results show the efficacy of the introduced loss\nfunction compared to softmax and entropy alternatives.", "published": "2022-08-05 09:19:46", "link": "http://arxiv.org/abs/2208.03054v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large vocabulary speech recognition for languages of Africa:\n  multilingual modeling and self-supervised learning", "abstract": "Almost none of the 2,000+ languages spoken in Africa have widely available\nautomatic speech recognition systems, and the required data is also only\navailable for a few languages. We have experimented with two techniques which\nmay provide pathways to large vocabulary speech recognition for African\nlanguages: multilingual modeling and self-supervised learning. We gathered\navailable open source data and collected data for 15 languages, and trained\nexperimental models using these techniques. Our results show that pooling the\nsmall amounts of data available in multilingual end-to-end models, and\npre-training on unsupervised data can help improve speech recognition quality\nfor many African languages.", "published": "2022-08-05 09:54:19", "link": "http://arxiv.org/abs/2208.03067v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Chronological Self-Training for Real-Time Speaker Diarization", "abstract": "Diarization partitions an audio stream into segments based on the voices of\nthe speakers. Real-time diarization systems that include an enrollment step\nshould limit enrollment training samples to reduce user interaction time.\nAlthough training on a small number of samples yields poor performance, we show\nthat the accuracy can be improved dramatically using a chronological\nself-training approach. We studied the tradeoff between training time and\nclassification performance and found that 1 second is sufficient to reach over\n95% accuracy. We evaluated on 700 audio conversation files of about 10 minutes\neach from 6 different languages and demonstrated average diarization error\nrates as low as 10%.", "published": "2022-08-05 21:45:00", "link": "http://arxiv.org/abs/2208.03393v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hybrid Multimodal Feature Extraction, Mining and Fusion for Sentiment\n  Analysis", "abstract": "In this paper, we present our solutions for the Multimodal Sentiment Analysis\nChallenge (MuSe) 2022, which includes MuSe-Humor, MuSe-Reaction and MuSe-Stress\nSub-challenges. The MuSe 2022 focuses on humor detection, emotional reactions\nand multimodal emotional stress utilizing different modalities and data sets.\nIn our work, different kinds of multimodal features are extracted, including\nacoustic, visual, text and biological features. These features are fused by\nTEMMA and GRU with self-attention mechanism frameworks. In this paper, 1)\nseveral new audio features, facial expression features and paragraph-level text\nembeddings are extracted for accuracy improvement. 2) we substantially improve\nthe accuracy and reliability of multimodal sentiment prediction by mining and\nblending the multimodal features. 3) effective data augmentation strategies are\napplied in model training to alleviate the problem of sample imbalance and\nprevent the model from learning biased subject characters. For the MuSe-Humor\nsub-challenge, our model obtains the AUC score of 0.8932. For the MuSe-Reaction\nsub-challenge, the Pearson's Correlations Coefficient of our approach on the\ntest set is 0.3879, which outperforms all other participants. For the\nMuSe-Stress sub-challenge, our approach outperforms the baseline in both\narousal and valence on the test dataset, reaching a final combined result of\n0.5151.", "published": "2022-08-05 09:07:58", "link": "http://arxiv.org/abs/2208.03051v2", "categories": ["cs.CV", "cs.CL", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "AID: Open-source Anechoic Interferer Dataset", "abstract": "A dataset of anechoic recordings of various sound sources encountered in\ndomestic environments is presented. The dataset is intended to be a resource of\nnon-stationary, environmental noise signals that, when convolved with acoustic\nimpulse responses, can be used to simulate complex acoustic scenes.\nAdditionally, a Python library is provided to generate random mixtures of the\nrecordings in the dataset, which can be used as non-stationary interference\nsignals.", "published": "2022-08-05 07:34:37", "link": "http://arxiv.org/abs/2208.03023v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust Acoustic Domain Identification with its Application to Speaker\n  Diarization", "abstract": "With the rise in multimedia content over the years, more variety is observed\nin the recording environments of audio. An audio processing system might\nbenefit when it has a module to identify the acoustic domain at its front-end.\nIn this paper, we demonstrate the idea of \\emph{acoustic domain identification}\n(ADI) for \\emph{speaker diarization}. For this, we first present a detailed\nstudy of the various domains of the third DIHARD challenge highlighting the\nfactors that differentiated them from each other. Our main contribution is to\ndevelop a simple and efficient solution for ADI. In the present work, we\nexplore speaker embeddings for this task. Next, we integrate the ADI module\nwith the speaker diarization framework of the DIHARD III challenge. The\nperformance substantially improved over that of the baseline when the\nthresholds for agglomerative hierarchical clustering were optimized according\nto the respective domains. We achieved a relative improvement of more than\n$5\\%$ and $8\\%$ in DER for core and full conditions, respectively, on Track 1\nof the DIHARD III evaluation set.", "published": "2022-08-05 13:41:14", "link": "http://arxiv.org/abs/2208.03162v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Model You Can Hear: Audio Identification with Playable Prototypes", "abstract": "Machine learning techniques have proved useful for classifying and analyzing\naudio content. However, recent methods typically rely on abstract and\nhigh-dimensional representations that are difficult to interpret. Inspired by\ntransformation-invariant approaches developed for image and 3D data, we propose\nan audio identification model based on learnable spectral prototypes. Equipped\nwith dedicated transformation networks, these prototypes can be used to cluster\nand classify input audio samples from large collections of sounds. Our model\ncan be trained with or without supervision and reaches state-of-the-art results\nfor speaker and instrument identification, while remaining easily\ninterpretable. The code is available at:\nhttps://github.com/romainloiseau/a-model-you-can-hear", "published": "2022-08-05 17:57:20", "link": "http://arxiv.org/abs/2208.03311v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Feature Learning for Medical Acoustics", "abstract": "The purpose of this paper is to compare different learnable frontends in\nmedical acoustics tasks. A framework has been implemented to classify human\nrespiratory sounds and heartbeats in two categories, i.e. healthy or affected\nby pathologies. After obtaining two suitable datasets, we proceeded to classify\nthe sounds using two learnable state-of-art frontends -- LEAF and nnAudio --\nplus a non-learnable baseline frontend, i.e. Mel-filterbanks. The computed\nfeatures are then fed into two different CNN models, namely VGG16 and\nEfficientNet. The frontends are carefully benchmarked in terms of the number of\nparameters, computational resources, and effectiveness.\n  This work demonstrates how the integration of learnable frontends in neural\naudio classification systems may improve performance, especially in the field\nof medical acoustics. However, the usage of such frameworks makes the needed\namount of data even larger. Consequently, they are useful if the amount of data\navailable for training is adequately large to assist the feature learning\nprocess.", "published": "2022-08-05 10:39:37", "link": "http://arxiv.org/abs/2208.03084v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Time-Frequency Distributions of Heart Sound Signals: A Comparative Study\n  using Convolutional Neural Networks", "abstract": "Time-Frequency Distributions (TFDs) support the heart sound characterisation\nand classification in early cardiac screening. However, despite the frequent\nuse of TFDs in signal analysis, no study comprehensively compared their\nperformances on deep learning for automatic diagnosis. Furthermore, the\ncombination of signal processing methods as inputs for Convolutional Neural\nNetworks (CNNs) has been proved as a practical approach to increasing signal\nclassification performance. Therefore, this study aimed to investigate the\noptimal use of TFD/ combined TFDs as input for CNNs. The presented results\nrevealed that: 1) The transformation of the heart sound signal into the TF\ndomain achieves higher classification performance than using of raw signals.\nAmong the TFDs, the difference in the performance was slight for all the CNN\nmodels (within $1.3\\%$ in average accuracy). However, Continuous wavelet\ntransform (CWT) and Chirplet transform (CT) outperformed the rest. 2) The\nappropriate increase of the CNN capacity and architecture optimisation can\nimprove the performance, while the network architecture should not be overly\ncomplicated. Based on the ResNet or SEResNet family results, the increase in\nthe number of parameters and the depth of the structure do not improve the\nperformance apparently. 3) Combining TFDs as CNN inputs did not significantly\nimprove the classification results. The findings of this study provided the\nknowledge for selecting TFDs as CNN input and designing CNN architecture for\nheart sound classification.", "published": "2022-08-05 12:44:54", "link": "http://arxiv.org/abs/2208.03128v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Variational Autoencoders for Anomaly Detection in Respiratory Sounds", "abstract": "This paper proposes a weakly-supervised machine learning-based approach\naiming at a tool to alert patients about possible respiratory diseases. Various\ntypes of pathologies may affect the respiratory system, potentially leading to\nsevere diseases and, in certain cases, death. In general, effective prevention\npractices are considered as major actors towards the improvement of the\npatient's health condition. The proposed method strives to realize an easily\naccessible tool for the automatic diagnosis of respiratory diseases.\nSpecifically, the method leverages Variational Autoencoder architectures\npermitting the usage of training pipelines of limited complexity and relatively\nsmall-sized datasets. Importantly, it offers an accuracy of 57 %, which is in\nline with the existing strongly-supervised approaches.", "published": "2022-08-05 10:32:08", "link": "http://arxiv.org/abs/2208.03326v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
