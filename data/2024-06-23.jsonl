{"title": "Serial Position Effects of Large Language Models", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in zero-shot\nlearning applications, generating responses to queries using only pre-training\ninformation without the need for additional fine-tuning. This represents a\nsignificant departure from traditional machine learning approaches. Previous\nresearch has indicated that LLMs may exhibit serial position effects, such as\nprimacy and recency biases, which are well-documented cognitive biases in human\npsychology. Our extensive testing across various tasks and models confirms the\nwidespread occurrence of these effects, although their intensity varies. We\nalso discovered that while carefully designed prompts can somewhat mitigate\nthese biases, their effectiveness is inconsistent. These findings underscore\nthe significance of serial position effects during the inference process,\nparticularly in scenarios where there are no ground truth labels, highlighting\nthe need for greater focus on addressing these effects in LLM applications.", "published": "2024-06-23 02:02:52", "link": "http://arxiv.org/abs/2406.15981v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can LLM Graph Reasoning Generalize beyond Pattern Memorization?", "abstract": "Large language models (LLMs) demonstrate great potential for problems with\nimplicit graphical structures, while recent works seek to enhance the graph\nreasoning capabilities of LLMs through specialized instruction tuning. The\nresulting 'graph LLMs' are evaluated with in-distribution settings only, thus\nit remains underexplored whether LLMs are learning generalizable graph\nreasoning skills or merely memorizing patterns in the synthetic training data.\nTo this end, we propose the NLGift benchmark, an evaluation suite of LLM graph\nreasoning generalization: whether LLMs could go beyond semantic, numeric,\nstructural, reasoning patterns in the synthetic training data and improve\nutility on real-world graph-based tasks. Extensive experiments with two LLMs\nacross four graph reasoning tasks demonstrate that while generalization on\nsimple patterns (semantic, numeric) is somewhat satisfactory, LLMs struggle to\ngeneralize across reasoning and real-world patterns, casting doubt on the\nbenefit of synthetic graph tuning for real-world tasks with underlying network\nstructures. We explore three strategies to improve LLM graph reasoning\ngeneralization, and we find that while post-training alignment is most\npromising for real-world tasks, empowering LLM graph reasoning to go beyond\npattern memorization remains an open research question.", "published": "2024-06-23 02:59:15", "link": "http://arxiv.org/abs/2406.15992v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Distributed Rule Vectors is A Key Mechanism in Large Language Models'\n  In-Context Learning", "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities, one of\nthe most important being In-Context Learning (ICL). With ICL, LLMs can derive\nthe underlying rule from a few demonstrations and provide answers that comply\nwith the rule. Previous work hypothesized that the network creates a \"task\nvector\" in specific positions during ICL. Patching the \"task vector\" allows\nLLMs to achieve zero-shot performance similar to few-shot learning. However, we\ndiscover that such \"task vectors\" do not exist in tasks where the rule has to\nbe defined through multiple demonstrations. Instead, the rule information\nprovided by each demonstration is first transmitted to its answer position and\nforms its own rule vector. Importantly, all the rule vectors contribute to the\noutput in a distributed manner. We further show that the rule vectors encode a\nhigh-level abstraction of rules extracted from the demonstrations. These\nresults are further validated in a series of tasks that rely on rules dependent\non multiple demonstrations. Our study provides novel insights into the\nmechanism underlying ICL in LLMs, demonstrating how ICL may be achieved through\nan information aggregation mechanism.", "published": "2024-06-23 04:29:13", "link": "http://arxiv.org/abs/2406.16007v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlocking the Future: Exploring Look-Ahead Planning Mechanistic\n  Interpretability in Large Language Models", "abstract": "Planning, as the core module of agents, is crucial in various fields such as\nembodied agents, web navigation, and tool using. With the development of large\nlanguage models (LLMs), some researchers treat large language models as\nintelligent agents to stimulate and evaluate their planning capabilities.\nHowever, the planning mechanism is still unclear. In this work, we focus on\nexploring the look-ahead planning mechanism in large language models from the\nperspectives of information flow and internal representations. First, we study\nhow planning is done internally by analyzing the multi-layer perception (MLP)\nand multi-head self-attention (MHSA) components at the last token. We find that\nthe output of MHSA in the middle layers at the last token can directly decode\nthe decision to some extent. Based on this discovery, we further trace the\nsource of MHSA by information flow, and we reveal that MHSA mainly extracts\ninformation from spans of the goal states and recent steps. According to\ninformation flow, we continue to study what information is encoded within it.\nSpecifically, we explore whether future decisions have been encoded in advance\nin the representation of flow. We demonstrate that the middle and upper layers\nencode a few short-term future decisions to some extent when planning is\nsuccessful. Overall, our research analyzes the look-ahead planning mechanisms\nof LLMs, facilitating future research on LLMs performing planning tasks.", "published": "2024-06-23 06:54:47", "link": "http://arxiv.org/abs/2406.16033v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dancing in the syntax forest: fast, accurate and explainable sentiment\n  analysis with SALSA", "abstract": "Sentiment analysis is a key technology for companies and institutions to\ngauge public opinion on products, services or events. However, for large-scale\nsentiment analysis to be accessible to entities with modest computational\nresources, it needs to be performed in a resource-efficient way. While some\nefficient sentiment analysis systems exist, they tend to apply shallow\nheuristics, which do not take into account syntactic phenomena that can\nradically change sentiment. Conversely, alternatives that take syntax into\naccount are computationally expensive. The SALSA project, funded by the\nEuropean Research Council under a Proof-of-Concept Grant, aims to leverage\nrecently-developed fast syntactic parsing techniques to build sentiment\nanalysis systems that are lightweight and efficient, while still providing\naccuracy and explainability through the explicit use of syntax. We intend our\napproaches to be the backbone of a working product of interest for SMEs to use\nin production.", "published": "2024-06-23 10:47:01", "link": "http://arxiv.org/abs/2406.16071v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "First Heuristic Then Rational: Dynamic Use of Heuristics in Language\n  Model Reasoning", "abstract": "Multi-step reasoning instruction, such as chain-of-thought prompting, is\nwidely adopted to explore better language models (LMs) performance. We report\non the systematic strategy that LMs employ in such a multi-step reasoning\nprocess. Our controlled experiments reveal that LMs rely more heavily on\nheuristics, such as lexical overlap, in the earlier stages of reasoning, where\nmore reasoning steps remain to reach a goal. Conversely, their reliance on\nheuristics decreases as LMs progress closer to the final answer through\nmultiple reasoning steps. This suggests that LMs can backtrack only a limited\nnumber of future steps and dynamically combine heuristic strategies with\nrationale ones in tasks involving multi-step reasoning.", "published": "2024-06-23 11:11:46", "link": "http://arxiv.org/abs/2406.16078v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SEAM: A Stochastic Benchmark for Multi-Document Tasks", "abstract": "Various tasks, such as summarization, multi-hop question answering, or\ncoreference resolution, are naturally phrased over collections of real-world\ndocuments. Such tasks present a unique set of challenges, revolving around the\nlack of coherent narrative structure across documents, which often leads to\ncontradiction, omission, or repetition of information. Despite their real-world\napplication and challenging properties, there is currently no benchmark which\nspecifically measures the abilities of large language models (LLMs) on\nmulti-document tasks. To bridge this gap, we present SEAM (a Stochastic\nEvaluation Approach for Multi-document tasks), a conglomerate benchmark over a\ndiverse set of multi-document datasets, setting conventional evaluation\ncriteria, input-output formats, and evaluation protocols. In particular, SEAM\naddresses the sensitivity of LLMs to minor prompt variations through repeated\nevaluations, where in each evaluation we sample uniformly at random the values\nof arbitrary factors (e.g., the order of documents). We evaluate different LLMs\non SEAM finding that multi-document tasks pose a significant challenge for\nLLMs, even for state-of-the-art models with 70B parameters. In addition, we\nshow that the stochastic approach uncovers underlying statistical trends which\ncannot be observed in a static benchmark. We hope that SEAM will spur progress\nvia consistent and meaningful evaluation of multi-document tasks.", "published": "2024-06-23 11:57:53", "link": "http://arxiv.org/abs/2406.16086v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chain-of-Probe: Examining the Necessity and Accuracy of CoT Step-by-Step", "abstract": "Current research found the issue of Early Answering in large language models\n(LLMs), where the models already have an answer before generating the\nChain-of-Thought (CoT). This phenomenon suggests a potential lack of necessary\ndependency between the predicted answer and the reasoning process.\nConsequently, two important questions arise: (1) Is CoT still necessary if the\nmodel already has an answer? (2) Can the correctness of the answer serve as\nvalid evidence for the correctness of CoT? To address these questions, we\npropose a method, namely Chain-of-Probe (CoP), to probe changes in the mind\nduring the model's reasoning. The probing results show that in a significant\nnumber of question-answer cases, CoT appears to be unnecessary, and this\nnecessity correlates with the simplicity of the task, defined by reasoning\nsteps required. Furthermore, by analyzing patterns in mind change, we examine\nthe correctness of the model's reasoning. Our validation reveals that many\nresponses, although correct in their final answer, contain errors in their\nreasoning process. To this end, we propose a strategic approach based on CoP to\nprioritize answers with correct reasoning among multiple candidates, thereby\nbolstering the reliability of the model's reasoning.", "published": "2024-06-23 15:50:22", "link": "http://arxiv.org/abs/2406.16144v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Region-aware Bias Evaluation Metrics", "abstract": "When exposed to human-generated data, language models are known to learn and\namplify societal biases. While previous works introduced benchmarks that can be\nused to assess the bias in these models, they rely on assumptions that may not\nbe universally true. For instance, a gender bias dimension commonly used by\nthese metrics is that of family--career, but this may not be the only common\nbias in certain regions of the world. In this paper, we identify topical\ndifferences in gender bias across different regions and propose a region-aware\nbottom-up approach for bias assessment. Our proposed approach uses\ngender-aligned topics for a given region and identifies gender bias dimensions\nin the form of topic pairs that are likely to capture gender societal biases.\nSeveral of our proposed bias topic pairs are on par with human perception of\ngender biases in these regions in comparison to the existing ones, and we also\nidentify new pairs that are more aligned than the existing ones. In addition,\nwe use our region-aware bias topic pairs in a Word Embedding Association Test\n(WEAT)-based evaluation metric to test for gender biases across different\nregions in different data domains. We also find that LLMs have a higher\nalignment to bias pairs for highly-represented regions showing the importance\nof region-aware bias evaluation metric.", "published": "2024-06-23 16:26:27", "link": "http://arxiv.org/abs/2406.16152v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy\n  in Large Language Models", "abstract": "We present a novel extension to Retrieval Augmented Generation with the goal\nof mitigating factual inaccuracies in the output of large language models.\nSpecifically, our method draws on the cognitive linguistic theory of frame\nsemantics for the indexing and retrieval of factual information relevant to\nhelping large language models answer queries. We conduct experiments to\ndemonstrate the effectiveness of this method both in terms of retrieval\neffectiveness and in terms of the relevance of the frames and frame relations\nautomatically generated. Our results show that this novel mechanism of Frame\nSemantic-based retrieval, designed to improve Retrieval Augmented Generation\n(FS-RAG), is effective and offers potential for providing data-driven insights\ninto frame semantics theory. We provide open access to our program code and\nprompts.", "published": "2024-06-23 17:18:19", "link": "http://arxiv.org/abs/2406.16167v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs' Classification Performance is Overclaimed", "abstract": "In many classification tasks designed for AI or human to solve, gold labels\nare typically included within the label space by default, often posed as \"which\nof the following is correct?\" This standard setup has traditionally highlighted\nthe strong performance of advanced AI, particularly top-performing Large\nLanguage Models (LLMs), in routine classification tasks. However, when the gold\nlabel is intentionally excluded from the label space, it becomes evident that\nLLMs still attempt to select from the available label candidates, even when\nnone are correct. This raises a pivotal question: Do LLMs truly demonstrate\ntheir intelligence in understanding the essence of classification tasks?\n  In this study, we evaluate both closed-source and open-source LLMs across\nrepresentative classification tasks, arguing that the perceived performance of\nLLMs is overstated due to their inability to exhibit the expected comprehension\nof the task. This paper makes a threefold contribution: i) To our knowledge,\nthis is the first work to identify the limitations of LLMs in classification\ntasks when gold labels are absent. We define this task as Classify-w/o-Gold and\npropose it as a new testbed for LLMs. ii) We introduce a benchmark, Know-No,\ncomprising two existing classification tasks and one new task, to evaluate\nClassify-w/o-Gold. iii) This work defines and advocates for a new evaluation\nmetric, OmniAccuracy, which assesses LLMs' performance in classification tasks\nboth when gold labels are present and absent.", "published": "2024-06-23 19:49:10", "link": "http://arxiv.org/abs/2406.16203v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Objective Linguistic Control of Large Language Models", "abstract": "Large language models (LLMs), despite their breakthroughs on many challenging\nbenchmark tasks, lean to generate verbose responses and lack the\ncontrollability of output complexity, which is usually preferred by human users\nin practice. In this paper, we study how to precisely control multiple\nlinguistic complexities of LLM output by finetuning using off-the-shelf data.\nTo this end, we propose multi-control tuning (MCTune), which includes multiple\nlinguistic complexity values of ground-truth responses as controls in the input\nfor instruction tuning. We finetune LLaMA2-7B on Alpaca-GPT4 and WizardLM\ndatasets. Evaluations on widely used benchmarks demonstrate that our method\ndoes not only improve LLMs' multi-complexity controllability substantially but\nalso retains or even enhances the quality of the responses as a side benefit.", "published": "2024-06-23 21:56:48", "link": "http://arxiv.org/abs/2406.16229v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Effectiveness of the Foundational Models for Q&A\n  Classification in Mental Health care", "abstract": "Pre-trained Language Models (PLMs) have the potential to transform mental\nhealth support by providing accessible and culturally sensitive resources.\nHowever, despite this potential, their effectiveness in mental health care and\nspecifically for the Arabic language has not been extensively explored. To\nbridge this gap, this study evaluates the effectiveness of foundational models\nfor classification of Questions and Answers (Q&A) in the domain of mental\nhealth care. We leverage the MentalQA dataset, an Arabic collection featuring\nQ&A interactions related to mental health. In this study, we conducted\nexperiments using four different types of learning approaches: traditional\nfeature extraction, PLMs as feature extractors, Fine-tuning PLMs and prompting\nlarge language models (GPT-3.5 and GPT-4) in zero-shot and few-shot learning\nsettings. While traditional feature extractors combined with Support Vector\nMachines (SVM) showed promising performance, PLMs exhibited even better results\ndue to their ability to capture semantic meaning. For example, MARBERT achieved\nthe highest performance with a Jaccard Score of 0.80 for question\nclassification and a Jaccard Score of 0.86 for answer classification. We\nfurther conducted an in-depth analysis including examining the effects of\nfine-tuning versus non-fine-tuning, the impact of varying data size, and\nconducting error analysis. Our analysis demonstrates that fine-tuning proved to\nbe beneficial for enhancing the performance of PLMs, and the size of the\ntraining data played a crucial role in achieving high performance. We also\nexplored prompting, where few-shot learning with GPT-3.5 yielded promising\nresults. There was an improvement of 12% for question and classification and\n45% for answer classification. Based on our findings, it can be concluded that\nPLMs and prompt-based approaches hold promise for mental health support in\nArabic.", "published": "2024-06-23 00:11:07", "link": "http://arxiv.org/abs/2406.15966v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ReCaLL: Membership Inference via Relative Conditional Log-Likelihoods", "abstract": "The rapid scaling of large language models (LLMs) has raised concerns about\nthe transparency and fair use of the pretraining data used for training them.\nDetecting such content is challenging due to the scale of the data and limited\nexposure of each instance during training. We propose ReCaLL (Relative\nConditional Log-Likelihood), a novel membership inference attack (MIA) to\ndetect LLMs' pretraining data by leveraging their conditional language modeling\ncapabilities. ReCaLL examines the relative change in conditional\nlog-likelihoods when prefixing target data points with non-member context. Our\nempirical findings show that conditioning member data on non-member prefixes\ninduces a larger decrease in log-likelihood compared to non-member data. We\nconduct comprehensive experiments and show that ReCaLL achieves\nstate-of-the-art performance on the WikiMIA dataset, even with random and\nsynthetic prefixes, and can be further improved using an ensemble approach.\nMoreover, we conduct an in-depth analysis of LLMs' behavior with different\nmembership contexts, providing insights into how LLMs leverage membership\ninformation for effective inference at both the sequence and token level.", "published": "2024-06-23 00:23:13", "link": "http://arxiv.org/abs/2406.15968v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Cross-Document Event Coreference Resolution by Discourse\n  Structure and Semantic Information", "abstract": "Existing cross-document event coreference resolution models, which either\ncompute mention similarity directly or enhance mention representation by\nextracting event arguments (such as location, time, agent, and patient),\nlacking the ability to utilize document-level information. As a result, they\nstruggle to capture long-distance dependencies. This shortcoming leads to their\nunderwhelming performance in determining coreference for the events where their\nargument information relies on long-distance dependencies. In light of these\nlimitations, we propose the construction of document-level Rhetorical Structure\nTheory (RST) trees and cross-document Lexical Chains to model the structural\nand semantic information of documents. Subsequently, cross-document\nheterogeneous graphs are constructed and GAT is utilized to learn the\nrepresentations of events. Finally, a pair scorer calculates the similarity\nbetween each pair of events and co-referred events can be recognized using\nstandard clustering algorithm. Additionally, as the existing cross-document\nevent coreference datasets are limited to English, we have developed a\nlarge-scale Chinese cross-document event coreference dataset to fill this gap,\nwhich comprises 53,066 event mentions and 4,476 clusters. After applying our\nmodel on the English and Chinese datasets respectively, it outperforms all\nbaselines by large margins.", "published": "2024-06-23 02:54:48", "link": "http://arxiv.org/abs/2406.15990v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Memorizing Documents with Guidance in Large Language Models", "abstract": "Training data plays a pivotal role in AI models. Large language models (LLMs)\nare trained with massive amounts of documents, and their parameters hold\ndocument-related contents. Recently, several studies identified\ncontent-specific locations in LLMs by examining the parameters. Instead of the\npost hoc interpretation, we propose another approach. We propose document-wise\nmemory architecture to track document memories in training. The proposed\narchitecture maps document representations to memory entries, which softly mask\nmemories in the forward process of LLMs. Additionally, we propose document\nguidance loss, which increases the likelihood of text with document memories\nand reduces the likelihood of the text with the memories of other documents.\nExperimental results on Wikitext-103-v1 with Pythia-1B show that the proposed\nmethods provide different memory entries for documents and high recall of\ndocument-related content in generation with trained document-wise memories.", "published": "2024-06-23 03:12:03", "link": "http://arxiv.org/abs/2406.15996v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Harvesting Events from Multiple Sources: Towards a Cross-Document Event\n  Extraction Paradigm", "abstract": "Document-level event extraction aims to extract structured event information\nfrom unstructured text. However, a single document often contains limited event\ninformation and the roles of different event arguments may be biased due to the\ninfluence of the information source. This paper addresses the limitations of\ntraditional document-level event extraction by proposing the task of\ncross-document event extraction (CDEE) to integrate event information from\nmultiple documents and provide a comprehensive perspective on events. We\nconstruct a novel cross-document event extraction dataset, namely CLES, which\ncontains 20,059 documents and 37,688 mention-level events, where over 70% of\nthem are cross-document. To build a benchmark, we propose a CDEE pipeline that\nincludes 5 steps, namely event extraction, coreference resolution, entity\nnormalization, role normalization and entity-role resolution. Our CDEE pipeline\nachieves about 72% F1 in end-to-end cross-document event extraction, suggesting\nthe challenge of this task. Our work builds a new line of information\nextraction research and will attract new research attention.", "published": "2024-06-23 06:01:11", "link": "http://arxiv.org/abs/2406.16021v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Cross-Lingual NER Using Phonemic Representations for\n  Low-Resource Languages", "abstract": "Existing zero-shot cross-lingual NER approaches require substantial prior\nknowledge of the target language, which is impractical for low-resource\nlanguages. In this paper, we propose a novel approach to NER using phonemic\nrepresentation based on the International Phonetic Alphabet (IPA) to bridge the\ngap between representations of different languages. Our experiments show that\nour method significantly outperforms baseline models in extremely low-resource\nlanguages, with the highest average F1 score (46.38%) and lowest standard\ndeviation (12.67), particularly demonstrating its robustness with non-Latin\nscripts. Our codes are available at\nhttps://github.com/Gabriel819/zeroshot_ner.git", "published": "2024-06-23 06:38:56", "link": "http://arxiv.org/abs/2406.16030v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PORT: Preference Optimization on Reasoning Traces", "abstract": "Preference optimization methods have been successfully applied to improve not\nonly the alignment of large language models (LLMs) with human values, but also\nspecific natural language tasks such as summarization and stylistic\ncontinuations. This paper proposes using preference optimization methods on\nChain-of-Thought steps in order to improve the mathematical reasoning\nperformances of language models. While the chosen answers are obtained from\ndatasets that include reasoning traces, we propose two complementary schemes\nfor generating rejected answers: weak LLM prompting, and digit corruption. Our\napproach leads to increased accuracy on the GSM8K and AQuA-RAT mathematical\nreasoning benchmarks for Falcon2-11B and Mistral-7B. Additionally, the improved\nabilities transfer to non-mathematical tasks, including the ARC benchmark and\nsymbolic reasoning challenges. For example, our method can lead to up to\nrelative 8.47% and 18.73% increases in accuracy on the GSM8K and AQuA\nbenchmarks respectively, without any extra annotations. This work suggests that\nthe path towards better language reasoning abilities goes through spending\nresources on creating high-quality datasets of reasoning traces.", "published": "2024-06-23 09:51:06", "link": "http://arxiv.org/abs/2406.16061v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large\n  Language Models", "abstract": "Large language models (LLMs) excel in generating coherent text, but they\noften struggle with context awareness, leading to inaccuracies in tasks\nrequiring faithful adherence to provided information. We introduce FastMem, a\nnovel method designed to enhance instruction fine-tuned LLMs' context awareness\nthrough fast memorization of the prompt. FastMem maximizes the likelihood of\nthe prompt before inference by updating only the last Feed-Forward Network\n(FFN) module. This targeted approach ensures efficient optimization without\noverfitting, significantly improving the model's ability to comprehend and\naccurately follow the context. Our experiments demonstrate substantial gains in\nreading comprehension, text summarization and adherence to output structures.\nFor instance, FastMem improves the accuracy of Llama 3-8B-Inst on the NQ-SWAP\ndataset from 59.1% to 71.6%, and reduces the output structure failure rate of\nQwen 1.5-4B-Chat from 34.9% to 25.5%. Extensive experimental results highlight\nFastMem's potential to offer a robust solution to enhance the reliability and\naccuracy of LLMs in various applications. Our code is available at:\nhttps://github.com/IAAR-Shanghai/FastMem", "published": "2024-06-23 10:36:35", "link": "http://arxiv.org/abs/2406.16069v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EERPD: Leveraging Emotion and Emotion Regulation for Improving\n  Personality Detection", "abstract": "Personality is a fundamental construct in psychology, reflecting an\nindividual's behavior, thinking, and emotional patterns. Previous researches\nhave made some progress in personality detection, primarily by utilizing the\nwhole text to predict personality. However, these studies generally tend to\noverlook psychological knowledge: they rarely apply the well-established\ncorrelations between emotion regulation and personality. Based on this, we\npropose a new personality detection method called EERPD. This method introduces\nthe use of emotion regulation, a psychological concept highly correlated with\npersonality, for personality prediction. By combining this feature with emotion\nfeatures, it retrieves few-shot examples and provides process CoTs for\ninferring labels from text. This approach enhances the understanding of LLM for\npersonality within text and improves the performance in personality detection.\nExperimental results demonstrate that EERPD significantly enhances the accuracy\nand robustness of personality detection, outperforming previous SOTA by\n15.05/4.29 in average F1 on the two benchmark datasets.", "published": "2024-06-23 11:18:55", "link": "http://arxiv.org/abs/2406.16079v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Decoder-only Architecture for Streaming End-to-end Speech Recognition", "abstract": "Decoder-only language models (LMs) have been successfully adopted for\nspeech-processing tasks including automatic speech recognition (ASR). The LMs\nhave ample expressiveness and perform efficiently. This efficiency is a\nsuitable characteristic for streaming applications of ASR. In this work, we\npropose to use a decoder-only architecture for blockwise streaming ASR. In our\napproach, speech features are compressed using CTC output and context embedding\nusing blockwise speech subnetwork, and are sequentially provided as prompts to\nthe decoder. The decoder estimates the output tokens promptly at each block. To\nthis end, we also propose a novel training scheme using random-length prefix\nprompts to make the model robust to the truncated prompts caused by blockwise\nprocessing. An experimental comparison shows that our proposed decoder-only\nstreaming ASR achieves 8% relative word error rate reduction in the LibriSpeech\ntest-other set while being twice as fast as the baseline model.", "published": "2024-06-23 13:50:08", "link": "http://arxiv.org/abs/2406.16107v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Crosslingual Capabilities and Knowledge Barriers in Multilingual Large\n  Language Models", "abstract": "Large language models (LLMs) are typically multilingual due to pretraining on\ndiverse multilingual corpora. But can these models relate corresponding\nconcepts across languages, i.e., be crosslingual? This study evaluates\nstate-of-the-art LLMs on inherently crosslingual tasks. We observe that while\nthese models show promising surface-level crosslingual abilities on machine\ntranslation and embedding space analyses, they struggle with deeper\ncrosslingual knowledge transfer, revealing a crosslingual knowledge barrier in\nboth general (MMLU benchmark) and domain-specific (Harry Potter quiz and TOFU\nbenchmark) contexts. Since simple inference-time mitigation methods offer only\nlimited improvement, we propose fine-tuning of LLMs on mixed-language data,\nwhich effectively reduces these gaps, even when using out-of-domain datasets\nlike WikiText. Our findings suggest the need for explicit optimization to\nunlock the full crosslingual potential of LLMs. Our code is publicly available\nat https://github.com/google-research/crosslingual-knowledge-barriers.", "published": "2024-06-23 15:15:17", "link": "http://arxiv.org/abs/2406.16135v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Continuous Output Personality Detection Models via Mixed Strategy\n  Training", "abstract": "The traditional personality models only yield binary results. This paper\npresents a novel approach for training personality detection models that\nproduce continuous output values, using mixed strategies. By leveraging the\nPANDORA dataset, which includes extensive personality labeling of Reddit\ncomments, we developed models that predict the Big Five personality traits with\nhigh accuracy. Our approach involves fine-tuning a RoBERTa-base model with\nvarious strategies such as Multi-Layer Perceptron (MLP) integration, and\nhyperparameter tuning. The results demonstrate that our models significantly\noutperform traditional binary classification methods, offering precise\ncontinuous outputs for personality traits, thus enhancing applications in AI,\npsychology, human resources, marketing and health care fields.", "published": "2024-06-23 21:32:15", "link": "http://arxiv.org/abs/2406.16223v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Commentary Strategies for Imperfect Information Card Games: A\n  Study of Large Language Models in Guandan Commentary", "abstract": "Recent advancements in large language models (LLMs) have unlocked the\npotential for generating high-quality game commentary. However, producing\ninsightful and engaging commentary for complex games with incomplete\ninformation remains a significant challenge. In this paper, we introduce a\nnovel commentary method that combine Reinforcement Learning (RL) and LLMs,\ntailored specifically for the Chinese card game \\textit{Guandan}. Our system\nleverages RL to generate intricate card-playing scenarios and employs LLMs to\ngenerate corresponding commentary text, effectively emulating the strategic\nanalysis and narrative prowess of professional commentators. The framework\ncomprises a state commentary guide, a Theory of Mind (ToM)-based strategy\nanalyzer, and a style retrieval module, which seamlessly collaborate to deliver\ndetailed and context-relevant game commentary in the Chinese language\nenvironment. We empower LLMs with ToM capabilities and refine both retrieval\nand information filtering mechanisms. This facilitates the generation of\npersonalized commentary content. Our experimental results showcase the\nsubstantial enhancement in performance achieved by the proposed commentary\nframework when applied to open-source LLMs, surpassing the performance of GPT-4\nacross multiple evaluation metrics.", "published": "2024-06-23 11:58:26", "link": "http://arxiv.org/abs/2406.17807v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Effectiveness of ChatGPT in explaining complex medical reports to\n  patients", "abstract": "Electronic health records contain detailed information about the medical\ncondition of patients, but they are difficult for patients to understand even\nif they have access to them. We explore whether ChatGPT (GPT 4) can help\nexplain multidisciplinary team (MDT) reports to colorectal and prostate cancer\npatients. These reports are written in dense medical language and assume\nclinical knowledge, so they are a good test of the ability of ChatGPT to\nexplain complex medical reports to patients. We asked clinicians and lay people\n(not patients) to review explanations and responses of ChatGPT. We also ran\nthree focus groups (including cancer patients, caregivers, computer scientists,\nand clinicians) to discuss output of ChatGPT. Our studies highlighted issues\nwith inaccurate information, inappropriate language, limited personalization,\nAI distrust, and challenges integrating large language models (LLMs) into\nclinical workflow. These issues will need to be resolved before LLMs can be\nused to explain complex personal medical information to patients.", "published": "2024-06-23 00:04:07", "link": "http://arxiv.org/abs/2406.15963v1", "categories": ["cs.HC", "cs.CL", "q-bio.OT"], "primary_category": "cs.HC"}
{"title": "Found in the Middle: Calibrating Positional Attention Bias Improves Long\n  Context Utilization", "abstract": "Large language models (LLMs), even when specifically trained to process long\ninput contexts, struggle to capture relevant information located in the middle\nof their input. This phenomenon has been known as the lost-in-the-middle\nproblem. In this work, we make three contributions. First, we set out to\nunderstand the factors that cause this phenomenon. In doing so, we establish a\nconnection between lost-in-the-middle to LLMs' intrinsic attention bias: LLMs\nexhibit a U-shaped attention bias where the tokens at the beginning and at the\nend of its input receive higher attention, regardless of their relevance.\nSecond, we mitigate this positional bias through a calibration mechanism,\nfound-in-the-middle, that allows the model to attend to contexts faithfully\naccording to their relevance, even though when they are in the middle. Third,\nwe show found-in-the-middle not only achieves better performance in locating\nrelevant information within a long context, but also eventually leads to\nimproved retrieval-augmented generation (RAG) performance across various tasks,\noutperforming existing methods by up to 15 percentage points. These findings\nopen up future directions in understanding LLM attention bias and its potential\nconsequences.", "published": "2024-06-23 04:35:42", "link": "http://arxiv.org/abs/2406.16008v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Database-Augmented Query Representation for Information Retrieval", "abstract": "Information retrieval models that aim to search for the documents relevant to\nthe given query have shown many successes, which have been applied to diverse\ntasks. However, the query provided by the user is oftentimes very short, which\nchallenges the retrievers to correctly fetch relevant documents. To tackle\nthis, existing studies have proposed expanding the query with a couple of\nadditional (user-related) features related to the query. Yet, they may be\nsuboptimal to effectively augment the query, though there is plenty of\ninformation available to augment it in a relational database. Motivated by\nthis, we present a novel retrieval framework called Database-Augmented Query\nrepresentation (DAQu), which augments the original query with various\n(query-related) metadata across multiple tables. In addition, as the number of\nfeatures in the metadata can be very large and there is no order among them, we\nencode them with our graph-based set encoding strategy, which considers\nhierarchies of features in the database without order. We validate DAQu in\ndiverse retrieval scenarios that can incorporate metadata from the relational\ndatabase, demonstrating that ours significantly enhances overall retrieval\nperformance, compared to existing query augmentation methods.", "published": "2024-06-23 05:02:21", "link": "http://arxiv.org/abs/2406.16013v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "AudioBench: A Universal Benchmark for Audio Large Language Models", "abstract": "We introduce AudioBench, a universal benchmark designed to evaluate Audio\nLarge Language Models (AudioLLMs). It encompasses 8 distinct tasks and 26\ndatasets, among which, 7 are newly proposed datasets. The evaluation targets\nthree main aspects: speech understanding, audio scene understanding, and voice\nunderstanding (paralinguistic). Despite recent advancements, there lacks a\ncomprehensive benchmark for AudioLLMs on instruction following capabilities\nconditioned on audio signals. AudioBench addresses this gap by setting up\ndatasets as well as desired evaluation metrics. Besides, we also evaluated the\ncapabilities of five popular models and found that no single model excels\nconsistently across all tasks. We outline the research outlook for AudioLLMs\nand anticipate that our open-sourced evaluation toolkit, data, and leaderboard\nwill offer a robust testbed for future model developments.", "published": "2024-06-23 05:40:26", "link": "http://arxiv.org/abs/2406.16020v4", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Contextualized End-to-end Automatic Speech Recognition with Intermediate\n  Biasing Loss", "abstract": "Contextualized end-to-end automatic speech recognition has been an active\nresearch area, with recent efforts focusing on the implicit learning of\ncontextual phrases based on the final loss objective. However, these approaches\nignore the useful contextual knowledge encoded in the intermediate layers. We\nhypothesize that employing explicit biasing loss as an auxiliary task in the\nencoder intermediate layers may better align text tokens or audio frames with\nthe desired objectives. Our proposed intermediate biasing loss brings more\nregularization and contextualization to the network. Our method outperforms a\nconventional contextual biasing baseline on the LibriSpeech corpus, achieving a\nrelative improvement of 22.5% in biased word error rate (B-WER) and up to 44%\ncompared to the non-contextual baseline with a biasing list size of 100.\nMoreover, employing RNN-transducer-driven joint decoding further reduces the\nunbiased word error rate (U-WER), resulting in a more robust network.", "published": "2024-06-23 14:22:59", "link": "http://arxiv.org/abs/2406.16120v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "GraphEval36K: Benchmarking Coding and Reasoning Capabilities of Large\n  Language Models on Graph Datasets", "abstract": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing (NLP), demonstrating significant capabilities in processing\nand understanding text data. However, recent studies have identified\nlimitations in LLMs' ability to manipulate, program, and reason about\nstructured data, especially graphs. We introduce GraphEval36K, the first\ncomprehensive graph dataset, comprising 40 graph coding problems and 36,900\ntest cases to evaluate the ability of LLMs on graph problem-solving. Our\ndataset is categorized into eight primary and four sub-categories to ensure a\nthorough evaluation across different types of graphs. We benchmark ten LLMs,\nfinding that private models outperform open-source ones, though the gap is\nnarrowing. We also analyze the performance of LLMs across directed vs\nundirected graphs, different kinds of graph concepts, and network models.\nFurthermore, to improve the usability of our evaluation framework, we propose\nStructured Symbolic Decomposition (SSD), an instruction-based method designed\nto enhance LLM performance on complex graph tasks. Results show that SSD\nimproves the average passing rate of GPT-4, GPT-4o, Gemini-Pro and\nClaude-3-Sonnet by 8.38%, 6.78%, 29.28% and 25.28%, respectively.", "published": "2024-06-23 18:01:56", "link": "http://arxiv.org/abs/2406.16176v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "H.2.8, I.2.6, I.2.7"], "primary_category": "cs.AI"}
{"title": "Blind Baselines Beat Membership Inference Attacks for Foundation Models", "abstract": "Membership inference (MI) attacks try to determine if a data sample was used\nto train a machine learning model. For foundation models trained on unknown Web\ndata, MI attacks are often used to detect copyrighted training materials,\nmeasure test set contamination, or audit machine unlearning. Unfortunately, we\nfind that evaluations of MI attacks for foundation models are flawed, because\nthey sample members and non-members from different distributions. For 8\npublished MI evaluation datasets, we show that blind attacks -- that\ndistinguish the member and non-member distributions without looking at any\ntrained model -- outperform state-of-the-art MI attacks. Existing evaluations\nthus tell us nothing about membership leakage of a foundation model's training\ndata.", "published": "2024-06-23 19:40:11", "link": "http://arxiv.org/abs/2406.16201v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Preference Tuning For Toxicity Mitigation Generalizes Across Languages", "abstract": "Detoxifying multilingual Large Language Models (LLMs) has become crucial due\nto their increasing global use. In this work, we explore zero-shot\ncross-lingual generalization of preference tuning in detoxifying LLMs. Unlike\nprevious studies that show limited cross-lingual generalization for other\nsafety tasks, we demonstrate that Direct Preference Optimization (DPO) training\nwith only English data can significantly reduce toxicity in multilingual\nopen-ended generations. For example, the probability of mGPT-1.3B generating\ntoxic continuations drops from 46.8% to 3.9% across 17 different languages\nafter training. Our results also extend to other multilingual LLMs, such as\nBLOOM, Llama3, and Aya-23. Using mechanistic interpretability tools like causal\nintervention and activation analysis, we identified the dual multilinguality\nproperty of MLP layers in LLMs, which explains the cross-lingual generalization\nof DPO. Finally, we show that bilingual sentence retrieval can predict the\ncross-lingual transferability of DPO preference tuning.", "published": "2024-06-23 22:53:47", "link": "http://arxiv.org/abs/2406.16235v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unveiling LLM Mechanisms Through Neural ODEs and Control Theory", "abstract": "This paper proposes a framework combining Neural Ordinary Differential\nEquations (Neural ODEs) and robust control theory to enhance the\ninterpretability and control of large language models (LLMs). By utilizing\nNeural ODEs to model the dynamic evolution of input-output relationships and\nintroducing control mechanisms to optimize output quality, we demonstrate the\neffectiveness of this approach across multiple question-answer datasets.\nExperimental results show that the integration of Neural ODEs and control\ntheory significantly improves output consistency and model interpretability,\nadvancing the development of explainable AI technologies.", "published": "2024-06-23 22:56:34", "link": "http://arxiv.org/abs/2406.16985v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Do Large Language Models Understand Verbal Indicators of Romantic\n  Attraction?", "abstract": "What makes people 'click' on a first date and become mutually attracted to\none another? While understanding and predicting the dynamics of romantic\ninteractions used to be exclusive to human judgment, we show that Large\nLanguage Models (LLMs) can detect romantic attraction during brief\ngetting-to-know-you interactions. Examining data from 964 speed dates, we show\nthat ChatGPT (and Claude 3) can predict both objective and subjective\nindicators of speed dating success (r=0.12-0.23). ChatGPT's predictions of\nactual matching (i.e., the exchange of contact information) were not only on\npar with those of human judges who had access to the same information but\nincremental to speed daters' own predictions. While some of the variance in\nChatGPT's predictions can be explained by common content dimensions (such as\nthe valence of the conversations) the fact that there remains a substantial\nproportion of unexplained variance suggests that ChatGPT also picks up on\nconversational dynamics. In addition, ChatGPT's judgments showed substantial\noverlap with those made by the human observers (mean r=0.29), highlighting\nsimilarities in their representation of romantic attraction that is, partially,\nindependent of accuracy.", "published": "2024-06-23 17:50:30", "link": "http://arxiv.org/abs/2407.10989v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "INDICT: Code Generation with Internal Dialogues of Critiques for Both\n  Security and Helpfulness", "abstract": "Large language models (LLMs) for code are typically trained to align with\nnatural language instructions to closely follow their intentions and\nrequirements. However, in many practical scenarios, it becomes increasingly\nchallenging for these models to navigate the intricate boundary between\nhelpfulness and safety, especially against highly complex yet potentially\nmalicious instructions. In this work, we introduce INDICT: a new framework that\nempowers LLMs with Internal Dialogues of Critiques for both safety and\nhelpfulness guidance. The internal dialogue is a dual cooperative system\nbetween a safety-driven critic and a helpfulness-driven critic. Each critic\nprovides analysis against the given task and corresponding generated response,\nequipped with external knowledge queried through relevant code snippets and\ntools like web search and code interpreter. We engage the dual critic system in\nboth code generation stage as well as code execution stage, providing\npreemptive and post-hoc guidance respectively to LLMs. We evaluated INDICT on 8\ndiverse tasks across 8 programming languages from 5 benchmarks, using LLMs from\n7B to 70B parameters. We observed that our approach can provide an advanced\nlevel of critiques of both safety and helpfulness analysis, significantly\nimproving the quality of output codes ($+10\\%$ absolute improvements in all\nmodels).", "published": "2024-06-23 15:55:07", "link": "http://arxiv.org/abs/2407.02518v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR", "cs.MA", "cs.PL"], "primary_category": "cs.SE"}
{"title": "Text-Queried Target Sound Event Localization", "abstract": "Sound event localization and detection (SELD) aims to determine the\nappearance of sound classes, together with their Direction of Arrival (DOA).\nHowever, current SELD systems can only predict the activities of specific\nclasses, for example, 13 classes in DCASE challenges. In this paper, we propose\ntext-queried target sound event localization (SEL), a new paradigm that allows\nthe user to input the text to describe the sound event, and the SEL model can\npredict the location of the related sound event. The proposed task presents a\nmore user-friendly way for human-computer interaction. We provide a benchmark\nstudy for the proposed task and perform experiments on datasets created by\nsimulated room impulse response (RIR) and real RIR to validate the\neffectiveness of the proposed methods. We hope that our benchmark will inspire\nthe interest and additional research for text-queried sound source\nlocalization.", "published": "2024-06-23 09:39:06", "link": "http://arxiv.org/abs/2406.16058v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Exploiting Foundation Models and Speech Enhancement for Parkinson's\n  Disease Detection from Speech in Real-World Operative Conditions", "abstract": "This work is concerned with devising a robust Parkinson's (PD) disease\ndetector from speech in real-world operating conditions using (i) foundational\nmodels, and (ii) speech enhancement (SE) methods. To this end, we first\nfine-tune several foundational-based models on the standard PC-GITA (s-PC-GITA)\nclean data. Our results demonstrate superior performance to previously proposed\nmodels. Second, we assess the generalization capability of the PD models on the\nextended PC-GITA (e-PC-GITA) recordings, collected in real-world operative\nconditions, and observe a severe drop in performance moving from ideal to\nreal-world conditions. Third, we align training and testing conditions\napplaying off-the-shelf SE techniques on e-PC-GITA, and a significant boost in\nperformance is observed only for the foundational-based models. Finally,\ncombining the two best foundational-based models trained on s-PC-GITA, namely\nWavLM Base and Hubert Base, yielded top performance on the enhanced e-PC-GITA.", "published": "2024-06-23 14:56:16", "link": "http://arxiv.org/abs/2406.16128v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Speech Representation Analysis based on Inter- and Intra-Model\n  Similarities", "abstract": "Self-supervised models have revolutionized speech processing, achieving new\nlevels of performance in a wide variety of tasks with limited resources.\nHowever, the inner workings of these models are still opaque. In this paper, we\naim to analyze the encoded contextual representation of these foundation models\nbased on their inter- and intra-model similarity, independent of any external\nannotation and task-specific constraint. We examine different SSL models\nvarying their training paradigm -- Contrastive (Wav2Vec2.0) and Predictive\nmodels (HuBERT); and model sizes (base and large). We explore these models on\ndifferent levels of localization/distributivity of information including (i)\nindividual neurons; (ii) layer representation; (iii) attention weights and (iv)\ncompare the representations with their finetuned counterparts.Our results\nhighlight that these models converge to similar representation subspaces but\nnot to similar neuron-localized concepts\\footnote{A concept represents a\ncoherent fragment of knowledge, such as ``a class containing certain objects as\nelements, where the objects have certain properties. We made the code publicly\navailable for facilitating further research, we publicly released our code.", "published": "2024-06-23 13:00:03", "link": "http://arxiv.org/abs/2406.16099v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Predicting Individual Depression Symptoms from Acoustic Features During\n  Speech", "abstract": "Current automatic depression detection systems provide predictions directly\nwithout relying on the individual symptoms/items of depression as denoted in\nthe clinical depression rating scales. In contrast, clinicians assess each item\nin the depression rating scale in a clinical setting, thus implicitly providing\na more detailed rationale for a depression diagnosis. In this work, we make a\nfirst step towards using the acoustic features of speech to predict individual\nitems of the depression rating scale before obtaining the final depression\nprediction. For this, we use convolutional (CNN) and recurrent (long short-term\nmemory (LSTM)) neural networks. We consider different approaches to learning\nthe temporal context of speech. Further, we analyze two variants of voting\nschemes for individual item prediction and depression detection. We also\ninclude an animated visualization that shows an example of item prediction over\ntime as the speech progresses.", "published": "2024-06-23 03:26:47", "link": "http://arxiv.org/abs/2406.16000v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Open Respiratory Acoustic Foundation Models: Pretraining and\n  Benchmarking", "abstract": "Respiratory audio, such as coughing and breathing sounds, has predictive\npower for a wide range of healthcare applications, yet is currently\nunder-explored. The main problem for those applications arises from the\ndifficulty in collecting large labeled task-specific data for model\ndevelopment. Generalizable respiratory acoustic foundation models pretrained\nwith unlabeled data would offer appealing advantages and possibly unlock this\nimpasse. However, given the safety-critical nature of healthcare applications,\nit is pivotal to also ensure openness and replicability for any proposed\nfoundation model solution. To this end, we introduce OPERA, an OPEn Respiratory\nAcoustic foundation model pretraining and benchmarking system, as the first\napproach answering this need. We curate large-scale respiratory audio datasets\n(~136K samples, over 400 hours), pretrain three pioneering foundation models,\nand build a benchmark consisting of 19 downstream respiratory health tasks for\nevaluation. Our pretrained models demonstrate superior performance (against\nexisting acoustic models pretrained with general audio on 16 out of 19 tasks)\nand generalizability (to unseen datasets and new respiratory audio modalities).\nThis highlights the great promise of respiratory acoustic foundation models and\nencourages more studies using OPERA as an open resource to accelerate research\non respiratory audio for health. The system is accessible from\nhttps://github.com/evelyn0414/OPERA.", "published": "2024-06-23 16:04:26", "link": "http://arxiv.org/abs/2406.16148v3", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Listen and Move: Improving GANs Coherency in Agnostic Sound-to-Video\n  Generation", "abstract": "Deep generative models have demonstrated the ability to create realistic\naudiovisual content, sometimes driven by domains of different nature. However,\nsmooth temporal dynamics in video generation is a challenging problem. This\nwork focuses on generic sound-to-video generation and proposes three main\nfeatures to enhance both image quality and temporal coherency in generative\nadversarial models: a triple sound routing scheme, a multi-scale residual and\ndilated recurrent network for extended sound analysis, and a novel recurrent\nand directional convolutional layer for video prediction. Each of the proposed\nfeatures improves, in both quality and coherency, the baseline neural\narchitecture typically used in the SoTA, with the video prediction layer\nproviding an extra temporal refinement.", "published": "2024-06-23 16:30:11", "link": "http://arxiv.org/abs/2406.16155v1", "categories": ["cs.SD", "cs.GR", "eess.AS"], "primary_category": "cs.SD"}
