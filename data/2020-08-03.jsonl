{"title": "Deep Learning based Topic Analysis on Financial Emerging Event Tweets", "abstract": "Financial analyses of stock markets rely heavily on quantitative approaches\nin an attempt to predict subsequent or market movements based on historical\nprices and other measurable metrics. These quantitative analyses might have\nmissed out on un-quantifiable aspects like sentiment and speculation that also\nimpact the market. Analyzing vast amounts of qualitative text data to\nunderstand public opinion on social media platform is one approach to address\nthis gap. This work carried out topic analysis on 28264 financial tweets [1]\nvia clustering to discover emerging events in the stock market. Three main\ntopics were discovered to be discussed frequently within the period. First, the\nfinancial ratio EPS is a measure that has been discussed frequently by\ninvestors. Secondly, short selling of shares were discussed heavily, it was\noften mentioned together with Morgan Stanley. Thirdly, oil and energy sectors\nwere often discussed together with policy. These tweets were semantically\nclustered by a method consisting of word2vec algorithm to obtain word\nembeddings that map words to vectors. Semantic word clusters were then formed.\nEach tweet was then vectorized using the Term Frequency-Inverse Document\nFrequency (TF-IDF) values of the words it consisted of and based on which\nclusters its words were in. Tweet vectors were then converted to compressed\nrepresentations by training a deep-autoencoder. K-means clusters were then\nformed. This method reduces dimensionality and produces dense vectors, in\ncontrast to the usual Vector Space Model. Topic modelling with Latent Dirichlet\nAllocation (LDA) and top frequent words were used to analyze clusters and\nreveal emerging events.", "published": "2020-08-03 06:43:11", "link": "http://arxiv.org/abs/2008.00670v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LT@Helsinki at SemEval-2020 Task 12: Multilingual or language-specific\n  BERT?", "abstract": "This paper presents the different models submitted by the LT@Helsinki team\nfor the SemEval 2020 Shared Task 12. Our team participated in sub-tasks A and\nC; titled offensive language identification and offense target identification,\nrespectively. In both cases we used the so-called Bidirectional Encoder\nRepresentation from Transformer (BERT), a model pre-trained by Google and\nfine-tuned by us on the OLID and SOLID datasets. The results show that\noffensive tweet classification is one of several language-based tasks where\nBERT can achieve state-of-the-art results.", "published": "2020-08-03 12:03:17", "link": "http://arxiv.org/abs/2008.00805v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting the Humorousness of Tweets Using Gaussian Process Preference\n  Learning", "abstract": "Most humour processing systems to date make at best discrete, coarse-grained\ndistinctions between the comical and the conventional, yet such notions are\nbetter conceptualized as a broad spectrum. In this paper, we present a\nprobabilistic approach, a variant of Gaussian process preference learning\n(GPPL), that learns to rank and rate the humorousness of short texts by\nexploiting human preference judgments and automatically sourced linguistic\nannotations. We apply our system, which is similar to one that had previously\nshown good performance on English-language one-liners annotated with pairwise\nhumorousness annotations, to the Spanish-language data set of the\nHAHA@IberLEF2019 evaluation campaign. We report system performance for the\ncampaign's two subtasks, humour detection and funniness score prediction, and\ndiscuss some issues arising from the conversion between the numeric scores used\nin the HAHA@IberLEF2019 data and the pairwise judgment annotations required for\nour method.", "published": "2020-08-03 13:05:42", "link": "http://arxiv.org/abs/2008.00853v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "DeLighT: Deep and Light-weight Transformer", "abstract": "We introduce a deep and light-weight transformer, DeLighT, that delivers\nsimilar or better performance than standard transformer-based models with\nsignificantly fewer parameters. DeLighT more efficiently allocates parameters\nboth (1) within each Transformer block using the DeLighT transformation, a deep\nand light-weight transformation, and (2) across blocks using block-wise\nscaling, which allows for shallower and narrower DeLighT blocks near the input\nand wider and deeper DeLighT blocks near the output. Overall, DeLighT networks\nare 2.5 to 4 times deeper than standard transformer models and yet have fewer\nparameters and operations. Experiments on benchmark machine translation and\nlanguage modeling tasks show that DeLighT matches or improves the performance\nof baseline Transformers with 2 to 3 times fewer parameters on average. Our\nsource code is available at: \\url{https://github.com/sacmehta/delight}", "published": "2020-08-03 03:08:29", "link": "http://arxiv.org/abs/2008.00623v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multimodal Semi-supervised Learning Framework for Punctuation Prediction\n  in Conversational Speech", "abstract": "In this work, we explore a multimodal semi-supervised learning approach for\npunctuation prediction by learning representations from large amounts of\nunlabelled audio and text data. Conventional approaches in speech processing\ntypically use forced alignment to encoder per frame acoustic features to word\nlevel features and perform multimodal fusion of the resulting acoustic and\nlexical representations. As an alternative, we explore attention based\nmultimodal fusion and compare its performance with forced alignment based\nfusion. Experiments conducted on the Fisher corpus show that our proposed\napproach achieves ~6-9% and ~3-4% absolute improvement (F1 score) over the\nbaseline BLSTM model on reference transcripts and ASR outputs respectively. We\nfurther improve the model robustness to ASR errors by performing data\naugmentation with N-best lists which achieves up to an additional ~2-6%\nimprovement on ASR outputs. We also demonstrate the effectiveness of\nsemi-supervised learning approach by performing ablation study on various sizes\nof the corpus. When trained on 1 hour of speech and text data, the proposed\nmodel achieved ~9-18% absolute improvement over baseline model.", "published": "2020-08-03 08:13:09", "link": "http://arxiv.org/abs/2008.00702v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Unsupervised Discovery of Recurring Speech Patterns Using Probabilistic\n  Adaptive Metrics", "abstract": "Unsupervised spoken term discovery (UTD) aims at finding recurring segments\nof speech from a corpus of acoustic speech data. One potential approach to this\nproblem is to use dynamic time warping (DTW) to find well-aligning patterns\nfrom the speech data. However, automatic selection of initial candidate\nsegments for the DTW-alignment and detection of \"sufficiently good\" alignments\namong those require some type of pre-defined criteria, often operationalized as\nthreshold parameters for pair-wise distance metrics between signal\nrepresentations. In the existing UTD systems, the optimal hyperparameters may\ndiffer across datasets, limiting their applicability to new corpora and truly\nlow-resource scenarios. In this paper, we propose a novel probabilistic\napproach to DTW-based UTD named as PDTW. In PDTW, distributional\ncharacteristics of the processed corpus are utilized for adaptive evaluation of\nalignment quality, thereby enabling systematic discovery of pattern pairs that\nhave similarity what would be expected by coincidence. We test PDTW on Zero\nResource Speech Challenge 2017 datasets as a part of 2020 implementation of the\nchallenge. The results show that the system performs consistently on all five\ntested languages using fixed hyperparameters, clearly outperforming the earlier\nDTW-based system in terms of coverage of the detected patterns.", "published": "2020-08-03 09:09:12", "link": "http://arxiv.org/abs/2008.00731v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Elsevier OA CC-By Corpus", "abstract": "We introduce the Elsevier OA CC-BY corpus. This is the first open corpus of\nScientific Research papers which has a representative sample from across\nscientific disciplines. This corpus not only includes the full text of the\narticle, but also the metadata of the documents, along with the bibliographic\ninformation for each reference.", "published": "2020-08-03 10:51:10", "link": "http://arxiv.org/abs/2008.00774v3", "categories": ["cs.CL", "cs.DL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Characterizing COVID-19 Misinformation Communities Using a Novel Twitter\n  Dataset", "abstract": "From conspiracy theories to fake cures and fake treatments, COVID-19 has\nbecome a hot-bed for the spread of misinformation online. It is more important\nthan ever to identify methods to debunk and correct false information online.\nIn this paper, we present a methodology and analyses to characterize the two\ncompeting COVID-19 misinformation communities online: (i) misinformed users or\nusers who are actively posting misinformation, and (ii) informed users or users\nwho are actively spreading true information, or calling out misinformation. The\ngoals of this study are two-fold: (i) collecting a diverse set of annotated\nCOVID-19 Twitter dataset that can be used by the research community to conduct\nmeaningful analysis; and (ii) characterizing the two target communities in\nterms of their network structure, linguistic patterns, and their membership in\nother communities. Our analyses show that COVID-19 misinformed communities are\ndenser, and more organized than informed communities, with a possibility of a\nhigh volume of the misinformation being part of disinformation campaigns. Our\nanalyses also suggest that a large majority of misinformed users may be\nanti-vaxxers. Finally, our sociolinguistic analyses suggest that COVID-19\ninformed users tend to use more narratives than misinformed users.", "published": "2020-08-03 11:44:22", "link": "http://arxiv.org/abs/2008.00791v4", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "On The Plurality of Graphs", "abstract": "We conduct a series of experiments designed to empirically demonstrate the\neffects of varying the structural features of a multi-agent emergent\ncommunication game framework. Specifically, we model the interactions (edges)\nbetween individual agents (nodes)as the structure of a graph generated\naccording to a series of known random graph generating algorithms. Confirming\nthe hypothesis proposed in [10], we show that the two factors of variation\ninduced in this work, namely 1) the graph-generating process and 2) the\ncentrality measure according to which edges are sampled, in fact play a\nsignificant role in determining the dynamics of language emergence within the\npopulation at hand.", "published": "2020-08-03 14:54:09", "link": "http://arxiv.org/abs/2008.00920v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Lanfrica: A Participatory Approach to Documenting Machine Translation\n  Research on African Languages", "abstract": "Over the years, there have been campaigns to include the African languages in\nthe growing research on machine translation (MT) in particular, and natural\nlanguage processing (NLP) in general. Africa has the highest language\ndiversity, with 1500-2000 documented languages and many more undocumented or\nextinct languages(Lewis, 2009; Bendor-Samuel, 2017). This makes it hard to keep\ntrack of the MT research, models and dataset that have been developed for some\nof them. As the internet and social media make up the daily lives of more than\nhalf of the world(Lin, 2020), as well as over 40% of Africans(Campbell, 2019),\nonline platforms can be useful in creating accessibility to researches,\nbenchmarks and datasets in these African languages, thereby improving\nreproducibility and sharing of existing research and their results. In this\npaper, we introduce Lanfrica, a novel, on-going framework that employs a\nparticipatory approach to documenting researches, projects, benchmarks and\ndataset on African languages.", "published": "2020-08-03 18:14:04", "link": "http://arxiv.org/abs/2008.07302v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Audiovisual Speech Synthesis using Tacotron2", "abstract": "Audiovisual speech synthesis is the problem of synthesizing a talking face\nwhile maximizing the coherency of the acoustic and visual speech. In this\npaper, we propose and compare two audiovisual speech synthesis systems for 3D\nface models. The first system is the AVTacotron2, which is an end-to-end\ntext-to-audiovisual speech synthesizer based on the Tacotron2 architecture.\nAVTacotron2 converts a sequence of phonemes representing the sentence to\nsynthesize into a sequence of acoustic features and the corresponding\ncontrollers of a face model. The output acoustic features are used to condition\na WaveRNN to reconstruct the speech waveform, and the output facial controllers\nare used to generate the corresponding video of the talking face. The second\naudiovisual speech synthesis system is modular, where acoustic speech is\nsynthesized from text using the traditional Tacotron2. The reconstructed\nacoustic speech signal is then used to drive the facial controls of the face\nmodel using an independently trained audio-to-facial-animation neural network.\nWe further condition both the end-to-end and modular approaches on emotion\nembeddings that encode the required prosody to generate emotional audiovisual\nspeech. We analyze the performance of the two systems and compare them to the\nground truth videos using subjective evaluation tests. The end-to-end and\nmodular systems are able to synthesize close to human-like audiovisual speech\nwith mean opinion scores (MOS) of 4.1 and 3.9, respectively, compared to a MOS\nof 4.1 for the ground truth generated from professionally recorded videos.\nWhile the end-to-end system gives a better overall quality, the modular\napproach is more flexible and the quality of acoustic speech and visual speech\nsynthesis is almost independent of each other.", "published": "2020-08-03 02:45:06", "link": "http://arxiv.org/abs/2008.00620v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech", "abstract": "We introduce an approach to multilingual speech synthesis which uses the\nmeta-learning concept of contextual parameter generation and produces\nnatural-sounding multilingual speech using more languages and less training\ndata than previous approaches. Our model is based on Tacotron 2 with a fully\nconvolutional input text encoder whose weights are predicted by a separate\nparameter generator network. To boost voice cloning, the model uses an\nadversarial speaker classifier with a gradient reversal layer that removes\nspeaker-specific information from the encoder.\n  We arranged two experiments to compare our model with baselines using various\nlevels of cross-lingual parameter sharing, in order to evaluate: (1) stability\nand performance when training on low amounts of data, (2) pronunciation\naccuracy and voice quality of code-switching synthesis. For training, we used\nthe CSS10 dataset and our new small dataset based on Common Voice recordings in\nfive languages. Our model is shown to effectively share information across\nlanguages and according to a subjective evaluation test, it produces more\nnatural and accurate code-switching speech than the baselines.", "published": "2020-08-03 10:43:30", "link": "http://arxiv.org/abs/2008.00768v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "A Survey on the Evolution of Stream Processing Systems", "abstract": "Stream processing has been an active research field for more than 20 years,\nbut it is now witnessing its prime time due to recent successful efforts by the\nresearch community and numerous worldwide open-source communities. This survey\nprovides a comprehensive overview of fundamental aspects of stream processing\nsystems and their evolution in the functional areas of out-of-order data\nmanagement, state management, fault tolerance, high availability, load\nmanagement, elasticity, and reconfiguration. We review noteworthy past research\nfindings, outline the similarities and differences between early ('00-'10) and\nmodern ('11-'22) streaming systems, and discuss recent trends and open\nproblems.", "published": "2020-08-03 12:43:46", "link": "http://arxiv.org/abs/2008.00842v2", "categories": ["cs.DC", "cs.CL", "cs.DB", "cs.PF"], "primary_category": "cs.DC"}
{"title": "Learning Intonation Pattern Embeddings for Arabic Dialect Identification", "abstract": "This article presents a full end-to-end pipeline for Arabic Dialect\nIdentification (ADI) using intonation patterns and acoustic representations.\nRecent approaches to language and dialect identification use linguistic-aware\ndeep architectures that are able to capture phonetic differences amongst\nlanguages and dialects. Specifically, in ADI tasks, different combinations of\nlinguistic features and acoustic representations have been successful with deep\nlearning models. The approach presented in this article uses intonation\npatterns and hybrid residual and bidirectional LSTM networks to learn acoustic\nembeddings with no additional linguistic information. Results of the\nexperiments show that intonation patterns for Arabic dialects provide\nsufficient information to achieve state-of-the-art results on the VarDial 17\nADI dataset, outperforming single-feature systems. The pipeline presented is\nrobust to data sparsity, in contrast to other deep learning approaches that\nrequire large quantities of data. We conjecture on the importance of sufficient\ninformation as a criterion for optimality in a deep learning ADI task, and more\ngenerally, its application to acoustic modeling problems. Small intonation\npatterns, when sufficient in an information-theoretic sense, allow deep\nlearning architectures to learn more accurate speech representations.", "published": "2020-08-03 06:39:16", "link": "http://arxiv.org/abs/2008.00667v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "TutorNet: Towards Flexible Knowledge Distillation for End-to-End Speech\n  Recognition", "abstract": "In recent years, there has been a great deal of research in developing\nend-to-end speech recognition models, which enable simplifying the traditional\npipeline and achieving promising results. Despite their remarkable performance\nimprovements, end-to-end models typically require expensive computational cost\nto show successful performance. To reduce this computational burden, knowledge\ndistillation (KD), which is a popular model compression method, has been used\nto transfer knowledge from a deep and complex model (teacher) to a shallower\nand simpler model (student). Previous KD approaches have commonly designed the\narchitecture of the student model by reducing the width per layer or the number\nof layers of the teacher model. This structural reduction scheme might limit\nthe flexibility of model selection since the student model structure should be\nsimilar to that of the given teacher. To cope with this limitation, we propose\na new KD method for end-to-end speech recognition, namely TutorNet, that can\ntransfer knowledge across different types of neural networks at the hidden\nrepresentation-level as well as the output-level. For concrete realizations, we\nfirstly apply representation-level knowledge distillation (RKD) during the\ninitialization step, and then apply the softmax-level knowledge distillation\n(SKD) combined with the original task learning. When the student is trained\nwith RKD, we make use of frame weighting that points out the frames to which\nthe teacher model pays more attention. Through a number of experiments on\nLibriSpeech dataset, it is verified that the proposed method not only distills\nthe knowledge between networks with different topologies but also significantly\ncontributes to improving the word error rate (WER) performance of the distilled\nstudent. Interestingly, TutorNet allows the student model to surpass its\nteacher's performance in some particular cases.", "published": "2020-08-03 06:44:18", "link": "http://arxiv.org/abs/2008.00671v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Exploiting Deep Sentential Context for Expressive End-to-End Speech\n  Synthesis", "abstract": "Attention-based seq2seq text-to-speech systems, especially those use\nself-attention networks (SAN), have achieved state-of-art performance. But an\nexpressive corpus with rich prosody is still challenging to model as 1)\nprosodic aspects, which span across different sentential granularities and\nmainly determine acoustic expressiveness, are difficult to quantize and label\nand 2) the current seq2seq framework extracts prosodic information solely from\na text encoder, which is easily collapsed to an averaged expression for\nexpressive contents. In this paper, we propose a context extractor, which is\nbuilt upon SAN-based text encoder, to sufficiently exploit the sentential\ncontext over an expressive corpus for seq2seq-based TTS. Our context extractor\nfirst collects prosodic-related sentential context information from different\nSAN layers and then aggregates them to learn a comprehensive sentence\nrepresentation to enhance the expressiveness of the final generated speech.\nSpecifically, we investigate two methods of context aggregation: 1) direct\naggregation which directly concatenates the outputs of different SAN layers,\nand 2) weighted aggregation which uses multi-head attention to automatically\nlearn contributions for different SAN layers. Experiments on two expressive\ncorpora show that our approach can produce more natural speech with much richer\nprosodic variations, and weighted aggregation is more superior in modeling\nexpressivity.", "published": "2020-08-03 02:22:32", "link": "http://arxiv.org/abs/2008.00613v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multitask learning for instrument activation aware music source\n  separation", "abstract": "Music source separation is a core task in music information retrieval which\nhas seen a dramatic improvement in the past years. Nevertheless, most of the\nexisting systems focus exclusively on the problem of source separation itself\nand ignore the utilization of other~---possibly related---~MIR tasks which\ncould lead to additional quality gains. In this work, we propose a novel\nmultitask structure to investigate using instrument activation information to\nimprove source separation performance. Furthermore, we investigate our system\non six independent instruments, a more realistic scenario than the three\ninstruments included in the widely-used MUSDB dataset, by leveraging a\ncombination of the MedleyDB and Mixing Secrets datasets. The results show that\nour proposed multitask model outperforms the baseline Open-Unmix model on the\nmixture of Mixing Secrets and MedleyDB dataset while maintaining comparable\nperformance on the MUSDB dataset.", "published": "2020-08-03 02:35:00", "link": "http://arxiv.org/abs/2008.00616v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "MusiCoder: A Universal Music-Acoustic Encoder Based on Transformers", "abstract": "Music annotation has always been one of the critical topics in the field of\nMusic Information Retrieval (MIR). Traditional models use supervised learning\nfor music annotation tasks. However, as supervised machine learning approaches\nincrease in complexity, the increasing need for more annotated training data\ncan often not be matched with available data. In this paper, a new\nself-supervised music acoustic representation learning approach named MusiCoder\nis proposed. Inspired by the success of BERT, MusiCoder builds upon the\narchitecture of self-attention bidirectional transformers. Two pre-training\nobjectives, including Contiguous Frames Masking (CFM) and Contiguous Channels\nMasking (CCM), are designed to adapt BERT-like masked reconstruction\npre-training to continuous acoustic frame domain. The performance of MusiCoder\nis evaluated in two downstream music annotation tasks. The results show that\nMusiCoder outperforms the state-of-the-art models in both music genre\nclassification and auto-tagging tasks. The effectiveness of MusiCoder indicates\na great potential of a new self-supervised learning approach to understand\nmusic: first apply masked reconstruction tasks to pre-train a transformer-based\nmodel with massive unlabeled music acoustic data, and then finetune the model\non specific downstream tasks with labeled data.", "published": "2020-08-03 11:15:28", "link": "http://arxiv.org/abs/2008.00781v2", "categories": ["eess.AS", "cs.MM"], "primary_category": "eess.AS"}
{"title": "Structure and Automatic Segmentation of Dhrupad Vocal Bandish Audio", "abstract": "A Dhrupad vocal concert comprises a composition section that is interspersed\nwith improvised episodes of increased rhythmic activity involving the\ninteraction between the vocals and the percussion. Tracking the changing\nrhythmic density, in relation to the underlying metric tempo of the piece, thus\nfacilitates the detection and labeling of the improvised sections in the\nconcert structure. This work concerns the automatic detection of the musically\nrelevant rhythmic densities as they change in time across the bandish\n(composition) performance. An annotated dataset of Dhrupad bandish concert\nsections is presented. We investigate a CNN-based system, trained to detect\nlocal tempo relationships, and follow it with temporal smoothing. We also\nemploy audio source separation as a pre-processing step to the detection of the\nindividual surface densities of the vocals and the percussion. This helps us\nobtain the complete musical description of the concert sections in terms of\ncapturing the changing rhythmic interaction of the two performers.", "published": "2020-08-03 10:16:42", "link": "http://arxiv.org/abs/2008.00756v1", "categories": ["eess.AS", "cs.IR", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Evolving Multi-Resolution Pooling CNN for Monaural Singing Voice\n  Separation", "abstract": "Monaural Singing Voice Separation (MSVS) is a challenging task and has been\nstudied for decades. Deep neural networks (DNNs) are the current\nstate-of-the-art methods for MSVS. However, the existing DNNs are often\ndesigned manually, which is time-consuming and error-prone. In addition, the\nnetwork architectures are usually pre-defined, and not adapted to the training\ndata. To address these issues, we introduce a Neural Architecture Search (NAS)\nmethod to the structure design of DNNs for MSVS. Specifically, we propose a new\nmulti-resolution Convolutional Neural Network (CNN) framework for MSVS namely\nMulti-Resolution Pooling CNN (MRP-CNN), which uses various-size pooling\noperators to extract multi-resolution features. Based on the NAS, we then\ndevelop an evolving framework namely Evolving MRP-CNN (E-MRP-CNN), by\nautomatically searching the effective MRP-CNN structures using genetic\nalgorithms, optimized in terms of a single-objective considering only\nseparation performance, or multi-objective considering both the separation\nperformance and the model complexity. The multi-objective E-MRP-CNN gives a set\nof Pareto-optimal solutions, each providing a trade-off between separation\nperformance and model complexity. Quantitative and qualitative evaluations on\nthe MIR-1K and DSD100 datasets are used to demonstrate the advantages of the\nproposed framework over several recent baselines.", "published": "2020-08-03 12:09:42", "link": "http://arxiv.org/abs/2008.00816v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker dependent articulatory-to-acoustic mapping using real-time MRI\n  of the vocal tract", "abstract": "Articulatory-to-acoustic (forward) mapping is a technique to predict speech\nusing various articulatory acquisition techniques (e.g. ultrasound tongue\nimaging, lip video). Real-time MRI (rtMRI) of the vocal tract has not been used\nbefore for this purpose. The advantage of MRI is that it has a high `relative'\nspatial resolution: it can capture not only lingual, labial and jaw motion, but\nalso the velum and the pharyngeal region, which is typically not possible with\nother techniques. In the current paper, we train various DNNs (fully connected,\nconvolutional and recurrent neural networks) for articulatory-to-speech\nconversion, using rtMRI as input, in a speaker-specific way. We use two male\nand two female speakers of the USC-TIMIT articulatory database, each of them\nuttering 460 sentences. We evaluate the results with objective (Normalized MSE\nand MCD) and subjective measures (perceptual test) and show that CNN-LSTM\nnetworks are preferred which take multiple images as input, and achieve MCD\nscores between 2.8-4.5 dB. In the experiments, we find that the predictions of\nspeaker `m1' are significantly weaker than other speakers. We show that this is\ncaused by the fact that 74% of the recordings of speaker `m1' are out of sync.", "published": "2020-08-03 14:09:28", "link": "http://arxiv.org/abs/2008.00889v1", "categories": ["eess.AS", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Self-attention encoding and pooling for speaker recognition", "abstract": "The computing power of mobile devices limits the end-user applications in\nterms of storage size, processing, memory and energy consumption. These\nlimitations motivate researchers for the design of more efficient deep models.\nOn the other hand, self-attention networks based on Transformer architecture\nhave attracted remarkable interests due to their high parallelization\ncapabilities and strong performance on a variety of Natural Language Processing\n(NLP) applications. Inspired by the Transformer, we propose a tandem\nSelf-Attention Encoding and Pooling (SAEP) mechanism to obtain a discriminative\nspeaker embedding given non-fixed length speech utterances. SAEP is a stack of\nidentical blocks solely relied on self-attention and position-wise feed-forward\nnetworks to create vector representation of speakers. This approach encodes\nshort-term speaker spectral features into speaker embeddings to be used in\ntext-independent speaker verification. We have evaluated this approach on both\nVoxCeleb1 & 2 datasets. The proposed architecture is able to outperform the\nbaseline x-vector, and shows competitive performance to some other benchmarks\nbased on convolutions, with a significant reduction in model size. It employs\n94%, 95%, and 73% less parameters compared to ResNet-34, ResNet-50, and\nx-vector, respectively. This indicates that the proposed fully attention based\narchitecture is more efficient in extracting time-invariant features from\nspeaker utterances.", "published": "2020-08-03 09:31:27", "link": "http://arxiv.org/abs/2008.01077v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Spectral Energy Distance for Parallel Speech Synthesis", "abstract": "Speech synthesis is an important practical generative modeling problem that\nhas seen great progress over the last few years, with likelihood-based\nautoregressive neural models now outperforming traditional concatenative\nsystems. A downside of such autoregressive models is that they require\nexecuting tens of thousands of sequential operations per second of generated\naudio, making them ill-suited for deployment on specialized deep learning\nhardware. Here, we propose a new learning method that allows us to train highly\nparallel models of speech, without requiring access to an analytical likelihood\nfunction. Our approach is based on a generalized energy distance between the\ndistributions of the generated and real audio. This spectral energy distance is\na proper scoring rule with respect to the distribution over\nmagnitude-spectrograms of the generated waveform audio and offers statistical\nconsistency guarantees. The distance can be calculated from minibatches without\nbias, and does not involve adversarial learning, yielding a stable and\nconsistent method for training implicit generative models. Empirically, we\nachieve state-of-the-art generation quality among implicit generative models,\nas judged by the recently-proposed cFDSD metric. When combining our method with\nadversarial techniques, we also improve upon the recently-proposed GAN-TTS\nmodel in terms of Mean Opinion Score as judged by trained human evaluators.", "published": "2020-08-03 19:56:04", "link": "http://arxiv.org/abs/2008.01160v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
