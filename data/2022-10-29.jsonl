{"title": "Differentiable Data Augmentation for Contrastive Sentence Representation\n  Learning", "abstract": "Fine-tuning a pre-trained language model via the contrastive learning\nframework with a large amount of unlabeled sentences or labeled sentence pairs\nis a common way to obtain high-quality sentence representations. Although the\ncontrastive learning framework has shown its superiority on sentence\nrepresentation learning over previous methods, the potential of such a\nframework is under-explored so far due to the simple method it used to\nconstruct positive pairs. Motivated by this, we propose a method that makes\nhard positives from the original training examples. A pivotal ingredient of our\napproach is the use of prefix that is attached to a pre-trained language model,\nwhich allows for differentiable data augmentation during contrastive learning.\nOur method can be summarized in two steps: supervised prefix-tuning followed by\njoint contrastive fine-tuning with unlabeled or labeled examples. Our\nexperiments confirm the effectiveness of our data augmentation approach. The\nproposed method yields significant improvements over existing methods under\nboth semi-supervised and supervised settings. Our experiments under a low\nlabeled data setting also show that our method is more label-efficient than the\nstate-of-the-art contrastive learning methods.", "published": "2022-10-29 08:57:45", "link": "http://arxiv.org/abs/2210.16536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity-centered Cross-document Relation Extraction", "abstract": "Relation Extraction (RE) is a fundamental task of information extraction,\nwhich has attracted a large amount of research attention. Previous studies\nfocus on extracting the relations within a sentence or document, while\ncurrently researchers begin to explore cross-document RE. However, current\ncross-document RE methods directly utilize text snippets surrounding target\nentities in multiple given documents, which brings considerable noisy and\nnon-relevant sentences. Moreover, they utilize all the text paths in a document\nbag in a coarse-grained way, without considering the connections between these\ntext paths.In this paper, we aim to address both of these shortages and push\nthe state-of-the-art for cross-document RE. First, we focus on input\nconstruction for our RE model and propose an entity-based document-context\nfilter to retain useful information in the given documents by using the bridge\nentities in the text paths. Second, we propose a cross-document RE model based\non cross-path entity relation attention, which allow the entity relations\nacross text paths to interact with each other. We compare our cross-document RE\nmethod with the state-of-the-art methods in the dataset CodRED. Our method\noutperforms them by at least 10% in F1, thus demonstrating its effectiveness.", "published": "2022-10-29 09:27:15", "link": "http://arxiv.org/abs/2210.16541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Attribute-Entangled Controllable Text Generation: A Pilot Study\n  of Blessing Generation", "abstract": "Controllable Text Generation (CTG) has obtained great success due to its\nfine-grained generation ability obtained by focusing on multiple attributes.\nHowever, most existing CTG researches overlook how to utilize the attribute\nentanglement to enhance the diversity of the controlled generated texts. Facing\nthis dilemma, we focus on a novel CTG scenario, i.e., blessing generation which\nis challenging because high-quality blessing texts require CTG models to\ncomprehensively consider the entanglement between multiple attributes (e.g.,\nobjects and occasions). To promote the research on blessing generation, we\npresent EBleT, a large-scale Entangled Blessing Text dataset containing 293K\nEnglish sentences annotated with multiple attributes. Furthermore, we propose\nnovel evaluation metrics to measure the quality of the blessing texts generated\nby the baseline models we designed. Our study opens a new research direction\nfor controllable text generation and enables the development of\nattribute-entangled CTG models. Our dataset and source codes are available at\n\\url{https://github.com/huangshulin123/Blessing-Generation}.", "published": "2022-10-29 10:19:48", "link": "http://arxiv.org/abs/2210.16557v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Critical Reflection and Forward Perspective on Empathy and Natural\n  Language Processing", "abstract": "We review the state of research on empathy in natural language processing and\nidentify the following issues: (1) empathy definitions are absent or abstract,\nwhich (2) leads to low construct validity and reproducibility. Moreover, (3)\nemotional empathy is overemphasized, skewing our focus to a narrow subset of\nsimplified tasks. We believe these issues hinder research progress and argue\nthat current directions will benefit from a clear conceptualization that\nincludes operationalizing cognitive empathy components. Our main objectives are\nto provide insight and guidance on empathy conceptualization for NLP research\nobjectives and to encourage researchers to pursue the overlooked opportunities\nin this area, highly relevant, e.g., for clinical and educational sectors.", "published": "2022-10-29 13:57:02", "link": "http://arxiv.org/abs/2210.16604v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empirical Evaluation of Post-Training Quantization Methods for Language\n  Tasks", "abstract": "Transformer-based architectures like BERT have achieved great success in a\nwide range of Natural Language tasks. Despite their decent performance, the\nmodels still have numerous parameters and high computational complexity,\nimpeding their deployment in resource-constrained environments. Post-Training\nQuantization (PTQ), which enables low-bit computations without extra training,\ncould be a promising tool. In this work, we conduct an empirical evaluation of\nthree PTQ methods on BERT-Base and BERT-Large: Linear Quantization (LQ),\nAnalytical Clipping for Integer Quantization (ACIQ), and Outlier Channel\nSplitting (OCS). OCS theoretically surpasses the others in minimizing the Mean\nSquare quantization Error and avoiding distorting the weights' outliers. That\nis consistent with the evaluation results of most language tasks of GLUE\nbenchmark and a reading comprehension task, SQuAD. Moreover, low-bit quantized\nBERT models could outperform the corresponding 32-bit baselines on several\nsmall language tasks, which we attribute to the alleviation of\nover-parameterization. We further explore the limit of quantization bit and\nshow that OCS could quantize BERT-Base and BERT-Large to 3-bits and retain 98%\nand 96% of the performance on the GLUE benchmark accordingly. Moreover, we\nconduct quantization on the whole BERT family, i.e., BERT models in different\nconfigurations, and comprehensively evaluate their performance on the GLUE\nbenchmark and SQuAD, hoping to provide valuable guidelines for their deployment\nin various computation environments.", "published": "2022-10-29 14:51:41", "link": "http://arxiv.org/abs/2210.16621v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Dependencies of Discrete Speech Representations with Neural\n  Hidden Markov Models", "abstract": "While discrete latent variable models have had great success in\nself-supervised learning, most models assume that frames are independent. Due\nto the segmental nature of phonemes in speech perception, modeling dependencies\namong latent variables at the frame level can potentially improve the learned\nrepresentations on phonetic-related tasks. In this work, we assume Markovian\ndependencies among latent variables, and propose to learn speech\nrepresentations with neural hidden Markov models. Our general framework allows\nus to compare to self-supervised models that assume independence, while keeping\nthe number of parameters fixed. The added dependencies improve the\naccessibility of phonetic information, phonetic segmentation, and the cluster\npurity of phones, showcasing the benefit of the assumed dependencies.", "published": "2022-10-29 17:46:58", "link": "http://arxiv.org/abs/2210.16659v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Classification of Code-Switched Text using Pre-trained\n  Multilingual Embeddings and Segmentation", "abstract": "With increasing globalization and immigration, various studies have estimated\nthat about half of the world population is bilingual. Consequently, individuals\nconcurrently use two or more languages or dialects in casual conversational\nsettings. However, most research is natural language processing is focused on\nmonolingual text. To further the work in code-switched sentiment analysis, we\npropose a multi-step natural language processing algorithm utilizing points of\ncode-switching in mixed text and conduct sentiment analysis around those\nidentified points. The proposed sentiment analysis algorithm uses semantic\nsimilarity derived from large pre-trained multilingual models with a\nhandcrafted set of positive and negative words to determine the polarity of\ncode-switched text. The proposed approach outperforms a comparable baseline\nmodel by 11.2% for accuracy and 11.64% for F1-score on a Spanish-English\ndataset. Theoretically, the proposed algorithm can be expanded for sentiment\nanalysis of multiple languages with limited human expertise.", "published": "2022-10-29 01:52:25", "link": "http://arxiv.org/abs/2210.16461v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "STPrompt: Semantic-guided and Task-driven prompts for Effective Few-shot\n  Classification", "abstract": "The effectiveness of prompt learning has been demonstrated in different\npre-trained language models. By formulating suitable template and choosing\nrepresentative label mapping, prompt learning can be used as an efficient\nknowledge probe. However, finding suitable prompt in existing methods requires\nmultiple experimental attempts or appropriate vector initialization on\nformulating suitable template and choosing representative label mapping, which\nit is more common in few-shot learning tasks. Motivating by PLM working\nprocess, we try to construct the prompt from task semantic perspective and thus\npropose the STPrompt -Semantic-guided and Task-driven Prompt model.\nSpecifically, two novel prompts generated from the semantic dependency tree\n(Dep-prompt) and task-specific metadata description (Meta-prompt), are firstly\nconstructed in a prompt augmented pool, and the proposed model would\nautomatically select a suitable semantic prompt to motivating the prompt\nlearning process. Our results show that the proposed model achieves the\nstate-of-the-art performance in five different datasets of few-shot text\nclassification tasks, which prove that more semantic and significant prompts\ncould assume as a better knowledge proving tool.", "published": "2022-10-29 04:42:30", "link": "http://arxiv.org/abs/2210.16489v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Prompting: Making Pre-trained Language Models Better Zero-shot\n  Learners by Clustering Representations", "abstract": "Recent work has demonstrated that pre-trained language models (PLMs) are\nzero-shot learners. However, most existing zero-shot methods involve heavy\nhuman engineering or complicated self-training pipelines, hindering their\napplication to new situations. In this work, we show that zero-shot text\nclassification can be improved simply by clustering texts in the embedding\nspaces of PLMs. Specifically, we fit the unlabeled texts with a Bayesian\nGaussian Mixture Model after initializing cluster positions and shapes using\nclass names. Despite its simplicity, this approach achieves superior or\ncomparable performance on both topic and sentiment classification datasets and\noutperforms prior works significantly on unbalanced datasets. We further\nexplore the applicability of our clustering approach by evaluating it on 14\ndatasets with more diverse topics, text lengths, and numbers of classes. Our\napproach achieves an average of 20% absolute improvement over prompt-based\nzero-shot learning. Finally, we compare different PLM embedding spaces and find\nthat texts are well-clustered by topics even if the PLM is not explicitly\npre-trained to generate meaningful sentence embeddings. This work indicates\nthat PLM embeddings can categorize texts without task-specific fine-tuning,\nthus providing a new way to analyze and utilize their knowledge and zero-shot\nlearning ability.", "published": "2022-10-29 16:01:51", "link": "http://arxiv.org/abs/2210.16637v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BERT Meets CTC: New Formulation of End-to-End Speech Recognition with\n  Pre-trained Masked Language Model", "abstract": "This paper presents BERT-CTC, a novel formulation of end-to-end speech\nrecognition that adapts BERT for connectionist temporal classification (CTC).\nOur formulation relaxes the conditional independence assumptions used in\nconventional CTC and incorporates linguistic knowledge through the explicit\noutput dependency obtained by BERT contextual embedding. BERT-CTC attends to\nthe full contexts of the input and hypothesized output sequences via the\nself-attention mechanism. This mechanism encourages a model to learn\ninner/inter-dependencies between the audio and token representations while\nmaintaining CTC's training efficiency. During inference, BERT-CTC combines a\nmask-predict algorithm with CTC decoding, which iteratively refines an output\nsequence. The experimental results reveal that BERT-CTC improves over\nconventional approaches across variations in speaking styles and languages.\nFinally, we show that the semantic representations in BERT-CTC are beneficial\ntowards downstream spoken language understanding tasks.", "published": "2022-10-29 18:19:44", "link": "http://arxiv.org/abs/2210.16663v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Accelerating RNN-T Training and Inference Using CTC guidance", "abstract": "We propose a novel method to accelerate training and inference process of\nrecurrent neural network transducer (RNN-T) based on the guidance from a\nco-trained connectionist temporal classification (CTC) model. We made a key\nassumption that if an encoder embedding frame is classified as a blank frame by\nthe CTC model, it is likely that this frame will be aligned to blank for all\nthe partial alignments or hypotheses in RNN-T and it can be discarded from the\ndecoder input. We also show that this frame reduction operation can be applied\nin the middle of the encoder, which result in significant speed up for the\ntraining and inference in RNN-T. We further show that the CTC alignment, a\nby-product of the CTC decoder, can also be used to perform lattice reduction\nfor RNN-T during training. Our method is evaluated on the Librispeech and\nSpeechStew tasks. We demonstrate that the proposed method is able to accelerate\nthe RNN-T inference by 2.2 times with similar or slightly better word error\nrates (WER).", "published": "2022-10-29 03:39:18", "link": "http://arxiv.org/abs/2210.16481v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Two is Better than Many? Binary Classification as an Effective Approach\n  to Multi-Choice Question Answering", "abstract": "We propose a simple refactoring of multi-choice question answering (MCQA)\ntasks as a series of binary classifications. The MCQA task is generally\nperformed by scoring each (question, answer) pair normalized over all the\npairs, and then selecting the answer from the pair that yield the highest\nscore. For n answer choices, this is equivalent to an n-class classification\nsetup where only one class (true answer) is correct. We instead show that\nclassifying (question, true answer) as positive instances and (question, false\nanswer) as negative instances is significantly more effective across various\nmodels and datasets. We show the efficacy of our proposed approach in different\ntasks -- abductive reasoning, commonsense question answering, science question\nanswering, and sentence completion. Our DeBERTa binary classification model\nreaches the top or close to the top performance on public leaderboards for\nthese tasks. The source code of the proposed approach is available at\nhttps://github.com/declare-lab/TEAM.", "published": "2022-10-29 05:11:45", "link": "http://arxiv.org/abs/2210.16495v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Phonemic Representation and Transcription for Speech to Text\n  Applications for Under-resourced Indigenous African Languages: The Case of\n  Kiswahili", "abstract": "Building automatic speech recognition (ASR) systems is a challenging task,\nespecially for under-resourced languages that need to construct corpora nearly\nfrom scratch and lack sufficient training data. It has emerged that several\nAfrican indigenous languages, including Kiswahili, are technologically\nunder-resourced. ASR systems are crucial, particularly for the hearing-impaired\npersons who can benefit from having transcripts in their native languages.\nHowever, the absence of transcribed speech datasets has complicated efforts to\ndevelop ASR models for these indigenous languages. This paper explores the\ntranscription process and the development of a Kiswahili speech corpus, which\nincludes both read-out texts and spontaneous speech data from native Kiswahili\nspeakers. The study also discusses the vowels and consonants in Kiswahili and\nprovides an updated Kiswahili phoneme dictionary for the ASR model that was\ncreated using the CMU Sphinx speech recognition toolbox, an open-source speech\nrecognition toolkit. The ASR model was trained using an extended phonetic set\nthat yielded a WER and SER of 18.87% and 49.5%, respectively, an improved\nperformance than previous similar research for under-resourced languages.", "published": "2022-10-29 09:04:09", "link": "http://arxiv.org/abs/2210.16537v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Exploiting prompt learning with pre-trained language models for\n  Alzheimer's Disease detection", "abstract": "Early diagnosis of Alzheimer's disease (AD) is crucial in facilitating\npreventive care and to delay further progression. Speech based automatic AD\nscreening systems provide a non-intrusive and more scalable alternative to\nother clinical screening techniques. Textual embedding features produced by\npre-trained language models (PLMs) such as BERT are widely used in such\nsystems. However, PLM domain fine-tuning is commonly based on the masked word\nor sentence prediction costs that are inconsistent with the back-end AD\ndetection task. To this end, this paper investigates the use of prompt-based\nfine-tuning of PLMs that consistently uses AD classification errors as the\ntraining objective function. Disfluency features based on hesitation or pause\nfiller token frequencies are further incorporated into prompt phrases during\nPLM fine-tuning. The decision voting based combination among systems using\ndifferent PLMs (BERT and RoBERTa) or systems with different fine-tuning\nparadigms (conventional masked-language modelling fine-tuning and prompt-based\nfine-tuning) is further applied. Mean, standard deviation and the maximum among\naccuracy scores over 15 experiment runs are adopted as performance measurements\nfor the AD detection system. Mean detection accuracy of 84.20% (with std 2.09%,\nbest 87.5%) and 82.64% (with std 4.0%, best 89.58%) were obtained using manual\nand ASR speech transcripts respectively on the ADReSS20 test set consisting of\n48 elderly speakers.", "published": "2022-10-29 09:18:41", "link": "http://arxiv.org/abs/2210.16539v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "End-to-end Spoken Language Understanding with Tree-constrained Pointer\n  Generator", "abstract": "End-to-end spoken language understanding (SLU) suffers from the long-tail\nword problem. This paper exploits contextual biasing, a technique to improve\nthe speech recognition of rare words, in end-to-end SLU systems. Specifically,\na tree-constrained pointer generator (TCPGen), a powerful and efficient biasing\nmodel component, is studied, which leverages a slot shortlist with\ncorresponding entities to extract biasing lists. Meanwhile, to bias the SLU\nmodel output slot distribution, a slot probability biasing (SPB) mechanism is\nproposed to calculate a slot distribution from TCPGen. Experiments on the SLURP\ndataset showed consistent SLU-F1 improvements using TCPGen and SPB, especially\non unseen entities. On a new split by holding out 5 slot types for the test,\nTCPGen with SPB achieved zero-shot learning with an SLU-F1 score over 50%\ncompared to baselines which can not deal with it. In addition to slot filling,\nthe intent classification accuracy was also improved.", "published": "2022-10-29 10:03:56", "link": "http://arxiv.org/abs/2210.16554v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "NTULM: Enriching Social Media Text Representations with Non-Textual\n  Units", "abstract": "On social media, additional context is often present in the form of\nannotations and meta-data such as the post's author, mentions, Hashtags, and\nhyperlinks. We refer to these annotations as Non-Textual Units (NTUs). We posit\nthat NTUs provide social context beyond their textual semantics and leveraging\nthese units can enrich social media text representations. In this work we\nconstruct an NTU-centric social heterogeneous network to co-embed NTUs. We then\nprincipally integrate these NTU embeddings into a large pretrained language\nmodel by fine-tuning with these additional units. This adds context to noisy\nshort-text social media. Experiments show that utilizing NTU-augmented text\nrepresentations significantly outperforms existing text-only baselines by 2-5\\%\nrelative points on many downstream tasks highlighting the importance of context\nto social media NLP. We also highlight that including NTU context into the\ninitial layers of language model alongside text is better than using it after\nthe text embedding is generated. Our work leads to the generation of holistic\ngeneral purpose social media content embedding.", "published": "2022-10-29 12:18:04", "link": "http://arxiv.org/abs/2210.16586v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI", "68T50, 68T07", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Application of Knowledge Distillation to Multi-task Speech\n  Representation Learning", "abstract": "Model architectures such as wav2vec 2.0 and HuBERT have been proposed to\nlearn speech representations from audio waveforms in a self-supervised manner.\nWhen they are combined with downstream tasks such as keyword spotting and\nspeaker verification, they provide state-of-the-art performance. However, these\nmodels use a large number of parameters, the smallest version of which has 95\nmillion parameters. This constitutes a challenge for edge AI device\ndeployments. In this paper, we investigate the application of knowledge\ndistillation to speech representation learning (SRL) models followed by joint\nfine-tuning with multiple downstream voice-activated tasks. In our experiments\non two such tasks, our approach results in nearly 75% reduction in model size\nwhile suffering only 0.1% accuracy and 0.9% equal error rate degradation\ncompared to the full-size model. In addition, we show that fine-tuning the SRL\nmodels results in a significant performance boost compared to using frozen SRL\nmodels.", "published": "2022-10-29 14:22:43", "link": "http://arxiv.org/abs/2210.16611v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Diverse Parallel Data Synthesis for Cross-Database Adaptation of\n  Text-to-SQL Parsers", "abstract": "Text-to-SQL parsers typically struggle with databases unseen during the train\ntime. Adapting parsers to new databases is a challenging problem due to the\nlack of natural language queries in the new schemas. We present ReFill, a\nframework for synthesizing high-quality and textually diverse parallel datasets\nfor adapting a Text-to-SQL parser to a target schema. ReFill learns to\nretrieve-and-edit text queries from the existing schemas and transfers them to\nthe target schema. We show that retrieving diverse existing text, masking their\nschema-specific tokens, and refilling with tokens relevant to the target\nschema, leads to significantly more diverse text queries than achievable by\nstandard SQL-to-Text generation methods. Through experiments spanning multiple\ndatabases, we demonstrate that fine-tuning parsers on datasets synthesized\nusing ReFill consistently outperforms the prior data-augmentation methods.", "published": "2022-10-29 14:30:53", "link": "http://arxiv.org/abs/2210.16613v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unifying the Discrete and Continuous Emotion labels for Speech Emotion\n  Recognition", "abstract": "Traditionally, in paralinguistic analysis for emotion detection from speech,\nemotions have been identified with discrete or dimensional (continuous-valued)\nlabels. Accordingly, models that have been proposed for emotion detection use\none or the other of these label types. However, psychologists like Russell and\nPlutchik have proposed theories and models that unite these views, maintaining\nthat these representations have shared and complementary information. This\npaper is an attempt to validate these viewpoints computationally. To this end,\nwe propose a model to jointly predict continuous and discrete emotional\nattributes and show how the relationship between these can be utilized to\nimprove the robustness and performance of emotion recognition tasks. Our\napproach comprises multi-task and hierarchical multi-task learning frameworks\nthat jointly model the relationships between continuous-valued and discrete\nemotion labels. Experimental results on two widely used datasets (IEMOCAP and\nMSPPodcast) for speech-based emotion recognition show that our model results in\nstatistically significant improvements in performance over strong baselines\nwith non-unified approaches. We also demonstrate that using one type of label\n(discrete or continuous-valued) for training improves recognition performance\nin tasks that use the other type of label. Experimental results and reasoning\nfor this approach (called the mismatched training approach) are also presented.", "published": "2022-10-29 16:12:31", "link": "http://arxiv.org/abs/2210.16642v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "XNOR-FORMER: Learning Accurate Approximations in Long Speech\n  Transformers", "abstract": "Transformers are among the state of the art for many tasks in speech, vision,\nand natural language processing, among others. Self-attentions, which are\ncrucial contributors to this performance have quadratic computational\ncomplexity, which makes training on longer input sequences challenging. Prior\nwork has produced state-of-the-art transformer variants with linear attention,\nhowever, current models sacrifice performance to achieve efficient\nimplementations. In this work, we develop a novel linear transformer by\nexamining the properties of the key-query product within self-attentions. Our\nmodel outperforms state of the art approaches on speech recognition and speech\nsummarization, resulting in 1 % absolute WER improvement on the Librispeech-100\nspeech recognition benchmark and a new INTERVIEW speech recognition benchmark,\nand 5 points on ROUGE for summarization with How2.", "published": "2022-10-29 16:21:30", "link": "http://arxiv.org/abs/2210.16643v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CascadeXML: Rethinking Transformers for End-to-end Multi-resolution\n  Training in Extreme Multi-label Classification", "abstract": "Extreme Multi-label Text Classification (XMC) involves learning a classifier\nthat can assign an input with a subset of most relevant labels from millions of\nlabel choices. Recent approaches, such as XR-Transformer and LightXML, leverage\na transformer instance to achieve state-of-the-art performance. However, in\nthis process, these approaches need to make various trade-offs between\nperformance and computational requirements. A major shortcoming, as compared to\nthe Bi-LSTM based AttentionXML, is that they fail to keep separate feature\nrepresentations for each resolution in a label tree. We thus propose\nCascadeXML, an end-to-end multi-resolution learning pipeline, which can harness\nthe multi-layered architecture of a transformer model for attending to\ndifferent label resolutions with separate feature representations. CascadeXML\nsignificantly outperforms all existing approaches with non-trivial gains\nobtained on benchmark datasets consisting of up to three million labels. Code\nfor CascadeXML will be made publicly available at\n\\url{https://github.com/xmc-aalto/cascadexml}.", "published": "2022-10-29 11:03:23", "link": "http://arxiv.org/abs/2211.00640v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The Secret Source : Incorporating Source Features to Improve\n  Acoustic-to-Articulatory Speech Inversion", "abstract": "In this work, we incorporated acoustically derived source features,\naperiodicity, periodicity and pitch as additional targets to an\nacoustic-to-articulatory speech inversion (SI) system. We also propose a\nTemporal Convolution based SI system, which uses auditory spectrograms as the\ninput speech representation, to learn long-range dependencies and complex\ninteractions between the source and vocal tract, to improve the SI task. The\nexperiments are conducted with both the Wisconsin X-ray microbeam (XRMB) and\nHaskins Production Rate Comparison (HPRC) datasets, with comparisons done with\nrespect to three baseline SI model architectures. The proposed SI system with\nthe HPRC dataset gains an improvement of close to 28% when the source features\nare used as additional targets. The same SI system outperforms the current best\nperforming SI models by around 9% on the XRMB dataset.", "published": "2022-10-29 00:28:29", "link": "http://arxiv.org/abs/2210.16450v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning to Compute the Articulatory Representations of Speech with the\n  MIRRORNET", "abstract": "Most organisms including humans function by coordinating and integrating\nsensory signals with motor actions to survive and accomplish desired tasks.\nLearning these complex sensorimotor mappings proceeds simultaneously and often\nin an unsupervised or semi-supervised fashion. An autoencoder architecture\n(MirrorNet) inspired by this sensorimotor learning paradigm is explored in this\nwork to control an articulatory synthesizer, with minimal exposure to\nground-truth articulatory data. The articulatory synthesizer takes as input a\nset of six vocal Tract Variables (TVs) and source features (voicing indicators\nand pitch) and is able to synthesize continuous speech for unseen speakers. We\nshow that the MirrorNet, once initialized (with ~30 mins of articulatory data)\nand further trained in unsupervised fashion (`learning phase'), can learn\nmeaningful articulatory representations with comparable accuracy to\narticulatory speech-inversion systems trained in a completely supervised\nfashion.", "published": "2022-10-29 00:46:48", "link": "http://arxiv.org/abs/2210.16454v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Articulatory Representation Learning Via Joint Factor Analysis and\n  Neural Matrix Factorization", "abstract": "Articulatory representation learning is the fundamental research in modeling\nneural speech production system. Our previous work has established a deep\nparadigm to decompose the articulatory kinematics data into gestures, which\nexplicitly model the phonological and linguistic structure encoded with human\nspeech production mechanism, and corresponding gestural scores. We continue\nwith this line of work by raising two concerns: (1) The articulators are\nentangled together in the original algorithm such that some of the articulators\ndo not leverage effective moving patterns, which limits the interpretability of\nboth gestures and gestural scores; (2) The EMA data is sparsely sampled from\narticulators, which limits the intelligibility of learned representations. In\nthis work, we propose a novel articulatory representation decomposition\nalgorithm that takes the advantage of guided factor analysis to derive the\narticulatory-specific factors and factor scores. A neural convolutive matrix\nfactorization algorithm is then employed on the factor scores to derive the new\ngestures and gestural scores. We experiment with the rtMRI corpus that captures\nthe fine-grained vocal tract contours. Both subjective and objective evaluation\nresults suggest that the newly proposed system delivers the articulatory\nrepresentations that are intelligible, generalizable, efficient and\ninterpretable.", "published": "2022-10-29 05:28:54", "link": "http://arxiv.org/abs/2210.16498v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Discriminative Speaker Representation via Contrastive Learning with\n  Class-Aware Attention in Angular Space", "abstract": "The challenges in applying contrastive learning to speaker verification (SV)\nare that the softmax-based contrastive loss lacks discriminative power and that\nthe hard negative pairs can easily influence learning. To overcome the first\nchallenge, we propose a contrastive learning SV framework incorporating an\nadditive angular margin into the supervised contrastive loss in which the\nmargin improves the speaker representation's discrimination ability. For the\nsecond challenge, we introduce a class-aware attention mechanism through which\nhard negative samples contribute less significantly to the supervised\ncontrastive loss. We also employed gradient-based multi-objective optimization\nto balance the classification and contrastive loss. Experimental results on\nCN-Celeb and Voxceleb1 show that this new learning objective can cause the\nencoder to find an embedding space that exhibits great speaker discrimination\nacross languages.", "published": "2022-10-29 14:55:24", "link": "http://arxiv.org/abs/2210.16622v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker Representation Learning via Contrastive Loss with Maximal\n  Speaker Separability", "abstract": "A great challenge in speaker representation learning using deep models is to\ndesign learning objectives that can enhance the discrimination of unseen\nspeakers under unseen domains. This work proposes a supervised contrastive\nlearning objective to learn a speaker embedding space by effectively leveraging\nthe label information in the training data. In such a space, utterance pairs\nspoken by the same or similar speakers will stay close, while utterance pairs\nspoken by different speakers will be far apart. For each training speaker, we\nperform random data augmentation on their utterances to form positive pairs,\nand utterances from different speakers form negative pairs. To maximize speaker\nseparability in the embedding space, we incorporate the additive angular-margin\nloss into the contrastive learning objective. Experimental results on CN-Celeb\nshow that this new learning objective can cause ECAPA-TDNN to find an embedding\nspace that exhibits great speaker discrimination. The contrastive learning\nobjective is easy to implement, and we provide PyTorch code at\nhttps://github.com/shanmon110/AAMSupCon.", "published": "2022-10-29 16:01:29", "link": "http://arxiv.org/abs/2210.16636v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning Audio-Visual Dynamics Using Scene Graphs for Audio Source\n  Separation", "abstract": "There exists an unequivocal distinction between the sound produced by a\nstatic source and that produced by a moving one, especially when the source\nmoves towards or away from the microphone. In this paper, we propose to use\nthis connection between audio and visual dynamics for solving two challenging\ntasks simultaneously, namely: (i) separating audio sources from a mixture using\nvisual cues, and (ii) predicting the 3D visual motion of a sounding source\nusing its separated audio. Towards this end, we present Audio Separator and\nMotion Predictor (ASMP) -- a deep learning framework that leverages the 3D\nstructure of the scene and the motion of sound sources for better audio source\nseparation. At the heart of ASMP is a 2.5D scene graph capturing various\nobjects in the video and their pseudo-3D spatial proximities. This graph is\nconstructed by registering together 2.5D monocular depth predictions from the\n2D video frames and associating the 2.5D scene regions with the outputs of an\nobject detector applied on those frames. The ASMP task is then mathematically\nmodeled as the joint problem of: (i) recursively segmenting the 2.5D scene\ngraph into several sub-graphs, each associated with a constituent sound in the\ninput audio mixture (which is then separated) and (ii) predicting the 3D\nmotions of the corresponding sound sources from the separated audio. To\nempirically evaluate ASMP, we present experiments on two challenging\naudio-visual datasets, viz. Audio Separation in the Wild (ASIW) and Audio\nVisual Event (AVE). Our results demonstrate that ASMP achieves a clear\nimprovement in source separation quality, outperforming prior works on both\ndatasets, while also estimating the direction of motion of the sound sources\nbetter than other methods.", "published": "2022-10-29 02:55:39", "link": "http://arxiv.org/abs/2210.16472v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Relating Human Perception of Musicality to Prediction in a Predictive\n  Coding Model", "abstract": "We explore the use of a neural network inspired by predictive coding for\nmodeling human music perception. This network was developed based on the\ncomputational neuroscience theory of recurrent interactions in the hierarchical\nvisual cortex. When trained with video data using self-supervised learning, the\nmodel manifests behaviors consistent with human visual illusions. Here, we\nadapt this network to model the hierarchical auditory system and investigate\nwhether it will make similar choices to humans regarding the musicality of a\nset of random pitch sequences. When the model is trained with a large corpus of\ninstrumental classical music and popular melodies rendered as mel spectrograms,\nit exhibits greater prediction errors for random pitch sequences that are rated\nless musical by human subjects. We found that the prediction error depends on\nthe amount of information regarding the subsequent note, the pitch interval,\nand the temporal context. Our findings suggest that predictability is\ncorrelated with human perception of musicality and that a predictive coding\nneural network trained on music can be used to characterize the features and\nmotifs contributing to human perception of music.", "published": "2022-10-29 12:20:01", "link": "http://arxiv.org/abs/2210.16587v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
