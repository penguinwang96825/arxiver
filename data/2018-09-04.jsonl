{"title": "Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic\n  Parsing", "abstract": "This paper proposes a neural semantic parsing approach -- Sequence-to-Action,\nwhich models semantic parsing as an end-to-end semantic graph generation\nprocess. Our method simultaneously leverages the advantages from two recent\npromising directions of semantic parsing. Firstly, our model uses a semantic\ngraph to represent the meaning of a sentence, which has a tight-coupling with\nknowledge bases. Secondly, by leveraging the powerful representation learning\nand prediction ability of neural network models, we propose a RNN model which\ncan effectively map sentences to action sequences for semantic graph\ngeneration. Experiments show that our method achieves state-of-the-art\nperformance on OVERNIGHT dataset and gets competitive performance on GEO and\nATIS datasets.", "published": "2018-09-04 02:30:57", "link": "http://arxiv.org/abs/1809.00773v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mapping Instructions to Actions in 3D Environments with Visual Goal\n  Prediction", "abstract": "We propose to decompose instruction execution to goal prediction and action\ngeneration. We design a model that maps raw visual observations to goals using\nLINGUNET, a language-conditioned image generation network, and then generates\nthe actions required to complete them. Our model is trained from demonstration\nonly without external resources. To evaluate our approach, we introduce two\nbenchmarks for instruction following: LANI, a navigation task; and CHAI, where\nan agent executes household instructions. Our evaluation demonstrates the\nadvantages of our model decomposition, and illustrates the challenges posed by\nour new benchmarks.", "published": "2018-09-04 03:36:21", "link": "http://arxiv.org/abs/1809.00786v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving generalization of vocal tract feature reconstruction: from\n  augmented acoustic inversion to articulatory feature reconstruction without\n  articulatory data", "abstract": "We address the problem of reconstructing articulatory movements, given audio\nand/or phonetic labels. The scarce availability of multi-speaker articulatory\ndata makes it difficult to learn a reconstruction that generalizes to new\nspeakers and across datasets. We first consider the XRMB dataset where audio,\narticulatory measurements and phonetic transcriptions are available. We show\nthat phonetic labels, used as input to deep recurrent neural networks that\nreconstruct articulatory features, are in general more helpful than acoustic\nfeatures in both matched and mismatched training-testing conditions. In a\nsecond experiment, we test a novel approach that attempts to build articulatory\nfeatures from prior articulatory information extracted from phonetic labels.\nSuch approach recovers vocal tract movements directly from an acoustic-only\ndataset without using any articulatory measurement. Results show that\narticulatory features generated by this approach can correlate up to 0.59\nPearson product-moment correlation with measured articulatory features.", "published": "2018-09-04 13:19:16", "link": "http://arxiv.org/abs/1809.00938v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\u00c9tude de l'informativit\u00e9 des transcriptions : une approche bas\u00e9e\n  sur le r\u00e9sum\u00e9 automatique", "abstract": "In this paper we propose a new approach to evaluate the informativeness of\ntranscriptions coming from Automatic Speech Recognition systems. This approach,\nbased in the notion of informativeness, is focused on the framework of\nAutomatic Text Summarization performed over these transcriptions. At a first\nglance we estimate the informative content of the various automatic\ntranscriptions, then we explore the capacity of Automatic Text Summarization to\novercome the informative loss. To do this we use an automatic summary\nevaluation protocol without reference (based on the informative content), which\ncomputes the divergence between probability distributions of different textual\nrepresentations: manual and automatic transcriptions and their summaries. After\na set of evaluations this analysis allowed us to judge both the quality of the\ntranscriptions in terms of informativeness and to assess the ability of\nautomatic text summarization to compensate the problems raised during the\ntranscription phase.", "published": "2018-09-04 14:07:40", "link": "http://arxiv.org/abs/1809.00994v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Effect of Context on Metaphor Paraphrase Aptness Judgments", "abstract": "We conduct two experiments to study the effect of context on metaphor\nparaphrase aptness judgments. The first is an AMT crowd source task in which\nspeakers rank metaphor paraphrase candidate sentence pairs in short document\ncontexts for paraphrase aptness. In the second we train a composite DNN to\npredict these human judgments, first in binary classifier mode, and then as\ngradient ratings. We found that for both mean human judgments and our DNN's\npredictions, adding document context compresses the aptness scores towards the\ncenter of the scale, raising low out of context ratings and decreasing high out\nof context scores. We offer a provisional explanation for this compression\neffect.", "published": "2018-09-04 16:03:06", "link": "http://arxiv.org/abs/1809.01060v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IEST: WASSA-2018 Implicit Emotions Shared Task", "abstract": "Past shared tasks on emotions use data with both overt expressions of\nemotions (I am so happy to see you!) as well as subtle expressions where the\nemotions have to be inferred, for instance from event descriptions. Further,\nmost datasets do not focus on the cause or the stimulus of the emotion. Here,\nfor the first time, we propose a shared task where systems have to predict the\nemotions in a large automatically labeled dataset of tweets without access to\nwords denoting emotions. Based on this intention, we call this the Implicit\nEmotion Shared Task (IEST) because the systems have to infer the emotion mostly\nfrom the context. Every tweet has an occurrence of an explicit emotion word\nthat is masked. The tweets are collected in a manner such that they are likely\nto include a description of the cause of the emotion - the stimulus.\nAltogether, 30 teams submitted results which range from macro F1 scores of 21 %\nto 71 %. The baseline (MaxEnt bag of words and bigrams) obtains an F1 score of\n60 % which was available to the participants during the development phase. A\nstudy with human annotators suggests that automatic methods outperform human\npredictions, possibly by honing into subtle textual clues not used by humans.\nCorpora, resources, and results are available at the shared task website at\nhttp://implicitemotions.wassa2018.com.", "published": "2018-09-04 16:44:16", "link": "http://arxiv.org/abs/1809.01083v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Causal Explanation Analysis on Social Media", "abstract": "Understanding causal explanations - reasons given for happenings in one's\nlife - has been found to be an important psychological factor linked to\nphysical and mental health. Causal explanations are often studied through\nmanual identification of phrases over limited samples of personal writing.\nAutomatic identification of causal explanations in social media, while\nchallenging in relying on contextual and sequential cues, offers a larger-scale\nalternative to expensive manual ratings and opens the door for new applications\n(e.g. studying prevailing beliefs about causes, such as climate change). Here,\nwe explore automating causal explanation analysis, building on discourse\nparsing, and presenting two novel subtasks: causality detection (determining\nwhether a causal explanation exists at all) and causal explanation\nidentification (identifying the specific phrase that is the explanation). We\nachieve strong accuracies for both tasks but find different approaches best: an\nSVM for causality prediction (F1 = 0.791) and a hierarchy of Bidirectional\nLSTMs for causal explanation identification (F1 = 0.853). Finally, we explore\napplications of our complete pipeline (F1 = 0.868), showing demographic\ndifferences in mentions of causal explanation and that the association between\na word and sentiment can change when it is used within a causal explanation.", "published": "2018-09-04 19:06:34", "link": "http://arxiv.org/abs/1809.01202v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating More Interesting Responses in Neural Conversation Models with\n  Distributional Constraints", "abstract": "Neural conversation models tend to generate safe, generic responses for most\ninputs. This is due to the limitations of likelihood-based decoding objectives\nin generation tasks with diverse outputs, such as conversation. To address this\nchallenge, we propose a simple yet effective approach for incorporating side\ninformation in the form of distributional constraints over the generated\nresponses. We propose two constraints that help generate more content rich\nresponses that are based on a model of syntax and topics (Griffiths et al.,\n2005) and semantic similarity (Arora et al., 2016). We evaluate our approach\nagainst a variety of competitive baselines, using both automatic metrics and\nhuman judgments, showing that our proposed approach generates responses that\nare much less generic without sacrificing plausibility. A working demo of our\ncode can be found at https://github.com/abaheti95/DC-NeuralConversation.", "published": "2018-09-04 19:29:03", "link": "http://arxiv.org/abs/1809.01215v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph-based Deep-Tree Recursive Neural Network (DTRNN) for Text\n  Classification", "abstract": "A novel graph-to-tree conversion mechanism called the deep-tree generation\n(DTG) algorithm is first proposed to predict text data represented by graphs.\nThe DTG method can generate a richer and more accurate representation for nodes\n(or vertices) in graphs. It adds flexibility in exploring the vertex\nneighborhood information to better reflect the second order proximity and\nhomophily equivalence in a graph. Then, a Deep-Tree Recursive Neural Network\n(DTRNN) method is presented and used to classify vertices that contains text\ndata in graphs. To demonstrate the effectiveness of the DTRNN method, we apply\nit to three real-world graph datasets and show that the DTRNN method\noutperforms several state-of-the-art benchmarking methods.", "published": "2018-09-04 19:39:24", "link": "http://arxiv.org/abs/1809.01219v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open Domain Question Answering Using Early Fusion of Knowledge Bases and\n  Text", "abstract": "Open Domain Question Answering (QA) is evolving from complex pipelined\nsystems to end-to-end deep neural networks. Specialized neural models have been\ndeveloped for extracting answers from either text alone or Knowledge Bases\n(KBs) alone. In this paper we look at a more practical setting, namely QA over\nthe combination of a KB and entity-linked text, which is appropriate when an\nincomplete KB is available with a large text corpus. Building on recent\nadvances in graph representation learning we propose a novel model, GRAFT-Net,\nfor extracting answers from a question-specific subgraph containing text and KB\nentities and relations. We construct a suite of benchmark tasks for this\nproblem, varying the difficulty of questions, the amount of training data, and\nKB completeness. We show that GRAFT-Net is competitive with the\nstate-of-the-art when tested using either KBs or text alone, and vastly\noutperforms existing methods in the combined setting. Source code is available\nat https://github.com/OceanskySun/GraftNet .", "published": "2018-09-04 03:15:56", "link": "http://arxiv.org/abs/1809.00782v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pointwise HSIC: A Linear-Time Kernelized Co-occurrence Norm for Sparse\n  Linguistic Expressions", "abstract": "In this paper, we propose a new kernel-based co-occurrence measure that can\nbe applied to sparse linguistic expressions (e.g., sentences) with a very short\nlearning time, as an alternative to pointwise mutual information (PMI). As well\nas deriving PMI from mutual information, we derive this new measure from the\nHilbert--Schmidt independence criterion (HSIC); thus, we call the new measure\nthe pointwise HSIC (PHSIC). PHSIC can be interpreted as a smoothed variant of\nPMI that allows various similarity metrics (e.g., sentence embeddings) to be\nplugged in as kernels. Moreover, PHSIC can be estimated by simple and fast\n(linear in the size of the data) matrix calculations regardless of whether we\nuse linear or nonlinear kernels. Empirically, in a dialogue response selection\ntask, PHSIC is learned thousands of times faster than an RNN-based PMI while\noutperforming PMI in accuracy. In addition, we also demonstrate that PHSIC is\nbeneficial as a criterion of a data selection task for machine translation\nowing to its ability to give high (low) scores to a consistent (inconsistent)\npair with other pairs.", "published": "2018-09-04 05:33:00", "link": "http://arxiv.org/abs/1809.00800v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking\n  Recipes", "abstract": "Understanding and reasoning about cooking recipes is a fruitful research\ndirection towards enabling machines to interpret procedural text. In this work,\nwe introduce RecipeQA, a dataset for multimodal comprehension of cooking\nrecipes. It comprises of approximately 20K instructional recipes with multiple\nmodalities such as titles, descriptions and aligned set of images. With over\n36K automatically generated question-answer pairs, we design a set of\ncomprehension and reasoning tasks that require joint understanding of images\nand text, capturing the temporal flow of events and making sense of procedural\nknowledge. Our preliminary results indicate that RecipeQA will serve as a\nchallenging test bed and an ideal benchmark for evaluating machine\ncomprehension systems. The data and leaderboard are available at\nhttp://hucvl.github.io/recipeqa.", "published": "2018-09-04 07:04:55", "link": "http://arxiv.org/abs/1809.00812v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Segmentation-free Compositional $n$-gram Embedding", "abstract": "We propose a new type of representation learning method that models words,\nphrases and sentences seamlessly. Our method does not depend on word\nsegmentation and any human-annotated resources (e.g., word dictionaries), yet\nit is very effective for noisy corpora written in unsegmented languages such as\nChinese and Japanese. The main idea of our method is to ignore word boundaries\ncompletely (i.e., segmentation-free), and construct representations for all\ncharacter $n$-grams in a raw corpus with embeddings of compositional\nsub-$n$-grams. Although the idea is simple, our experiments on various\nbenchmarks and real-world datasets show the efficacy of our proposal.", "published": "2018-09-04 12:32:54", "link": "http://arxiv.org/abs/1809.00918v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Novel Neural Sequence Model with Multiple Attentions for Word Sense\n  Disambiguation", "abstract": "Word sense disambiguation (WSD) is a well researched problem in computational\nlinguistics. Different research works have approached this problem in different\nways. Some state of the art results that have been achieved for this problem\nare by supervised models in terms of accuracy, but they often fall behind\nflexible knowledge-based solutions which use engineered features as well as\nhuman annotators to disambiguate every target word. This work focuses on\nbridging this gap using neural sequence models incorporating the well-known\nattention mechanism. The main gist of our work is to combine multiple\nattentions on different linguistic features through weights and to provide a\nunified framework for doing this. This weighted attention allows the model to\neasily disambiguate the sense of an ambiguous word by attending over a suitable\nportion of a sentence. Our extensive experiments show that multiple attention\nenables a more versatile encoder-decoder model leading to state of the art\nresults.", "published": "2018-09-04 16:28:36", "link": "http://arxiv.org/abs/1809.01074v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Text2Scene: Generating Compositional Scenes from Textual Descriptions", "abstract": "In this paper, we propose Text2Scene, a model that generates various forms of\ncompositional scene representations from natural language descriptions. Unlike\nrecent works, our method does NOT use Generative Adversarial Networks (GANs).\nText2Scene instead learns to sequentially generate objects and their attributes\n(location, size, appearance, etc) at every time step by attending to different\nparts of the input text and the current status of the generated scene. We show\nthat under minor modifications, the proposed framework can handle the\ngeneration of different forms of scene representations, including cartoon-like\nscenes, object layouts corresponding to real images, and synthetic images. Our\nmethod is not only competitive when compared with state-of-the-art GAN-based\nmethods using automatic metrics and superior based on human judgments but also\nhas the advantage of producing interpretable results.", "published": "2018-09-04 17:31:13", "link": "http://arxiv.org/abs/1809.01110v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Texar: A Modularized, Versatile, and Extensible Toolkit for Text\n  Generation", "abstract": "We introduce Texar, an open-source toolkit aiming to support the broad set of\ntext generation tasks that transform any inputs into natural language, such as\nmachine translation, summarization, dialog, content manipulation, and so forth.\nWith the design goals of modularity, versatility, and extensibility in mind,\nTexar extracts common patterns underlying the diverse tasks and methodologies,\ncreates a library of highly reusable modules, and allows arbitrary model\narchitectures and algorithmic paradigms. In Texar, model architecture,\ninference, and learning processes are properly decomposed. Modules at a high\nconcept level can be freely assembled and plugged in/swapped out. The toolkit\nalso supports a rich set of large-scale pretrained models. Texar is thus\nparticularly suitable for researchers and practitioners to do fast prototyping\nand experimentation. The versatile toolkit also fosters technique sharing\nacross different text generation tasks. Texar supports both TensorFlow and\nPyTorch, and is released under Apache License 2.0 at https://www.texar.io.", "published": "2018-09-04 04:40:34", "link": "http://arxiv.org/abs/1809.00794v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving the Expressiveness of Deep Learning Frameworks with Recursion", "abstract": "Recursive neural networks have widely been used by researchers to handle\napplications with recursively or hierarchically structured data. However,\nembedded control flow deep learning frameworks such as TensorFlow, Theano,\nCaffe2, and MXNet fail to efficiently represent and execute such neural\nnetworks, due to lack of support for recursion. In this paper, we add recursion\nto the programming model of existing frameworks by complementing their design\nwith recursive execution of dataflow graphs as well as additional APIs for\nrecursive definitions. Unlike iterative implementations, which can only\nunderstand the topological index of each node in recursive data structures, our\nrecursive implementation is able to exploit the recursive relationships between\nnodes for efficient execution based on parallel computation. We present an\nimplementation on TensorFlow and evaluation results with various recursive\nneural network models, showing that our recursive implementation not only\nconveys the recursive nature of recursive neural networks better than other\nimplementations, but also uses given resources more effectively to reduce\ntraining and inference time.", "published": "2018-09-04 08:31:21", "link": "http://arxiv.org/abs/1809.00832v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Recurrent Neural Network for Sentiment Quantification", "abstract": "Quantification is a supervised learning task that consists in predicting,\ngiven a set of classes C and a set D of unlabelled items, the prevalence (or\nrelative frequency) p(c|D) of each class c in C. Quantification can in\nprinciple be solved by classifying all the unlabelled items and counting how\nmany of them have been attributed to each class. However, this \"classify and\ncount\" approach has been shown to yield suboptimal quantification accuracy;\nthis has established quantification as a task of its own, and given rise to a\nnumber of methods specifically devised for it. We propose a recurrent neural\nnetwork architecture for quantification (that we call QuaNet) that observes the\nclassification predictions to learn higher-order \"quantification embeddings\",\nwhich are then refined by incorporating quantification predictions of simple\nclassify-and-count-like methods. We test {QuaNet on sentiment quantification on\ntext, showing that it substantially outperforms several state-of-the-art\nbaselines.", "published": "2018-09-04 08:41:53", "link": "http://arxiv.org/abs/1809.00836v1", "categories": ["cs.LG", "cs.CL", "stat.ML", "I.2.6; I.2.7"], "primary_category": "cs.LG"}
{"title": "Random Language Model", "abstract": "Many complex generative systems use languages to create structured objects.\nWe consider a model of random languages, defined by weighted context-free\ngrammars. As the distribution of grammar weights broadens, a transition is\nfound from a random phase, in which sentences are indistinguishable from noise,\nto an organized phase in which nontrivial information is carried. This marks\nthe emergence of deep structure in the language, and can be understood by a\ncompetition between energy and entropy.", "published": "2018-09-04 19:06:18", "link": "http://arxiv.org/abs/1809.01201v2", "categories": ["cond-mat.dis-nn", "cs.CL", "cs.FL"], "primary_category": "cond-mat.dis-nn"}
{"title": "t-Exponential Memory Networks for Question-Answering Machines", "abstract": "Recent advances in deep learning have brought to the fore models that can\nmake multiple computational steps in the service of completing a task; these\nare capable of describ- ing long-term dependencies in sequential data. Novel\nrecurrent attention models over possibly large external memory modules\nconstitute the core mechanisms that enable these capabilities. Our work\naddresses learning subtler and more complex underlying temporal dynamics in\nlanguage modeling tasks that deal with sparse sequential data. To this end, we\nimprove upon these recent advances, by adopting concepts from the field of\nBayesian statistics, namely variational inference. Our proposed approach\nconsists in treating the network parameters as latent variables with a prior\ndistribution imposed over them. Our statistical assumptions go beyond the\nstandard practice of postulating Gaussian priors. Indeed, to allow for handling\noutliers, which are prevalent in long observed sequences of multivariate data,\nmultivariate t-exponential distributions are imposed. On this basis, we proceed\nto infer corresponding posteriors; these can be used for inference and\nprediction at test time, in a way that accounts for the uncertainty in the\navailable sparse training data. Specifically, to allow for our approach to best\nexploit the merits of the t-exponential family, our method considers a new\nt-divergence measure, which generalizes the concept of the Kullback-Leibler\ndivergence. We perform an extensive experimental evaluation of our approach,\nusing challenging language modeling benchmarks, and illustrate its superiority\nover existing state-of-the-art techniques.", "published": "2018-09-04 20:09:01", "link": "http://arxiv.org/abs/1809.01229v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Unsupervised Statistical Machine Translation", "abstract": "While modern machine translation has relied on large parallel corpora, a\nrecent line of work has managed to train Neural Machine Translation (NMT)\nsystems from monolingual corpora only (Artetxe et al., 2018c; Lample et al.,\n2018). Despite the potential of this approach for low-resource settings,\nexisting systems are far behind their supervised counterparts, limiting their\npractical interest. In this paper, we propose an alternative approach based on\nphrase-based Statistical Machine Translation (SMT) that significantly closes\nthe gap with supervised systems. Our method profits from the modular\narchitecture of SMT: we first induce a phrase table from monolingual corpora\nthrough cross-lingual embedding mappings, combine it with an n-gram language\nmodel, and fine-tune hyperparameters through an unsupervised MERT variant. In\naddition, iterative backtranslation improves results further, yielding, for\ninstance, 14.08 and 26.22 BLEU points in WMT 2014 English-German and\nEnglish-French, respectively, an improvement of more than 7-10 BLEU points over\nprevious unsupervised systems, and closing the gap with supervised SMT (Moses\ntrained on Europarl) down to 2-5 BLEU points. Our implementation is available\nat https://github.com/artetxem/monoses", "published": "2018-09-04 23:22:28", "link": "http://arxiv.org/abs/1809.01272v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "End-to-end Multimodal Emotion and Gender Recognition with Dynamic Joint\n  Loss Weights", "abstract": "Multi-task learning is a method for improving the generalizability of\nmultiple tasks. In order to perform multiple classification tasks with one\nneural network model, the losses of each task should be combined. Previous\nstudies have mostly focused on multiple prediction tasks using joint loss with\nstatic weights for training models, choosing the weights between tasks without\nmaking sufficient considerations by setting them uniformly or empirically. In\nthis study, we propose a method to calculate joint loss using dynamic weights\nto improve the total performance, instead of the individual performance, of\ntasks. We apply this method to design an end-to-end multimodal emotion and\ngender recognition model using audio and video data. This approach provides\nproper weights for the loss of each task when the training process ends. In our\nexperiments, emotion and gender recognition with the proposed method yielded a\nlower joint loss, which is computed as the negative log-likelihood, than using\nstatic weights for joint loss. Moreover, our proposed model has better\ngeneralizability than other models. To the best of our knowledge, this research\nis the first to demonstrate the strength of using dynamic weights for joint\nloss for maximizing overall performance in emotion and gender recognition\ntasks.", "published": "2018-09-04 00:52:25", "link": "http://arxiv.org/abs/1809.00758v3", "categories": ["cs.LG", "cs.CV", "cs.SD", "eess.AS", "stat.ML", "68T05"], "primary_category": "cs.LG"}
{"title": "Automated bird sound recognition in realistic settings", "abstract": "We evaluated the effectiveness of an automated bird sound identification\nsystem in a situation that emulates a realistic, typical application. We\ntrained classification algorithms on a crowd-sourced collection of bird audio\nrecording data and restricted our training methods to be completely free of\nmanual intervention. The approach is hence directly applicable to the analysis\nof multiple species collections, with labelling provided by crowd-sourced\ncollection. We evaluated the performance of the bird sound recognition system\non a realistic number of candidate classes, corresponding to real conditions.\nWe investigated the use of two canonical classification methods, chosen due to\ntheir widespread use and ease of interpretation, namely a k Nearest Neighbour\n(kNN) classifier with histogram-based features and a Support Vector Machine\n(SVM) with time-summarisation features. We further investigated the use of a\ncertainty measure, derived from the output probabilities of the classifiers, to\nenhance the interpretability and reliability of the class decisions. Our\nresults demonstrate that both identification methods achieved similar\nperformance, but we argue that the use of the kNN classifier offers somewhat\nmore flexibility. Furthermore, we show that employing an outcome certainty\nmeasure provides a valuable and consistent indicator of the reliability of\nclassification results. Our use of generic training data and our investigation\nof probabilistic classification methodologies that can flexibly address the\nvariable number of candidate species/classes that are expected to be\nencountered in the field, directly contribute to the development of a practical\nbird sound identification system with potentially global application. Further,\nwe show that certainty measures associated with identification outcomes can\nsignificantly contribute to the practical usability of the overall system.", "published": "2018-09-04 10:26:37", "link": "http://arxiv.org/abs/1809.01133v1", "categories": ["cs.SD", "cs.CY", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "HASP: A High-Performance Adaptive Mobile Security Enhancement Against\n  Malicious Speech Recognition", "abstract": "Nowadays, machine learning based Automatic Speech Recognition (ASR) technique\nhas widely spread in smartphones, home devices, and public facilities. As\nconvenient as this technology can be, a considerable security issue also raises\n-- the users' speech content might be exposed to malicious ASR monitoring and\ncause severe privacy leakage. In this work, we propose HASP -- a\nhigh-performance security enhancement approach to solve this security issue on\nmobile devices. Leveraging ASR systems' vulnerability to the adversarial\nexamples, HASP is designed to cast human imperceptible adversarial noises to\nreal-time speech and effectively perturb malicious ASR monitoring by increasing\nthe Word Error Rate (WER). To enhance the practical performance on mobile\ndevices, HASP is also optimized for effective adaptation to the human speech\ncharacteristics, environmental noises, and mobile computation scenarios. The\nexperiments show that HASP can achieve optimal real-time security enhancement:\nit can lead an average WER of 84.55% for perturbing the malicious ASR\nmonitoring, and the data processing speed is 15x to 40x faster compared to the\nstate-of-the-art methods. Moreover, HASP can effectively perturb various ASR\nsystems, demonstrating a strong transferability.", "published": "2018-09-04 00:47:50", "link": "http://arxiv.org/abs/1809.01697v1", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS", "eess.SP", "stat.ML"], "primary_category": "cs.CR"}
