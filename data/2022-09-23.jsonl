{"title": "Extending Word-Level Quality Estimation for Post-Editing Assistance", "abstract": "We define a novel concept called extended word alignment in order to improve\npost-editing assistance efficiency. Based on extended word alignment, we\nfurther propose a novel task called refined word-level QE that outputs refined\ntags and word-level correspondences. Compared to original word-level QE, the\nnew task is able to directly point out editing operations, thus improves\nefficiency. To extract extended word alignment, we adopt a supervised method\nbased on mBERT. To solve refined word-level QE, we firstly predict original QE\ntags by training a regression model for sequence tagging based on mBERT and\nXLM-R. Then, we refine original word tags with extended word alignment. In\naddition, we extract source-gap correspondences, meanwhile, obtaining gap tags.\nExperiments on two language pairs show the feasibility of our method and give\nus inspirations for further improvement.", "published": "2022-09-23 02:42:11", "link": "http://arxiv.org/abs/2209.11378v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot Domain Adaptation for Neural Machine Translation with\n  Retrieved Phrase-level Prompts", "abstract": "Domain adaptation is an important challenge for neural machine translation.\nHowever, the traditional fine-tuning solution requires multiple extra training\nand yields a high cost. In this paper, we propose a non-tuning paradigm,\nresolving domain adaptation with a prompt-based method. Specifically, we\nconstruct a bilingual phrase-level database and retrieve relevant pairs from it\nas a prompt for the input sentences. By utilizing Retrieved Phrase-level\nPrompts (RePP), we effectively boost the translation quality. Experiments show\nthat our method improves domain-specific machine translation for 6.2 BLEU\nscores and improves translation constraints for 11.5% accuracy without\nadditional training.", "published": "2022-09-23 04:58:17", "link": "http://arxiv.org/abs/2209.11409v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "News Category Dataset", "abstract": "People rely on news to know what is happening around the world and inform\ntheir daily lives. In today's world, when the proliferation of fake news is\nrampant, having a large-scale and high-quality source of authentic news\narticles with the published category information is valuable to learning\nauthentic news' Natural Language syntax and semantics. As part of this work, we\npresent a News Category Dataset that contains around 210k news headlines from\nthe year 2012 to 2022 obtained from HuffPost, along with useful metadata to\nenable various NLP tasks. In this paper, we also produce some novel insights\nfrom the dataset and describe various existing and potential applications of\nour dataset.", "published": "2022-09-23 06:13:16", "link": "http://arxiv.org/abs/2209.11429v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ET5: A Novel End-to-end Framework for Conversational Machine Reading\n  Comprehension", "abstract": "Conversational machine reading comprehension (CMRC) aims to assist computers\nto understand an natural language text and thereafter engage in a multi-turn\nconversation to answer questions related to the text. Existing methods\ntypically require three steps: (1) decision making based on entailment\nreasoning; (2) span extraction if required by the above decision; (3) question\nrephrasing based on the extracted span. However, for nearly all these methods,\nthe span extraction and question rephrasing steps cannot fully exploit the\nfine-grained entailment reasoning information in decision making step because\nof their relative independence, which will further enlarge the information gap\nbetween decision making and question phrasing. Thus, to tackle this problem, we\npropose a novel end-to-end framework for conversational machine reading\ncomprehension based on shared parameter mechanism, called entailment reasoning\nT5 (ET5). Despite the lightweight of our proposed framework, experimental\nresults show that the proposed ET5 achieves new state-of-the-art results on the\nShARC leaderboard with the BLEU-4 score of 55.2. Our model and code are\npublicly available at https://github.com/Yottaxx/ET5.", "published": "2022-09-23 08:58:03", "link": "http://arxiv.org/abs/2209.11484v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MetaPrompting: Learning to Learn Better Prompts", "abstract": "Prompting method is regarded as one of the crucial progress for few-shot\nnature language processing. Recent research on prompting moves from discrete\ntokens based ``hard prompts'' to continuous ``soft prompts'', which employ\nlearnable vectors as pseudo prompt tokens and achieve better performance.\nThough showing promising prospects, these soft-prompting methods are observed\nto rely heavily on good initialization to take effect. Unfortunately, obtaining\na perfect initialization for soft prompts requires understanding of inner\nlanguage models working and elaborate design, which is no easy task and has to\nrestart from scratch for each new task. To remedy this, we propose a\ngeneralized soft prompting method called MetaPrompting, which adopts the\nwell-recognized model-agnostic meta-learning algorithm to automatically find\nbetter prompt initialization that facilitates fast adaptation to new prompting\ntasks.Extensive experiments show MetaPrompting tackles soft prompt\ninitialization problem and brings significant improvement on four different\ndatasets (over 6 points improvement in accuracy for 1-shot setting), achieving\nnew state-of-the-art performance.", "published": "2022-09-23 09:01:05", "link": "http://arxiv.org/abs/2209.11486v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Temporal Analysis on Topics Using Word2Vec", "abstract": "The present study proposes a novel method of trend detection and\nvisualization - more specifically, modeling the change in a topic over time.\nWhere current models used for the identification and visualization of trends\nonly convey the popularity of a singular word based on stochastic counting of\nusage, the approach in the present study illustrates the popularity and\ndirection that a topic is moving in. The direction in this case is a distinct\nsubtopic within the selected corpus. Such trends are generated by modeling the\nmovement of a topic by using k-means clustering and cosine similarity to group\nthe distances between clusters over time. In a convergent scenario, it can be\ninferred that the topics as a whole are meshing (tokens between topics,\nbecoming interchangeable). On the contrary, a divergent scenario would imply\nthat each topics' respective tokens would not be found in the same context (the\nwords are increasingly different to each other). The methodology was tested on\na group of articles from various media houses present in the 20 Newsgroups\ndataset.", "published": "2022-09-23 16:51:29", "link": "http://arxiv.org/abs/2209.11717v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KeypartX: Graph-based Perception (Text) Representation", "abstract": "The availability of big data has opened up big opportunities for individuals,\nbusinesses and academics to view big into what is happening in their world.\nPrevious works of text representation mostly focused on informativeness from\nmassive words' frequency or cooccurrence. However, big data is a double-edged\nsword which is big in volume but unstructured in format. The unstructured edge\nrequires specific techniques to transform 'big' into meaningful instead of\ninformative alone.\n  This study presents KeypartX, a graph-based approach to represent perception\n(text in general) by key parts of speech. Different from\nbag-of-words/vector-based machine learning, this technique is human-like\nlearning that could extracts meanings from linguistic (semantic, syntactic and\npragmatic) information. Moreover, KeypartX is big-data capable but not hungry,\nwhich is even applicable to the minimum unit of text:sentence.", "published": "2022-09-23 20:15:50", "link": "http://arxiv.org/abs/2209.11844v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cem Mil Podcasts: A Spoken Portuguese Document Corpus For Multi-modal,\n  Multi-lingual and Multi-Dialect Information Access Research", "abstract": "In this paper we describe the Portuguese-language podcast dataset we have\nreleased for academic research purposes. We give an overview of how the data\nwas sampled, descriptive statistics over the collection, as well as information\nabout the distribution over Brazilian and Portuguese dialects. We give results\nfrom experiments on multi-lingual summarization, showing that summarizing\npodcast transcripts can be performed well by a system supporting both English\nand Portuguese. We also show experiments on Portuguese podcast genre\nclassification using text metadata. Combining this collection with previously\nreleased English-language collection opens up the potential for multi-modal,\nmulti-lingual and multi-dialect podcast information access research.", "published": "2022-09-23 21:41:10", "link": "http://arxiv.org/abs/2209.11871v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Conversational Recommender System via Contextual and\n  Time-Aware Modeling with Less Domain-Specific Knowledge", "abstract": "Conversational Recommender Systems (CRS) has become an emerging research\ntopic seeking to perform recommendations through interactive conversations,\nwhich generally consist of generation and recommendation modules. Prior work on\nCRS tends to incorporate more external and domain-specific knowledge like item\nreviews to enhance performance. Despite the fact that the collection and\nannotation of the external domain-specific information needs much human effort\nand degenerates the generalizability, too much extra knowledge introduces more\ndifficulty to balance among them. Therefore, we propose to fully discover and\nextract internal knowledge from the context. We capture both entity-level and\ncontextual-level representations to jointly model user preferences for the\nrecommendation, where a time-aware attention is designed to emphasize the\nrecently appeared items in entity-level representations. We further use the\npre-trained BART to initialize the generation module to alleviate the data\nscarcity and enhance the context modeling. In addition to conducting\nexperiments on a popular dataset (ReDial), we also include a multi-domain\ndataset (OpenDialKG) to show the effectiveness of our model. Experiments on\nboth datasets show that our model achieves better performance on most\nevaluation metrics with less external knowledge and generalizes well to other\ndomains. Additional analyses on the recommendation and generation tasks\ndemonstrate the effectiveness of our model in different scenarios.", "published": "2022-09-23 03:30:22", "link": "http://arxiv.org/abs/2209.11386v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Conversational QA Dataset Generation with Answer Revision", "abstract": "Conversational question--answer generation is a task that automatically\ngenerates a large-scale conversational question answering dataset based on\ninput passages. In this paper, we introduce a novel framework that extracts\nquestion-worthy phrases from a passage and then generates corresponding\nquestions considering previous conversations. In particular, our framework\nrevises the extracted answers after generating questions so that answers\nexactly match paired questions. Experimental results show that our simple\nanswer revision approach leads to significant improvement in the quality of\nsynthetic data. Moreover, we prove that our framework can be effectively\nutilized for domain adaptation of conversational question answering.", "published": "2022-09-23 04:05:38", "link": "http://arxiv.org/abs/2209.11396v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IDEA: Interactive DoublE Attentions from Label Embedding for Text\n  Classification", "abstract": "Current text classification methods typically encode the text merely into\nembedding before a naive or complicated classifier, which ignores the\nsuggestive information contained in the label text. As a matter of fact, humans\nclassify documents primarily based on the semantic meaning of the\nsubcategories. We propose a novel model structure via siamese BERT and\ninteractive double attentions named IDEA ( Interactive DoublE Attentions) to\ncapture the information exchange of text and label names. Interactive double\nattentions enable the model to exploit the inter-class and intra-class\ninformation from coarse to fine, which involves distinguishing among all labels\nand matching the semantical subclasses of ground truth labels. Our proposed\nmethod outperforms the state-of-the-art methods using label texts significantly\nwith more stable results.", "published": "2022-09-23 04:50:47", "link": "http://arxiv.org/abs/2209.11407v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "An Interdisciplinary Perspective on Evaluation and Experimental Design\n  for Visual Text Analytics: Position Paper", "abstract": "Appropriate evaluation and experimental design are fundamental for empirical\nsciences, particularly in data-driven fields. Due to the successes in\ncomputational modeling of languages, for instance, research outcomes are having\nan increasingly immediate impact on end users. As the gap in adoption by end\nusers decreases, the need increases to ensure that tools and models developed\nby the research communities and practitioners are reliable, trustworthy, and\nsupportive of the users in their goals. In this position paper, we focus on the\nissues of evaluating visual text analytics approaches. We take an\ninterdisciplinary perspective from the visualization and natural language\nprocessing communities, as we argue that the design and validation of visual\ntext analytics include concerns beyond computational or visual/interactive\nmethods on their own. We identify four key groups of challenges for evaluating\nvisual text analytics approaches (data ambiguity, experimental design, user\ntrust, and \"big picture\" concerns) and provide suggestions for research\nopportunities from an interdisciplinary perspective.", "published": "2022-09-23 11:47:37", "link": "http://arxiv.org/abs/2209.11534v2", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Robust Domain Adaptation for Machine Reading Comprehension", "abstract": "Most domain adaptation methods for machine reading comprehension (MRC) use a\npre-trained question-answer (QA) construction model to generate pseudo QA pairs\nfor MRC transfer. Such a process will inevitably introduce mismatched pairs\n(i.e., noisy correspondence) due to i) the unavailable QA pairs in target\ndocuments, and ii) the domain shift during applying the QA construction model\nto the target domain. Undoubtedly, the noisy correspondence will degenerate the\nperformance of MRC, which however is neglected by existing works. To solve such\nan untouched problem, we propose to construct QA pairs by additionally using\nthe dialogue related to the documents, as well as a new domain adaptation\nmethod for MRC. Specifically, we propose Robust Domain Adaptation for Machine\nReading Comprehension (RMRC) method which consists of an answer extractor (AE),\na question selector (QS), and an MRC model. Specifically, RMRC filters out the\nirrelevant answers by estimating the correlation to the document via the AE,\nand extracts the questions by fusing the candidate questions in multiple rounds\nof dialogue chats via the QS. With the extracted QA pairs, MRC is fine-tuned\nand provides the feedback to optimize the QS through a novel reinforced\nself-training method. Thanks to the optimization of the QS, our method will\ngreatly alleviate the noisy correspondence problem caused by the domain shift.\nTo the best of our knowledge, this could be the first study to reveal the\ninfluence of noisy correspondence in domain adaptation MRC models and show a\nfeasible way to achieve robustness to mismatched pairs. Extensive experiments\non three datasets demonstrate the effectiveness of our method.", "published": "2022-09-23 14:38:40", "link": "http://arxiv.org/abs/2209.11615v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Neural Model for Regular Grammar Induction", "abstract": "Grammatical inference is a classical problem in computational learning theory\nand a topic of wider influence in natural language processing. We treat\ngrammars as a model of computation and propose a novel neural approach to\ninduction of regular grammars from positive and negative examples. Our model is\nfully explainable, its intermediate results are directly interpretable as\npartial parses, and it can be used to learn arbitrary regular grammars when\nprovided with sufficient data. We find that our method consistently attains\nhigh recall and precision scores across a range of tests of varying complexity.", "published": "2022-09-23 14:53:23", "link": "http://arxiv.org/abs/2209.11628v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Promptagator: Few-shot Dense Retrieval From 8 Examples", "abstract": "Much recent research on information retrieval has focused on how to transfer\nfrom one task (typically with abundant supervised data) to various other tasks\nwhere supervision is limited, with the implicit assumption that it is possible\nto generalize from one task to all the rest. However, this overlooks the fact\nthat there are many diverse and unique retrieval tasks, each targeting\ndifferent search intents, queries, and search domains. In this paper, we\nsuggest to work on Few-shot Dense Retrieval, a setting where each task comes\nwith a short description and a few examples. To amplify the power of a few\nexamples, we propose Prompt-base Query Generation for Retriever (Promptagator),\nwhich leverages large language models (LLM) as a few-shot query generator, and\ncreates task-specific retrievers based on the generated data. Powered by LLM's\ngeneralization ability, Promptagator makes it possible to create task-specific\nend-to-end retrievers solely based on a few examples {without} using Natural\nQuestions or MS MARCO to train %question generators or dual encoders.\nSurprisingly, LLM prompting with no more than 8 examples allows dual encoders\nto outperform heavily engineered models trained on MS MARCO like ColBERT v2 by\nmore than 1.2 nDCG on average on 11 retrieval sets. Further training\nstandard-size re-rankers using the same generated data yields another 5.0 point\nnDCG improvement. Our studies determine that query generation can be far more\neffective than previously observed, especially when a small amount of\ntask-specific knowledge is given.", "published": "2022-09-23 17:59:06", "link": "http://arxiv.org/abs/2209.11755v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Multiple-Choice Question Generation: Towards an Automated Assessment\n  Framework", "abstract": "Automated question generation is an important approach to enable\npersonalisation of English comprehension assessment. Recently,\ntransformer-based pretrained language models have demonstrated the ability to\nproduce appropriate questions from a context paragraph. Typically, these\nsystems are evaluated against a reference set of manually generated questions\nusing n-gram based metrics, or manual qualitative assessment. Here, we focus on\na fully automated multiple-choice question generation (MCQG) system where both\nthe question and possible answers must be generated from the context paragraph.\nApplying n-gram based approaches is challenging for this form of system as the\nreference set is unlikely to capture the full range of possible questions and\nanswer options. Conversely manual assessment scales poorly and is expensive for\nMCQG system development. In this work, we propose a set of performance criteria\nthat assess different aspects of the generated multiple-choice questions of\ninterest. These qualities include: grammatical correctness, answerability,\ndiversity and complexity. Initial systems for each of these metrics are\ndescribed, and individually evaluated on standard multiple-choice reading\ncomprehension corpora.", "published": "2022-09-23 19:51:46", "link": "http://arxiv.org/abs/2209.11830v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Best Prompts for Text-to-Image Models and How to Find Them", "abstract": "Recent progress in generative models, especially in text-guided diffusion\nmodels, has enabled the production of aesthetically-pleasing imagery resembling\nthe works of professional human artists. However, one has to carefully compose\nthe textual description, called the prompt, and augment it with a set of\nclarifying keywords. Since aesthetics are challenging to evaluate\ncomputationally, human feedback is needed to determine the optimal prompt\nformulation and keyword combination. In this paper, we present a\nhuman-in-the-loop approach to learning the most useful combination of prompt\nkeywords using a genetic algorithm. We also show how such an approach can\nimprove the aesthetic appeal of images depicting the same descriptions.", "published": "2022-09-23 16:39:13", "link": "http://arxiv.org/abs/2209.11711v3", "categories": ["cs.HC", "cs.CL", "cs.CV", "H.5.2; H.3.3"], "primary_category": "cs.HC"}
{"title": "Augmenting Interpretable Models with LLMs during Training", "abstract": "Recent large language models (LLMs) have demonstrated remarkable prediction\nperformance for a growing array of tasks. However, their proliferation into\nhigh-stakes domains (e.g. medicine) and compute-limited settings has created a\nburgeoning need for interpretability and efficiency. We address this need by\nproposing Augmented Interpretable Models (Aug-imodels), a framework for\nleveraging the knowledge learned by LLMs to build extremely efficient and\ninterpretable models. Aug-imodels use LLMs during fitting but not during\ninference, allowing complete transparency and often a speed/memory improvement\nof greater than 1,000x for inference compared to LLMs. We explore two\ninstantiations of Aug-imodels in natural-language processing: (i) Aug-GAM,\nwhich augments a generalized additive model with decoupled embeddings from an\nLLM and (ii) Aug-Tree, which augments a decision tree with LLM feature\nexpansions. Across a variety of text-classification datasets, both outperform\ntheir non-augmented counterparts. Aug-GAM can even outperform much larger\nmodels (e.g. a 6-billion parameter GPT-J model), despite having 10,000x fewer\nparameters and being fully transparent. We further explore Aug-imodels in a\nnatural-language fMRI study, where they generate interesting interpretations\nfrom scientific data. All code for using Aug-imodels and reproducing results is\nmade available on Github.", "published": "2022-09-23 18:36:01", "link": "http://arxiv.org/abs/2209.11799v3", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ME"], "primary_category": "cs.AI"}
{"title": "Comparison of Lexical Alignment with a Teachable Robot in Human-Robot\n  and Human-Human-Robot Interactions", "abstract": "Speakers build rapport in the process of aligning conversational behaviors\nwith each other. Rapport engendered with a teachable agent while instructing\ndomain material has been shown to promote learning. Past work on lexical\nalignment in the field of education suffers from limitations in both the\nmeasures used to quantify alignment and the types of interactions in which\nalignment with agents has been studied. In this paper, we apply alignment\nmeasures based on a data-driven notion of shared expressions (possibly composed\nof multiple words) and compare alignment in one-on-one human-robot (H-R)\ninteractions with the H-R portions of collaborative human-human-robot (H-H-R)\ninteractions. We find that students in the H-R setting align with a teachable\nrobot more than in the H-H-R setting and that the relationship between lexical\nalignment and rapport is more complex than what is predicted by previous\ntheoretical and empirical work.", "published": "2022-09-23 20:10:04", "link": "http://arxiv.org/abs/2209.11842v1", "categories": ["cs.CL", "cs.HC", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Whodunit? Learning to Contrast for Authorship Attribution", "abstract": "Authorship attribution is the task of identifying the author of a given text.\nThe key is finding representations that can differentiate between authors.\nExisting approaches typically use manually designed features that capture a\ndataset's content and style, but these approaches are dataset-dependent and\nyield inconsistent performance across corpora. In this work, we propose\n\\textit{learning} author-specific representations by fine-tuning pre-trained\ngeneric language representations with a contrastive objective (Contra-X). We\nshow that Contra-X learns representations that form highly separable clusters\nfor different authors. It advances the state-of-the-art on multiple human and\nmachine authorship attribution benchmarks, enabling improvements of up to 6.8%\nover cross-entropy fine-tuning. However, we find that Contra-X improves overall\naccuracy at the cost of sacrificing performance for some authors. Resolving\nthis tension will be an important direction for future work. To the best of our\nknowledge, we are the first to integrate contrastive learning with pre-trained\nlanguage model fine-tuning for authorship attribution.", "published": "2022-09-23 23:45:08", "link": "http://arxiv.org/abs/2209.11887v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Variational Open-Domain Question Answering", "abstract": "Retrieval-augmented models have proven to be effective in natural language\nprocessing tasks, yet there remains a lack of research on their optimization\nusing variational inference. We introduce the Variational Open-Domain (VOD)\nframework for end-to-end training and evaluation of retrieval-augmented models,\nfocusing on open-domain question answering and language modelling. The VOD\nobjective, a self-normalized estimate of the R\\'enyi variational bound,\napproximates the task marginal likelihood and is evaluated under samples drawn\nfrom an auxiliary sampling distribution (cached retriever and/or approximate\nposterior). It remains tractable, even for retriever distributions defined on\nlarge corpora. We demonstrate VOD's versatility by training reader-retriever\nBERT-sized models on multiple-choice medical exam questions. On the MedMCQA\ndataset, we outperform the domain-tuned Med-PaLM by +5.3% despite using\n2.500$\\times$ fewer parameters. Our retrieval-augmented BioLinkBERT model\nscored 62.9% on the MedMCQA and 55.0% on the MedQA-USMLE. Last, we show the\neffectiveness of our learned retriever component in the context of medical\nsemantic search.", "published": "2022-09-23 10:25:59", "link": "http://arxiv.org/abs/2210.06345v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7; H.3.3; I.2.1"], "primary_category": "cs.CL"}
{"title": "MMS-MSG: A Multi-purpose Multi-Speaker Mixture Signal Generator", "abstract": "The scope of speech enhancement has changed from a monolithic view of single,\nindependent tasks, to a joint processing of complex conversational speech\nrecordings. Training and evaluation of these single tasks requires synthetic\ndata with access to intermediate signals that is as close as possible to the\nevaluation scenario. As such data often is not available, many works instead\nuse specialized databases for the training of each system component, e.g\nWSJ0-mix for source separation. We present a Multi-purpose Multi-Speaker\nMixture Signal Generator (MMS-MSG) for generating a variety of speech mixture\nsignals based on any speech corpus, ranging from classical anechoic mixtures\n(e.g., WSJ0-mix) over reverberant mixtures (e.g., SMS-WSJ) to meeting-style\ndata. Its highly modular and flexible structure allows for the simulation of\ndiverse environments and dynamic mixing, while simultaneously enabling an easy\nextension and modification to generate new scenarios and mixture types. These\nmeetings can be used for prototyping, evaluation, or training purposes. We\nprovide example evaluation data and baseline results for meetings based on the\nWSJ corpus. Further, we demonstrate the usefulness for realistic scenarios by\nusing MMS-MSG to provide training data for the LibriCSS database.", "published": "2022-09-23 09:40:01", "link": "http://arxiv.org/abs/2209.11494v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "UniKW-AT: Unified Keyword Spotting and Audio Tagging", "abstract": "Within the audio research community and the industry, keyword spotting (KWS)\nand audio tagging (AT) are seen as two distinct tasks and research fields.\nHowever, from a technical point of view, both of these tasks are identical:\nthey predict a label (keyword in KWS, sound event in AT) for some fixed-sized\ninput audio segment. This work proposes UniKW-AT: An initial approach for\njointly training both KWS and AT. UniKW-AT enhances the noise-robustness for\nKWS, while also being able to predict specific sound events and enabling\nconditional wake-ups on sound events. Our approach extends the AT pipeline with\nadditional labels describing the presence of a keyword. Experiments are\nconducted on the Google Speech Commands V1 (GSCV1) and the balanced Audioset\n(AS) datasets. The proposed MobileNetV2 model achieves an accuracy of 97.53% on\nthe GSCV1 dataset and an mAP of 33.4 on the AS evaluation set. Further, we show\nthat significant noise-robustness gains can be observed on a real-world KWS\ndataset, greatly outperforming standard KWS approaches. Our study shows that\nKWS and AT can be merged into a single framework without significant\nperformance degradation.", "published": "2022-09-23 02:39:59", "link": "http://arxiv.org/abs/2209.11377v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Kriston AI System for the VoxCeleb Speaker Recognition Challenge\n  2022", "abstract": "This technical report describes our system for track 1, 2 and 4 of the\nVoxCeleb Speaker Recognition Challenge 2022 (VoxSRC-22). By combining several\nResNet variants, our submission for track 1 attained a minDCF of 0:090 with EER\n1:401%. By further incorporating three fine-tuned pre-trained models, our\nsubmission for track 2 achieved a minDCF of 0:072 with EER 1:119%. For track 4,\nour system consisted of voice activity detection (VAD), speaker embedding\nextraction, agglomerative hierarchical clustering (AHC) followed by a\nre-clustering step based on a Bayesian hidden Markov model and overlapped\nspeech detection and handling. Our submission for track 4 achieved a\ndiarisation error rate (DER) of 4.86%. The submissions all ranked the 2nd\nplaces for the corresponding tracks.", "published": "2022-09-23 06:18:48", "link": "http://arxiv.org/abs/2209.11433v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Synthetic Voice Spoofing Detection Based On Online Hard Example Mining", "abstract": "The automatic speaker verification spoofing (ASVspoof) challenge series is\ncrucial for enhancing the spoofing consideration and the countermeasures\ngrowth. Although the recent ASVspoof 2019 validation results indicate the\nsignificant capability to identify most attacks, the model's recognition effect\nis still poor for some attacks. This paper presents the Online Hard Example\nMining (OHEM) algorithm for detecting unknown voice spoofing attacks. The OHEM\nis utilized to overcome the imbalance between simple and hard samples in the\ndataset. The presented system provides an equal error rate (EER) of 0.77% on\nthe ASVspoof 2019 Challenge logical access scenario's evaluation set.", "published": "2022-09-23 13:32:15", "link": "http://arxiv.org/abs/2209.11585v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Stereo InSE-NET: Stereo Audio Quality Predictor Transfer Learned from\n  Mono InSE-NET", "abstract": "Automatic coded audio quality predictors are typically designed for\nevaluating single channels without considering any spatial aspects. With\nInSE-NET [1], we demonstrated mimicking a state-of-the-art coded audio quality\nmetric (ViSQOL-v3 [2]) with deep neural networks (DNN) and subsequently\nimproving it - completely with programmatically generated data. In this study,\nwe take steps towards building a DNN-based coded stereo audio quality predictor\nand we propose an extension of the InSE-NET for handling stereo signals. The\ndesign considers stereo/spatial aspects by conditioning the model with left,\nright, mid, and side channels; and we name our model Stereo InSE-NET. By\ntransferring selected weights from the pre-trained mono InSE-NET and retraining\nwith both real and synthetically augmented listening tests, we demonstrate a\nsignificant improvement of 12% and 6% of Pearson and Spearman Rank correlation\ncoefficient, respectively, over the latest ViSQOL-v3 [3].", "published": "2022-09-23 15:52:33", "link": "http://arxiv.org/abs/2209.11666v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "ControlVC: Zero-Shot Voice Conversion with Time-Varying Controls on\n  Pitch and Speed", "abstract": "Recent developments in neural speech synthesis and vocoding have sparked a\nrenewed interest in voice conversion (VC). Beyond timbre transfer, achieving\ncontrollability on para-linguistic parameters such as pitch and Speed is\ncritical in deploying VC systems in many application scenarios. Existing\nstudies, however, either only provide utterance-level global control or lack\ninterpretability on the controls. In this paper, we propose ControlVC, the\nfirst neural voice conversion system that achieves time-varying controls on\npitch and speed. ControlVC uses pre-trained encoders to compute pitch and\nlinguistic embeddings from the source utterance and speaker embeddings from the\ntarget utterance. These embeddings are then concatenated and converted to\nspeech using a vocoder. It achieves speed control through TD-PSOLA\npre-processing on the source utterance, and achieves pitch control by\nmanipulating the pitch contour before feeding it to the pitch encoder.\nSystematic subjective and objective evaluations are conducted to assess the\nspeech quality and controllability. Results show that, on non-parallel and\nzero-shot conversion tasks, ControlVC significantly outperforms two other\nself-constructed baselines on speech quality, and it can successfully achieve\ntime-varying pitch and speed control.", "published": "2022-09-23 21:28:59", "link": "http://arxiv.org/abs/2209.11866v5", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An artificial neural network-based system for detecting machine failures\n  using tiny sound data: A case study", "abstract": "In an effort to advocate the research for a deep learning-based machine\nfailure detection system, we present a case study of our proposed system based\non a tiny sound dataset. Our case study investigates a variational autoencoder\n(VAE) for augmenting a small drill sound dataset from Valmet AB. A Valmet\ndataset contains 134 sounds that have been divided into two categories:\n\"Anomaly\" and \"Normal\" recorded from a drilling machine in Valmet AB, a company\nin Sundsvall, Sweden that supplies equipment and processes for the production\nof biofuels. Using deep learning models to detect failure drills on such a\nsmall sound dataset is typically unsuccessful. We employed a VAE to increase\nthe number of sounds in the tiny dataset by synthesizing new sounds from\noriginal sounds. The augmented dataset was created by combining these\nsynthesized sounds with the original sounds. We used a high-pass filter with a\npassband frequency of 1000 Hz and a low-pass filter with a passband frequency\nof 22\\kern 0.16667em000 Hz to pre-process sounds in the augmented dataset\nbefore transforming them to Mel spectrograms. The pre-trained 2D-CNN Alexnet\nwas then trained using these Mel spectrograms. When compared to using the\noriginal tiny sound dataset to train pre-trained Alexnet, using the augmented\nsound dataset enhanced the CNN model's classification results by 6.62\\%(94.12\\%\nwhen trained on the augmented dataset versus 87.5\\% when trained on the\noriginal dataset).", "published": "2022-09-23 11:13:22", "link": "http://arxiv.org/abs/2209.11527v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The SpeakIn Speaker Verification System for Far-Field Speaker\n  Verification Challenge 2022", "abstract": "This paper describes speaker verification (SV) systems submitted by the\nSpeakIn team to the Task 1 and Task 2 of the Far-Field Speaker Verification\nChallenge 2022 (FFSVC2022). SV tasks of the challenge focus on the problem of\nfully supervised far-field speaker verification (Task 1) and semi-supervised\nfar-field speaker verification (Task 2). In Task 1, we used the VoxCeleb and\nFFSVC2020 datasets as train datasets. And for Task 2, we only used the VoxCeleb\ndataset as train set. The ResNet-based and RepVGG-based architectures were\ndeveloped for this challenge. Global statistic pooling structure and MQMHA\npooling structure were used to aggregate the frame-level features across time\nto obtain utterance-level representation. We adopted AM-Softmax and AAM-Softmax\nto classify the resulting embeddings. We innovatively propose a staged transfer\nlearning method. In the pre-training stage we reserve the speaker weights, and\nthere are no positive samples to train them in this stage. Then we fine-tune\nthese weights with both positive and negative samples in the second stage.\nCompared with the traditional transfer learning strategy, this strategy can\nbetter improve the model performance. The Sub-Mean and AS-Norm backend methods\nwere used to solve the problem of domain mismatch. In the fusion stage, three\nmodels were fused in Task1 and two models were fused in Task2. On the FFSVC2022\nleaderboard, the EER of our submission is 3.0049% and the corresponding minDCF\nis 0.2938 in Task1. In Task2, EER and minDCF are 6.2060% and 0.5232\nrespectively. Our approach leads to excellent performance and ranks 1st in both\nchallenge tasks.", "published": "2022-09-23 14:51:55", "link": "http://arxiv.org/abs/2209.11625v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
