{"title": "Dialog as a Vehicle for Lifelong Learning", "abstract": "Dialog systems research has primarily been focused around two main types of\napplications - task-oriented dialog systems that learn to use clarification to\naid in understanding a goal, and open-ended dialog systems that are expected to\ncarry out unconstrained \"chit chat\" conversations. However, dialog interactions\ncan also be used to obtain various types of knowledge that can be used to\nimprove an underlying language understanding system, or other machine learning\nsystems that the dialog acts over. In this position paper, we present the\nproblem of designing dialog systems that enable lifelong learning as an\nimportant challenge problem, in particular for applications involving\nphysically situated robots. We include examples of prior work in this\ndirection, and discuss challenges that remain to be addressed.", "published": "2020-06-26 03:08:33", "link": "http://arxiv.org/abs/2006.14767v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of Text Generation: A Survey", "abstract": "The paper surveys evaluation methods of natural language generation (NLG)\nsystems that have been developed in the last few years. We group NLG evaluation\nmethods into three categories: (1) human-centric evaluation metrics, (2)\nautomatic metrics that require no training, and (3) machine-learned metrics.\nFor each category, we discuss the progress that has been made and the\nchallenges still being faced, with a focus on the evaluation of recently\nproposed NLG tasks and neural NLG models. We then present two examples for\ntask-specific NLG evaluations for automatic text summarization and long text\ngeneration, and conclude the paper by proposing future research directions.", "published": "2020-06-26 04:52:48", "link": "http://arxiv.org/abs/2006.14799v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TURL: Table Understanding through Representation Learning", "abstract": "Relational tables on the Web store a vast amount of knowledge. Owing to the\nwealth of such tables, there has been tremendous progress on a variety of tasks\nin the area of table understanding. However, existing work generally relies on\nheavily-engineered task-specific features and model architectures. In this\npaper, we present TURL, a novel framework that introduces the\npre-training/fine-tuning paradigm to relational Web tables. During\npre-training, our framework learns deep contextualized representations on\nrelational tables in an unsupervised manner. Its universal model design with\npre-trained representations can be applied to a wide range of tasks with\nminimal task-specific fine-tuning. Specifically, we propose a structure-aware\nTransformer encoder to model the row-column structure of relational tables, and\npresent a new Masked Entity Recovery (MER) objective for pre-training to\ncapture the semantics and knowledge in large-scale unlabeled data. We\nsystematically evaluate TURL with a benchmark consisting of 6 different tasks\nfor table understanding (e.g., relation extraction, cell filling). We show that\nTURL generalizes well to all tasks and substantially outperforms existing\nmethods in almost all instances.", "published": "2020-06-26 05:44:54", "link": "http://arxiv.org/abs/2006.14806v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ProVe -- Self-supervised pipeline for automated product replacement and\n  cold-starting based on neural language models", "abstract": "In retail vertical industries, businesses are dealing with human limitation\nof quickly understanding and adapting to new purchasing behaviors. Moreover,\nretail businesses need to overcome the human limitation of properly managing a\nmassive selection of products/brands/categories. These limitations lead to\ndeficiencies from both commercial (e.g. loss of sales, decrease in customer\nsatisfaction) and operational perspective (e.g. out-of-stock, over-stock). In\nthis paper, we propose a pipeline approach based on Natural Language\nUnderstanding, for recommending the most suitable replacements for products\nthat are out-of-stock. Moreover, we will propose a solution for managing\nproducts that were newly introduced in a retailer's portfolio with almost no\ntransactional history. This solution will help businesses: automatically assign\nthe new products to the right category; recommend complementary products for\ncross-sell from day 1; perform sales predictions even with almost no\ntransactional history. Finally, the vector space model resulted by applying the\npipeline presented in this paper is directly used as semantic information in\ndeep learning-based demand forecasting solutions, leading to more accurate\npredictions. The whole research and experimentation process have been done\nusing real-life private transactional data, however the source code is\navailable on https://github.com/Lummetry/ProVe", "published": "2020-06-26 14:03:18", "link": "http://arxiv.org/abs/2006.14994v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Graph Optimal Transport for Cross-Domain Alignment", "abstract": "Cross-domain alignment between two sets of entities (e.g., objects in an\nimage, words in a sentence) is fundamental to both computer vision and natural\nlanguage processing. Existing methods mainly focus on designing advanced\nattention mechanisms to simulate soft alignment, with no training signals to\nexplicitly encourage alignment. The learned attention matrices are also dense\nand lacks interpretability. We propose Graph Optimal Transport (GOT), a\nprincipled framework that germinates from recent advances in Optimal Transport\n(OT). In GOT, cross-domain alignment is formulated as a graph matching problem,\nby representing entities into a dynamically-constructed graph. Two types of OT\ndistances are considered: (i) Wasserstein distance (WD) for node (entity)\nmatching; and (ii) Gromov-Wasserstein distance (GWD) for edge (structure)\nmatching. Both WD and GWD can be incorporated into existing neural network\nmodels, effectively acting as a drop-in regularizer. The inferred transport\nplan also yields sparse and self-normalized alignment, enhancing the\ninterpretability of the learned model. Experiments show consistent\noutperformance of GOT over baselines across a wide range of tasks, including\nimage-text retrieval, visual question answering, image captioning, machine\ntranslation, and text summarization.", "published": "2020-06-26 01:14:23", "link": "http://arxiv.org/abs/2006.14744v3", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Does the Whole Exceed its Parts? The Effect of AI Explanations on\n  Complementary Team Performance", "abstract": "Many researchers motivate explainable AI with studies showing that human-AI\nteam performance on decision-making tasks improves when the AI explains its\nrecommendations. However, prior studies observed improvements from explanations\nonly when the AI, alone, outperformed both the human and the best team. Can\nexplanations help lead to complementary performance, where team accuracy is\nhigher than either the human or the AI working solo? We conduct mixed-method\nuser studies on three datasets, where an AI with accuracy comparable to humans\nhelps participants solve a task (explaining itself in some conditions). While\nwe observed complementary improvements from AI augmentation, they were not\nincreased by explanations. Rather, explanations increased the chance that\nhumans will accept the AI's recommendation, regardless of its correctness. Our\nresult poses new challenges for human-centered AI: Can we develop explanatory\napproaches that encourage appropriate trust in AI, and therefore help generate\n(or improve) complementary performance?", "published": "2020-06-26 03:34:04", "link": "http://arxiv.org/abs/2006.14779v3", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.AI"}
{"title": "What they do when in doubt: a study of inductive biases in seq2seq\n  learners", "abstract": "Sequence-to-sequence (seq2seq) learners are widely used, but we still have\nonly limited knowledge about what inductive biases shape the way they\ngeneralize. We address that by investigating how popular seq2seq learners\ngeneralize in tasks that have high ambiguity in the training data. We use SCAN\nand three new tasks to study learners' preferences for memorization,\narithmetic, hierarchical, and compositional reasoning. Further, we connect to\nSolomonoff's theory of induction and propose to use description length as a\nprincipled and sensitive measure of inductive biases.\n  In our experimental study, we find that LSTM-based learners can learn to\nperform counting, addition, and multiplication by a constant from a single\ntraining example. Furthermore, Transformer and LSTM-based learners show a bias\ntoward the hierarchical induction over the linear one, while CNN-based learners\nprefer the opposite. On the SCAN dataset, we find that CNN-based, and, to a\nlesser degree, Transformer- and LSTM-based learners have a preference for\ncompositional generalization over memorization. Finally, across all our\nexperiments, description length proved to be a sensitive measure of inductive\nbiases.", "published": "2020-06-26 12:43:10", "link": "http://arxiv.org/abs/2006.14953v2", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Pre-training via Paraphrasing", "abstract": "We introduce MARGE, a pre-trained sequence-to-sequence model learned with an\nunsupervised multi-lingual multi-document paraphrasing objective. MARGE\nprovides an alternative to the dominant masked language modeling paradigm,\nwhere we self-supervise the reconstruction of target text by retrieving a set\nof related texts (in many languages) and conditioning on them to maximize the\nlikelihood of generating the original. We show it is possible to jointly learn\nto do retrieval and reconstruction, given only a random initialization. The\nobjective noisily captures aspects of paraphrase, translation, multi-document\nsummarization, and information retrieval, allowing for strong zero-shot\nperformance on several tasks. For example, with no additional task-specific\ntraining we achieve BLEU scores of up to 35.8 for document translation. We\nfurther show that fine-tuning gives strong performance on a range of\ndiscriminative and generative tasks in many languages, making MARGE the most\ngenerally applicable pre-training method to date.", "published": "2020-06-26 14:43:43", "link": "http://arxiv.org/abs/2006.15020v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "BERTology Meets Biology: Interpreting Attention in Protein Language\n  Models", "abstract": "Transformer architectures have proven to learn useful representations for\nprotein classification and generation tasks. However, these representations\npresent challenges in interpretability. In this work, we demonstrate a set of\nmethods for analyzing protein Transformer models through the lens of attention.\nWe show that attention: (1) captures the folding structure of proteins,\nconnecting amino acids that are far apart in the underlying sequence, but\nspatially close in the three-dimensional structure, (2) targets binding sites,\na key functional component of proteins, and (3) focuses on progressively more\ncomplex biophysical properties with increasing layer depth. We find this\nbehavior to be consistent across three Transformer architectures (BERT, ALBERT,\nXLNet) and two distinct protein datasets. We also present a three-dimensional\nvisualization of the interaction between attention and protein structure. Code\nfor visualization and analysis is available at\nhttps://github.com/salesforce/provis.", "published": "2020-06-26 21:50:17", "link": "http://arxiv.org/abs/2006.15222v3", "categories": ["cs.CL", "cs.LG", "q-bio.BM", "I.2"], "primary_category": "cs.CL"}
