{"title": "Comprehensive Study: How the Context Information of Different\n  Granularity Affects Dialogue State Tracking?", "abstract": "Dialogue state tracking (DST) plays a key role in task-oriented dialogue\nsystems to monitor the user's goal. In general, there are two strategies to\ntrack a dialogue state: predicting it from scratch and updating it from\nprevious state. The scratch-based strategy obtains each slot value by inquiring\nall the dialogue history, and the previous-based strategy relies on the current\nturn dialogue to update the previous dialogue state. However, it is hard for\nthe scratch-based strategy to correctly track short-dependency dialogue state\nbecause of noise; meanwhile, the previous-based strategy is not very useful for\nlong-dependency dialogue state tracking. Obviously, it plays different roles\nfor the context information of different granularity to track different kinds\nof dialogue states. Thus, in this paper, we will study and discuss how the\ncontext information of different granularity affects dialogue state tracking.\nFirst, we explore how greatly different granularities affect dialogue state\ntracking. Then, we further discuss how to combine multiple granularities for\ndialogue state tracking. Finally, we apply the findings about context\ngranularity to few-shot learning scenario. Besides, we have publicly released\nall codes.", "published": "2021-05-08 03:18:13", "link": "http://arxiv.org/abs/2105.03571v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Cross-Lingual Reading Comprehension with Self-Training", "abstract": "Substantial improvements have been made in machine reading comprehension,\nwhere the machine answers questions based on a given context. Current\nstate-of-the-art models even surpass human performance on several benchmarks.\nHowever, their abilities in the cross-lingual scenario are still to be\nexplored. Previous works have revealed the abilities of pre-trained\nmultilingual models for zero-shot cross-lingual reading comprehension. In this\npaper, we further utilized unlabeled data to improve the performance. The model\nis first supervised-trained on source language corpus, and then self-trained\nwith unlabeled target language data. The experiment results showed improvements\nfor all languages, and we also analyzed how self-training benefits\ncross-lingual reading comprehension in qualitative aspects.", "published": "2021-05-08 08:04:30", "link": "http://arxiv.org/abs/2105.03627v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Logic-Driven Context Extension and Data Augmentation for Logical\n  Reasoning of Text", "abstract": "Logical reasoning of text requires understanding critical logical information\nin the text and performing inference over them. Large-scale pre-trained models\nfor logical reasoning mainly focus on word-level semantics of text while\nstruggling to capture symbolic logic. In this paper, we propose to understand\nlogical symbols and expressions in the text to arrive at the answer. Based on\nsuch logical information, we not only put forward a context extension framework\nbut also propose a data augmentation algorithm. The former extends the context\nto cover implicit logical expressions following logical equivalence laws. The\nlatter augments literally similar but logically different instances to better\ncapture logical information, especially logical negative and conditional\nrelationships. We conduct experiments on ReClor dataset. The results show that\nour method achieves the state-of-the-art performance, and both logic-driven\ncontext extension framework and data augmentation algorithm can help improve\nthe accuracy. And our multi-model ensemble system is the first to surpass human\nperformance on both EASY set and HARD set of ReClor.", "published": "2021-05-08 10:09:36", "link": "http://arxiv.org/abs/2105.03659v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "D2S: Document-to-Slide Generation Via Query-Based Text Summarization", "abstract": "Presentations are critical for communication in all areas of our lives, yet\nthe creation of slide decks is often tedious and time-consuming. There has been\nlimited research aiming to automate the document-to-slides generation process\nand all face a critical challenge: no publicly available dataset for training\nand benchmarking. In this work, we first contribute a new dataset, SciDuet,\nconsisting of pairs of papers and their corresponding slides decks from recent\nyears' NLP and ML conferences (e.g., ACL). Secondly, we present D2S, a novel\nsystem that tackles the document-to-slides task with a two-step approach: 1)\nUse slide titles to retrieve relevant and engaging text, figures, and tables;\n2) Summarize the retrieved context into bullet points with long-form question\nanswering. Our evaluation suggests that long-form QA outperforms\nstate-of-the-art summarization baselines on both automated ROUGE metrics and\nqualitative human evaluation.", "published": "2021-05-08 10:29:41", "link": "http://arxiv.org/abs/2105.03664v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Falling Through the Gaps: Neural Architectures as Models of\n  Morphological Rule Learning", "abstract": "Recent advances in neural architectures have revived the problem of\nmorphological rule learning. We evaluate the Transformer as a model of\nmorphological rule learning and compare it with Recurrent Neural Networks (RNN)\non English, German, and Russian. We bring to the fore a hitherto overlooked\nproblem, the morphological gaps, where the expected inflection of a word is\nmissing. For example, 63 Russian verbs lack a first-person-singular present\nform such that one cannot comfortably say \"*o\\v{s}\\v{c}u\\v{s}\\v{c}u\" (\"I\nfeel\"). Even English has gaps, such as the past participle of \"stride\": the\nfunction of morphological inflection can be partial. Both neural architectures\nproduce inflections that ought to be missing. Analyses reveal that Transformers\nrecapitulate the statistical distribution of inflections in the training data,\nsimilar to RNNs. Models' success on English and German is driven by the fact\nthat rules in these languages can be identified with the majority forms, which\nis not universal.", "published": "2021-05-08 14:48:29", "link": "http://arxiv.org/abs/2105.03710v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Certified Robustness to Text Adversarial Attacks by Randomized [MASK]", "abstract": "Recently, few certified defense methods have been developed to provably\nguarantee the robustness of a text classifier to adversarial synonym\nsubstitutions. However, all existing certified defense methods assume that the\ndefenders are informed of how the adversaries generate synonyms, which is not a\nrealistic scenario. In this paper, we propose a certifiably robust defense\nmethod by randomly masking a certain proportion of the words in an input text,\nin which the above unrealistic assumption is no longer necessary. The proposed\nmethod can defend against not only word substitution-based attacks, but also\ncharacter-level perturbations. We can certify the classifications of over 50%\ntexts to be robust to any perturbation of 5 words on AGNEWS, and 2 words on\nSST2 dataset. The experimental results show that our randomized smoothing\nmethod significantly outperforms recently proposed defense methods across\nmultiple datasets.", "published": "2021-05-08 16:59:10", "link": "http://arxiv.org/abs/2105.03743v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Transformers with Gradient Boosted Decision Trees for NLI\n  Fine-Tuning", "abstract": "Transfer learning has become the dominant paradigm for many natural language\nprocessing tasks. In addition to models being pretrained on large datasets,\nthey can be further trained on intermediate (supervised) tasks that are similar\nto the target task. For small Natural Language Inference (NLI) datasets,\nlanguage modelling is typically followed by pretraining on a large (labelled)\nNLI dataset before fine-tuning with each NLI subtask. In this work, we explore\nGradient Boosted Decision Trees (GBDTs) as an alternative to the commonly used\nMulti-Layer Perceptron (MLP) classification head. GBDTs have desirable\nproperties such as good performance on dense, numerical features and are\neffective where the ratio of the number of samples w.r.t the number of features\nis low. We then introduce FreeGBDT, a method of fitting a GBDT head on the\nfeatures computed during fine-tuning to increase performance without additional\ncomputation by the neural network. We demonstrate the effectiveness of our\nmethod on several NLI datasets using a strong baseline model (RoBERTa-large\nwith MNLI pretraining). The FreeGBDT shows a consistent improvement over the\nMLP classification head.", "published": "2021-05-08 22:31:51", "link": "http://arxiv.org/abs/2105.03791v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Long-Span Summarization via Local Attention and Content Selection", "abstract": "Transformer-based models have achieved state-of-the-art results in a wide\nrange of natural language processing (NLP) tasks including document\nsummarization. Typically these systems are trained by fine-tuning a large\npre-trained model to the target task. One issue with these transformer-based\nmodels is that they do not scale well in terms of memory and compute\nrequirements as the input length grows. Thus, for long document summarization,\nit can be challenging to train or fine-tune these models. In this work, we\nexploit large pre-trained transformer-based models and address long-span\ndependencies in abstractive summarization using two methods: local\nself-attention; and explicit content selection. These approaches are compared\non a range of network configurations. Experiments are carried out on standard\nlong-span summarization tasks, including Spotify Podcast, arXiv, and PubMed\ndatasets. We demonstrate that by combining these methods, we can achieve\nstate-of-the-art results on all three tasks in the ROUGE scores. Moreover,\nwithout a large-scale GPU card, our approach can achieve comparable or better\nresults than existing approaches.", "published": "2021-05-08 23:53:03", "link": "http://arxiv.org/abs/2105.03801v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Document Representations by Generating Pseudo Query Embeddings\n  for Dense Retrieval", "abstract": "Recently, the retrieval models based on dense representations have been\ngradually applied in the first stage of the document retrieval tasks, showing\nbetter performance than traditional sparse vector space models. To obtain high\nefficiency, the basic structure of these models is Bi-encoder in most cases.\nHowever, this simple structure may cause serious information loss during the\nencoding of documents since the queries are agnostic. To address this problem,\nwe design a method to mimic the queries on each of the documents by an\niterative clustering process and represent the documents by multiple pseudo\nqueries (i.e., the cluster centroids). To boost the retrieval process using\napproximate nearest neighbor search library, we also optimize the matching\nfunction with a two-step score calculation procedure. Experimental results on\nseveral popular ranking and QA datasets show that our model can achieve\nstate-of-the-art results.", "published": "2021-05-08 05:28:24", "link": "http://arxiv.org/abs/2105.03599v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "On Guaranteed Optimal Robust Explanations for NLP Models", "abstract": "We build on abduction-based explanations for ma-chine learning and develop a\nmethod for computing local explanations for neural network models in natural\nlanguage processing (NLP). Our explanations comprise a subset of the words of\nthe in-put text that satisfies two key features: optimality w.r.t. a\nuser-defined cost function, such as the length of explanation, and robustness,\nin that they ensure prediction invariance for any bounded perturbation in the\nembedding space of the left out words. We present two solution algorithms,\nrespectively based on implicit hitting sets and maximum universal subsets,\nintroducing a number of algorithmic improvements to speed up convergence of\nhard instances. We show how our method can be con-figured with different\nperturbation sets in the em-bedded space and used to detect bias in predictions\nby enforcing include/exclude constraints on biased terms, as well as to enhance\nexisting heuristic-based NLP explanation frameworks such as Anchors. We\nevaluate our framework on three widely used sentiment analysis tasks and texts\nof up to100words from SST, Twitter and IMDB datasets,demonstrating the\neffectiveness of the derived explanations.", "published": "2021-05-08 08:44:48", "link": "http://arxiv.org/abs/2105.03640v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Diversifying Neural Text Generation with Part-of-Speech Guided Softmax\n  and Sampling", "abstract": "Neural text generation models are likely to suffer from the low-diversity\nproblem. Various decoding strategies and training-based methods have been\nproposed to promote diversity only by exploiting contextual features, but\nrarely do they consider incorporating syntactic structure clues. In this work,\nwe propose using linguistic annotation, i.e., part-of-speech (POS), to guide\nthe text generation. In detail, we introduce POS Guided Softmax to explicitly\nmodel two posterior probabilities: (i) next-POS, and (ii) next-token from the\nvocabulary of the target POS. A POS Guided Sampling strategy is further\nproposed to address the low-diversity problem by enriching the diversity of\nPOS. Extensive experiments and human evaluations show that, compared with\nexisting state-of-the-art methods, our POS Guided Softmax and Sampling (POSG)\ncan generate more diverse text while maintaining comparable quality.", "published": "2021-05-08 08:53:16", "link": "http://arxiv.org/abs/2105.03641v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Named Entity Recognition by External Context Retrieving and\n  Cooperative Learning", "abstract": "Recent advances in Named Entity Recognition (NER) show that document-level\ncontexts can significantly improve model performance. In many application\nscenarios, however, such contexts are not available. In this paper, we propose\nto find external contexts of a sentence by retrieving and selecting a set of\nsemantically relevant texts through a search engine, with the original sentence\nas the query. We find empirically that the contextual representations computed\non the retrieval-based input view, constructed through the concatenation of a\nsentence and its external contexts, can achieve significantly improved\nperformance compared to the original input view based only on the sentence.\nFurthermore, we can improve the model performance of both input views by\nCooperative Learning, a training method that encourages the two input views to\nproduce similar contextual representations or output label distributions.\nExperiments show that our approach can achieve new state-of-the-art performance\non 8 NER data sets across 5 domains.", "published": "2021-05-08 09:45:21", "link": "http://arxiv.org/abs/2105.03654v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Continuous representations of intents for dialogue systems", "abstract": "Intent modelling has become an important part of modern dialogue systems.\nWith the rapid expansion of practical dialogue systems and virtual assistants,\nsuch as Amazon Alexa, Apple Siri, and Google Assistant, the interest has only\nincreased. However, up until recently the focus has been on detecting a fixed,\ndiscrete, number of seen intents. Recent years have seen some work done on\nunseen intent detection in the context of zero-shot learning. This paper\ncontinues the prior work by proposing a novel model where intents are\ncontinuous points placed in a specialist Intent Space that yields several\nadvantages. First, the continuous representation enables to investigate\nrelationships between the seen intents. Second, it allows any unseen intent to\nbe reliably represented given limited quantities of data. Finally, this paper\nwill show how the proposed model can be augmented with unseen intents without\nretraining any of the seen ones. Experiments show that the model can reliably\nadd unseen intents with a high accuracy while retaining a high performance on\nthe seen intents.", "published": "2021-05-08 15:08:20", "link": "http://arxiv.org/abs/2105.03716v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "e-ViL: A Dataset and Benchmark for Natural Language Explanations in\n  Vision-Language Tasks", "abstract": "Recently, there has been an increasing number of efforts to introduce models\ncapable of generating natural language explanations (NLEs) for their\npredictions on vision-language (VL) tasks. Such models are appealing, because\nthey can provide human-friendly and comprehensive explanations. However, there\nis a lack of comparison between existing methods, which is due to a lack of\nre-usable evaluation frameworks and a scarcity of datasets. In this work, we\nintroduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable\nvision-language tasks that establishes a unified evaluation framework and\nprovides the first comprehensive comparison of existing approaches that\ngenerate NLEs for VL tasks. It spans four models and three datasets and both\nautomatic metrics and human evaluation are used to assess model-generated\nexplanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs\n(over 430k instances). We also propose a new model that combines UNITER, which\nlearns joint embeddings of images and text, and GPT-2, a pre-trained language\nmodel that is well-suited for text generation. It surpasses the previous state\nof the art by a large margin across all datasets. Code and data are available\nhere: https://github.com/maximek3/e-ViL.", "published": "2021-05-08 18:46:33", "link": "http://arxiv.org/abs/2105.03761v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Robustness of end-to-end Automatic Speech Recognition Models -- A Case\n  Study using Mozilla DeepSpeech", "abstract": "When evaluating the performance of automatic speech recognition models,\nusually word error rate within a certain dataset is used. Special care must be\ntaken in understanding the dataset in order to report realistic performance\nnumbers. We argue that many performance numbers reported probably underestimate\nthe expected error rate. We conduct experiments controlling for selection bias,\ngender as well as overlap (between training and test data) in content, voices,\nand recording conditions. We find that content overlap has the biggest impact,\nbut other factors like gender also play a role.", "published": "2021-05-08 16:46:44", "link": "http://arxiv.org/abs/2105.09742v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "NLP-IIS@UT at SemEval-2021 Task 4: Machine Reading Comprehension using\n  the Long Document Transformer", "abstract": "This paper presents a technical report of our submission to the 4th task of\nSemEval-2021, titled: Reading Comprehension of Abstract Meaning. In this task,\nwe want to predict the correct answer based on a question given a context.\nUsually, contexts are very lengthy and require a large receptive field from the\nmodel. Thus, common contextualized language models like BERT miss fine\nrepresentation and performance due to the limited capacity of the input tokens.\nTo tackle this problem, we used the Longformer model to better process the\nsequences. Furthermore, we utilized the method proposed in the Longformer\nbenchmark on Wikihop dataset which improved the accuracy on our task data from\n23.01% and 22.95% achieved by the baselines for subtask 1 and 2, respectively,\nto 70.30% and 64.38%.", "published": "2021-05-08 20:48:32", "link": "http://arxiv.org/abs/2105.03775v1", "categories": ["cs.CL", "cs.IR", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.CL"}
{"title": "Domestic activities clustering from audio recordings using convolutional\n  capsule autoencoder network", "abstract": "Recent efforts have been made on domestic activities classification from\naudio recordings, especially the works submitted to the challenge of DCASE\n(Detection and Classification of Acoustic Scenes and Events) since 2018. In\ncontrast, few studies were done on domestic activities clustering, which is a\nnewly emerging problem. Domestic activities clustering from audio recordings\naims at merging audio clips which belong to the same class of domestic activity\ninto a single cluster. Domestic activities clustering is an effective way for\nunsupervised estimation of daily activities performed in home environment. In\nthis study, we propose a method for domestic activities clustering using a\nconvolutional capsule autoencoder network (CCAN). In the method, the deep\nembeddings are learned by the autoencoder in the CCAN, while the deep\nembeddings which belong to the same class of domestic activities are merged\ninto a single cluster by a clustering layer in the CCAN. Evaluated on a public\ndataset adopted in DCASE-2018 Task 5, the results show that the proposed method\noutperforms state-of-the-art methods in terms of the metrics of clustering\naccuracy and normalized mutual information.", "published": "2021-05-08 03:49:55", "link": "http://arxiv.org/abs/2105.03583v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Latency-Controlled Neural Architecture Search for Streaming Speech\n  Recognition", "abstract": "Neural architecture search (NAS) has attracted much attention and has been\nexplored for automatic speech recognition (ASR). In this work, we focus on\nstreaming ASR scenarios and propose the latency-controlled NAS for acoustic\nmodeling. First, based on the vanilla neural architecture, normal cells are\naltered to causal cells to control the total latency of the architecture.\nSecond, a revised operation space with a smaller receptive field is proposed to\ngenerate the final architecture with low latency. Extensive experiments show\nthat: 1) Based on the proposed neural architecture, the neural networks with a\nmedium latency of 550ms (millisecond) and a low latency of 190ms can be learned\nin the vanilla and revised operation space respectively. 2) For the low latency\nsetting, the evaluation network can achieve more than 19\\% (average on the four\ntest sets) relative improvements compared with the hybrid CLDNN baseline, on a\n10k-hour large-scale dataset.", "published": "2021-05-08 09:01:15", "link": "http://arxiv.org/abs/2105.03643v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Zero-Shot Personalized Speech Enhancement through Speaker-Informed Model\n  Selection", "abstract": "This paper presents a novel zero-shot learning approach towards personalized\nspeech enhancement through the use of a sparsely active ensemble model.\nOptimizing speech denoising systems towards a particular test-time speaker can\nimprove performance and reduce run-time complexity. However, test-time model\nadaptation may be challenging if collecting data from the test-time speaker is\nnot possible. To this end, we propose using an ensemble model wherein each\nspecialist module denoises noisy utterances from a distinct partition of\ntraining set speakers. The gating module inexpensively estimates test-time\nspeaker characteristics in the form of an embedding vector and selects the most\nappropriate specialist module for denoising the test signal. Grouping the\ntraining set speakers into non-overlapping semantically similar groups is\nnon-trivial and ill-defined. To do this, we first train a Siamese network using\nnoisy speech pairs to maximize or minimize the similarity of its output vectors\ndepending on whether the utterances derive from the same speaker or not. Next,\nwe perform k-means clustering on the latent space formed by the averaged\nembedding vectors per training set speaker. In this way, we designate speaker\ngroups and train specialist modules optimized around partitions of the complete\ntraining set. Our experiments show that ensemble models made up of low-capacity\nspecialists can outperform high-capacity generalist models with greater\nefficiency and improved adaptation towards unseen test-time speakers.", "published": "2021-05-08 00:15:57", "link": "http://arxiv.org/abs/2105.03542v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Test-Time Adaptation Toward Personalized Speech Enhancement: Zero-Shot\n  Learning with Knowledge Distillation", "abstract": "In realistic speech enhancement settings for end-user devices, we often\nencounter only a few speakers and noise types that tend to reoccur in the\nspecific acoustic environment. We propose a novel personalized speech\nenhancement method to adapt a compact denoising model to the test-time\nspecificity. Our goal in this test-time adaptation is to utilize no clean\nspeech target of the test speaker, thus fulfilling the requirement for\nzero-shot learning. To complement the lack of clean utterance, we employ the\nknowledge distillation framework. Instead of the missing clean utterance\ntarget, we distill the more advanced denoising results from an overly large\nteacher model, and use it as the pseudo target to train the small student\nmodel. This zero-shot learning procedure circumvents the process of collecting\nusers' clean speech, a process that users are reluctant to comply due to\nprivacy concerns and technical difficulty of recording clean voice. Experiments\non various test-time conditions show that the proposed personalization method\nachieves significant performance gains compared to larger baseline networks\ntrained from a large speaker- and noise-agnostic datasets. In addition, since\nthe compact personalized models can outperform larger general-purpose models,\nwe claim that the proposed method performs model compression with no loss of\ndenoising performance.", "published": "2021-05-08 00:42:03", "link": "http://arxiv.org/abs/2105.03544v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
