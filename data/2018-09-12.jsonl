{"title": "Multimodal neural pronunciation modeling for spoken languages with\n  logographic origin", "abstract": "Graphemes of most languages encode pronunciation, though some are more\nexplicit than others. Languages like Spanish have a straightforward mapping\nbetween its graphemes and phonemes, while this mapping is more convoluted for\nlanguages like English. Spoken languages such as Cantonese present even more\nchallenges in pronunciation modeling: (1) they do not have a standard written\nform, (2) the closest graphemic origins are logographic Han characters, of\nwhich only a subset of these logographic characters implicitly encodes\npronunciation. In this work, we propose a multimodal approach to predict the\npronunciation of Cantonese logographic characters, using neural networks with a\ngeometric representation of logographs and pronunciation of cognates in\nhistorically related languages. The proposed framework improves performance by\n18.1% and 25.0% respective to unimodal and multimodal baselines.", "published": "2018-09-12 00:09:20", "link": "http://arxiv.org/abs/1809.04203v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Based Machine Reading Comprehension", "abstract": "Machine reading comprehension (MRC) requires reasoning about both the\nknowledge involved in a document and knowledge about the world. However,\nexisting datasets are typically dominated by questions that can be well solved\nby context matching, which fail to test this capability. To encourage the\nprogress on knowledge-based reasoning in MRC, we present knowledge-based MRC in\nthis paper, and build a new dataset consisting of 40,047 question-answer pairs.\nThe annotation of this dataset is designed so that successfully answering the\nquestions requires understanding and the knowledge involved in a document. We\nimplement a framework consisting of both a question answering model and a\nquestion generation model, both of which take the knowledge extracted from the\ndocument as well as relevant facts from an external knowledge base such as\nFreebase/ProBase/Reverb/NELL. Results show that incorporating side information\nfrom external KB improves the accuracy of the baseline question answer system.\nWe compare it with a standard MRC model BiDAF, and also provide the difficulty\nof the dataset and lay out remaining challenges.", "published": "2018-09-12 06:21:32", "link": "http://arxiv.org/abs/1809.04267v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-Aware Conversational Semantic Parsing Over Web Tables", "abstract": "Conversational semantic parsing over tables requires knowledge acquiring and\nreasoning abilities, which have not been well explored by current\nstate-of-the-art approaches. Motivated by this fact, we propose a\nknowledge-aware semantic parser to improve parsing performance by integrating\nvarious types of knowledge. In this paper, we consider three types of\nknowledge, including grammar knowledge, expert knowledge, and external resource\nknowledge. First, grammar knowledge empowers the model to effectively replicate\npreviously generated logical form, which effectively handles the co-reference\nand ellipsis phenomena in conversation Second, based on expert knowledge, we\npropose a decomposable model, which is more controllable compared with\ntraditional end-to-end models that put all the burdens of learning on\ntrial-and-error in an end-to-end way. Third, external resource knowledge, i.e.,\nprovided by a pre-trained language model or an entity typing model, is used to\nimprove the representation of question and table for a better semantic\nunderstanding. We conduct experiments on the SequentialQA dataset. Results show\nthat our knowledge-aware model outperforms the state-of-the-art approaches.\nIncremental experimental results also prove the usefulness of various\nknowledge. Further analysis shows that our approach has the ability to derive\nthe meaning representation of a context-dependent utterance by leveraging\npreviously generated outcomes.", "published": "2018-09-12 06:37:51", "link": "http://arxiv.org/abs/1809.04271v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieval-Enhanced Adversarial Training for Neural Response Generation", "abstract": "Dialogue systems are usually built on either generation-based or\nretrieval-based approaches, yet they do not benefit from the advantages of\ndifferent models. In this paper, we propose a Retrieval-Enhanced Adversarial\nTraining (REAT) method for neural response generation. Distinct from existing\napproaches, the REAT method leverages an encoder-decoder framework in terms of\nan adversarial training paradigm, while taking advantage of N-best response\ncandidates from a retrieval-based system to construct the discriminator. An\nempirical study on a large scale public available benchmark dataset shows that\nthe REAT method significantly outperforms the vanilla Seq2Seq model as well as\nthe conventional adversarial training approach.", "published": "2018-09-12 06:47:21", "link": "http://arxiv.org/abs/1809.04276v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hate Speech Dataset from a White Supremacy Forum", "abstract": "Hate speech is commonly defined as any communication that disparages a target\ngroup of people based on some characteristic such as race, colour, ethnicity,\ngender, sexual orientation, nationality, religion, or other characteristic. Due\nto the massive rise of user-generated web content on social media, the amount\nof hate speech is also steadily increasing. Over the past years, interest in\nonline hate speech detection and, particularly, the automation of this task has\ncontinuously grown, along with the societal impact of the phenomenon. This\npaper describes a hate speech dataset composed of thousands of sentences\nmanually labelled as containing hate speech or not. The sentences have been\nextracted from Stormfront, a white supremacist forum. A custom annotation tool\nhas been developed to carry out the manual labelling task which, among other\nthings, allows the annotators to choose whether to read the context of a\nsentence before labelling it. The paper also provides a thoughtful qualitative\nand quantitative study of the resulting dataset and several baseline\nexperiments with different classification models. The dataset is publicly\navailable.", "published": "2018-09-12 13:51:02", "link": "http://arxiv.org/abs/1809.04444v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emo2Vec: Learning Generalized Emotion Representation by Multi-task\n  Training", "abstract": "In this paper, we propose Emo2Vec which encodes emotional semantics into\nvectors. We train Emo2Vec by multi-task learning six different emotion-related\ntasks, including emotion/sentiment analysis, sarcasm classification, stress\ndetection, abusive language classification, insult detection, and personality\nrecognition. Our evaluation of Emo2Vec shows that it outperforms existing\naffect-related representations, such as Sentiment-Specific Word Embedding and\nDeepMoji embeddings with much smaller training corpora. When concatenated with\nGloVe, Emo2Vec achieves competitive performances to state-of-the-art results on\nseveral tasks using a simple logistic regression classifier.", "published": "2018-09-12 15:08:00", "link": "http://arxiv.org/abs/1809.04505v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jump to better conclusions: SCAN both left and right", "abstract": "Lake and Baroni (2018) recently introduced the SCAN data set, which consists\nof simple commands paired with action sequences and is intended to test the\nstrong generalization abilities of recurrent sequence-to-sequence models. Their\ninitial experiments suggested that such models may fail because they lack the\nability to extract systematic rules. Here, we take a closer look at SCAN and\nshow that it does not always capture the kind of generalization that it was\ndesigned for. To mitigate this we propose a complementary dataset, which\nrequires mapping actions back to the original commands, called NACS. We show\nthat models that do well on SCAN do not necessarily do well on NACS, and that\nNACS exhibits properties more closely aligned with realistic use-cases for\nsequence-to-sequence models.", "published": "2018-09-12 19:18:16", "link": "http://arxiv.org/abs/1809.04640v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic WordRank: Generating Finer Single-Document Summarizations", "abstract": "We present Semantic WordRank (SWR), an unsupervised method for generating an\nextractive summary of a single document. Built on a weighted word graph with\nsemantic and co-occurrence edges, SWR scores sentences using an\narticle-structure-biased PageRank algorithm with a Softplus function\nadjustment, and promotes topic diversity using spectral subtopic clustering\nunder the Word-Movers-Distance metric. We evaluate SWR on the DUC-02 and\nSummBank datasets and show that SWR produces better summaries than the\nstate-of-the-art algorithms over DUC-02 under common ROUGE measures. We then\nshow that, under the same measures over SummBank, SWR outperforms each of the\nthree human annotators (aka. judges) and compares favorably with the combined\nperformance of all judges.", "published": "2018-09-12 19:53:56", "link": "http://arxiv.org/abs/1809.04649v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Summarize Radiology Findings", "abstract": "The Impression section of a radiology report summarizes crucial radiology\nfindings in natural language and plays a central role in communicating these\nfindings to physicians. However, the process of generating impressions by\nsummarizing findings is time-consuming for radiologists and prone to errors. We\npropose to automate the generation of radiology impressions with neural\nsequence-to-sequence learning. We further propose a customized neural model for\nthis task which learns to encode the study background information and use this\ninformation to guide the decoding process. On a large dataset of radiology\nreports collected from actual hospital studies, our model outperforms existing\nnon-neural and neural baselines under the ROUGE metrics. In a blind experiment,\na board-certified radiologist indicated that 67% of sampled system summaries\nare at least as good as the corresponding human-written summaries, suggesting\nsignificant clinical validity. To our knowledge our work represents the first\nattempt in this direction.", "published": "2018-09-12 22:41:47", "link": "http://arxiv.org/abs/1809.04698v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalizing Word Embeddings using Bag of Subwords", "abstract": "We approach the problem of generalizing pre-trained word embeddings beyond\nfixed-size vocabularies without using additional contextual information. We\npropose a subword-level word vector generation model that views words as bags\nof character $n$-grams. The model is simple, fast to train and provides good\nvectors for rare or unseen words. Experiments show that our model achieves\nstate-of-the-art performances in English word similarity task and in joint\nprediction of part-of-speech tag and morphosyntactic attributes in 23\nlanguages, suggesting our model's ability in capturing the relationship between\nwords' textual representations and their embeddings.", "published": "2018-09-12 05:15:32", "link": "http://arxiv.org/abs/1809.04259v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Incorporating Syntactic and Semantic Information in Word Embeddings\n  using Graph Convolutional Networks", "abstract": "Word embeddings have been widely adopted across several NLP applications.\nMost existing word embedding methods utilize sequential context of a word to\nlearn its embedding. While there have been some attempts at utilizing syntactic\ncontext of a word, such methods result in an explosion of the vocabulary size.\nIn this paper, we overcome this problem by proposing SynGCN, a flexible Graph\nConvolution based method for learning word embeddings. SynGCN utilizes the\ndependency context of a word without increasing the vocabulary size. Word\nembeddings learned by SynGCN outperform existing methods on various intrinsic\nand extrinsic tasks and provide an advantage when used with ELMo. We also\npropose SemGCN, an effective framework for incorporating diverse semantic\nknowledge for further enhancing learned word representations. We make the\nsource code of both models available to encourage reproducible research.", "published": "2018-09-12 07:31:06", "link": "http://arxiv.org/abs/1809.04283v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Chinese Poetry Generation with a Working Memory Model", "abstract": "As an exquisite and concise literary form, poetry is a gem of human culture.\nAutomatic poetry generation is an essential step towards computer creativity.\nIn recent years, several neural models have been designed for this task.\nHowever, among lines of a whole poem, the coherence in meaning and topics still\nremains a big challenge. In this paper, inspired by the theoretical concept in\ncognitive psychology, we propose a novel Working Memory model for poetry\ngeneration. Different from previous methods, our model explicitly maintains\ntopics and informative limited history in a neural memory. During the\ngeneration process, our model reads the most relevant parts from memory slots\nto generate the current line. After each line is generated, it writes the most\nsalient parts of the previous line into memory slots. By dynamic manipulation\nof the memory, our model keeps a coherent information flow and learns to\nexpress each topic flexibly and naturally. We experiment on three different\ngenres of Chinese poetry: quatrain, iambic and chinoiserie lyric. Both\nautomatic and human evaluation results show that our model outperforms current\nstate-of-the-art methods.", "published": "2018-09-12 08:31:20", "link": "http://arxiv.org/abs/1809.04306v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Chinese Poetry Generation with a Salient-Clue Mechanism", "abstract": "As a precious part of the human cultural heritage, Chinese poetry has\ninfluenced people for generations. Automatic poetry composition is a challenge\nfor AI. In recent years, significant progress has been made in this area\nbenefiting from the development of neural networks. However, the coherence in\nmeaning, theme or even artistic conception for a generated poem as a whole\nstill remains a big problem. In this paper, we propose a novel Salient-Clue\nmechanism for Chinese poetry generation. Different from previous work which\ntried to exploit all the context information, our model selects the most\nsalient characters automatically from each so-far generated line to gradually\nform a salient clue, which is utilized to guide successive poem generation\nprocess so as to eliminate interruptions and improve coherence. Besides, our\nmodel can be flexibly extended to control the generated poem in different\naspects, for example, poetry style, which further enhances the coherence.\nExperimental results show that our model is very effective, outperforming three\nstrong baselines.", "published": "2018-09-12 08:50:30", "link": "http://arxiv.org/abs/1809.04313v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Neural Melody Composition from Lyrics", "abstract": "In this paper, we study a novel task that learns to compose music from\nnatural language. Given the lyrics as input, we propose a melody composition\nmodel that generates lyrics-conditional melody as well as the exact alignment\nbetween the generated melody and the given lyrics simultaneously. More\nspecifically, we develop the melody composition model based on the\nsequence-to-sequence framework. It consists of two neural encoders to encode\nthe current lyrics and the context melody respectively, and a hierarchical\ndecoder to jointly produce musical notes and the corresponding alignment.\nExperimental results on lyrics-melody pairs of 18,451 pop songs demonstrate the\neffectiveness of our proposed methods. In addition, we apply a singing voice\nsynthesizer software to synthesize the \"singing\" of the lyrics and melodies for\nhuman evaluation. Results indicate that our generated melodies are more\nmelodious and tuneful compared with the baseline method.", "published": "2018-09-12 09:03:20", "link": "http://arxiv.org/abs/1809.04318v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Closed-Book Training to Improve Summarization Encoder Memory", "abstract": "A good neural sequence-to-sequence summarization model should have a strong\nencoder that can distill and memorize the important information from long input\ntexts so that the decoder can generate salient summaries based on the encoder's\nmemory. In this paper, we aim to improve the memorization capabilities of the\nencoder of a pointer-generator model by adding an additional 'closed-book'\ndecoder without attention and pointer mechanisms. Such a decoder forces the\nencoder to be more selective in the information encoded in its memory state\nbecause the decoder can't rely on the extra information provided by the\nattention and possibly copy modules, and hence improves the entire model. On\nthe CNN/Daily Mail dataset, our 2-decoder model outperforms the baseline\nsignificantly in terms of ROUGE and METEOR metrics, for both cross-entropy and\nreinforced setups (and on human evaluation). Moreover, our model also achieves\nhigher scores in a test-only DUC-2002 generalizability setup. We further\npresent a memory ability test, two saliency metrics, as well as several\nsanity-check ablations (based on fixed-encoder, gradient-flow cut, and model\ncapacity) to prove that the encoder of our 2-decoder model does in fact learn\nstronger memory representations than the baseline encoder.", "published": "2018-09-12 17:50:07", "link": "http://arxiv.org/abs/1809.04585v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Cross-lingual Classification Using Multilingual Neural Machine\n  Translation", "abstract": "Transferring representations from large supervised tasks to downstream tasks\nhas shown promising results in AI fields such as Computer Vision and Natural\nLanguage Processing (NLP). In parallel, the recent progress in Machine\nTranslation (MT) has enabled one to train multilingual Neural MT (NMT) systems\nthat can translate between multiple languages and are also capable of\nperforming zero-shot translation. However, little attention has been paid to\nleveraging representations learned by a multilingual NMT system to enable\nzero-shot multilinguality in other NLP tasks. In this paper, we demonstrate a\nsimple framework, a multilingual Encoder-Classifier, for cross-lingual transfer\nlearning by reusing the encoder from a multilingual NMT system and stitching it\nwith a task-specific classifier component. Our proposed model achieves\nsignificant improvements in the English setup on three benchmark tasks - Amazon\nReviews, SST and SNLI. Further, our system can perform classification in a new\nlanguage for which no classification data was seen during training, showing\nthat zero-shot classification is possible and remarkably competitive. In order\nto understand the underlying factors contributing to this finding, we conducted\na series of analyses on the effect of the shared vocabulary, the training data\ntype for NMT, classifier complexity, encoder representation power, and model\ngeneralization on zero-shot performance. Our results provide strong evidence\nthat the representations learned from multilingual NMT systems are widely\napplicable across languages and tasks.", "published": "2018-09-12 21:34:03", "link": "http://arxiv.org/abs/1809.04686v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantically Enhanced Models for Commonsense Knowledge Acquisition", "abstract": "Commonsense knowledge is paramount to enable intelligent systems. Typically,\nit is characterized as being implicit and ambiguous, hindering thereby the\nautomation of its acquisition. To address these challenges, this paper presents\nsemantically enhanced models to enable reasoning through resolving part of\ncommonsense ambiguity. The proposed models enhance in a knowledge graph\nembedding (KGE) framework for knowledge base completion. Experimental results\nshow the effectiveness of the new semantic models in commonsense reasoning.", "published": "2018-09-12 23:23:46", "link": "http://arxiv.org/abs/1809.04708v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Temporal Pattern Attention for Multivariate Time Series Forecasting", "abstract": "Forecasting multivariate time series data, such as prediction of electricity\nconsumption, solar power production, and polyphonic piano pieces, has numerous\nvaluable applications. However, complex and non-linear interdependencies\nbetween time steps and series complicate the task. To obtain accurate\nprediction, it is crucial to model long-term dependency in time series data,\nwhich can be achieved to some good extent by recurrent neural network (RNN)\nwith attention mechanism. Typical attention mechanism reviews the information\nat each previous time step and selects the relevant information to help\ngenerate the outputs, but it fails to capture the temporal patterns across\nmultiple time steps. In this paper, we propose to use a set of filters to\nextract time-invariant temporal patterns, which is similar to transforming time\nseries data into its \"frequency domain\". Then we proposed a novel attention\nmechanism to select relevant time series, and use its \"frequency domain\"\ninformation for forecasting. We applied the proposed model on several\nreal-world tasks and achieved state-of-the-art performance in all of them with\nonly one exception.", "published": "2018-09-12 00:40:40", "link": "http://arxiv.org/abs/1809.04206v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The Wisdom of MaSSeS: Majority, Subjectivity, and Semantic Similarity in\n  the Evaluation of VQA", "abstract": "We introduce MASSES, a simple evaluation metric for the task of Visual\nQuestion Answering (VQA). In its standard form, the VQA task is operationalized\nas follows: Given an image and an open-ended question in natural language,\nsystems are required to provide a suitable answer. Currently, model performance\nis evaluated by means of a somehow simplistic metric: If the predicted answer\nis chosen by at least 3 human annotators out of 10, then it is 100% correct.\nThough intuitively valuable, this metric has some important limitations. First,\nit ignores whether the predicted answer is the one selected by the Majority\n(MA) of annotators. Second, it does not account for the quantitative\nSubjectivity (S) of the answers in the sample (and dataset). Third, information\nabout the Semantic Similarity (SES) of the responses is completely neglected.\nBased on such limitations, we propose a multi-component metric that accounts\nfor all these issues. We show that our metric is effective in providing a more\nfine-grained evaluation both on the quantitative and qualitative level.", "published": "2018-09-12 10:11:39", "link": "http://arxiv.org/abs/1809.04344v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Unsupervised Representation Learning of Speech for Dialect\n  Identification", "abstract": "In this paper, we explore the use of a factorized hierarchical variational\nautoencoder (FHVAE) model to learn an unsupervised latent representation for\ndialect identification (DID). An FHVAE can learn a latent space that separates\nthe more static attributes within an utterance from the more dynamic attributes\nby encoding them into two different sets of latent variables. Useful factors\nfor dialect identification, such as phonetic or linguistic content, are encoded\nby a segmental latent variable, while irrelevant factors that are relatively\nconstant within a sequence, such as a channel or a speaker information, are\nencoded by a sequential latent variable. The disentanglement property makes the\nsegmental latent variable less susceptible to channel and speaker variation,\nand thus reduces degradation from channel domain mismatch. We demonstrate that\non fully-supervised DID tasks, an end-to-end model trained on the features\nextracted from the FHVAE model achieves the best performance, compared to the\nsame model trained on conventional acoustic features and an i-vector based\nsystem. Moreover, we also show that the proposed approach can leverage a large\namount of unlabeled data for FHVAE training to learn domain-invariant features\nfor DID, and significantly improve the performance in a low-resource condition,\nwhere the labels for the in-domain data are not available.", "published": "2018-09-12 13:57:06", "link": "http://arxiv.org/abs/1809.04458v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "End-to-end Audiovisual Speech Activity Detection with Bimodal Recurrent\n  Neural Models", "abstract": "Speech activity detection (SAD) plays an important role in current speech\nprocessing systems, including automatic speech recognition (ASR). SAD is\nparticularly difficult in environments with acoustic noise. A practical\nsolution is to incorporate visual information, increasing the robustness of the\nSAD approach. An audiovisual system has the advantage of being robust to\ndifferent speech modes (e.g., whisper speech) or background noise. Recent\nadvances in audiovisual speech processing using deep learning have opened\nopportunities to capture in a principled way the temporal relationships between\nacoustic and visual features. This study explores this idea proposing a\n\\emph{bimodal recurrent neural network} (BRNN) framework for SAD. The approach\nmodels the temporal dynamic of the sequential audiovisual data, improving the\naccuracy and robustness of the proposed SAD system. Instead of estimating\nhand-crafted features, the study investigates an end-to-end training approach,\nwhere acoustic and visual features are directly learned from the raw data\nduring training. The experimental evaluation considers a large audiovisual\ncorpus with over 60.8 hours of recordings, collected from 105 speakers. The\nresults demonstrate that the proposed framework leads to absolute improvements\nup to 1.2% under practical scenarios over a VAD baseline using only audio\nimplemented with deep neural network (DNN). The proposed approach achieves\n92.7% F1-score when it is evaluated using the sensors from a portable tablet\nunder noisy acoustic environment, which is only 1.0% lower than the performance\nobtained under ideal conditions (e.g., clean speech obtained with a high\ndefinition camera and a close-talking microphone).", "published": "2018-09-12 16:44:46", "link": "http://arxiv.org/abs/1809.04553v1", "categories": ["cs.CL", "cs.CV", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Game-Based Video-Context Dialogue", "abstract": "Current dialogue systems focus more on textual and speech context knowledge\nand are usually based on two speakers. Some recent work has investigated static\nimage-based dialogue. However, several real-world human interactions also\ninvolve dynamic visual context (similar to videos) as well as dialogue\nexchanges among multiple speakers. To move closer towards such multimodal\nconversational skills and visually-situated applications, we introduce a new\nvideo-context, many-speaker dialogue dataset based on live-broadcast soccer\ngame videos and chats from Twitch.tv. This challenging testbed allows us to\ndevelop visually-grounded dialogue models that should generate relevant\ntemporal and spatial event language from the live video, while also being\nrelevant to the chat history. For strong baselines, we also present several\ndiscriminative and generative models, e.g., based on tridirectional attention\nflow (TriDAF). We evaluate these models via retrieval ranking-recall, automatic\nphrase-matching metrics, as well as human evaluation studies. We also present\ndataset analyses, model ablations, and visualizations to understand the\ncontribution of different modalities and model components.", "published": "2018-09-12 16:53:13", "link": "http://arxiv.org/abs/1809.04560v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Distilled Wasserstein Learning for Word Embedding and Topic Modeling", "abstract": "We propose a novel Wasserstein method with a distillation mechanism, yielding\njoint learning of word embeddings and topics. The proposed method is based on\nthe fact that the Euclidean distance between word embeddings may be employed as\nthe underlying distance in the Wasserstein topic model. The word distributions\nof topics, their optimal transports to the word distributions of documents, and\nthe embeddings of words are learned in a unified framework. When learning the\ntopic model, we leverage a distilled underlying distance matrix to update the\ntopic distributions and smoothly calculate the corresponding optimal\ntransports. Such a strategy provides the updating of word embeddings with\nrobust guidance, improving the algorithmic convergence. As an application, we\nfocus on patient admission records, in which the proposed method embeds the\ncodes of diseases and procedures and learns the topics of admissions, obtaining\nsuperior performance on clinically-meaningful disease network construction,\nmortality prediction as a function of admission codes, and procedure\nrecommendation.", "published": "2018-09-12 23:10:23", "link": "http://arxiv.org/abs/1809.04705v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Automatic, Personalized, and Flexible Playlist Generation using\n  Reinforcement Learning", "abstract": "Songs can be well arranged by professional music curators to form a riveting\nplaylist that creates engaging listening experiences. However, it is\ntime-consuming for curators to timely rearrange these playlists for fitting\ntrends in future. By exploiting the techniques of deep learning and\nreinforcement learning, in this paper, we consider music playlist generation as\na language modeling problem and solve it by the proposed attention language\nmodel with policy gradient. We develop a systematic and interactive approach so\nthat the resulting playlists can be tuned flexibly according to user\npreferences. Considering a playlist as a sequence of words, we first train our\nattention RNN language model on baseline recommended playlists. By optimizing\nsuitable imposed reward functions, the model is thus refined for corresponding\npreferences. The experimental results demonstrate that our approach not only\ngenerates coherent playlists automatically but is also able to flexibly\nrecommend personalized playlists for diversity, novelty and freshness.", "published": "2018-09-12 01:32:11", "link": "http://arxiv.org/abs/1809.04214v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Frame-level speaker embeddings for text-independent speaker recognition\n  and analysis of end-to-end model", "abstract": "In this paper, we propose a Convolutional Neural Network (CNN) based speaker\nrecognition model for extracting robust speaker embeddings. The embedding can\nbe extracted efficiently with linear activation in the embedding layer. To\nunderstand how the speaker recognition model operates with text-independent\ninput, we modify the structure to extract frame-level speaker embeddings from\neach hidden layer. We feed utterances from the TIMIT dataset to the trained\nnetwork and use several proxy tasks to study the networks ability to represent\nspeech input and differentiate voice identity. We found that the networks are\nbetter at discriminating broad phonetic classes than individual phonemes. In\nparticular, frame-level embeddings that belong to the same phonetic classes are\nsimilar (based on cosine distance) for the same speaker. The frame level\nrepresentation also allows us to analyze the networks at the frame level, and\nhas the potential for other analyses to improve speaker recognition.", "published": "2018-09-12 13:48:44", "link": "http://arxiv.org/abs/1809.04437v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Transforming acoustic characteristics to deceive playback spoofing\n  countermeasures of speaker verification systems", "abstract": "Automatic speaker verification (ASV) systems use a playback detector to\nfilter out playback attacks and ensure verification reliability. Since current\nplayback detection models are almost always trained using genuine and\nplayed-back speech, it may be possible to degrade their performance by\ntransforming the acoustic characteristics of the played-back speech close to\nthat of the genuine speech. One way to do this is to enhance speech \"stolen\"\nfrom the target speaker before playback. We tested the effectiveness of a\nplayback attack using this method by using the speech enhancement generative\nadversarial network to transform acoustic characteristics. Experimental results\nshowed that use of this \"enhanced stolen speech\" method significantly increases\nthe equal error rates for the baseline used in the ASVspoof 2017 challenge and\nfor a light convolutional neural network-based method. The results also showed\nthat its use degrades the performance of a Gaussian mixture model-universal\nbackground model-based ASV system. This type of attack is thus an urgent\nproblem needing to be solved.", "published": "2018-09-12 06:45:42", "link": "http://arxiv.org/abs/1809.04274v2", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music Transformer", "abstract": "Music relies heavily on repetition to build structure and meaning.\nSelf-reference occurs on multiple timescales, from motifs to phrases to reusing\nof entire sections of music, such as in pieces with ABA structure. The\nTransformer (Vaswani et al., 2017), a sequence model based on self-attention,\nhas achieved compelling results in many generation tasks that require\nmaintaining long-range coherence. This suggests that self-attention might also\nbe well-suited to modeling music. In musical composition and performance,\nhowever, relative timing is critically important. Existing approaches for\nrepresenting relative positional information in the Transformer modulate\nattention based on pairwise distance (Shaw et al., 2018). This is impractical\nfor long sequences such as musical compositions since their memory complexity\nfor intermediate relative information is quadratic in the sequence length. We\npropose an algorithm that reduces their intermediate memory requirement to\nlinear in the sequence length. This enables us to demonstrate that a\nTransformer with our modified relative attention mechanism can generate\nminute-long compositions (thousands of steps, four times the length modeled in\nOore et al., 2018) with compelling structure, generate continuations that\ncoherently elaborate on a given motif, and in a seq2seq setup generate\naccompaniments conditioned on melodies. We evaluate the Transformer with our\nrelative attention mechanism on two datasets, JSB Chorales and\nPiano-e-Competition, and obtain state-of-the-art results on the latter.", "published": "2018-09-12 07:15:26", "link": "http://arxiv.org/abs/1809.04281v3", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
