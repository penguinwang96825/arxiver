{"title": "BridgeNets: Student-Teacher Transfer Learning Based on Recursive Neural\n  Networks and its Application to Distant Speech Recognition", "abstract": "Despite the remarkable progress achieved on automatic speech recognition,\nrecognizing far-field speeches mixed with various noise sources is still a\nchallenging task. In this paper, we introduce novel student-teacher transfer\nlearning, BridgeNet which can provide a solution to improve distant speech\nrecognition. There are two key features in BridgeNet. First, BridgeNet extends\ntraditional student-teacher frameworks by providing multiple hints from a\nteacher network. Hints are not limited to the soft labels from a teacher\nnetwork. Teacher's intermediate feature representations can better guide a\nstudent network to learn how to denoise or dereverberate noisy input. Second,\nthe proposed recursive architecture in the BridgeNet can iteratively improve\ndenoising and recognition performance. The experimental results of BridgeNet\nshowed significant improvements in tackling the distant speech recognition\nproblem, where it achieved up to 13.24% relative WER reductions on AMI corpus\ncompared to a baseline neural network without teacher's hints.", "published": "2017-10-27 16:16:05", "link": "http://arxiv.org/abs/1710.10224v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "One-shot and few-shot learning of word embeddings", "abstract": "Standard deep learning systems require thousands or millions of examples to\nlearn a concept, and cannot integrate new concepts easily. By contrast, humans\nhave an incredible ability to do one-shot or few-shot learning. For instance,\nfrom just hearing a word used in a sentence, humans can infer a great deal\nabout it, by leveraging what the syntax and semantics of the surrounding words\ntells us. Here, we draw inspiration from this to highlight a simple technique\nby which deep recurrent networks can similarly exploit their prior knowledge to\nlearn a useful representation for a new word from little data. This could make\nnatural language processing systems much more flexible, by allowing them to\nlearn continually from the new words they encounter.", "published": "2017-10-27 18:05:22", "link": "http://arxiv.org/abs/1710.10280v2", "categories": ["cs.CL", "cs.LG", "stat.ML", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Tensor network language model", "abstract": "We propose a new statistical model suitable for machine learning of systems\nwith long distance correlations such as natural languages. The model is based\non directed acyclic graph decorated by multi-linear tensor maps in the vertices\nand vector spaces in the edges, called tensor network. Such tensor networks\nhave been previously employed for effective numerical computation of the\nrenormalization group flow on the space of effective quantum field theories and\nlattice models of statistical mechanics. We provide explicit algebro-geometric\nanalysis of the parameter moduli space for tree graphs, discuss model\nproperties and applications such as statistical translation.", "published": "2017-10-27 17:26:57", "link": "http://arxiv.org/abs/1710.10248v2", "categories": ["cs.CL", "cond-mat.dis-nn", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Acoustic Landmarks Contain More Information About the Phone String than\n  Other Frames for Automatic Speech Recognition with Deep Neural Network\n  Acoustic Model", "abstract": "Most mainstream Automatic Speech Recognition (ASR) systems consider all\nfeature frames equally important. However, acoustic landmark theory is based on\na contradictory idea, that some frames are more important than others. Acoustic\nlandmark theory exploits quantal non-linearities in the articulatory-acoustic\nand acoustic-perceptual relations to define landmark times at which the speech\nspectrum abruptly changes or reaches an extremum; frames overlapping landmarks\nhave been demonstrated to be sufficient for speech perception. In this work, we\nconduct experiments on the TIMIT corpus, with both GMM and DNN based ASR\nsystems and find that frames containing landmarks are more informative for ASR\nthan others. We find that altering the level of emphasis on landmarks by\nre-weighting acoustic likelihood tends to reduce the phone error rate (PER).\nFurthermore, by leveraging the landmark as a heuristic, one of our hybrid DNN\nframe dropping strategies maintained a PER within 0.44% of optimal when scoring\nless than half (45.8% to be precise) of the frames. This hybrid strategy\nout-performs other non-heuristic-based methods and demonstrate the potential of\nlandmarks for reducing computation.", "published": "2017-10-27 04:18:07", "link": "http://arxiv.org/abs/1710.09985v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Separation of Moving Sound Sources Using Multichannel NMF and Acoustic\n  Tracking", "abstract": "In this paper we propose a method for separation of moving sound sources. The\nmethod is based on first tracking the sources and then estimation of source\nspectrograms using multichannel non-negative matrix factorization (NMF) and\nextracting the sources from the mixture by single-channel Wiener filtering. We\npropose a novel multichannel NMF model with time-varying mixing of the sources\ndenoted by spatial covariance matrices (SCM) and provide update equations for\noptimizing model parameters minimizing squared Frobenius norm. The SCMs of the\nmodel are obtained based on estimated directions of arrival of tracked sources\nat each time frame. The evaluation is based on established objective separation\ncriteria and using real recordings of two and three simultaneous moving sound\nsources. The compared methods include conventional beamforming and ideal ratio\nmask separation. The proposed method is shown to exceed the separation quality\nof other evaluated blind approaches according to all measured quantities.\nAdditionally, we evaluate the method's susceptibility towards tracking errors\nby comparing the separation quality achieved using annotated ground truth\nsource trajectories.", "published": "2017-10-27 06:49:04", "link": "http://arxiv.org/abs/1710.10005v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Direction of arrival estimation for multiple sound sources using\n  convolutional recurrent neural network", "abstract": "This paper proposes a deep neural network for estimating the directions of\narrival (DOA) of multiple sound sources. The proposed stacked convolutional and\nrecurrent neural network (DOAnet) generates a spatial pseudo-spectrum (SPS)\nalong with the DOA estimates in both azimuth and elevation. We avoid any\nexplicit feature extraction step by using the magnitudes and phases of the\nspectrograms of all the channels as input to the network. The proposed DOAnet\nis evaluated by estimating the DOAs of multiple concurrently present sources in\nanechoic, matched and unmatched reverberant conditions. The results show that\nthe proposed DOAnet is capable of estimating the number of sources and their\nrespective DOAs with good precision and generate SPS with high signal-to-noise\nratio.", "published": "2017-10-27 10:24:00", "link": "http://arxiv.org/abs/1710.10059v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Advanced LSTM: A Study about Better Time Dependency Modeling in Emotion\n  Recognition", "abstract": "Long short-term memory (LSTM) is normally used in recurrent neural network\n(RNN) as basic recurrent unit. However,conventional LSTM assumes that the state\nat current time step depends on previous time step. This assumption constraints\nthe time dependency modeling capability. In this study, we propose a new\nvariation of LSTM, advanced LSTM (A-LSTM), for better temporal context\nmodeling. We employ A-LSTM in weighted pooling RNN for emotion recognition. The\nA-LSTM outperforms the conventional LSTM by 5.5% relatively. The A-LSTM based\nweighted pooling RNN can also complement the state-of-the-art emotion\nclassification framework. This shows the advantage of A-LSTM.", "published": "2017-10-27 15:29:09", "link": "http://arxiv.org/abs/1710.10197v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Sound Source Localization in a Multipath Environment Using Convolutional\n  Neural Networks", "abstract": "The propagation of sound in a shallow water environment is characterized by\nboundary reflections from the sea surface and sea floor. These reflections\nresult in multiple (indirect) sound propagation paths, which can degrade the\nperformance of passive sound source localization methods. This paper proposes\nthe use of convolutional neural networks (CNNs) for the localization of sources\nof broadband acoustic radiated noise (such as motor vessels) in shallow water\nmultipath environments. It is shown that CNNs operating on cepstrogram and\ngeneralized cross-correlogram inputs are able to more reliably estimate the\ninstantaneous range and bearing of transiting motor vessels when the source\nlocalization performance of conventional passive ranging methods is degraded.\nThe ensuing improvement in source localization performance is demonstrated\nusing real data collected during an at-sea experiment.", "published": "2017-10-27 01:14:51", "link": "http://arxiv.org/abs/1710.10948v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
