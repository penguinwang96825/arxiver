{"title": "Lifelong Intent Detection via Multi-Strategy Rebalancing", "abstract": "Conventional Intent Detection (ID) models are usually trained offline, which\nrelies on a fixed dataset and a predefined set of intent classes. However, in\nreal-world applications, online systems usually involve continually emerging\nnew user intents, which pose a great challenge to the offline training\nparadigm. Recently, lifelong learning has received increasing attention and is\nconsidered to be the most promising solution to this challenge. In this paper,\nwe propose Lifelong Intent Detection (LID), which continually trains an ID\nmodel on new data to learn newly emerging intents while avoiding\ncatastrophically forgetting old data. Nevertheless, we find that existing\nlifelong learning methods usually suffer from a serious imbalance between old\nand new data in the LID task. Therefore, we propose a novel lifelong learning\nmethod, Multi-Strategy Rebalancing (MSR), which consists of cosine\nnormalization, hierarchical knowledge distillation, and inter-class margin loss\nto alleviate the multiple negative effects of the imbalance problem.\nExperimental results demonstrate the effectiveness of our method, which\nsignificantly outperforms previous state-of-the-art lifelong learning methods\non the ATIS, SNIPS, HWU64, and CLINC150 benchmarks.", "published": "2021-08-10 04:35:13", "link": "http://arxiv.org/abs/2108.04445v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BROS: A Pre-trained Language Model Focusing on Text and Layout for\n  Better Key Information Extraction from Documents", "abstract": "Key information extraction (KIE) from document images requires understanding\nthe contextual and spatial semantics of texts in two-dimensional (2D) space.\nMany recent studies try to solve the task by developing pre-trained language\nmodels focusing on combining visual features from document images with texts\nand their layout. On the other hand, this paper tackles the problem by going\nback to the basic: effective combination of text and layout. Specifically, we\npropose a pre-trained language model, named BROS (BERT Relying On Spatiality),\nthat encodes relative positions of texts in 2D space and learns from unlabeled\ndocuments with area-masking strategy. With this optimized training scheme for\nunderstanding texts in 2D space, BROS shows comparable or better performance\ncompared to previous methods on four KIE benchmarks (FUNSD, SROIE*, CORD, and\nSciTSR) without relying on visual features. This paper also reveals two\nreal-world challenges in KIE tasks-(1) minimizing the error from incorrect text\nordering and (2) efficient learning from fewer downstream examples-and\ndemonstrates the superiority of BROS over previous methods. Code is available\nat https://github.com/clovaai/bros.", "published": "2021-08-10 09:30:23", "link": "http://arxiv.org/abs/2108.04539v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hope Speech detection in under-resourced Kannada language", "abstract": "Numerous methods have been developed to monitor the spread of negativity in\nmodern years by eliminating vulgar, offensive, and fierce comments from social\nmedia platforms. However, there are relatively lesser amounts of study that\nconverges on embracing positivity, reinforcing supportive and reassuring\ncontent in online forums. Consequently, we propose creating an English-Kannada\nHope speech dataset, KanHope and comparing several experiments to benchmark the\ndataset. The dataset consists of 6,176 user-generated comments in code mixed\nKannada scraped from YouTube and manually annotated as bearing hope speech or\nNot-hope speech. In addition, we introduce DC-BERT4HOPE, a dual-channel model\nthat uses the English translation of KanHope for additional training to promote\nhope speech detection. The approach achieves a weighted F1-score of 0.756,\nbettering other models. Henceforth, KanHope aims to instigate research in\nKannada while broadly promoting researchers to take a pragmatic approach\ntowards online content that encourages, positive, and supportive.", "published": "2021-08-10 11:59:42", "link": "http://arxiv.org/abs/2108.04616v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Differentiable Subset Pruning of Transformer Heads", "abstract": "Multi-head attention, a collection of several attention mechanisms that\nindependently attend to different parts of the input, is the key ingredient in\nthe Transformer. Recent work has shown, however, that a large proportion of the\nheads in a Transformer's multi-head attention mechanism can be safely pruned\naway without significantly harming the performance of the model; such pruning\nleads to models that are noticeably smaller and faster in practice. Our work\nintroduces a new head pruning technique that we term differentiable subset\npruning. Intuitively, our method learns per-head importance variables and then\nenforces a user-specified hard constraint on the number of unpruned heads. The\nimportance variables are learned via stochastic gradient descent. We conduct\nexperiments on natural language inference and machine translation; we show that\ndifferentiable subset pruning performs comparably or better than previous works\nwhile offering precise control of the sparsity level.", "published": "2021-08-10 13:08:34", "link": "http://arxiv.org/abs/2108.04657v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing with Commonsense Knowledge: A Survey", "abstract": "Commonsense knowledge is essential for advancing natural language processing\n(NLP) by enabling models to engage in human-like reasoning, which requires a\ndeeper understanding of context and often involves making inferences based on\nimplicit external knowledge. This paper explores the integration of commonsense\nknowledge into various NLP tasks. We begin by reviewing prominent commonsense\nknowledge bases and then discuss the benchmarks used to evaluate the\ncommonsense reasoning capabilities of NLP models, particularly language models.\nFurthermore, we highlight key methodologies for incorporating commonsense\nknowledge and their applications across different NLP tasks. The paper also\nexamines the challenges and emerging trends in enhancing NLP systems with\ncommonsense reasoning. All literature referenced in this survey can be accessed\nvia our GitHub repository: https://github.com/yuboxie/awesome-commonsense.", "published": "2021-08-10 13:25:29", "link": "http://arxiv.org/abs/2108.04674v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sampling-Based Approximations to Minimum Bayes Risk Decoding for Neural\n  Machine Translation", "abstract": "In NMT we search for the mode of the model distribution to form predictions.\nThe mode and other high-probability translations found by beam search have been\nshown to often be inadequate in a number of ways. This prevents improving\ntranslation quality through better search, as these idiosyncratic translations\nend up selected by the decoding algorithm, a problem known as the beam search\ncurse. Recently, an approximation to minimum Bayes risk (MBR) decoding has been\nproposed as an alternative decision rule that would likely not suffer from the\nsame problems. We analyse this approximation and establish that it has no\nequivalent to the beam search curse. We then design approximations that\ndecouple the cost of exploration from the cost of robust estimation of expected\nutility. This allows for much larger hypothesis spaces, which we show to be\nbeneficial. We also show that mode-seeking strategies can aid in constructing\ncompact sets of promising hypotheses and that MBR is effective in identifying\ngood translations in them. We conduct experiments on three language pairs\nvarying in amounts of resources available: English into and from German,\nRomanian, and Nepali.", "published": "2021-08-10 14:35:24", "link": "http://arxiv.org/abs/2108.04718v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Headed-Span-Based Projective Dependency Parsing", "abstract": "We propose a new method for projective dependency parsing based on headed\nspans. In a projective dependency tree, the largest subtree rooted at each word\ncovers a contiguous sequence (i.e., a span) in the surface order. We call such\na span marked by a root word \\textit{headed span}.\n  A projective dependency tree can be represented as a collection of headed\nspans. We decompose the score of a dependency tree into the scores of the\nheaded spans and design a novel $O(n^3)$ dynamic programming algorithm to\nenable global training and exact inference. Our model achieves state-of-the-art\nor competitive results on PTB, CTB, and UD. Our code is publicly available at\n\\url{https://github.com/sustcsonglin/span-based-dependency-parsing}.", "published": "2021-08-10 15:27:47", "link": "http://arxiv.org/abs/2108.04750v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Study of Social and Behavioral Determinants of Health in Lung Cancer\n  Patients Using Transformers-based Natural Language Processing Models", "abstract": "Social and behavioral determinants of health (SBDoH) have important roles in\nshaping people's health. In clinical research studies, especially comparative\neffectiveness studies, failure to adjust for SBDoH factors will potentially\ncause confounding issues and misclassification errors in either statistical\nanalyses and machine learning-based models. However, there are limited studies\nto examine SBDoH factors in clinical outcomes due to the lack of structured\nSBDoH information in current electronic health record (EHR) systems, while much\nof the SBDoH information is documented in clinical narratives. Natural language\nprocessing (NLP) is thus the key technology to extract such information from\nunstructured clinical text. However, there is not a mature clinical NLP system\nfocusing on SBDoH. In this study, we examined two state-of-the-art\ntransformer-based NLP models, including BERT and RoBERTa, to extract SBDoH\nconcepts from clinical narratives, applied the best performing model to extract\nSBDoH concepts on a lung cancer screening patient cohort, and examined the\ndifference of SBDoH information between NLP extracted results and structured\nEHRs (SBDoH information captured in standard vocabularies such as the\nInternational Classification of Diseases codes). The experimental results show\nthat the BERT-based NLP model achieved the best strict/lenient F1-score of\n0.8791 and 0.8999, respectively. The comparison between NLP extracted SBDoH\ninformation and structured EHRs in the lung cancer patient cohort of 864\npatients with 161,933 various types of clinical notes showed that much more\ndetailed information about smoking, education, and employment were only\ncaptured in clinical narratives and that it is necessary to use both clinical\nnarratives and structured EHRs to construct a more complete picture of\npatients' SBDoH factors.", "published": "2021-08-10 22:11:31", "link": "http://arxiv.org/abs/2108.04949v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SynCoBERT: Syntax-Guided Multi-Modal Contrastive Pre-Training for Code\n  Representation", "abstract": "Code representation learning, which aims to encode the semantics of source\ncode into distributed vectors, plays an important role in recent\ndeep-learning-based models for code intelligence. Recently, many pre-trained\nlanguage models for source code (e.g., CuBERT and CodeBERT) have been proposed\nto model the context of code and serve as a basis for downstream code\nintelligence tasks such as code search, code clone detection, and program\ntranslation. Current approaches typically consider the source code as a plain\nsequence of tokens, or inject the structure information (e.g., AST and\ndata-flow) into the sequential model pre-training. To further explore the\nproperties of programming languages, this paper proposes SynCoBERT, a\nsyntax-guided multi-modal contrastive pre-training approach for better code\nrepresentations. Specially, we design two novel pre-training objectives\noriginating from the symbolic and syntactic properties of source code, i.e.,\nIdentifier Prediction (IP) and AST Edge Prediction (TEP), which are designed to\npredict identifiers, and edges between two nodes of AST, respectively.\nMeanwhile, to exploit the complementary information in semantically equivalent\nmodalities (i.e., code, comment, AST) of the code, we propose a multi-modal\ncontrastive learning strategy to maximize the mutual information among\ndifferent modalities. Extensive experiments on four downstream tasks related to\ncode intelligence show that SynCoBERT advances the state-of-the-art with the\nsame pre-training corpus and model size.", "published": "2021-08-10 10:08:21", "link": "http://arxiv.org/abs/2108.04556v3", "categories": ["cs.CL", "cs.AI", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Automated Audio Captioning using Transfer Learning and Reconstruction\n  Latent Space Similarity Regularization", "abstract": "In this paper, we examine the use of Transfer Learning using Pretrained Audio\nNeural Networks (PANNs), and propose an architecture that is able to better\nleverage the acoustic features provided by PANNs for the Automated Audio\nCaptioning Task. We also introduce a novel self-supervised objective,\nReconstruction Latent Space Similarity Regularization (RLSSR). The RLSSR module\nsupplements the training of the model by minimizing the similarity between the\nencoder and decoder embedding. The combination of both methods allows us to\nsurpass state of the art results by a significant margin on the Clotho dataset\nacross several metrics and benchmarks.", "published": "2021-08-10 13:49:41", "link": "http://arxiv.org/abs/2108.04692v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Continual Learning for Grounded Instruction Generation by Observing\n  Human Following Behavior", "abstract": "We study continual learning for natural language instruction generation, by\nobserving human users' instruction execution. We focus on a collaborative\nscenario, where the system both acts and delegates tasks to human users using\nnatural language. We compare user execution of generated instructions to the\noriginal system intent as an indication to the system's success communicating\nits intent. We show how to use this signal to improve the system's ability to\ngenerate instructions via contextual bandit learning. In interaction with real\nusers, our system demonstrates dramatic improvements in its ability to generate\nlanguage over time.", "published": "2021-08-10 17:53:44", "link": "http://arxiv.org/abs/2108.04812v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Post-hoc Interpretability for Neural NLP: A Survey", "abstract": "Neural networks for NLP are becoming increasingly complex and widespread, and\nthere is a growing concern if these models are responsible to use. Explaining\nmodels helps to address the safety and ethical concerns and is essential for\naccountability. Interpretability serves to provide these explanations in terms\nthat are understandable to humans. Additionally, post-hoc methods provide\nexplanations after a model is learned and are generally model-agnostic. This\nsurvey provides a categorization of how recent post-hoc interpretability\nmethods communicate explanations to humans, it discusses each method in-depth,\nand how they are validated, as the latter is often a common concern.", "published": "2021-08-10 18:00:14", "link": "http://arxiv.org/abs/2108.04840v5", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Embodied BERT: A Transformer Model for Embodied, Language-guided Visual\n  Task Completion", "abstract": "Language-guided robots performing home and office tasks must navigate in and\ninteract with the world. Grounding language instructions against visual\nobservations and actions to take in an environment is an open challenge. We\npresent Embodied BERT (EmBERT), a transformer-based model which can attend to\nhigh-dimensional, multi-modal inputs across long temporal horizons for\nlanguage-conditioned task completion. Additionally, we bridge the gap between\nsuccessful object-centric navigation models used for non-interactive agents and\nthe language-guided visual task completion benchmark, ALFRED, by introducing\nobject navigation targets for EmBERT training. We achieve competitive\nperformance on the ALFRED benchmark, and EmBERT marks the first\ntransformer-based model to successfully handle the long-horizon, dense,\nmulti-modal histories of ALFRED, and the first ALFRED model to utilize\nobject-centric navigation targets.", "published": "2021-08-10 21:24:05", "link": "http://arxiv.org/abs/2108.04927v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "BERTHop: An Effective Vision-and-Language Model for Chest X-ray Disease\n  Diagnosis", "abstract": "Vision-and-language(V&L) models take image and text as input and learn to\ncapture the associations between them. Prior studies show that pre-trained V&L\nmodels can significantly improve the model performance for downstream tasks\nsuch as Visual Question Answering (VQA). However, V&L models are less effective\nwhen applied in the medical domain (e.g., on X-ray images and clinical notes)\ndue to the domain gap. In this paper, we investigate the challenges of applying\npre-trained V&L models in medical applications. In particular, we identify that\nthe visual representation in general V&L models is not suitable for processing\nmedical data. To overcome this limitation, we propose BERTHop, a\ntransformer-based model based on PixelHop++ and VisualBERT, for better\ncapturing the associations between the two modalities. Experiments on the OpenI\ndataset, a commonly used thoracic disease diagnosis benchmark, show that\nBERTHop achieves an average Area Under the Curve (AUC) of 98.12% which is 1.62%\nhigher than state-of-the-art (SOTA) while it is trained on a 9 times smaller\ndataset.", "published": "2021-08-10 21:51:25", "link": "http://arxiv.org/abs/2108.04938v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Novel Markovian Framework for Integrating Absolute and Relative\n  Ordinal Emotion Information", "abstract": "There is growing interest in affective computing for the representation and\nprediction of emotions along ordinal scales. However, the term ordinal emotion\nlabel has been used to refer to both absolute notions such as low or high\narousal, as well as relation notions such as arousal is higher at one instance\ncompared to another. In this paper, we introduce the terminology absolute and\nrelative ordinal labels to make this distinction clear and investigate both\nwith a view to integrate them and exploit their complementary nature. We\npropose a Markovian framework referred to as Dynamic Ordinal Markov Model\n(DOMM) that makes use of both absolute and relative ordinal information, to\nimprove speech based ordinal emotion prediction. Finally, the proposed\nframework is validated on two speech corpora commonly used in affective\ncomputing, the RECOLA and the IEMOCAP databases, across a range of system\nconfigurations. The results consistently indicate that integrating relative\nordinal information improves absolute ordinal emotion prediction.", "published": "2021-08-10 11:24:36", "link": "http://arxiv.org/abs/2108.04605v1", "categories": ["cs.AI", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Tracked 3D Ultrasound and Deep Neural Network-based Thyroid Segmentation\n  reduce Interobserver Variability in Thyroid Volumetry", "abstract": "Background: Thyroid volumetry is crucial in diagnosis, treatment and\nmonitoring of thyroid diseases. However, conventional thyroid volumetry with 2D\nultrasound is highly operator-dependent. This study compares 2D ultrasound and\ntracked 3D ultrasound with an automatic thyroid segmentation based on a deep\nneural network regarding inter- and intraobserver variability, time and\naccuracy. Volume reference was MRI. Methods: 28 healthy volunteers were scanned\nwith 2D and 3D ultrasound as well as by MRI. Three physicians (MD 1, 2, 3) with\ndifferent levels of experience (6, 4 and 1 a) performed three 2D ultrasound and\nthree tracked 3D ultrasound scans on each volunteer. In the 2D scans the\nthyroid lobe volumes were calculated with the ellipsoid formula. A\nconvolutional deep neural network (CNN) segmented the 3D thyroid lobes\nautomatically. On MRI (T1 VIBE sequence) the thyroid was manually segmented by\nan experienced medical doctor. Results: The CNN was trained to obtain a dice\nscore of 0.94. The interobserver variability comparing two MDs showed mean\ndifferences for 2D and 3D respectively of 0.58 ml to 0.52 ml (MD1 vs. 2), -1.33\nml to -0.17 ml (MD1 vs. 3) and -1.89 ml to -0.70 ml (MD2 vs. 3). Paired samples\nt-tests showed significant differences in two comparisons for 2D and none for\n3D. Intraobsever variability was similar for 2D and 3D ultrasound. Comparison\nof ultrasound volumes and MRI volumes by paired samples t-tests showed a\nsignificant difference for the 2D volumetry of all MDs, and no significant\ndifference for 3D ultrasound. Acquisition time was significantly shorter for 3D\nultrasound. Conclusion: Tracked 3D ultrasound combined with a CNN segmentation\nsignificantly reduces interobserver variability in thyroid volumetry and\nincreases the accuracy of the measurements with shorter acquisition times.", "published": "2021-08-10 23:28:27", "link": "http://arxiv.org/abs/2108.10118v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
