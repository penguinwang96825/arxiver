{"title": "Crossing penalised CAViaR", "abstract": "Dynamic quantiles, or Conditional Autoregressive Value at Risk (CAViaR)\nmodels, have been extensively studied at the individual level. However, efforts\nto estimate multiple dynamic quantiles jointly have been limited. Existing\napproaches either sequentially estimate fitted quantiles or impose restrictive\nassumptions on the data generating process. This paper fills this gap by\nproposing an objective function for the joint estimation of all quantiles,\nintroducing a crossing penalty to guide the process. Monte Carlo experiments\nand an empirical application on the FTSE100 validate the effectiveness of the\nmethod, offering a flexible and robust approach to modelling multiple dynamic\nquantiles in time-series data.", "published": "2025-01-17 21:43:41", "link": "http://arxiv.org/abs/2501.10564v1", "categories": ["q-fin.ST", "stat.ME"], "primary_category": "q-fin.ST"}
{"title": "Lead Times in Flux: Analyzing Airbnb Booking Dynamics During Global Upheavals (2018-2022)", "abstract": "Short-term shifts in booking behaviors can disrupt forecasting in the travel\nand hospitality industry, especially during global crises. Traditional metrics\nlike average or median lead times often overlook important distribution\nchanges. This study introduces a normalized L1 (Manhattan) distance to assess\nAirbnb booking lead time divergences from 2018 to 2022, focusing on the\nCOVID-19 pandemic across four major U.S. cities. We identify a two-phase\ndisruption: an abrupt change at the pandemic's onset followed by partial\nrecovery with persistent deviations from pre-2018 patterns. Our method reveals\nchanges in travelers' planning horizons that standard statistics miss,\nhighlighting the need to analyze the entire lead-time distribution for more\naccurate demand forecasting and pricing strategies. The normalized L1 metric\nprovides valuable insights for tourism stakeholders navigating ongoing market\nvolatility.", "published": "2025-01-17 20:16:32", "link": "http://arxiv.org/abs/2501.10535v1", "categories": ["stat.AP", "q-fin.ST"], "primary_category": "stat.AP"}
{"title": "Indigenous Languages Spoken in Argentina: A Survey of NLP and Speech\n  Resources", "abstract": "Argentina has a large yet little-known Indigenous linguistic diversity,\nencompassing at least 40 different languages. The majority of these languages\nare at risk of disappearing, resulting in a significant loss of world heritage\nand cultural knowledge. Currently, unified information on speakers and\ncomputational tools is lacking for these languages. In this work, we present a\nsystematization of the Indigenous languages spoken in Argentina, classifying\nthem into seven language families: Mapuche, Tup\\'i-Guaran\\'i, Guaycur\\'u,\nQuechua, Mataco-Mataguaya, Aymara, and Chon. For each one, we present an\nestimation of the national Indigenous population size, based on the most recent\nArgentinian census. We discuss potential reasons why the census questionnaire\ndesign may underestimate the actual number of speakers. We also provide a\nconcise survey of computational resources available for these languages,\nwhether or not they were specifically developed for Argentinian varieties.", "published": "2025-01-17 03:47:19", "link": "http://arxiv.org/abs/2501.09943v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FRAG: A Flexible Modular Framework for Retrieval-Augmented Generation\n  based on Knowledge Graphs", "abstract": "To mitigate the hallucination and knowledge deficiency in large language\nmodels (LLMs), Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG)\nhas shown promising potential by utilizing KGs as external resource to enhance\nLLMs reasoning. However, existing KG-RAG approaches struggle with a trade-off\nbetween flexibility and retrieval quality. Modular methods prioritize\nflexibility by avoiding the use of KG-fine-tuned models during retrieval,\nleading to fixed retrieval strategies and suboptimal retrieval quality.\nConversely, coupled methods embed KG information within models to improve\nretrieval quality, but at the expense of flexibility. In this paper, we propose\na novel flexible modular KG-RAG framework, termed FRAG, which synergizes the\nadvantages of both approaches. FRAG estimates the hop range of reasoning paths\nbased solely on the query and classify it as either simple or complex. To match\nthe complexity of the query, tailored pipelines are applied to ensure efficient\nand accurate reasoning path retrieval, thus fostering the final reasoning\nprocess. By using the query text instead of the KG to infer the structural\ninformation of reasoning paths and employing adaptable retrieval strategies,\nFRAG improves retrieval quality while maintaining flexibility. Moreover, FRAG\ndoes not require extra LLMs fine-tuning or calls, significantly boosting\nefficiency and conserving resources. Extensive experiments show that FRAG\nachieves state-of-the-art performance with high efficiency and low resource\nconsumption.", "published": "2025-01-17 05:19:14", "link": "http://arxiv.org/abs/2501.09957v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Multi-Turn Interaction Capabilities of Large Language Models", "abstract": "Multi-turn interaction in the dialogue system research refers to a system's\nability to maintain context across multiple dialogue turns, enabling it to\ngenerate coherent and contextually relevant responses. Recent advancements in\nlarge language models (LLMs) have significantly expanded the scope of\nmulti-turn interaction, moving beyond chatbots to enable more dynamic agentic\ninteractions with users or environments. In this paper, we provide a focused\nreview of the multi-turn capabilities of LLMs, which are critical for a wide\nrange of downstream applications, including conversational search and\nrecommendation, consultation services, and interactive tutoring. This survey\nexplores four key aspects: (1) the core model capabilities that contribute to\neffective multi-turn interaction, (2) how multi-turn interaction is evaluated\nin current practice, (3) the general algorithms used to enhance multi-turn\ninteraction, and (4) potential future directions for research in this field.", "published": "2025-01-17 05:21:49", "link": "http://arxiv.org/abs/2501.09959v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Agent-as-Judge for Factual Summarization of Long Narratives", "abstract": "Large Language Models (LLMs) have demonstrated near-human performance in\nsummarization tasks based on traditional metrics such as ROUGE and BERTScore.\nHowever, these metrics do not adequately capture critical aspects of\nsummarization quality, such as factual accuracy, particularly for long\nnarratives (>100K tokens). Recent advances, such as LLM-as-a-Judge, address the\nlimitations of metrics based on lexical similarity but still exhibit factual\ninconsistencies, especially in understanding character relationships and\nstates. In this work, we introduce NarrativeFactScore, a novel\n\"Agent-as-a-Judge\" framework for evaluating and refining summaries. By\nleveraging a Character Knowledge Graph (CKG) extracted from input and generated\nsummaries, NarrativeFactScore assesses the factual consistency and provides\nactionable guidance for refinement, such as identifying missing or erroneous\nfacts. We demonstrate the effectiveness of NarrativeFactScore through a\ndetailed workflow illustration and extensive validation on widely adopted\nbenchmarks, achieving superior performance compared to competitive methods. Our\nresults highlight the potential of agent-driven evaluation systems to improve\nthe factual reliability of LLM-generated summaries.", "published": "2025-01-17 07:23:06", "link": "http://arxiv.org/abs/2501.09993v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MSTS: A Multimodal Safety Test Suite for Vision-Language Models", "abstract": "Vision-language models (VLMs), which process image and text inputs, are\nincreasingly integrated into chat assistants and other consumer AI\napplications. Without proper safeguards, however, VLMs may give harmful advice\n(e.g. how to self-harm) or encourage unsafe behaviours (e.g. to consume drugs).\nDespite these clear hazards, little work so far has evaluated VLM safety and\nthe novel risks created by multimodal inputs. To address this gap, we introduce\nMSTS, a Multimodal Safety Test Suite for VLMs. MSTS comprises 400 test prompts\nacross 40 fine-grained hazard categories. Each test prompt consists of a text\nand an image that only in combination reveal their full unsafe meaning. With\nMSTS, we find clear safety issues in several open VLMs. We also find some VLMs\nto be safe by accident, meaning that they are safe because they fail to\nunderstand even simple test prompts. We translate MSTS into ten languages,\nshowing non-English prompts to increase the rate of unsafe model responses. We\nalso show models to be safer when tested with text only rather than multimodal\nprompts. Finally, we explore the automation of VLM safety assessments, finding\neven the best safety classifiers to be lacking.", "published": "2025-01-17 09:22:35", "link": "http://arxiv.org/abs/2501.10057v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Author-Specific Linguistic Patterns Unveiled: A Deep Learning Study on\n  Word Class Distributions", "abstract": "Deep learning methods have been increasingly applied to computational\nlinguistics to uncover patterns in text data. This study investigates\nauthor-specific word class distributions using part-of-speech (POS) tagging and\nbigram analysis. By leveraging deep neural networks, we classify literary\nauthors based on POS tag vectors and bigram frequency matrices derived from\ntheir works. We employ fully connected and convolutional neural network\narchitectures to explore the efficacy of unigram and bigram-based\nrepresentations. Our results demonstrate that while unigram features achieve\nmoderate classification accuracy, bigram-based models significantly improve\nperformance, suggesting that sequential word class patterns are more\ndistinctive of authorial style. Multi-dimensional scaling (MDS) visualizations\nreveal meaningful clustering of authors' works, supporting the hypothesis that\nstylistic nuances can be captured through computational methods. These findings\nhighlight the potential of deep learning and linguistic feature analysis for\nauthor profiling and literary studies.", "published": "2025-01-17 09:43:49", "link": "http://arxiv.org/abs/2501.10072v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling\n  under Long-Context Scenario", "abstract": "Enhancing large language models (LLMs) with real-time APIs can help generate\nmore accurate and up-to-date responses. However, evaluating the function\ncalling abilities of LLMs in real-world scenarios remains under-explored due to\nthe complexity of data collection and evaluation. In this work, we introduce\nComplexFuncBench, a benchmark for complex function calling across five\nreal-world scenarios. Compared to existing benchmarks, ComplexFuncBench\nencompasses multi-step and constrained function calling, which requires\nlong-parameter filing, parameter value reasoning, and 128k long context.\nAdditionally, we propose an automatic framework, ComplexEval, for\nquantitatively evaluating complex function calling tasks. Through comprehensive\nexperiments, we demonstrate the deficiencies of state-of-the-art LLMs in\nfunction calling and suggest future directions for optimizing these\ncapabilities. The data and code are available at\n\\url{https://github.com/THUDM/ComplexFuncBench}.", "published": "2025-01-17 11:41:53", "link": "http://arxiv.org/abs/2501.10132v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-stage Training of Bilingual Islamic LLM for Neural Passage\n  Retrieval", "abstract": "This study examines the use of Natural Language Processing (NLP) technology\nwithin the Islamic domain, focusing on developing an Islamic neural retrieval\nmodel. By leveraging the robust XLM-R model, the research employs a language\nreduction technique to create a lightweight bilingual large language model\n(LLM). Our approach for domain adaptation addresses the unique challenges faced\nin the Islamic domain, where substantial in-domain corpora exist only in Arabic\nwhile limited in other languages, including English.\n  The work utilizes a multi-stage training process for retrieval models,\nincorporating large retrieval datasets, such as MS MARCO, and smaller,\nin-domain datasets to improve retrieval performance. Additionally, we have\ncurated an in-domain retrieval dataset in English by employing data\naugmentation techniques and involving a reliable Islamic source. This approach\nenhances the domain-specific dataset for retrieval, leading to further\nperformance gains.\n  The findings suggest that combining domain adaptation and a multi-stage\ntraining method for the bilingual Islamic neural retrieval model enables it to\noutperform monolingual models on downstream retrieval tasks.", "published": "2025-01-17 13:17:42", "link": "http://arxiv.org/abs/2501.10175v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Know Your Mistakes: Towards Preventing Overreliance on Task-Oriented\n  Conversational AI Through Accountability Modeling", "abstract": "Recent LLMs have enabled significant advancements for conversational agents.\nHowever, they are also well known to hallucinate, producing responses that seem\nplausible but are factually incorrect. On the other hand, users tend to\nover-rely on LLM-based AI agents, accepting AI's suggestion even when it is\nwrong. Adding positive friction, such as explanations or getting user\nconfirmations, has been proposed as a mitigation in AI-supported\ndecision-making systems. In this paper, we propose an accountability model for\nLLM-based task-oriented dialogue agents to address user overreliance via\nfriction turns in cases of model uncertainty and errors associated with\ndialogue state tracking (DST). The accountability model is an augmented LLM\nwith an additional accountability head that functions as a binary classifier to\npredict the relevant slots of the dialogue state mentioned in the conversation.\nWe perform our experiments with multiple backbone LLMs on two established\nbenchmarks (MultiWOZ and Snips). Our empirical findings demonstrate that the\nproposed approach not only enables reliable estimation of AI agent errors but\nalso guides the decoder in generating more accurate actions. We observe around\n3% absolute improvement in joint goal accuracy (JGA) of DST output by\nincorporating accountability heads into modern LLMs. Self-correcting the\ndetected errors further increases the JGA from 67.13 to 70.51, achieving\nstate-of-the-art DST performance. Finally, we show that error correction\nthrough user confirmations (friction turn) achieves a similar performance gain,\nhighlighting its potential to reduce user overreliance.", "published": "2025-01-17 17:40:12", "link": "http://arxiv.org/abs/2501.10316v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing of Privacy Policies: A Survey", "abstract": "Natural Language Processing (NLP) is an essential subset of artificial\nintelligence. It has become effective in several domains, such as healthcare,\nfinance, and media, to identify perceptions, opinions, and misuse, among\nothers. Privacy is no exception, and initiatives have been taken to address the\nchallenges of usable privacy notifications to users with the help of NLP. To\nthis aid, we conduct a literature review by analyzing 109 papers at the\nintersection of NLP and privacy policies. First, we provide a brief\nintroduction to privacy policies and discuss various facets of associated\nproblems, which necessitate the application of NLP to elevate the current state\nof privacy notices and disclosures to users. Subsequently, we a) provide an\noverview of the implementation and effectiveness of NLP approaches for better\nprivacy policy communication; b) identify the methodologies that can be further\nenhanced to provide robust privacy policies; and c) identify the gaps in the\ncurrent state-of-the-art research. Our systematic analysis reveals that several\nresearch papers focus on annotating and classifying privacy texts for analysis\nbut need to adequately dwell on other aspects of NLP applications, such as\nsummarization. More specifically, ample research opportunities exist in this\ndomain, covering aspects such as corpus generation, summarization vectors,\ncontextualized word embedding, identification of privacy-relevant statement\ncategories, fine-grained classification, and domain-specific model tuning.", "published": "2025-01-17 17:47:15", "link": "http://arxiv.org/abs/2501.10319v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BoK: Introducing Bag-of-Keywords Loss for Interpretable Dialogue\n  Response Generation", "abstract": "The standard language modeling (LM) loss by itself has been shown to be\ninadequate for effective dialogue modeling. As a result, various training\napproaches, such as auxiliary loss functions and leveraging human feedback, are\nbeing adopted to enrich open-domain dialogue systems. One such auxiliary loss\nfunction is Bag-of-Words (BoW) loss, defined as the cross-entropy loss for\npredicting all the words/tokens of the next utterance. In this work, we propose\na novel auxiliary loss named Bag-of-Keywords (BoK) loss to capture the central\nthought of the response through keyword prediction and leverage it to enhance\nthe generation of meaningful and interpretable responses in open-domain\ndialogue systems. BoK loss upgrades the BoW loss by predicting only the\nkeywords or critical words/tokens of the next utterance, intending to estimate\nthe core idea rather than the entire response. We incorporate BoK loss in both\nencoder-decoder (T5) and decoder-only (DialoGPT) architecture and train the\nmodels to minimize the weighted sum of BoK and LM (BoK-LM) loss. We perform our\nexperiments on two popular open-domain dialogue datasets, DailyDialog and\nPersona-Chat. We show that the inclusion of BoK loss improves the dialogue\ngeneration of backbone models while also enabling post-hoc interpretability. We\nalso study the effectiveness of BoK-LM loss as a reference-free metric and\nobserve comparable performance to the state-of-the-art metrics on various\ndialogue evaluation datasets.", "published": "2025-01-17 17:57:49", "link": "http://arxiv.org/abs/2501.10328v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialogue Benchmark Generation from Knowledge Graphs with Cost-Effective\n  Retrieval-Augmented LLMs", "abstract": "Dialogue benchmarks are crucial in training and evaluating chatbots engaging\nin domain-specific conversations. Knowledge graphs (KGs) represent semantically\nrich and well-organized data spanning various domains, such as DBLP, DBpedia,\nand YAGO. Traditionally, dialogue benchmarks have been manually created from\ndocuments, neglecting the potential of KGs in automating this process. Some\nquestion-answering benchmarks are automatically generated using extensive\npreprocessing from KGs, but they do not support dialogue generation. This paper\nintroduces Chatty-Gen, a novel multi-stage retrieval-augmented generation\nplatform for automatically generating high-quality dialogue benchmarks tailored\nto a specific domain using a KG. Chatty-Gen decomposes the generation process\ninto manageable stages and uses assertion rules for automatic validation\nbetween stages. Our approach enables control over intermediate results to\nprevent time-consuming restarts due to hallucinations. It also reduces reliance\non costly and more powerful commercial LLMs. Chatty-Gen eliminates upfront\nprocessing of the entire KG using efficient query-based retrieval to find\nrepresentative subgraphs based on the dialogue context. Our experiments with\nseveral real and large KGs demonstrate that Chatty-Gen significantly\noutperforms state-of-the-art systems and ensures consistent model and system\nperformance across multiple LLMs of diverse capabilities, such as GPT-4o,\nGemini 1.5, Llama 3, and Mistral.", "published": "2025-01-17 02:48:29", "link": "http://arxiv.org/abs/2501.09928v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Passage Segmentation of Documents for Extractive Question Answering", "abstract": "Retrieval-Augmented Generation (RAG) has proven effective in open-domain\nquestion answering. However, the chunking process, which is essential to this\npipeline, often receives insufficient attention relative to retrieval and\nsynthesis components. This study emphasizes the critical role of chunking in\nimproving the performance of both dense passage retrieval and the end-to-end\nRAG pipeline. We then introduce the Logits-Guided Multi-Granular Chunker\n(LGMGC), a novel framework that splits long documents into contextualized,\nself-contained chunks of varied granularity. Our experimental results,\nevaluated on two benchmark datasets, demonstrate that LGMGC not only improves\nthe retrieval step but also outperforms existing chunking methods when\nintegrated into a RAG pipeline.", "published": "2025-01-17 03:42:18", "link": "http://arxiv.org/abs/2501.09940v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Sympathy over Polarization: A Computational Discourse Analysis of Social\n  Media Posts about the July 2024 Trump Assassination Attempt", "abstract": "On July 13, 2024, at the Trump rally in Pennsylvania, someone attempted to\nassassinate Republican Presidential Candidate Donald Trump. This attempt\nsparked a large-scale discussion on social media. We collected posts from X\n(formerly known as Twitter) one week before and after the assassination attempt\nand aimed to model the short-term effects of such a ``shock'' on public\nopinions and discussion topics. Specifically, our study addresses three key\nquestions: first, we investigate how public sentiment toward Donald Trump\nshifts over time and across regions (RQ1) and examine whether the assassination\nattempt itself significantly affects public attitudes, independent of the\nexisting political alignments (RQ2). Finally, we explore the major themes in\nonline conversations before and after the crisis, illustrating how discussion\ntopics evolved in response to this politically charged event (RQ3). By\nintegrating large language model-based sentiment analysis,\ndifference-in-differences modeling, and topic modeling techniques, we find that\nfollowing the attempt the public response was broadly sympathetic to Trump\nrather than polarizing, despite baseline ideological and regional disparities.", "published": "2025-01-17 04:26:13", "link": "http://arxiv.org/abs/2501.09950v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Attention-guided Self-reflection for Zero-shot Hallucination Detection\n  in Large Language Models", "abstract": "Hallucination has emerged as a significant barrier to the effective\napplication of Large Language Models (LLMs). In this work, we introduce a novel\nAttention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination\ndetection in LLMs. The AGSER method utilizes attention contributions to\ncategorize the input query into attentive and non-attentive queries. Each query\nis then processed separately through the LLMs, allowing us to compute\nconsistency scores between the generated responses and the original answer. The\ndifference between the two consistency scores serves as a hallucination\nestimator. In addition to its efficacy in detecting hallucinations, AGSER\nnotably reduces computational overhead, requiring only three passes through the\nLLM and utilizing two sets of tokens. We have conducted extensive experiments\nwith four widely-used LLMs across three different hallucination benchmarks,\ndemonstrating that our approach significantly outperforms existing methods in\nzero-shot hallucination detection.", "published": "2025-01-17 07:30:01", "link": "http://arxiv.org/abs/2501.09997v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Speech Recognition for Sanskrit with Transfer Learning", "abstract": "Sanskrit, one of humanity's most ancient languages, has a vast collection of\nbooks and manuscripts on diverse topics that have been accumulated over\nmillennia. However, its digital content (audio and text), which is vital for\nthe training of AI systems, is profoundly limited. Furthermore, its intricate\nlinguistics make it hard to develop robust NLP tools for wider accessibility.\nGiven these constraints, we have developed an automatic speech recognition\nmodel for Sanskrit by employing transfer learning mechanism on OpenAI's Whisper\nmodel. After carefully optimising the hyper-parameters, we obtained promising\nresults with our transfer-learned model achieving a word error rate of 15.42%\non Vaksancayah dataset. An online demo of our model is made available for the\nuse of public and to evaluate its performance firsthand thereby paving the way\nfor improved accessibility and technological support for Sanskrit learning in\nthe modern era.", "published": "2025-01-17 08:20:32", "link": "http://arxiv.org/abs/2501.10024v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OMoE: Diversifying Mixture of Low-Rank Adaptation by Orthogonal\n  Finetuning", "abstract": "Building mixture-of-experts (MoE) architecture for Low-rank adaptation (LoRA)\nis emerging as a potential direction in parameter-efficient fine-tuning (PEFT)\nfor its modular design and remarkable performance. However, simply stacking the\nnumber of experts cannot guarantee significant improvement. In this work, we\nfirst conduct qualitative analysis to indicate that experts collapse to similar\nrepresentations in vanilla MoE, limiting the capacity of modular design and\ncomputational efficiency. Ulteriorly, Our analysis reveals that the performance\nof previous MoE variants maybe limited by a lack of diversity among experts.\nMotivated by these findings, we propose Orthogonal Mixture-of-Experts (OMoE), a\nresource-efficient MoE variant that trains experts in an orthogonal manner to\npromote diversity. In OMoE, a Gram-Schmidt process is leveraged to enforce that\nthe experts' representations lie within the Stiefel manifold. By applying\northogonal constraints directly to the architecture, OMoE keeps the learning\nobjective unchanged, without compromising optimality. Our method is simple and\nalleviates memory bottlenecks, as it incurs minimal experts compared to vanilla\nMoE models. Experiments on diverse commonsense reasoning benchmarks demonstrate\nthat OMoE can consistently achieve stable and efficient performance improvement\nwhen compared with the state-of-the-art methods while significantly reducing\nthe number of required experts.", "published": "2025-01-17 09:27:08", "link": "http://arxiv.org/abs/2501.10062v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "BBPOS: BERT-based Part-of-Speech Tagging for Uzbek", "abstract": "This paper advances NLP research for the low-resource Uzbek language by\nevaluating two previously untested monolingual Uzbek BERT models on the\npart-of-speech (POS) tagging task and introducing the first publicly available\nUPOS-tagged benchmark dataset for Uzbek. Our fine-tuned models achieve 91%\naverage accuracy, outperforming the baseline multi-lingual BERT as well as the\nrule-based tagger. Notably, these models capture intermediate POS changes\nthrough affixes and demonstrate context sensitivity, unlike existing rule-based\ntaggers.", "published": "2025-01-17 10:50:22", "link": "http://arxiv.org/abs/2501.10107v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dual Debiasing: Remove Stereotypes and Keep Factual Gender for Fair\n  Language Modeling and Translation", "abstract": "Mitigation of biases, such as language models' reliance on gender\nstereotypes, is a crucial endeavor required for the creation of reliable and\nuseful language technology. The crucial aspect of debiasing is to ensure that\nthe models preserve their versatile capabilities, including their ability to\nsolve language tasks and equitably represent various genders. To address this\nissue, we introduce a streamlined Dual Dabiasing Algorithm through Model\nAdaptation (2DAMA). Novel Dual Debiasing enables robust reduction of\nstereotypical bias while preserving desired factual gender information encoded\nby language models. We show that 2DAMA effectively reduces gender bias in\nEnglish and is one of the first approaches facilitating the mitigation of\nstereotypical tendencies in translation. The proposed method's key advantage is\nthe preservation of factual gender cues, which are useful in a wide range of\nnatural language processing tasks.", "published": "2025-01-17 12:23:30", "link": "http://arxiv.org/abs/2501.10150v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ArxEval: Evaluating Retrieval and Generation in Language Models for\n  Scientific Literature", "abstract": "Language Models [LMs] are now playing an increasingly large role in\ninformation generation and synthesis; the representation of scientific\nknowledge in these systems needs to be highly accurate. A prime challenge is\nhallucination; that is, generating apparently plausible but actually false\ninformation, including invented citations and nonexistent research papers. This\nkind of inaccuracy is dangerous in all the domains that require high levels of\nfactual correctness, such as academia and education. This work presents a\npipeline for evaluating the frequency with which language models hallucinate in\ngenerating responses in the scientific literature. We propose ArxEval, an\nevaluation pipeline with two tasks using ArXiv as a repository: Jumbled Titles\nand Mixed Titles. Our evaluation includes fifteen widely used language models\nand provides comparative insights into their reliability in handling scientific\nliterature.", "published": "2025-01-17 05:19:24", "link": "http://arxiv.org/abs/2501.10483v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Theme-Explanation Structure for Table Summarization using Large Language\n  Models: A Case Study on Korean Tabular Data", "abstract": "This paper proposes the Theme-Explanation Structure-based Table Summarization\n(Tabular-TX) pipeline designed to process tabular data efficiently. Tabular-TX\npreprocesses tabular data by focusing on highlighted cells. It then generates\nsummary sentences following a structured format, where the Theme Part appears\nas an adverbial phrase, and the Explanation Part follows as a predictive\nclause. This approach enables tailored analysis by considering the structural\ncharacteristics of tables and their comparability. Unlike conventional\nfine-tuning approaches that require extensive labeled data and computational\nresources, our method leverages In-Context Learning to dynamically adapt to\ndifferent table structures without additional training, ensuring efficient and\nscalable table interpretation. Experimental results demonstrate that Tabular-TX\nsignificantly outperforms conventional fine-tuning-based methods, particularly\nin low-resource scenarios, by leveraging table structures and metadata more\neffectively through structured prompts. The results confirm that Tabular-TX\nenables more effective processing of complex tabular data. Furthermore, it\nserves as a viable alternative for table-based question answering and\nsummarization tasks in resource-constrained environments.", "published": "2025-01-17 08:42:49", "link": "http://arxiv.org/abs/2501.10487v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Geometry of Tokens in Internal Representations of Large Language\n  Models", "abstract": "We investigate the relationship between the geometry of token embeddings and\ntheir role in the next token prediction within transformer models. An important\naspect of this connection uses the notion of empirical measure, which encodes\nthe distribution of token point clouds across transformer layers and drives the\nevolution of token representations in the mean-field interacting picture. We\nuse metrics such as intrinsic dimension, neighborhood overlap, and cosine\nsimilarity to observationally probe these empirical measures across layers. To\nvalidate our approach, we compare these metrics to a dataset where the tokens\nare shuffled, which disrupts the syntactic and semantic structure. Our findings\nreveal a correlation between the geometric properties of token embeddings and\nthe cross-entropy loss of next token predictions, implying that prompts with\nhigher loss values have tokens represented in higher-dimensional spaces.", "published": "2025-01-17 22:02:17", "link": "http://arxiv.org/abs/2501.10573v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adapting Large Language Models for Character-based Augmentative and\n  Alternative Communication", "abstract": "Users of Augmentative and Alternative Communication (AAC) may write\nletter-by-letter via an interface that uses a character language model.\nHowever, most state-of-the-art large pretrained language models predict subword\ntokens of variable length. We investigate how to practically use such models to\nmake accurate and efficient character predictions. We fine-tune models using a\nlarge dataset of sentences we curated in which each sentence is rated according\nto how useful it might be for spoken or written AAC communication. We find that\nusing an algorithm to produce character predictions from a subword large\nlanguage model provides more accurate predictions than adding a classification\nlayer or using a byte-level model. We also find that our domain adaptation\nprocedure is effective at improving model performance on simple, conversational\ntext.", "published": "2025-01-17 22:20:55", "link": "http://arxiv.org/abs/2501.10582v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Interpretable Steering of Large Language Models with Feature Guided\n  Activation Additions", "abstract": "Effective and reliable control over large language model (LLM) behavior is a\nsignificant challenge. While activation steering methods, which add steering\nvectors to a model's hidden states, are a promising approach, existing\ntechniques often lack precision and interpretability in how they influence\nmodel outputs. We introduce Feature Guided Activation Additions (FGAA), a novel\nactivation steering method that leverages insights from Contrastive Activation\nAddition (CAA) and Sparse Autoencoder-Targeted Steering (SAE-TS). By operating\nin the latent space of a Sparse Autoencoder (SAE) and employing optimization\ntechniques to select desired SAE features, FGAA constructs precise steering\nvectors that provide better steering effects while maintaining coherence of\nsteered model outputs. In this regard, evaluations on Gemma-2-2B and Gemma-2-9B\nmodels across various steering tasks demonstrate that FGAA outperforms existing\nsteering methods of CAA, SAE decoder steering, and SAE-TS. Our results also\nhighlight important trade-offs between steering scale and general model\ncapabilities that are consistent across all tested steering methods.", "published": "2025-01-17 02:55:23", "link": "http://arxiv.org/abs/2501.09929v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "RichSpace: Enriching Text-to-Video Prompt Space via Text Embedding\n  Interpolation", "abstract": "Text-to-video generation models have made impressive progress, but they still\nstruggle with generating videos with complex features. This limitation often\narises from the inability of the text encoder to produce accurate embeddings,\nwhich hinders the video generation model. In this work, we propose a novel\napproach to overcome this challenge by selecting the optimal text embedding\nthrough interpolation in the embedding space. We demonstrate that this method\nenables the video generation model to produce the desired videos. Additionally,\nwe introduce a simple algorithm using perpendicular foot embeddings and cosine\nsimilarity to identify the optimal interpolation embedding. Our findings\nhighlight the importance of accurate text embeddings and offer a pathway for\nimproving text-to-video generation performance.", "published": "2025-01-17 06:46:10", "link": "http://arxiv.org/abs/2501.09982v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Simple but Effective Closed-form Solution for Extreme Multi-label\n  Learning", "abstract": "Extreme multi-label learning (XML) is a task of assigning multiple labels\nfrom an extremely large set of labels to each data instance. Many current\nhigh-performance XML models are composed of a lot of hyperparameters, which\ncomplicates the tuning process. Additionally, the models themselves are adapted\nspecifically to XML, which complicates their reimplementation. To remedy this\nproblem, we propose a simple method based on ridge regression for XML. The\nproposed method not only has a closed-form solution but also is composed of a\nsingle hyperparameter. Since there are no precedents on applying ridge\nregression to XML, this paper verified the performance of the method by using\nvarious XML benchmark datasets. Furthermore, we enhanced the prediction of\nlow-frequency labels in XML, which hold informative content. This prediction is\nessential yet challenging because of the limited amount of data. Here, we\nemployed a simple frequency-based weighting. This approach greatly simplifies\nthe process compared with existing techniques. Experimental results revealed\nthat it can achieve levels of performance comparable to, or even exceeding,\nthose of models with numerous hyperparameters. Additionally, we found that the\nfrequency-based weighting significantly improved the predictive performance for\nlow-frequency labels, while requiring almost no changes in implementation. The\nsource code for the proposed method is available on github at\nhttps://github.com/cars1015/XML-ridge.", "published": "2025-01-17 13:24:13", "link": "http://arxiv.org/abs/2501.10179v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Computational Protein Science in the Era of Large Language Models (LLMs)", "abstract": "Considering the significance of proteins, computational protein science has\nalways been a critical scientific field, dedicated to revealing knowledge and\ndeveloping applications within the protein sequence-structure-function\nparadigm. In the last few decades, Artificial Intelligence (AI) has made\nsignificant impacts in computational protein science, leading to notable\nsuccesses in specific protein modeling tasks. However, those previous AI models\nstill meet limitations, such as the difficulty in comprehending the semantics\nof protein sequences, and the inability to generalize across a wide range of\nprotein modeling tasks. Recently, LLMs have emerged as a milestone in AI due to\ntheir unprecedented language processing & generalization capability. They can\npromote comprehensive progress in fields rather than solving individual tasks.\nAs a result, researchers have actively introduced LLM techniques in\ncomputational protein science, developing protein Language Models (pLMs) that\nskillfully grasp the foundational knowledge of proteins and can be effectively\ngeneralized to solve a diversity of sequence-structure-function reasoning\nproblems. While witnessing prosperous developments, it's necessary to present a\nsystematic overview of computational protein science empowered by LLM\ntechniques. First, we summarize existing pLMs into categories based on their\nmastered protein knowledge, i.e., underlying sequence patterns, explicit\nstructural and functional information, and external scientific languages.\nSecond, we introduce the utilization and adaptation of pLMs, highlighting their\nremarkable achievements in promoting protein structure prediction, protein\nfunction prediction, and protein design studies. Then, we describe the\npractical application of pLMs in antibody design, enzyme design, and drug\ndiscovery. Finally, we specifically discuss the promising future directions in\nthis fast-growing field.", "published": "2025-01-17 16:21:18", "link": "http://arxiv.org/abs/2501.10282v2", "categories": ["cs.CE", "cs.CL", "q-bio.BM"], "primary_category": "cs.CE"}
{"title": "Hierarchical Autoregressive Transformers: Combining Byte- and Word-Level\n  Processing for Robust, Adaptable Language Models", "abstract": "Tokenization is a fundamental step in natural language processing, breaking\ntext into units that computational models can process. While learned subword\ntokenizers have become the de-facto standard, they present challenges such as\nlarge vocabularies, limited adaptability to new domains or languages, and\nsensitivity to spelling errors and variations. To overcome these limitations,\nwe investigate a hierarchical architecture for autoregressive language\nmodelling that combines character-level and word-level processing. It employs a\nlightweight character-level encoder to convert character sequences into word\nembeddings, which are then processed by a word-level backbone model and decoded\nback into characters via a compact character-level decoder. This method retains\nthe sequence compression benefits of word-level tokenization without relying on\na rigid, predefined vocabulary. We demonstrate, at scales up to 7 billion\nparameters, that hierarchical transformers match the downstream task\nperformance of subword-tokenizer-based models while exhibiting significantly\ngreater robustness to input perturbations. Additionally, during continued\npretraining on an out-of-domain language, our model trains almost twice as\nfast, achieves superior performance on the target language, and retains more of\nits previously learned knowledge. Hierarchical transformers pave the way for\nNLP systems that are more robust, flexible, and generalizable across languages\nand domains.", "published": "2025-01-17 17:51:53", "link": "http://arxiv.org/abs/2501.10322v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large language models for automated scholarly paper review: A survey", "abstract": "Large language models (LLMs) have significantly impacted human society,\ninfluencing various domains. Among them, academia is not simply a domain\naffected by LLMs, but it is also the pivotal force in the development of LLMs.\nIn academic publications, this phenomenon is represented during the\nincorporation of LLMs into the peer review mechanism for reviewing manuscripts.\nWe proposed the concept of automated scholarly paper review (ASPR) in our\nprevious paper. As the incorporation grows, it now enters the coexistence phase\nof ASPR and peer review, which is described in that paper. LLMs hold\ntransformative potential for the full-scale implementation of ASPR, but they\nalso pose new issues and challenges that need to be addressed. In this survey\npaper, we aim to provide a holistic view of ASPR in the era of LLMs. We begin\nwith a survey to find out which LLMs are used to conduct ASPR. Then, we review\nwhat ASPR-related technological bottlenecks have been solved with the\nincorporation of LLM technology. After that, we move on to explore new methods,\nnew datasets, new source code, and new online systems that come with LLMs for\nASPR. Furthermore, we summarize the performance and issues of LLMs in ASPR, and\ninvestigate the attitudes and reactions of publishers and academia to ASPR.\nLastly, we discuss the challenges associated with the development of LLMs for\nASPR. We hope this survey can serve as an inspirational reference for the\nresearchers and promote the progress of ASPR for its actual implementation.", "published": "2025-01-17 17:56:58", "link": "http://arxiv.org/abs/2501.10326v1", "categories": ["cs.AI", "cs.CL", "cs.DL"], "primary_category": "cs.AI"}
{"title": "Improved IR-based Bug Localization with Intelligent Relevance Feedback", "abstract": "Software bugs pose a significant challenge during development and\nmaintenance, and practitioners spend nearly 50% of their time dealing with\nbugs. Many existing techniques adopt Information Retrieval (IR) to localize a\nreported bug using textual and semantic relevance between bug reports and\nsource code. However, they often struggle to bridge a critical gap between bug\nreports and code that requires in-depth contextual understanding, which goes\nbeyond textual or semantic relevance. In this paper, we present a novel\ntechnique for bug localization - BRaIn - that addresses the contextual gaps by\nassessing the relevance between bug reports and code with Large Language Models\n(LLM). It then leverages the LLM's feedback (a.k.a., Intelligent Relevance\nFeedback) to reformulate queries and re-rank source documents, improving bug\nlocalization. We evaluate BRaIn using a benchmark dataset, Bench4BL, and three\nperformance metrics and compare it against six baseline techniques from the\nliterature. Our experimental results show that BRaIn outperforms baselines by\n87.6%, 89.5%, and 48.8% margins in MAP, MRR, and HIT@K, respectively.\nAdditionally, it can localize approximately 52% of bugs that cannot be\nlocalized by the baseline techniques due to the poor quality of corresponding\nbug reports. By addressing the contextual gaps and introducing Intelligent\nRelevance Feedback, BRaIn advances not only theory but also improves IR-based\nbug localization.", "published": "2025-01-17 20:29:38", "link": "http://arxiv.org/abs/2501.10542v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "F.2.2; I.2.7"], "primary_category": "cs.SE"}
{"title": "When language and vision meet road safety: leveraging multimodal large\n  language models for video-based traffic accident analysis", "abstract": "The increasing availability of traffic videos functioning on a 24/7/365 time\nscale has the great potential of increasing the spatio-temporal coverage of\ntraffic accidents, which will help improve traffic safety. However, analyzing\nfootage from hundreds, if not thousands, of traffic cameras in a 24/7/365\nworking protocol remains an extremely challenging task, as current vision-based\napproaches primarily focus on extracting raw information, such as vehicle\ntrajectories or individual object detection, but require laborious\npost-processing to derive actionable insights. We propose SeeUnsafe, a new\nframework that integrates Multimodal Large Language Model (MLLM) agents to\ntransform video-based traffic accident analysis from a traditional\nextraction-then-explanation workflow to a more interactive, conversational\napproach. This shift significantly enhances processing throughput by automating\ncomplex tasks like video classification and visual grounding, while improving\nadaptability by enabling seamless adjustments to diverse traffic scenarios and\nuser-defined queries. Our framework employs a severity-based aggregation\nstrategy to handle videos of various lengths and a novel multimodal prompt to\ngenerate structured responses for review and evaluation and enable fine-grained\nvisual grounding. We introduce IMS (Information Matching Score), a new\nMLLM-based metric for aligning structured responses with ground truth. We\nconduct extensive experiments on the Toyota Woven Traffic Safety dataset,\ndemonstrating that SeeUnsafe effectively performs accident-aware video\nclassification and visual grounding by leveraging off-the-shelf MLLMs. Source\ncode will be available at \\url{https://github.com/ai4ce/SeeUnsafe}.", "published": "2025-01-17 23:35:34", "link": "http://arxiv.org/abs/2501.10604v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "GaussMark: A Practical Approach for Structural Watermarking of Language\n  Models", "abstract": "Recent advances in Large Language Models (LLMs) have led to significant\nimprovements in natural language processing tasks, but their ability to\ngenerate human-quality text raises significant ethical and operational concerns\nin settings where it is important to recognize whether or not a given text was\ngenerated by a human. Thus, recent work has focused on developing techniques\nfor watermarking LLM-generated text, i.e., introducing an almost imperceptible\nsignal that allows a provider equipped with a secret key to determine if given\ntext was generated by their model. Current watermarking techniques are often\nnot practical due to concerns with generation latency, detection time,\ndegradation in text quality, or robustness. Many of these drawbacks come from\nthe focus on token-level watermarking, which ignores the inherent structure of\ntext. In this work, we introduce a new scheme, GaussMark, that is simple and\nefficient to implement, has formal statistical guarantees on its efficacy,\ncomes at no cost in generation latency, and embeds the watermark into the\nweights of the model itself, providing a structural watermark. Our approach is\nbased on Gaussian independence testing and is motivated by recent empirical\nobservations that minor additive corruptions to LLM weights can result in\nmodels of identical (or even improved) quality. We show that by adding a small\namount of Gaussian noise to the weights of a given LLM, we can watermark the\nmodel in a way that is statistically detectable by a provider who retains the\nsecret key. We provide formal statistical bounds on the validity and power of\nour procedure. Through an extensive suite of experiments, we demonstrate that\nGaussMark is reliable, efficient, and relatively robust to corruptions such as\ninsertions, deletions, substitutions, and roundtrip translations and can be\ninstantiated with essentially no loss in model quality.", "published": "2025-01-17 22:30:08", "link": "http://arxiv.org/abs/2501.13941v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "On Ambisonic Source Separation with Spatially Informed Non-negative\n  Tensor Factorization", "abstract": "This article presents a Non-negative Tensor Factorization based method for\nsound source separation from Ambisonic microphone signals. The proposed method\nenables the use of prior knowledge about the Directions-of-Arrival (DOAs) of\nthe sources, incorporated through a constraint on the Spatial Covariance Matrix\n(SCM) within a Maximum a Posteriori (MAP) framework. Specifically, this article\npresents a detailed derivation of four algorithms that are based on two types\nof cost functions, namely the squared Euclidean distance and the Itakura-Saito\ndivergence, which are then combined with two prior probability distributions on\nthe SCM, that is the Wishart and the Inverse Wishart. The experimental\nevaluation of the baseline Maximum Likelihood (ML) and the proposed MAP methods\nis primarily based on first-order Ambisonic recordings, using four different\nsource signal datasets, three with musical pieces and one containing speech\nutterances. We consider under-determined, determined, as well as\nover-determined scenarios by separating two, four and six sound sources,\nrespectively. Furthermore, we evaluate the proposed algorithms for different\nspherical harmonic orders and at different reverberation time levels, as well\nas in non-ideal prior knowledge conditions, for increasingly more corrupted\nDOAs. Overall, in comparison with beamforming and a state-of-the-art separation\ntechnique, as well as the baseline ML methods, the proposed MAP approach offers\nsuperior separation performance in a variety of scenarios, as shown by the\nanalysis of the experimental evaluation results, in terms of the standard\nobjective separation measures, such as the SDR, ISR, SIR and SAR.", "published": "2025-01-17 16:58:31", "link": "http://arxiv.org/abs/2501.10305v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "HiFi-SR: A Unified Generative Transformer-Convolutional Adversarial\n  Network for High-Fidelity Speech Super-Resolution", "abstract": "The application of generative adversarial networks (GANs) has recently\nadvanced speech super-resolution (SR) based on intermediate representations\nlike mel-spectrograms. However, existing SR methods that typically rely on\nindependently trained and concatenated networks may lead to inconsistent\nrepresentations and poor speech quality, especially in out-of-domain scenarios.\nIn this work, we propose HiFi-SR, a unified network that leverages end-to-end\nadversarial training to achieve high-fidelity speech super-resolution. Our\nmodel features a unified transformer-convolutional generator designed to\nseamlessly handle both the prediction of latent representations and their\nconversion into time-domain waveforms. The transformer network serves as a\npowerful encoder, converting low-resolution mel-spectrograms into latent space\nrepresentations, while the convolutional network upscales these representations\ninto high-resolution waveforms. To enhance high-frequency fidelity, we\nincorporate a multi-band, multi-scale time-frequency discriminator, along with\na multi-scale mel-reconstruction loss in the adversarial training process.\nHiFi-SR is versatile, capable of upscaling any input speech signal between 4\nkHz and 32 kHz to a 48 kHz sampling rate. Experimental results demonstrate that\nHiFi-SR significantly outperforms existing speech SR methods across both\nobjective metrics and ABX preference tests, for both in-domain and\nout-of-domain scenarios (https://github.com/modelscope/ClearerVoice-Studio).", "published": "2025-01-17 09:04:38", "link": "http://arxiv.org/abs/2501.10045v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Conditional Latent Diffusion-Based Speech Enhancement Via Dual Context\n  Learning", "abstract": "Recently, the application of diffusion probabilistic models has advanced\nspeech enhancement through generative approaches. However, existing\ndiffusion-based methods have focused on the generation process in\nhigh-dimensional waveform or spectral domains, leading to increased generation\ncomplexity and slower inference speeds. Additionally, these methods have\nprimarily modelled clean speech distributions, with limited exploration of\nnoise distributions, thereby constraining the discriminative capability of\ndiffusion models for speech enhancement. To address these issues, we propose a\nnovel approach that integrates a conditional latent diffusion model (cLDM) with\ndual-context learning (DCL). Our method utilizes a variational autoencoder\n(VAE) to compress mel-spectrograms into a low-dimensional latent space. We then\napply cLDM to transform the latent representations of both clean speech and\nbackground noise into Gaussian noise by the DCL process, and a parameterized\nmodel is trained to reverse this process, conditioned on noisy latent\nrepresentations and text embeddings. By operating in a lower-dimensional space,\nthe latent representations reduce the complexity of the generation process,\nwhile the DCL process enhances the model's ability to handle diverse and unseen\nnoise environments. Our experiments demonstrate the strong performance of the\nproposed approach compared to existing diffusion-based methods, even with fewer\niterative steps, and highlight the superior generalization capability of our\nmodels to out-of-domain noise datasets\n(https://github.com/modelscope/ClearerVoice-Studio).", "published": "2025-01-17 09:15:53", "link": "http://arxiv.org/abs/2501.10052v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AI-Generated Music Detection and its Challenges", "abstract": "In the face of a new era of generative models, the detection of artificially\ngenerated content has become a matter of utmost importance. In particular, the\nability to create credible minute-long synthetic music in a few seconds on\nuser-friendly platforms poses a real threat of fraud on streaming services and\nunfair competition to human artists. This paper demonstrates the possibility\n(and surprising ease) of training classifiers on datasets comprising real audio\nand artificial reconstructions, achieving a convincing accuracy of 99.8%. To\nour knowledge, this marks the first publication of a AI-music detector, a tool\nthat will help in the regulation of synthetic media. Nevertheless, informed by\ndecades of literature on forgery detection in other fields, we stress that\ngetting a good test score is not the end of the story. We expose and discuss\nseveral facets that could be problematic with such a deployed detector:\nrobustness to audio manipulation, generalisation to unseen models. This second\npart acts as a position for future research steps in the field and a caveat to\na flourishing market of artificial content checkers.", "published": "2025-01-17 10:53:07", "link": "http://arxiv.org/abs/2501.10111v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards An Integrated Approach for Expressive Piano Performance\n  Synthesis from Music Scores", "abstract": "This paper presents an integrated system that transforms symbolic music\nscores into expressive piano performance audio. By combining a\nTransformer-based Expressive Performance Rendering (EPR) model with a\nfine-tuned neural MIDI synthesiser, our approach directly generates expressive\naudio performances from score inputs. To the best of our knowledge, this is the\nfirst system to offer a streamlined method for converting score MIDI files\nlacking expression control into rich, expressive piano performances. We\nconducted experiments using subsets of the ATEPP dataset, evaluating the system\nwith both objective metrics and subjective listening tests. Our system not only\naccurately reconstructs human-like expressiveness, but also captures the\nacoustic ambience of environments such as concert halls and recording studios.\nAdditionally, the proposed system demonstrates its ability to achieve musical\nexpressiveness while ensuring good audio quality in its outputs.", "published": "2025-01-17 14:38:58", "link": "http://arxiv.org/abs/2501.10222v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GVMGen: A General Video-to-Music Generation Model with Hierarchical\n  Attentions", "abstract": "Composing music for video is essential yet challenging, leading to a growing\ninterest in automating music generation for video applications. Existing\napproaches often struggle to achieve robust music-video correspondence and\ngenerative diversity, primarily due to inadequate feature alignment methods and\ninsufficient datasets. In this study, we present General Video-to-Music\nGeneration model (GVMGen), designed for generating high-related music to the\nvideo input. Our model employs hierarchical attentions to extract and align\nvideo features with music in both spatial and temporal dimensions, ensuring the\npreservation of pertinent features while minimizing redundancy. Remarkably, our\nmethod is versatile, capable of generating multi-style music from different\nvideo inputs, even in zero-shot scenarios. We also propose an evaluation model\nalong with two novel objective metrics for assessing video-music alignment.\nAdditionally, we have compiled a large-scale dataset comprising diverse types\nof video-music pairs. Experimental results demonstrate that GVMGen surpasses\nprevious models in terms of music-video correspondence, generative diversity,\nand application universality.", "published": "2025-01-17 06:30:11", "link": "http://arxiv.org/abs/2501.09972v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unsupervised Rhythm and Voice Conversion of Dysarthric to Healthy Speech\n  for ASR", "abstract": "Automatic speech recognition (ASR) systems are well known to perform poorly\non dysarthric speech. Previous works have addressed this by speaking rate\nmodification to reduce the mismatch with typical speech. Unfortunately, these\napproaches rely on transcribed speech data to estimate speaking rates and\nphoneme durations, which might not be available for unseen speakers. Therefore,\nwe combine unsupervised rhythm and voice conversion methods based on\nself-supervised speech representations to map dysarthric to typical speech. We\nevaluate the outputs with a large ASR model pre-trained on healthy speech\nwithout further fine-tuning and find that the proposed rhythm conversion\nespecially improves performance for speakers of the Torgo corpus with more\nsevere cases of dysarthria. Code and audio samples are available at\nhttps://idiap.github.io/RnV .", "published": "2025-01-17 15:39:21", "link": "http://arxiv.org/abs/2501.10256v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DFingerNet: Noise-Adaptive Speech Enhancement for Hearing Aids", "abstract": "The DeepFilterNet (DFN) architecture was recently proposed as a deep learning\nmodel suited for hearing aid devices. Despite its competitive performance on\nnumerous benchmarks, it still follows a `one-size-fits-all' approach, which\naims to train a single, monolithic architecture that generalises across\ndifferent noises and environments. However, its limited size and computation\nbudget can hamper its generalisability. Recent work has shown that in-context\nadaptation can improve performance by conditioning the denoising process on\nadditional information extracted from background recordings to mitigate this.\nThese recordings can be offloaded outside the hearing aid, thus improving\nperformance while adding minimal computational overhead. We introduce these\nprinciples to the DFN model, thus proposing the DFingerNet (DFiN) model, which\nshows superior performance on various benchmarks inspired by the DNS Challenge.", "published": "2025-01-17 19:56:22", "link": "http://arxiv.org/abs/2501.10525v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP", "I.2.6; H.5.5; I.5.1; I.4.8"], "primary_category": "cs.SD"}
