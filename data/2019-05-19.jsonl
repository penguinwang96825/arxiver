{"title": "Structured Summarization of Academic Publications", "abstract": "We propose SUSIE, a novel summarization method that can work with\nstate-of-the-art summarization models in order to produce structured scientific\nsummaries for academic articles. We also created PMC-SA, a new dataset of\nacademic publications, suitable for the task of structured summarization with\nneural networks. We apply SUSIE combined with three different summarization\nmodels on the new PMC-SA dataset and we show that the proposed method improves\nthe performance of all models by as much as 4 ROUGE points.", "published": "2019-05-19 07:12:22", "link": "http://arxiv.org/abs/1905.07695v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Earlier Attention? Aspect-Aware LSTM for Aspect-Based Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) aims to predict fine-grained\nsentiments of comments with respect to given aspect terms or categories. In\nprevious ABSA methods, the importance of aspect has been realized and verified.\nMost existing LSTM-based models take aspect into account via the attention\nmechanism, where the attention weights are calculated after the context is\nmodeled in the form of contextual vectors. However, aspect-related information\nmay be already discarded and aspect-irrelevant information may be retained in\nclassic LSTM cells in the context modeling process, which can be improved to\ngenerate more effective context representations. This paper proposes a novel\nvariant of LSTM, termed as aspect-aware LSTM (AA-LSTM), which incorporates\naspect information into LSTM cells in the context modeling stage before the\nattention mechanism. Therefore, our AA-LSTM can dynamically produce\naspect-aware contextual representations. We experiment with several\nrepresentative LSTM-based models by replacing the classic LSTM cells with the\nAA-LSTM cells. Experimental results on SemEval-2014 Datasets demonstrate the\neffectiveness of AA-LSTM.", "published": "2019-05-19 10:20:09", "link": "http://arxiv.org/abs/1905.07719v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Annotation Difficulty to Improve Task Routing and Model\n  Performance for Biomedical Information Extraction", "abstract": "Modern NLP systems require high-quality annotated data. In specialized\ndomains, expert annotations may be prohibitively expensive. An alternative is\nto rely on crowdsourcing to reduce costs at the risk of introducing noise. In\nthis paper we demonstrate that directly modeling instance difficulty can be\nused to improve model performance, and to route instances to appropriate\nannotators. Our difficulty prediction model combines two learned\nrepresentations: a `universal' encoder trained on out-of-domain data, and a\ntask-specific encoder. Experiments on a complex biomedical information\nextraction task using expert and lay annotators show that: (i) simply excluding\nfrom the training data instances predicted to be difficult yields a small boost\nin performance; (ii) using difficulty scores to weight instances during\ntraining provides further, consistent gains; (iii) assigning instances\npredicted to be difficult to domain experts is an effective strategy for task\nrouting. Our experiments confirm the expectation that for specialized tasks\nexpert annotations are higher quality than crowd labels, and hence preferable\nto obtain if practical. Moreover, augmenting small amounts of expert data with\na larger set of lay annotations leads to further improvements in model\nperformance.", "published": "2019-05-19 18:28:34", "link": "http://arxiv.org/abs/1905.07791v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HellaSwag: Can a Machine Really Finish Your Sentence?", "abstract": "Recent work by Zellers et al. (2018) introduced a new task of commonsense\nnatural language inference: given an event description such as \"A woman sits at\na piano,\" a machine must select the most likely followup: \"She sets her fingers\non the keys.\" With the introduction of BERT, near human-level performance was\nreached. Does this mean that machines can perform human level commonsense\ninference?\n  In this paper, we show that commonsense inference still proves difficult for\neven state-of-the-art models, by presenting HellaSwag, a new challenge dataset.\nThough its questions are trivial for humans (>95% accuracy), state-of-the-art\nmodels struggle (<48%). We achieve this via Adversarial Filtering (AF), a data\ncollection paradigm wherein a series of discriminators iteratively select an\nadversarial set of machine-generated wrong answers. AF proves to be\nsurprisingly robust. The key insight is to scale up the length and complexity\nof the dataset examples towards a critical 'Goldilocks' zone wherein generated\ntext is ridiculous to humans, yet often misclassified by state-of-the-art\nmodels.\n  Our construction of HellaSwag, and its resulting difficulty, sheds light on\nthe inner workings of deep pretrained models. More broadly, it suggests a new\npath forward for NLP research, in which benchmarks co-evolve with the evolving\nstate-of-the-art in an adversarial way, so as to present ever-harder\nchallenges.", "published": "2019-05-19 23:57:23", "link": "http://arxiv.org/abs/1905.07830v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Memorize in Neural Task-Oriented Dialogue Systems", "abstract": "In this thesis, we leverage the neural copy mechanism and memory-augmented\nneural networks (MANNs) to address existing challenge of neural task-oriented\ndialogue learning. We show the effectiveness of our strategy by achieving good\nperformance in multi-domain dialogue state tracking, retrieval-based dialogue\nsystems, and generation-based dialogue systems. We first propose a transferable\ndialogue state generator (TRADE) that leverages its copy mechanism to get rid\nof dialogue ontology and share knowledge between domains. We also evaluate\nunseen domain dialogue state tracking and show that TRADE enables zero-shot\ndialogue state tracking and can adapt to new few-shot domains without\nforgetting the previous domains. Second, we utilize MANNs to improve\nretrieval-based dialogue learning. They are able to capture dialogue sequential\ndependencies and memorize long-term information. We also propose a recorded\ndelexicalization copy strategy to replace real entity values with ordered\nentity types. Our models are shown to surpass other retrieval baselines,\nespecially when the conversation has a large number of turns. Lastly, we tackle\ngeneration-based dialogue learning with two proposed models, the\nmemory-to-sequence (Mem2Seq) and global-to-local memory pointer network (GLMP).\nMem2Seq is the first model to combine multi-hop memory attention with the idea\nof the copy mechanism. GLMP further introduces the concept of response\nsketching and double pointers copying. We show that GLMP achieves the\nstate-of-the-art performance on human evaluation.", "published": "2019-05-19 04:00:08", "link": "http://arxiv.org/abs/1905.07687v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DivGraphPointer: A Graph Pointer Network for Extracting Diverse\n  Keyphrases", "abstract": "Keyphrase extraction from documents is useful to a variety of applications\nsuch as information retrieval and document summarization. This paper presents\nan end-to-end method called DivGraphPointer for extracting a set of diversified\nkeyphrases from a document. DivGraphPointer combines the advantages of\ntraditional graph-based ranking methods and recent neural network-based\napproaches. Specifically, given a document, a word graph is constructed from\nthe document based on word proximity and is encoded with graph convolutional\nnetworks, which effectively capture document-level word salience by modeling\nlong-range dependency between words in the document and aggregating multiple\nappearances of identical words into one node. Furthermore, we propose a\ndiversified point network to generate a set of diverse keyphrases out of the\nword graph in the decoding process. Experimental results on five benchmark data\nsets show that our proposed method significantly outperforms the existing\nstate-of-the-art approaches.", "published": "2019-05-19 05:17:12", "link": "http://arxiv.org/abs/1905.07689v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Correlation Coefficients and Semantic Textual Similarity", "abstract": "A large body of research into semantic textual similarity has focused on\nconstructing state-of-the-art embeddings using sophisticated modelling, careful\nchoice of learning signals and many clever tricks. By contrast, little\nattention has been devoted to similarity measures between these embeddings,\nwith cosine similarity being used unquestionably in the majority of cases. In\nthis work, we illustrate that for all common word vectors, cosine similarity is\nessentially equivalent to the Pearson correlation coefficient, which provides\nsome justification for its use. We thoroughly characterise cases where Pearson\ncorrelation (and thus cosine similarity) is unfit as similarity measure.\nImportantly, we show that Pearson correlation is appropriate for some word\nvectors but not others. When it is not appropriate, we illustrate how common\nnon-parametric rank correlation coefficients can be used instead to\nsignificantly improve performance. We support our analysis with a series of\nevaluations on word-level and sentence-level semantic textual similarity\nbenchmarks. On the latter, we show that even the simplest averaged word vectors\ncompared by rank correlation easily rival the strongest deep representations\ncompared by cosine similarity.", "published": "2019-05-19 18:23:14", "link": "http://arxiv.org/abs/1905.07790v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Human Vocal Sentiment Analysis", "abstract": "In this paper, we use several techniques with conventional vocal feature\nextraction (MFCC, STFT), along with deep-learning approaches such as CNN, and\nalso context-level analysis, by providing the textual data, and combining\ndifferent approaches for improved emotion-level classification. We explore\nmodels that have not been tested to gauge the difference in performance and\naccuracy. We apply hyperparameter sweeps and data augmentation to improve\nperformance. Finally, we see if a real-time approach is feasible, and can be\nreadily integrated into existing systems.", "published": "2019-05-19 06:27:49", "link": "http://arxiv.org/abs/1905.08632v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
