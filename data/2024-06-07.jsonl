{"title": "An Algebraic Framework for the Modeling of Limit Order Books", "abstract": "Introducing an algebraic framework for modeling limit order books (LOBs) with\ntools from physics and stochastic processes, our proposed framework captures\nthe creation and annihilation of orders, order matching, and the time evolution\nof the LOB state. It also enables compositional settings, accommodating the\ninteraction of heterogeneous traders and different market structures. We employ\nDirac notation and generalized generating functions to describe the state space\nand dynamics of LOBs. The utility of this framework is shown through\nsimulations of simplified market scenarios, illustrating how variations in\ntrader behavior impact key market observables such as spread, return\nvolatility, and liquidity. The algebraic representation allows for exact\nsimulations using the Gillespie algorithm, providing a robust tool for\nexploring the implications of market design and policy changes on LOB dynamics.\nFuture research can expand this framework to incorporate more complex order\ntypes, adaptive event rates, and multi-asset trading environments, offering\ndeeper insights into market microstructure and trader behavior and estimation\nof key drivers for market microstructure dynamics.", "published": "2024-06-07 14:33:57", "link": "http://arxiv.org/abs/2406.04969v1", "categories": ["q-fin.TR", "q-fin.MF", "q-fin.ST", "60G10, 62M10, 91B26, 65C40", "G.3; I.6.1; I.6.4; J.4"], "primary_category": "q-fin.TR"}
{"title": "SC2: Towards Enhancing Content Preservation and Style Consistency in\n  Long Text Style Transfer", "abstract": "Text style transfer (TST) aims to vary the style polarity of text while\npreserving the semantic content. Although recent advancements have demonstrated\nremarkable progress in short TST, it remains a relatively straightforward task\nwith limited practical applications. The more comprehensive long TST task\npresents two challenges: (1) existing methods encounter difficulties in\naccurately evaluating content attributes in multiple words, leading to content\ndegradation; (2) the conventional vanilla style classifier loss encounters\nobstacles in maintaining consistent style across multiple generated sentences.\n  In this paper, we propose a novel method SC2, where a multilayer Joint\nStyle-Content Weighed (JSCW) module and a Style Consistency loss are designed\nto address the two issues. The JSCW simultaneously assesses the amounts of\nstyle and content attributes within a token, aiming to acquire a lossless\ncontent representation and thereby enhancing content preservation. The multiple\nJSCW layers further progressively refine content representations. We design a\nstyle consistency loss to ensure the generated multiple sentences consistently\nreflect the target style polarity. Moreover, we incorporate a denoising\nnon-autoregressive decoder to accelerate the training. We conduct plentiful\nexperiments and the results show significant improvements of SC2 over\ncompetitive baselines. Our code: https://github.com/jiezhao6/SC2.", "published": "2024-06-07 01:43:07", "link": "http://arxiv.org/abs/2406.04578v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extroversion or Introversion? Controlling The Personality of Your Large\n  Language Models", "abstract": "Large language models (LLMs) exhibit robust capabilities in text generation\nand comprehension, mimicking human behavior and exhibiting synthetic\npersonalities. However, some LLMs have displayed offensive personality,\npropagating toxic discourse. Existing literature neglects the origin and\nevolution of LLM personalities, as well as the effective personality control.\nTo fill these gaps, our study embarked on a comprehensive investigation into\nLLM personality control. We investigated several typical methods to influence\nLLMs, including three training methods: Continual Pre-training, Supervised\nFine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF), along\nwith inference phase considerations (prompts). Our investigation revealed a\nhierarchy of effectiveness in control: Prompt > SFT > RLHF > Continual\nPre-train. Notably, SFT exhibits a higher control success rate compared to\nprompt induction. While prompts prove highly effective, we found that\nprompt-induced personalities are less robust than those trained, making them\nmore prone to showing conflicting personalities under reverse personality\nprompt induction. Besides, harnessing the strengths of both SFT and prompt, we\nproposed $\\underline{\\text{P}}$rompt $\\underline{\\text{I}}$nduction post\n$\\underline{\\text{S}}$upervised $\\underline{\\text{F}}$ine-tuning (PISF), which\nemerges as the most effective and robust strategy for controlling LLMs'\npersonality, displaying high efficacy, high success rates, and high robustness.\nEven under reverse personality prompt induction, LLMs controlled by PISF still\nexhibit stable and robust personalities.", "published": "2024-06-07 02:11:49", "link": "http://arxiv.org/abs/2406.04583v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-Resource Cross-Lingual Summarization through Few-Shot Learning with\n  Large Language Models", "abstract": "Cross-lingual summarization (XLS) aims to generate a summary in a target\nlanguage different from the source language document. While large language\nmodels (LLMs) have shown promising zero-shot XLS performance, their few-shot\ncapabilities on this task remain unexplored, especially for low-resource\nlanguages with limited parallel data. In this paper, we investigate the\nfew-shot XLS performance of various models, including Mistral-7B-Instruct-v0.2,\nGPT-3.5, and GPT-4. Our experiments demonstrate that few-shot learning\nsignificantly improves the XLS performance of LLMs, particularly GPT-3.5 and\nGPT-4, in low-resource settings. However, the open-source model\nMistral-7B-Instruct-v0.2 struggles to adapt effectively to the XLS task with\nlimited examples. Our findings highlight the potential of few-shot learning for\nimproving XLS performance and the need for further research in designing LLM\narchitectures and pre-training objectives tailored for this task. We provide a\nfuture work direction to explore more effective few-shot learning strategies\nand to investigate the transfer learning capabilities of LLMs for cross-lingual\nsummarization.", "published": "2024-06-07 04:31:41", "link": "http://arxiv.org/abs/2406.04630v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Model-guided Document Selection", "abstract": "Large Language Model (LLM) pre-training exhausts an ever growing compute\nbudget, yet recent research has demonstrated that careful document selection\nenables comparable model quality with only a fraction of the FLOPs. Inspired by\nefforts suggesting that domain-specific training document selection is in fact\nan interpretable process [Gunasekar et al., 2023], as well as research showing\nthat instruction-finetuned LLMs are adept zero-shot data labelers [Gilardi et\nal.,2023], we explore a promising direction for scalable general-domain\ndocument selection; employing a prompted LLM as a document grader, we distill\nquality labels into a classifier model, which is applied at scale to a large,\nand already heavily-filtered, web-crawl-derived corpus autonomously. Following\nthe guidance of this classifier, we drop 75% of the corpus and train LLMs on\nthe remaining data. Results across multiple benchmarks show that: 1. Filtering\nallows us to quality-match a model trained on the full corpus across diverse\nbenchmarks with at most 70% of the FLOPs, 2. More capable LLM labelers and\nclassifier models lead to better results that are less sensitive to the\nlabeler's prompt, 3. In-context learning helps to boost the performance of\nless-capable labeling models. In all cases we use open-source datasets, models,\nrecipes, and evaluation frameworks, so that results can be reproduced by the\ncommunity.", "published": "2024-06-07 04:52:46", "link": "http://arxiv.org/abs/2406.04638v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "More Victories, Less Cooperation: Assessing Cicero's Diplomacy Play", "abstract": "The boardgame Diplomacy is a challenging setting for communicative and\ncooperative artificial intelligence. The most prominent communicative Diplomacy\nAI, Cicero, has excellent strategic abilities, exceeding human players.\nHowever, the best Diplomacy players master communication, not just tactics,\nwhich is why the game has received attention as an AI challenge. This work\nseeks to understand the degree to which Cicero succeeds at communication.\nFirst, we annotate in-game communication with abstract meaning representation\nto separate in-game tactics from general language. Second, we run two dozen\ngames with humans and Cicero, totaling over 200 human-player hours of\ncompetition. While AI can consistently outplay human players, AI-Human\ncommunication is still limited because of AI's difficulty with deception and\npersuasion. This shows that Cicero relies on strategy and has not yet reached\nthe full promise of communicative and cooperative AI.", "published": "2024-06-07 05:03:44", "link": "http://arxiv.org/abs/2406.04643v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiNeR: a Large Realistic Dataset for Evaluating Compositional\n  Generalization", "abstract": "Most of the existing compositional generalization datasets are\nsynthetically-generated, resulting in a lack of natural language variation.\nWhile there have been recent attempts to introduce non-synthetic datasets for\ncompositional generalization, they suffer from either limited data scale or a\nlack of diversity in the forms of combinations. To better investigate\ncompositional generalization with more linguistic phenomena and compositional\ndiversity, we propose the DIsh NamE Recognition (DiNeR) task and create a large\nrealistic Chinese dataset. Given a recipe instruction, models are required to\nrecognize the dish name composed of diverse combinations of food, actions, and\nflavors. Our dataset consists of 3,811 dishes and 228,114 recipes, and involves\nplenty of linguistic phenomena such as anaphora, omission and ambiguity. We\nprovide two strong baselines based on T5 and large language models (LLMs). This\nwork contributes a challenging task, baseline methods to tackle the task, and\ninsights into compositional generalization in the context of dish name\nrecognition. Code and data are available at https://github.com/Jumpy-pku/DiNeR.", "published": "2024-06-07 06:35:21", "link": "http://arxiv.org/abs/2406.04669v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixture-of-Agents Enhances Large Language Model Capabilities", "abstract": "Recent advances in large language models (LLMs) demonstrate substantial\ncapabilities in natural language understanding and generation tasks. With the\ngrowing number of LLMs, how to harness the collective expertise of multiple\nLLMs is an exciting open direction. Toward this goal, we propose a new approach\nthat leverages the collective strengths of multiple LLMs through a\nMixture-of-Agents (MoA) methodology. In our approach, we construct a layered\nMoA architecture wherein each layer comprises multiple LLM agents. Each agent\ntakes all the outputs from agents in the previous layer as auxiliary\ninformation in generating its response. MoA models achieves state-of-art\nperformance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For\nexample, our MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by\na substantial gap, achieving a score of 65.1% compared to 57.5% by GPT-4 Omni.", "published": "2024-06-07 07:04:10", "link": "http://arxiv.org/abs/2406.04692v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AICoderEval: Improving AI Domain Code Generation of Large Language\n  Models", "abstract": "Automated code generation is a pivotal capability of large language models\n(LLMs). However, assessing this capability in real-world scenarios remains\nchallenging. Previous methods focus more on low-level code generation, such as\nmodel loading, instead of generating high-level codes catering for real-world\ntasks, such as image-to-text, text classification, in various domains.\nTherefore, we construct AICoderEval, a dataset focused on real-world tasks in\nvarious domains based on HuggingFace, PyTorch, and TensorFlow, along with\ncomprehensive metrics for evaluation and enhancing LLMs' task-specific code\ngeneration capability. AICoderEval contains test cases and complete programs\nfor automated evaluation of these tasks, covering domains such as natural\nlanguage processing, computer vision, and multimodal learning. To facilitate\nresearch in this area, we open-source the AICoderEval dataset at\n\\url{https://huggingface.co/datasets/vixuowis/AICoderEval}. After that, we\npropose CoderGen, an agent-based framework, to help LLMs generate codes related\nto real-world tasks on the constructed AICoderEval. Moreover, we train a more\npowerful task-specific code generation model, named AICoder, which is refined\non llama-3 based on AICoderEval. Our experiments demonstrate the effectiveness\nof CoderGen in improving LLMs' task-specific code generation capability (by\n12.00\\% on pass@1 for original model and 9.50\\% on pass@1 for ReAct Agent).\nAICoder also outperforms current code generation LLMs, indicating the great\nquality of the AICoderEval benchmark.", "published": "2024-06-07 07:45:38", "link": "http://arxiv.org/abs/2406.04712v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CRAG -- Comprehensive RAG Benchmark", "abstract": "Retrieval-Augmented Generation (RAG) has recently emerged as a promising\nsolution to alleviate Large Language Model (LLM)'s deficiency in lack of\nknowledge. Existing RAG datasets, however, do not adequately represent the\ndiverse and dynamic nature of real-world Question Answering (QA) tasks. To\nbridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual\nquestion answering benchmark of 4,409 question-answer pairs and mock APIs to\nsimulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a\ndiverse array of questions across five domains and eight question categories,\nreflecting varied entity popularity from popular to long-tail, and temporal\ndynamisms ranging from years to seconds. Our evaluation of this benchmark\nhighlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve\n<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the\naccuracy only to 44%. State-of-the-art industry RAG solutions only answer 63%\nof questions without any hallucination. CRAG also reveals much lower accuracy\nin answering questions regarding facts with higher dynamism, lower popularity,\nor higher complexity, suggesting future research directions. The CRAG benchmark\nlaid the groundwork for a KDD Cup 2024 challenge and attracted thousands of\nparticipants and submissions. We commit to maintaining CRAG to serve research\ncommunities in advancing RAG solutions and general QA solutions. CRAG is\navailable at https://github.com/facebookresearch/CRAG/.", "published": "2024-06-07 08:43:07", "link": "http://arxiv.org/abs/2406.04744v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CRiskEval: A Chinese Multi-Level Risk Evaluation Benchmark Dataset for\n  Large Language Models", "abstract": "Large language models (LLMs) are possessed of numerous beneficial\ncapabilities, yet their potential inclination harbors unpredictable risks that\nmay materialize in the future. We hence propose CRiskEval, a Chinese dataset\nmeticulously designed for gauging the risk proclivities inherent in LLMs such\nas resource acquisition and malicious coordination, as part of efforts for\nproactive preparedness. To curate CRiskEval, we define a new risk taxonomy with\n7 types of frontier risks and 4 safety levels, including extremely\nhazardous,moderately hazardous, neutral and safe. We follow the philosophy of\ntendency evaluation to empirically measure the stated desire of LLMs via\nfine-grained multiple-choice question answering. The dataset consists of 14,888\nquestions that simulate scenarios related to predefined 7 types of frontier\nrisks. Each question is accompanied with 4 answer choices that state opinions\nor behavioral tendencies corresponding to the question. All answer choices are\nmanually annotated with one of the defined risk levels so that we can easily\nbuild a fine-grained frontier risk profile for each assessed LLM. Extensive\nevaluation with CRiskEval on a spectrum of prevalent Chinese LLMs has unveiled\na striking revelation: most models exhibit risk tendencies of more than 40%\n(weighted tendency to the four risk levels). Furthermore, a subtle increase in\nthe model's inclination toward urgent self-sustainability, power seeking and\nother dangerous goals becomes evident as the size of models increase. To\npromote further research on the frontier risk evaluation of LLMs, we publicly\nrelease our dataset at https://github.com/lingshi6565/Risk_eval.", "published": "2024-06-07 08:52:24", "link": "http://arxiv.org/abs/2406.04752v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Think out Loud: Emotion Deducing Explanation in Dialogues", "abstract": "Humans convey emotions through daily dialogues, making emotion understanding\na crucial step of affective intelligence. To understand emotions in dialogues,\nmachines are asked to recognize the emotion for an utterance (Emotion\nRecognition in Dialogues, ERD); based on the emotion, then find causal\nutterances for the emotion (Emotion Cause Extraction in Dialogues, ECED). The\nsetting of the two tasks requires first ERD and then ECED, ignoring the mutual\ncomplement between emotion and cause. To fix this, some new tasks are proposed\nto extract them simultaneously. Although the current research on these tasks\nhas excellent achievements, simply identifying emotion-related factors by\nclassification modeling lacks realizing the specific thinking process of causes\nstimulating the emotion in an explainable way. This thinking process especially\nreflected in the reasoning ability of Large Language Models (LLMs) is\nunder-explored. To this end, we propose a new task \"Emotion Deducing\nExplanation in Dialogues\" (EDEN). EDEN recognizes emotion and causes in an\nexplicitly thinking way. That is, models need to generate an explanation text,\nwhich first summarizes the causes; analyzes the inner activities of the\nspeakers triggered by the causes using common sense; then guesses the emotion\naccordingly. To support the study of EDEN, based on the existing resources in\nECED, we construct two EDEN datasets by human effort. We further evaluate\ndifferent models on EDEN and find that LLMs are more competent than\nconventional PLMs. Besides, EDEN can help LLMs achieve better recognition of\nemotions and causes, which explores a new research direction of explainable\nemotion understanding in dialogues.", "published": "2024-06-07 08:58:29", "link": "http://arxiv.org/abs/2406.04758v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotating FrameNet via Structure-Conditioned Language Generation", "abstract": "Despite the remarkable generative capabilities of language models in\nproducing naturalistic language, their effectiveness on explicit manipulation\nand generation of linguistic structures remain understudied. In this paper, we\ninvestigate the task of generating new sentences preserving a given semantic\nstructure, following the FrameNet formalism. We propose a framework to produce\nnovel frame-semantically annotated sentences following an\novergenerate-and-filter approach. Our results show that conditioning on rich,\nexplicit semantic information tends to produce generations with high human\nacceptance, under both prompting and finetuning. Our generated frame-semantic\nstructured annotations are effective at training data augmentation for\nframe-semantic role labeling in low-resource settings; however, we do not see\nbenefits under higher resource settings. Our study concludes that while\ngenerating high-quality, semantically rich data might be within reach, the\ndownstream utility of such generations remains to be seen, highlighting the\noutstanding challenges with automating linguistic annotation tasks.", "published": "2024-06-07 11:01:15", "link": "http://arxiv.org/abs/2406.04834v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Language Models Exhibit Human-like Structural Priming Effects?", "abstract": "We explore which linguistic factors -- at the sentence and token level --\nplay an important role in influencing language model predictions, and\ninvestigate whether these are reflective of results found in humans and human\ncorpora (Gries and Kootstra, 2017). We make use of the structural priming\nparadigm, where recent exposure to a structure facilitates processing of the\nsame structure. We don't only investigate whether, but also where priming\neffects occur, and what factors predict them. We show that these effects can be\nexplained via the inverse frequency effect, known in human priming, where rarer\nelements within a prime increase priming effects, as well as lexical dependence\nbetween prime and target. Our results provide an important piece in the puzzle\nof understanding how properties within their context affect structural\nprediction in language models.", "published": "2024-06-07 11:21:52", "link": "http://arxiv.org/abs/2406.04847v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncertainty Aware Learning for Language Model Alignment", "abstract": "As instruction-tuned large language models (LLMs) evolve, aligning pretrained\nfoundation models presents increasing challenges. Existing alignment\nstrategies, which typically leverage diverse and high-quality data sources,\noften overlook the intrinsic uncertainty of tasks, learning all data samples\nequally. This may lead to suboptimal data efficiency and model performance. In\nresponse, we propose uncertainty-aware learning (UAL) to improve the model\nalignment of different task scenarios, by introducing the sample uncertainty\n(elicited from more capable LLMs). We implement UAL in a simple fashion --\nadaptively setting the label smoothing value of training according to the\nuncertainty of individual samples. Analysis shows that our UAL indeed\nfacilitates better token clustering in the feature space, validating our\nhypothesis. Extensive experiments on widely used benchmarks demonstrate that\nour UAL significantly and consistently outperforms standard supervised\nfine-tuning. Notably, LLMs aligned in a mixed scenario have achieved an average\nimprovement of 10.62\\% on high-entropy tasks (i.e., AlpacaEval leaderboard),\nand 1.81\\% on complex low-entropy tasks (i.e., MetaMath and GSM8K).", "published": "2024-06-07 11:37:45", "link": "http://arxiv.org/abs/2406.04854v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Russian Legislative Corpus", "abstract": "We present the comprehensive Russian primary and secondary legislation corpus\ncovering 1991 to 2023. The corpus collects all 281,413 texts (176,523,268\ntokens) of non-secret federal regulations and acts, along with their metadata.\nThe corpus has two versions the original text with minimal preprocessing and a\nversion prepared for linguistic analysis with morphosyntactic markup.", "published": "2024-06-07 11:38:12", "link": "http://arxiv.org/abs/2406.04855v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ComplexTempQA: A Large-Scale Dataset for Complex Temporal Question\n  Answering", "abstract": "We introduce ComplexTempQA, a large-scale dataset consisting of over 100\nmillion question-answer pairs designed to tackle the challenges in temporal\nquestion answering. ComplexTempQA significantly surpasses existing benchmarks\nlike HOTPOTQA, TORQUE, and TEQUILA in scale and scope. Utilizing data from\nWikipedia and Wikidata, the dataset covers questions spanning over two decades\nand offers an unmatched breadth of topics. We introduce a unique taxonomy that\ncategorizes questions as attributes, comparisons, and counting questions, each\nrevolving around events, entities, and time periods. One standout feature of\nComplexTempQA is the high complexity of its questions, which demand effective\ncapabilities for answering such as across-time comparison, temporal\naggregation, and multi-hop reasoning involving temporal event ordering and\nentity recognition. Additionally, each question is accompanied by detailed\nmetadata, including specific time scopes, allowing for comprehensive evaluation\nand enhancement of the temporal reasoning abilities of large language models.\nComplexTempQA serves both as a testing ground for developing sophisticated AI\nmodels and as a foundation for advancing research in question answering,\ninformation retrieval, and language understanding.", "published": "2024-06-07 12:01:59", "link": "http://arxiv.org/abs/2406.04866v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HateDebias: On the Diversity and Variability of Hate Speech Debiasing", "abstract": "Hate speech on social media is ubiquitous but urgently controlled. Without\ndetecting and mitigating the biases brought by hate speech, different types of\nethical problems. While a number of datasets have been proposed to address the\nproblem of hate speech detection, these datasets seldom consider the diversity\nand variability of bias, making it far from real-world scenarios. To fill this\ngap, we propose a benchmark, named HateDebias, to analyze the model ability of\nhate speech detection under continuous, changing environments. Specifically, to\nmeet the diversity of biases, we collect existing hate speech detection\ndatasets with different types of biases. To further meet the variability (i.e.,\nthe changing of bias attributes in datasets), we reorganize datasets to follow\nthe continuous learning setting. We evaluate the detection accuracy of models\ntrained on the datasets with a single type of bias with the performance on the\nHateDebias, where a significant performance drop is observed. To provide a\npotential direction for debiasing, we further propose a debiasing framework\nbased on continuous learning and bias information regularization, as well as\nthe memory replay strategies to ensure the debiasing ability of the model.\nExperiment results on the proposed benchmark show that the aforementioned\nmethod can improve several baselines with a distinguished margin, highlighting\nits effectiveness in real-world applications.", "published": "2024-06-07 12:18:02", "link": "http://arxiv.org/abs/2406.04876v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Deep Dive into the Trade-Offs of Parameter-Efficient Preference\n  Alignment Techniques", "abstract": "Large language models are first pre-trained on trillions of tokens and then\ninstruction-tuned or aligned to specific preferences. While pre-training\nremains out of reach for most researchers due to the compute required,\nfine-tuning has become affordable thanks to parameter-efficient methods such as\nLoRA and QLoRA. Alignment is known to be sensitive to the many factors\ninvolved, including the quantity and quality of data, the alignment method, and\nthe adapter rank. However, there has not yet been an extensive study of their\neffect on downstream performance. To address this gap, we conduct an in-depth\ninvestigation of the impact of popular choices for three crucial axes: (i) the\nalignment dataset (HH-RLHF and BeaverTails), (ii) the alignment technique (SFT\nand DPO), and (iii) the model (LLaMA-1, Vicuna-v1.3, Mistral-7b, and\nMistral-7b-Instruct). Our extensive setup spanning over 300 experiments reveals\nconsistent trends and unexpected findings. We observe how more informative data\nhelps with preference alignment, cases where supervised fine-tuning outperforms\npreference optimization, and how aligning to a distinct preference boosts\nperformance on downstream tasks. Through our in-depth analyses, we put forward\nkey guidelines to help researchers perform more effective parameter-efficient\nLLM alignment.", "published": "2024-06-07 12:25:51", "link": "http://arxiv.org/abs/2406.04879v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sexism Detection on a Data Diet", "abstract": "There is an increase in the proliferation of online hate commensurate with\nthe rise in the usage of social media. In response, there is also a significant\nadvancement in the creation of automated tools aimed at identifying harmful\ntext content using approaches grounded in Natural Language Processing and Deep\nLearning. Although it is known that training Deep Learning models require a\nsubstantial amount of annotated data, recent line of work suggests that models\ntrained on specific subsets of the data still retain performance comparable to\nthe model that was trained on the full dataset. In this work, we show how we\ncan leverage influence scores to estimate the importance of a data point while\ntraining a model and designing a pruning strategy applied to the case of sexism\ndetection. We evaluate the model performance trained on data pruned with\ndifferent pruning strategies on three out-of-domain datasets and find, that in\naccordance with other work a large fraction of instances can be removed without\nsignificant performance drop. However, we also discover that the strategies for\npruning data, previously successful in Natural Language Inference tasks, do not\nreadily apply to the detection of harmful content and instead amplify the\nalready prevalent class imbalance even more, leading in the worst-case to a\ncomplete absence of the hateful class.", "published": "2024-06-07 12:39:54", "link": "http://arxiv.org/abs/2406.04892v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TCMD: A Traditional Chinese Medicine QA Dataset for Evaluating Large\n  Language Models", "abstract": "The recently unprecedented advancements in Large Language Models (LLMs) have\npropelled the medical community by establishing advanced medical-domain models.\nHowever, due to the limited collection of medical datasets, there are only a\nfew comprehensive benchmarks available to gauge progress in this area. In this\npaper, we introduce a new medical question-answering (QA) dataset that contains\nmassive manual instruction for solving Traditional Chinese Medicine examination\ntasks, called TCMD. Specifically, our TCMD collects massive questions across\ndiverse domains with their annotated medical subjects and thus supports us in\ncomprehensively assessing the capability of LLMs in the TCM domain. Extensive\nevaluation of various general LLMs and medical-domain-specific LLMs is\nconducted. Moreover, we also analyze the robustness of current LLMs in solving\nTCM QA tasks by introducing randomness. The inconsistency of the experimental\nresults also reveals the shortcomings of current LLMs in solving QA tasks. We\nalso expect that our dataset can further facilitate the development of LLMs in\nthe TCM area.", "published": "2024-06-07 13:48:15", "link": "http://arxiv.org/abs/2406.04941v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common\n  Sense", "abstract": "This paper outlines our approach to SemEval 2024 Task 9, BRAINTEASER: A Novel\nTask Defying Common Sense. The task aims to evaluate the ability of language\nmodels to think creatively. The dataset comprises multi-choice questions that\nchallenge models to think \"outside of the box\". We fine-tune 2 models, BERT and\nRoBERTa Large. Next, we employ a Chain of Thought (CoT) zero-shot prompting\napproach with 6 large language models, such as GPT-3.5, Mixtral, and Llama2.\nFinally, we utilize ReConcile, a technique that employs a \"round table\nconference\" approach with multiple agents for zero-shot learning, to generate\nconsensus answers among 3 selected language models. Our best method achieves an\noverall accuracy of 85 percent on the sentence puzzles subtask.", "published": "2024-06-07 14:01:56", "link": "http://arxiv.org/abs/2406.04947v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MEFT: Memory-Efficient Fine-Tuning through Sparse Adapter", "abstract": "Parameter-Efficient Fine-tuning (PEFT) facilitates the fine-tuning of Large\nLanguage Models (LLMs) under limited resources. However, the fine-tuning\nperformance with PEFT on complex, knowledge-intensive tasks is limited due to\nthe constrained model capacity, which originates from the limited number of\nadditional trainable parameters. To overcome this limitation, we introduce a\nnovel mechanism that fine-tunes LLMs with adapters of larger size yet\nmemory-efficient. This is achieved by leveraging the inherent activation\nsparsity in the Feed-Forward Networks (FFNs) of LLMs and utilizing the larger\ncapacity of Central Processing Unit (CPU) memory compared to Graphics\nProcessing Unit (GPU). We store and update the parameters of larger adapters on\nthe CPU. Moreover, we employ a Mixture of Experts (MoE)-like architecture to\nmitigate unnecessary CPU computations and reduce the communication volume\nbetween the GPU and CPU. This is particularly beneficial over the limited\nbandwidth of PCI Express (PCIe). Our method can achieve fine-tuning results\ncomparable to those obtained with larger memory capacities, even when operating\nunder more limited resources such as a 24GB memory single GPU setup, with\nacceptable loss in training efficiency. Our codes are available at\nhttps://github.com/CURRENTF/MEFT.", "published": "2024-06-07 14:49:22", "link": "http://arxiv.org/abs/2406.04984v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language models emulate certain cognitive profiles: An investigation of\n  how predictability measures interact with individual differences", "abstract": "To date, most investigations on surprisal and entropy effects in reading have\nbeen conducted on the group level, disregarding individual differences. In this\nwork, we revisit the predictive power of surprisal and entropy measures\nestimated from a range of language models (LMs) on data of human reading times\nas a measure of processing effort by incorporating information of language\nusers' cognitive capacities. To do so, we assess the predictive power of\nsurprisal and entropy estimated from generative LMs on reading data obtained\nfrom individuals who also completed a wide range of psychometric tests.\nSpecifically, we investigate if modulating surprisal and entropy relative to\ncognitive scores increases prediction accuracy of reading times, and we examine\nwhether LMs exhibit systematic biases in the prediction of reading times for\ncognitively high- or low-performing groups, revealing what type of\npsycholinguistic subject a given LM emulates. Our study finds that in most\ncases, incorporating cognitive capacities increases predictive power of\nsurprisal and entropy on reading times, and that generally, high performance in\nthe psychometric tests is associated with lower sensitivity to predictability\neffects. Finally, our results suggest that the analyzed LMs emulate readers\nwith lower verbal intelligence, suggesting that for a given target group (i.e.,\nindividuals with high verbal intelligence), these LMs provide less accurate\npredictability estimates.", "published": "2024-06-07 14:54:56", "link": "http://arxiv.org/abs/2406.04988v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compositional Generalization with Grounded Language Models", "abstract": "Grounded language models use external sources of information, such as\nknowledge graphs, to meet some of the general challenges associated with\npre-training. By extending previous work on compositional generalization in\nsemantic parsing, we allow for a controlled evaluation of the degree to which\nthese models learn and generalize from patterns in knowledge graphs. We develop\na procedure for generating natural language questions paired with knowledge\ngraphs that targets different aspects of compositionality and further avoids\ngrounding the language models in information already encoded implicitly in\ntheir weights. We evaluate existing methods for combining language models with\nknowledge graphs and find them to struggle with generalization to sequences of\nunseen lengths and to novel combinations of seen base components. While our\nexperimental results provide some insight into the expressive power of these\nmodels, we hope our work and released datasets motivate future research on how\nto better combine language models with structured knowledge representations.", "published": "2024-06-07 14:56:51", "link": "http://arxiv.org/abs/2406.04989v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models More Empathetic than Humans?", "abstract": "With the emergence of large language models (LLMs), investigating if they can\nsurpass humans in areas such as emotion recognition and empathetic responding\nhas become a focal point of research. This paper presents a comprehensive study\nexploring the empathetic responding capabilities of four state-of-the-art LLMs:\nGPT-4, LLaMA-2-70B-Chat, Gemini-1.0-Pro, and Mixtral-8x7B-Instruct in\ncomparison to a human baseline. We engaged 1,000 participants in a\nbetween-subjects user study, assessing the empathetic quality of responses\ngenerated by humans and the four LLMs to 2,000 emotional dialogue prompts\nmeticulously selected to cover a broad spectrum of 32 distinct positive and\nnegative emotions. Our findings reveal a statistically significant superiority\nof the empathetic responding capability of LLMs over humans. GPT-4 emerged as\nthe most empathetic, marking approximately 31% increase in responses rated as\n\"Good\" compared to the human benchmark. It was followed by LLaMA-2,\nMixtral-8x7B, and Gemini-Pro, which showed increases of approximately 24%, 21%,\nand 10% in \"Good\" ratings, respectively. We further analyzed the response\nratings at a finer granularity and discovered that some LLMs are significantly\nbetter at responding to specific emotions compared to others. The suggested\nevaluation framework offers a scalable and adaptable approach for assessing the\nempathy of new LLMs, avoiding the need to replicate this study's findings in\nfuture research.", "published": "2024-06-07 16:33:43", "link": "http://arxiv.org/abs/2406.05063v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal\n  Large Language Models", "abstract": "Multimodal large language models (MLLMs) fine-tuned with multimodal\ninstruction datasets have demonstrated remarkable capabilities in multimodal\ntasks. However, fine-tuning all parameters of MLLMs has become challenging as\nthey usually contain billions of parameters. To address this issue, we study\nparameter-efficient fine-tuning (PEFT) methods for MLLMs. We aim to identify\neffective methods for enhancing the performance of MLLMs in scenarios where\nonly a limited number of parameters are trained. This paper conducts empirical\nstudies using four popular PEFT methods to fine-tune the LLM component of\nopen-source MLLMs. We present a comprehensive analysis that encompasses various\naspects, including the impact of PEFT methods on various models, parameters and\nlocation of the PEFT module, size of fine-tuning data, model stability based on\nPEFT methods, MLLM's generalization, and hallucination. We evaluated four PEFT\nmethods on seven datasets from two different categories: unseen and seen\ndatasets. Across all experiments, we show that the adapter is the\nbest-performing PEFT method. At the same time, fine-tuning the connector layers\nleads to improved performance in most MLLMs. Code and data are available at\nhttps://github.com/alenai97/PEFT-MLLM.git.", "published": "2024-06-07 17:58:11", "link": "http://arxiv.org/abs/2406.05130v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Correlation Does Not Imply Compensation: Complexity and Irregularity in\n  the Lexicon", "abstract": "It has been claimed that within a language, morphologically irregular words\nare more likely to be phonotactically simple and morphologically regular words\nare more likely to be phonotactically complex. This inverse correlation has\nbeen demonstrated in English for a small sample of words, but has yet to be\nshown for a larger sample of languages. Furthermore, frequency and word length\nare known to influence both phonotactic complexity and morphological\nirregularity, and they may be confounding factors in this relationship.\nTherefore, we examine the relationships between all pairs of these four\nvariables both to assess the robustness of previous findings using improved\nmethodology and as a step towards understanding the underlying causal\nrelationship. Using information-theoretic measures of phonotactic complexity\nand morphological irregularity (Pimentel et al., 2020; Wu et al., 2019) on 25\nlanguages from UniMorph, we find that there is evidence of a positive\nrelationship between morphological irregularity and phonotactic complexity\nwithin languages on average, although the direction varies within individual\nlanguages. We also find weak evidence of a negative relationship between word\nlength and morphological irregularity that had not been previously identified,\nand that some existing findings about the relationships between these four\nvariables are not as robust as previously thought.", "published": "2024-06-07 18:09:21", "link": "http://arxiv.org/abs/2406.05186v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Creating an AI Observer: Generative Semantic Workspaces", "abstract": "An experienced human Observer reading a document -- such as a crime report --\ncreates a succinct plot-like $\\textit{``Working Memory''}$ comprising different\nactors, their prototypical roles and states at any point, their evolution over\ntime based on their interactions, and even a map of missing Semantic parts\nanticipating them in the future. $\\textit{An equivalent AI Observer currently\ndoes not exist}$. We introduce the $\\textbf{[G]}$enerative\n$\\textbf{[S]}$emantic $\\textbf{[W]}$orkspace (GSW) -- comprising an\n$\\textit{``Operator''}$ and a $\\textit{``Reconciler''}$ -- that leverages\nadvancements in LLMs to create a generative-style Semantic framework, as\nopposed to a traditionally predefined set of lexicon labels. Given a text\nsegment $C_n$ that describes an ongoing situation, the $\\textit{Operator}$\ninstantiates actor-centric Semantic maps (termed ``Workspace instance''\n$\\mathcal{W}_n$). The $\\textit{Reconciler}$ resolves differences between\n$\\mathcal{W}_n$ and a ``Working memory'' $\\mathcal{M}_n^*$ to generate the\nupdated $\\mathcal{M}_{n+1}^*$. GSW outperforms well-known baselines on several\ntasks ($\\sim 94\\%$ vs. FST, GLEN, BertSRL - multi-sentence Semantics\nextraction, $\\sim 15\\%$ vs. NLI-BERT, $\\sim 35\\%$ vs. QA). By mirroring the\nreal Observer, GSW provides the first step towards Spatial Computing assistants\ncapable of understanding individual intentions and predicting future behavior.", "published": "2024-06-07 00:09:13", "link": "http://arxiv.org/abs/2406.04555v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Task Decomposition to Assist Humans in Competitive Programming", "abstract": "When using language models (LMs) to solve complex problems, humans might\nstruggle to understand the LM-generated solutions and repair the flawed ones.\nTo assist humans in repairing them, we propose to automatically decompose\ncomplex solutions into multiple simpler pieces that correspond to specific\nsubtasks. We introduce a novel objective for learning task decomposition,\ntermed assistive value (AssistV), which measures the feasibility and speed for\nhumans to repair the decomposed solution. We collect a dataset of human repair\nexperiences on different decomposed solutions. Utilizing the collected data as\nin-context examples, we then learn to critique, refine, and rank decomposed\nsolutions to improve AssistV. We validate our method under competitive\nprogramming problems: under 177 hours of human study, our method enables\nnon-experts to solve 33.3\\% more problems, speeds them up by 3.3x, and empowers\nthem to match unassisted experts.", "published": "2024-06-07 03:27:51", "link": "http://arxiv.org/abs/2406.04604v4", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "LawGPT: A Chinese Legal Knowledge-Enhanced Large Language Model", "abstract": "Large language models (LLMs), including both proprietary and open-source\nmodels, have showcased remarkable capabilities in addressing a wide range of\ndownstream tasks. Nonetheless, when it comes to practical Chinese legal tasks,\nthese models fail to meet the actual requirements. Proprietary models do not\nensure data privacy for sensitive legal cases, while open-source models\ndemonstrate unsatisfactory performance due to their lack of legal knowledge. To\naddress this problem, we introduce LawGPT, the first open-source model\nspecifically designed for Chinese legal applications. LawGPT comprises two key\ncomponents: legal-oriented pre-training and legal supervised fine-tuning.\nSpecifically, we employ large-scale Chinese legal documents for legal-oriented\npre-training to incorporate legal domain knowledge. To further improve the\nmodel's performance on downstream legal tasks, we create a knowledge-driven\ninstruction dataset for legal supervised fine-tuning. Our experimental results\ndemonstrate that LawGPT outperforms the open-source LLaMA 7B model. Our code\nand resources are publicly available at https://github.com/pengxiao-song/LaWGPT\nand have received 5.7K stars on GitHub.", "published": "2024-06-07 03:52:56", "link": "http://arxiv.org/abs/2406.04614v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Key-Element-Informed sLLM Tuning for Document Summarization", "abstract": "Remarkable advances in large language models (LLMs) have enabled high-quality\ntext summarization. However, this capability is currently accessible only\nthrough LLMs of substantial size or proprietary LLMs with usage fees. In\nresponse, smaller-scale LLMs (sLLMs) of easy accessibility and low costs have\nbeen extensively studied, yet they often suffer from missing key information\nand entities, i.e., low relevance, in particular, when input documents are\nlong. We hence propose a key-element-informed instruction tuning for\nsummarization, so-called KEITSum, which identifies key elements in documents\nand instructs sLLM to generate summaries capturing these key elements.\nExperimental results on dialogue and news datasets demonstrate that sLLM with\nKEITSum indeed provides high-quality summarization with higher relevance and\nless hallucinations, competitive to proprietary LLM.", "published": "2024-06-07 04:19:01", "link": "http://arxiv.org/abs/2406.04625v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MATTER: Memory-Augmented Transformer Using Heterogeneous Knowledge\n  Sources", "abstract": "Leveraging external knowledge is crucial for achieving high performance in\nknowledge-intensive tasks, such as question answering. The retrieve-and-read\napproach is widely adopted for integrating external knowledge into a language\nmodel. However, this approach suffers from increased computational cost and\nlatency due to the long context length, which grows proportionally with the\nnumber of retrieved knowledge. Furthermore, existing retrieval-augmented models\ntypically retrieve information from a single type of knowledge source, limiting\ntheir scalability to diverse knowledge sources with varying structures. In this\nwork, we introduce an efficient memory-augmented transformer called MATTER,\ndesigned to retrieve relevant knowledge from multiple heterogeneous knowledge\nsources. Specifically, our model retrieves and reads from both unstructured\nsources (paragraphs) and semi-structured sources (QA pairs) in the form of\nfixed-length neural memories. We demonstrate that our model outperforms\nexisting efficient retrieval-augmented models on popular QA benchmarks in terms\nof both accuracy and speed. Furthermore, MATTER achieves competitive results\ncompared to conventional read-and-retrieve models while having 100x throughput\nduring inference.", "published": "2024-06-07 06:35:37", "link": "http://arxiv.org/abs/2406.04670v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in\n  the Wild", "abstract": "We introduce WildBench, an automated evaluation framework designed to\nbenchmark large language models (LLMs) using challenging, real-world user\nqueries. WildBench consists of 1,024 tasks carefully selected from over one\nmillion human-chatbot conversation logs. For automated evaluation with\nWildBench, we have developed two metrics, WB-Reward and WB-Score, which are\ncomputable using advanced LLMs such as GPT-4-turbo. WildBench evaluation uses\ntask-specific checklists to evaluate model outputs systematically and provides\nstructured explanations that justify the scores and comparisons, resulting in\nmore reliable and interpretable automatic judgments. WB-Reward employs\nfine-grained pairwise comparisons between model responses, generating five\npotential outcomes: much better, slightly better, slightly worse, much worse,\nor a tie. Unlike previous evaluations that employed a single baseline model, we\nselected three baseline models at varying performance levels to ensure a\ncomprehensive pairwise evaluation. Additionally, we propose a simple method to\nmitigate length bias, by converting outcomes of ``slightly better/worse'' to\n``tie'' if the winner response exceeds the loser one by more than $K$\ncharacters. WB-Score evaluates the quality of model outputs individually,\nmaking it a fast and cost-efficient evaluation metric. WildBench results\ndemonstrate a strong correlation with the human-voted Elo ratings from Chatbot\nArena on hard tasks. Specifically, WB-Reward achieves a Pearson correlation of\n0.98 with top-ranking models. Additionally, WB-Score reaches 0.95, surpassing\nboth ArenaHard's 0.91 and AlpacaEval2.0's 0.89 for length-controlled win rates,\nas well as the 0.87 for regular win rates.", "published": "2024-06-07 09:15:44", "link": "http://arxiv.org/abs/2406.04770v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SelfGoal: Your Language Agents Already Know How to Achieve High-level\n  Goals", "abstract": "Language agents powered by large language models (LLMs) are increasingly\nvaluable as decision-making tools in domains such as gaming and programming.\nHowever, these agents often face challenges in achieving high-level goals\nwithout detailed instructions and in adapting to environments where feedback is\ndelayed. In this paper, we present SelfGoal, a novel automatic approach\ndesigned to enhance agents' capabilities to achieve high-level goals with\nlimited human prior and environmental feedback. The core concept of SelfGoal\ninvolves adaptively breaking down a high-level goal into a tree structure of\nmore practical subgoals during the interaction with environments while\nidentifying the most useful subgoals and progressively updating this structure.\nExperimental results demonstrate that SelfGoal significantly enhances the\nperformance of language agents across various tasks, including competitive,\ncooperative, and deferred feedback environments. Project page:\nhttps://selfgoal-agent.github.io.", "published": "2024-06-07 09:32:03", "link": "http://arxiv.org/abs/2406.04784v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero, Finite, and Infinite Belief History of Theory of Mind Reasoning in\n  Large Language Models", "abstract": "Large Language Models (LLMs) have recently shown a promise and emergence of\nTheory of Mind (ToM) ability and even outperform humans in certain ToM tasks.\nTo evaluate and extend the boundaries of the ToM reasoning ability of LLMs, we\npropose a novel concept, taxonomy, and framework, the ToM reasoning with Zero,\nFinite, and Infinite Belief History and develop a multi-round text-based game,\ncalled $\\textit{Pick the Right Stuff}$, as a benchmark. We have evaluated six\nLLMs with this game and found their performance on Zero Belief History is\nconsistently better than on Finite Belief History. In addition, we have found\ntwo of the models with small parameter sizes outperform all the evaluated\nmodels with large parameter sizes. We expect this work to pave the way for\nfuture ToM benchmark development and also for the promotion and development of\nmore complex AI agents or systems which are required to be equipped with more\ncomplex ToM reasoning ability.", "published": "2024-06-07 10:04:39", "link": "http://arxiv.org/abs/2406.04800v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "BERTs are Generative In-Context Learners", "abstract": "While in-context learning is commonly associated with causal language models,\nsuch as GPT, we demonstrate that this capability also 'emerges' in masked\nlanguage models. Through an embarrassingly simple inference technique, we\nenable an existing masked model, DeBERTa, to perform generative tasks without\nadditional training or architectural changes. Our evaluation reveals that the\nmasked and causal language models behave very differently, as they clearly\noutperform each other on different categories of tasks. These complementary\nstrengths suggest that the field's focus on causal models for in-context\nlearning may be limiting - both architectures can develop these capabilities,\nbut with distinct advantages; pointing toward promising hybrid approaches that\ncombine the strengths of both objectives.", "published": "2024-06-07 10:48:45", "link": "http://arxiv.org/abs/2406.04823v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Revisiting Catastrophic Forgetting in Large Language Model Tuning", "abstract": "Catastrophic Forgetting (CF) means models forgetting previously acquired\nknowledge when learning new data. It compromises the effectiveness of large\nlanguage models (LLMs) during fine-tuning, yet the underlying causes have not\nbeen thoroughly investigated. This paper takes the first step to reveal the\ndirect link between the flatness of the model loss landscape and the extent of\nCF in the field of LLMs. Based on this, we introduce the sharpness-aware\nminimization to mitigate CF by flattening the loss landscape. Experiments on\nthree widely-used fine-tuning datasets, spanning different model scales,\ndemonstrate the effectiveness of our method in alleviating CF. Analyses show\nthat we nicely complement the existing anti-forgetting strategies, further\nenhancing the resistance of LLMs to CF.", "published": "2024-06-07 11:09:13", "link": "http://arxiv.org/abs/2406.04836v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Through the Thicket: A Study of Number-Oriented LLMs derived from Random\n  Forest Models", "abstract": "Large Language Models (LLMs) have shown exceptional performance in text\nprocessing. Notably, LLMs can synthesize information from large datasets and\nexplain their decisions similarly to human reasoning through a chain of thought\n(CoT). An emerging application of LLMs is the handling and interpreting of\nnumerical data, where fine-tuning enhances their performance over basic\ninference methods. This paper proposes a novel approach to training LLMs using\nknowledge transfer from a random forest (RF) ensemble, leveraging its\nefficiency and accuracy. By converting RF decision paths into natural language\nstatements, we generate outputs for LLM fine-tuning, enhancing the model's\nability to classify and explain its decisions. Our method includes verifying\nthese rules through established classification metrics, ensuring their\ncorrectness. We also examine the impact of preprocessing techniques on the\nrepresentation of numerical data and their influence on classification accuracy\nand rule correctness", "published": "2024-06-07 13:31:51", "link": "http://arxiv.org/abs/2406.04926v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM-based speaker diarization correction: A generalizable approach", "abstract": "Speaker diarization is necessary for interpreting conversations transcribed\nusing automated speech recognition (ASR) tools. Despite significant\ndevelopments in diarization methods, diarization accuracy remains an issue.\nHere, we investigate the use of large language models (LLMs) for diarization\ncorrection as a post-processing step. LLMs were fine-tuned using the Fisher\ncorpus, a large dataset of transcribed conversations. The ability of the models\nto improve diarization accuracy in a holdout dataset from the Fisher corpus as\nwell as an independent dataset was measured. We report that fine-tuned LLMs can\nmarkedly improve diarization accuracy. However, model performance is\nconstrained to transcripts produced using the same ASR tool as the transcripts\nused for fine-tuning, limiting generalizability. To address this constraint, an\nensemble model was developed by combining weights from three separate models,\neach fine-tuned using transcripts from a different ASR tool. The ensemble model\ndemonstrated better overall performance than each of the ASR-specific models,\nsuggesting that a generalizable and ASR-agnostic approach may be achievable. We\nhave made the weights of these models publicly available on HuggingFace at\nhttps://huggingface.co/bklynhlth.", "published": "2024-06-07 13:33:22", "link": "http://arxiv.org/abs/2406.04927v3", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Quantifying Geospatial in the Common Crawl Corpus", "abstract": "Large language models (LLMs) exhibit emerging geospatial capabilities,\nstemming from their pre-training on vast unlabelled text datasets that are\noften derived from the Common Crawl (CC) corpus. However, the geospatial\ncontent within CC remains largely unexplored, impacting our understanding of\nLLMs' spatial reasoning. This paper investigates the prevalence of geospatial\ndata in recent Common Crawl releases using Gemini 1.5, a powerful language\nmodel. By analyzing a sample of documents and manually revising the results, we\nestimate that 18.7% of web documents in CC contain geospatial information such\nas coordinates and addresses. We find little difference in prevalence between\nEnlgish- and non-English-language documents. Our findings provide quantitative\ninsights into the nature and extent of geospatial data in CC, and lay the\ngroundwork for future studies of geospatial biases of LLMs.", "published": "2024-06-07 14:16:37", "link": "http://arxiv.org/abs/2406.04952v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CHIQ: Contextual History Enhancement for Improving Query Rewriting in\n  Conversational Search", "abstract": "In this paper, we study how open-source large language models (LLMs) can be\neffectively deployed for improving query rewriting in conversational search,\nespecially for ambiguous queries. We introduce CHIQ, a two-step method that\nleverages the capabilities of LLMs to resolve ambiguities in the conversation\nhistory before query rewriting. This approach contrasts with prior studies that\npredominantly use closed-source LLMs to directly generate search queries from\nconversation history. We demonstrate on five well-established benchmarks that\nCHIQ leads to state-of-the-art results across most settings, showing highly\ncompetitive performances with systems leveraging closed-source LLMs. Our study\nprovides a first step towards leveraging open-source LLMs in conversational\nsearch, as a competitive alternative to the prevailing reliance on commercial\nLLMs. Data, models, and source code will be publicly available upon acceptance\nat https://github.com/fengranMark/CHIQ.", "published": "2024-06-07 15:23:53", "link": "http://arxiv.org/abs/2406.05013v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Scenarios and Approaches for Situated Natural Language Explanations", "abstract": "Large language models (LLMs) can be used to generate natural language\nexplanations (NLE) that are adapted to different users' situations. However,\nthere is yet to be a quantitative evaluation of the extent of such adaptation.\nTo bridge this gap, we collect a benchmarking dataset, Situation-Based\nExplanation. This dataset contains 100 explanandums. Each explanandum is paired\nwith explanations targeted at three distinct audience types-such as educators,\nstudents, and professionals-enabling us to assess how well the explanations\nmeet the specific informational needs and contexts of these diverse groups e.g.\nstudents, teachers, and parents. For each \"explanandum paired with an audience\"\nsituation, we include a human-written explanation. These allow us to compute\nscores that quantify how the LLMs adapt the explanations to the situations. On\nan array of pretrained language models with varying sizes, we examine three\ncategories of prompting methods: rule-based prompting, meta-prompting, and\nin-context learning prompting. We find that 1) language models can generate\nprompts that result in explanations more precisely aligned with the target\nsituations, 2) explicitly modeling an \"assistant\" persona by prompting \"You are\na helpful assistant...\" is not a necessary prompt technique for situated NLE\ntasks, and 3) the in-context learning prompts only can help LLMs learn the\ndemonstration template but can't improve their inference performance. SBE and\nour analysis facilitate future research towards generating situated natural\nlanguage explanations.", "published": "2024-06-07 15:56:32", "link": "http://arxiv.org/abs/2406.05035v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bootstrapping Referring Multi-Object Tracking", "abstract": "Referring multi-object tracking (RMOT) aims at detecting and tracking\nmultiple objects following human instruction represented by a natural language\nexpression. Existing RMOT benchmarks are usually formulated through manual\nannotations, integrated with static regulations. This approach results in a\ndearth of notable diversity and a constrained scope of implementation. In this\nwork, our key idea is to bootstrap the task of referring multi-object tracking\nby introducing discriminative language words as much as possible. In specific,\nwe first develop Refer-KITTI into a large-scale dataset, named Refer-KITTI-V2.\nIt starts with 2,719 manual annotations, addressing the issue of class\nimbalance and introducing more keywords to make it closer to real-world\nscenarios compared to Refer-KITTI. They are further expanded to a total of\n9,758 annotations by prompting large language models, which create 617\ndifferent words, surpassing previous RMOT benchmarks. In addition, the\nend-to-end framework in RMOT is also bootstrapped by a simple yet elegant\ntemporal advancement strategy, which achieves better performance than previous\napproaches. The source code and dataset is available at\nhttps://github.com/zyn213/TempRMOT.", "published": "2024-06-07 16:02:10", "link": "http://arxiv.org/abs/2406.05039v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SUMIE: A Synthetic Benchmark for Incremental Entity Summarization", "abstract": "No existing dataset adequately tests how well language models can\nincrementally update entity summaries - a crucial ability as these models\nrapidly advance. The Incremental Entity Summarization (IES) task is vital for\nmaintaining accurate, up-to-date knowledge. To address this, we introduce\nSUMIE, a fully synthetic dataset designed to expose real-world IES challenges.\nThis dataset effectively highlights problems like incorrect entity association\nand incomplete information presentation. Unlike common synthetic datasets, ours\ncaptures the complexity and nuances found in real-world data. We generate\ninformative and diverse attributes, summaries, and unstructured paragraphs in\nsequence, ensuring high quality. The alignment between generated summaries and\nparagraphs exceeds 96%, confirming the dataset's quality. Extensive experiments\ndemonstrate the dataset's difficulty - state-of-the-art LLMs struggle to update\nsummaries with an F1 higher than 80.4%. We will open source the benchmark and\nthe evaluation metrics to help the community make progress on IES tasks.", "published": "2024-06-07 16:49:21", "link": "http://arxiv.org/abs/2406.05079v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Ambiguity and the Expressive Function of Law: The Role of Pragmatics\n  in Smart Legal Ecosystems", "abstract": "This is a long paper, an essay, on ambiguity, pragmatics, legal ecosystems,\nand the expressive function of law. It is divided into two parts and fifteen\nsections. The first part (Pragmatics) addresses ambiguity from the perspective\nof linguistic and cognitive pragmatics in the legal field. The second part\n(Computing) deals with this issue from the point of view of human-centered\ndesign and artificial intelligence, specifically focusing on the notion and\nmodelling of rules and what it means to comply with the rules. This is\nnecessary for the scaffolding of smart legal ecosystems (SLE). I will develop\nthis subject with the example of the architecture, information flows, and smart\necosystem of OPTIMAI, an EU project of Industry 4.0 for zero-defect\nmanufacturing (Optimizing Manufacturing Processes through Artificial\nIntelligence and Virtualization).", "published": "2024-06-07 16:58:15", "link": "http://arxiv.org/abs/2406.05084v1", "categories": ["cs.CY", "cs.CL", "C.5; H.1; H.4; I.2; J.4; K.4"], "primary_category": "cs.CY"}
{"title": "DALD: Improving Logits-based Detector without Logits from Black-box LLMs", "abstract": "The advent of Large Language Models (LLMs) has revolutionized text\ngeneration, producing outputs that closely mimic human writing. This blurring\nof lines between machine- and human-written text presents new challenges in\ndistinguishing one from the other a task further complicated by the frequent\nupdates and closed nature of leading proprietary LLMs. Traditional logits-based\ndetection methods leverage surrogate models for identifying LLM-generated\ncontent when the exact logits are unavailable from black-box LLMs. However,\nthese methods grapple with the misalignment between the distributions of the\nsurrogate and the often undisclosed target models, leading to performance\ndegradation, particularly with the introduction of new, closed-source models.\nFurthermore, while current methodologies are generally effective when the\nsource model is identified, they falter in scenarios where the model version\nremains unknown, or the test set comprises outputs from various source models.\nTo address these limitations, we present Distribution-Aligned LLMs Detection\n(DALD), an innovative framework that redefines the state-of-the-art performance\nin black-box text detection even without logits from source LLMs. DALD is\ndesigned to align the surrogate model's distribution with that of unknown\ntarget LLMs, ensuring enhanced detection capability and resilience against\nrapid model iterations with minimal training investment. By leveraging corpus\nsamples from publicly accessible outputs of advanced models such as ChatGPT,\nGPT-4 and Claude-3, DALD fine-tunes surrogate models to synchronize with\nunknown source model distributions effectively.", "published": "2024-06-07 19:38:05", "link": "http://arxiv.org/abs/2406.05232v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generative Explore-Exploit: Training-free Optimization of Generative\n  Recommender Systems using LLM Optimizers", "abstract": "Recommender systems are widely used to suggest engaging content, and Large\nLanguage Models (LLMs) have given rise to generative recommenders. Such systems\ncan directly generate items, including for open-set tasks like question\nsuggestion. While the world knowledge of LLMs enable good recommendations,\nimproving the generated content through user feedback is challenging as\ncontinuously fine-tuning LLMs is prohibitively expensive. We present a\ntraining-free approach for optimizing generative recommenders by connecting\nuser feedback loops to LLM-based optimizers. We propose a generative\nexplore-exploit method that can not only exploit generated items with known\nhigh engagement, but also actively explore and discover hidden population\npreferences to improve recommendation quality. We evaluate our approach on\nquestion generation in two domains (e-commerce and general knowledge), and\nmodel user feedback with Click Through Rate (CTR). Experiments show our\nLLM-based explore-exploit approach can iteratively improve recommendations, and\nconsistently increase CTR. Ablation analysis shows that generative exploration\nis key to learning user preferences, avoiding the pitfalls of greedy\nexploit-only approaches. A human evaluation strongly supports our quantitative\nfindings.", "published": "2024-06-07 20:41:59", "link": "http://arxiv.org/abs/2406.05255v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Behavior Structformer: Learning Players Representations with Structured\n  Tokenization", "abstract": "In this paper, we introduce the Behavior Structformer, a method for modeling\nuser behavior using structured tokenization within a Transformer-based\narchitecture. By converting tracking events into dense tokens, this approach\nenhances model training efficiency and effectiveness. We demonstrate its\nsuperior performance through ablation studies and benchmarking against\ntraditional tabular and semi-structured baselines. The results indicate that\nstructured tokenization with sequential processing significantly improves\nbehavior modeling.", "published": "2024-06-07 21:59:55", "link": "http://arxiv.org/abs/2406.05274v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SuperPos-Prompt: Enhancing Soft Prompt Tuning of Language Models with\n  Superposition of Multi Token Embeddings", "abstract": "Soft prompt tuning techniques have recently gained traction as an effective\nstrategy for the parameter-efficient tuning of pretrained language models,\nparticularly minimizing the required adjustment of model parameters. Despite\ntheir growing use, achieving optimal tuning with soft prompts, especially for\nsmaller datasets, remains a substantial challenge. This study makes two\ncontributions in this domain: (i) we introduce SuperPos-Prompt, a new\nreparameterization technique employing the superposition of multiple pretrained\nvocabulary embeddings to improve the learning of soft prompts. Our experiments\nacross several GLUE and SuperGLUE benchmarks consistently highlight\nSuperPos-Prompt's superiority over Residual Prompt tuning, exhibiting an\naverage score increase of $+6.4$ in T5-Small and $+5.0$ in T5-Base along with a\nfaster convergence. Remarkably, SuperPos-Prompt occasionally outperforms even\nfull fine-tuning methods. (ii) Additionally, we demonstrate enhanced\nperformance and rapid convergence by omitting dropouts from the frozen network,\nyielding consistent improvements across various scenarios and tuning methods.", "published": "2024-06-07 22:18:49", "link": "http://arxiv.org/abs/2406.05279v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents", "abstract": "Large language models have demonstrated remarkable few-shot performance on\nmany natural language understanding tasks. Despite several demonstrations of\nusing large language models in complex, strategic scenarios, there lacks a\ncomprehensive framework for evaluating agents' performance across various types\nof reasoning found in games. To address this gap, we introduce GameBench, a\ncross-domain benchmark for evaluating strategic reasoning abilities of LLM\nagents. We focus on 9 different game environments, where each covers at least\none axis of key reasoning skill identified in strategy games, and select games\nfor which strategy explanations are unlikely to form a significant portion of\nmodels' pretraining corpuses. Our evaluations use GPT-3 and GPT-4 in their base\nform along with two scaffolding frameworks designed to enhance strategic\nreasoning ability: Chain-of-Thought (CoT) prompting and Reasoning Via Planning\n(RAP). Our results show that none of the tested models match human performance,\nand at worst GPT-4 performs worse than random action. CoT and RAP both improve\nscores but not comparable to human levels.", "published": "2024-06-07 00:28:43", "link": "http://arxiv.org/abs/2406.06613v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SpaRC and SpaRP: Spatial Reasoning Characterization and Path Generation\n  for Understanding Spatial Reasoning Capability of Large Language Models", "abstract": "Spatial reasoning is a crucial component of both biological and artificial\nintelligence. In this work, we present a comprehensive study of the capability\nof current state-of-the-art large language models (LLMs) on spatial reasoning.\nTo support our study, we created and contribute a novel Spatial Reasoning\nCharacterization (SpaRC) framework and Spatial Reasoning Paths (SpaRP)\ndatasets, to enable an in-depth understanding of the spatial relations and\ncompositions as well as the usefulness of spatial reasoning chains. We found\nthat all the state-of-the-art LLMs do not perform well on the datasets -- their\nperformances are consistently low across different setups. The spatial\nreasoning capability improves substantially as model sizes scale up. Finetuning\nboth large language models (e.g., Llama-2-70B) and smaller ones (e.g.,\nLlama-2-13B) can significantly improve their F1-scores by 7--32 absolute\npoints. We also found that the top proprietary LLMs still significantly\noutperform their open-source counterparts in topological spatial understanding\nand reasoning.", "published": "2024-06-07 01:06:34", "link": "http://arxiv.org/abs/2406.04566v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pitch-Aware RNN-T for Mandarin Chinese Mispronunciation Detection and\n  Diagnosis", "abstract": "Mispronunciation Detection and Diagnosis (MDD) systems, leveraging Automatic\nSpeech Recognition (ASR), face two main challenges in Mandarin Chinese: 1) The\ntwo-stage models create an information gap between the phoneme or tone\nclassification stage and the MDD stage. 2) The scarcity of Mandarin MDD\ndatasets limits model training. In this paper, we introduce a stateless RNN-T\nmodel for Mandarin MDD, utilizing HuBERT features with pitch embedding through\na Pitch Fusion Block. Our model, trained solely on native speaker data, shows a\n3% improvement in Phone Error Rate and a 7% increase in False Acceptance Rate\nover the state-of-the-art baseline in non-native scenarios", "published": "2024-06-07 02:59:58", "link": "http://arxiv.org/abs/2406.04595v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "What do MLLMs hear? Examining reasoning with text and sound components\n  in Multimodal Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated remarkable reasoning\ncapabilities, notably in connecting ideas and adhering to logical rules to\nsolve problems. These models have evolved to accommodate various data\nmodalities, including sound and images, known as multimodal LLMs (MLLMs), which\nare capable of describing images or sound recordings. Previous work has\ndemonstrated that when the LLM component in MLLMs is frozen, the audio or\nvisual encoder serves to caption the sound or image input facilitating\ntext-based reasoning with the LLM component. We are interested in using the\nLLM's reasoning capabilities in order to facilitate classification. In this\npaper, we demonstrate through a captioning/classification experiment that an\naudio MLLM cannot fully leverage its LLM's text-based reasoning when generating\naudio captions. We also consider how this may be due to MLLMs separately\nrepresenting auditory and textual information such that it severs the reasoning\npathway from the LLM to the audio encoder.", "published": "2024-06-07 03:55:00", "link": "http://arxiv.org/abs/2406.04615v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Generative AI Models: Opportunities and Risks for Industry and\n  Authorities", "abstract": "Generative AI models are capable of performing a wide variety of tasks that\nhave traditionally required creativity and human understanding. During\ntraining, they learn patterns from existing data and can subsequently generate\nnew content such as texts, images, audio, and videos that align with these\npatterns. Due to their versatility and generally high-quality results, they\nrepresent, on the one hand, an opportunity for digitalisation. On the other\nhand, the use of generative AI models introduces novel IT security risks that\nmust be considered as part of a comprehensive analysis of the IT security\nthreat landscape. In response to this risk potential, companies or authorities\nintending to use generative AI should conduct an individual risk analysis\nbefore integrating it into their workflows. The same applies to developers and\noperators, as many risks associated with generative AI must be addressed during\ndevelopment or can only be influenced by the operating organisation. Based on\nthis, existing security measures can be adapted, and additional measures\nimplemented.", "published": "2024-06-07 08:34:30", "link": "http://arxiv.org/abs/2406.04734v2", "categories": ["cs.AI", "cs.CL", "cs.CR", "68T50 (Primary), 68M25, 68T07 (Secondary)", "I.2.7; I.2.10"], "primary_category": "cs.AI"}
{"title": "PQPP: A Joint Benchmark for Text-to-Image Prompt and Query Performance\n  Prediction", "abstract": "Text-to-image generation has recently emerged as a viable alternative to\ntext-to-image retrieval, driven by the visually impressive results of\ngenerative diffusion models. Although query performance prediction is an active\nresearch topic in information retrieval, to the best of our knowledge, there is\nno prior study that analyzes the difficulty of queries (referred to as prompts)\nin text-to-image generation, based on human judgments. To this end, we\nintroduce the first dataset of prompts which are manually annotated in terms of\nimage generation performance. Additionally, we extend these evaluations to\ntext-to-image retrieval by collecting manual annotations that represent\nretrieval performance. We thus establish the first joint benchmark for prompt\nand query performance prediction (PQPP) across both tasks, comprising over 10K\nqueries. Our benchmark enables (i) the comparative assessment of prompt/query\ndifficulty in both image generation and image retrieval, and (ii) the\nevaluation of prompt/query performance predictors addressing both generation\nand retrieval. We evaluate several pre- and post-generation/retrieval\nperformance predictors, thus providing competitive baselines for future\nresearch. Our benchmark and code are publicly available at\nhttps://github.com/Eduard6421/PQPP.", "published": "2024-06-07 08:46:19", "link": "http://arxiv.org/abs/2406.04746v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Digital assistant in a point of sales", "abstract": "This article investigates the deployment of a Voice User Interface\n(VUI)-powered digital assistant in a retail setting and assesses its impact on\ncustomer engagement and service efficiency. The study explores how digital\nassistants can enhance user interactions through advanced conversational\ncapabilities with multilingual support. By integrating a digital assistant into\na high-traffic retail environment, we evaluate its effectiveness in improving\nthe quality of customer service and operational efficiency. Data collected\nduring the experiment demonstrate varied impacts on customer interaction,\nrevealing insights into the future optimizations of digital assistant\ntechnologies in customer-facing roles. This study contributes to the\nunderstanding of digital transformation strategies within the customer\nrelations domain emphasizing the need for service flexibility and user-centric\ndesign in modern retail stores.", "published": "2024-06-07 11:33:21", "link": "http://arxiv.org/abs/2406.04851v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "InstructNav: Zero-shot System for Generic Instruction Navigation in\n  Unexplored Environment", "abstract": "Enabling robots to navigate following diverse language instructions in\nunexplored environments is an attractive goal for human-robot interaction.\nHowever, this goal is challenging because different navigation tasks require\ndifferent strategies. The scarcity of instruction navigation data hinders\ntraining an instruction navigation model with varied strategies. Therefore,\nprevious methods are all constrained to one specific type of navigation\ninstruction. In this work, we propose InstructNav, a generic instruction\nnavigation system. InstructNav makes the first endeavor to handle various\ninstruction navigation tasks without any navigation training or pre-built maps.\nTo reach this goal, we introduce Dynamic Chain-of-Navigation (DCoN) to unify\nthe planning process for different types of navigation instructions.\nFurthermore, we propose Multi-sourced Value Maps to model key elements in\ninstruction navigation so that linguistic DCoN planning can be converted into\nrobot actionable trajectories. With InstructNav, we complete the R2R-CE task in\na zero-shot way for the first time and outperform many task-training methods.\nBesides, InstructNav also surpasses the previous SOTA method by 10.48% on the\nzero-shot Habitat ObjNav and by 86.34% on demand-driven navigation DDN. Real\nrobot experiments on diverse indoor scenes further demonstrate our method's\nrobustness in coping with the environment and instruction variations.", "published": "2024-06-07 12:26:34", "link": "http://arxiv.org/abs/2406.04882v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Unveiling the Invisible: Captioning Videos with Metaphors", "abstract": "Metaphors are a common communication tool used in our day-to-day life. The\ndetection and generation of metaphors in textual form have been studied\nextensively but metaphors in other forms have been under-explored. Recent\nstudies have shown that Vision-Language (VL) models cannot understand visual\nmetaphors in memes and adverts. As of now, no probing studies have been done\nthat involve complex language phenomena like metaphors with videos. Hence, we\nintroduce a new VL task of describing the metaphors present in the videos in\nour work. To facilitate this novel task, we construct and release a manually\ncreated dataset with 705 videos and 2115 human-written captions, along with a\nnew metric called Average Concept Distance (ACD), to automatically evaluate the\ncreativity of the metaphors generated. We also propose a novel low-resource\nvideo metaphor captioning system: GIT-LLaVA, which obtains comparable\nperformance to SoTA video language models on the proposed task. We perform a\ncomprehensive analysis of existing video language models on this task and\npublish our dataset, models, and benchmark results to enable further research.", "published": "2024-06-07 12:32:44", "link": "http://arxiv.org/abs/2406.04886v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "XTTS: a Massively Multilingual Zero-Shot Text-to-Speech Model", "abstract": "Most Zero-shot Multi-speaker TTS (ZS-TTS) systems support only a single\nlanguage. Although models like YourTTS, VALL-E X, Mega-TTS 2, and Voicebox\nexplored Multilingual ZS-TTS they are limited to just a few high/medium\nresource languages, limiting the applications of these models in most of the\nlow/medium resource languages. In this paper, we aim to alleviate this issue by\nproposing and making publicly available the XTTS system. Our method builds upon\nthe Tortoise model and adds several novel modifications to enable multilingual\ntraining, improve voice cloning, and enable faster training and inference. XTTS\nwas trained in 16 languages and achieved state-of-the-art (SOTA) results in\nmost of them.", "published": "2024-06-07 12:56:11", "link": "http://arxiv.org/abs/2406.04904v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "I2EDL: Interactive Instruction Error Detection and Localization", "abstract": "In the Vision-and-Language Navigation in Continuous Environments (VLN-CE)\ntask, the human user guides an autonomous agent to reach a target goal via a\nseries of low-level actions following a textual instruction in natural\nlanguage. However, most existing methods do not address the likely case where\nusers may make mistakes when providing such instruction (e.g. \"turn left\"\ninstead of \"turn right\"). In this work, we address a novel task of Interactive\nVLN in Continuous Environments (IVLN-CE), which allows the agent to interact\nwith the user during the VLN-CE navigation to verify any doubts regarding the\ninstruction errors. We propose an Interactive Instruction Error Detector and\nLocalizer (I2EDL) that triggers the user-agent interaction upon the detection\nof instruction errors during the navigation. We leverage a pre-trained module\nto detect instruction errors and pinpoint them in the instruction by\ncross-referencing the textual input and past observations. In such way, the\nagent is able to query the user for a timely correction, without demanding the\nuser's cognitive load, as we locate the probable errors to a precise part of\nthe instruction. We evaluate the proposed I2EDL on a dataset of instructions\ncontaining errors, and further devise a novel metric, the Success weighted by\nInteraction Number (SIN), to reflect both the navigation performance and the\ninteraction effectiveness. We show how the proposed method can ask focused\nrequests for corrections to the user, which in turn increases the navigation\nsuccess, while minimizing the interactions.", "published": "2024-06-07 16:52:57", "link": "http://arxiv.org/abs/2406.05080v2", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs", "abstract": "Retrieval Augmented Generation (RAG) enhances the abilities of Large Language\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovide more accurate and relevant responses. Existing RAG solutions do not\nfocus on queries that may require fetching multiple documents with\nsubstantially different contents. Such queries occur frequently, but are\nchallenging because the embeddings of these documents may be distant in the\nembedding space, making it hard to retrieve them all. This paper introduces\nMulti-Head RAG (MRAG), a novel scheme designed to address this gap with a\nsimple yet powerful idea: leveraging activations of Transformer's multi-head\nattention layer, instead of the decoder layer, as keys for fetching\nmulti-aspect documents. The driving motivation is that different attention\nheads can learn to capture different data aspects. Harnessing the corresponding\nactivations results in embeddings that represent various facets of data items\nand queries, improving the retrieval accuracy for complex queries. We provide\nan evaluation methodology and metrics, multi-aspect datasets that we release\nonline, and real-world use cases to demonstrate MRAG's effectiveness, showing\nimprovements of up to 20% in relevance over standard RAG baselines. MRAG can be\nseamlessly integrated with existing RAG frameworks and benchmarking tools like\nRAGAS as well as different classes of data stores.", "published": "2024-06-07 16:59:38", "link": "http://arxiv.org/abs/2406.05085v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "The Factorization Curse: Which Tokens You Predict Underlie the Reversal\n  Curse and More", "abstract": "Today's best language models still struggle with hallucinations: factually\nincorrect generations, which impede their ability to reliably retrieve\ninformation seen during training. The reversal curse, where models cannot\nrecall information when probed in a different order than was encountered during\ntraining, exemplifies this in information retrieval. We reframe the reversal\ncurse as a factorization curse - a failure of models to learn the same joint\ndistribution under different factorizations. Through a series of controlled\nexperiments with increasing levels of realism including WikiReversal, a setting\nwe introduce to closely simulate a knowledge intensive finetuning task, we find\nthat the factorization curse is an inherent failure of the next-token\nprediction objective used in popular large language models. Moreover, we\ndemonstrate reliable information retrieval cannot be solved with scale,\nreversed tokens, or even naive bidirectional-attention training. Consequently,\nvarious approaches to finetuning on specialized data would necessarily provide\nmixed results on downstream tasks, unless the model has already seen the right\nsequence of tokens. Across five tasks of varying levels of complexity, our\nresults uncover a promising path forward: factorization-agnostic objectives can\nsignificantly mitigate the reversal curse and hint at improved knowledge\nstorage and planning capabilities.", "published": "2024-06-07 18:00:37", "link": "http://arxiv.org/abs/2406.05183v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Evaluating the Effectiveness of Data Augmentation for Emotion\n  Classification in Low-Resource Settings", "abstract": "Data augmentation has the potential to improve the performance of machine\nlearning models by increasing the amount of training data available. In this\nstudy, we evaluated the effectiveness of different data augmentation techniques\nfor a multi-label emotion classification task using a low-resource dataset. Our\nresults showed that Back Translation outperformed autoencoder-based approaches\nand that generating multiple examples per training instance led to further\nperformance improvement. In addition, we found that Back Translation generated\nthe most diverse set of unigrams and trigrams. These findings demonstrate the\nutility of Back Translation in enhancing the performance of emotion\nclassification models in resource-limited situations.", "published": "2024-06-07 18:13:27", "link": "http://arxiv.org/abs/2406.05190v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLMs Are Not Intelligent Thinkers: Introducing Mathematical Topic Tree\n  Benchmark for Comprehensive Evaluation of LLMs", "abstract": "Large language models (LLMs) demonstrate impressive capabilities in\nmathematical reasoning. However, despite these achievements, current\nevaluations are mostly limited to specific mathematical topics, and it remains\nunclear whether LLMs are genuinely engaging in reasoning. To address these\ngaps, we present the Mathematical Topics Tree (MaTT) benchmark, a challenging\nand structured benchmark that offers 1,958 questions across a wide array of\nmathematical subjects, each paired with a detailed hierarchical chain of\ntopics. Upon assessing different LLMs using the MaTT benchmark, we find that\nthe most advanced model, GPT-4, achieved a mere 54\\% accuracy in a\nmultiple-choice scenario. Interestingly, even when employing Chain-of-Thought\nprompting, we observe mostly no notable improvement. Moreover, LLMs accuracy\ndramatically reduced by up to 24.2 percentage point when the questions were\npresented without providing choices. Further detailed analysis of the LLMs'\nperformance across a range of topics showed significant discrepancy even for\nclosely related subtopics within the same general mathematical area. In an\neffort to pinpoint the reasons behind LLMs performances, we conducted a manual\nevaluation of the completeness and correctness of the explanations generated by\nGPT-4 when choices were available. Surprisingly, we find that in only 53.3\\% of\nthe instances where the model provided a correct answer, the accompanying\nexplanations were deemed complete and accurate, i.e., the model engaged in\ngenuine reasoning.", "published": "2024-06-07 18:21:26", "link": "http://arxiv.org/abs/2406.05194v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Subjective Uncertainty Quantification and Calibration in Natural\n  Language Generation", "abstract": "Applications of large language models often involve the generation of\nfree-form responses, in which case uncertainty quantification becomes\nchallenging. This is due to the need to identify task-specific uncertainties\n(e.g., about the semantics) which appears difficult to define in general cases.\nThis work addresses these challenges from a perspective of Bayesian decision\ntheory, starting from the assumption that our utility is characterized by a\nsimilarity measure that compares a generated response with a hypothetical true\nresponse. We discuss how this assumption enables principled quantification of\nthe model's subjective uncertainty and its calibration. We further derive a\nmeasure for epistemic uncertainty, based on a missing data perspective and its\ncharacterization as an excess risk. The proposed methods can be applied to\nblack-box language models. We illustrate the methods on question answering and\nmachine translation tasks. Our experiments provide a principled evaluation of\ntask-specific calibration, and demonstrate that epistemic uncertainty offers a\npromising deferral strategy for efficient data acquisition in in-context\nlearning.", "published": "2024-06-07 18:54:40", "link": "http://arxiv.org/abs/2406.05213v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A model of early word acquisition based on realistic-scale audiovisual\n  naming events", "abstract": "Infants gradually learn to parse continuous speech into words and connect\nnames with objects, yet the mechanisms behind development of early word\nperception skills remain unknown. We studied the extent to which early words\ncan be acquired through statistical learning from regularities in audiovisual\nsensory input. We simulated word learning in infants up to 12 months of age in\na realistic setting, using a model that solely learns from statistical\nregularities in unannotated raw speech and pixel-level visual input. Crucially,\nthe quantity of object naming events was carefully designed to match that\naccessible to infants of comparable ages. Results show that the model\neffectively learns to recognize words and associate them with corresponding\nvisual objects, with a vocabulary growth rate comparable to that observed in\ninfants. The findings support the viability of general statistical learning for\nearly word perception, demonstrating how learning can operate without assuming\nany prior linguistic capabilities.", "published": "2024-06-07 21:05:59", "link": "http://arxiv.org/abs/2406.05259v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "TLEX: An Efficient Method for Extracting Exact Timelines from TimeML\n  Temporal Graphs", "abstract": "A timeline provides a total ordering of events and times, and is useful for a\nnumber of natural language understanding tasks. However, qualitative temporal\ngraphs that can be derived directly from text -- such as TimeML annotations --\nusually explicitly reveal only partial orderings of events and times. In this\nwork, we apply prior work on solving point algebra problems to the task of\nextracting timelines from TimeML annotated texts, and develop an exact,\nend-to-end solution which we call TLEX (TimeLine EXtraction). TLEX transforms\nTimeML annotations into a collection of timelines arranged in a\ntrunk-and-branch structure. Like what has been done in prior work, TLEX checks\nthe consistency of the temporal graph and solves it; however, it adds two novel\nfunctionalities. First, it identifies specific relations involved in an\ninconsistency (which could then be manually corrected) and, second, TLEX\nperforms a novel identification of sections of the timelines that have\nindeterminate order, information critical for downstream tasks such as aligning\nevents from different timelines. We provide detailed descriptions and analysis\nof the algorithmic components in TLEX, and conduct experimental evaluations by\napplying TLEX to 385 TimeML annotated texts from four corpora. We show that 123\nof the texts are inconsistent, 181 of them have more than one ``real world'' or\nmain timeline, and there are 2,541 indeterminate sections across all four\ncorpora. A sampling evaluation showed that TLEX is 98--100% accurate with 95%\nconfidence along five dimensions: the ordering of time-points, the number of\nmain timelines, the placement of time-points on main versus subordinate\ntimelines, the connecting point of branch timelines, and the location of the\nindeterminate sections. We provide a reference implementation of TLEX, the\nextracted timelines for all texts, and the manual corrections of the\ninconsistent texts.", "published": "2024-06-07 21:20:32", "link": "http://arxiv.org/abs/2406.05265v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Language Guided Skill Discovery", "abstract": "Skill discovery methods enable agents to learn diverse emergent behaviors\nwithout explicit rewards. To make learned skills useful for unknown downstream\ntasks, obtaining a semantically diverse repertoire of skills is essential.\nWhile some approaches introduce a discriminator to distinguish skills and\nothers aim to increase state coverage, no existing work directly addresses the\n\"semantic diversity\" of skills. We hypothesize that leveraging the semantic\nknowledge of large language models (LLMs) can lead us to improve semantic\ndiversity of resulting behaviors. In this sense, we introduce Language Guided\nSkill Discovery (LGSD), a skill discovery framework that aims to directly\nmaximize the semantic diversity between skills. LGSD takes user prompts as\ninput and outputs a set of semantically distinctive skills. The prompts serve\nas a means to constrain the search space into a semantically desired subspace,\nand the generated LLM outputs guide the agent to visit semantically diverse\nstates within the subspace. We demonstrate that LGSD enables legged robots to\nvisit different user-intended areas on a plane by simply changing the prompt.\nFurthermore, we show that language guidance aids in discovering more diverse\nskills compared to five existing skill discovery methods in robot-arm\nmanipulation environments. Lastly, LGSD provides a simple way of utilizing\nlearned skills via natural language.", "published": "2024-06-07 04:25:38", "link": "http://arxiv.org/abs/2406.06615v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Transforming Dental Diagnostics with Artificial Intelligence: Advanced\n  Integration of ChatGPT and Large Language Models for Patient Care", "abstract": "Artificial intelligence has dramatically reshaped our interaction with\ndigital technologies, ushering in an era where advancements in AI algorithms\nand Large Language Models (LLMs) have natural language processing (NLP) systems\nlike ChatGPT. This study delves into the impact of cutting-edge LLMs, notably\nOpenAI's ChatGPT, on medical diagnostics, with a keen focus on the dental\nsector. Leveraging publicly accessible datasets, these models augment the\ndiagnostic capabilities of medical professionals, streamline communication\nbetween patients and healthcare providers, and enhance the efficiency of\nclinical procedures. The advent of ChatGPT-4 is poised to make substantial\ninroads into dental practices, especially in the realm of oral surgery. This\npaper sheds light on the current landscape and explores potential future\nresearch directions in the burgeoning field of LLMs, offering valuable insights\nfor both practitioners and developers. Furthermore, it critically assesses the\nbroad implications and challenges within various sectors, including academia\nand healthcare, thus mapping out an overview of AI's role in transforming\ndental diagnostics for enhanced patient care.", "published": "2024-06-07 06:44:09", "link": "http://arxiv.org/abs/2406.06616v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LoRA-Whisper: Parameter-Efficient and Extensible Multilingual ASR", "abstract": "Recent years have witnessed significant progress in multilingual automatic\nspeech recognition (ASR), driven by the emergence of end-to-end (E2E) models\nand the scaling of multilingual datasets. Despite that, two main challenges\npersist in multilingual ASR: language interference and the incorporation of new\nlanguages without degrading the performance of the existing ones. This paper\nproposes LoRA-Whisper, which incorporates LoRA matrix into Whisper for\nmultilingual ASR, effectively mitigating language interference. Furthermore, by\nleveraging LoRA and the similarities between languages, we can achieve better\nperformance on new languages while upholding consistent performance on original\nones. Experiments on a real-world task across eight languages demonstrate that\nour proposed LoRA-Whisper yields a relative gain of 18.5% and 23.0% over the\nbaseline system for multilingual ASR and language expansion respectively.", "published": "2024-06-07 08:01:51", "link": "http://arxiv.org/abs/2406.06619v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "DualTime: A Dual-Adapter Multimodal Language Model for Time Series\n  Representation", "abstract": "The recent rapid development of language models (LMs) has attracted attention\nin the field of time series, including multimodal time series modeling.\nHowever, we note that current time series multimodal methods are biased, often\nassigning a primary role to one modality while the other assumes a secondary\nrole. They overlook the mutual benefits and complementary of different\nmodalities. For example, in seizure diagnosis, relying solely on textual\nclinical reports makes it difficult to pinpoint the area and type of the\ndisease, while electroencephalograms (EEGs) alone cannot provide an accurate\ndiagnosis without considering the symptoms. In this study, based on the\ncomplementary information mining of time series multimodal data, we propose\nDualTime, a Dual-adapter multimodal language model for Time series\nrepresentation implementing temporal-primary and textual-primary modeling\nsimultaneously. By injecting lightweight adaption tokens, the LM pipeline\nshared by dual adapters encourages embedding alignment and achieves efficient\nfine-tuning. Empirically, our method outperforms state-of-the-art models in\nboth supervised and unsupervised settings, highlighting the complementary\nbenefits of different modalities. In addition, we conduct few-shot label\ntransfer experiments, which further verifies the transferability and\nexpressiveness of our proposed DualTime.", "published": "2024-06-07 14:34:28", "link": "http://arxiv.org/abs/2406.06620v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph\n  Question-Answering", "abstract": "We present LinkQ, a system that leverages a large language model (LLM) to\nfacilitate knowledge graph (KG) query construction through natural language\nquestion-answering. Traditional approaches often require detailed knowledge of\na graph querying language, limiting the ability for users -- even experts -- to\nacquire valuable insights from KGs. LinkQ simplifies this process by\nimplementing a multistep protocol in which the LLM interprets a user's\nquestion, then systematically converts it into a well-formed query. LinkQ helps\nusers iteratively refine any open-ended questions into precise ones, supporting\nboth targeted and exploratory analysis. Further, LinkQ guards against the LLM\nhallucinating outputs by ensuring users' questions are only ever answered from\nground truth KG data. We demonstrate the efficacy of LinkQ through a\nqualitative study with five KG practitioners. Our results indicate that\npractitioners find LinkQ effective for KG question-answering, and desire future\nLLM-assisted exploratory data analysis systems.", "published": "2024-06-07 15:28:31", "link": "http://arxiv.org/abs/2406.06621v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adversarial Tuning: Defending Against Jailbreak Attacks for LLMs", "abstract": "Although safely enhanced Large Language Models (LLMs) have achieved\nremarkable success in tackling various complex tasks in a zero-shot manner,\nthey remain susceptible to jailbreak attacks, particularly the unknown\njailbreak attack. To enhance LLMs' generalized defense capabilities, we propose\na two-stage adversarial tuning framework, which generates adversarial prompts\nto explore worst-case scenarios by optimizing datasets containing pairs of\nadversarial prompts and their safe responses. In the first stage, we introduce\nthe hierarchical meta-universal adversarial prompt learning to efficiently and\neffectively generate token-level adversarial prompts. In the second stage, we\npropose the automatic adversarial prompt learning to iteratively refine\nsemantic-level adversarial prompts, further enhancing LLM's defense\ncapabilities. We conducted comprehensive experiments on three widely used\njailbreak datasets, comparing our framework with six defense baselines under\nfive representative attack scenarios. The results underscore the superiority of\nour proposed methods. Furthermore, our adversarial tuning framework exhibits\nempirical generalizability across various attack strategies and target LLMs,\nhighlighting its potential as a transferable defense mechanism.", "published": "2024-06-07 15:37:15", "link": "http://arxiv.org/abs/2406.06622v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "FedLLM-Bench: Realistic Benchmarks for Federated Learning of Large\n  Language Models", "abstract": "Federated learning has enabled multiple parties to collaboratively train\nlarge language models without directly sharing their data (FedLLM). Following\nthis training paradigm, the community has put massive efforts from diverse\naspects including framework, performance, and privacy. However, an unpleasant\nfact is that there are currently no realistic datasets and benchmarks for\nFedLLM and previous works all rely on artificially constructed datasets,\nfailing to capture properties in real-world scenarios. Addressing this, we\npropose FedLLM-Bench, which involves 8 training methods, 4 training datasets,\nand 6 evaluation metrics, to offer a comprehensive testbed for the FedLLM\ncommunity. FedLLM-Bench encompasses three datasets (e.g., user-annotated\nmultilingual dataset) for federated instruction tuning and one dataset (e.g.,\nuser-annotated preference dataset) for federated preference alignment, whose\nscale of client number ranges from 38 to 747. Our datasets incorporate several\nrepresentative diversities: language, quality, quantity, instruction, length,\nembedding, and preference, capturing properties in real-world scenarios. Based\non FedLLM-Bench, we conduct experiments on all datasets to benchmark existing\nFL methods and provide empirical insights (e.g., multilingual collaboration).\nWe believe that our FedLLM-Bench can benefit the FedLLM community by reducing\nrequired efforts, providing a practical testbed, and promoting fair\ncomparisons. Code and datasets are available at\nhttps://github.com/rui-ye/FedLLM-Bench.", "published": "2024-06-07 11:19:30", "link": "http://arxiv.org/abs/2406.04845v1", "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and\n  Less Hallucination", "abstract": "The integration of language and 3D perception is crucial for embodied agents\nand robots that comprehend and interact with the physical world. While large\nlanguage models (LLMs) have demonstrated impressive language understanding and\ngeneration capabilities, their adaptation to 3D environments (3D-LLMs) remains\nin its early stages. A primary challenge is a lack of large-scale datasets with\ndense grounding between language and 3D scenes. We introduce 3D-GRAND, a\npioneering large-scale dataset comprising 40,087 household scenes paired with\n6.2 million densely-grounded scene-language instructions. Our results show that\ninstruction tuning with 3D-GRAND significantly enhances grounding capabilities\nand reduces hallucinations in 3D-LLMs. As part of our contributions, we propose\na comprehensive benchmark 3D-POPE to systematically evaluate hallucination in\n3D-LLMs, enabling fair comparisons of models. Our experiments highlight a\nscaling effect between dataset size and 3D-LLM performance, emphasizing the\nimportance of large-scale 3D-text datasets for embodied AI research. Our\nresults demonstrate early signals for effective sim-to-real transfer,\nindicating that models trained on large synthetic data can perform well on\nreal-world 3D scans. Through 3D-GRAND and 3D-POPE, we aim to equip the embodied\nAI community with resources and insights to lead to more reliable and\nbetter-grounded 3D-LLMs. Project website: https://3d-grand.github.io", "published": "2024-06-07 17:59:59", "link": "http://arxiv.org/abs/2406.05132v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "CPLIP: Zero-Shot Learning for Histopathology with Comprehensive\n  Vision-Language Alignment", "abstract": "This paper proposes Comprehensive Pathology Language Image Pre-training\n(CPLIP), a new unsupervised technique designed to enhance the alignment of\nimages and text in histopathology for tasks such as classification and\nsegmentation. This methodology enriches vision-language models by leveraging\nextensive data without needing ground truth annotations. CPLIP involves\nconstructing a pathology-specific dictionary, generating textual descriptions\nfor images using language models, and retrieving relevant images for each text\nsnippet via a pre-trained model. The model is then fine-tuned using a\nmany-to-many contrastive learning method to align complex interrelated concepts\nacross both modalities. Evaluated across multiple histopathology tasks, CPLIP\nshows notable improvements in zero-shot learning scenarios, outperforming\nexisting methods in both interpretability and robustness and setting a higher\nbenchmark for the application of vision-language models in the field. To\nencourage further research and replication, the code for CPLIP is available on\nGitHub at https://cplip.github.io/", "published": "2024-06-07 18:39:58", "link": "http://arxiv.org/abs/2406.05205v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Boosting Diffusion Model for Spectrogram Up-sampling in Text-to-speech:\n  An Empirical Study", "abstract": "Scaling text-to-speech (TTS) with autoregressive language model (LM) to\nlarge-scale datasets by quantizing waveform into discrete speech tokens is\nmaking great progress to capture the diversity and expressiveness in human\nspeech, but the speech reconstruction quality from discrete speech token is far\nfrom satisfaction depending on the compressed speech token compression ratio.\nGenerative diffusion models trained with score-matching loss and continuous\nnormalized flow trained with flow-matching loss have become prominent in\ngeneration of images as well as speech. LM based TTS systems usually quantize\nspeech into discrete tokens and generate these tokens autoregressively, and\nfinally use a diffusion model to up sample coarse-grained speech tokens into\nfine-grained codec features or mel-spectrograms before reconstructing into\nwaveforms with vocoder, which has a high latency and is not realistic for real\ntime speech applications. In this paper, we systematically investigate varied\ndiffusion models for up sampling stage, which is the main bottleneck for\nstreaming synthesis of LM and diffusion-based architecture, we present the\nmodel architecture, objective and subjective metrics to show quality and\nefficiency improvement.", "published": "2024-06-07 04:34:03", "link": "http://arxiv.org/abs/2406.04633v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "The Database and Benchmark for the Source Speaker Tracing Challenge 2024", "abstract": "Voice conversion (VC) systems can transform audio to mimic another speaker's\nvoice, thereby attacking speaker verification (SV) systems. However, ongoing\nstudies on source speaker verification (SSV) are hindered by limited data\navailability and methodological constraints. This paper presents the Source\nSpeaker Tracking Challenge (SSTC) on STL 2024, which aims to fill the gap in\nthe database and benchmark for the SSV task. In this study, we generate a\nlarge-scale converted speech database with 16 common VC methods and train a\nbatch of baseline systems based on the MFA-Conformer architecture. In addition,\nwe introduced a related task called conversion method recognition, with the aim\nof assisting the SSV task. We expect SSTC to be a platform for advancing the\ndevelopment of the SSV task and provide further insights into the performance\nand limitations of current SV systems against VC attacks. Further details about\nSSTC can be found in https://sstc-challenge.github.io/.", "published": "2024-06-07 14:13:20", "link": "http://arxiv.org/abs/2406.04951v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Emo-bias: A Large Scale Evaluation of Social Bias on Speech Emotion\n  Recognition", "abstract": "The rapid growth of Speech Emotion Recognition (SER) has diverse global\napplications, from improving human-computer interactions to aiding mental\nhealth diagnostics. However, SER models might contain social bias toward\ngender, leading to unfair outcomes. This study analyzes gender bias in SER\nmodels trained with Self-Supervised Learning (SSL) at scale, exploring factors\ninfluencing it. SSL-based SER models are chosen for their cutting-edge\nperformance. Our research pioneering research gender bias in SER from both\nupstream model and data perspectives. Our findings reveal that females exhibit\nslightly higher overall SER performance than males. Modified CPC and XLS-R, two\nwell-known SSL models, notably exhibit significant bias. Moreover, models\ntrained with Mandarin datasets display a pronounced bias toward valence.\nLastly, we find that gender-wise emotion distribution differences in training\ndata significantly affect gender bias, while upstream model representation has\na limited impact.", "published": "2024-06-07 16:36:50", "link": "http://arxiv.org/abs/2406.05065v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Spectral Codecs: Spectrogram-Based Audio Codecs for High Quality Speech\n  Synthesis", "abstract": "Historically, most speech models in machine-learning have used the\nmel-spectrogram as a speech representation. Recently, discrete audio tokens\nproduced by neural audio codecs have become a popular alternate speech\nrepresentation for speech synthesis tasks such as text-to-speech (TTS).\nHowever, the data distribution produced by such codecs is too complex for some\nTTS models to predict, hence requiring large autoregressive models to get\nreasonable quality. Typical audio codecs compress and reconstruct the\ntime-domain audio signal. We propose a spectral codec which compresses the\nmel-spectrogram and reconstructs the time-domain audio signal. A study of\nobjective audio quality metrics suggests that our spectral codec has comparable\nperceptual quality to equivalent audio codecs. Furthermore, non-autoregressive\nTTS models trained with the proposed spectral codec generate audio with\nsignificantly higher quality than when trained with mel-spectrograms or audio\ncodecs.", "published": "2024-06-07 23:47:51", "link": "http://arxiv.org/abs/2406.05298v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Neural Codec-based Adversarial Sample Detection for Speaker Verification", "abstract": "Automatic Speaker Verification (ASV), increasingly used in security-critical\napplications, faces vulnerabilities from rising adversarial attacks, with few\neffective defenses available. In this paper, we propose a neural codec-based\nadversarial sample detection method for ASV. The approach leverages the codec's\nability to discard redundant perturbations and retain essential information.\nSpecifically, we distinguish between genuine and adversarial samples by\ncomparing ASV score differences between original and re-synthesized audio (by\ncodec models). This comprehensive study explores all open-source neural codecs\nand their variant models for experiments. The Descript-audio-codec model stands\nout by delivering the highest detection rate among 15 neural codecs and\nsurpassing seven prior state-of-the-art (SOTA) detection methods. Note that,\nour single-model method even outperforms a SOTA ensemble method by a large\nmargin.", "published": "2024-06-07 02:03:27", "link": "http://arxiv.org/abs/2406.04582v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "URGENT Challenge: Universality, Robustness, and Generalizability For\n  Speech Enhancement", "abstract": "The last decade has witnessed significant advancements in deep learning-based\nspeech enhancement (SE). However, most existing SE research has limitations on\nthe coverage of SE sub-tasks, data diversity and amount, and evaluation\nmetrics. To fill this gap and promote research toward universal SE, we\nestablish a new SE challenge, named URGENT, to focus on the universality,\nrobustness, and generalizability of SE. We aim to extend the SE definition to\ncover different sub-tasks to explore the limits of SE models, starting from\ndenoising, dereverberation, bandwidth extension, and declipping. A novel\nframework is proposed to unify all these sub-tasks in a single model, allowing\nthe use of all existing SE approaches. We collected public speech and noise\ndata from different domains to construct diverse evaluation data. Finally, we\ndiscuss the insights gained from our preliminary baseline experiments based on\nboth generative and discriminative SE methods with 12 curated metrics.", "published": "2024-06-07 05:59:01", "link": "http://arxiv.org/abs/2406.04660v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PPPR: Portable Plug-in Prompt Refiner for Text to Audio Generation", "abstract": "Text-to-Audio (TTA) aims to generate audio that corresponds to the given text\ndescription, playing a crucial role in media production. The text descriptions\nin TTA datasets lack rich variations and diversity, resulting in a drop in TTA\nmodel performance when faced with complex text. To address this issue, we\npropose a method called Portable Plug-in Prompt Refiner, which utilizes rich\nknowledge about textual descriptions inherent in large language models to\neffectively enhance the robustness of TTA acoustic models without altering the\nacoustic training set. Furthermore, a Chain-of-Thought that mimics human\nverification is introduced to enhance the accuracy of audio descriptions,\nthereby improving the accuracy of generated content in practical applications.\nThe experiments show that our method achieves a state-of-the-art Inception\nScore (IS) of 8.72, surpassing AudioGen, AudioLDM and Tango.", "published": "2024-06-07 06:54:01", "link": "http://arxiv.org/abs/2406.04683v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker-Smoothed kNN Speaker Adaptation for End-to-End ASR", "abstract": "Despite recent improvements in End-to-End Automatic Speech Recognition (E2E\nASR) systems, the performance can degrade due to vocal characteristic\nmismatches between training and testing data, particularly with limited target\nspeaker adaptation data. We propose a novel speaker adaptation approach\nSpeaker-Smoothed kNN that leverages k-Nearest Neighbors (kNN) retrieval\ntechniques to improve model output by finding correctly pronounced tokens from\nits pre-built datastore during the decoding phase. Moreover, we utilize\nx-vector to dynamically adjust kNN interpolation parameters for data sparsity\nissue. This approach was validated using KeSpeech and MagicData corpora under\nin-domain and all-domain settings. Our method consistently performs comparably\nto fine-tuning without the associated performance degradation during speaker\nchanges. Furthermore, in the all-domain setting, our method achieves\nstate-of-the-art results, reducing the CER in both single speaker and\nmulti-speaker test scenarios.", "published": "2024-06-07 09:38:38", "link": "http://arxiv.org/abs/2406.04791v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TraceableSpeech: Towards Proactively Traceable Text-to-Speech with\n  Watermarking", "abstract": "Various threats posed by the progress in text-to-speech (TTS) have prompted\nthe need to reliably trace synthesized speech. However, contemporary approaches\nto this task involve adding watermarks to the audio separately after\ngeneration, a process that hurts both speech quality and watermark\nimperceptibility. In addition, these approaches are limited in robustness and\nflexibility. To address these problems, we propose TraceableSpeech, a novel TTS\nmodel that directly generates watermarked speech, improving watermark\nimperceptibility and speech quality. Furthermore, We design the frame-wise\nimprinting and extraction of watermarks, achieving higher robustness against\nresplicing attacks and temporal flexibility in operation. Experimental results\nshow that TraceableSpeech outperforms the strong baseline where VALL-E or\nHiFicodec individually uses WavMark in watermark imperceptibility, speech\nquality and resilience against resplicing attacks. It also can apply to speech\nof various durations. The code is avaliable at\nhttps://github.com/zjzser/TraceableSpeech", "published": "2024-06-07 11:13:03", "link": "http://arxiv.org/abs/2406.04840v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the social bias of speech self-supervised models", "abstract": "Self-supervised learning (SSL) speech models have achieved remarkable\nperformance in various tasks, yet the biased outcomes, especially affecting\nmarginalized groups, raise significant concerns. Social bias refers to the\nphenomenon where algorithms potentially amplify disparate properties between\nsocial groups present in the data used for training. Bias in SSL models can\nperpetuate injustice by automating discriminatory patterns and reinforcing\ninequitable systems. This work reveals that prevalent SSL models inadvertently\nacquire biased associations. We probe how various factors, such as model\narchitecture, size, and training methodologies, influence the propagation of\nsocial bias within these models. Finally, we explore the efficacy of debiasing\nSSL models through regularization techniques, specifically via model\ncompression. Our findings reveal that employing techniques such as row-pruning\nand training wider, shallower models can effectively mitigate social bias\nwithin SSL model.", "published": "2024-06-07 15:07:07", "link": "http://arxiv.org/abs/2406.04997v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Differentiable Time-Varying Linear Prediction in the Context of\n  End-to-End Analysis-by-Synthesis", "abstract": "Training the linear prediction (LP) operator end-to-end for audio synthesis\nin modern deep learning frameworks is slow due to its recursive formulation. In\naddition, frame-wise approximation as an acceleration method cannot generalise\nwell to test time conditions where the LP is computed sample-wise. Efficient\ndifferentiable sample-wise LP for end-to-end training is the key to removing\nthis barrier. We generalise the efficient time-invariant LP implementation from\nthe GOLF vocoder to time-varying cases. Combining this with the classic\nsource-filter model, we show that the improved GOLF learns LP coefficients and\nreconstructs the voice better than its frame-wise counterparts. Moreover, in\nour listening test, synthesised outputs from GOLF scored higher in quality\nratings than the state-of-the-art differentiable WORLD vocoder.", "published": "2024-06-07 17:57:29", "link": "http://arxiv.org/abs/2406.05128v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "XANE: eXplainable Acoustic Neural Embeddings", "abstract": "We present a novel method for extracting neural embeddings that model the\nbackground acoustics of a speech signal. The extracted embeddings are used to\nestimate specific parameters related to the background acoustic properties of\nthe signal in a non-intrusive manner, which allows the embeddings to be\nexplainable in terms of those parameters. We illustrate the value of these\nembeddings by performing clustering experiments on unseen test data and show\nthat the proposed embeddings achieve a mean F1 score of 95.2\\% for three\ndifferent tasks, outperforming significantly the WavLM based signal embeddings.\nWe also show that the proposed method can explain the embeddings by estimating\n14 acoustic parameters characterizing the background acoustics, including\nreverberation and noise levels, overlapped speech detection, CODEC type\ndetection and noise type detection with high accuracy and a real-time factor 17\ntimes lower than an external baseline method.", "published": "2024-06-07 18:29:26", "link": "http://arxiv.org/abs/2406.05199v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Signal processing algorithm effective for sound quality of hearing loss\n  simulators", "abstract": "Hearing loss (HL) simulators, which allow normal hearing (NH) listeners to\nexperience HL, have been used in speech intelligibility experiments, but not in\nsound quality experiments due to perceptible distortion. If they produced less\ndistortion, they might be useful for NH listeners to evaluate the sound quality\nof, for example, hearing aids. We conducted perceptual sound quality\nexperiments to compare the Cambridge version of HL simulator (CamHLS) and the\nWakayama version of the HL simulator (WHIS), which has the two algorithms of\nfilterbank analysis synthesis (FBAS) and direct time-varying filter (DTVF). The\nexperimental results showed that WHIS with DTVF produces less perceptible\ndistortion in speech sounds than CamHLS and WHIS with FBAS, even when the\nnonlinear process is working. This advantage is mainly due to the use of the\nDTVF algorithm, which could be applied to various signal synthesis applications\nwith filterbank analysis.", "published": "2024-06-07 22:43:44", "link": "http://arxiv.org/abs/2406.05286v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MeLFusion: Synthesizing Music from Image and Language Cues using\n  Diffusion Models", "abstract": "Music is a universal language that can communicate emotions and feelings. It\nforms an essential part of the whole spectrum of creative media, ranging from\nmovies to social media posts. Machine learning models that can synthesize music\nare predominantly conditioned on textual descriptions of it. Inspired by how\nmusicians compose music not just from a movie script, but also through\nvisualizations, we propose MeLFusion, a model that can effectively use cues\nfrom a textual description and the corresponding image to synthesize music.\nMeLFusion is a text-to-music diffusion model with a novel \"visual synapse\",\nwhich effectively infuses the semantics from the visual modality into the\ngenerated music. To facilitate research in this area, we introduce a new\ndataset MeLBench, and propose a new evaluation metric IMSM. Our exhaustive\nexperimental evaluation suggests that adding visual information to the music\nsynthesis pipeline significantly improves the quality of generated music,\nmeasured both objectively and subjectively, with a relative gain of up to\n67.98% on the FAD score. We hope that our work will gather attention to this\npragmatic, yet relatively under-explored research area.", "published": "2024-06-07 06:38:59", "link": "http://arxiv.org/abs/2406.04673v1", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.CV"}
{"title": "MA-AVT: Modality Alignment for Parameter-Efficient Audio-Visual\n  Transformers", "abstract": "Recent advances in pre-trained vision transformers have shown promise in\nparameter-efficient audio-visual learning without audio pre-training. However,\nfew studies have investigated effective methods for aligning multimodal\nfeatures in parameter-efficient audio-visual transformers. In this paper, we\npropose MA-AVT, a new parameter-efficient audio-visual transformer employing\ndeep modality alignment for corresponding multimodal semantic features.\nSpecifically, we introduce joint unimodal and multimodal token learning for\naligning the two modalities with a frozen modality-shared transformer. This\nallows the model to learn separate representations for each modality, while\nalso attending to the cross-modal relationships between them. In addition,\nunlike prior work that only aligns coarse features from the output of unimodal\nencoders, we introduce blockwise contrastive learning to align\ncoarse-to-fine-grain hierarchical features throughout the encoding phase.\nFurthermore, to suppress the background features in each modality from\nforeground matched audio-visual features, we introduce a robust discriminative\nforeground mining scheme. Through extensive experiments on benchmark AVE,\nVGGSound, and CREMA-D datasets, we achieve considerable performance\nimprovements over SOTA methods.", "published": "2024-06-07 13:35:44", "link": "http://arxiv.org/abs/2406.04930v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Towards objective and interpretable speech disorder assessment: a\n  comparative analysis of CNN and transformer-based models", "abstract": "Head and Neck Cancers (HNC) significantly impact patients' ability to speak,\naffecting their quality of life. Commonly used metrics for assessing\npathological speech are subjective, prompting the need for automated and\nunbiased evaluation methods. This study proposes a self-supervised\nWav2Vec2-based model for phone classification with HNC patients, to enhance\naccuracy and improve the discrimination of phonetic features for subsequent\ninterpretability purpose. The impact of pre-training datasets, model size, and\nfine-tuning datasets and parameters are explored. Evaluation on diverse corpora\nreveals the effectiveness of the Wav2Vec2 architecture, outperforming a\nCNN-based approach, used in previous work. Correlation with perceptual measures\nalso affirms the model relevance for impaired speech analysis. This work paves\nthe way for better understanding of pathological speech with interpretable\napproaches for clinicians, by leveraging complex self-learnt speech\nrepresentations.", "published": "2024-06-07 08:51:52", "link": "http://arxiv.org/abs/2406.07576v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MUSE: Flexible Voiceprint Receptive Fields and Multi-Path Fusion\n  Enhanced Taylor Transformer for U-Net-based Speech Enhancement", "abstract": "Achieving a balance between lightweight design and high performance remains a\nchallenging task for speech enhancement. In this paper, we introduce Multi-path\nEnhanced Taylor (MET) Transformer based U-net for Speech Enhancement (MUSE), a\nlightweight speech enhancement network built upon the Unet architecture. Our\napproach incorporates a novel Multi-path Enhanced Taylor (MET) Transformer\nblock, which integrates Deformable Embedding (DE) to enable flexible receptive\nfields for voiceprints. The MET Transformer is uniquely designed to fuse\nChannel and Spatial Attention (CSA) branches, facilitating channel information\nexchange and addressing spatial attention deficits within the\nTaylor-Transformer framework. Through extensive experiments conducted on the\nVoiceBank+DEMAND dataset, we demonstrate that MUSE achieves competitive\nperformance while significantly reducing both training and deployment costs,\nboasting a mere 0.51M parameters.", "published": "2024-06-07 02:41:14", "link": "http://arxiv.org/abs/2406.04589v2", "categories": ["cs.SD", "cs.IR", "cs.IT", "cs.LG", "eess.AS", "math.IT"], "primary_category": "cs.SD"}
