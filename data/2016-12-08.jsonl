{"title": "Embedding Words and Senses Together via Joint Knowledge-Enhanced\n  Training", "abstract": "Word embeddings are widely used in Natural Language Processing, mainly due to\ntheir success in capturing semantic information from massive corpora. However,\ntheir creation process does not allow the different meanings of a word to be\nautomatically separated, as it conflates them into a single vector. We address\nthis issue by proposing a new model which learns word and sense embeddings\njointly. Our model exploits large corpora and knowledge from semantic networks\nin order to produce a unified vector space of word and sense embeddings. We\nevaluate the main features of our approach both qualitatively and\nquantitatively in a variety of tasks, highlighting the advantages of the\nproposed method in comparison to state-of-the-art word- and sense-based models.", "published": "2016-12-08 15:54:00", "link": "http://arxiv.org/abs/1612.02703v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity Identification as Multitasking", "abstract": "Standard approaches in entity identification hard-code boundary detection and\ntype prediction into labels (e.g., John/B-PER Smith/I-PER) and then perform\nViterbi. This has two disadvantages: 1. the runtime complexity grows\nquadratically in the number of types, and 2. there is no natural segment-level\nrepresentation. In this paper, we propose a novel neural architecture that\naddresses these disadvantages. We frame the problem as multitasking, separating\nboundary detection and type prediction but optimizing them jointly. Despite its\nsimplicity, this architecture performs competitively with fully structured\nmodels such as BiLSTM-CRFs while scaling linearly in the number of types.\nFurthermore, by construction, the model induces type-disambiguating embeddings\nof predicted mentions.", "published": "2016-12-08 16:05:03", "link": "http://arxiv.org/abs/1612.02706v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discovering Conversational Dependencies between Messages in Dialogs", "abstract": "We investigate the task of inferring conversational dependencies between\nmessages in one-on-one online chat, which has become one of the most popular\nforms of customer service. We propose a novel probabilistic classifier that\nleverages conversational, lexical and semantic information. The approach is\nevaluated empirically on a set of customer service chat logs from a Chinese\ne-commerce website. It outperforms heuristic baselines.", "published": "2016-12-08 20:33:17", "link": "http://arxiv.org/abs/1612.02801v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards better decoding and language model integration in sequence to\n  sequence models", "abstract": "The recently proposed Sequence-to-Sequence (seq2seq) framework advocates\nreplacing complex data processing pipelines, such as an entire automatic speech\nrecognition system, with a single neural network trained in an end-to-end\nfashion. In this contribution, we analyse an attention-based seq2seq speech\nrecognition system that directly transcribes recordings into characters. We\nobserve two shortcomings: overconfidence in its predictions and a tendency to\nproduce incomplete transcriptions when language models are used. We propose\npractical solutions to both problems achieving competitive speaker independent\nword error rates on the Wall Street Journal dataset: without separate language\nmodels we reach 10.6% WER, while together with a trigram language model, we\nreach 6.7% WER.", "published": "2016-12-08 15:23:44", "link": "http://arxiv.org/abs/1612.02695v1", "categories": ["cs.NE", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.NE"}
{"title": "Coupling Distributed and Symbolic Execution for Natural Language Queries", "abstract": "Building neural networks to query a knowledge base (a table) with natural\nlanguage is an emerging research topic in deep learning. An executor for table\nquerying typically requires multiple steps of execution because queries may\nhave complicated structures. In previous studies, researchers have developed\neither fully distributed executors or symbolic executors for table querying. A\ndistributed executor can be trained in an end-to-end fashion, but is weak in\nterms of execution efficiency and explicit interpretability. A symbolic\nexecutor is efficient in execution, but is very difficult to train especially\nat initial stages. In this paper, we propose to couple distributed and symbolic\nexecution for natural language queries, where the symbolic executor is\npretrained with the distributed executor's intermediate execution results in a\nstep-by-step fashion. Experiments show that our approach significantly\noutperforms both distributed and symbolic executors, exhibiting high accuracy,\nhigh learning efficiency, high execution efficiency, and high interpretability.", "published": "2016-12-08 17:45:16", "link": "http://arxiv.org/abs/1612.02741v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE", "cs.SE"], "primary_category": "cs.LG"}
