{"title": "Do Language Models Learn Position-Role Mappings?", "abstract": "How is knowledge of position-role mappings in natural language learned? We\nexplore this question in a computational setting, testing whether a variety of\nwell-performing pertained language models (BERT, RoBERTa, and DistilBERT)\nexhibit knowledge of these mappings, and whether this knowledge persists across\nalternations in syntactic, structural, and lexical alternations. In Experiment\n1, we show that these neural models do indeed recognize distinctions between\ntheme and recipient roles in ditransitive constructions, and that these\ndistinct patterns are shared across construction type. We strengthen this\nfinding in Experiment 2 by showing that fine-tuning these language models on\nnovel theme- and recipient-like tokens in one paradigm allows the models to\nmake correct predictions about their placement in other paradigms, suggesting\nthat the knowledge of these mappings is shared rather than independently\nlearned. We do, however, observe some limitations of this generalization when\ntasks involve constructions with novel ditransitive verbs, hinting at a degree\nof lexical specificity which underlies model performance.", "published": "2022-02-08 02:50:53", "link": "http://arxiv.org/abs/2202.03611v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Survey of Hallucination in Natural Language Generation", "abstract": "Natural Language Generation (NLG) has improved exponentially in recent years\nthanks to the development of sequence-to-sequence deep learning technologies\nsuch as Transformer-based language models. This advancement has led to more\nfluent and coherent NLG, leading to improved development in downstream tasks\nsuch as abstractive summarization, dialogue generation and data-to-text\ngeneration. However, it is also apparent that deep learning based generation is\nprone to hallucinate unintended text, which degrades the system performance and\nfails to meet user expectations in many real-world scenarios. To address this\nissue, many studies have been presented in measuring and mitigating\nhallucinated texts, but these have never been reviewed in a comprehensive\nmanner before. In this survey, we thus provide a broad overview of the research\nprogress and challenges in the hallucination problem in NLG. The survey is\norganized into two parts: (1) a general overview of metrics, mitigation\nmethods, and future directions; (2) an overview of task-specific research\nprogress on hallucinations in the following downstream tasks, namely\nabstractive summarization, dialogue generation, generative question answering,\ndata-to-text generation, machine translation, and visual-language generation;\nand (3) hallucinations in large language models (LLMs). This survey serves to\nfacilitate collaborative efforts among researchers in tackling the challenge of\nhallucinated texts in NLG.", "published": "2022-02-08 03:55:01", "link": "http://arxiv.org/abs/2202.03629v7", "categories": ["cs.CL", "A.1"], "primary_category": "cs.CL"}
{"title": "Differentiable N-gram Objective on Abstractive Summarization", "abstract": "ROUGE is a standard automatic evaluation metric based on n-grams for\nsequence-to-sequence tasks, while cross-entropy loss is an essential objective\nof neural network language model that optimizes at a unigram level. We present\ndifferentiable n-gram objectives, attempting to alleviate the discrepancy\nbetween training criterion and evaluating criterion. The objective maximizes\nthe probabilistic weight of matched sub-sequences, and the novelty of our work\nis the objective weights the matched sub-sequences equally and does not ceil\nthe number of matched sub-sequences by the ground truth count of n-grams in\nreference sequence. We jointly optimize cross-entropy loss and the proposed\nobjective, providing decent ROUGE score enhancement over abstractive\nsummarization dataset CNN/DM and XSum, outperforming alternative n-gram\nobjectives.", "published": "2022-02-08 17:19:23", "link": "http://arxiv.org/abs/2202.04003v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating question answering and text-to-SQL in Portuguese", "abstract": "Deep learning transformers have drastically improved systems that\nautomatically answer questions in natural language. However, different\nquestions demand different answering techniques; here we propose, build and\nvalidate an architecture that integrates different modules to answer two\ndistinct kinds of queries. Our architecture takes a free-form natural language\ntext and classifies it to send it either to a Neural Question Answering\nReasoner or a Natural Language parser to SQL. We implemented a complete system\nfor the Portuguese language, using some of the main tools available for the\nlanguage and translating training and testing datasets. Experiments show that\nour system selects the appropriate answering method with high accuracy (over\n99\\%), thus validating a modular question answering strategy.", "published": "2022-02-08 18:23:03", "link": "http://arxiv.org/abs/2202.04048v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HistBERT: A Pre-trained Language Model for Diachronic Lexical Semantic\n  Analysis", "abstract": "Contextualized word embeddings have demonstrated state-of-the-art performance\nin various natural language processing tasks including those that concern\nhistorical semantic change. However, language models such as BERT was trained\nprimarily on contemporary corpus data. To investigate whether training on\nhistorical corpus data improves diachronic semantic analysis, we present a\npre-trained BERT-based language model, HistBERT, trained on the balanced Corpus\nof Historical American English. We examine the effectiveness of our approach by\ncomparing the performance of the original BERT and that of HistBERT, and we\nreport promising results in word similarity and semantic shift analysis. Our\nwork suggests that the effectiveness of contextual embeddings in diachronic\nsemantic analysis is dependent on the temporal profile of the input text and\ncare should be taken in applying this methodology to study historical semantic\nchange.", "published": "2022-02-08 02:53:48", "link": "http://arxiv.org/abs/2202.03612v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Property-Based Tests in Natural Language", "abstract": "We consider a new approach to generate tests from natural language. Rather\nthan relying on machine learning or templated extraction from structured\ncomments, we propose to apply classic ideas from linguistics to translate\nnatural-language sentences into executable tests. This paper explores the\napplication of combinatory categorial grammars (CCGs) to generating\nproperty-based tests. Our prototype is able to generate tests from English\ndescriptions for each example in a textbook chapter on property-based testing.", "published": "2022-02-08 03:15:42", "link": "http://arxiv.org/abs/2202.03616v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Semantic features of object concepts generated with GPT-3", "abstract": "Semantic features have been playing a central role in investigating the\nnature of our conceptual representations. Yet the enormous time and effort\nrequired to empirically sample and norm features from human raters has\nrestricted their use to a limited set of manually curated concepts. Given\nrecent promising developments with transformer-based language models, here we\nasked whether it was possible to use such models to automatically generate\nmeaningful lists of properties for arbitrary object concepts and whether these\nmodels would produce features similar to those found in humans. To this end, we\nprobed a GPT-3 model to generate semantic features for 1,854 objects and\ncompared automatically-generated features to existing human feature norms.\nGPT-3 generated many more features than humans, yet showed a similar\ndistribution in the types of generated features. Generated feature norms\nrivaled human norms in predicting similarity, relatedness, and category\nmembership, while variance partitioning demonstrated that these predictions\nwere driven by similar variance in humans and GPT-3. Together, these results\nhighlight the potential of large language models to capture important facets of\nhuman knowledge and yield a new approach for automatically generating\ninterpretable feature sets, thus drastically expanding the potential use of\nsemantic features in psychological and linguistic studies.", "published": "2022-02-08 09:51:48", "link": "http://arxiv.org/abs/2202.03753v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Modeling Structure with Undirected Neural Networks", "abstract": "Neural networks are powerful function estimators, leading to their status as\na paradigm of choice for modeling structured data. However, unlike other\nstructured representations that emphasize the modularity of the problem --\ne.g., factor graphs -- neural networks are usually monolithic mappings from\ninputs to outputs, with a fixed computation order. This limitation prevents\nthem from capturing different directions of computation and interaction between\nthe modeled variables.\n  In this paper, we combine the representational strengths of factor graphs and\nof neural networks, proposing undirected neural networks (UNNs): a flexible\nframework for specifying computations that can be performed in any order. For\nparticular choices, our proposed models subsume and extend many existing\narchitectures: feed-forward, recurrent, self-attention networks, auto-encoders,\nand networks with implicit layers. We demonstrate the effectiveness of\nundirected neural architectures, both unstructured and structured, on a range\nof tasks: tree-constrained dependency parsing, convolutional image\nclassification, and sequence completion with attention. By varying the\ncomputation order, we show how a single UNN can be used both as a classifier\nand a prototype generator, and how it can fill in missing parts of an input\nsequence, making them a promising field for further research.", "published": "2022-02-08 10:06:51", "link": "http://arxiv.org/abs/2202.03760v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Counterfactual Multi-Token Fairness in Text Classification", "abstract": "The counterfactual token generation has been limited to perturbing only a\nsingle token in texts that are generally short and single sentences. These\ntokens are often associated with one of many sensitive attributes. With limited\ncounterfactuals generated, the goal to achieve invariant nature for machine\nlearning classification models towards any sensitive attribute gets bounded,\nand the formulation of Counterfactual Fairness gets narrowed. In this paper, we\novercome these limitations by solving root problems and opening bigger domains\nfor understanding. We have curated a resource of sensitive tokens and their\ncorresponding perturbation tokens, even extending the support beyond\ntraditionally used sensitive attributes like Age, Gender, Race to Nationality,\nDisability, and Religion. The concept of Counterfactual Generation has been\nextended to multi-token support valid over all forms of texts and documents. We\ndefine the method of generating counterfactuals by perturbing multiple\nsensitive tokens as Counterfactual Multi-token Generation. The method has been\nconceptualized to showcase significant performance improvement over\nsingle-token methods and validated over multiple benchmark datasets. The\nemendation in counterfactual generation propagates in achieving improved\nCounterfactual Multi-token Fairness.", "published": "2022-02-08 11:30:19", "link": "http://arxiv.org/abs/2202.03792v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What are the best systems? New perspectives on NLP Benchmarking", "abstract": "In Machine Learning, a benchmark refers to an ensemble of datasets associated\nwith one or multiple metrics together with a way to aggregate different systems\nperformances. They are instrumental in (i) assessing the progress of new\nmethods along different axes and (ii) selecting the best systems for practical\nuse. This is particularly the case for NLP with the development of large\npre-trained models (e.g. GPT, BERT) that are expected to generalize well on a\nvariety of tasks. While the community mainly focused on developing new datasets\nand metrics, there has been little interest in the aggregation procedure, which\nis often reduced to a simple average over various performance measures.\nHowever, this procedure can be problematic when the metrics are on a different\nscale, which may lead to spurious conclusions. This paper proposes a new\nprocedure to rank systems based on their performance across different tasks.\nMotivated by the social choice theory, the final system ordering is obtained\nthrough aggregating the rankings induced by each task and is theoretically\ngrounded. We conduct extensive numerical experiments (on over 270k scores) to\nassess the soundness of our approach both on synthetic and real scores (e.g.\nGLUE, EXTREM, SEVAL, TAC, FLICKR). In particular, we show that our method\nyields different conclusions on state-of-the-art systems than the\nmean-aggregation procedure while being both more reliable and robust.", "published": "2022-02-08 11:44:20", "link": "http://arxiv.org/abs/2202.03799v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TimeLMs: Diachronic Language Models from Twitter", "abstract": "Despite its importance, the time variable has been largely neglected in the\nNLP and language model literature. In this paper, we present TimeLMs, a set of\nlanguage models specialized on diachronic Twitter data. We show that a\ncontinual learning strategy contributes to enhancing Twitter-based language\nmodels' capacity to deal with future and out-of-distribution tweets, while\nmaking them competitive with standardized and more monolithic benchmarks. We\nalso perform a number of qualitative analyses showing how they cope with trends\nand peaks in activity involving specific named entities or concept drift.", "published": "2022-02-08 12:47:38", "link": "http://arxiv.org/abs/2202.03829v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Logical Reasoning for Task Oriented Dialogue Systems", "abstract": "In recent years, large pretrained models have been used in dialogue systems\nto improve successful task completion rates. However, lack of reasoning\ncapabilities of dialogue platforms make it difficult to provide relevant and\nfluent responses, unless the designers of a conversational experience spend a\nconsiderable amount of time implementing these capabilities in external rule\nbased modules. In this work, we propose a novel method to fine-tune pretrained\ntransformer models such as Roberta and T5. to reason over a set of facts in a\ngiven dialogue context. Our method includes a synthetic data generation\nmechanism which helps the model learn logical relations, such as comparison\nbetween list of numerical values, inverse relations (and negation), inclusion\nand exclusion for categorical attributes, and application of a combination of\nattributes over both numerical and categorical values, and spoken form for\nnumerical values, without need for additional training dataset. We show that\nthe transformer based model can perform logical reasoning to answer questions\nwhen the dialogue context contains all the required information, otherwise it\nis able to extract appropriate constraints to pass to downstream components\n(e.g. a knowledge base) when partial information is available. We observe that\ntransformer based models such as UnifiedQA-T5 can be fine-tuned to perform\nlogical reasoning (such as numerical and categorical attributes' comparison)\nover attributes that been seen in training time (e.g., accuracy of 90\\%+ for\ncomparison of smaller than $k_{\\max}$=5 values over heldout test dataset).", "published": "2022-02-08 21:46:27", "link": "http://arxiv.org/abs/2202.04161v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RNN Transducers for Nested Named Entity Recognition with constraints on\n  alignment for long sequences", "abstract": "Popular solutions to Named Entity Recognition (NER) include conditional\nrandom fields, sequence-to-sequence models, or utilizing the question-answering\nframework. However, they are not suitable for nested and overlapping spans with\nlarge ontologies and for predicting the position of the entities. To fill this\ngap, we introduce a new model for NER task -- an RNN transducer (RNN-T). These\nmodels are trained using paired input and output sequences without explicitly\nspecifying the alignment between them, similar to other seq-to-seq models.\nRNN-T models learn the alignment using a loss function that sums over all\nalignments. In NER tasks, however, the alignment between words and target\nlabels are available from the human annotations. We propose a fixed alignment\nRNN-T model that utilizes the given alignment, while preserving the benefits of\nRNN-Ts such as modeling output dependencies. As a more general case, we also\npropose a constrained alignment model where users can specify a relaxation of\nthe given input alignment and the model will learn an alignment within the\ngiven constraints. In other words, we propose a family of seq-to-seq models\nwhich can leverage alignments between input and target sequences when\navailable. Through empirical experiments on a challenging real-world medical\nNER task with multiple nested ontologies, we demonstrate that our fixed\nalignment model outperforms the standard RNN-T model, improving F1-score from\n0.70 to 0.74.", "published": "2022-02-08 05:38:20", "link": "http://arxiv.org/abs/2203.03543v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A two-step approach to leverage contextual data: speech recognition in\n  air-traffic communications", "abstract": "Automatic Speech Recognition (ASR), as the assistance of speech communication\nbetween pilots and air-traffic controllers, can significantly reduce the\ncomplexity of the task and increase the reliability of transmitted information.\nASR application can lead to a lower number of incidents caused by\nmisunderstanding and improve air traffic management (ATM) efficiency.\nEvidently, high accuracy predictions, especially, of key information, i.e.,\ncallsigns and commands, are required to minimize the risk of errors. We prove\nthat combining the benefits of ASR and Natural Language Processing (NLP)\nmethods to make use of surveillance data (i.e. additional modality) helps to\nconsiderably improve the recognition of callsigns (named entity). In this\npaper, we investigate a two-step callsign boosting approach: (1) at the 1 step\n(ASR), weights of probable callsign n-grams are reduced in G.fst and/or in the\ndecoding FST (lattices), (2) at the 2 step (NLP), callsigns extracted from the\nimproved recognition outputs with Named Entity Recognition (NER) are correlated\nwith the surveillance data to select the most suitable one. Boosting callsign\nn-grams with the combination of ASR and NLP methods eventually leads up to\n53.7% of an absolute, or 60.4% of a relative, improvement in callsign\nrecognition.", "published": "2022-02-08 08:59:54", "link": "http://arxiv.org/abs/2202.03725v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "DALL-Eval: Probing the Reasoning Skills and Social Biases of\n  Text-to-Image Generation Models", "abstract": "Recently, DALL-E, a multimodal transformer language model, and its variants,\nincluding diffusion models, have shown high-quality text-to-image generation\ncapabilities. However, despite the realistic image generation results, there\nhas not been a detailed analysis of how to evaluate such models. In this work,\nwe investigate the visual reasoning capabilities and social biases of different\ntext-to-image models, covering both multimodal transformer language models and\ndiffusion models. First, we measure three visual reasoning skills: object\nrecognition, object counting, and spatial relation understanding. For this, we\npropose PaintSkills, a compositional diagnostic evaluation dataset that\nmeasures these skills. Despite the high-fidelity image generation capability, a\nlarge gap exists between the performance of recent models and the upper bound\naccuracy in object counting and spatial relation understanding skills. Second,\nwe assess the gender and skin tone biases by measuring the gender/skin tone\ndistribution of generated images across various professions and attributes. We\ndemonstrate that recent text-to-image generation models learn specific biases\nabout gender and skin tone from web image-text pairs. We hope our work will\nhelp guide future progress in improving text-to-image generation models on\nvisual reasoning skills and learning socially unbiased representations. Code\nand data: https://github.com/j-min/DallEval", "published": "2022-02-08 18:36:52", "link": "http://arxiv.org/abs/2202.04053v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Machine Explanations and Human Understanding", "abstract": "Explanations are hypothesized to improve human understanding of machine\nlearning models and achieve a variety of desirable outcomes, ranging from model\ndebugging to enhancing human decision making. However, empirical studies have\nfound mixed and even negative results. An open question, therefore, is under\nwhat conditions explanations can improve human understanding and in what way.\nUsing adapted causal diagrams, we provide a formal characterization of the\ninterplay between machine explanations and human understanding, and show how\nhuman intuitions play a central role in enabling human understanding.\nSpecifically, we identify three core concepts of interest that cover all\nexisting quantitative measures of understanding in the context of human-AI\ndecision making: task decision boundary, model decision boundary, and model\nerror. Our key result is that without assumptions about task-specific\nintuitions, explanations may potentially improve human understanding of model\ndecision boundary, but they cannot improve human understanding of task decision\nboundary or model error. To achieve complementary human-AI performance, we\narticulate possible ways on how explanations need to work with human\nintuitions. For instance, human intuitions about the relevance of features\n(e.g., education is more important than age in predicting a person's income)\ncan be critical in detecting model error. We validate the importance of human\nintuitions in shaping the outcome of machine explanations with empirical\nhuman-subject studies. Overall, our work provides a general framework along\nwith actionable implications for future algorithmic development and empirical\nexperiments of machine explanations.", "published": "2022-02-08 19:00:38", "link": "http://arxiv.org/abs/2202.04092v3", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Exploring the Limits of Domain-Adaptive Training for Detoxifying\n  Large-Scale Language Models", "abstract": "Pre-trained language models (LMs) are shown to easily generate toxic\nlanguage. In this work, we systematically explore domain-adaptive training to\nreduce the toxicity of language models. We conduct this study on three\ndimensions: training corpus, model size, and parameter efficiency. For the\ntraining corpus, we propose to leverage the generative power of LMs and\ngenerate nontoxic datasets for domain-adaptive training, which mitigates the\nexposure bias and is shown to be more data-efficient than using a curated\npre-training corpus. We demonstrate that the self-generation method\nconsistently outperforms the existing baselines across various model sizes on\nboth automatic and human evaluations, even when it uses a 1/3 smaller training\ncorpus. We then comprehensively study detoxifying LMs with parameter sizes\nranging from 126M up to 530B (3x larger than GPT-3), a scale that has never\nbeen studied before. We find that i) large LMs have similar toxicity levels as\nsmaller ones given the same pre-training corpus, and ii) large LMs require more\nendeavor to detoxify. We also explore parameter-efficient training methods for\ndetoxification. We demonstrate that adding and training adapter-only layers in\nLMs not only saves a lot of parameters but also achieves a better trade-off\nbetween toxicity and perplexity than whole model adaptation for the large-scale\nmodels.", "published": "2022-02-08 22:10:40", "link": "http://arxiv.org/abs/2202.04173v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Crime Hot-Spot Modeling via Topic Modeling and Relative Density\n  Estimation", "abstract": "We present a method to capture groupings of similar calls and determine their\nrelative spatial distribution from a collection of crime record narratives. We\nfirst obtain a topic distribution for each narrative, and then propose a\nnearest neighbors relative density estimation (kNN-RDE) approach to obtain\nspatial relative densities per topic. Experiments over a large corpus\n($n=475,019$) of narrative documents from the Atlanta Police Department\ndemonstrate the viability of our method in capturing geographic hot-spot trends\nwhich call dispatchers do not initially pick up on and which go unnoticed due\nto conflation with elevated event density in general.", "published": "2022-02-08 22:18:25", "link": "http://arxiv.org/abs/2202.04176v3", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "An Executable Formal Model of the VHDL in Isabelle/HOL", "abstract": "In the hardware design process, hardware components are usually described in\na hardware description language. Most of the hardware description languages,\nsuch as Verilog and VHDL, do not have mathematical foundation and hence are not\nfit for formal reasoning about the design. To enable formal reasoning in one of\nthe most commonly used description language VHDL, we define a formal model of\nthe VHDL language in Isabelle/HOL. Our model targets the functional part of\nVHDL designs used in industry, specifically the design of the LEON3 processor's\ninteger unit. We cover a wide range of features in the VHDL language that are\nusually not modelled in the literature and define a novel operational semantics\nfor it. Furthermore, our model can be exported to OCaml code for execution,\nturning the formal model into a VHDL simulator. We have tested our simulator\nagainst simple designs used in the literature, as well as the div32 module in\nthe LEON3 design. The Isabelle/HOL code is publicly available:\nhttps://zhehou.github.io/apps/VHDLModel.zip", "published": "2022-02-08 23:10:25", "link": "http://arxiv.org/abs/2202.04192v1", "categories": ["cs.CL", "cs.FL", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Enhancing ASR for Stuttered Speech with Limited Data Using Detect and\n  Pass", "abstract": "It is estimated that around 70 million people worldwide are affected by a\nspeech disorder called stuttering. With recent advances in Automatic Speech\nRecognition (ASR), voice assistants are increasingly useful in our everyday\nlives. Many technologies in education, retail, telecommunication and healthcare\ncan now be operated through voice. Unfortunately, these benefits are not\naccessible for People Who Stutter (PWS). We propose a simple but effective\nmethod called 'Detect and Pass' to make modern ASR systems accessible for\nPeople Who Stutter in a limited data setting. The algorithm uses a context\naware classifier trained on a limited amount of data, to detect acoustic frames\nthat contain stutter. To improve robustness on stuttered speech, this extra\ninformation is passed on to the ASR model to be utilized during inference. Our\nexperiments show a reduction of 12.18% to 71.24% in Word Error Rate (WER)\nacross various state of the art ASR systems. Upon varying the threshold of the\nassociated posterior probability of stutter for each stacked frame used in\ndetermining low frame rate (LFR) acoustic features, we were able to determine\nan optimal setting that reduced the WER by 23.93% to 71.67% across different\nASR systems.", "published": "2022-02-08 19:55:23", "link": "http://arxiv.org/abs/2202.05396v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "InferGrad: Improving Diffusion Models for Vocoder by Considering\n  Inference in Training", "abstract": "Denoising diffusion probabilistic models (diffusion models for short) require\na large number of iterations in inference to achieve the generation quality\nthat matches or surpasses the state-of-the-art generative models, which\ninvariably results in slow inference speed. Previous approaches aim to optimize\nthe choice of inference schedule over a few iterations to speed up inference.\nHowever, this results in reduced generation quality, mainly because the\ninference process is optimized separately, without jointly optimizing with the\ntraining process. In this paper, we propose InferGrad, a diffusion model for\nvocoder that incorporates inference process into training, to reduce the\ninference iterations while maintaining high generation quality. More\nspecifically, during training, we generate data from random noise through a\nreverse process under inference schedules with a few iterations, and impose a\nloss to minimize the gap between the generated and ground-truth data samples.\nThen, unlike existing approaches, the training of InferGrad considers the\ninference process. The advantages of InferGrad are demonstrated through\nexperiments on the LJSpeech dataset showing that InferGrad achieves better\nvoice quality than the baseline WaveGrad under same conditions while\nmaintaining the same voice quality as the baseline but with $3$x speedup ($2$\niterations for InferGrad vs $6$ iterations for WaveGrad).", "published": "2022-02-08 09:40:58", "link": "http://arxiv.org/abs/2202.03751v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Summary On The ICASSP 2022 Multi-Channel Multi-Party Meeting\n  Transcription Grand Challenge", "abstract": "The ICASSP 2022 Multi-channel Multi-party Meeting Transcription Grand\nChallenge (M2MeT) focuses on one of the most valuable and the most challenging\nscenarios of speech technologies. The M2MeT challenge has particularly set up\ntwo tracks, speaker diarization (track 1) and multi-speaker automatic speech\nrecognition (ASR) (track 2). Along with the challenge, we released 120 hours of\nreal-recorded Mandarin meeting speech data with manual annotation, including\nfar-field data collected by 8-channel microphone array as well as near-field\ndata collected by each participants' headset microphone. We briefly describe\nthe released dataset, track setups, baselines and summarize the challenge\nresults and major techniques used in the submissions.", "published": "2022-02-08 05:03:39", "link": "http://arxiv.org/abs/2202.03647v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Speech Intelligibility Enhancement Model based on Canonical\n  Correlation and Deep Learning for Hearing-Assistive Technologies", "abstract": "Current deep learning (DL) based approaches to speech intelligibility\nenhancement in noisy environments are generally trained to minimise the\ndistance between clean and enhanced speech features. These often result in\nimproved speech quality however they suffer from a lack of generalisation and\nmay not deliver the required speech intelligibility in everyday noisy\nsituations. In an attempt to address these challenges, researchers have\nexplored intelligibility-oriented (I-O) loss functions to train DL approaches\nfor robust speech enhancement (SE). In this paper, we formulate a novel\ncanonical correlation-based I-O loss function to more effectively train DL\nalgorithms. Specifically, we present a fully convolutional SE model that uses a\nmodified canonical-correlation based short-time objective intelligibility\n(CC-STOI) metric as a training cost function. To the best of our knowledge,\nthis is the first work that exploits the integration of canonical correlation\nin an I-O based loss function for SE. Comparative experimental results\ndemonstrate that our proposed CC-STOI based SE framework outperforms DL models\ntrained with conventional STOI and distance-based loss functions, in terms of\nboth standard objective and subjective evaluation measures when dealing with\nunseen speakers and noises.", "published": "2022-02-08 22:10:29", "link": "http://arxiv.org/abs/2202.04172v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CALM: Contrastive Aligned Audio-Language Multirate and Multimodal\n  Representations", "abstract": "Deriving multimodal representations of audio and lexical inputs is a central\nproblem in Natural Language Understanding (NLU). In this paper, we present\nContrastive Aligned Audio-Language Multirate and Multimodal Representations\n(CALM), an approach for learning multimodal representations using contrastive\nand multirate information inherent in audio and lexical inputs. The proposed\nmodel aligns acoustic and lexical information in the input embedding space of a\npretrained language-only contextual embedding model. By aligning audio\nrepresentations to pretrained language representations and utilizing\ncontrastive information between acoustic inputs, CALM is able to bootstrap\naudio embedding competitive with existing audio representation models in only a\nfew hours of training time. Operationally, audio spectrograms are processed\nusing linearized patches through a Spectral Transformer (SpecTran) which is\ntrained using a Contrastive Audio-Language Pretraining objective to align audio\nand language from similar queries. Subsequently, the derived acoustic and\nlexical tokens representations are input into a multimodal transformer to\nincorporate utterance level context and derive the proposed CALM\nrepresentations. We show that these pretrained embeddings can subsequently be\nused in multimodal supervised tasks and demonstrate the benefits of the\nproposed pretraining steps in terms of the alignment of the two embedding\nspaces and the multirate nature of the pretraining. Our system shows 10-25\\%\nimprovement over existing emotion recognition systems including\nstate-of-the-art three-modality systems under various evaluation objectives.", "published": "2022-02-08 01:20:37", "link": "http://arxiv.org/abs/2202.03587v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "MixCycle: Unsupervised Speech Separation via Cyclic Mixture Permutation\n  Invariant Training", "abstract": "We introduce two unsupervised source separation methods, which involve\nself-supervised training from single-channel two-source speech mixtures. Our\nfirst method, mixture permutation invariant training (MixPIT), enables learning\na neural network model which separates the underlying sources via a challenging\nproxy task without supervision from the reference sources. Our second method,\ncyclic mixture permutation invariant training (MixCycle), uses MixPIT as a\nbuilding block in a cyclic fashion for continuous learning. MixCycle gradually\nconverts the problem from separating mixtures of mixtures into separating\nsingle mixtures. We compare our methods to common supervised and unsupervised\nbaselines: permutation invariant training with dynamic mixing (PIT-DM) and\nmixture invariant training (MixIT). We show that MixCycle outperforms MixIT and\nreaches a performance level very close to the supervised baseline (PIT-DM)\nwhile circumventing the over-separation issue of MixIT. Also, we propose a\nself-evaluation technique inspired by MixCycle that estimates model performance\nwithout utilizing any reference sources. We show that it yields results\nconsistent with an evaluation on reference sources (LibriMix) and also with an\ninformal listening test conducted on a real-life mixtures dataset (REAL-M).", "published": "2022-02-08 14:02:50", "link": "http://arxiv.org/abs/2202.03875v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Time-varying harmonic models for voice signal analysis", "abstract": "Assessment of voice signals has long been performed with the assumption of\nperiodicity as this facilitates analysis. Near periodicity of normal voice\nsignals makes short-time harmonic modeling an appealing choice to extract vocal\nfeature parameters. For dysphonic voice, however, a fixed harmonic structure\ncould be too constrained as it strictly enforces periodicity in the model.\nSlight variation in amplitude or frequency in the signal may cause the model to\nmisrepresent the observed signal. To address these issues, this paper presents\na time-varying harmonic model, which allows its fundamental frequency and\nharmonic amplitudes to be polynomial functions of time. The model decouples the\nslow deviations of frequency and amplitude from fast irregular vocal fold\nvibratory behaviors such as subharmonics and diplophonia. The time-varying\nmodel is shown to track the frequency and amplitude modulations present in\nvoice with severe tremor. This reduces the sensitivity of the model-based\nharmonics-to-noise ratio measures to slow frequency and amplitude variations\nwhile maintaining its sensitivity to increase in turbulent noise or the\npresence of irregular vibration. Other uses of the model include the vocal\ntract filter estimation and the rates of frequency and intensity changes. These\nuse cases are experimentally demonstrated along with the modeling accuracy.", "published": "2022-02-08 21:19:02", "link": "http://arxiv.org/abs/2202.04150v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
