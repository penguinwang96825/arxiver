{"title": "Diversity, Density, and Homogeneity: Quantitative Characteristic Metrics\n  for Text Collections", "abstract": "Summarizing data samples by quantitative measures has a long history, with\ndescriptive statistics being a case in point. However, as natural language\nprocessing methods flourish, there are still insufficient characteristic\nmetrics to describe a collection of texts in terms of the words, sentences, or\nparagraphs they comprise. In this work, we propose metrics of diversity,\ndensity, and homogeneity that quantitatively measure the dispersion, sparsity,\nand uniformity of a text collection. We conduct a series of simulations to\nverify that each metric holds desired properties and resonates with human\nintuitions. Experiments on real-world datasets demonstrate that the proposed\ncharacteristic metrics are highly correlated with text classification\nperformance of a renowned model, BERT, which could inspire future applications.", "published": "2020-03-19 00:48:32", "link": "http://arxiv.org/abs/2003.08529v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Factual Consistency of Abstractive Summarization", "abstract": "Automatic abstractive summaries are found to often distort or fabricate facts\nin the article. This inconsistency between summary and original text has\nseriously impacted its applicability. We propose a fact-aware summarization\nmodel FASum to extract and integrate factual relations into the summary\ngeneration process via graph attention. We then design a factual corrector\nmodel FC to automatically correct factual errors from summaries generated by\nexisting systems. Empirical results show that the fact-aware summarization can\nproduce abstractive summaries with higher factual consistency compared with\nexisting systems, and the correction model improves the factual consistency of\ngiven summaries via modifying only a few keywords.", "published": "2020-03-19 07:36:10", "link": "http://arxiv.org/abs/2003.08612v8", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Utilizing Language Relatedness to improve Machine Translation: A Case\n  Study on Languages of the Indian Subcontinent", "abstract": "In this work, we present an extensive study of statistical machine\ntranslation involving languages of the Indian subcontinent. These languages are\nrelated by genetic and contact relationships. We describe the similarities\nbetween Indic languages arising from these relationships. We explore how\nlexical and orthographic similarity among these languages can be utilized to\nimprove translation quality between Indic languages when limited parallel\ncorpora is available. We also explore how the structural correspondence between\nIndic languages can be utilized to re-use linguistic resources for English to\nIndic language translation. Our observations span 90 language pairs from 9\nIndic languages and English. To the best of our knowledge, this is the first\nlarge-scale study specifically devoted to utilizing language relatedness to\nimprove translation between related languages.", "published": "2020-03-19 17:43:28", "link": "http://arxiv.org/abs/2003.08925v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NSURL-2019 Task 7: Named Entity Recognition (NER) in Farsi", "abstract": "NSURL-2019 Task 7 focuses on Named Entity Recognition (NER) in Farsi. This\ntask was chosen to compare different approaches to find phrases that specify\nNamed Entities in Farsi texts, and to establish a standard testbed for future\nresearches on this task in Farsi. This paper describes the process of making\ntraining and test data, a list of participating teams (6 teams), and evaluation\nresults of their systems. The best system obtained 85.4% of F1 score based on\nphrase-level evaluation on seven classes of NEs including person, organization,\nlocation, date, time, money and percent.", "published": "2020-03-19 22:09:51", "link": "http://arxiv.org/abs/2003.09029v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QnAMaker: Data to Bot in 2 Minutes", "abstract": "Having a bot for seamless conversations is a much-desired feature that\nproducts and services today seek for their websites and mobile apps. These bots\nhelp reduce traffic received by human support significantly by handling\nfrequent and directly answerable known questions. Many such services have huge\nreference documents such as FAQ pages, which makes it hard for users to browse\nthrough this data. A conversation layer over such raw data can lower traffic to\nhuman support by a great margin. We demonstrate QnAMaker, a service that\ncreates a conversational layer over semi-structured data such as FAQ pages,\nproduct manuals, and support documents. QnAMaker is the popular choice for\nExtraction and Question-Answering as a service and is used by over 15,000 bots\nin production. It is also used by search interfaces and not just bots.", "published": "2020-03-19 03:32:03", "link": "http://arxiv.org/abs/2003.08553v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Temporal Embeddings and Transformer Models for Narrative Text\n  Understanding", "abstract": "We present two deep learning approaches to narrative text understanding for\ncharacter relationship modelling. The temporal evolution of these relations is\ndescribed by dynamic word embeddings, that are designed to learn semantic\nchanges over time. An empirical analysis of the corresponding character\ntrajectories shows that such approaches are effective in depicting dynamic\nevolution. A supervised learning approach based on the state-of-the-art\ntransformer model BERT is used instead to detect static relations between\ncharacters. The empirical validation shows that such events (e.g., two\ncharacters belonging to the same family) might be spotted with good accuracy,\neven when using automatically annotated data. This provides a deeper\nunderstanding of narrative plots based on the identification of key facts.\nStandard clustering techniques are finally used for character de-aliasing, a\nnecessary pre-processing step for both approaches. Overall, deep learning\nmodels appear to be suitable for narrative text understanding, while also\nproviding a challenging and unexploited benchmark for general natural language\nunderstanding.", "published": "2020-03-19 14:23:12", "link": "http://arxiv.org/abs/2003.08811v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beheshti-NER: Persian Named Entity Recognition Using BERT", "abstract": "Named entity recognition is a natural language processing task to recognize\nand extract spans of text associated with named entities and classify them in\nsemantic Categories.\n  Google BERT is a deep bidirectional language model, pre-trained on large\ncorpora that can be fine-tuned to solve many NLP tasks such as question\nanswering, named entity recognition, part of speech tagging and etc. In this\npaper, we use the pre-trained deep bidirectional network, BERT, to make a model\nfor named entity recognition in Persian.\n  We also compare the results of our model with the previous state of the art\nresults achieved on Persian NER. Our evaluation metric is CONLL 2003 score in\ntwo levels of word and phrase. This model achieved second place in NSURL-2019\ntask 7 competition which associated with NER for the Persian language. our\nresults in this competition are 83.5 and 88.4 f1 CONLL score respectively in\nphrase and word level evaluation.", "published": "2020-03-19 15:55:21", "link": "http://arxiv.org/abs/2003.08875v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The value of text for small business default prediction: A deep learning\n  approach", "abstract": "Compared to consumer lending, Micro, Small and Medium Enterprise (mSME)\ncredit risk modelling is particularly challenging, as, often, the same sources\nof information are not available. Therefore, it is standard policy for a loan\nofficer to provide a textual loan assessment to mitigate limited data\navailability. In turn, this statement is analysed by a credit expert alongside\nany available standard credit data. In our paper, we exploit recent advances\nfrom the field of Deep Learning and Natural Language Processing (NLP),\nincluding the BERT (Bidirectional Encoder Representations from Transformers)\nmodel, to extract information from 60 000 textual assessments provided by a\nlender. We consider the performance in terms of the AUC (Area Under the\nreceiver operating characteristic Curve) and Brier Score metrics and find that\nthe text alone is surprisingly effective for predicting default. However, when\ncombined with traditional data, it yields no additional predictive capability,\nwith performance dependent on the text's length. Our proposed deep learning\nmodel does, however, appear to be robust to the quality of the text and\ntherefore suitable for partly automating the mSME lending process. We also\ndemonstrate how the content of loan assessments influences performance, leading\nus to a series of recommendations on a new strategy for collecting future mSME\nloan assessments.", "published": "2020-03-19 18:15:05", "link": "http://arxiv.org/abs/2003.08964v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Techniques for Vocabulary Expansion in Hybrid Speech Recognition Systems", "abstract": "The problem of out of vocabulary words (OOV) is typical for any speech\nrecognition system, hybrid systems are usually constructed to recognize a fixed\nset of words and rarely can include all the words that will be encountered\nduring exploitation of the system. One of the popular approach to cover OOVs is\nto use subword units rather then words. Such system can potentially recognize\nany previously unseen word if the word can be constructed from present subword\nunits, but also non-existing words can be recognized. The other popular\napproach is to modify HMM part of the system so that it can be easily and\neffectively expanded with custom set of words we want to add to the system. In\nthis paper we explore different existing methods of this solution on both graph\nconstruction and search method levels. We also present a novel vocabulary\nexpansion techniques which solve some common internal subroutine problems\nregarding recognition graph processing.", "published": "2020-03-19 21:24:45", "link": "http://arxiv.org/abs/2003.09024v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EQL -- an extremely easy to learn knowledge graph query language,\n  achieving highspeed and precise search", "abstract": "EQL, also named as Extremely Simple Query Language, can be widely used in the\nfield of knowledge graph, precise search, strong artificial intelligence,\ndatabase, smart speaker ,patent search and other fields. EQL adopt the\nprinciple of minimalism in design and pursues simplicity and easy to learn so\nthat everyone can master it quickly. EQL language and lambda calculus are\ninterconvertible, that reveals the mathematical nature of EQL language, and\nlays a solid foundation for rigor and logical integrity of EQL language. The\nEQL language and a comprehensive knowledge graph system with the world's\ncommonsense can together form the foundation of strong AI in the future, and\nmake up for the current lack of understanding of world's commonsense by current\nAI system. EQL language can be used not only by humans, but also as a basic\nlanguage for data query and data exchange between robots.", "published": "2020-03-19 03:32:04", "link": "http://arxiv.org/abs/2003.11105v1", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Giving Commands to a Self-driving Car: A Multimodal Reasoner for Visual\n  Grounding", "abstract": "We propose a new spatial memory module and a spatial reasoner for the Visual\nGrounding (VG) task. The goal of this task is to find a certain object in an\nimage based on a given textual query. Our work focuses on integrating the\nregions of a Region Proposal Network (RPN) into a new multi-step reasoning\nmodel which we have named a Multimodal Spatial Region Reasoner (MSRR). The\nintroduced model uses the object regions from an RPN as initialization of a 2D\nspatial memory and then implements a multi-step reasoning process scoring each\nregion according to the query, hence why we call it a multimodal reasoner. We\nevaluate this new model on challenging datasets and our experiments show that\nour model that jointly reasons over the object regions of the image and words\nof the query largely improves accuracy compared to current state-of-the-art\nmodels.", "published": "2020-03-19 12:40:41", "link": "http://arxiv.org/abs/2003.08717v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Normalized and Geometry-Aware Self-Attention Network for Image\n  Captioning", "abstract": "Self-attention (SA) network has shown profound value in image captioning. In\nthis paper, we improve SA from two aspects to promote the performance of image\ncaptioning. First, we propose Normalized Self-Attention (NSA), a\nreparameterization of SA that brings the benefits of normalization inside SA.\nWhile normalization is previously only applied outside SA, we introduce a novel\nnormalization method and demonstrate that it is both possible and beneficial to\nperform it on the hidden activations inside SA. Second, to compensate for the\nmajor limit of Transformer that it fails to model the geometry structure of the\ninput objects, we propose a class of Geometry-aware Self-Attention (GSA) that\nextends SA to explicitly and efficiently consider the relative geometry\nrelations between the objects in the image. To construct our image captioning\nmodel, we combine the two modules and apply it to the vanilla self-attention\nnetwork. We extensively evaluate our proposals on MS-COCO image captioning\ndataset and superior results are achieved when comparing to state-of-the-art\napproaches. Further experiments on three challenging tasks, i.e. video\ncaptioning, machine translation, and visual question answering, show the\ngenerality of our methods.", "published": "2020-03-19 16:54:16", "link": "http://arxiv.org/abs/2003.08897v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Predicting Unplanned Readmissions with Highly Unstructured Data", "abstract": "Deep learning techniques have been successfully applied to predict unplanned\nreadmissions of patients in medical centers. The training data for these models\nis usually based on historical medical records that contain a significant\namount of free-text from admission reports, referrals, exam notes, etc. Most of\nthe models proposed so far are tailored to English text data and assume that\nelectronic medical records follow standards common in developed countries.\nThese two characteristics make them difficult to apply in developing countries\nthat do not necessarily follow international standards for registering patient\ninformation, or that store text information in languages other than English.\n  In this paper we propose a deep learning architecture for predicting\nunplanned readmissions that consumes data that is significantly less structured\ncompared with previous models in the literature. We use it to present the first\nresults for this task in a large clinical dataset that mainly contains Spanish\ntext data. The dataset is composed of almost 10 years of records in a Chilean\nmedical center. On this dataset, our model achieves results that are comparable\nto some of the most recent results obtained in US medical centers for the same\ntask (0.76 AUROC).", "published": "2020-03-19 23:21:00", "link": "http://arxiv.org/abs/2003.11622v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Voice and accompaniment separation in music using self-attention\n  convolutional neural network", "abstract": "Music source separation has been a popular topic in signal processing for\ndecades, not only because of its technical difficulty, but also due to its\nimportance to many commercial applications, such as automatic karoake and\nremixing. In this work, we propose a novel self-attention network to separate\nvoice and accompaniment in music. First, a convolutional neural network (CNN)\nwith densely-connected CNN blocks is built as our base network. We then insert\nself-attention subnets at different levels of the base CNN to make use of the\nlong-term intra-dependency of music, i.e., repetition. Within self-attention\nsubnets, repetitions of the same musical patterns inform reconstruction of\nother repetitions, for better source separation performance. Results show the\nproposed method leads to 19.5% relative improvement in vocals separation in\nterms of SDR. We compare our methods with state-of-the-art systems i.e.\nMMDenseNet and MMDenseLSTM.", "published": "2020-03-19 18:00:56", "link": "http://arxiv.org/abs/2003.08954v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
