{"title": "Accenture at CheckThat! 2020: If you say so: Post-hoc fact-checking of\n  claims using transformer-based models", "abstract": "We introduce the strategies used by the Accenture Team for the CLEF2020\nCheckThat! Lab, Task 1, on English and Arabic. This shared task evaluated\nwhether a claim in social media text should be professionally fact checked. To\na journalist, a statement presented as fact, which would be of interest to a\nlarge audience, requires professional fact-checking before dissemination. We\nutilized BERT and RoBERTa models to identify claims in social media text a\nprofessional fact-checker should review, and rank these in priority order for\nthe fact-checker. For the English challenge, we fine-tuned a RoBERTa model and\nadded an extra mean pooling layer and a dropout layer to enhance\ngeneralizability to unseen text. For the Arabic task, we fine-tuned\nArabic-language BERT models and demonstrate the use of back-translation to\namplify the minority class and balance the dataset. The work presented here was\nscored 1st place in the English track, and 1st, 2nd, 3rd, and 4th place in the\nArabic track.", "published": "2020-09-05 01:44:11", "link": "http://arxiv.org/abs/2009.02431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis and representation of Igbo text document for a text-based\n  system", "abstract": "The advancement in Information Technology (IT) has assisted in inculcating\nthe three Nigeria major languages in text-based application such as text\nmining, information retrieval and natural language processing. The interest of\nthis paper is the Igbo language, which uses compounding as a common type of\nword formation and as well has many vocabularies of compound words. The issues\nof collocation, word ordering and compounding play high role in Igbo language.\nThe ambiguity in dealing with these compound words has made the representation\nof Igbo language text document very difficult because this cannot be addressed\nusing the most common and standard approach of the Bag-Of-Words (BOW) model of\ntext representation, which ignores the word order and relation. However, this\ncause for a concern and the need to develop an improved model to capture this\nsituation. This paper presents the analysis of Igbo language text document,\nconsidering its compounding nature and describes its representation with the\nWord-based N-gram model to properly prepare it for any text-based application.\nThe result shows that Bigram and Trigram n-gram text representation models\nprovide more semantic information as well addresses the issues of compounding,\nword ordering and collocations which are the major language peculiarities in\nIgbo. They are likely to give better performance when used in any Igbo\ntext-based system.", "published": "2020-09-05 19:07:17", "link": "http://arxiv.org/abs/2009.06376v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bio-inspired Structure Identification in Language Embeddings", "abstract": "Word embeddings are a popular way to improve downstream performances in\ncontemporary language modeling. However, the underlying geometric structure of\nthe embedding space is not well understood. We present a series of explorations\nusing bio-inspired methodology to traverse and visualize word embeddings,\ndemonstrating evidence of discernible structure. Moreover, our model also\nproduces word similarity rankings that are plausible yet very different from\ncommon similarity metrics, mainly cosine similarity and Euclidean distance. We\nshow that our bio-inspired model can be used to investigate how different word\nembedding techniques result in different semantic outputs, which can emphasize\nor obscure particular interpretations in textual data.", "published": "2020-09-05 04:44:15", "link": "http://arxiv.org/abs/2009.02459v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Profiling US Restaurants from Billions of Payment Card Transactions", "abstract": "A payment card (such as debit or credit) is one of the most convenient\npayment methods for purchasing goods and services. Hundreds of millions of card\ntransactions take place across the globe every day, generating a massive volume\nof transaction data. The data render a holistic view of cardholder-merchant\ninteractions, containing insights that can benefit various applications, such\nas payment fraud detection and merchant recommendation. However, utilizing\nthese insights often requires additional information about merchants missing\nfrom the data owner's (i.e., payment company's) perspective. For example,\npayment companies do not know the exact type of product a merchant serves.\nCollecting merchant attributes from external sources for commercial purposes\ncan be expensive. Motivated by this limitation, we aim to infer latent merchant\nattributes from transaction data. As proof of concept, we concentrate on\nrestaurants and infer the cuisine types of restaurants from transactions. To\nthis end, we present a framework for inferring the cuisine types of restaurants\nfrom transaction data. Our proposed framework consists of three steps. In the\nfirst step, we generate cuisine labels for a limited number of restaurants via\nweak supervision. In the second step, we extract a wide variety of statistical\nfeatures and neural embeddings from the restaurant transactions. In the third\nstep, we use deep neural networks (DNNs) to infer the remaining restaurants'\ncuisine types. The proposed framework achieved a 76.2% accuracy in classifying\nthe US restaurants. To the best of our knowledge, this is the first framework\nto infer the cuisine types of restaurants by analyzing transaction data as the\nonly source.", "published": "2020-09-05 04:50:27", "link": "http://arxiv.org/abs/2009.02461v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Visually Analyzing Contextualized Embeddings", "abstract": "In this paper we introduce a method for visually analyzing contextualized\nembeddings produced by deep neural network-based language models. Our approach\nis inspired by linguistic probes for natural language processing, where tasks\nare designed to probe language models for linguistic structure, such as\nparts-of-speech and named entities. These approaches are largely confirmatory,\nhowever, only enabling a user to test for information known a priori. In this\nwork, we eschew supervised probing tasks, and advocate for unsupervised probes,\ncoupled with visual exploration techniques, to assess what is learned by\nlanguage models. Specifically, we cluster contextualized embeddings produced\nfrom a large text corpus, and introduce a visualization design based on this\nclustering and textual structure - cluster co-occurrences, cluster spans, and\ncluster-word membership - to help elicit the functionality of, and relationship\nbetween, individual clusters. User feedback highlights the benefits of our\ndesign in discovering different types of linguistic structures.", "published": "2020-09-05 15:40:51", "link": "http://arxiv.org/abs/2009.02554v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "A multi-view approach for Mandarin non-native mispronunciation\n  verification", "abstract": "Traditionally, the performance of non-native mispronunciation verification\nsystems relied on effective phone-level labelling of non-native corpora. In\nthis study, a multi-view approach is proposed to incorporate discriminative\nfeature representations which requires less annotation for non-native\nmispronunciation verification of Mandarin. Here, models are jointly learned to\nembed acoustic sequence and multi-source information for speech attributes and\nbottleneck features. Bidirectional LSTM embedding models with contrastive\nlosses are used to map acoustic sequences and multi-source information into\nfixed-dimensional embeddings. The distance between acoustic embeddings is taken\nas the similarity between phones. Accordingly, examples of mispronounced phones\nare expected to have a small similarity score with their canonical\npronunciations. The approach shows improvement over GOP-based approach by\n+11.23% and single-view approach by +1.47% in diagnostic accuracy for a\nmispronunciation verification task.", "published": "2020-09-05 17:42:39", "link": "http://arxiv.org/abs/2009.02573v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cross-domain Adaptation with Discrepancy Minimization for\n  Text-independent Forensic Speaker Verification", "abstract": "Forensic audio analysis for speaker verification offers unique challenges due\nto location/scenario uncertainty and diversity mismatch between reference and\nnaturalistic field recordings. The lack of real naturalistic forensic audio\ncorpora with ground-truth speaker identity represents a major challenge in this\nfield. It is also difficult to directly employ small-scale domain-specific data\nto train complex neural network architectures due to domain mismatch and loss\nin performance. Alternatively, cross-domain speaker verification for multiple\nacoustic environments is a challenging task which could advance research in\naudio forensics. In this study, we introduce a CRSS-Forensics audio dataset\ncollected in multiple acoustic environments. We pre-train a CNN-based network\nusing the VoxCeleb data, followed by an approach which fine-tunes part of the\nhigh-level network layers with clean speech from CRSS-Forensics. Based on this\nfine-tuned model, we align domain-specific distributions in the embedding space\nwith the discrepancy loss and maximum mean discrepancy (MMD). This maintains\neffective performance on the clean set, while simultaneously generalizes the\nmodel to other acoustic domains. From the results, we demonstrate that diverse\nacoustic environments affect the speaker verification performance, and that our\nproposed approach of cross-domain adaptation can significantly improve the\nresults in this scenario.", "published": "2020-09-05 02:54:33", "link": "http://arxiv.org/abs/2009.02444v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Semi-supervised Multi-modal Emotion Recognition with Cross-Modal\n  Distribution Matching", "abstract": "Automatic emotion recognition is an active research topic with wide range of\napplications. Due to the high manual annotation cost and inevitable label\nambiguity, the development of emotion recognition dataset is limited in both\nscale and quality. Therefore, one of the key challenges is how to build\neffective models with limited data resource. Previous works have explored\ndifferent approaches to tackle this challenge including data enhancement,\ntransfer learning, and semi-supervised learning etc. However, the weakness of\nthese existing approaches includes such as training instability, large\nperformance loss during transfer, or marginal improvement.\n  In this work, we propose a novel semi-supervised multi-modal emotion\nrecognition model based on cross-modality distribution matching, which\nleverages abundant unlabeled data to enhance the model training under the\nassumption that the inner emotional status is consistent at the utterance level\nacross modalities.\n  We conduct extensive experiments to evaluate the proposed model on two\nbenchmark datasets, IEMOCAP and MELD. The experiment results prove that the\nproposed semi-supervised learning model can effectively utilize unlabeled data\nand combine multi-modalities to boost the emotion recognition performance,\nwhich outperforms other state-of-the-art approaches under the same condition.\nThe proposed model also achieves competitive capacity compared with existing\napproaches which take advantage of additional auxiliary information such as\nspeaker and interaction context.", "published": "2020-09-05 20:51:01", "link": "http://arxiv.org/abs/2009.02598v1", "categories": ["eess.AS", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
