{"title": "Multilingual Text Classification for Dravidian Languages", "abstract": "As the fourth largest language family in the world, the Dravidian languages\nhave become a research hotspot in natural language processing (NLP). Although\nthe Dravidian languages contain a large number of languages, there are\nrelatively few public available resources. Besides, text classification task,\nas a basic task of natural language processing, how to combine it to multiple\nlanguages in the Dravidian languages, is still a major difficulty in Dravidian\nNatural Language Processing. Hence, to address these problems, we proposed a\nmultilingual text classification framework for the Dravidian languages. On the\none hand, the framework used the LaBSE pre-trained model as the base model.\nAiming at the problem of text information bias in multi-task learning, we\npropose to use the MLM strategy to select language-specific words, and used\nadversarial training to perturb them. On the other hand, in view of the problem\nthat the model cannot well recognize and utilize the correlation among\nlanguages, we further proposed a language-specific representation module to\nenrich semantic information for the model. The experimental results\ndemonstrated that the framework we proposed has a significant performance in\nmultilingual text classification tasks with each strategy achieving certain\nimprovements.", "published": "2021-12-03 04:26:49", "link": "http://arxiv.org/abs/2112.01705v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multitask Finetuning for Improving Neural Machine Translation in Indian\n  Languages", "abstract": "Transformer based language models have led to impressive results across all\ndomains in Natural Language Processing. Pretraining these models on language\nmodeling tasks and finetuning them on downstream tasks such as Text\nClassification, Question Answering and Neural Machine Translation has\nconsistently shown exemplary results. In this work, we propose a Multitask\nFinetuning methodology which combines the Bilingual Machine Translation task\nwith an auxiliary Causal Language Modeling task to improve performance on the\nformer task on Indian Languages. We conduct an empirical study on three\nlanguage pairs, Marathi-Hindi, Marathi-English and Hindi-English, where we\ncompare the multitask finetuning approach to the standard finetuning approach,\nfor which we use the mBART50 model. Our study indicates that the multitask\nfinetuning method could be a better technique than standard finetuning, and\ncould improve Bilingual Machine Translation across language pairs.", "published": "2021-12-03 06:43:56", "link": "http://arxiv.org/abs/2112.01742v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Creating and Managing a large annotated parallel corpora of Indian\n  languages", "abstract": "This paper presents the challenges in creating and managing large parallel\ncorpora of 12 major Indian languages (which is soon to be extended to 23\nlanguages) as part of a major consortium project funded by the Department of\nInformation Technology (DIT), Govt. of India, and running parallel in 10\ndifferent universities of India. In order to efficiently manage the process of\ncreation and dissemination of these huge corpora, the web-based (with a reduced\nstand-alone version also) annotation tool ILCIANN (Indian Languages Corpora\nInitiative Annotation Tool) has been developed. It was primarily developed for\nthe POS annotation as well as the management of the corpus annotation by people\nwith differing amount of competence and at locations physically situated far\napart. In order to maintain consistency and standards in the creation of the\ncorpora, it was necessary that everyone works on a common platform which was\nprovided by this tool.", "published": "2021-12-03 07:44:22", "link": "http://arxiv.org/abs/2112.01764v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Translating Politeness Across Cultures: Case of Hindi and English", "abstract": "In this paper, we present a corpus based study of politeness across two\nlanguages-English and Hindi. It studies the politeness in a translated parallel\ncorpus of Hindi and English and sees how politeness in a Hindi text is\ntranslated into English. We provide a detailed theoretical background in which\nthe comparison is carried out, followed by a brief description of the\ntranslated data within this theoretical model. Since politeness may become one\nof the major reasons of conflict and misunderstanding, it is a very important\nphenomenon to be studied and understood cross-culturally, particularly for such\npurposes as machine translation.", "published": "2021-12-03 10:22:02", "link": "http://arxiv.org/abs/2112.01822v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating NLP Systems On a Novel Cloze Task: Judging the Plausibility\n  of Possible Fillers in Instructional Texts", "abstract": "Cloze task is a widely used task to evaluate an NLP system's language\nunderstanding ability. However, most of the existing cloze tasks only require\nNLP systems to give the relative best prediction for each input data sample,\nrather than the absolute quality of all possible predictions, in a consistent\nway across the input domain. Thus a new task is proposed: predicting if a\nfiller word in a cloze task is a good, neutral, or bad candidate. Complicated\nversions can be extended to predicting more discrete classes or continuous\nscores. We focus on subtask A in Semeval 2022 task 7, explored some possible\narchitectures to solve this new task, provided a detailed comparison of them,\nand proposed an ensemble method to improve traditional models in this new task.", "published": "2021-12-03 12:02:52", "link": "http://arxiv.org/abs/2112.01867v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HS-BAN: A Benchmark Dataset of Social Media Comments for Hate Speech\n  Detection in Bangla", "abstract": "In this paper, we present HS-BAN, a binary class hate speech (HS) dataset in\nBangla language consisting of more than 50,000 labeled comments, including\n40.17% hate and rest are non hate speech. While preparing the dataset a strict\nand detailed annotation guideline was followed to reduce human annotation bias.\nThe HS dataset was also preprocessed linguistically to extract different types\nof slang currently people write using symbols, acronyms, or alternative\nspellings. These slang words were further categorized into traditional and\nnon-traditional slang lists and included in the results of this paper. We\nexplored traditional linguistic features and neural network-based methods to\ndevelop a benchmark system for hate speech detection for the Bangla language.\nOur experimental results show that existing word embedding models trained with\ninformal texts perform better than those trained with formal text. Our\nbenchmark shows that a Bi-LSTM model on top of the FastText informal word\nembedding achieved 86.78% F1-score. We will make the dataset available for\npublic use.", "published": "2021-12-03 13:35:18", "link": "http://arxiv.org/abs/2112.01902v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmenting Customer Support with an NLP-based Receptionist", "abstract": "In this paper, we show how a Portuguese BERT model can be combined with\nstructured data in order to deploy a chatbot based on a finite state machine to\ncreate a conversational AI system that helps a real-estate company to predict\nits client's contact motivation. The model achieves human level results in a\ndataset that contains 235 unbalanced labels. Then, we also show its benefits\nconsidering the business impact comparing it against classical NLP methods.", "published": "2021-12-03 15:03:25", "link": "http://arxiv.org/abs/2112.01959v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploratory Data Analysis of Urdu Poetry", "abstract": "The study presented here provides numerical insight into ghazal -- the most\nappreciated genre in Urdu poetry. Using 48,761 poetic works from 4,754 poets\nproduced over a period of 800 years, this study explores the main features of\nUrdu ghazal that make it popular and admired more than other forms. A detailed\nexplanation is provided as to the types of words used for expressing love,\nnature, birds, and flowers etc. Also considered is the way in which the poets\naddressed their loved ones in their poetry. The style of poetry is numerically\nanalyzed using Multi Dimensional Scaling to reveal the lexical diversity and\nsimilarities/differences between the different poetic works that have drawn the\nattention of critics, such as Iqbal and Ghalib, Mir Taqi Mir and Mir Dard. The\nanalysis produced here is particularly helpful for research in computational\nstylistics, neurocognitive poetics, and sentiment analysis.", "published": "2021-12-03 20:06:11", "link": "http://arxiv.org/abs/2112.02145v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Influence of Data Pre-processing and Post-processing on Long\n  Document Summarization", "abstract": "Long document summarization is an important and hard task in the field of\nnatural language processing. A good performance of the long document\nsummarization reveals the model has a decent understanding of the human\nlanguage. Currently, most researches focus on how to modify the attention\nmechanism of the transformer to achieve a higher ROUGE score. The study of data\npre-processing and post-processing are relatively few. In this paper, we use\ntwo pre-processing methods and a post-processing method and analyze the effect\nof these methods on various long document summarization models.", "published": "2021-12-03 00:56:17", "link": "http://arxiv.org/abs/2112.01660v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TransCouplet:Transformer based Chinese Couplet Generation", "abstract": "Chinese couplet is a special form of poetry composed of complex syntax with\nancient Chinese language. Due to the complexity of semantic and grammatical\nrules, creation of a suitable couplet is a formidable challenge. This paper\npresents a transformer-based sequence-to-sequence couplet generation model.\nWith the utilization of AnchiBERT, the model is able to capture ancient Chinese\nlanguage understanding. Moreover, we evaluate the Glyph, PinYin and\nPart-of-Speech tagging on the couplet grammatical rules to further improve the\nmodel.", "published": "2021-12-03 04:34:48", "link": "http://arxiv.org/abs/2112.01707v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Given Users Recommendations Based on Reviews on Yelp", "abstract": "In our project, we focus on NLP-based hybrid recommendation systems. Our data\nis from Yelp Data. For our hybrid recommendation system, we have two major\ncomponents: the first part is to embed the reviews with the Bert model and\nword2vec model; the second part is the implementation of an item-based\ncollaborative filtering algorithm to compute the similarity of each review\nunder different categories of restaurants. In the end, with the help of\nsimilarity scores, we are able to recommend users the most matched restaurant\nbased on their recorded reviews. The coding work is split into several parts:\nselecting samples and data cleaning, processing, embedding, computing\nsimilarity, and computing prediction and error. Due to the size of the data,\neach part will generate one or more JSON files as the milestone to reduce the\npressure on memory and the communication between each part.", "published": "2021-12-03 07:42:45", "link": "http://arxiv.org/abs/2112.01762v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Siamese BERT-based Model for Web Search Relevance Ranking Evaluated on a\n  New Czech Dataset", "abstract": "Web search engines focus on serving highly relevant results within hundreds\nof milliseconds. Pre-trained language transformer models such as BERT are\ntherefore hard to use in this scenario due to their high computational demands.\nWe present our real-time approach to the document ranking problem leveraging a\nBERT-based siamese architecture. The model is already deployed in a commercial\nsearch engine and it improves production performance by more than 3%. For\nfurther research and evaluation, we release DaReCzech, a unique data set of 1.6\nmillion Czech user query-document pairs with manually assigned relevance\nlevels. We also release Small-E-Czech, an Electra-small language model\npre-trained on a large Czech corpus. We believe this data will support\nendeavours both of search relevance and multilingual-focused research\ncommunities.", "published": "2021-12-03 09:45:18", "link": "http://arxiv.org/abs/2112.01810v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "The Catalan Language CLUB", "abstract": "The Catalan Language Understanding Benchmark (CLUB) encompasses various\ndatasets representative of different NLU tasks that enable accurate evaluations\nof language models, following the General Language Understanding Evaluation\n(GLUE) example. It is part of AINA and PlanTL, two public funding initiatives\nto empower the Catalan language in the Artificial Intelligence era.", "published": "2021-12-03 13:15:17", "link": "http://arxiv.org/abs/2112.01894v1", "categories": ["cs.CL", "cs.AI", "91F20", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Linear algebra with transformers", "abstract": "Transformers can learn to perform numerical computations from examples only.\nI study nine problems of linear algebra, from basic matrix operations to\neigenvalue decomposition and inversion, and introduce and discuss four encoding\nschemes to represent real numbers. On all problems, transformers trained on\nsets of random matrices achieve high accuracies (over 90%). The models are\nrobust to noise, and can generalize out of their training distribution. In\nparticular, models trained to predict Laplace-distributed eigenvalues\ngeneralize to different classes of matrices: Wigner matrices or matrices with\npositive eigenvalues. The reverse is not true.", "published": "2021-12-03 13:21:57", "link": "http://arxiv.org/abs/2112.01898v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MetaQA: Combining Expert Agents for Multi-Skill Question Answering", "abstract": "The recent explosion of question answering (QA) datasets and models has\nincreased the interest in the generalization of models across multiple domains\nand formats by either training on multiple datasets or by combining multiple\nmodels. Despite the promising results of multi-dataset models, some domains or\nQA formats may require specific architectures, and thus the adaptability of\nthese models might be limited. In addition, current approaches for combining\nmodels disregard cues such as question-answer compatibility. In this work, we\npropose to combine expert agents with a novel, flexible, and training-efficient\narchitecture that considers questions, answer predictions, and\nanswer-prediction confidence scores to select the best answer among a list of\nanswer candidates. Through quantitative and qualitative experiments we show\nthat our model i) creates a collaboration between agents that outperforms\nprevious multi-agent and multi-dataset approaches in both in-domain and\nout-of-domain scenarios, ii) is highly data-efficient to train, and iii) can be\nadapted to any QA format. We release our code and a dataset of answer\npredictions from expert agents for 16 QA datasets to foster future developments\nof multi-agent systems on https://github.com/UKPLab/MetaQA.", "published": "2021-12-03 14:05:52", "link": "http://arxiv.org/abs/2112.01922v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ESAN: Efficient Sentiment Analysis Network of A-Shares Research Reports\n  for Stock Price Prediction", "abstract": "In this paper, we are going to develop a natural language processing model to\nhelp us to predict stocks in the long term. The whole network includes two\nmodules. The first module is a natural language processing model which seeks\nout reliable factors from input reports. While the other is a time-series\nforecasting model which takes the factors as input and aims to predict stocks\nearnings yield. To indicate the efficiency of our model to combine the\nsentiment analysis module and the time-series forecasting module, we name our\nmethod ESAN.", "published": "2021-12-03 01:33:28", "link": "http://arxiv.org/abs/2112.11444v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Probing Linguistic Information For Logical Inference In Pre-trained\n  Language Models", "abstract": "Progress in pre-trained language models has led to a surge of impressive\nresults on downstream tasks for natural language understanding. Recent work on\nprobing pre-trained language models uncovered a wide range of linguistic\nproperties encoded in their contextualized representations. However, it is\nunclear whether they encode semantic knowledge that is crucial to symbolic\ninference methods. We propose a methodology for probing linguistic information\nfor logical inference in pre-trained language model representations. Our\nprobing datasets cover a list of linguistic phenomena required by major\nsymbolic inference systems. We find that (i) pre-trained language models do\nencode several types of linguistic information for inference, but there are\nalso some types of information that are weakly encoded, (ii) language models\ncan effectively learn missing linguistic information through fine-tuning.\nOverall, our findings provide insights into which aspects of linguistic\ninformation for logical inference do language models and their pre-training\nprocedures capture. Moreover, we have demonstrated language models' potential\nas semantic and background knowledge bases for supporting symbolic inference\nmethods.", "published": "2021-12-03 07:19:42", "link": "http://arxiv.org/abs/2112.01753v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BBS-KWS:The Mandarin Keyword Spotting System Won the Video Keyword\n  Wakeup Challenge", "abstract": "This paper introduces the system submitted by the Yidun NISP team to the\nvideo keyword wakeup challenge. We propose a mandarin keyword spotting system\n(KWS) with several novel and effective improvements, including a big backbone\n(B) model, a keyword biasing (B) mechanism and the introduction of syllable\nmodeling units (S). By considering this, we term the total system BBS-KWS as an\nabbreviation. The BBS-KWS system consists of an end-to-end automatic speech\nrecognition (ASR) module and a KWS module. The ASR module converts speech\nfeatures to text representations, which applies a big backbone network to the\nacoustic model and takes syllable modeling units into consideration as well. In\naddition, the keyword biasing mechanism is used to improve the recall rate of\nkeywords in the ASR inference stage. The KWS module applies multiple criteria\nto determine the absence or presence of the keywords, such as multi-stage\nmatching, fuzzy matching, and connectionist temporal classification (CTC)\nprefix score. To further improve our system, we conduct semi-supervised\nlearning on the CN-Celeb dataset for better generalization. In the VKW task,\nthe BBS-KWS system achieves significant gains over the baseline and won the\nfirst place in two tracks.", "published": "2021-12-03 07:27:13", "link": "http://arxiv.org/abs/2112.01757v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Catch Me If You Can: Blackbox Adversarial Attacks on Automatic Speech\n  Recognition using Frequency Masking", "abstract": "Automatic speech recognition (ASR) models are prevalent, particularly in\napplications for voice navigation and voice control of domestic appliances. The\ncomputational core of ASRs are deep neural networks (DNNs) that have been shown\nto be susceptible to adversarial perturbations; easily misused by attackers to\ngenerate malicious outputs. To help test the security and robustnesss of ASRS,\nwe propose techniques that generate blackbox (agnostic to the DNN), untargeted\nadversarial attacks that are portable across ASRs. This is in contrast to\nexisting work that focuses on whitebox targeted attacks that are time consuming\nand lack portability.\n  Our techniques generate adversarial attacks that have no human audible\ndifference by manipulating the audio signal using a psychoacoustic model that\nmaintains the audio perturbations below the thresholds of human perception. We\nevaluate portability and effectiveness of our techniques using three popular\nASRs and two input audio datasets using the metrics - Word Error Rate (WER) of\noutput transcription, Similarity to original audio, attack Success Rate on\ndifferent ASRs and Detection score by a defense system. We found our\nadversarial attacks were portable across ASRs, not easily detected by a\nstate-of-the-art defense system, and had significant difference in output\ntranscriptions while sounding similar to original audio.", "published": "2021-12-03 10:21:47", "link": "http://arxiv.org/abs/2112.01821v2", "categories": ["cs.SD", "cs.CL", "cs.SE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Semantic Segmentation of Legal Documents via Rhetorical Roles", "abstract": "Legal documents are unstructured, use legal jargon, and have considerable\nlength, making them difficult to process automatically via conventional text\nprocessing techniques. A legal document processing system would benefit\nsubstantially if the documents could be segmented into coherent information\nunits. This paper proposes a new corpus of legal documents annotated (with the\nhelp of legal experts) with a set of 13 semantically coherent units labels\n(referred to as Rhetorical Roles), e.g., facts, arguments, statute, issue,\nprecedent, ruling, and ratio. We perform a thorough analysis of the corpus and\nthe annotations. For automatically segmenting the legal documents, we\nexperiment with the task of rhetorical role prediction: given a document,\npredict the text segments corresponding to various roles. Using the created\ncorpus, we experiment extensively with various deep learning-based baseline\nmodels for the task. Further, we develop a multitask learning (MTL) based deep\nmodel with document rhetorical role label shift as an auxiliary task for\nsegmenting a legal document. The proposed model shows superior performance over\nthe existing models. We also experiment with model performance in the case of\ndomain transfer and model distillation techniques to see the model performance\nin limited data conditions.", "published": "2021-12-03 10:49:19", "link": "http://arxiv.org/abs/2112.01836v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Shapes of Emotions: Multimodal Emotion Recognition in Conversations via\n  Emotion Shifts", "abstract": "Emotion Recognition in Conversations (ERC) is an important and active\nresearch area. Recent work has shown the benefits of using multiple modalities\n(e.g., text, audio, and video) for the ERC task. In a conversation,\nparticipants tend to maintain a particular emotional state unless some stimuli\nevokes a change. There is a continuous ebb and flow of emotions in a\nconversation. Inspired by this observation, we propose a multimodal ERC model\nand augment it with an emotion-shift component that improves performance. The\nproposed emotion-shift component is modular and can be added to any existing\nmultimodal ERC model (with a few modifications). We experiment with different\nvariants of the model, and results show that the inclusion of emotion shift\nsignal helps the model to outperform existing models for ERC on MOSEI and\nIEMOCAP datasets.", "published": "2021-12-03 14:39:04", "link": "http://arxiv.org/abs/2112.01938v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Survey on English Entity Linking on Wikidata", "abstract": "Wikidata is a frequently updated, community-driven, and multilingual\nknowledge graph. Hence, Wikidata is an attractive basis for Entity Linking,\nwhich is evident by the recent increase in published papers. This survey\nfocuses on four subjects: (1) Which Wikidata Entity Linking datasets exist, how\nwidely used are they and how are they constructed? (2) Do the characteristics\nof Wikidata matter for the design of Entity Linking datasets and if so, how?\n(3) How do current Entity Linking approaches exploit the specific\ncharacteristics of Wikidata? (4) Which Wikidata characteristics are unexploited\nby existing Entity Linking approaches? This survey reveals that current\nWikidata-specific Entity Linking datasets do not differ in their annotation\nscheme from schemes for other knowledge graphs like DBpedia. Thus, the\npotential for multilingual and time-dependent datasets, naturally suited for\nWikidata, is not lifted. Furthermore, we show that most Entity Linking\napproaches use Wikidata in the same way as any other knowledge graph missing\nthe chance to leverage Wikidata-specific characteristics to increase quality.\nAlmost all approaches employ specific properties like labels and sometimes\ndescriptions but ignore characteristics such as the hyper-relational structure.\nHence, there is still room for improvement, for example, by including\nhyper-relational graph embeddings or type information. Many approaches also\ninclude information from Wikipedia, which is easily combinable with Wikidata\nand provides valuable textual information, which Wikidata lacks.", "published": "2021-12-03 16:02:42", "link": "http://arxiv.org/abs/2112.01989v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Controversy Detection: a Text and Graph Neural Network Based Approach", "abstract": "Controversial content refers to any content that attracts both positive and\nnegative feedback. Its automatic identification, especially on social media, is\na challenging task as it should be done on a large number of continuously\nevolving posts, covering a large variety of topics. Most of the existing\napproaches rely on the graph structure of a topic-discussion and/or the content\nof messages. This paper proposes a controversy detection approach based on both\ngraph structure of a discussion and text features. Our proposed approach relies\non Graph Neural Network (gnn) to encode the graph representation (including its\ntexts) in an embedding vector before performing a graph classification task.\nThe latter will classify the post as controversial or not. Two controversy\ndetection strategies are proposed. The first one is based on a hierarchical\ngraph representation learning. Graph user nodes are embedded hierarchically and\niteratively to compute the whole graph embedding vector. The second one is\nbased on the attention mechanism, which allows each user node to give more or\nless importance to its neighbors when computing node embeddings. We conduct\nexperiments to evaluate our approach using different real-world datasets.\nConducted experiments show the positive impact of combining textual features\nand structural information in terms of performance.", "published": "2021-12-03 09:06:46", "link": "http://arxiv.org/abs/2112.11445v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "cs.SI"], "primary_category": "cs.CL"}
{"title": "LMR-CBT: Learning Modality-fused Representations with CB-Transformer for\n  Multimodal Emotion Recognition from Unaligned Multimodal Sequences", "abstract": "Learning modality-fused representations and processing unaligned multimodal\nsequences are meaningful and challenging in multimodal emotion recognition.\nExisting approaches use directional pairwise attention or a message hub to fuse\nlanguage, visual, and audio modalities. However, those approaches introduce\ninformation redundancy when fusing features and are inefficient without\nconsidering the complementarity of modalities. In this paper, we propose an\nefficient neural network to learn modality-fused representations with\nCB-Transformer (LMR-CBT) for multimodal emotion recognition from unaligned\nmultimodal sequences. Specifically, we first perform feature extraction for the\nthree modalities respectively to obtain the local structure of the sequences.\nThen, we design a novel transformer with cross-modal blocks (CB-Transformer)\nthat enables complementary learning of different modalities, mainly divided\ninto local temporal learning,cross-modal feature fusion and global\nself-attention representations. In addition, we splice the fused features with\nthe original features to classify the emotions of the sequences. Finally, we\nconduct word-aligned and unaligned experiments on three challenging datasets,\nIEMOCAP, CMU-MOSI, and CMU-MOSEI. The experimental results show the superiority\nand efficiency of our proposed method in both settings. Compared with the\nmainstream methods, our approach reaches the state-of-the-art with a minimum\nnumber of parameters.", "published": "2021-12-03 03:43:18", "link": "http://arxiv.org/abs/2112.01697v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning\n  Research", "abstract": "Benchmark datasets play a central role in the organization of machine\nlearning research. They coordinate researchers around shared research problems\nand serve as a measure of progress towards shared goals. Despite the\nfoundational role of benchmarking practices in this field, relatively little\nattention has been paid to the dynamics of benchmark dataset use and reuse,\nwithin or across machine learning subcommunities. In this paper, we dig into\nthese dynamics. We study how dataset usage patterns differ across machine\nlearning subcommunities and across time from 2015-2020. We find increasing\nconcentration on fewer and fewer datasets within task communities, significant\nadoption of datasets from other tasks, and concentration across the field on\ndatasets that have been introduced by researchers situated within a small\nnumber of elite institutions. Our results have implications for scientific\nevaluation, AI ethics, and equity/access within the field.", "published": "2021-12-03 05:01:47", "link": "http://arxiv.org/abs/2112.01716v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.CY", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Music-to-Dance Generation with Optimal Transport", "abstract": "Dance choreography for a piece of music is a challenging task, having to be\ncreative in presenting distinctive stylistic dance elements while taking into\naccount the musical theme and rhythm. It has been tackled by different\napproaches such as similarity retrieval, sequence-to-sequence modeling and\ngenerative adversarial networks, but their generated dance sequences are often\nshort of motion realism, diversity and music consistency. In this paper, we\npropose a Music-to-Dance with Optimal Transport Network (MDOT-Net) for learning\nto generate 3D dance choreographies from music. We introduce an optimal\ntransport distance for evaluating the authenticity of the generated dance\ndistribution and a Gromov-Wasserstein distance to measure the correspondence\nbetween the dance distribution and the input music. This gives a well defined\nand non-divergent training objective that mitigates the limitation of standard\nGAN training which is frequently plagued with instability and divergent\ngenerator loss issues. Extensive experiments demonstrate that our MDOT-Net can\nsynthesize realistic and diverse dances which achieve an organic unity with the\ninput music, reflecting the shared intentionality and matching the rhythmic\narticulation. Sample results are found at\nhttps://www.youtube.com/watch?v=dErfBkrlUO8.", "published": "2021-12-03 09:37:26", "link": "http://arxiv.org/abs/2112.01806v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
