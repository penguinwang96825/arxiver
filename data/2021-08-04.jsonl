{"title": "Controlled Text Generation as Continuous Optimization with Multiple\n  Constraints", "abstract": "As large-scale language model pretraining pushes the state-of-the-art in text\ngeneration, recent work has turned to controlling attributes of the text such\nmodels generate. While modifying the pretrained models via fine-tuning remains\nthe popular approach, it incurs a significant computational cost and can be\ninfeasible due to lack of appropriate data. As an alternative, we propose\nMuCoCO -- a flexible and modular algorithm for controllable inference from\npretrained models. We formulate the decoding process as an optimization problem\nwhich allows for multiple attributes we aim to control to be easily\nincorporated as differentiable constraints to the optimization. By relaxing\nthis discrete optimization to a continuous one, we make use of Lagrangian\nmultipliers and gradient-descent based techniques to generate the desired text.\nWe evaluate our approach on controllable machine translation and style transfer\nwith multiple sentence-level attributes and observe significant improvements\nover baselines.", "published": "2021-08-04 05:25:20", "link": "http://arxiv.org/abs/2108.01850v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quality Evaluation of the Low-Resource Synthetically Generated\n  Code-Mixed Hinglish Text", "abstract": "In this shared task, we seek the participating teams to investigate the\nfactors influencing the quality of the code-mixed text generation systems. We\nsynthetically generate code-mixed Hinglish sentences using two distinct\napproaches and employ human annotators to rate the generation quality. We\npropose two subtasks, quality rating prediction and annotators' disagreement\nprediction of the synthetic Hinglish dataset. The proposed subtasks will put\nforward the reasoning and explanation of the factors influencing the quality\nand human perception of the code-mixed text.", "published": "2021-08-04 06:02:46", "link": "http://arxiv.org/abs/2108.01861v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Summary Explorer: Visualizing the State of the Art in Text Summarization", "abstract": "This paper introduces Summary Explorer, a new tool to support the manual\ninspection of text summarization systems by compiling the outputs of\n55~state-of-the-art single document summarization approaches on three benchmark\ndatasets, and visually exploring them during a qualitative assessment. The\nunderlying design of the tool considers three well-known summary quality\ncriteria (coverage, faithfulness, and position bias), encapsulated in a guided\nassessment based on tailored visualizations. The tool complements existing\napproaches for locally debugging summarization models and improves upon them.\nThe tool is available at https://tldr.webis.de/", "published": "2021-08-04 07:11:19", "link": "http://arxiv.org/abs/2108.01879v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TextCNN with Attention for Text Classification", "abstract": "The vast majority of textual content is unstructured, making automated\nclassification an important task for many applications. The goal of text\nclassification is to automatically classify text documents into one or more\npredefined categories. Recently proposed simple architectures for text\nclassification such as Convolutional Neural Networks for Sentence\nClassification by Kim, Yoon showed promising results. In this paper, we propose\nincorporating an attention mechanism into the network to boost its performance,\nwe also propose WordRank for vocabulary selection to reduce the network\nembedding parameters and speed up training with minimum accuracy loss. By\nadopting the proposed ideas TextCNN accuracy on 20News increased from 94.79 to\n96.88, moreover, the number of parameters for the embedding layer can be\nreduced substantially with little accuracy loss by using WordRank. By using\nWordRank for vocabulary selection we can reduce the number of parameters by\nmore than 5x from 7.9M to 1.5M, and the accuracy will only decrease by 1.2%.", "published": "2021-08-04 09:15:30", "link": "http://arxiv.org/abs/2108.01921v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt\n  Verbalizer for Text Classification", "abstract": "Tuning pre-trained language models (PLMs) with task-specific prompts has been\na promising approach for text classification. Particularly, previous studies\nsuggest that prompt-tuning has remarkable superiority in the low-data scenario\nover the generic fine-tuning methods with extra classifiers. The core idea of\nprompt-tuning is to insert text pieces, i.e., template, to the input and\ntransform a classification problem into a masked language modeling problem,\nwhere a crucial step is to construct a projection, i.e., verbalizer, between a\nlabel space and a label word space. A verbalizer is usually handcrafted or\nsearched by gradient descent, which may lack coverage and bring considerable\nbias and high variances to the results. In this work, we focus on incorporating\nexternal knowledge into the verbalizer, forming a knowledgeable prompt-tuning\n(KPT), to improve and stabilize prompt-tuning. Specifically, we expand the\nlabel word space of the verbalizer using external knowledge bases (KBs) and\nrefine the expanded label word space with the PLM itself before predicting with\nthe expanded label word space. Extensive experiments on zero and few-shot text\nclassification tasks demonstrate the effectiveness of knowledgeable\nprompt-tuning.", "published": "2021-08-04 13:00:16", "link": "http://arxiv.org/abs/2108.02035v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Round Parsing-based Multiword Rules for Scientific OpenIE", "abstract": "Information extraction (IE) in scientific literature has facilitated many\ndown-stream tasks. OpenIE, which does not require any relation schema but\nidentifies a relational phrase to describe the relationship between a subject\nand an object, is being a trending topic of IE in sciences. The subjects,\nobjects, and relations are often multiword expressions, which brings challenges\nfor methods to identify the boundaries of the expressions given very limited or\neven no training data. In this work, we present a set of rules for extracting\nstructured information based on dependency parsing that can be applied to any\nscientific dataset requiring no expert's annotation. Results on novel datasets\nshow the effectiveness of the proposed method. We discuss negative results as\nwell.", "published": "2021-08-04 14:17:48", "link": "http://arxiv.org/abs/2108.02074v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Biologically Plausible Parser", "abstract": "We describe a parser of English effectuated by biologically plausible neurons\nand synapses, and implemented through the Assembly Calculus, a recently\nproposed computational framework for cognitive function. We demonstrate that\nthis device is capable of correctly parsing reasonably nontrivial sentences.\nWhile our experiments entail rather simple sentences in English, our results\nsuggest that the parser can be extended beyond what we have implemented, to\nseveral directions encompassing much of language. For example, we present a\nsimple Russian version of the parser, and discuss how to handle recursion,\nembedding, and polysemy.", "published": "2021-08-04 17:27:06", "link": "http://arxiv.org/abs/2108.02189v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of UMLS Concept Extraction from Clinical Notes using\n  Boolean Combination Ensembles", "abstract": "Our objective in this study is to investigate the behavior of Boolean\noperators on combining annotation output from multiple Natural Language\nProcessing (NLP) systems across multiple corpora and to assess how filtering by\naggregation of Unified Medical Language System (UMLS) Metathesaurus concepts\naffects system performance for Named Entity Recognition (NER) of UMLS concepts.\nWe used three corpora annotated for UMLS concepts: 2010 i2b2 VA challenge set\n(31,161 annotations), Multi-source Integrated Platform for Answering Clinical\nQuestions (MiPACQ) corpus (17,457 annotations including UMLS concept unique\nidentifiers), and Fairview Health Services corpus (44,530 annotations). Our\nresults showed that for UMLS concept matching, Boolean ensembling of the MiPACQ\ncorpus trended towards higher performance over individual systems. Use of an\napproximate grid-search can help optimize the precision-recall tradeoff and can\nprovide a set of heuristics for choosing an optimal set of ensembles.", "published": "2021-08-04 19:28:03", "link": "http://arxiv.org/abs/2108.02255v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Detection of COVID-19 Vaccine Misinformation with Graph Link\n  Prediction", "abstract": "Enormous hope in the efficacy of vaccines became recently a successful\nreality in the fight against the COVID-19 pandemic. However, vaccine hesitancy,\nfueled by exposure to social media misinformation about COVID-19 vaccines\nbecame a major hurdle. Therefore, it is essential to automatically detect where\nmisinformation about COVID-19 vaccines on social media is spread and what kind\nof misinformation is discussed, such that inoculation interventions can be\ndelivered at the right time and in the right place, in addition to\ninterventions designed to address vaccine hesitancy. This paper is addressing\nthe first step in tackling hesitancy against COVID-19 vaccines, namely the\nautomatic detection of known misinformation about the vaccines on Twitter, the\nsocial media platform that has the highest volume of conversations about\nCOVID-19 and its vaccines. We present CoVaxLies, a new dataset of tweets judged\nrelevant to several misinformation targets about COVID-19 vaccines on which a\nnovel method of detecting misinformation was developed. Our method organizes\nCoVaxLies in a Misinformation Knowledge Graph as it casts misinformation\ndetection as a graph link prediction problem. The misinformation detection\nmethod detailed in this paper takes advantage of the link scoring functions\nprovided by several knowledge embedding methods. The experimental results\ndemonstrate the superiority of this method when compared with\nclassification-based methods, widely used currently.", "published": "2021-08-04 23:27:10", "link": "http://arxiv.org/abs/2108.02314v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PARADISE: Exploiting Parallel Data for Multilingual Sequence-to-Sequence\n  Pretraining", "abstract": "Despite the success of multilingual sequence-to-sequence pretraining, most\nexisting approaches rely on monolingual corpora, and do not make use of the\nstrong cross-lingual signal contained in parallel data. In this paper, we\npresent PARADISE (PARAllel & Denoising Integration in SEquence-to-sequence\nmodels), which extends the conventional denoising objective used to train these\nmodels by (i) replacing words in the noised sequence according to a\nmultilingual dictionary, and (ii) predicting the reference translation\naccording to a parallel corpus instead of recovering the original sequence. Our\nexperiments on machine translation and cross-lingual natural language inference\nshow an average improvement of 2.0 BLEU points and 6.7 accuracy points from\nintegrating parallel data into pretraining, respectively, obtaining results\nthat are competitive with several popular models at a fraction of their\ncomputational cost.", "published": "2021-08-04 07:32:56", "link": "http://arxiv.org/abs/2108.01887v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Optimizing Latency for Online Video CaptioningUsing Audio-Visual\n  Transformers", "abstract": "Video captioning is an essential technology to understand scenes and describe\nevents in natural language. To apply it to real-time monitoring, a system needs\nnot only to describe events accurately but also to produce the captions as soon\nas possible. Low-latency captioning is needed to realize such functionality,\nbut this research area for online video captioning has not been pursued yet.\nThis paper proposes a novel approach to optimize each caption's output timing\nbased on a trade-off between latency and caption quality. An audio-visual\nTrans-former is trained to generate ground-truth captions using only a small\nportion of all video frames, and to mimic outputs of a pre-trained Transformer\nto which all the frames are given. A CNN-based timing detector is also trained\nto detect a proper output timing, where the captions generated by the two\nTrans-formers become sufficiently close to each other. With the jointly trained\nTransformer and timing detector, a caption can be generated in the early stages\nof an event-triggered video clip, as soon as an event happens or when it can be\nforecasted. Experiments with the ActivityNet Captions dataset show that our\napproach achieves 94% of the caption quality of the upper bound given by the\npre-trained Transformer using the entire video clips, using only 28% of frames\nfrom the beginning.", "published": "2021-08-04 16:20:00", "link": "http://arxiv.org/abs/2108.02147v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Acyclic and Cyclic Reversing Computations in Petri Nets", "abstract": "Reversible computations constitute an unconventional form of computing where\nany sequence of performed operations can be undone by executing in reverse\norder at any point during a computation. It has been attracting increasing\nattention as it provides opportunities for low-power computation, being at the\nsame time essential or eligible in various applications. In recent work, we\nhave proposed a structural way of translating Reversing Petri Nets (RPNs) - a\ntype of Petri nets that embeds reversible computation, to bounded Coloured\nPetri Nets (CPNs) - an extension of traditional Petri Nets, where tokens carry\ndata values. Three reversing semantics are possible in RPNs: backtracking\n(reversing of the lately executed action), causal reversing (action can be\nreversed only when all its effects have been undone) and out of causal\nreversing (any previously performed action can be reversed). In this paper, we\nextend the RPN to CPN translation with formal proofs of correctness. Moreover,\nthe possibility of introduction of cycles to RPNs is discussed. We analyze\nwhich type of cycles could be allowed in RPNs to ensure consistency with the\ncurrent semantics. It emerged that the most interesting case related to cycles\nin RPNs occurs in causal semantics, where various interpretations of dependency\nresult in different net's behaviour during reversing. Three definitions of\ndependence are presented and discussed.", "published": "2021-08-04 16:50:14", "link": "http://arxiv.org/abs/2108.02167v2", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Curriculum learning for language modeling", "abstract": "Language Models like ELMo and BERT have provided robust representations of\nnatural language, which serve as the language understanding component for a\ndiverse range of downstream tasks.Curriculum learning is a method that employs\na structured training regime instead, which has been leveraged in computer\nvision and machine translation to improve model training speed and model\nperformance. While language models have proven transformational for the natural\nlanguage processing community, these models have proven expensive,\nenergy-intensive, and challenging to train. In this work, we explore the effect\nof curriculum learning on language model pretraining using various\nlinguistically motivated curricula and evaluate transfer performance on the\nGLUE Benchmark. Despite a broad variety of training methodologies and\nexperiments we do not find compelling evidence that curriculum learning methods\nimprove language model training.", "published": "2021-08-04 16:53:43", "link": "http://arxiv.org/abs/2108.02170v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ordered Attention for Coherent Visual Storytelling", "abstract": "We address the problem of visual storytelling, i.e., generating a story for a\ngiven sequence of images. While each sentence of the story should describe a\ncorresponding image, a coherent story also needs to be consistent and relate to\nboth future and past images. To achieve this we develop ordered image attention\n(OIA). OIA models interactions between the sentence-corresponding image and\nimportant regions in other images of the sequence. To highlight the important\nobjects, a message-passing-like algorithm collects representations of those\nobjects in an order-aware manner. To generate the story's sentences, we then\nhighlight important image attention vectors with an Image-Sentence Attention\n(ISA). Further, to alleviate common linguistic mistakes like repetitiveness, we\nintroduce an adaptive prior. The obtained results improve the METEOR score on\nthe VIST dataset by 1%. In addition, an extensive human study verifies\ncoherency improvements and shows that OIA and ISA generated stories are more\nfocused, shareable, and image-grounded.", "published": "2021-08-04 17:12:39", "link": "http://arxiv.org/abs/2108.02180v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Mitigating harm in language models with conditional-likelihood\n  filtration", "abstract": "Language models trained on large-scale unfiltered datasets curated from the\nopen web acquire systemic biases, prejudices, and harmful views from their\ntraining data. We present a methodology for programmatically identifying and\nremoving harmful text from web-scale datasets. A pretrained language model is\nused to calculate the log-likelihood of researcher-written trigger phrases\nconditioned on a specific document, which is used to identify and filter\ndocuments from the dataset. We demonstrate that models trained on this filtered\ndataset exhibit lower propensity to generate harmful text, with a marginal\ndecrease in performance on standard language modeling benchmarks compared to\nunfiltered baselines. We provide a partial explanation for this performance gap\nby surfacing examples of hate speech and other undesirable content from\nstandard language modeling benchmarks. Finally, we discuss the generalization\nof this method and how trigger phrases which reflect specific values can be\nused by researchers to build language models which are more closely aligned\nwith their values.", "published": "2021-08-04 22:18:10", "link": "http://arxiv.org/abs/2108.07790v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Distinction between ASR Errors and Speech Disfluencies with\n  Feature Space Interpolation", "abstract": "Fine-tuning pretrained language models (LMs) is a popular approach to\nautomatic speech recognition (ASR) error detection during post-processing.\nWhile error detection systems often take advantage of statistical language\narchetypes captured by LMs, at times the pretrained knowledge can hinder error\ndetection performance. For instance, presence of speech disfluencies might\nconfuse the post-processing system into tagging disfluent but accurate\ntranscriptions as ASR errors. Such confusion occurs because both error\ndetection and disfluency detection tasks attempt to identify tokens at\nstatistically unlikely positions. This paper proposes a scheme to improve\nexisting LM-based ASR error detection systems, both in terms of detection\nscores and resilience to such distracting auxiliary tasks. Our approach adopts\nthe popular mixup method in text feature space and can be utilized with any\nblack-box ASR output. To demonstrate the effectiveness of our method, we\nconduct post-processing experiments with both traditional and end-to-end ASR\nsystems (both for English and Korean languages) with 5 different speech\ncorpora. We find that our method improves both ASR error detection F 1 scores\nand reduces the number of correctly transcribed disfluencies wrongly detected\nas ASR errors. Finally, we suggest methods to utilize resulting LMs directly in\nsemi-supervised ASR training.", "published": "2021-08-04 02:11:37", "link": "http://arxiv.org/abs/2108.01812v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Emergent Discrete Communication in Semantic Spaces", "abstract": "Neural agents trained in reinforcement learning settings can learn to\ncommunicate among themselves via discrete tokens, accomplishing as a team what\nagents would be unable to do alone. However, the current standard of using\none-hot vectors as discrete communication tokens prevents agents from acquiring\nmore desirable aspects of communication such as zero-shot understanding.\nInspired by word embedding techniques from natural language processing, we\npropose neural agent architectures that enables them to communicate via\ndiscrete tokens derived from a learned, continuous space. We show in a decision\ntheoretic framework that our technique optimizes communication over a wide\nrange of scenarios, whereas one-hot tokens are only optimal under restrictive\nassumptions. In self-play experiments, we validate that our trained agents\nlearn to cluster tokens in semantically-meaningful ways, allowing them\ncommunicate in noisy environments where other techniques fail. Lastly, we\ndemonstrate both that agents using our method can effectively respond to novel\nhuman communication and that humans can understand unlabeled emergent agent\ncommunication, outperforming the use of one-hot communication.", "published": "2021-08-04 03:32:48", "link": "http://arxiv.org/abs/2108.01828v3", "categories": ["cs.LG", "cs.CL", "cs.MA", "cs.RO"], "primary_category": "cs.LG"}
{"title": "How to Query Language Models?", "abstract": "Large pre-trained language models (LMs) are capable of not only recovering\nlinguistic but also factual and commonsense knowledge. To access the knowledge\nstored in mask-based LMs, we can use cloze-style questions and let the model\nfill in the blank. The flexibility advantage over structured knowledge bases\ncomes with the drawback of finding the right query for a certain information\nneed. Inspired by human behavior to disambiguate a question, we propose to\nquery LMs by example. To clarify the ambivalent question \"Who does Neuer play\nfor?\", a successful strategy is to demonstrate the relation using another\nsubject, e.g., \"Ronaldo plays for Portugal. Who does Neuer play for?\". We apply\nthis approach of querying by example to the LAMA probe and obtain substantial\nimprovements of up to 37.8% for BERT-large on the T-REx data when providing\nonly 10 demonstrations--even outperforming a baseline that queries the model\nwith up to 40 paraphrases of the question. The examples are provided through\nthe model's context and thus require neither fine-tuning nor an additional\nforward pass. This suggests that LMs contain more factual and commonsense\nknowledge than previously assumed--if we query the model in the right way.", "published": "2021-08-04 09:38:16", "link": "http://arxiv.org/abs/2108.01928v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dyn-ASR: Compact, Multilingual Speech Recognition via Spoken Language\n  and Accent Identification", "abstract": "Running automatic speech recognition (ASR) on edge devices is non-trivial due\nto resource constraints, especially in scenarios that require supporting\nmultiple languages. We propose a new approach to enable multilingual speech\nrecognition on edge devices. This approach uses both language identification\nand accent identification to select one of multiple monolingual ASR models\non-the-fly, each fine-tuned for a particular accent. Initial results for both\nrecognition performance and resource usage are promising with our approach\nusing less than 1/12th of the memory consumed by other solutions.", "published": "2021-08-04 12:59:53", "link": "http://arxiv.org/abs/2108.02034v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Fake News and Phishing Detection Using a Machine Learning Trained Expert\n  System", "abstract": "Expert systems have been used to enable computers to make recommendations and\ndecisions. This paper presents the use of a machine learning trained expert\nsystem (MLES) for phishing site detection and fake news detection. Both topics\nshare a similar goal: to design a rule-fact network that allows a computer to\nmake explainable decisions like domain experts in each respective area. The\nphishing website detection study uses a MLES to detect potential phishing\nwebsites by analyzing site properties (like URL length and expiration time).\nThe fake news detection study uses a MLES rule-fact network to gauge news story\ntruthfulness based on factors such as emotion, the speaker's political\naffiliation status, and job. The two studies use different MLES network\nimplementations, which are presented and compared herein. The fake news study\nutilized a more linear design while the phishing project utilized a more\ncomplex connection structure. Both networks' inputs are based on commonly\navailable data sets.", "published": "2021-08-04 15:25:32", "link": "http://arxiv.org/abs/2108.08264v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "With One Voice: Composing a Travel Voice Assistant from Re-purposed\n  Models", "abstract": "Voice assistants provide users a new way of interacting with digital\nproducts, allowing them to retrieve information and complete tasks with an\nincreased sense of control and flexibility. Such products are comprised of\nseveral machine learning models, like Speech-to-Text transcription, Named\nEntity Recognition and Resolution, and Text Classification. Building a voice\nassistant from scratch takes the prolonged efforts of several teams\nconstructing numerous models and orchestrating between components. Alternatives\nsuch as using third-party vendors or re-purposing existing models may be\nconsidered to shorten time-to-market and development costs. However, each\noption has its benefits and drawbacks. We present key insights from building a\nvoice search assistant for Booking.com search and recommendation system. Our\npaper compares the achieved performance and development efforts in dedicated\ntailor-made solutions against existing re-purposed models. We share and discuss\nour data-driven decisions about implementation trade-offs and their estimated\noutcomes in hindsight, showing that a fully functional machine learning product\ncan be built from existing models.", "published": "2021-08-04 10:34:11", "link": "http://arxiv.org/abs/2108.11463v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Information Sieve: Content Leakage Reduction in End-to-End Prosody For\n  Expressive Speech Synthesis", "abstract": "Expressive neural text-to-speech (TTS) systems incorporate a style encoder to\nlearn a latent embedding as the style information. However, this embedding\nprocess may encode redundant textual information. This phenomenon is called\ncontent leakage. Researchers have attempted to resolve this problem by adding\nan ASR or other auxiliary supervision loss functions. In this study, we propose\nan unsupervised method called the \"information sieve\" to reduce the effect of\ncontent leakage in prosody transfer. The rationale of this approach is that the\nstyle encoder can be forced to focus on style information rather than on\ntextual information contained in the reference speech by a well-designed\ndownsample-upsample filter, i.e., the extracted style embeddings can be\ndownsampled at a certain interval and then upsampled by duplication.\nFurthermore, we used instance normalization in convolution layers to help the\nsystem learn a better latent style space. Objective metrics such as the\nsignificantly lower word error rate (WER) demonstrate the effectiveness of this\nmodel in mitigating content leakage. Listening tests indicate that the model\nretains its prosody transferability compared with the baseline models such as\nthe original GST-Tacotron and ASR-guided Tacotron.", "published": "2021-08-04 03:45:16", "link": "http://arxiv.org/abs/2108.01831v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Daft-Exprt: Cross-Speaker Prosody Transfer on Any Text for Expressive\n  Speech Synthesis", "abstract": "This paper presents Daft-Exprt, a multi-speaker acoustic model advancing the\nstate-of-the-art for cross-speaker prosody transfer on any text. This is one of\nthe most challenging, and rarely directly addressed, task in speech synthesis,\nespecially for highly expressive data. Daft-Exprt uses FiLM conditioning layers\nto strategically inject different prosodic information in all parts of the\narchitecture. The model explicitly encodes traditional low-level prosody\nfeatures such as pitch, loudness and duration, but also higher level prosodic\ninformation that helps generating convincing voices in highly expressive\nstyles. Speaker identity and prosodic information are disentangled through an\nadversarial training strategy that enables accurate prosody transfer across\nspeakers. Experimental results show that Daft-Exprt significantly outperforms\nstrong baselines on inter-text cross-speaker prosody transfer tasks, while\nyielding naturalness comparable to state-of-the-art expressive models.\nMoreover, results indicate that the model discards speaker identity information\nfrom the prosody representation, and consistently generate speech with the\ndesired voice. We publicly release our code and provide speech samples from our\nexperiments.", "published": "2021-08-04 20:13:00", "link": "http://arxiv.org/abs/2108.02271v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Blind and neural network-guided convolutional beamformer for joint\n  denoising, dereverberation, and source separation", "abstract": "This paper proposes an approach for optimizing a Convolutional BeamFormer\n(CBF) that can jointly perform denoising (DN), dereverberation (DR), and source\nseparation (SS). First, we develop a blind CBF optimization algorithm that\nrequires no prior information on the sources or the room acoustics, by\nextending a conventional joint DR and SS method. For making the optimization\ncomputationally tractable, we incorporate two techniques into the approach: the\nSource-Wise Factorization (SW-Fact) of a CBF and the Independent Vector\nExtraction (IVE). To further improve the performance, we develop a method that\nintegrates a neural network(NN) based source power spectra estimation with CBF\noptimization by an inverse-Gamma prior. Experiments using noisy reverberant\nmixtures reveal that our proposed method with both blind and NN-guided\nscenarios greatly outperforms the conventional state-of-the-art NN-supported\nmask-based CBF in terms of the improvement in automatic speech recognition and\nsignal distortion reduction performance.", "published": "2021-08-04 04:03:45", "link": "http://arxiv.org/abs/2108.01836v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Lung Sound Classification Using Co-tuning and Stochastic Normalization", "abstract": "In this paper, we use pre-trained ResNet models as backbone architectures for\nclassification of adventitious lung sounds and respiratory diseases. The\nknowledge of the pre-trained model is transferred by using vanilla fine-tuning,\nco-tuning, stochastic normalization and the combination of the co-tuning and\nstochastic normalization techniques. Furthermore, data augmentation in both\ntime domain and time-frequency domain is used to account for the class\nimbalance of the ICBHI and our multi-channel lung sound dataset. Additionally,\nwe apply spectrum correction to consider the variations of the recording device\nproperties on the ICBHI dataset. Empirically, our proposed systems mostly\noutperform all state-of-the-art lung sound classification systems for the\nadventitious lung sounds and respiratory diseases of both datasets.", "published": "2021-08-04 12:16:02", "link": "http://arxiv.org/abs/2108.01991v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Domain Adaptation in Speech Recognition using Phonetic\n  Features", "abstract": "Automatic speech recognition is a difficult problem in pattern recognition\nbecause several sources of variability exist in the speech input like the\nchannel variations, the input might be clean or noisy, the speakers may have\ndifferent accent and variations in the gender, etc. As a result, domain\nadaptation is important in speech recognition where we train the model for a\nparticular source domain and test it on a different target domain. In this\npaper, we propose a technique to perform unsupervised gender-based domain\nadaptation in speech recognition using phonetic features. The experiments are\nperformed on the TIMIT dataset and there is a considerable decrease in the\nphoneme error rate using the proposed approach.", "published": "2021-08-04 06:22:12", "link": "http://arxiv.org/abs/2108.02850v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Pervasive Hand Gesture Recognition for Smartphones using Non-audible\n  Sound and Deep Learning", "abstract": "Due to the mass advancement in ubiquitous technologies nowadays, new\npervasive methods have come into the practice to provide new innovative\nfeatures and stimulate the research on new human-computer interactions. This\npaper presents a hand gesture recognition method that utilizes the smartphone's\nbuilt-in speakers and microphones. The proposed system emits an ultrasonic\nsonar-based signal (inaudible sound) from the smartphone's stereo speakers,\nwhich is then received by the smartphone's microphone and processed via a\nConvolutional Neural Network (CNN) for Hand Gesture Recognition. Data\naugmentation techniques are proposed to improve the detection accuracy and\nthree dual-channel input fusion methods are compared. The first method merges\nthe dual-channel audio as a single input spectrogram image. The second method\nadopts early fusion by concatenating the dual-channel spectrograms. The third\nmethod adopts late fusion by having two convectional input branches processing\neach of the dual-channel spectrograms and then the outputs are merged by the\nlast layers. Our experimental results demonstrate a promising detection\naccuracy for the six gestures presented in our publicly available dataset with\nan accuracy of 93.58\\% as a baseline.", "published": "2021-08-04 16:23:26", "link": "http://arxiv.org/abs/2108.02148v1", "categories": ["cs.SD", "cs.CV", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
