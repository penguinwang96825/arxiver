{"title": "Innovative Sentiment Analysis and Prediction of Stock Price Using FinBERT, GPT-4 and Logistic Regression: A Data-Driven Approach", "abstract": "This study explores the comparative performance of cutting-edge AI models,\ni.e., Finaance Bidirectional Encoder representations from Transsformers\n(FinBERT), Generatice Pre-trained Transformer GPT-4, and Logistic Regression,\nfor sentiment analysis and stock index prediction using financial news and the\nNGX All-Share Index data label. By leveraging advanced natural language\nprocessing models like GPT-4 and FinBERT, alongside a traditional machine\nlearning model, Logistic Regression, we aim to classify market sentiment,\ngenerate sentiment scores, and predict market price movements. This research\nhighlights global AI advancements in stock markets, showcasing how\nstate-of-the-art language models can contribute to understanding complex\nfinancial data. The models were assessed using metrics such as accuracy,\nprecision, recall, F1 score, and ROC AUC. Results indicate that Logistic\nRegression outperformed the more computationally intensive FinBERT and\npredefined approach of versatile GPT-4, with an accuracy of 81.83% and a ROC\nAUC of 89.76%. The GPT-4 predefined approach exhibited a lower accuracy of\n54.19% but demonstrated strong potential in handling complex data. FinBERT,\nwhile offering more sophisticated analysis, was resource-demanding and yielded\na moderate performance. Hyperparameter optimization using Optuna and\ncross-validation techniques ensured the robustness of the models. This study\nhighlights the strengths and limitations of the practical applications of AI\napproaches in stock market prediction and presents Logistic Regression as the\nmost efficient model for this task, with FinBERT and GPT-4 representing\nemerging tools with potential for future exploration and innovation in\nAI-driven financial analytics", "published": "2024-12-07 05:20:31", "link": "http://arxiv.org/abs/2412.06837v1", "categories": ["cs.LG", "cs.AI", "q-fin.ST", "stat.AP", "stat.CO", "H.3.3"], "primary_category": "cs.LG"}
{"title": "A polar coordinate system represents syntax in large language models", "abstract": "Originally formalized with symbolic representations, syntactic trees may also\nbe effectively represented in the activations of large language models (LLMs).\nIndeed, a 'Structural Probe' can find a subspace of neural activations, where\nsyntactically related words are relatively close to one-another. However, this\nsyntactic code remains incomplete: the distance between the Structural Probe\nword embeddings can represent the existence but not the type and direction of\nsyntactic relations. Here, we hypothesize that syntactic relations are, in\nfact, coded by the relative direction between nearby embeddings. To test this\nhypothesis, we introduce a 'Polar Probe' trained to read syntactic relations\nfrom both the distance and the direction between word embeddings. Our approach\nreveals three main findings. First, our Polar Probe successfully recovers the\ntype and direction of syntactic relations, and substantially outperforms the\nStructural Probe by nearly two folds. Second, we confirm that this polar\ncoordinate system exists in a low-dimensional subspace of the intermediate\nlayers of many LLMs and becomes increasingly precise in the latest frontier\nmodels. Third, we demonstrate with a new benchmark that similar syntactic\nrelations are coded similarly across the nested levels of syntactic trees.\nOverall, this work shows that LLMs spontaneously learn a geometry of neural\nactivations that explicitly represents the main symbolic structures of\nlinguistic theory.", "published": "2024-12-07 07:37:20", "link": "http://arxiv.org/abs/2412.05571v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixture of Hidden-Dimensions Transformer", "abstract": "Transformer models encounter challenges in scaling hidden dimensions\nefficiently, as uniformly increasing them inflates computational and memory\ncosts while failing to emphasize the most relevant features for each token. For\nfurther understanding, we study hidden dimension sparsity and observe that\ntrained Transformers utilize only a small fraction of token dimensions,\nrevealing an \"activation flow\" pattern. Notably, there are shared\nsub-dimensions with sustained activation across multiple consecutive tokens and\nspecialized sub-dimensions uniquely activated for each token. To better model\ntoken-relevant sub-dimensions, we propose MoHD (Mixture of Hidden Dimensions),\na sparse conditional activation architecture. Particularly, MoHD employs shared\nsub-dimensions for common token features and a routing mechanism to dynamically\nactivate specialized sub-dimensions. To mitigate potential information loss\nfrom sparsity, we design activation scaling and group fusion mechanisms to\npreserve activation flow. In this way, MoHD expands hidden dimensions with\nnegligible increases in computation or parameters, efficient training and\ninference while maintaining performance. Evaluations across 10 NLP tasks show\nthat MoHD surpasses Vanilla Transformers in parameter efficiency and task\nperformance. It achieves 1.7% higher performance with 50% fewer activation\nparameters and 3.7% higher performance with a 3x parameter expansion at\nconstant activation cost. MOHD offers a new perspective for scaling the model,\nshowcasing the potential of hidden dimension sparsity to boost efficiency", "published": "2024-12-07 13:15:22", "link": "http://arxiv.org/abs/2412.05644v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shifting NER into High Gear: The Auto-AdvER Approach", "abstract": "This paper presents a case study on the development of Auto-AdvER, a\nspecialised named entity recognition schema and dataset for text in the car\nadvertisement genre. Developed with industry needs in mind, Auto-AdvER is\ndesigned to enhance text mining analytics in this domain and contributes a\nlinguistically unique NER dataset. We present a schema consisting of three\nlabels: \"Condition\", \"Historic\" and \"Sales Options\". We outline the guiding\nprinciples for annotation, describe the methodology for schema development, and\nshow the results of an annotation study demonstrating inter-annotator agreement\nof 92% F1-Score. Furthermore, we compare the performance by using encoder-only\nmodels: BERT, DeBERTaV3 and decoder-only open and closed source Large Language\nModels (LLMs): Llama, Qwen, GPT-4 and Gemini. Our results show that the class\nof LLMs outperforms the smaller encoder-only models. However, the LLMs are\ncostly and far from perfect for this task. We present this work as a stepping\nstone toward more fine-grained analysis and discuss Auto-AdvER's potential\nimpact on advertisement analytics and customer insights, including applications\nsuch as the analysis of market dynamics and data-driven predictive maintenance.\nOur schema, as well as our associated findings, are suitable for both private\nand public entities considering named entity recognition in the automotive\ndomain, or other specialist domains.", "published": "2024-12-07 14:00:06", "link": "http://arxiv.org/abs/2412.05655v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph with Sequence: Broad-Range Semantic Modeling for Fake News\n  Detection", "abstract": "The rapid proliferation of fake news on social media threatens social\nstability, creating an urgent demand for more effective detection methods.\nWhile many promising approaches have emerged, most rely on content analysis\nwith limited semantic depth, leading to suboptimal comprehension of news\ncontent.To address this limitation, capturing broader-range semantics is\nessential yet challenging, as it introduces two primary types of noise: fully\nconnecting sentences in news graphs often adds unnecessary structural noise,\nwhile highly similar but authenticity-irrelevant sentences introduce feature\nnoise, complicating the detection process. To tackle these issues, we propose\nBREAK, a broad-range semantics model for fake news detection that leverages a\nfully connected graph to capture comprehensive semantics while employing dual\ndenoising modules to minimize both structural and feature noise. The semantic\nstructure denoising module balances the graph's connectivity by iteratively\nrefining it between two bounds: a sequence-based structure as a lower bound and\na fully connected graph as the upper bound. This refinement uncovers\nlabel-relevant semantic interrelations structures. Meanwhile, the semantic\nfeature denoising module reduces noise from similar semantics by diversifying\nrepresentations, aligning distinct outputs from the denoised graph and sequence\nencoders using KL-divergence to achieve feature diversification in\nhigh-dimensional space. The two modules are jointly optimized in a bi-level\nframework, enhancing the integration of denoised semantics into a comprehensive\nrepresentation for detection. Extensive experiments across four datasets\ndemonstrate that BREAK significantly outperforms existing fake news detection\nmethods.", "published": "2024-12-07 14:35:46", "link": "http://arxiv.org/abs/2412.05672v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression", "abstract": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.", "published": "2024-12-07 16:41:54", "link": "http://arxiv.org/abs/2412.05693v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SplaXBERT: Leveraging Mixed Precision Training and Context Splitting for\n  Question Answering", "abstract": "SplaXBERT, built on ALBERT-xlarge with context-splitting and mixed precision\ntraining, achieves high efficiency in question-answering tasks on lengthy\ntexts. Tested on SQuAD v1.1, it attains an Exact Match of 85.95% and an F1\nScore of 92.97%, outperforming traditional BERT-based models in both accuracy\nand resource efficiency.", "published": "2024-12-07 02:01:27", "link": "http://arxiv.org/abs/2412.05499v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey on Uncertainty Quantification of Large Language Models:\n  Taxonomy, Open Research Challenges, and Future Directions", "abstract": "The remarkable performance of large language models (LLMs) in content\ngeneration, coding, and common-sense reasoning has spurred widespread\nintegration into many facets of society. However, integration of LLMs raises\nvalid questions on their reliability and trustworthiness, given their\npropensity to generate hallucinations: plausible, factually-incorrect\nresponses, which are expressed with striking confidence. Previous work has\nshown that hallucinations and other non-factual responses generated by LLMs can\nbe detected by examining the uncertainty of the LLM in its response to the\npertinent prompt, driving significant research efforts devoted to quantifying\nthe uncertainty of LLMs. This survey seeks to provide an extensive review of\nexisting uncertainty quantification methods for LLMs, identifying their salient\nfeatures, along with their strengths and weaknesses. We present existing\nmethods within a relevant taxonomy, unifying ostensibly disparate methods to\naid understanding of the state of the art. Furthermore, we highlight\napplications of uncertainty quantification methods for LLMs, spanning chatbot\nand textual applications to embodied artificial intelligence applications in\nrobotics. We conclude with open research challenges in uncertainty\nquantification of LLMs, seeking to motivate future research.", "published": "2024-12-07 06:56:01", "link": "http://arxiv.org/abs/2412.05563v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods", "abstract": "The rapid advancement of Large Language Models (LLMs) has driven their\nexpanding application across various fields. One of the most promising\napplications is their role as evaluators based on natural language responses,\nreferred to as ''LLMs-as-judges''. This framework has attracted growing\nattention from both academia and industry due to their excellent effectiveness,\nability to generalize across tasks, and interpretability in the form of natural\nlanguage. This paper presents a comprehensive survey of the LLMs-as-judges\nparadigm from five key perspectives: Functionality, Methodology, Applications,\nMeta-evaluation, and Limitations. We begin by providing a systematic definition\nof LLMs-as-Judges and introduce their functionality (Why use LLM judges?). Then\nwe address methodology to construct an evaluation system with LLMs (How to use\nLLM judges?). Additionally, we investigate the potential domains for their\napplication (Where to use LLM judges?) and discuss methods for evaluating them\nin various contexts (How to evaluate LLM judges?). Finally, we provide a\ndetailed analysis of the limitations of LLM judges and discuss potential future\ndirections. Through a structured and comprehensive analysis, we aim aims to\nprovide insights on the development and application of LLMs-as-judges in both\nresearch and practice. We will continue to maintain the relevant resource list\nat https://github.com/CSHaitao/Awesome-LLMs-as-Judges.", "published": "2024-12-07 08:07:24", "link": "http://arxiv.org/abs/2412.05579v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CharacterBox: Evaluating the Role-Playing Capabilities of LLMs in\n  Text-Based Virtual Worlds", "abstract": "Role-playing is a crucial capability of Large Language Models (LLMs),\nenabling a wide range of practical applications, including intelligent\nnon-player characters, digital twins, and emotional companions. Evaluating this\ncapability in LLMs is challenging due to the complex dynamics involved in\nrole-playing, such as maintaining character fidelity throughout a storyline and\nnavigating open-ended narratives without a definitive ground truth. Current\nevaluation methods, which primarily focus on question-answering or\nconversational snapshots, fall short of adequately capturing the nuanced\ncharacter traits and behaviors essential for authentic role-playing. In this\npaper, we propose CharacterBox, which is a simulation sandbox designed to\ngenerate situational fine-grained character behavior trajectories. These\nbehavior trajectories enable a more comprehensive and in-depth evaluation of\nrole-playing capabilities. CharacterBox consists of two main components: the\ncharacter agent and the narrator agent. The character agent, grounded in\npsychological and behavioral science, exhibits human-like behaviors, while the\nnarrator agent coordinates interactions between character agents and\nenvironmental changes. Additionally, we introduce two trajectory-based methods\nthat leverage CharacterBox to enhance LLM performance. To reduce costs and\nfacilitate the adoption of CharacterBox by public communities, we fine-tune two\nsmaller models, CharacterNR and CharacterRM, as substitutes for GPT API calls,\nand demonstrate their competitive performance compared to advanced GPT APIs.", "published": "2024-12-07 12:09:35", "link": "http://arxiv.org/abs/2412.05631v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Comparative Study on Code Generation with Transformers", "abstract": "In an era of widespread influence of Natural Language Processing (NLP), there\nhave been multiple research efforts to supplant traditional manual coding\ntechniques with automated systems capable of generating solutions autonomously.\nWith rapid research for code generation and a sole focus on large language\nmodels, there emerges a need to compare and evaluate the performance of\ntransformer architectures based on several complexities of the model. This\npaper introduces the concept of a \"A Comparative Study on Code Generation with\nTransformers,\" a model based on Transformer architecture, and NLP methodologies\nto automatically generate C++ source code for different varieties of problems.\nHere, a comparative study is performed to evaluate the robustness of\ntransformer-based models on the basis of their architecture complexities and\ntheir capability to handle diverse problem sets, from basic arithmetic to\ncomplex computations.", "published": "2024-12-07 21:18:23", "link": "http://arxiv.org/abs/2412.05749v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic Loss Guided Data Efficient Supervised Fine Tuning for Safe\n  Responses in LLMs", "abstract": "Large Language Models (LLMs) generating unsafe responses to toxic prompts is\na significant issue in their applications. While various efforts aim to address\nthis safety concern, previous approaches often demand substantial human data\ncollection or rely on the less dependable option of using another LLM to\ngenerate corrective data. In this paper, we aim to take this problem and\novercome limitations of requiring significant high-quality human data. Our\nmethod requires only a small set of unsafe responses to toxic prompts, easily\nobtained from the unsafe LLM itself. By employing a semantic cost combined with\na negative Earth Mover Distance (EMD) loss, we guide the LLM away from\ngenerating unsafe responses. Additionally, we propose a novel lower bound for\nEMD loss, enabling more efficient optimization. Our results demonstrate\nsuperior performance and data efficiency compared to baselines, and we further\nexamine the nuanced effects of over-alignment and potential degradation of\nlanguage capabilities when using contrastive data.", "published": "2024-12-07 16:35:14", "link": "http://arxiv.org/abs/2412.06843v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comprehensive Evaluation of Multimodal AI Models in Medical Imaging\n  Diagnosis: From Data Augmentation to Preference-Based Comparison", "abstract": "This study introduces an evaluation framework for multimodal models in\nmedical imaging diagnostics. We developed a pipeline incorporating data\npreprocessing, model inference, and preference-based evaluation, expanding an\ninitial set of 500 clinical cases to 3,000 through controlled augmentation. Our\nmethod combined medical images with clinical observations to generate\nassessments, using Claude 3.5 Sonnet for independent evaluation against\nphysician-authored diagnoses. The results indicated varying performance across\nmodels, with Llama 3.2-90B outperforming human diagnoses in 85.27% of cases. In\ncontrast, specialized vision models like BLIP2 and Llava showed preferences in\n41.36% and 46.77% of cases, respectively. This framework highlights the\npotential of large multimodal models to outperform human diagnostics in certain\ntasks.", "published": "2024-12-07 04:38:44", "link": "http://arxiv.org/abs/2412.05536v1", "categories": ["eess.IV", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "eess.IV"}
{"title": "On the Expressive Power of Modern Hopfield Networks", "abstract": "Modern Hopfield networks (MHNs) have emerged as powerful tools in deep\nlearning, capable of replacing components such as pooling layers, LSTMs, and\nattention mechanisms. Recent advancements have enhanced their storage capacity,\nretrieval speed, and error rates. However, the fundamental limits of their\ncomputational expressiveness remain unexplored. Understanding the expressive\npower of MHNs is crucial for optimizing their integration into deep learning\narchitectures. In this work, we establish rigorous theoretical bounds on the\ncomputational capabilities of MHNs using circuit complexity theory. Our key\ncontribution is that we show that MHNs are $\\mathsf{DLOGTIME}$-uniform\n$\\mathsf{TC}^0$. Hence, unless $\\mathsf{TC}^0 = \\mathsf{NC}^1$, a\n$\\mathrm{poly}(n)$-precision modern Hopfield networks with a constant number of\nlayers and $O(n)$ hidden dimension cannot solve $\\mathsf{NC}^1$-hard problems\nsuch as the undirected graph connectivity problem and the tree isomorphism\nproblem. We also extended our results to Kernelized Hopfield Networks. These\nresults demonstrate the limitation in the expressive power of the modern\nHopfield networks. Moreover, Our theoretical analysis provides insights to\nguide the development of new Hopfield-based architectures.", "published": "2024-12-07 06:52:41", "link": "http://arxiv.org/abs/2412.05562v1", "categories": ["cs.CC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CC"}
{"title": "UNet++ and LSTM combined approach for Breast Ultrasound Image\n  Segmentation", "abstract": "Breast cancer stands as a prevalent cause of fatality among females on a\nglobal scale, with prompt detection playing a pivotal role in diminishing\nmortality rates. The utilization of ultrasound scans in the BUSI dataset for\nmedical imagery pertaining to breast cancer has exhibited commendable\nsegmentation outcomes through the application of UNet and UNet++ networks.\nNevertheless, a notable drawback of these models resides in their inattention\ntowards the temporal aspects embedded within the images. This research\nendeavors to enrich the UNet++ architecture by integrating LSTM layers and\nself-attention mechanisms to exploit temporal characteristics for segmentation\npurposes. Furthermore, the incorporation of a Multiscale Feature Extraction\nModule aims to grasp varied scale features within the UNet++. Through the\namalgamation of our proposed methodology with data augmentation on the BUSI\nwith GT dataset, an accuracy rate of 98.88%, specificity of 99.53%, precision\nof 95.34%, sensitivity of 91.20%, F1-score of 93.74, and Dice coefficient of\n92.74% are achieved. These findings demonstrate competitiveness with\ncutting-edge techniques outlined in existing literature.", "published": "2024-12-07 08:39:31", "link": "http://arxiv.org/abs/2412.05585v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "BERTCaps: BERT Capsule for Persian Multi-Domain Sentiment Analysis", "abstract": "Multidomain sentiment analysis involves estimating the polarity of an\nunstructured text by exploiting domain specific information. One of the main\nissues common to the approaches discussed in the literature is their poor\napplicability to domains that differ from those used to construct opinion\nmodels.This paper aims to present a new method for Persian multidomain SA\nanalysis using deep learning approaches. The proposed BERTCapsules approach\nconsists of a combination of BERT and Capsule models. In this approach, BERT\nwas used for Instance representation, and Capsule Structure was used to learn\nthe extracted graphs. Digikala dataset, including ten domains with both\npositive and negative polarity, was used to evaluate this approach. The\nevaluation of the BERTCaps model achieved an accuracy of 0.9712 in sentiment\nclassification binary classification and 0.8509 in domain classification .", "published": "2024-12-07 09:08:25", "link": "http://arxiv.org/abs/2412.05591v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the effective transfer of knowledge from English to Hindi Wikipedia", "abstract": "Although Wikipedia is the largest multilingual encyclopedia, it remains\ninherently incomplete. There is a significant disparity in the quality of\ncontent between high-resource languages (HRLs, e.g., English) and low-resource\nlanguages (LRLs, e.g., Hindi), with many LRL articles lacking adequate\ninformation. To bridge these content gaps, we propose a lightweight framework\nto enhance knowledge equity between English and Hindi. In case the English\nWikipedia page is not up-to-date, our framework extracts relevant information\nfrom external resources readily available (such as English books) and adapts it\nto align with Wikipedia's distinctive style, including its \\textit{neutral\npoint of view} (NPOV) policy, using in-context learning capabilities of large\nlanguage models. The adapted content is then machine-translated into Hindi for\nintegration into the corresponding Wikipedia articles. On the other hand, if\nthe English version is comprehensive and up-to-date, the framework directly\ntransfers knowledge from English to Hindi. Our framework effectively generates\nnew content for Hindi Wikipedia sections, enhancing Hindi Wikipedia articles\nrespectively by 65% and 62% according to automatic and human judgment-based\nevaluations.", "published": "2024-12-07 17:43:21", "link": "http://arxiv.org/abs/2412.05708v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PromptRefine: Enhancing Few-Shot Performance on Low-Resource Indic\n  Languages with Example Selection from Related Example Banks", "abstract": "Large Language Models (LLMs) have recently demonstrated impressive few-shot\nlearning capabilities through in-context learning (ICL). However, ICL\nperformance is highly dependent on the choice of few-shot demonstrations,\nmaking the selection of the most optimal examples a persistent research\nchallenge. This issue is further amplified in low-resource Indic languages,\nwhere the scarcity of ground-truth data complicates the selection process. In\nthis work, we propose PromptRefine, a novel Alternating Minimization approach\nfor example selection that improves ICL performance on low-resource Indic\nlanguages. PromptRefine leverages auxiliary example banks from related\nhigh-resource Indic languages and employs multi-task learning techniques to\nalign language-specific retrievers, enabling effective cross-language\nretrieval. Additionally, we incorporate diversity in the selected examples to\nenhance generalization and reduce bias. Through comprehensive evaluations on\nfour text generation tasks -- Cross-Lingual Question Answering, Multilingual\nQuestion Answering, Machine Translation, and Cross-Lingual Summarization using\nstate-of-the-art LLMs such as LLAMA-3.1-8B, LLAMA-2-7B, Qwen-2-7B, and\nQwen-2.5-7B, we demonstrate that PromptRefine significantly outperforms\nexisting frameworks for retrieving examples.", "published": "2024-12-07 17:51:31", "link": "http://arxiv.org/abs/2412.05710v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Training-Free Bayesianization for Low-Rank Adapters of Large Language\n  Models", "abstract": "Estimating the uncertainty of responses of Large Language Models~(LLMs)\nremains a critical challenge. While recent Bayesian methods have demonstrated\neffectiveness in quantifying uncertainty through low-rank weight updates, they\ntypically require complex fine-tuning or post-training procedures. In this\npaper, we propose Training-Free Bayesianization~(TFB), a novel framework that\ntransforms existing off-the-shelf trained LoRA adapters into Bayesian ones\nwithout additional training. TFB systematically searches for the maximally\nacceptable level of variance in the weight posterior, constrained within a\nfamily of low-rank isotropic Gaussian distributions. We theoretically\ndemonstrate that under mild conditions, this search process is equivalent to\nvariational inference for the weights. Through comprehensive experiments, we\nshow that TFB achieves superior uncertainty estimation and generalization\ncompared to existing methods while eliminating the need for complex training\nprocedures. Code will be available at\nhttps://github.com/Wang-ML-Lab/bayesian-peft.", "published": "2024-12-07 18:49:27", "link": "http://arxiv.org/abs/2412.05723v1", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "TransitGPT: A Generative AI-based framework for interacting with GTFS\n  data using Large Language Models", "abstract": "This paper introduces a framework that leverages Large Language Models (LLMs)\nto answer natural language queries about General Transit Feed Specification\n(GTFS) data. The framework is implemented in a chatbot called TransitGPT with\nopen-source code. TransitGPT works by guiding LLMs to generate Python code that\nextracts and manipulates GTFS data relevant to a query, which is then executed\non a server where the GTFS feed is stored. It can accomplish a wide range of\ntasks, including data retrieval, calculations, and interactive visualizations,\nwithout requiring users to have extensive knowledge of GTFS or programming. The\nLLMs that produce the code are guided entirely by prompts, without fine-tuning\nor access to the actual GTFS feeds. We evaluate TransitGPT using GPT-4o and\nClaude-3.5-Sonnet LLMs on a benchmark dataset of 100 tasks, to demonstrate its\neffectiveness and versatility. The results show that TransitGPT can\nsignificantly enhance the accessibility and usability of transit data.", "published": "2024-12-07 00:35:41", "link": "http://arxiv.org/abs/2412.06831v1", "categories": ["cs.CL", "cs.AI", "stat.AP"], "primary_category": "cs.CL"}
{"title": "SLA Management in Reconfigurable Multi-Agent RAG: A Systems Approach to\n  Question Answering", "abstract": "Retrieval Augmented Generation (RAG) enables Large Language Models (LLMs) to\ngeneralize to new information by decoupling reasoning capabilities from static\nknowledge bases. Traditional RAG enhancements have explored vertical scaling --\nassigning subtasks to specialized modules -- and horizontal scaling --\nreplicating tasks across multiple agents -- to improve performance. However,\nreal-world applications impose diverse Service Level Agreements (SLAs) and\nQuality of Service (QoS) requirements, involving trade-offs among objectives\nsuch as reducing cost, ensuring answer quality, and adhering to specific\noperational constraints.\n  In this work, we present a systems-oriented approach to multi-agent RAG\ntailored for real-world Question Answering (QA) applications. By integrating\ntask-specific non-functional requirements -- such as answer quality, cost, and\nlatency -- into the system, we enable dynamic reconfiguration to meet diverse\nSLAs. Our method maps these Service Level Objectives (SLOs) to system-level\nparameters, allowing the generation of optimal results within specified\nresource constraints.\n  We conduct a case study in the QA domain, demonstrating how dynamic\nre-orchestration of a multi-agent RAG system can effectively manage the\ntrade-off between answer quality and cost. By adjusting the system based on\nquery intent and operational conditions, we systematically balance performance\nand resource utilization. This approach allows the system to meet SLOs for\nvarious query types, showcasing its practicality for real-world applications.", "published": "2024-12-07 01:32:13", "link": "http://arxiv.org/abs/2412.06832v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.DC", "H.3.4; H.3.3; I.2.7; I.2.11; C.2.4"], "primary_category": "cs.SE"}
{"title": "LABIIUM: AI-Enhanced Zero-configuration Measurement Automation System", "abstract": "The complexity of laboratory environments requires solutions that simplify\ninstrument interaction and enhance measurement automation. Traditional tools\noften require configuration, software, and programming skills, creating\nbarriers to productivity. Previous approaches, including dedicated software\nsuites and custom scripts, frequently fall short in providing user-friendly\nsolutions that align with programming practices. We present LABIIUM, an\nAI-enhanced, zero-configuration measurement automation system designed to\nstreamline experimental workflows and improve user productivity. LABIIUM\nintegrates an AI assistant powered by Large Language Models (LLMs) to generate\ncode. LABIIUM's Lab-Automation-Measurement Bridges (LAMBs) enable seamless\ninstrument connectivity using standard tools such as VSCode and Python,\neliminating setup overhead. To demonstrate its capabilities, we conducted\nexperiments involving the measurement of the parametric transfer curve of a\nsimple two-transistor inverting amplifier with a current source load. The AI\nassistant was evaluated using different prompt scenarios and compared with\nmultiple models, including Claude Sonnet 3.5, Gemini Pro 1.5, and GPT-4o. An\nexpert solution implementing the Gradient-Weighted Adaptive Stochastic Sampling\n(GWASS) method was used as a baseline. The solutions generated by the AI\nassistant were compared with the expert solution and a uniform linear sweep\nbaseline with 10,000 points. The graph results show that the LLMs were able to\nsuccessfully complete the most basic uniform sweep, but LLMs were unable to\ndevelop adaptive sweeping algorithms to compete with GWASS. The evaluation\nunderscores LABIIUM's ability to enhance laboratory productivity and support\ndigital transformation in research and industry, and emphasizes the future work\nrequired to improve LLM performance in Electronic Measurement Science Tasks.", "published": "2024-12-07 00:15:24", "link": "http://arxiv.org/abs/2412.16172v2", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.SE"], "primary_category": "cs.AI"}
{"title": "The Illusion-Illusion: Vision Language Models See Illusions Where There\n  are None", "abstract": "Illusions are entertaining, but they are also a useful diagnostic tool in\ncognitive science, philosophy, and neuroscience. A typical illusion shows a gap\nbetween how something \"really is\" and how something \"appears to be\", and this\ngap helps us understand the mental processing that lead to how something\nappears to be. Illusions are also useful for investigating artificial systems,\nand much research has examined whether computational models of perceptions fall\nprey to the same illusions as people. Here, I invert the standard use of\nperceptual illusions to examine basic processing errors in current vision\nlanguage models. I present these models with illusory-illusions, neighbors of\ncommon illusions that should not elicit processing errors. These include such\nthings as perfectly reasonable ducks, crooked lines that truly are crooked,\ncircles that seem to have different sizes because they are, in fact, of\ndifferent sizes, and so on. I show that many current vision language systems\nmistakenly see these illusion-illusions as illusions. I suggest that such\nfailures are part of broader failures already discussed in the literature.", "published": "2024-12-07 03:30:51", "link": "http://arxiv.org/abs/2412.18613v1", "categories": ["q-bio.NC", "cs.CL", "cs.CV"], "primary_category": "q-bio.NC"}
{"title": "SAME: Learning Generic Language-Guided Visual Navigation with\n  State-Adaptive Mixture of Experts", "abstract": "The academic field of learning instruction-guided visual navigation can be\ngenerally categorized into high-level category-specific search and low-level\nlanguage-guided navigation, depending on the granularity of language\ninstruction, in which the former emphasizes the exploration process, while the\nlatter concentrates on following detailed textual commands. Despite the\ndiffering focuses of these tasks, the underlying requirements of interpreting\ninstructions, comprehending the surroundings, and inferring action decisions\nremain consistent. This paper consolidates diverse navigation tasks into a\nunified and generic framework -- we investigate the core difficulties of\nsharing general knowledge and exploiting task-specific capabilities in learning\nnavigation and propose a novel State-Adaptive Mixture of Experts (SAME) model\nthat effectively enables an agent to infer decisions based on\ndifferent-granularity language and dynamic observations. Powered by SAME, we\npresent a versatile agent capable of addressing seven navigation tasks\nsimultaneously that outperforms or achieves highly comparable performance to\ntask-specific agents.", "published": "2024-12-07 06:12:53", "link": "http://arxiv.org/abs/2412.05552v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "SQ-Whisper: Speaker-Querying based Whisper Model for Target-Speaker ASR", "abstract": "Benefiting from massive and diverse data sources, speech foundation models\nexhibit strong generalization and knowledge transfer capabilities to a wide\nrange of downstream tasks. However, a limitation arises from their exclusive\nhandling of single-speaker speech input, making them ineffective in recognizing\nmulti-speaker overlapped speech, a common occurrence in real-world scenarios.\nIn this study, we delve into the adaptation of speech foundation models to\neliminate interfering speakers from overlapping speech and perform\ntarget-speaker automatic speech recognition (TS-ASR). Initially, we utilize the\nWhisper model as the foundation for adaptation and conduct a thorough\ncomparison of its integration with existing target-speaker adaptation\ntechniques. We then propose an innovative model termed Speaker-Querying Whisper\n(SQ-Whisper), which employs a set number of trainable queries to capture\nspeaker prompts from overlapping speech based on target-speaker enrollment.\nThese prompts serve to steer the model in extracting speaker-specific features\nand accurately recognizing target-speaker transcriptions. Experimental results\ndemonstrate that our approach effectively adapts the pre-trained speech\nfoundation model to TS-ASR. Compared with the robust TS-HuBERT model, the\nproposed SQ-Whisper significantly improves performance, yielding up to 15% and\n10% relative reductions in word error rates (WERs) on the Libri2Mix and\nWSJ0-2Mix datasets, respectively. With data augmentation, we establish new\nstate-of-the-art WERs of 14.6% on the Libri2Mix Test set and 4.4% on the\nWSJ0-2Mix Test set. Furthermore, we evaluate our model on the real-world AMI\nmeeting dataset, which shows consistent improvement over other adaptation\nmethods.", "published": "2024-12-07 08:53:37", "link": "http://arxiv.org/abs/2412.05589v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Combining Genre Classification and Harmonic-Percussive Features with\n  Diffusion Models for Music-Video Generation", "abstract": "This study presents a novel method for generating music visualisers using\ndiffusion models, combining audio input with user-selected artwork. The process\ninvolves two main stages: image generation and video creation. First, music\ncaptioning and genre classification are performed, followed by the retrieval of\nartistic style descriptions. A diffusion model then generates images based on\nthe user's input image and the derived artistic style descriptions. The video\ngeneration stage utilises the same diffusion model to interpolate frames,\ncontrolled by audio energy vectors derived from key musical features of\nharmonics and percussives. The method demonstrates promising results across\nvarious genres, and a new metric, Audio-Visual Synchrony (AVS), is introduced\nto quantitatively evaluate the synchronisation between visual and audio\nelements. Comparative analysis shows significantly higher AVS values for videos\ngenerated using the proposed method with audio energy vectors, compared to\nlinear interpolation. This approach has potential applications in diverse\nfields, including independent music video creation, film production, live music\nevents, and enhancing audio-visual experiences in public spaces.", "published": "2024-12-07 16:43:02", "link": "http://arxiv.org/abs/2412.05694v1", "categories": ["cs.MM", "cs.GR", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition", "abstract": "Speech emotion recognition (SER) remains a challenging yet crucial task due\nto the inherent complexity and diversity of human emotions. To address this\nproblem, researchers attempt to fuse information from other modalities via\nmultimodal learning. However, existing multimodal fusion techniques often\noverlook the intricacies of cross-modal interactions, resulting in suboptimal\nfeature representations. In this paper, we propose WavFusion, a multimodal\nspeech emotion recognition framework that addresses critical research problems\nin effective multimodal fusion, heterogeneity among modalities, and\ndiscriminative representation learning. By leveraging a gated cross-modal\nattention mechanism and multimodal homogeneous feature discrepancy learning,\nWavFusion demonstrates improved performance over existing state-of-the-art\nmethods on benchmark datasets. Our work highlights the importance of capturing\nnuanced cross-modal interactions and learning discriminative representations\nfor accurate multimodal SER. Experimental results on two benchmark datasets\n(IEMOCAP and MELD) demonstrate that WavFusion succeeds over the\nstate-of-the-art strategies on emotion recognition.", "published": "2024-12-07 06:43:39", "link": "http://arxiv.org/abs/2412.05558v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
