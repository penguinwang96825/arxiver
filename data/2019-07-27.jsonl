{"title": "Nefnir: A high accuracy lemmatizer for Icelandic", "abstract": "Lemmatization, finding the basic morphological form of a word in a corpus, is\nan important step in many natural language processing tasks when working with\nmorphologically rich languages. We describe and evaluate Nefnir, a new open\nsource lemmatizer for Icelandic. Nefnir uses suffix substitution rules, derived\nfrom a large morphological database, to lemmatize tagged text. Evaluation shows\nthat for correctly tagged text, Nefnir obtains an accuracy of 99.55%, and for\ntext tagged with a PoS tagger, the accuracy obtained is 96.88%.", "published": "2019-07-27 13:30:56", "link": "http://arxiv.org/abs/1907.11907v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Hybrid Neural Network Model for Commonsense Reasoning", "abstract": "This paper proposes a hybrid neural network (HNN) model for commonsense\nreasoning. An HNN consists of two component models, a masked language model and\na semantic similarity model, which share a BERT-based contextual encoder but\nuse different model-specific input and output layers. HNN obtains new\nstate-of-the-art results on three classic commonsense reasoning tasks, pushing\nthe WNLI benchmark to 89%, the Winograd Schema Challenge (WSC) benchmark to\n75.1%, and the PDP60 benchmark to 90.0%. An ablation study shows that language\nmodels and semantic similarity models are complementary approaches to\ncommonsense reasoning, and HNN effectively combines the strengths of both. The\ncode and pre-trained models will be publicly available at\nhttps://github.com/namisan/mt-dnn.", "published": "2019-07-27 21:51:52", "link": "http://arxiv.org/abs/1907.11983v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Linguistic Complexity and Scientific Impact", "abstract": "The number of publications and the number of citations received have become\nthe most common indicators of scholarly success. In this context, scientific\nwriting increasingly plays an important role in scholars' scientific careers.\nTo understand the relationship between scientific writing and scientific\nimpact, this paper selected 12 variables of linguistic complexity as a proxy\nfor depicting scientific writing. We then analyzed these features from 36,400\nfull-text Biology articles and 1,797 full-text Psychology articles. These\nfeatures were compared to the scientific impact of articles, grouped into high,\nmedium, and low categories. The results suggested no practical significant\nrelationship between linguistic complexity and citation strata in either\ndiscipline. This suggests that textual complexity plays little role in\nscientific impact in our data sets.", "published": "2019-07-27 04:08:55", "link": "http://arxiv.org/abs/1907.11843v1", "categories": ["cs.CL", "cs.DL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Towards Effective Rebuttal: Listening Comprehension using Corpus-Wide\n  Claim Mining", "abstract": "Engaging in a live debate requires, among other things, the ability to\neffectively rebut arguments claimed by your opponent. In particular, this\nrequires identifying these arguments. Here, we suggest doing so by\nautomatically mining claims from a corpus of news articles containing billions\nof sentences, and searching for them in a given speech. This raises the\nquestion of whether such claims indeed correspond to those made in spoken\nspeeches. To this end, we collected a large dataset of $400$ speeches in\nEnglish discussing $200$ controversial topics, mined claims for each topic, and\nasked annotators to identify the mined claims mentioned in each speech. Results\nshow that in the vast majority of speeches debaters indeed make use of such\nclaims. In addition, we present several baselines for the automatic detection\nof mined claims in speeches, forming the basis for future work. All collected\ndata is freely available for research.", "published": "2019-07-27 10:19:19", "link": "http://arxiv.org/abs/1907.11889v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is BERT Really Robust? A Strong Baseline for Natural Language Attack on\n  Text Classification and Entailment", "abstract": "Machine learning algorithms are often vulnerable to adversarial examples that\nhave imperceptible alterations from the original counterparts but can fool the\nstate-of-the-art models. It is helpful to evaluate or even improve the\nrobustness of these models by exposing the maliciously crafted adversarial\nexamples. In this paper, we present TextFooler, a simple but strong baseline to\ngenerate natural adversarial text. By applying it to two fundamental natural\nlanguage tasks, text classification and textual entailment, we successfully\nattacked three target models, including the powerful pre-trained BERT, and the\nwidely used convolutional and recurrent neural networks. We demonstrate the\nadvantages of this framework in three ways: (1) effective---it outperforms\nstate-of-the-art attacks in terms of success rate and perturbation rate, (2)\nutility-preserving---it preserves semantic content and grammaticality, and\nremains correctly classified by humans, and (3) efficient---it generates\nadversarial text with computational complexity linear to the text length. *The\ncode, pre-trained target models, and test examples are available at\nhttps://github.com/jind11/TextFooler.", "published": "2019-07-27 15:07:04", "link": "http://arxiv.org/abs/1907.11932v6", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generalization of Spectrum Differential based Direct Waveform\n  Modification for Voice Conversion", "abstract": "We present a modification to the spectrum differential based direct waveform\nmodification for voice conversion (DIFFVC) so that it can be directly applied\nas a waveform generation module to voice conversion models. The recently\nproposed DIFFVC avoids the use of a vocoder, meanwhile preserves rich spectral\ndetails hence capable of generating high quality converted voice. To apply the\nDIFFVC framework, a model that can estimate the spectral differential from the\nF0 transformed input speech needs to be trained beforehand. This requirement\nimposes several constraints, including a limitation on the estimation model to\nparallel training and the need of extra training on each conversion pair, which\nmake DIFFVC inflexible. Based on the above motivations, we propose a new DIFFVC\nframework based on an F0 transformation in the residual domain. By performing\ninverse filtering on the input signal followed by synthesis filtering on the F0\ntransformed residual signal using the converted spectral features directly, the\nspectral conversion model does not need to be retrained or capable of\npredicting the spectral differential. We describe several details that need to\nbe taken care of under this modification, and by applying our proposed method\nto a non-parallel, variational autoencoder (VAE)-based spectral conversion\nmodel, we demonstrate that this framework can be generalized to any spectral\nconversion model, and experimental evaluations show that it can outperform a\nbaseline framework whose waveform generation process is carried out by a\nvocoder.", "published": "2019-07-27 11:57:55", "link": "http://arxiv.org/abs/1907.11898v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Dilated FCN: Listening Longer to Hear Better", "abstract": "Deep neural network solutions have emerged as a new and powerful paradigm for\nspeech enhancement (SE). The capabilities to capture long context and extract\nmulti-scale patterns are crucial to design effective SE networks. Such\ncapabilities, however, are often in conflict with the goal of maintaining\ncompact networks to ensure good system generalization. In this paper, we\nexplore dilation operations and apply them to fully convolutional networks\n(FCNs) to address this issue. Dilations equip the networks with greatly\nexpanded receptive fields, without increasing the number of parameters.\nDifferent strategies to fuse multi-scale dilations, as well as to install the\ndilation modules are explored in this work. Using Noisy VCTK and AzBio\nsentences datasets, we demonstrate that the proposed dilation models\nsignificantly improve over the baseline FCN and outperform the state-of-the-art\nSE solutions.", "published": "2019-07-27 17:52:33", "link": "http://arxiv.org/abs/1907.11956v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
