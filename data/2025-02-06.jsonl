{"title": "Quantum Powered Credit Risk Assessment: A Novel Approach using hybrid Quantum-Classical Deep Neural Network for Row-Type Dependent Predictive Analysis", "abstract": "The integration of Quantum Deep Learning (QDL) techniques into the landscape\nof financial risk analysis presents a promising avenue for innovation. This\nstudy introduces a framework for credit risk assessment in the banking sector,\ncombining quantum deep learning techniques with adaptive modeling for Row-Type\nDependent Predictive Analysis (RTDPA). By leveraging RTDPA, the proposed\napproach tailors predictive models to different loan categories, aiming to\nenhance the accuracy and efficiency of credit risk evaluation. While this work\nexplores the potential of integrating quantum methods with classical deep\nlearning for risk assessment, it focuses on the feasibility and performance of\nthis hybrid framework rather than claiming transformative industry-wide\nimpacts. The findings offer insights into how quantum techniques can complement\ntraditional financial analysis, paving the way for further advancements in\npredictive modeling for credit risk.", "published": "2025-02-06 10:57:18", "link": "http://arxiv.org/abs/2502.07806v1", "categories": ["q-fin.CP", "cs.AI", "cs.LG"], "primary_category": "q-fin.CP"}
{"title": "On the Effect of Alpha Decay and Transaction Costs on the Multi-period Optimal Trading Strategy", "abstract": "We consider the multi-period portfolio optimization problem with a single\nasset that can be held long or short. Due to the presence of transaction costs,\nmaximizing the immediate reward at each period may prove detrimental, as\nfrequent trading results in consistent negative cash outflows. To simulate\nalpha decay, we consider a case where not only the present value of a signal,\nbut also past values, have predictive power. We formulate the problem as an\ninfinite horizon Markov Decision Process and seek to characterize the optimal\npolicy that realizes the maximum average expected reward. We propose a variant\nof the standard value iteration algorithm for computing the optimal policy.\nEstablishing convergence in our setting is nontrivial, and we provide a\nrigorous proof. Addtionally, we compute a first-order approximation and\nasymptotics of the optimal policy with small transaction costs.", "published": "2025-02-06 18:31:50", "link": "http://arxiv.org/abs/2502.04284v1", "categories": ["math.OC", "q-fin.MF", "49J22, 93E20"], "primary_category": "math.OC"}
{"title": "Impermanent loss and Loss-vs-Rebalancing II", "abstract": "This paper examines the relationship between impermanent loss (IL) and\nloss-versus-rebalancing (LVR) in automated market makers (AMMs). Our main focus\nis on statistical properties, the impact of fees, the role of block times, and,\nrelated to the latter, the continuous time limit. We find there are three\nrelevant regimes: (i) very short times where LVR and IL are identical; (ii)\nintermediate time where LVR and IL show distinct distribution functions but are\nconnected via the central limit theorem exhibiting the same expectation value;\n(iii) long time behavior where both the distribution functions and averages are\ndistinct. Subsequently, we study how fees change this dynamics with a special\nfocus on competing time scales like block times and 'arbitrage times'.", "published": "2025-02-06 14:16:10", "link": "http://arxiv.org/abs/2502.04097v2", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "High-Frequency Market Manipulation Detection with a Markov-modulated Hawkes process", "abstract": "This work focuses on a self-exciting point process defined by a Hawkes-like\nintensity and a switching mechanism based on a hidden Markov chain. Previous\nworks in such a setting assume constant intensities between consecutive events.\nWe extend the model to general Hawkes excitation kernels that are piecewise\nconstant between events. We develop an expectation-maximization algorithm for\nthe statistical inference of the Hawkes intensities parameters as well as the\nstate transition probabilities. The numerical convergence of the estimators is\nextensively tested on simulated data. Using high-frequency cryptocurrency data\non a top centralized exchange, we apply the model to the detection of anomalous\nbursts of trades. We benchmark the goodness-of-fit of the model with the\nMarkov-modulated Poisson process and demonstrate the relevance of the model in\ndetecting suspicious activities.", "published": "2025-02-06 12:31:17", "link": "http://arxiv.org/abs/2502.04027v1", "categories": ["stat.ME", "q-fin.ST", "q-fin.TR"], "primary_category": "stat.ME"}
{"title": "Rethinking the Residual Distribution of Locate-then-Editing Methods in\n  Model Editing", "abstract": "Model editing is a powerful technique for updating the knowledge of Large\nLanguage Models (LLMs). Locate-then-edit methods are a popular class of\napproaches that first identify the critical layers storing knowledge, then\ncompute the residual of the last critical layer based on the edited knowledge,\nand finally perform multi-layer updates using a least-squares solution by\nevenly distributing the residual from the first critical layer to the last.\nAlthough these methods achieve promising results, they have been shown to\ndegrade the original knowledge of LLMs. We argue that residual distribution\nleads to this issue. To explore this, we conduct a comprehensive analysis of\nresidual distribution in locate-then-edit methods from both empirical and\ntheoretical perspectives, revealing that residual distribution introduces\nediting errors, leading to inaccurate edits. To address this issue, we propose\nthe Boundary Layer UpdatE (BLUE) strategy to enhance locate-then-edit methods.\nSequential batch editing experiments on three LLMs and two datasets demonstrate\nthat BLUE not only delivers an average performance improvement of 35.59\\%,\nsignificantly advancing the state of the art in model editing, but also\nenhances the preservation of LLMs' general capabilities. Our code is available\nat https://github.com/xpq-tech/BLUE.", "published": "2025-02-06 03:20:17", "link": "http://arxiv.org/abs/2502.03748v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Contextual Manifold Alignment for Structuring Latent\n  Representations in Large Language Models", "abstract": "The organization of latent token representations plays a crucial role in\ndetermining the stability, generalization, and contextual consistency of\nlanguage models, yet conventional approaches to embedding refinement often rely\non parameter modifications that introduce additional computational overhead. A\nhierarchical alignment method was introduced to restructure token embeddings\nwithout altering core model weights, ensuring that representational\ndistributions maintained coherence across different linguistic contexts.\nExperimental evaluations demonstrated improvements in rare token retrieval,\nadversarial robustness, and long-range dependency tracking, highlighting the\nadvantages of hierarchical structuring in mitigating inconsistencies in latent\nspace organization. The comparative analysis against conventional fine-tuning\nand embedding perturbation methods revealed that hierarchical restructuring\nmaintained computational efficiency while achieving measurable gains in\nrepresentation quality. Structural refinements introduced through the alignment\nprocess resulted in improved contextual stability across varied linguistic\ntasks, reducing inconsistencies in token proximity relationships and enhancing\ninterpretability in language generation. A detailed computational assessment\nconfirmed that the realignment process introduced minimal inference overhead,\nensuring that representational improvements did not compromise model\nefficiency. The findings reinforced the broader significance of structured\nrepresentation learning, illustrating that hierarchical embedding modifications\ncould serve as an effective strategy for refining latent space distributions\nwhile preserving pre-learned semantic associations.", "published": "2025-02-06 04:01:27", "link": "http://arxiv.org/abs/2502.03766v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective", "abstract": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods.", "published": "2025-02-06 06:31:47", "link": "http://arxiv.org/abs/2502.03805v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PsyPlay: Personality-Infused Role-Playing Conversational Agents", "abstract": "The current research on Role-Playing Conversational Agents (RPCAs) with Large\nLanguage Models (LLMs) primarily focuses on imitating specific speaking styles\nand utilizing character backgrounds, neglecting the depiction of deeper\npersonality traits.~In this study, we introduce personality-infused\nrole-playing for LLM agents, which encourages agents to accurately portray\ntheir designated personality traits during dialogues. We then propose PsyPlay,\na dialogue generation framework that facilitates the expression of rich\npersonalities among multiple LLM agents. Specifically, PsyPlay enables agents\nto assume roles with distinct personality traits and engage in discussions\ncentered around specific topics, consistently exhibiting their designated\npersonality traits throughout the interactions. Validation on generated\ndialogue data demonstrates that PsyPlay can accurately portray the intended\npersonality traits, achieving an overall success rate of 80.31% on GPT-3.5.\nNotably, we observe that LLMs aligned with positive values are more successful\nin portraying positive personality roles compared to negative ones. Moreover,\nwe construct a dialogue corpus for personality-infused role-playing, called\nPsyPlay-Bench. The corpus, which consists of 4745 instances of correctly\nportrayed dialogues using PsyPlay, aims to further facilitate research in\npersonalized role-playing and dialogue personality detection.", "published": "2025-02-06 07:17:12", "link": "http://arxiv.org/abs/2502.03821v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BOLT: Bootstrap Long Chain-of-Thought in Language Models without\n  Distillation", "abstract": "Large language models (LLMs), such as o1 from OpenAI, have demonstrated\nremarkable reasoning capabilities. o1 generates a long chain-of-thought\n(LongCoT) before answering a question. LongCoT allows LLMs to analyze problems,\ndevise plans, reflect, and backtrack effectively. These actions empower LLM to\nsolve complex problems. After the release of o1, many teams have attempted to\nreplicate its LongCoT and reasoning capabilities. In terms of methods, they\nprimarily rely on knowledge distillation with data from existing models with\nLongCoT capacities (e.g., OpenAI-o1, Qwen-QwQ, DeepSeek-R1-Preview), leaving\nsignificant uncertainties on systematically developing such reasoning\nabilities. In terms of data domains, these works focus narrowly on math while a\nfew others include coding, limiting their generalizability. This paper\nintroduces a novel approach to enable LLM's LongCoT capacity without\ndistillation from o1-like models or expensive human annotations, where we\nbootstrap LongCoT (BOLT) from a standard instruct model. BOLT involves three\nstages: 1) LongCoT data bootstrapping with in-context learning on a standard\ninstruct model; 2) LongCoT supervised finetuning; 3) online training to further\nrefine LongCoT capacities. In BOLT, only a few in-context examples need to be\nconstructed during the bootstrapping stage; in our experiments, we created 10\nexamples, demonstrating the feasibility of this approach. We use\nLlama-3.1-70B-Instruct to bootstrap LongCoT and apply our method to various\nmodel scales (7B, 8B, 70B). We achieve impressive performance on a variety of\nbenchmarks, Arena-Hard, MT-Bench, WildBench, ZebraLogic, MATH500, which\nevaluate diverse task-solving and reasoning capabilities.", "published": "2025-02-06 08:19:59", "link": "http://arxiv.org/abs/2502.03860v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Afrispeech-Dialog: A Benchmark Dataset for Spontaneous English\n  Conversations in Healthcare and Beyond", "abstract": "Speech technologies are transforming interactions across various sectors,\nfrom healthcare to call centers and robots, yet their performance on\nAfrican-accented conversations remains underexplored. We introduce\nAfrispeech-Dialog, a benchmark dataset of 50 simulated medical and non-medical\nAfrican-accented English conversations, designed to evaluate automatic speech\nrecognition (ASR) and related technologies. We assess state-of-the-art (SOTA)\nspeaker diarization and ASR systems on long-form, accented speech, comparing\ntheir performance with native accents and discover a 10%+ performance\ndegradation. Additionally, we explore medical conversation summarization\ncapabilities of large language models (LLMs) to demonstrate the impact of ASR\nerrors on downstream medical summaries, providing insights into the challenges\nand opportunities for speech technologies in the Global South. Our work\nhighlights the need for more inclusive datasets to advance conversational AI in\nlow-resource settings.", "published": "2025-02-06 10:33:07", "link": "http://arxiv.org/abs/2502.03945v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantification of Biodiversity from Historical Survey Text with\n  LLM-based Best-Worst Scaling", "abstract": "In this study, we evaluate methods to determine the frequency of species via\nquantity estimation from historical survey text. To that end, we formulate\nclassification tasks and finally show that this problem can be adequately\nframed as a regression task using Best-Worst Scaling (BWS) with Large Language\nModels (LLMs). We test Ministral-8B, DeepSeek-V3, and GPT-4, finding that the\nlatter two have reasonable agreement with humans and each other. We conclude\nthat this approach is more cost-effective and similarly robust compared to a\nfine-grained multi-class approach, allowing automated quantity estimation\nacross species.", "published": "2025-02-06 12:25:16", "link": "http://arxiv.org/abs/2502.04022v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simulating the Emergence of Differential Case Marking with Communicating\n  Neural-Network Agents", "abstract": "Differential Case Marking (DCM) refers to the phenomenon where grammatical\ncase marking is applied selectively based on semantic, pragmatic, or other\nfactors. The emergence of DCM has been studied in artificial language learning\nexperiments with human participants, which were specifically aimed at\ndisentangling the effects of learning from those of communication (Smith &\nCulbertson, 2020). Multi-agent reinforcement learning frameworks based on\nneural networks have gained significant interest to simulate the emergence of\nhuman-like linguistic phenomena. In this study, we employ such a framework in\nwhich agents first acquire an artificial language before engaging in\ncommunicative interactions, enabling direct comparisons to human result. Using\na very generic communication optimization algorithm and neural-network learners\nthat have no prior experience with language or semantic preferences, our\nresults demonstrate that learning alone does not lead to DCM, but when agents\ncommunicate, differential use of markers arises. This supports Smith and\nCulbertson (2020)'s findings that highlight the critical role of communication\nin shaping DCM and showcases the potential of neural-agent models to complement\nexperimental research on language evolution.", "published": "2025-02-06 13:00:53", "link": "http://arxiv.org/abs/2502.04038v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controllable Emotion Generation with Emotion Vectors", "abstract": "In recent years, technologies based on large-scale language models (LLMs)\nhave made remarkable progress in many fields, especially in customer service,\ncontent creation, and embodied intelligence, showing broad application\npotential. However, The LLM's ability to express emotions with proper tone,\ntiming, and in both direct and indirect forms is still insufficient but\nsignificant. Few works have studied on how to build the controlable emotional\nexpression capability of LLMs. In this work, we propose a method for emotion\nexpression output by LLMs, which is universal, highly flexible, and well\ncontrollable proved with the extensive experiments and verifications. This\nmethod has broad application prospects in fields involving emotions output by\nLLMs, such as intelligent customer service, literary creation, and home\ncompanion robots. The extensive experiments on various LLMs with different\nmodel-scales and architectures prove the versatility and the effectiveness of\nthe proposed method.", "published": "2025-02-06 13:38:57", "link": "http://arxiv.org/abs/2502.04075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Order Effect: Investigating Prompt Sensitivity in Closed-Source LLMs", "abstract": "As large language models (LLMs) become integral to diverse applications,\nensuring their reliability under varying input conditions is crucial. One key\nissue affecting this reliability is order sensitivity, wherein slight\nvariations in input arrangement can lead to inconsistent or biased outputs.\nAlthough recent advances have reduced this sensitivity, the problem remains\nunresolved. This paper investigates the extent of order sensitivity in\nclosed-source LLMs by conducting experiments across multiple tasks, including\nparaphrasing, relevance judgment, and multiple-choice questions. Our results\nshow that input order significantly affects performance across tasks, with\nshuffled inputs leading to measurable declines in output accuracy. Few-shot\nprompting demonstrates mixed effectiveness and offers partial mitigation,\nhowever, fails to fully resolve the problem. These findings highlight\npersistent risks, particularly in high-stakes applications, and point to the\nneed for more robust LLMs or improved input-handling techniques in future\ndevelopment.", "published": "2025-02-06 15:14:02", "link": "http://arxiv.org/abs/2502.04134v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexical Substitution is not Synonym Substitution: On the Importance of\n  Producing Contextually Relevant Word Substitutes", "abstract": "Lexical Substitution is the task of replacing a single word in a sentence\nwith a similar one. This should ideally be one that is not necessarily only\nsynonymous, but also fits well into the surrounding context of the target word,\nwhile preserving the sentence's grammatical structure. Recent advances in\nLexical Substitution have leveraged the masked token prediction task of\nPre-trained Language Models to generate replacements for a given word in a\nsentence. With this technique, we introduce ConCat, a simple augmented approach\nwhich utilizes the original sentence to bolster contextual information sent to\nthe model. Compared to existing approaches, it proves to be very effective in\nguiding the model to make contextually relevant predictions for the target\nword. Our study includes a quantitative evaluation, measured via sentence\nsimilarity and task performance. In addition, we conduct a qualitative human\nanalysis to validate that users prefer the substitutions proposed by our\nmethod, as opposed to previous methods. Finally, we test our approach on the\nprevailing benchmark for Lexical Substitution, CoInCo, revealing potential\npitfalls of the benchmark. These insights serve as the foundation for a\ncritical discussion on the way in which Lexical Substitution is evaluated.", "published": "2025-02-06 16:05:50", "link": "http://arxiv.org/abs/2502.04173v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sports and Women's Sports: Gender Bias in Text Generation with Olympic\n  Data", "abstract": "Large Language Models (LLMs) have been shown to be biased in prior work, as\nthey generate text that is in line with stereotypical views of the world or\nthat is not representative of the viewpoints and values of historically\nmarginalized demographic groups. In this work, we propose using data from\nparallel men's and women's events at the Olympic Games to investigate different\nforms of gender bias in language models. We define three metrics to measure\nbias, and find that models are consistently biased against women when the\ngender is ambiguous in the prompt. In this case, the model frequently retrieves\nonly the results of the men's event with or without acknowledging them as such,\nrevealing pervasive gender bias in LLMs in the context of athletics.", "published": "2025-02-06 17:01:00", "link": "http://arxiv.org/abs/2502.04218v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus\n  Expansion", "abstract": "Despite the remarkable capabilities of large language models across various\ntasks, their continued scaling faces a critical challenge: the scarcity of\nhigh-quality pretraining data. While model architectures continue to evolve,\nthe natural language data struggles to scale up. To tackle this bottleneck, we\npropose \\textbf{MA}ssive \\textbf{G}enre-\\textbf{A}udience~(MAGA) reformulation\nmethod, which systematic synthesizes diverse, contextually-rich pretraining\ndata from existing corpus. This work makes three main contributions: (1) We\npropose MAGA reformulation method, a lightweight and scalable approach for\npretraining corpus expansion, and build a 770B tokens MAGACorpus. (2) We\nevaluate MAGACorpus with different data budget scaling strategies,\ndemonstrating consistent improvements across various model sizes (134M-13B),\nestablishing the necessity for next-generation large-scale synthetic\npretraining language models. (3) Through comprehensive analysis, we investigate\nprompt engineering's impact on synthetic training collapse and reveal\nlimitations in conventional collapse detection metrics using validation losses.\nOur work shows that MAGA can substantially expand training datasets while\nmaintaining quality, offering a reliably pathway for scaling models beyond data\nlimitations.", "published": "2025-02-06 17:19:55", "link": "http://arxiv.org/abs/2502.04235v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Methodology for Studying Linguistic and Cultural Change in China,\n  1900-1950", "abstract": "This paper presents a quantitative approach to studying linguistic and\ncultural change in China during the first half of the twentieth century, a\nperiod that remains understudied in computational humanities research. The\ndramatic changes in Chinese language and culture during this time call for\ngreater reflection on the tools and methods used for text analysis. This\npreliminary study offers a framework for analyzing Chinese texts from the late\nnineteenth and twentieth centuries, demonstrating how established methods such\nas word counts and word embeddings can provide new historical insights into the\ncomplex negotiations between Western modernity and Chinese cultural discourse.", "published": "2025-02-06 18:33:50", "link": "http://arxiv.org/abs/2502.04286v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Prompt Content: Enhancing LLM Performance via Content-Format\n  Integrated Prompt Optimization", "abstract": "Large Language Models (LLMs) have shown significant capability across various\ntasks, with their real-world effectiveness often driven by prompt design. While\nrecent research has focused on optimizing prompt content, the role of prompt\nformatting, a critical but often overlooked dimension, has received limited\nsystematic investigation. In this paper, we introduce Content-Format Integrated\nPrompt Optimization (CFPO), an innovative methodology that jointly optimizes\nboth prompt content and formatting through an iterative refinement process.\nCFPO leverages natural language mutations to explore content variations and\nemploys a dynamic format exploration strategy that systematically evaluates\ndiverse format options. Our extensive evaluations across multiple tasks and\nopen-source LLMs demonstrate that CFPO demonstrates measurable performance\nimprovements compared to content-only optimization methods. This highlights the\nimportance of integrated content-format optimization and offers a practical,\nmodel-agnostic approach to enhancing LLM performance. Code is available at\nhttps://github.com/HenryLau7/CFPO.", "published": "2025-02-06 18:36:44", "link": "http://arxiv.org/abs/2502.04295v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference\n  Optimization", "abstract": "Recent research has leveraged large language model multi-agent systems for\ncomplex problem-solving while trying to reduce the manual effort required to\nbuild them, driving the development of automated agent workflow optimization\nmethods. However, existing methods remain inflexible due to representational\nlimitations, a lack of adaptability, and poor scalability when relying on\ndiscrete optimization techniques. We address these challenges with ScoreFlow, a\nsimple yet high-performance framework that leverages efficient gradient-based\noptimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel\nvariant of the direct preference optimization method that accounts for\nquantitative feedback. Across six benchmarks spanning question answering,\ncoding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over\nexisting baselines. Moreover, it empowers smaller models to outperform larger\nones with lower inference costs. Project:\nhttps://github.com/Gen-Verse/ScoreFlow", "published": "2025-02-06 18:47:49", "link": "http://arxiv.org/abs/2502.04306v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BOUQuET: dataset, Benchmark and Open initiative for Universal Quality\n  Evaluation in Translation", "abstract": "This paper presents BOUQuET, a multicentric and multi-register/domain dataset\nand benchmark, and its broader collaborative extension initiative. This dataset\nis handcrafted in non-English languages first, each of these source languages\nbeing represented among the 23 languages commonly used by half of the world's\npopulation and therefore having the potential to serve as pivot languages that\nwill enable more accurate translations. The dataset is specially designed to\navoid contamination and be multicentric, so as to enforce representation of\nmultilingual language features. In addition, the dataset goes beyond the\nsentence level, as it is organized in paragraphs of various lengths. Compared\nwith related machine translation (MT) datasets, we show that BOUQuET has a\nbroader representation of domains while simplifying the translation task for\nnon-experts. Therefore, BOUQuET is specially suitable for the open initiative\nand call for translation participation that we are launching to extend it to a\nmulti-way parallel corpus to any written language.", "published": "2025-02-06 18:56:37", "link": "http://arxiv.org/abs/2502.04314v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Variation of sentence length across time and genre", "abstract": "The goal of this paper is threefold: i) to present some practical aspects of\nusing full-text version of Corpus of Historical American English (COHA), the\nlargest diachronic multi-genre corpus of the English language, in the\ninvestigation of a linguistic trend of change; ii) to test a widely held\nassumption that sentence length in written English has been steadily decreasing\nover the past few centuries; iii) to point to a possible link between the\nchanges in sentence length and changes in the English syntactic usage. The\nempirical proof of concept for iii) is provided by the decline in the frequency\nof the non-finite purpose subordinator in order to. Sentence length, genre and\nthe likelihood of occurrence of in order to are shown to be interrelated.", "published": "2025-02-06 18:59:02", "link": "http://arxiv.org/abs/2502.04321v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Agent Reinforcement Learning with Focal Diversity Optimization", "abstract": "The advancement of Large Language Models (LLMs) and their finetuning\nstrategies has triggered the renewed interests in multi-agent reinforcement\nlearning. In this paper, we introduce a focal diversity-optimized multi-agent\nreinforcement learning approach, coined as MARL-Focal, with three unique\ncharacteristics. First, we develop an agent-fusion framework for encouraging\nmultiple LLM based agents to collaborate in producing the final inference\noutput for each LLM query. Second, we develop a focal-diversity optimized agent\nselection algorithm that can choose a small subset of the available agents\nbased on how well they can complement one another to generate the query output.\nFinally, we design a conflict-resolution method to detect output inconsistency\namong multiple agents and produce our MARL-Focal output through reward-aware\nand policy-adaptive inference fusion. Extensive evaluations on five benchmarks\nshow that MARL-Focal is cost-efficient and adversarial-robust. Our multi-agent\nfusion model achieves performance improvement of 5.51\\% compared to the best\nindividual LLM-agent and offers stronger robustness over the TruthfulQA\nbenchmark. Code is available at https://github.com/sftekin/rl-focal", "published": "2025-02-06 20:44:26", "link": "http://arxiv.org/abs/2502.04492v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Verifiable Format Control for Large Language Model Generations", "abstract": "Recent Large Language Models (LLMs) have demonstrated satisfying general\ninstruction following ability. However, small LLMs with about 7B parameters\nstill struggle fine-grained format following (e.g., JSON format), which\nseriously hinder the advancements of their applications. Most existing methods\nfocus on benchmarking general instruction following while overlook how to\nimprove the specific format following ability for small LLMs. Besides, these\nmethods often rely on evaluations based on advanced LLMs (e.g., GPT-4), which\ncan introduce the intrinsic bias of LLMs and be costly due to the API calls. In\nthis paper, we first curate a fully verifiable format following dataset VFF. In\ncontrast to existing works often adopting external LLMs for\ninstruction-following validations, every sample of VFF can be easily validated\nwith a Python function. Further, we propose to leverage this verifiable feature\nto synthesize massive data for progressively training small LLMs, in order to\nimprove their format following abilities. Experimental results highlight the\nprevalent limitations in the format following capabilities of 7B level\nopen-source LLMs and demonstrate the effectiveness of our method in enhancing\nthis essential ability.", "published": "2025-02-06 20:57:36", "link": "http://arxiv.org/abs/2502.04498v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ULPT: Prompt Tuning with Ultra-Low-Dimensional Optimization", "abstract": "Large language models achieve state-of-the-art performance but are costly to\nfine-tune due to their size. Parameter-efficient fine-tuning methods, such as\nprompt tuning, address this by reducing trainable parameters while maintaining\nstrong performance. However, prior methods tie prompt embeddings to the model's\ndimensionality, which may not scale well with larger LLMs and more customized\nLLMs. In this paper, we propose Ultra-Low-dimensional Prompt Tuning (ULPT),\nwhich optimizes prompts in a low-dimensional space (e.g., 2D) and use a random\nbut frozen matrix for the up-projection. To enhance alignment, we introduce\nlearnable shift and scale embeddings. ULPT drastically reduces the trainable\nparameters, e.g., 2D only using 2% parameters compared with vanilla prompt\ntuning while retaining most of the performance across 21 NLP tasks. Our\ntheoretical analysis shows that random projections can capture high-rank\nstructures effectively, and experimental results demonstrate ULPT's competitive\nperformance over existing parameter-efficient methods.", "published": "2025-02-06 21:00:29", "link": "http://arxiv.org/abs/2502.04501v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When One LLM Drools, Multi-LLM Collaboration Rules", "abstract": "This position paper argues that in many realistic (i.e., complex,\ncontextualized, subjective) scenarios, one LLM is not enough to produce a\nreliable output. We challenge the status quo of relying solely on a single\ngeneral-purpose LLM and argue for multi-LLM collaboration to better represent\nthe extensive diversity of data, skills, and people. We first posit that a\nsingle LLM underrepresents real-world data distributions, heterogeneous skills,\nand pluralistic populations, and that such representation gaps cannot be\ntrivially patched by further training a single LLM. We then organize existing\nmulti-LLM collaboration methods into a hierarchy, based on the level of access\nand information exchange, ranging from API-level, text-level, logit-level, to\nweight-level collaboration. Based on these methods, we highlight how multi-LLM\ncollaboration addresses challenges that a single LLM struggles with, such as\nreliability, democratization, and pluralism. Finally, we identify the\nlimitations of existing multi-LLM methods and motivate future work. We envision\nmulti-LLM collaboration as an essential path toward compositional intelligence\nand collaborative AI development.", "published": "2025-02-06 21:13:44", "link": "http://arxiv.org/abs/2502.04506v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Heterogeneous Swarms: Jointly Optimizing Model Roles and Weights for\n  Multi-LLM Systems", "abstract": "We propose Heterogeneous Swarms, an algorithm to design multi-LLM systems by\njointly optimizing model roles and weights. We represent multi-LLM systems as\ndirected acyclic graphs (DAGs) of LLMs with topological message passing for\ncollaborative generation. Given a pool of LLM experts and a utility function,\nHeterogeneous Swarms employs two iterative steps: role-step and weight-step.\nFor role-step, we interpret model roles as learning a DAG that specifies the\nflow of inputs and outputs between LLMs. Starting from a swarm of random\ncontinuous adjacency matrices, we decode them into discrete DAGs, call the LLMs\nin topological order, evaluate on the utility function (e.g. accuracy on a\ntask), and optimize the adjacency matrices with particle swarm optimization\nbased on the utility score. For weight-step, we assess the contribution of\nindividual LLMs in the multi-LLM systems and optimize model weights with swarm\nintelligence. We propose JFK-score to quantify the individual contribution of\neach LLM in the best-found DAG of the role-step, then optimize model weights\nwith particle swarm optimization based on the JFK-score. Experiments\ndemonstrate that Heterogeneous Swarms outperforms 15 role- and/or weight-based\nbaselines by 18.5% on average across 12 tasks. Further analysis reveals that\nHeterogeneous Swarms discovers multi-LLM systems with heterogeneous model roles\nand substantial collaborative gains, and benefits from the diversity of\nlanguage models.", "published": "2025-02-06 21:27:11", "link": "http://arxiv.org/abs/2502.04510v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Sample-Level Feedback: Using Reference-Level Feedback to Guide\n  Data Synthesis", "abstract": "LLMs demonstrate remarkable capabilities in following natural language\ninstructions, largely due to instruction-tuning on high-quality datasets. While\nsynthetic data generation has emerged as a scalable approach for creating such\ndatasets, maintaining consistent quality standards remains challenging. Recent\napproaches incorporate feedback to improve data quality, but typically operate\nat the sample level, generating and applying feedback for each response\nindividually. In this work, we propose Reference-Level Feedback, a novel\nmethodology that instead collects feedback based on high-quality reference\nsamples from carefully curated seed data. We use this feedback to capture rich\nsignals of desirable characteristics and propagate it throughout the data\nsynthesis process. We present REFED, a dataset of 10K instruction-response\npairs synthesized using such feedback. We demonstrate the effectiveness of our\napproach by showing that Llama-3.1-8B-Instruct finetuned on REFED achieves\nstate-of-the-art performance among similar-sized SFT-based models on AlpacaEval\n2.0 and strong results on Arena-Hard. Through extensive experiments, we show\nthat our approach consistently outperforms traditional sample-level feedback\nmethods with significantly fewer feedback collections and improves performance\nacross different model architectures.", "published": "2025-02-06 21:29:00", "link": "http://arxiv.org/abs/2502.04511v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linear Correlation in LM's Compositional Generalization and\n  Hallucination", "abstract": "The generalization of language models (LMs) is undergoing active debates,\ncontrasting their potential for general intelligence with their struggles with\nbasic knowledge composition (e.g., reverse/transition curse). This paper\nuncovers the phenomenon of linear correlations in LMs during knowledge\ncomposition. For explanation, there exists a linear transformation between\ncertain related knowledge that maps the next token prediction logits from one\nprompt to another, e.g., \"X lives in the city of\" $\\rightarrow$ \"X lives in the\ncountry of\" for every given X. This mirrors the linearity in human knowledge\ncomposition, such as Paris $\\rightarrow$ France. Our findings indicate that the\nlinear transformation is resilient to large-scale fine-tuning, generalizing\nupdated knowledge when aligned with real-world relationships, but causing\nhallucinations when it deviates. Empirical results suggest that linear\ncorrelation can serve as a potential identifier of LM's generalization.\nFinally, we show such linear correlations can be learned with a single\nfeedforward network and pre-trained vocabulary representations, indicating LM\ngeneralization heavily relies on the latter.", "published": "2025-02-06 21:44:30", "link": "http://arxiv.org/abs/2502.04520v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Decoding Algorithm for Length-Control Summarization Based on Directed\n  Acyclic Transformers", "abstract": "Length-control summarization aims to condense long texts into a short one\nwithin a certain length limit. Previous approaches often use autoregressive\n(AR) models and treat the length requirement as a soft constraint, which may\nnot always be satisfied. In this study, we propose a novel length-control\ndecoding algorithm based on the Directed Acyclic Transformer (DAT). Our\napproach allows for multiple plausible sequence fragments and predicts a\n\\emph{path} to connect them. In addition, we propose a Sequence Maximum a\nPosteriori (SeqMAP) decoding algorithm that marginalizes different possible\npaths and finds the most probable summary satisfying the length budget. Our\nalgorithm is based on beam search, which further facilitates a reranker for\nperformance improvement. Experimental results on the Gigaword and DUC2004\ndatasets demonstrate our state-of-the-art performance for length-control\nsummarization.", "published": "2025-02-06 22:12:55", "link": "http://arxiv.org/abs/2502.04535v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Non-Autoregressive Machine Translation without Knowledge\n  Distillation", "abstract": "Multilingual neural machine translation (MNMT) aims at using one single model\nfor multiple translation directions. Recent work applies non-autoregressive\nTransformers to improve the efficiency of MNMT, but requires expensive\nknowledge distillation (KD) processes. To this end, we propose an M-DAT\napproach to non-autoregressive multilingual machine translation. Our system\nleverages the recent advance of the directed acyclic Transformer (DAT), which\ndoes not require KD. We further propose a pivot back-translation (PivotBT)\napproach to improve the generalization to unseen translation directions.\nExperiments show that our M-DAT achieves state-of-the-art performance in\nnon-autoregressive MNMT.", "published": "2025-02-06 22:16:28", "link": "http://arxiv.org/abs/2502.04537v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Gradient Flow Modeling for Large Language Model\n  Generalization in Multi-Scale Feature Spaces", "abstract": "Optimization methodologies for training large-scale neural architectures\noften rely on uniform gradient propagation mechanisms that fail to align with\nhierarchical linguistic structures, limiting their capacity to generalize\nacross diverse language distributions. A structured gradient refinement\nframework was introduced to incorporate multi-scale contextual adjustments,\nimproving parameter adaptation through dynamic weighting strategies that\nenhanced representation coherence. Empirical evaluations demonstrated that\nstructured propagation mechanisms contributed to reductions in gradient\noscillations, resulting in more stable training dynamics and improved\noptimization efficiency. The comparative performance assessment indicated that\nmodels incorporating hierarchical propagation strategies exhibited greater\nrobustness in long-range dependency retention and cross-domain adaptation. The\nhierarchical adjustment of weight updates provided an alternative to\nconventional backpropagation, reducing sensitivity to initialization conditions\nwhile improving overall convergence efficiency. The experimental results\nconfirmed that structured gradient propagation influenced representation\nlearning trajectories, aligning parameter updates with broader linguistic\ndependencies rather than isolated token-level relationships. Statistical\nevaluations indicated that structured optimization strategies mitigated\noverfitting while preserving adaptability across heterogeneous text\ndistributions. The findings established that structured gradient propagation\nprovided an empirically validated framework for refining hierarchical\nrepresentation learning, supporting more effective integration of linguistic\ndependencies into optimization dynamics.", "published": "2025-02-06 22:57:40", "link": "http://arxiv.org/abs/2502.04548v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "My LLM might Mimic AAE -- But When Should it?", "abstract": "We examine the representation of African American English (AAE) in large\nlanguage models (LLMs), exploring (a) the perceptions Black Americans have of\nhow effective these technologies are at producing authentic AAE, and (b) in\nwhat contexts Black Americans find this desirable. Through both a survey of\nBlack Americans ($n=$ 104) and annotation of LLM-produced AAE by Black\nAmericans ($n=$ 228), we find that Black Americans favor choice and autonomy in\ndetermining when AAE is appropriate in LLM output. They tend to prefer that\nLLMs default to communicating in Mainstream U.S. English in formal settings,\nwith greater interest in AAE production in less formal settings. When LLMs were\nappropriately prompted and provided in context examples, our participants found\ntheir outputs to have a level of AAE authenticity on par with transcripts of\nBlack American speech. Select code and data for our project can be found here:\nhttps://github.com/smelliecat/AAEMime.git", "published": "2025-02-06 23:38:29", "link": "http://arxiv.org/abs/2502.04564v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparison of DeepSeek and Other LLMs", "abstract": "Recently, DeepSeek has been the focus of attention in and beyond the AI\ncommunity. An interesting problem is how DeepSeek compares to other large\nlanguage models (LLMs). There are many tasks an LLM can do, and in this paper,\nwe use the task of predicting an outcome using a short text for comparison. We\nconsider two settings, an authorship classification setting and a citation\nclassification setting. In the first one, the goal is to determine whether a\nshort text is written by human or AI. In the second one, the goal is to\nclassify a citation to one of four types using the textual content. For each\nexperiment, we compare DeepSeek with $4$ popular LLMs: Claude, Gemini, GPT, and\nLlama.\n  We find that, in terms of classification accuracy, DeepSeek outperforms\nGemini, GPT, and Llama in most cases, but underperforms Claude. We also find\nthat DeepSeek is comparably slower than others but with a low cost to use,\nwhile Claude is much more expensive than all the others. Finally, we find that\nin terms of similarity, the output of DeepSeek is most similar to those of\nGemini and Claude (and among all $5$ LLMs, Claude and Gemini have the most\nsimilar outputs).\n  In this paper, we also present a fully-labeled dataset collected by\nourselves, and propose a recipe where we can use the LLMs and a recent data\nset, MADStat, to generate new data sets. The datasets in our paper can be used\nas benchmarks for future study on LLMs.", "published": "2025-02-06 00:38:25", "link": "http://arxiv.org/abs/2502.03688v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adaptive Semantic Prompt Caching with VectorQ", "abstract": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%.", "published": "2025-02-06 04:16:20", "link": "http://arxiv.org/abs/2502.03771v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "It's All in The [MASK]: Simple Instruction-Tuning Enables BERT-like\n  Masked Language Models As Generative Classifiers", "abstract": "While encoder-only models such as BERT and ModernBERT are ubiquitous in\nreal-world NLP applications, their conventional reliance on task-specific\nclassification heads can limit their applicability compared to decoder-based\nlarge language models (LLMs). In this work, we introduce\nModernBERT-Large-Instruct, a 0.4B-parameter encoder model that leverages its\nmasked language modelling (MLM) head for generative classification. Our\napproach employs an intentionally simple training loop and inference mechanism\nthat requires no heavy pre-processing, heavily engineered prompting, or\narchitectural modifications. ModernBERT-Large-Instruct exhibits strong\nzero-shot performance on both classification and knowledge-based tasks,\noutperforming similarly sized LLMs on MMLU and achieving 93% of Llama3-1B's\nMMLU performance with 60% less parameters. We also demonstrate that, when\nfine-tuned, the generative approach using the MLM head matches or even\nsurpasses traditional classification-head methods across diverse NLU tasks.This\ncapability emerges specifically in models trained on contemporary, diverse data\nmixes, with models trained on lower volume, less-diverse data yielding\nconsiderably weaker performance. Although preliminary, these results\ndemonstrate the potential of using the original generative masked language\nmodelling head over traditional task-specific heads for downstream tasks. Our\nwork suggests that further exploration into this area is warranted,\nhighlighting many avenues for future improvements.", "published": "2025-02-06 05:47:37", "link": "http://arxiv.org/abs/2502.03793v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Syntriever: How to Train Your Retriever with Synthetic Data from LLMs", "abstract": "LLMs have boosted progress in many AI applications. Recently, there were\nattempts to distill the vast knowledge of LLMs into information retrieval\nsystems. Those distillation methods mostly use output probabilities of LLMs\nwhich are unavailable in the latest black-box LLMs. We propose Syntriever, a\ntraining framework for retrievers using synthetic data from black-box LLMs.\nSyntriever consists of two stages. Firstly in the distillation stage, we\nsynthesize relevant and plausibly irrelevant passages and augmented queries\nusing chain-of-thoughts for the given queries. LLM is asked to self-verify the\nsynthetic data for possible hallucinations, after which retrievers are trained\nwith a loss designed to cluster the embeddings of relevant passages. Secondly\nin the alignment stage, we align the retriever with the preferences of LLMs. We\npropose a preference modeling called partial Plackett-Luce ranking to learn LLM\npreferences with regularization which prevents the model from deviating\nexcessively from that trained in the distillation stage. Experiments show that\nSyntriever achieves state-of-the-art performances on benchmark datasets from\nvarious domains in nDCG@$K$. The code is available at\n\\href{https://github.com/kmswin1/Syntriever}{https://github.com/kmswin1/Syntriever}.", "published": "2025-02-06 07:19:59", "link": "http://arxiv.org/abs/2502.03824v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A comprehensive survey of contemporary Arabic sentiment analysis:\n  Methods, Challenges, and Future Directions", "abstract": "Sentiment Analysis, a popular subtask of Natural Language Processing, employs\ncomputational methods to extract sentiment, opinions, and other subjective\naspects from linguistic data. Given its crucial role in understanding human\nsentiment, research in sentiment analysis has witnessed significant growth in\nthe recent years. However, the majority of approaches are aimed at the English\nlanguage, and research towards Arabic sentiment analysis remains relatively\nunexplored. This paper presents a comprehensive and contemporary survey of\nArabic Sentiment Analysis, identifies the challenges and limitations of\nexisting literature in this field and presents avenues for future research. We\npresent a systematic review of Arabic sentiment analysis methods, focusing\nspecifically on research utilizing deep learning. We then situate Arabic\nSentiment Analysis within the broader context, highlighting research gaps in\nArabic sentiment analysis as compared to general sentiment analysis. Finally,\nwe outline the main challenges and promising future directions for research in\nArabic sentiment analysis.", "published": "2025-02-06 07:23:51", "link": "http://arxiv.org/abs/2502.03827v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Natural Language Understanding for LLMs via Large-Scale\n  Instruction Synthesis", "abstract": "High-quality, large-scale instructions are crucial for aligning large\nlanguage models (LLMs), however, there is a severe shortage of instruction in\nthe field of natural language understanding (NLU). Previous works on\nconstructing NLU instructions mainly focus on information extraction (IE),\nneglecting tasks such as machine reading comprehension, question answering, and\ntext classification. Furthermore, the lack of diversity in the data has led to\na decreased generalization ability of trained LLMs in other NLU tasks and a\nnoticeable decline in the fundamental model's general capabilities. To address\nthis issue, we propose Hum, a large-scale, high-quality synthetic instruction\ncorpus for NLU tasks, designed to enhance the NLU capabilities of LLMs.\nSpecifically, Hum includes IE (either close IE or open IE), machine reading\ncomprehension, text classification, and instruction generalist tasks, thereby\nenriching task diversity. Additionally, we introduce a human-LLMs collaborative\nmechanism to synthesize instructions, which enriches instruction diversity by\nincorporating guidelines, preference rules, and format variants. We conduct\nextensive experiments on 5 NLU tasks and 28 general capability evaluation\ndatasets for LLMs. Experimental results show that Hum enhances the NLU\ncapabilities of six LLMs by an average of 3.1\\%, with no significant decline\nobserved in other general capabilities.", "published": "2025-02-06 07:53:40", "link": "http://arxiv.org/abs/2502.03843v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Experiments with Large Language Models on Retrieval-Augmented Generation\n  for Closed-Source Simulation Software", "abstract": "Large Language Models (LLMs) are increasingly helpful in text generation,\neven writing code in programming languages based on user prompts written in\nnatural language. They are even applied to generate simulation models for\nmultibody systems from natural language. Research results suggest that LLMs\nsurpass the mere replication of existing code examples, where some LLMs have\nbeen trained on an open-source multibody simulation code. However, for\nclosed-source simulation software, such results are not to be expected as their\nideas and concepts might differ from other publicly available ones. LLMs can\nhallucinate for knowledge-intensive tasks, such as model creation, which can\nlead to wrong responses. This is especially the case for the LLM unknown\nclosed-source simulation software. The same applies to other internal knowledge\nkept private to protect intellectual property or data privacy. The\nRetrieval-Augmented Generation (RAG) approach might yield a solution for these\nknowledge-intensive tasks. This paper explores the application of RAG to\nclosed-source simulation software and presents first experiments. After a brief\nintroduction to LLMs, the RAG approach, and the simulation method applied by\nthe close-source simulation software, several examples are provided to test\nLLMs' knowledge of the simulation software and the creation of simulation\nmodels using two RAG systems. The examples show promising results indicating\nthe benefits of applying RAG systems to closed-source simulation software,\nhelping to access their knowledge. Nevertheless, they also reveal gaps in the\napplied information and open questions for further research.", "published": "2025-02-06 09:48:04", "link": "http://arxiv.org/abs/2502.03916v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MAQInstruct: Instruction-based Unified Event Relation Extraction", "abstract": "Extracting event relations that deviate from known schemas has proven\nchallenging for previous methods based on multi-class classification, MASK\nprediction, or prototype matching. Recent advancements in large language models\nhave shown impressive performance through instruction tuning. Nevertheless, in\nthe task of event relation extraction, instruction-based methods face several\nchallenges: there are a vast number of inference samples, and the relations\nbetween events are non-sequential. To tackle these challenges, we present an\nimproved instruction-based event relation extraction framework named\nMAQInstruct. Firstly, we transform the task from extracting event relations\nusing given event-event instructions to selecting events using given\nevent-relation instructions, which reduces the number of samples required for\ninference. Then, by incorporating a bipartite matching loss, we reduce the\ndependency of the instruction-based method on the generation sequence. Our\nexperimental results demonstrate that MAQInstruct significantly improves the\nperformance of event relation extraction across multiple LLMs.", "published": "2025-02-06 10:46:19", "link": "http://arxiv.org/abs/2502.03954v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PGB: One-Shot Pruning for BERT via Weight Grouping and Permutation", "abstract": "Large pretrained language models such as BERT suffer from slow inference and\nhigh memory usage, due to their huge size. Recent approaches to compressing\nBERT rely on iterative pruning and knowledge distillation, which, however, are\noften too complicated and computationally intensive. This paper proposes a\nnovel semi-structured one-shot pruning method for BERT, called\n$\\textit{Permutation and Grouping for BERT}$ (PGB), which achieves high\ncompression efficiency and sparsity while preserving accuracy. To this end, PGB\nidentifies important groups of individual weights by permutation and prunes all\nother weights as a structure in both multi-head attention and feed-forward\nlayers. Furthermore, if no important group is formed in a particular layer, PGB\ndrops the entire layer to produce an even more compact model. Our experimental\nresults on BERT$_{\\text{BASE}}$ demonstrate that PGB outperforms the\nstate-of-the-art structured pruning methods in terms of computational cost and\naccuracy preservation.", "published": "2025-02-06 11:34:41", "link": "http://arxiv.org/abs/2502.03984v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ontology-Guided, Hybrid Prompt Learning for Generalization in Knowledge\n  Graph Question Answering", "abstract": "Most existing Knowledge Graph Question Answering (KGQA) approaches are\ndesigned for a specific KG, such as Wikidata, DBpedia or Freebase. Due to the\nheterogeneity of the underlying graph schema, topology and assertions, most\nKGQA systems cannot be transferred to unseen Knowledge Graphs (KGs) without\nresource-intensive training data. We present OntoSCPrompt, a novel Large\nLanguage Model (LLM)-based KGQA approach with a two-stage architecture that\nseparates semantic parsing from KG-dependent interactions. OntoSCPrompt first\ngenerates a SPARQL query structure (including SPARQL keywords such as SELECT,\nASK, WHERE and placeholders for missing tokens) and then fills them with\nKG-specific information. To enhance the understanding of the underlying KG, we\npresent an ontology-guided, hybrid prompt learning strategy that integrates KG\nontology into the learning process of hybrid prompts (e.g., discrete and\ncontinuous vectors). We also present several task-specific decoding strategies\nto ensure the correctness and executability of generated SPARQL queries in both\nstages. Experimental results demonstrate that OntoSCPrompt performs as well as\nSOTA approaches without retraining on a number of KGQA datasets such as CWQ,\nWebQSP and LC-QuAD 1.0 in a resource-efficient manner and can generalize well\nto unseen domain-specific KGs like DBLP-QuAD and CoyPu KG Code:\n\\href{https://github.com/LongquanJiang/OntoSCPrompt}{https://github.com/LongquanJiang/OntoSCPrompt}", "published": "2025-02-06 11:47:58", "link": "http://arxiv.org/abs/2502.03992v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Imbalanced Annotations for Effective In-Context Learning", "abstract": "Large language models (LLMs) have shown impressive performance on downstream\ntasks through in-context learning (ICL), which heavily relies on the\ndemonstrations selected from annotated datasets. Existing selection methods may\nhinge on the distribution of annotated datasets, which can often be long-tailed\nin real-world scenarios. In this work, we show that imbalanced class\ndistributions in annotated datasets significantly degrade the performance of\nICL across various tasks and selection methods. Moreover, traditional rebalance\nmethods fail to ameliorate the issue of class imbalance in ICL. Our method is\nmotivated by decomposing the distributional differences between annotated and\ntest datasets into two-component weights: class-wise weights and conditional\nbias. The key idea behind our method is to estimate the conditional bias by\nminimizing the empirical error on a balanced validation dataset and to employ\nthe two-component weights to modify the original scoring functions during\nselection. Our approach can prevent selecting too many demonstrations from a\nsingle class while preserving the effectiveness of the original selection\nmethods. Extensive experiments demonstrate the effectiveness of our method,\nimproving the average accuracy by up to 5.46 on common benchmarks with\nimbalanced datasets.", "published": "2025-02-06 12:57:50", "link": "http://arxiv.org/abs/2502.04037v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predicting Large Language Model Capabilities on Closed-Book QA Tasks\n  Using Only Information Available Prior to Training", "abstract": "The GPT-4 technical report from OpenAI suggests that model performance on\nspecific tasks can be predicted prior to training, though methodologies remain\nunspecified. This approach is crucial for optimizing resource allocation and\nensuring data alignment with target tasks. To achieve this vision, we focus on\npredicting performance on Closed-book Question Answering (CBQA) tasks, which\nare closely tied to pre-training data and knowledge retention. We address three\nmajor challenges: 1) mastering the entire pre-training process, especially data\nconstruction; 2) evaluating a model's knowledge retention; and 3) predicting\ntask-specific knowledge retention using only information available prior to\ntraining. To tackle these challenges, we pre-train three large language models\n(i.e., 1.6B, 7B, and 13B) using 560k dollars and 520k GPU hours. We analyze the\npre-training data with knowledge triples and assess knowledge retention using\nestablished methods. Additionally, we introduce the SMI metric, an\ninformation-theoretic measure that quantifies the relationship between\npre-training data, model size, and task-specific knowledge retention. Our\nexperiments reveal a strong linear correlation ($\\text{R}^2 > 0.84$) between\nthe SMI metric and the model's accuracy on CBQA tasks across models of varying\nsizes (i.e., 1.1B, 1.6B, 7B, and 13B). The dataset, model, and code are\navailable at https://github.com/yuhui1038/SMI.", "published": "2025-02-06 13:23:53", "link": "http://arxiv.org/abs/2502.04066v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference", "abstract": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.", "published": "2025-02-06 13:41:46", "link": "http://arxiv.org/abs/2502.04077v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLMs to Support a Domain Specific Knowledge Assistant", "abstract": "This work presents a custom approach to developing a domain specific\nknowledge assistant for sustainability reporting using the International\nFinancial Reporting Standards (IFRS). In this domain, there is no publicly\navailable question-answer dataset, which has impeded the development of a\nhigh-quality chatbot to support companies with IFRS reporting. The two key\ncontributions of this project therefore are:\n  (1) A high-quality synthetic question-answer (QA) dataset based on IFRS\nsustainability standards, created using a novel generation and evaluation\npipeline leveraging Large Language Models (LLMs). This comprises 1,063 diverse\nQA pairs that address a wide spectrum of potential user queries in\nsustainability reporting. Various LLM-based techniques are employed to create\nthe dataset, including chain-of-thought reasoning and few-shot prompting. A\ncustom evaluation framework is developed to assess question and answer quality\nacross multiple dimensions, including faithfulness, relevance, and domain\nspecificity. The dataset averages a score range of 8.16 out of 10 on these\nmetrics.\n  (2) Two architectures for question-answering in the sustainability reporting\ndomain - a RAG pipeline and a fully LLM-based pipeline. The architectures are\ndeveloped by experimenting, fine-tuning, and training on the QA dataset. The\nfinal pipelines feature an LLM fine-tuned on domain specific data and an\nindustry classification component to improve the handling of complex queries.\nThe RAG architecture achieves an accuracy of 85.32% on single-industry and\n72.15% on cross-industry multiple-choice questions, outperforming the baseline\napproach by 4.67 and 19.21 percentage points, respectively. The LLM-based\npipeline achieves an accuracy of 93.45% on single-industry and 80.30% on\ncross-industry multiple-choice questions, an improvement of 12.80 and 27.36\npercentage points over the baseline, respectively.", "published": "2025-02-06 14:12:41", "link": "http://arxiv.org/abs/2502.04095v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UltraIF: Advancing Instruction Following from the Wild", "abstract": "Instruction-following made modern large language models (LLMs) helpful\nassistants. However, the key to taming LLMs on complex instructions remains\nmysterious, for that there are huge gaps between models trained by open-source\ncommunity and those trained by leading companies. To bridge the gap, we propose\na simple and scalable approach UltraIF for building LLMs that can follow\ncomplex instructions with open-source data. UltraIF first decomposes real-world\nuser prompts into simpler queries, constraints, and corresponding evaluation\nquestions for the constraints. Then, we train an UltraComposer to compose\nconstraint-associated prompts with evaluation questions. This prompt composer\nallows us to synthesize complicated instructions as well as filter responses\nwith evaluation questions. In our experiment, for the first time, we\nsuccessfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5\ninstruction-following benchmarks without any benchmark information, using only\n8B model as response generator and evaluator. The aligned model also achieved\ncompetitive scores on other benchmarks. Moreover, we also show that UltraIF\ncould further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating\nbroader use cases for the method. Our code will be available at\nhttps://github.com/kkk-an/UltraIF.", "published": "2025-02-06 15:39:16", "link": "http://arxiv.org/abs/2502.04153v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How does a Multilingual LM Handle Multiple Languages?", "abstract": "Multilingual language models have significantly advanced due to rapid\nprogress in natural language processing. Models like BLOOM 1.7B, trained on\ndiverse multilingual datasets, aim to bridge linguistic gaps. However, their\neffectiveness in capturing linguistic knowledge, particularly for low-resource\nlanguages, remains an open question. This study critically examines MLMs\ncapabilities in multilingual understanding, semantic representation, and\ncross-lingual knowledge transfer. While these models perform well for\nhigh-resource languages, they struggle with less-represented ones.\nAdditionally, traditional evaluation methods often overlook their internal\nsyntactic and semantic encoding.\n  This research addresses key limitations through three objectives. First, it\nassesses semantic similarity by analyzing multilingual word embeddings for\nconsistency using cosine similarity. Second, it examines BLOOM-1.7B and Qwen2\nthrough Named Entity Recognition and sentence similarity tasks to understand\ntheir linguistic structures. Third, it explores cross-lingual knowledge\ntransfer by evaluating generalization from high-resource to low-resource\nlanguages in sentiment analysis and text classification.\n  By leveraging linguistic probing, performance metrics, and visualizations,\nthis study provides insights into the strengths and limitations of MLMs. The\nfindings aim to enhance multilingual NLP models, ensuring better support for\nboth high- and low-resource languages, thereby promoting inclusivity in\nlanguage technologies.", "published": "2025-02-06 18:08:14", "link": "http://arxiv.org/abs/2502.04269v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Grammarly and ChatGPT accelerate language change? AI-powered\n  technologies and their impact on the English language: wordiness vs.\n  conciseness", "abstract": "The proliferation of NLP-powered language technologies, AI-based natural\nlanguage generation models, and English as a mainstream means of communication\namong both native and non-native speakers make the output of AI-powered tools\nespecially intriguing to linguists. This paper investigates how Grammarly and\nChatGPT affect the English language regarding wordiness vs. conciseness. A case\nstudy focusing on the purpose subordinator in order to is presented to\nillustrate the way in which Grammarly and ChatGPT recommend shorter grammatical\nstructures instead of longer and more elaborate ones. Although the analysed\nsentences were produced by native speakers, are perfectly correct, and were\nextracted from a language corpus of contemporary English, both Grammarly and\nChatGPT suggest more conciseness and less verbosity, even for relatively short\nsentences. The present article argues that technologies such as Grammarly not\nonly mirror language change but also have the potential to facilitate or\naccelerate it.", "published": "2025-02-06 18:59:26", "link": "http://arxiv.org/abs/2502.04324v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Division-of-Thoughts: Harnessing Hybrid Language Model Synergy for\n  Efficient On-Device Agents", "abstract": "The rapid expansion of web content has made on-device AI assistants\nindispensable for helping users manage the increasing complexity of online\ntasks. The emergent reasoning ability in large language models offer a\npromising path for next-generation on-device AI agents. However, deploying\nfull-scale Large Language Models (LLMs) on resource-limited local devices is\nchallenging. In this paper, we propose Division-of-Thoughts (DoT), a\ncollaborative reasoning framework leveraging the synergy between locally\ndeployed Smaller-scale Language Models (SLMs) and cloud-based LLMs. DoT\nleverages a Task Decomposer to elicit the inherent planning abilities in\nlanguage models to decompose user queries into smaller sub-tasks, which allows\nhybrid language models to fully exploit their respective strengths. Besides,\nDoT employs a Task Scheduler to analyze the pair-wise dependency of sub-tasks\nand create a dependency graph, facilitating parallel reasoning of sub-tasks and\nthe identification of key steps. To allocate the appropriate model based on the\ndifficulty of sub-tasks, DoT leverages a Plug-and-Play Adapter, which is an\nadditional task head attached to the SLM that does not alter the SLM's\nparameters. To boost adapter's task allocation capability, we propose a\nself-reinforced training method that relies solely on task execution feedback.\nExtensive experiments on various benchmarks demonstrate that our DoT\nsignificantly reduces LLM costs while maintaining competitive reasoning\naccuracy. Specifically, DoT reduces the average reasoning time and API costs by\n66.12% and 83.57%, while achieving comparable reasoning accuracy with the best\nbaseline methods.", "published": "2025-02-06 02:40:25", "link": "http://arxiv.org/abs/2502.04392v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DECT: Harnessing LLM-assisted Fine-Grained Linguistic Knowledge and\n  Label-Switched and Label-Preserved Data Generation for Diagnosis of\n  Alzheimer's Disease", "abstract": "Alzheimer's Disease (AD) is an irreversible neurodegenerative disease\naffecting 50 million people worldwide. Low-cost, accurate identification of key\nmarkers of AD is crucial for timely diagnosis and intervention. Language\nimpairment is one of the earliest signs of cognitive decline, which can be used\nto discriminate AD patients from normal control individuals.\nPatient-interviewer dialogues may be used to detect such impairments, but they\nare often mixed with ambiguous, noisy, and irrelevant information, making the\nAD detection task difficult. Moreover, the limited availability of AD speech\nsamples and variability in their speech styles pose significant challenges in\ndeveloping robust speech-based AD detection models. To address these\nchallenges, we propose DECT, a novel speech-based domain-specific approach\nleveraging large language models (LLMs) for fine-grained linguistic analysis\nand label-switched label-preserved data generation. Our study presents four\nnovelties: We harness the summarizing capabilities of LLMs to identify and\ndistill key Cognitive-Linguistic information from noisy speech transcripts,\neffectively filtering irrelevant information. We leverage the inherent\nlinguistic knowledge of LLMs to extract linguistic markers from unstructured\nand heterogeneous audio transcripts. We exploit the compositional ability of\nLLMs to generate AD speech transcripts consisting of diverse linguistic\npatterns to overcome the speech data scarcity challenge and enhance the\nrobustness of AD detection models. We use the augmented AD textual speech\ntranscript dataset and a more fine-grained representation of AD textual speech\ntranscript data to fine-tune the AD detection model. The results have shown\nthat DECT demonstrates superior model performance with an 11% improvement in AD\ndetection accuracy on the datasets from DementiaBank compared to the baselines.", "published": "2025-02-06 04:00:25", "link": "http://arxiv.org/abs/2502.04394v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of\n  Language Models", "abstract": "The integration of slow-thinking mechanisms into large language models (LLMs)\noffers a promising way toward achieving Level 2 AGI Reasoners, as exemplified\nby systems like OpenAI's o1. However, several significant challenges remain,\nincluding inefficient overthinking and an overreliance on auxiliary reward\nmodels. We point out that these limitations stem from LLMs' inability to\ninternalize the search process, a key component of effective reasoning. A\ncritical step toward addressing this issue is enabling LLMs to autonomously\ndetermine when and where to backtrack, a fundamental operation in traditional\nsearch algorithms. To this end, we propose a self-backtracking mechanism that\nequips LLMs with the ability to backtrack during both training and inference.\nThis mechanism not only enhances reasoning ability but also efficiency by\ntransforming slow-thinking processes into fast-thinking through\nself-improvement. Empirical evaluations demonstrate that our proposal\nsignificantly enhances the reasoning capabilities of LLMs, achieving a\nperformance gain of over 40 percent compared to the optimal-path supervised\nfine-tuning method. We believe this study introduces a novel and promising\npathway for developing more advanced and robust Reasoners.", "published": "2025-02-06 08:52:43", "link": "http://arxiv.org/abs/2502.04404v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large\n  Language Models", "abstract": "With the integration of Multimodal large language models (MLLMs) into robotic\nsystems and various AI applications, embedding emotional intelligence (EI)\ncapabilities into these models is essential for enabling robots to effectively\naddress human emotional needs and interact seamlessly in real-world scenarios.\nExisting static, text-based, or text-image benchmarks overlook the multimodal\ncomplexities of real-world interactions and fail to capture the dynamic,\nmultimodal nature of emotional expressions, making them inadequate for\nevaluating MLLMs' EI. Based on established psychological theories of EI, we\nbuild EmoBench-M, a novel benchmark designed to evaluate the EI capability of\nMLLMs across 13 valuation scenarios from three key dimensions: foundational\nemotion recognition, conversational emotion understanding, and socially complex\nemotion analysis. Evaluations of both open-source and closed-source MLLMs on\nEmoBench-M reveal a significant performance gap between them and humans,\nhighlighting the need to further advance their EI capabilities. All benchmark\nresources, including code and datasets, are publicly available at\nhttps://emo-gml.github.io/.", "published": "2025-02-06 18:13:35", "link": "http://arxiv.org/abs/2502.04424v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "\"In order that\" -- a data driven study of symptoms and causes of\n  obsolescence", "abstract": "The paper is an empirical case study of grammatical obsolescence in progress.\nThe main studied variable is the purpose subordinator in order that, which is\nshown to be steadily decreasing in the frequency of use starting from the\nbeginning of the twentieth century. This work applies a data-driven approach\nfor the investigation and description of obsolescence, recently developed by\nthe Rudnicka (2019). The methodology combines philological analysis with\nstatistical methods used on data acquired from mega-corpora. Moving from the\ndescription of possible symptoms of obsolescence to different causes for it,\nthe paper aims at presenting a comprehensive account of the studied phenomenon.\nInterestingly, a very significant role in the decline of in order that can be\nascribed to the so-called higher-order processes, understood as processes\ninfluencing the constructional level from above. Two kinds of higher-order\nprocesses are shown to play an important role, namely i) an\nexternally-motivated higher-order process exemplified by the drastic\nsocio-cultural changes of the 19th and 20th centuries; ii) an\ninternally-motivated higher-order processes instantiated by the rise of the\nto-infinitive (rise of infinite clauses).", "published": "2025-02-06 19:03:45", "link": "http://arxiv.org/abs/2502.04457v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Training Language Models to Reason Efficiently", "abstract": "Scaling model size and training data has led to great advances in the\nperformance of Large Language Models (LLMs). However, the diminishing returns\nof this approach necessitate alternative methods to improve model capabilities,\nparticularly in tasks requiring advanced reasoning. Large reasoning models,\nwhich leverage long chain-of-thoughts, bring unprecedented breakthroughs in\nproblem-solving capabilities but at a substantial deployment cost associated to\nlonger generations. Reducing inference costs is crucial for the economic\nfeasibility, user experience, and environmental sustainability of these models.\n  In this work, we propose to train large reasoning models to reason\nefficiently. More precisely, we use reinforcement learning (RL) to train\nreasoning models to dynamically allocate inference-time compute based on task\ncomplexity. Our method incentivizes models to minimize unnecessary\ncomputational overhead while maintaining accuracy, thereby achieving\nsubstantial efficiency gains. It enables the derivation of a family of\nreasoning models with varying efficiency levels, controlled via a single\nhyperparameter. Experiments on two open-weight large reasoning models\ndemonstrate significant reductions in inference cost while preserving most of\nthe accuracy.", "published": "2025-02-06 19:18:16", "link": "http://arxiv.org/abs/2502.04463v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Building A Unified AI-centric Language System: analysis, framework and\n  future work", "abstract": "Recent advancements in large language models have demonstrated that extended\ninference through techniques can markedly improve performance, yet these gains\ncome with increased computational costs and the propagation of inherent biases\nfound in natural languages. This paper explores the design of a unified\nAI-centric language system that addresses these challenges by offering a more\nconcise, unambiguous, and computationally efficient alternative to traditional\nhuman languages. We analyze the limitations of natural language such as gender\nbias, morphological irregularities, and contextual ambiguities and examine how\nthese issues are exacerbated within current Transformer architectures, where\nredundant attention heads and token inefficiencies prevail. Drawing on insights\nfrom emergent artificial communication systems and constructed languages like\nEsperanto and Lojban, we propose a framework that translates diverse natural\nlanguage inputs into a streamlined AI-friendly language, enabling more\nefficient model training and inference while reducing memory footprints.\nFinally, we outline a pathway for empirical validation through controlled\nexperiments, paving the way for a universal interchange format that could\nrevolutionize AI-to-AI and human-to-AI interactions by enhancing clarity,\nfairness, and overall performance.", "published": "2025-02-06 20:32:57", "link": "http://arxiv.org/abs/2502.04488v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Cost-Effective Reward Guided Text Generation", "abstract": "Reward-guided text generation (RGTG) has emerged as a viable alternative to\noffline reinforcement learning from human feedback (RLHF). RGTG methods can\nalign baseline language models to human preferences without further training\nlike in standard RLHF methods. However, they rely on a reward model to score\neach candidate token generated by the language model at inference, incurring\nsignificant test-time overhead. Additionally, the reward model is usually only\ntrained to score full sequences, which can lead to sub-optimal choices for\npartial sequences. In this work, we present a novel reward model architecture\nthat is trained, using a Bradley-Terry loss, to prefer the optimal expansion of\na sequence with just a \\emph{single call} to the reward model at each step of\nthe generation process. That is, a score for all possible candidate tokens is\ngenerated simultaneously, leading to efficient inference. We theoretically\nanalyze various RGTG reward models and demonstrate that prior techniques prefer\nsub-optimal sequences compared to our method during inference. Empirically, our\nreward model leads to significantly faster inference than other RGTG methods.\nIt requires fewer calls to the reward model and performs competitively compared\nto previous RGTG and offline RLHF methods.", "published": "2025-02-06 21:36:44", "link": "http://arxiv.org/abs/2502.04517v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Group-Adaptive Threshold Optimization for Robust AI-Generated Text\n  Detection", "abstract": "The advancement of large language models (LLMs) has made it difficult to\ndifferentiate human-written text from AI-generated text. Several AI-text\ndetectors have been developed in response, which typically utilize a fixed\nglobal threshold (e.g., {\\theta} = 0.5) to classify machine-generated text.\nHowever, we find that one universal threshold can fail to account for\nsubgroup-specific distributional variations. For example, when using a fixed\nthreshold, detectors make more false positive errors on shorter human-written\ntext than longer, and more positive classifications on neurotic writing styles\nthan open among long text. These discrepancies can lead to misclassification\nthat disproportionately affects certain groups. We address this critical\nlimitation by introducing FairOPT, an algorithm for group-specific threshold\noptimization in AI-generated content classifiers. Our approach partitions data\ninto subgroups based on attributes (e.g., text length and writing style) and\nlearns decision thresholds for each group, which enables careful balancing of\nperformance and fairness metrics within each subgroup. In experiments with four\nAI text classifiers on three datasets, FairOPT enhances overall F1 score and\ndecreases balanced error rate (BER) discrepancy across subgroups. Our framework\npaves the way for more robust and fair classification criteria in AI-generated\noutput detection.", "published": "2025-02-06 21:58:48", "link": "http://arxiv.org/abs/2502.04528v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Controlled LLM Decoding via Discrete Auto-regressive Biasing", "abstract": "Controlled text generation allows for enforcing user-defined constraints on\nlarge language model outputs, an increasingly important field as LLMs become\nmore prevalent in everyday life. One common approach uses energy-based\ndecoding, which defines a target distribution through an energy function that\ncombines multiple constraints into a weighted average. However, these methods\noften struggle to balance fluency with constraint satisfaction, even with\nextensive tuning of the energy function's coefficients. In this paper, we\nidentify that this suboptimal balance arises from sampling in continuous space\nrather than the natural discrete space of text tokens. To address this, we\npropose Discrete Auto-regressive Biasing, a controlled decoding algorithm that\nleverages gradients while operating entirely in the discrete text domain.\nSpecifically, we introduce a new formulation for controlled text generation by\ndefining a joint distribution over the generated sequence and an auxiliary bias\nsequence. To efficiently sample from this joint distribution, we propose a\nLangevin-within-Gibbs sampling algorithm using gradient-based discrete MCMC.\nOur method significantly improves constraint satisfaction while maintaining\ncomparable or better fluency, all with even lower computational costs. We\ndemonstrate the advantages of our controlled decoding method on sentiment\ncontrol, language detoxification, and keyword-guided generation.", "published": "2025-02-06 00:14:43", "link": "http://arxiv.org/abs/2502.03685v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "DocMIA: Document-Level Membership Inference Attacks against DocVQA\n  Models", "abstract": "Document Visual Question Answering (DocVQA) has introduced a new paradigm for\nend-to-end document understanding, and quickly became one of the standard\nbenchmarks for multimodal LLMs. Automating document processing workflows,\ndriven by DocVQA models, presents significant potential for many business\nsectors. However, documents tend to contain highly sensitive information,\nraising concerns about privacy risks associated with training such DocVQA\nmodels. One significant privacy vulnerability, exploited by the membership\ninference attack, is the possibility for an adversary to determine if a\nparticular record was part of the model's training data. In this paper, we\nintroduce two novel membership inference attacks tailored specifically to\nDocVQA models. These attacks are designed for two different adversarial\nscenarios: a white-box setting, where the attacker has full access to the model\narchitecture and parameters, and a black-box setting, where only the model's\noutputs are available. Notably, our attacks assume the adversary lacks access\nto auxiliary datasets, which is more realistic in practice but also more\nchallenging. Our unsupervised methods outperform existing state-of-the-art\nmembership inference attacks across a variety of DocVQA models and datasets,\ndemonstrating their effectiveness and highlighting the privacy risks in this\ndomain.", "published": "2025-02-06 00:58:21", "link": "http://arxiv.org/abs/2502.03692v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "LLM Alignment as Retriever Optimization: An Information Retrieval\n  Perspective", "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence with\ncapabilities in reasoning, coding, and communication, driving innovation across\nindustries. Their true potential depends on effective alignment to ensure\ncorrect, trustworthy and ethical behavior, addressing challenges like\nmisinformation, hallucinations, bias and misuse. While existing Reinforcement\nLearning (RL)-based alignment methods are notoriously complex, direct\noptimization approaches offer a simpler alternative. In this work, we introduce\na novel direct optimization approach for LLM alignment by drawing on\nestablished Information Retrieval (IR) principles. We present a systematic\nframework that bridges LLM alignment and IR methodologies, mapping LLM\ngeneration and reward models to IR's retriever-reranker paradigm. Building on\nthis foundation, we propose LLM Alignment as Retriever Preference Optimization\n(LarPO), a new alignment method that enhances overall alignment quality.\nExtensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 %\naveraged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work\nopens new avenues for advancing LLM alignment by integrating IR foundations,\noffering a promising direction for future research.", "published": "2025-02-06 01:22:06", "link": "http://arxiv.org/abs/2502.03699v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Aggregate and conquer: detecting and steering LLM concepts by combining\n  nonlinear predictors over multiple layers", "abstract": "A trained Large Language Model (LLM) contains much of human knowledge. Yet,\nit is difficult to gauge the extent or accuracy of that knowledge, as LLMs do\nnot always ``know what they know'' and may even be actively misleading. In this\nwork, we give a general method for detecting semantic concepts in the internal\nactivations of LLMs. Furthermore, we show that our methodology can be easily\nadapted to steer LLMs toward desirable outputs. Our innovations are the\nfollowing: (1) we use a nonlinear feature learning method to identify important\nlinear directions for predicting concepts from each layer; (2) we aggregate\nfeatures across layers to build powerful concept detectors and steering\nmechanisms. We showcase the power of our approach by attaining state-of-the-art\nresults for detecting hallucinations, harmfulness, toxicity, and untruthful\ncontent on seven benchmarks. We highlight the generality of our approach by\nsteering LLMs towards new concepts that, to the best of our knowledge, have not\nbeen previously considered in the literature, including: semantic\ndisambiguation, human languages, programming languages, hallucinated responses,\nscience subjects, poetic/Shakespearean English, and even multiple concepts\nsimultaneously. Moreover, our method can steer concepts with numerical\nattributes such as product reviews. We provide our code (including a simple API\nfor our methods) at https://github.com/dmbeaglehole/neural_controllers .", "published": "2025-02-06 01:41:48", "link": "http://arxiv.org/abs/2502.03708v1", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "MultiQ&A: An Analysis in Measuring Robustness via Automated\n  Crowdsourcing of Question Perturbations and Answers", "abstract": "One critical challenge in the institutional adoption journey of Large\nLanguage Models (LLMs) stems from their propensity to hallucinate in generated\nresponses. To address this, we propose MultiQ&A, a systematic approach for\nevaluating the robustness and consistency of LLM-generated answers. We\ndemonstrate MultiQ&A's ability to crowdsource question perturbations and their\nrespective answers through independent LLM agents at scale. Our experiments\nculminated in the examination of 1.9 million question perturbations and 2.3\nmillion answers. Furthermore, MultiQ&A shows that ensembled LLMs, such as\ngpt-3.5-turbo, remain relatively robust and consistent under perturbations.\nMultiQ&A provides clarity in the response generation space, offering an\neffective method for inspecting disagreements and variability. Therefore, our\nsystem offers a potential framework for institutional LLM adoption with the\nability to measure confidence, consistency, and the quantification of\nhallucinations.", "published": "2025-02-06 01:58:48", "link": "http://arxiv.org/abs/2502.03711v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Hallucination Detection through Noise Injection", "abstract": "Large Language Models (LLMs) are prone to generating plausible yet incorrect\nresponses, known as hallucinations. Effectively detecting hallucinations is\ntherefore crucial for the safe deployment of LLMs. Recent research has linked\nhallucinations to model uncertainty, suggesting that hallucinations can be\ndetected by measuring dispersion over answer distributions obtained from a set\nof samples drawn from a model. While drawing from the distribution over tokens\ndefined by the model is a natural way to obtain samples, in this work, we argue\nthat it is sub-optimal for the purpose of detecting hallucinations. We show\nthat detection can be improved significantly by taking into account model\nuncertainty in the Bayesian sense. To this end, we propose a very simple and\nefficient approach that perturbs an appropriate subset of model parameters, or\nequivalently hidden unit activations, during sampling. We demonstrate its\neffectiveness across a wide range of datasets and model architectures.", "published": "2025-02-06 06:02:20", "link": "http://arxiv.org/abs/2502.03799v2", "categories": ["cs.CL", "cs.SY", "eess.SY"], "primary_category": "cs.CL"}
{"title": "Enhancing Online Learning Efficiency Through Heterogeneous Resource\n  Integration with a Multi-Agent RAG System", "abstract": "Efficient online learning requires seamless access to diverse resources such\nas videos, code repositories, documentation, and general web content. This\nposter paper introduces early-stage work on a Multi-Agent Retrieval-Augmented\nGeneration (RAG) System designed to enhance learning efficiency by integrating\nthese heterogeneous resources. Using specialized agents tailored for specific\nresource types (e.g., YouTube tutorials, GitHub repositories, documentation\nwebsites, and search engines), the system automates the retrieval and synthesis\nof relevant information. By streamlining the process of finding and combining\nknowledge, this approach reduces manual effort and enhances the learning\nexperience. A preliminary user study confirmed the system's strong usability\nand moderate-high utility, demonstrating its potential to improve the\nefficiency of knowledge acquisition.", "published": "2025-02-06 10:36:17", "link": "http://arxiv.org/abs/2502.03948v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for\n  Enhancing Safety Alignment", "abstract": "Training safe LLMs is one of the most critical research challenge. However,\nthe commonly used method, Refusal Training (RT), struggles to generalize\nagainst various OOD jailbreaking attacks. Many safety training methods have\nbeen proposed to address this issue. While they offer valuable insights, we aim\nto complement this line of research by investigating whether OOD attacks truly\nexceed the capability of RT model. Conducting evaluation with BoN, we observe\nsignificant improvements on generalization as N increases. This underscores\nthat the model possesses sufficient safety-related latent knowledge, but RT\nfails to consistently elicit this knowledge when addressing OOD attacks.\nFurther analysis based on domain adaptation reveals that training with direct\nrefusal causes model to rely on superficial shortcuts, resulting in learning of\nnon-robust representation mappings. Based on our findings, we propose training\nmodel to perform safety reasoning for each query. Reasoning supervision\nencourages model to perform more computations, explicitly eliciting and using\nlatent knowledge through reasoning. To achieve this, we synthesize reasoning\nsupervision based on pre-guidelines, training the model to reason in alignment\nwith them, thereby effectively eliciting and utilizing latent knowledge from\ndiverse perspectives. Extensive experiments show that our method significantly\nimproves generalization performance against OOD attacks.", "published": "2025-02-06 13:01:44", "link": "http://arxiv.org/abs/2502.04040v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multi-agent Architecture Search via Agentic Supernet", "abstract": "Large Language Model (LLM)-empowered multi-agent systems extend the cognitive\nboundaries of individual agents through disciplined collaboration and\ninteraction, while constructing these systems often requires labor-intensive\nmanual designs. Despite the availability of methods to automate the design of\nagentic workflows, they typically seek to identify a static, complex,\none-size-fits-all system, which, however, fails to dynamically allocate\ninference resources based on the difficulty and domain of each query. To\naddress this challenge, we shift away from the pursuit of a monolithic agentic\nsystem, instead optimizing the \\textbf{agentic supernet}, a probabilistic and\ncontinuous distribution of agentic architectures. We introduce MaAS, an\nautomated framework that samples query-dependent agentic systems from the\nsupernet, delivering high-quality solutions and tailored resource allocation\n(\\textit{e.g.}, LLM calls, tool calls, token cost). Comprehensive evaluation\nacross six benchmarks demonstrates that MaAS \\textbf{(I)} requires only\n$6\\sim45\\%$ of the inference costs of existing handcrafted or automated\nmulti-agent systems, \\textbf{(II)} surpasses them by $0.54\\%\\sim11.82\\%$, and\n\\textbf{(III)} enjoys superior cross-dataset and cross-LLM-backbone\ntransferability.", "published": "2025-02-06 16:12:06", "link": "http://arxiv.org/abs/2502.04180v1", "categories": ["cs.LG", "cs.CL", "cs.MA"], "primary_category": "cs.LG"}
{"title": "The Best Instruction-Tuning Data are Those That Fit", "abstract": "High-quality supervised fine-tuning (SFT) data are crucial for eliciting\nstrong capabilities from pretrained large language models (LLMs). Typically,\ninstructions are paired with multiple responses sampled from other LLMs, which\nare often out of the distribution of the target model to be fine-tuned. This,\nat scale, can lead to diminishing returns and even hurt the models' performance\nand robustness. We propose **GRAPE**, a novel SFT framework that accounts for\nthe unique characteristics of the target model. For each instruction, it\ngathers responses from various LLMs and selects the one with the highest\nprobability measured by the target model, indicating that it aligns most\nclosely with the target model's pretrained distribution; it then proceeds with\nstandard SFT training.\n  We first evaluate GRAPE with a controlled experiment, where we sample various\nsolutions for each question in UltraInteract from multiple models and fine-tune\ncommonly used LMs like LLaMA3.1-8B, Mistral-7B, and Qwen2.5-7B on\nGRAPE-selected data. GRAPE significantly outperforms strong baselines,\nincluding distilling from the strongest model with an absolute gain of up to\n13.8%, averaged across benchmarks, and training on 3x more data with a maximum\nperformance improvement of 17.3%. GRAPE's strong performance generalizes to\nrealistic settings. We experiment with the post-training data used for Tulu3\nand Olmo-2. GRAPE outperforms strong baselines trained on 4.5 times more data\nby 6.1% and a state-of-the-art data selection approach by 3% on average\nperformance. Remarkably, using 1/3 of the data and half the number of epochs,\nGRAPE enables LLaMA3.1-8B to surpass the performance of Tulu3-SFT by 3.5%.", "published": "2025-02-06 16:31:21", "link": "http://arxiv.org/abs/2502.04194v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Classification System Approach in Predicting Chinese Censorship", "abstract": "This paper is dedicated to using a classifier to predict whether a Weibo post\nwould be censored under the Chinese internet. Through randomized sampling from\n\\citeauthor{Fu2021} and Chinese tokenizing strategies, we constructed a cleaned\nChinese phrase dataset with binary censorship markings. Utilizing various\nprobability-based information retrieval methods on the data, we were able to\nderive 4 logistic regression models for classification. Furthermore, we\nexperimented with pre-trained transformers to perform similar classification\ntasks. After evaluating both the macro-F1 and ROC-AUC metrics, we concluded\nthat the Fined-Tuned BERT model exceeds other strategies in performance.", "published": "2025-02-06 17:19:14", "link": "http://arxiv.org/abs/2502.04234v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "TriNER: A Series of Named Entity Recognition Models For Hindi, Bengali &\n  Marathi", "abstract": "India's rich cultural and linguistic diversity poses various challenges in\nthe domain of Natural Language Processing (NLP), particularly in Named Entity\nRecognition (NER). NER is a NLP task that aims to identify and classify tokens\ninto different entity groups like Person, Location, Organization, Number, etc.\nThis makes NER very useful for downstream tasks like context-aware\nanonymization. This paper details our work to build a multilingual NER model\nfor the three most spoken languages in India - Hindi, Bengali & Marathi. We\ntrain a custom transformer model and fine tune a few pretrained models,\nachieving an F1 Score of 92.11 for a total of 6 entity groups. Through this\npaper, we aim to introduce a single model to perform NER and significantly\nreduce the inconsistencies in entity groups and tag names, across the three\nlanguages.", "published": "2025-02-06 17:37:36", "link": "http://arxiv.org/abs/2502.04245v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Great Models Think Alike and this Undermines AI Oversight", "abstract": "As Language Model (LM) capabilities advance, evaluating and supervising them\nat scale is getting harder for humans. There is hope that other language models\ncan automate both these tasks, which we refer to as \"AI Oversight\". We study\nhow model similarity affects both aspects of AI oversight by proposing a\nprobabilistic metric for LM similarity based on overlap in model mistakes.\nUsing this metric, we first show that LLM-as-a-judge scores favor models\nsimilar to the judge, generalizing recent self-preference results. Then, we\nstudy training on LM annotations, and find complementary knowledge between the\nweak supervisor and strong student model plays a crucial role in gains from\n\"weak-to-strong generalization\". As model capabilities increase, it becomes\nharder to find their mistakes, and we might defer more to AI oversight.\nHowever, we observe a concerning trend -- model mistakes are becoming more\nsimilar with increasing capabilities, pointing to risks from correlated\nfailures. Our work underscores the importance of reporting and correcting for\nmodel similarity, especially in the emerging paradigm of AI oversight.", "published": "2025-02-06 18:56:01", "link": "http://arxiv.org/abs/2502.04313v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ChameleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time\n  Clusters", "abstract": "Recent advances in large language models (LLMs) have shown remarkable\nperformance across diverse tasks. However, these models are typically deployed\nwith fixed weights, which limits their ability to adapt dynamically to the\nvariability inherent in real-world data during inference. This paper introduces\nChameleonLLM, a novel framework that enables inference-time adaptation of LLMs\nby leveraging batch-aware clustering and on-the-fly generation of low-rank\nupdates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation\n(LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable\nmasks), our method dynamically generates adaptive modifications to the decoder\nweights based on the aggregated statistics of clustered batches. By\nintelligently grouping similar inputs and computing context-aware low-rank\nupdates via a hyper-network, ChameleonLLM achieves significant performance\ngains, outperforming conventional LoRA methods while eliminating the overhead\nof maintaining multiple expert models. Our experiments highlight the potential\nof our approach to serve as a versatile and highly adaptive solution for\nlanguage model inference. ChameleonLLM is open-sourced to ensure the\nreproducibility of our experiments:\nhttps://anonymous.4open.science/r/ChamaleonLLM/", "published": "2025-02-06 18:57:06", "link": "http://arxiv.org/abs/2502.04315v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple\n  Interactions", "abstract": "Despite extensive safety alignment efforts, large language models (LLMs)\nremain vulnerable to jailbreak attacks that elicit harmful behavior. While\nexisting studies predominantly focus on attack methods that require technical\nexpertise, two critical questions remain underexplored: (1) Are jailbroken\nresponses truly useful in enabling average users to carry out harmful actions?\n(2) Do safety vulnerabilities exist in more common, simple human-LLM\ninteractions? In this paper, we demonstrate that LLM responses most effectively\nfacilitate harmful actions when they are both actionable and informative--two\nattributes easily elicited in multi-step, multilingual interactions. Using this\ninsight, we propose HarmScore, a jailbreak metric that measures how effectively\nan LLM response enables harmful actions, and Speak Easy, a simple multi-step,\nmultilingual attack framework. Notably, by incorporating Speak Easy into direct\nrequest and jailbreak baselines, we see an average absolute increase of 0.319\nin Attack Success Rate and 0.426 in HarmScore in both open-source and\nproprietary LLMs across four safety benchmarks. Our work reveals a critical yet\noften overlooked vulnerability: Malicious users can easily exploit common\ninteraction patterns for harmful intentions.", "published": "2025-02-06 18:59:02", "link": "http://arxiv.org/abs/2502.04322v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Multimodal Medical Code Tokenizer", "abstract": "Foundation models trained on patient electronic health records (EHRs) require\ntokenizing medical data into sequences of discrete vocabulary items. Existing\ntokenizers treat medical codes from EHRs as isolated textual tokens. However,\neach medical code is defined by its textual description, its position in\nontological hierarchies, and its relationships to other codes, such as disease\nco-occurrences and drug-treatment associations. Medical vocabularies contain\nmore than 600,000 codes with critical information for clinical reasoning. We\nintroduce MedTok, a multimodal medical code tokenizer that uses the text\ndescriptions and relational context of codes. MedTok processes text using a\nlanguage model encoder and encodes the relational structure with a graph\nencoder. It then quantizes both modalities into a unified token space,\npreserving modality-specific and cross-modality information. We integrate\nMedTok into five EHR models and evaluate it on operational and clinical tasks\nacross in-patient and out-patient datasets, including outcome prediction,\ndiagnosis classification, drug recommendation, and risk stratification.\nSwapping standard EHR tokenizers with MedTok improves AUPRC across all EHR\nmodels, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.30% on EHRShot, with\nthe largest gains in drug recommendation. Beyond EHR modeling, we demonstrate\nusing MedTok tokenizer with medical QA systems. Our results demonstrate the\npotential of MedTok as a unified tokenizer for medical codes, improving\ntokenization for medical foundation models.", "published": "2025-02-06 06:58:09", "link": "http://arxiv.org/abs/2502.04397v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FAS: Fast ANN-SNN Conversion for Spiking Large Language Models", "abstract": "Spiking Large Language Models have been shown as a good alternative to LLMs\nin various scenarios. Existing methods for creating Spiking LLMs, i.e., direct\ntraining and ANN-SNN conversion, often suffer from performance degradation and\nrelatively high computational costs. To address these issues, we propose a\nnovel Fast ANN-SNN conversion strategy (FAS) that transforms LLMs into spiking\nLLMs in two stages. The first stage employs a full-parameter fine-tuning of\npre-trained models, so it does not need any direct training from scratch. The\nsecond stage introduces a coarse-to-fine calibration method to reduce\nconversion errors and improve accuracy. Our experiments on both language and\nvision-language tasks across four different scales of LLMs demonstrate that FAS\ncan achieve state-of-the-art performance yet with significantly reduced\ninference latency and computational costs. For example, FAS only takes 8\ntimesteps to achieve an accuracy of 3% higher than that of the OPT-7B model,\nwhile reducing energy consumption by 96.63%.", "published": "2025-02-06 09:08:12", "link": "http://arxiv.org/abs/2502.04405v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and\n  Uncertainty Based Routing", "abstract": "Model merging aggregates Large Language Models (LLMs) finetuned on different\ntasks into a stronger one. However, parameter conflicts between models leads to\nperformance degradation in averaging. While model routing addresses this issue\nby selecting individual models during inference, it imposes excessive storage\nand compute costs, and fails to leverage the common knowledge from different\nmodels. In this work, we observe that different layers exhibit varying levels\nof parameter conflicts. Building on this insight, we average layers with\nminimal parameter conflicts and use a novel task-level expert routing for\nlayers with significant conflicts. To further reduce storage costs, inspired by\ntask arithmetic sparsity, we decouple multiple fine-tuned experts into a dense\nexpert and several sparse experts. Considering the out-of-distribution samples,\nwe select and merge appropriate experts based on the task uncertainty of the\ninput data. We conduct extensive experiments on both LLaMA and Qwen with\nvarying parameter scales, and evaluate on real-world reasoning tasks. Results\ndemonstrate that our method consistently achieves significant performance\nimprovements while requiring less system cost compared to existing methods.", "published": "2025-02-06 11:26:30", "link": "http://arxiv.org/abs/2502.04411v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T50"], "primary_category": "cs.LG"}
{"title": "Decoder-Only LLMs are Better Controllers for Diffusion Models", "abstract": "Groundbreaking advancements in text-to-image generation have recently been\nachieved with the emergence of diffusion models. These models exhibit a\nremarkable ability to generate highly artistic and intricately detailed images\nbased on textual prompts. However, obtaining desired generation outcomes often\nnecessitates repetitive trials of manipulating text prompts just like casting\nspells on a magic mirror, and the reason behind that is the limited capability\nof semantic understanding inherent in current image generation models.\nSpecifically, existing diffusion models encode the text prompt input with a\npre-trained encoder structure, which is usually trained on a limited number of\nimage-caption pairs. The state-of-the-art large language models (LLMs) based on\nthe decoder-only structure have shown a powerful semantic understanding\ncapability as their architectures are more suitable for training on very\nlarge-scale unlabeled data. In this work, we propose to enhance text-to-image\ndiffusion models by borrowing the strength of semantic understanding from large\nlanguage models, and devise a simple yet effective adapter to allow the\ndiffusion models to be compatible with the decoder-only structure. Meanwhile,\nwe also provide a supporting theoretical analysis with various architectures\n(e.g., encoder-only, encoder-decoder, and decoder-only), and conduct extensive\nempirical evaluations to verify its effectiveness. The experimental results\nshow that the enhanced models with our adapter module are superior to the\nstat-of-the-art models in terms of text-to-image generation quality and\nreliability.", "published": "2025-02-06 12:17:35", "link": "http://arxiv.org/abs/2502.04412v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MedRAG: Enhancing Retrieval-augmented Generation with Knowledge\n  Graph-Elicited Reasoning for Healthcare Copilot", "abstract": "Retrieval-augmented generation (RAG) is a well-suited technique for\nretrieving privacy-sensitive Electronic Health Records (EHR). It can serve as a\nkey module of the healthcare copilot, helping reduce misdiagnosis for\nhealthcare practitioners and patients. However, the diagnostic accuracy and\nspecificity of existing heuristic-based RAG models used in the medical domain\nare inadequate, particularly for diseases with similar manifestations. This\npaper proposes MedRAG, a RAG model enhanced by knowledge graph (KG)-elicited\nreasoning for the medical domain that retrieves diagnosis and treatment\nrecommendations based on manifestations. MedRAG systematically constructs a\ncomprehensive four-tier hierarchical diagnostic KG encompassing critical\ndiagnostic differences of various diseases. These differences are dynamically\nintegrated with similar EHRs retrieved from an EHR database, and reasoned\nwithin a large language model. This process enables more accurate and specific\ndecision support, while also proactively providing follow-up questions to\nenhance personalized medical decision-making. MedRAG is evaluated on both a\npublic dataset DDXPlus and a private chronic pain diagnostic dataset (CPDD)\ncollected from Tan Tock Seng Hospital, and its performance is compared against\nvarious existing RAG methods. Experimental results show that, leveraging the\ninformation integration and relational abilities of the KG, our MedRAG provides\nmore specific diagnostic insights and outperforms state-of-the-art models in\nreducing misdiagnosis rates. Our code will be available at\nhttps://github.com/SNOWTEAM2023/MedRAG", "published": "2025-02-06 12:27:35", "link": "http://arxiv.org/abs/2502.04413v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Understanding and Mitigating the Bias Inheritance in LLM-based Data\n  Augmentation on Downstream Tasks", "abstract": "Generating synthetic datasets via large language models (LLMs) themselves has\nemerged as a promising approach to improve LLM performance. However, LLMs\ninherently reflect biases present in their training data, leading to a critical\nchallenge: when these models generate synthetic data for training, they may\npropagate and amplify their inherent biases that can significantly impact model\nfairness and robustness on downstream tasks--a phenomenon we term bias\ninheritance. This work presents the first systematic investigation in\nunderstanding, analyzing, and mitigating bias inheritance. We study this\nproblem by fine-tuning LLMs with a combined dataset consisting of original and\nLLM-augmented data, where bias ratio represents the proportion of augmented\ndata. Through systematic experiments across 10 classification and generation\ntasks, we analyze how 6 different types of biases manifest at varying bias\nratios. Our results reveal that bias inheritance has nuanced effects on\ndownstream tasks, influencing both classification tasks and generation tasks\ndifferently. Then, our analysis identifies three key misalignment factors:\nmisalignment of values, group data, and data distributions. Based on these\ninsights, we propose three mitigation strategies: token-based, mask-based, and\nloss-based approaches. Experiments demonstrate that these strategies also work\ndifferently on various tasks and bias, indicating the substantial challenges to\nfully mitigate bias inheritance. We hope this work can provide valuable\ninsights to the research of LLM data augmentation.", "published": "2025-02-06 15:20:58", "link": "http://arxiv.org/abs/2502.04419v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference", "abstract": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner.", "published": "2025-02-06 15:26:26", "link": "http://arxiv.org/abs/2502.04420v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Primary Care Diagnoses as a Reliable Predictor for Orthopedic Surgical\n  Interventions", "abstract": "Referral workflow inefficiencies, including misaligned referrals and delays,\ncontribute to suboptimal patient outcomes and higher healthcare costs. In this\nstudy, we investigated the possibility of predicting procedural needs based on\nprimary care diagnostic entries, thereby improving referral accuracy,\nstreamlining workflows, and providing better care to patients. A de-identified\ndataset of 2,086 orthopedic referrals from the University of Texas Health at\nTyler was analyzed using machine learning models built on Base General\nEmbeddings (BGE) for semantic extraction. To ensure real-world applicability,\nnoise tolerance experiments were conducted, and oversampling techniques were\nemployed to mitigate class imbalance. The selected optimum and parsimonious\nembedding model demonstrated high predictive accuracy (ROC-AUC: 0.874, Matthews\nCorrelation Coefficient (MCC): 0.540), effectively distinguishing patients\nrequiring surgical intervention. Dimensionality reduction techniques confirmed\nthe model's ability to capture meaningful clinical relationships. A threshold\nsensitivity analysis identified an optimal decision threshold (0.30) to balance\nprecision and recall, maximizing referral efficiency. In the predictive\nmodeling analysis, the procedure rate increased from 11.27% to an optimal\n60.1%, representing a 433% improvement with significant implications for\noperational efficiency and healthcare revenue.\n  The results of our study demonstrate that referral optimization can enhance\nprimary and surgical care integration. Through this approach, precise and\ntimely predictions of procedural requirements can be made, thereby minimizing\ndelays, improving surgical planning, and reducing administrative burdens. In\naddition, the findings highlight the potential of clinical decision support as\na scalable solution for improving patient outcomes and the efficiency of the\nhealthcare system.", "published": "2025-02-06 17:15:12", "link": "http://arxiv.org/abs/2502.04423v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7; J.3; H.2.8"], "primary_category": "cs.LG"}
{"title": "Decoding AI Judgment: How LLMs Assess News Credibility and Bias", "abstract": "Large Language Models (LLMs) are increasingly used to assess news\ncredibility, yet little is known about how they make these judgments. While\nprior research has examined political bias in LLM outputs or their potential\nfor automated fact-checking, their internal evaluation processes remain largely\nunexamined. Understanding how LLMs assess credibility provides insights into AI\nbehavior and how credibility is structured and applied in large-scale language\nmodels. This study benchmarks the reliability and political classifications of\nstate-of-the-art LLMs - Gemini 1.5 Flash (Google), GPT-4o mini (OpenAI), and\nLLaMA 3.1 (Meta) - against structured, expert-driven rating systems such as\nNewsGuard and Media Bias Fact Check. Beyond assessing classification\nperformance, we analyze the linguistic markers that shape LLM decisions,\nidentifying which words and concepts drive their evaluations. We uncover\npatterns in how LLMs associate credibility with specific linguistic features by\nexamining keyword frequency, contextual determinants, and rank distributions.\nBeyond static classification, we introduce a framework in which LLMs refine\ntheir credibility assessments by retrieving external information, querying\nother models, and adapting their responses. This allows us to investigate\nwhether their assessments reflect structured reasoning or rely primarily on\nprior learned associations.", "published": "2025-02-06 18:52:10", "link": "http://arxiv.org/abs/2502.04426v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Confident or Seek Stronger: Exploring Uncertainty-Based On-device LLM\n  Routing From Benchmarking to Generalization", "abstract": "Large language models (LLMs) are increasingly deployed and democratized on\nedge devices. To improve the efficiency of on-device deployment, small language\nmodels (SLMs) are often adopted due to their efficient decoding latency and\nreduced energy consumption. However, these SLMs often generate inaccurate\nresponses when handling complex queries. One promising solution is\nuncertainty-based SLM routing, offloading high-stakes queries to stronger LLMs\nwhen resulting in low-confidence responses on SLM. This follows the principle\nof \"If you lack confidence, seek stronger support\" to enhance reliability.\nRelying on more powerful LLMs is yet effective but increases invocation costs.\nTherefore, striking a routing balance between efficiency and efficacy remains a\ncritical challenge. Additionally, efficiently generalizing the routing strategy\nto new datasets remains under-explored. In this paper, we conduct a\ncomprehensive investigation into benchmarking and generalization of\nuncertainty-driven routing strategies from SLMs to LLMs over 1500+ settings.\nOur findings highlight: First, uncertainty-correctness alignment in different\nuncertainty quantification (UQ) methods significantly impacts routing\nperformance. Second, uncertainty distributions depend more on both the specific\nSLM and the chosen UQ method, rather than downstream data. Building on the\ninsight, we propose a calibration data construction instruction pipeline and\nopen-source a constructed hold-out set to enhance routing generalization on new\ndownstream scenarios. The experimental results indicate calibration data\neffectively bootstraps routing performance without any new data.", "published": "2025-02-06 18:59:11", "link": "http://arxiv.org/abs/2502.04428v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Active Task Disambiguation with LLMs", "abstract": "Despite the impressive performance of large language models (LLMs) across\nvarious benchmarks, their ability to address ambiguously specified\nproblems--frequent in real-world interactions--remains underexplored. To\naddress this gap, we introduce a formal definition of task ambiguity and frame\nthe problem of task disambiguation through the lens of Bayesian Experimental\nDesign. By posing clarifying questions, LLM agents can acquire additional task\nspecifications, progressively narrowing the space of viable solutions and\nreducing the risk of generating unsatisfactory outputs. Yet, generating\neffective clarifying questions requires LLM agents to engage in a form of\nmeta-cognitive reasoning, an ability LLMs may presently lack. Our proposed\napproach of active task disambiguation enables LLM agents to generate targeted\nquestions maximizing the information gain. Effectively, this approach shifts\nthe load from implicit to explicit reasoning about the space of viable\nsolutions. Empirical results demonstrate that this form of question selection\nleads to more effective task disambiguation in comparison to approaches relying\non reasoning solely within the space of questions.", "published": "2025-02-06 20:20:22", "link": "http://arxiv.org/abs/2502.04485v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Revisiting Intermediate-Layer Matching in Knowledge Distillation:\n  Layer-Selection Strategy Doesn't Matter (Much)", "abstract": "Knowledge distillation (KD) is a popular method of transferring knowledge\nfrom a large \"teacher\" model to a small \"student\" model. KD can be divided into\ntwo categories: prediction matching and intermediate-layer matching. We explore\nan intriguing phenomenon: layer-selection strategy does not matter (much) in\nintermediate-layer matching. In this paper, we show that seemingly nonsensical\nmatching strategies such as matching the teacher's layers in reverse still\nresult in surprisingly good student performance. We provide an interpretation\nfor this phenomenon by examining the angles between teacher layers viewed from\nthe student's perspective.", "published": "2025-02-06 21:00:01", "link": "http://arxiv.org/abs/2502.04499v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7; I.2.6"], "primary_category": "cs.LG"}
{"title": "TruthFlow: Truthful LLM Generation via Representation Flow Correction", "abstract": "Large language models (LLMs) are known to struggle with consistently\ngenerating truthful responses. While various representation intervention\ntechniques have been proposed, these methods typically apply a universal\nrepresentation correction vector to all input queries, limiting their\neffectiveness against diverse queries in practice. In this study, we introduce\nTruthFlow, a novel method that leverages the Flow Matching technique for\nquery-specific truthful representation correction. Specifically, TruthFlow\nfirst uses a flow model to learn query-specific correction vectors that\ntransition representations from hallucinated to truthful states. Then, during\ninference, the trained flow model generates these correction vectors to enhance\nthe truthfulness of LLM outputs. Experimental results demonstrate that\nTruthFlow significantly improves performance on open-ended generation tasks\nacross various advanced LLMs evaluated on TruthfulQA. Moreover, the trained\nTruthFlow model exhibits strong transferability, performing effectively on\nother unseen hallucination benchmarks.", "published": "2025-02-06 23:10:14", "link": "http://arxiv.org/abs/2502.04556v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robotouille: An Asynchronous Planning Benchmark for LLM Agents", "abstract": "Effective asynchronous planning, or the ability to efficiently reason and\nplan over states and actions that must happen in parallel or sequentially, is\nessential for agents that must account for time delays, reason over diverse\nlong-horizon tasks, and collaborate with other agents. While large language\nmodel (LLM) agents show promise in high-level task planning, current benchmarks\nfocus primarily on short-horizon tasks and do not evaluate such asynchronous\nplanning capabilities. We introduce Robotouille, a challenging benchmark\nenvironment designed to test LLM agents' ability to handle long-horizon\nasynchronous scenarios. Our synchronous and asynchronous datasets capture\nincreasingly complex planning challenges that go beyond existing benchmarks,\nrequiring agents to manage overlapping tasks and interruptions. Our results\nshow that ReAct (gpt4-o) achieves 47% on synchronous tasks but only 11% on\nasynchronous tasks, highlighting significant room for improvement. We further\nanalyze failure modes, demonstrating the need for LLM agents to better\nincorporate long-horizon feedback and self-audit their reasoning during task\nexecution. Code is available at https://github.com/portal-cornell/robotouille.", "published": "2025-02-06 05:50:37", "link": "http://arxiv.org/abs/2502.05227v1", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Exploring Model Invariance with Discrete Search for Ultra-Low-Bit\n  Quantization", "abstract": "Large language models have been increasing in size due to their success in a\nwide range of applications. This calls for a pressing need to reduce memory\nusage to make them more accessible. Post-training quantization is a popular\ntechnique which uses fewer bits (e.g., 4--8 bits) to represent the model\nwithout retraining it. However, it remains a challenging task to perform\nquantization in an ultra-low-bit setup (e.g., 2 bits). In this paper, we\npropose InvarExplore, a unified framework that systematically explores\ndifferent model invariance at the same time, allowing us to take advantage of\nthe synergy between each type of invariance. Importantly, InvarExplore features\na discrete search algorithm that enables us to explore permutation invariance,\nwhich is under-studied as it cannot be optimized with gradient-based methods.\nResults show that InvarExplore is compatible with existing state-of-the-art\nmethods, achieving an add-on performance improvement over strong competing\nmethods.", "published": "2025-02-06 21:00:13", "link": "http://arxiv.org/abs/2502.06844v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7; I.2.6; I.2.m; I.5.1; I.7.m"], "primary_category": "cs.LG"}
{"title": "QExplorer: Large Language Model Based Query Extraction for Toxic Content\n  Exploration", "abstract": "Automatically extracting effective queries is challenging in information\nretrieval, especially in toxic content exploration, as such content is likely\nto be disguised. With the recent achievements in generative Large Language\nModel (LLM), we are able to leverage the capabilities of LLMs to extract\neffective queries for similar content exploration directly. This study proposes\nQExplorer, an approach of large language model based Query Extraction for toxic\ncontent Exploration. The QExplorer approach involves a 2-stage training\nprocess: instruction Supervised FineTuning (SFT) and preference alignment using\nDirect Preference Optimization (DPO), as well as the datasets construction with\nfeedback of search system. To verify the effectiveness of QExplorer, a series\nof offline and online experiments are conducted on our real-world system. The\noffline empirical results demonstrate that the performance of our automatic\nquery extraction outperforms that of several LLMs and humans. The online\ndeployment shows a significant increase in the detection of toxic items.", "published": "2025-02-06 06:11:58", "link": "http://arxiv.org/abs/2502.18480v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "DiTAR: Diffusion Transformer Autoregressive Modeling for Speech\n  Generation", "abstract": "Several recent studies have attempted to autoregressively generate continuous\nspeech representations without discrete speech tokens by combining diffusion\nand autoregressive models, yet they often face challenges with excessive\ncomputational loads or suboptimal outcomes. In this work, we propose Diffusion\nTransformer Autoregressive Modeling (DiTAR), a patch-based autoregressive\nframework combining a language model with a diffusion transformer. This\napproach significantly enhances the efficacy of autoregressive models for\ncontinuous tokens and reduces computational demands. DiTAR utilizes a\ndivide-and-conquer strategy for patch generation, where the language model\nprocesses aggregated patch embeddings and the diffusion transformer\nsubsequently generates the next patch based on the output of the language\nmodel. For inference, we propose defining temperature as the time point of\nintroducing noise during the reverse diffusion ODE to balance diversity and\ndeterminism. We also show in the extensive scaling analysis that DiTAR has\nsuperb scalability. In zero-shot speech generation, DiTAR achieves\nstate-of-the-art performance in robustness, speaker similarity, and\nnaturalness.", "published": "2025-02-06 10:09:49", "link": "http://arxiv.org/abs/2502.03930v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based\n  Speech Synthesis", "abstract": "Recent advances in text-based large language models (LLMs), particularly in\nthe GPT series and the o1 model, have demonstrated the effectiveness of scaling\nboth training-time and inference-time compute. However, current\nstate-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring\nseparate models (e.g., diffusion models after LLM), complicating the decision\nof whether to scale a particular model during training or testing. This work\nmakes the following contributions: First, we explore the scaling of train-time\nand inference-time compute for speech synthesis. Second, we propose a simple\nframework Llasa for speech synthesis that employs a single-layer vector\nquantizer (VQ) codec and a single Transformer architecture to fully align with\nstandard LLMs such as Llama. Our experiments reveal that scaling train-time\ncompute for Llasa consistently improves the naturalness of synthesized speech\nand enables the generation of more complex and accurate prosody patterns.\nFurthermore, from the perspective of scaling inference-time compute, we employ\nspeech understanding models as verifiers during the search, finding that\nscaling inference-time compute shifts the sampling modes toward the preferences\nof specific verifiers, thereby improving emotional expressiveness, timbre\nconsistency, and content accuracy. In addition, we released the checkpoint and\ntraining code for our TTS model (1B, 3B, 8B) and codec model publicly\navailable.", "published": "2025-02-06 15:04:00", "link": "http://arxiv.org/abs/2502.04128v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment", "abstract": "Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola.", "published": "2025-02-06 18:59:55", "link": "http://arxiv.org/abs/2502.04328v2", "categories": ["cs.CV", "cs.CL", "cs.MM", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Towards Explainable Spoofed Speech Attribution and Detection:a\n  Probabilistic Approach for Characterizing Speech Synthesizer Components", "abstract": "We propose an explainable probabilistic framework for characterizing spoofed\nspeech by decomposing it into probabilistic attribute embeddings. Unlike raw\nhigh-dimensional countermeasure embeddings, which lack interpretability, the\nproposed probabilistic attribute embeddings aim to detect specific speech\nsynthesizer components, represented through high-level attributes and their\ncorresponding values. We use these probabilistic embeddings with four\nclassifier back-ends to address two downstream tasks: spoofing detection and\nspoofing attack attribution. The former is the well-known bonafide-spoof\ndetection task, whereas the latter seeks to identify the source method\n(generator) of a spoofed utterance. We additionally use Shapley values, a\nwidely used technique in machine learning, to quantify the relative\ncontribution of each attribute value to the decision-making process in each\ntask. Results on the ASVspoof2019 dataset demonstrate the substantial role of\nduration and conversion modeling in spoofing detection; and waveform generation\nand speaker modeling in spoofing attack attribution. In the detection task, the\nprobabilistic attribute embeddings achieve $99.7\\%$ balanced accuracy and\n$0.22\\%$ equal error rate (EER), closely matching the performance of raw\nembeddings ($99.9\\%$ balanced accuracy and $0.22\\%$ EER). Similarly, in the\nattribution task, our embeddings achieve $90.23\\%$ balanced accuracy and\n$2.07\\%$ EER, compared to $90.16\\%$ and $2.11\\%$ with raw embeddings. These\nresults demonstrate that the proposed framework is both inherently explainable\nby design and capable of achieving performance comparable to raw CM embeddings.", "published": "2025-02-06 13:06:33", "link": "http://arxiv.org/abs/2502.04049v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Blind Capon Beamformer Based on Independent Component Extraction:\n  Single-Parameter Algorithm,", "abstract": "We consider a phase-shift mixing model for linear sensor arrays in the\ncontext of blind source extraction. We derive a blind Capon beamformer that\nseeks the direction where the output is independent of the other signals in the\nmixture. The algorithm is based on Independent Component Extraction and imposes\nan orthogonal constraint, thanks to which it optimizes only one real-valued\nparameter related to the angle of arrival. The Cram\\'er-Rao lower bound for the\nmean interference-to-signal ratio is derived. The algorithm and the bound are\ncompared with conventional blind and direction-of-arrival\nestimation+beamforming methods, showing improvements in terms of extraction\naccuracy. An application is demonstrated in frequency-domain speaker extraction\nin a low-reverberation room.", "published": "2025-02-06 08:37:22", "link": "http://arxiv.org/abs/2502.03871v1", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
{"title": "GenVC: Self-Supervised Zero-Shot Voice Conversion", "abstract": "Zero-shot voice conversion has recently made substantial progress, but many\nmodels still depend on external supervised systems to disentangle speaker\nidentity and linguistic content. Furthermore, current methods often use\nparallel conversion, where the converted speech inherits the source utterance's\ntemporal structure, restricting speaker similarity and privacy. To overcome\nthese limitations, we introduce GenVC, a generative zero-shot voice conversion\nmodel. GenVC learns to disentangle linguistic content and speaker style in a\nself-supervised manner, eliminating the need for external models and enabling\nefficient training on large, unlabeled datasets. Experimental results show that\nGenVC achieves state-of-the-art speaker similarity while maintaining\nnaturalness competitive with leading approaches. Its autoregressive generation\nalso allows the converted speech to deviate from the source utterance's\ntemporal structure. This feature makes GenVC highly effective for voice\nanonymization, as it minimizes the preservation of source prosody and speaker\ncharacteristics, enhancing privacy protection.", "published": "2025-02-06 21:40:09", "link": "http://arxiv.org/abs/2502.04519v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Towards Unified Music Emotion Recognition across Dimensional and\n  Categorical Models", "abstract": "One of the most significant challenges in Music Emotion Recognition (MER)\ncomes from the fact that emotion labels can be heterogeneous across datasets\nwith regard to the emotion representation, including categorical (e.g., happy,\nsad) versus dimensional labels (e.g., valence-arousal). In this paper, we\npresent a unified multitask learning framework that combines these two types of\nlabels and is thus able to be trained on multiple datasets. This framework uses\nan effective input representation that combines musical features (i.e., key and\nchords) and MERT embeddings. Moreover, knowledge distillation is employed to\ntransfer the knowledge of teacher models trained on individual datasets to a\nstudent model, enhancing its ability to generalize across multiple tasks. To\nvalidate our proposed framework, we conducted extensive experiments on a\nvariety of datasets, including MTG-Jamendo, DEAM, PMEmo, and EmoMusic.\nAccording to our experimental results, the inclusion of musical features,\nmultitask learning, and knowledge distillation significantly enhances\nperformance. In particular, our model outperforms the state-of-the-art models,\nincluding the best-performing model from the MediaEval 2021 competition on the\nMTG-Jamendo dataset. Our work makes a significant contribution to MER by\nallowing the combination of categorical and dimensional emotion labels in one\nunified framework, thus enabling training across datasets.", "published": "2025-02-06 11:20:22", "link": "http://arxiv.org/abs/2502.03979v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A data-driven two-microphone method for in-situ sound absorption\n  measurements", "abstract": "This work presents a data-driven approach to estimating the sound absorption\ncoefficient of an infinite porous slab using a neural network and a\ntwo-microphone measurement on a finite porous sample. A 1D-convolutional\nnetwork predicts the sound absorption coefficient from the complex-valued\ntransfer function between the sound pressure measured at the two microphone\npositions. The network is trained and validated with numerical data generated\nby a boundary element model using the Delany-Bazley-Miki model, demonstrating\naccurate predictions for various numerical samples. The method is\nexperimentally validated with baffled rectangular samples of a fibrous\nmaterial, where sample size and source height are varied. The results show that\nthe neural network offers the possibility to reliably predict the in-situ sound\nabsorption of a porous material using the traditional two-microphone method as\nif the sample were infinite. The normal-incidence sound absorption coefficient\nobtained by the network compares well with that obtained theoretically and in\nan impedance tube. The proposed method has promising perspectives for\nestimating the sound absorption coefficient of acoustic materials after\ninstallation and in realistic operational conditions.", "published": "2025-02-06 15:23:28", "link": "http://arxiv.org/abs/2502.04143v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks", "abstract": "Large language models have revolutionized natural language processing through\nself-supervised pretraining on massive datasets. Inspired by this success,\nresearchers have explored adapting these methods to speech by discretizing\ncontinuous audio into tokens using neural audio codecs. However, existing\napproaches face limitations, including high bitrates, the loss of either\nsemantic or acoustic information, and the reliance on multi-codebook designs\nwhen trying to capture both, which increases architectural complexity for\ndownstream tasks. To address these challenges, we introduce FocalCodec, an\nefficient low-bitrate codec based on focal modulation that utilizes a single\nbinary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec\ndelivers competitive performance in speech resynthesis and voice conversion at\nlower bitrates than the current state-of-the-art, while effectively handling\nmultilingual speech and noisy environments. Evaluation on downstream tasks\nshows that FocalCodec successfully preserves sufficient semantic and acoustic\ninformation, while also being well-suited for generative modeling. Demo\nsamples, code and checkpoints are available at\nhttps://lucadellalib.github.io/focalcodec-web/.", "published": "2025-02-06 19:24:50", "link": "http://arxiv.org/abs/2502.04465v1", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "ADIFF: Explaining audio difference using natural language", "abstract": "Understanding and explaining differences between audio recordings is crucial\nfor fields like audio forensics, quality assessment, and audio generation. This\ninvolves identifying and describing audio events, acoustic scenes, signal\ncharacteristics, and their emotional impact on listeners. This paper stands out\nas the first work to comprehensively study the task of explaining audio\ndifferences and then propose benchmark, baselines for the task. First, we\npresent two new datasets for audio difference explanation derived from the\nAudioCaps and Clotho audio captioning datasets. Using Large Language Models\n(LLMs), we generate three levels of difference explanations: (1) concise\ndescriptions of audio events and objects, (2) brief sentences about audio\nevents, acoustic scenes, and signal properties, and (3) comprehensive\nexplanations that include semantics and listener emotions. For the baseline, we\nuse prefix tuning where audio embeddings from two audio files are used to\nprompt a frozen language model. Our empirical analysis and ablation studies\nreveal that the naive baseline struggles to distinguish perceptually similar\nsounds and generate detailed tier 3 explanations. To address these limitations,\nwe propose ADIFF, which introduces a cross-projection module, position\ncaptioning, and a three-step training process to enhance the model's ability to\nproduce detailed explanations. We evaluate our model using objective metrics\nand human evaluation and show our model enhancements lead to significant\nimprovements in performance over naive baseline and SoTA Audio-Language Model\n(ALM) Qwen Audio. Lastly, we conduct multiple ablation studies to study the\neffects of cross-projection, language model parameters, position captioning,\nthird stage fine-tuning, and present our findings. Our benchmarks, findings,\nand strong baseline pave the way for nuanced and human-like explanations of\naudio differences.", "published": "2025-02-06 20:00:43", "link": "http://arxiv.org/abs/2502.04476v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ImprovNet: Generating Controllable Musical Improvisations with Iterative\n  Corruption Refinement", "abstract": "Deep learning has enabled remarkable advances in style transfer across\nvarious domains, offering new possibilities for creative content generation.\nHowever, in the realm of symbolic music, generating controllable and expressive\nperformance-level style transfers for complete musical works remains\nchallenging due to limited datasets, especially for genres such as jazz, and\nthe lack of unified models that can handle multiple music generation tasks.\nThis paper presents ImprovNet, a transformer-based architecture that generates\nexpressive and controllable musical improvisations through a self-supervised\ncorruption-refinement training strategy. ImprovNet unifies multiple\ncapabilities within a single model: it can perform cross-genre and intra-genre\nimprovisations, harmonize melodies with genre-specific styles, and execute\nshort prompt continuation and infilling tasks. The model's iterative generation\nframework allows users to control the degree of style transfer and structural\nsimilarity to the original composition. Objective and subjective evaluations\ndemonstrate ImprovNet's effectiveness in generating musically coherent\nimprovisations while maintaining structural relationships with the original\npieces. The model outperforms Anticipatory Music Transformer in short\ncontinuation and infilling tasks and successfully achieves recognizable genre\nconversion, with 79\\% of participants correctly identifying jazz-style\nimprovisations. Our code and demo page can be found at\nhttps://github.com/keshavbhandari/improvnet.", "published": "2025-02-06 21:45:38", "link": "http://arxiv.org/abs/2502.04522v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Aligner-Encoders: Self-Attention Transformers Can Be Self-Transducers", "abstract": "Modern systems for automatic speech recognition, including the RNN-Transducer\nand Attention-based Encoder-Decoder (AED), are designed so that the encoder is\nnot required to alter the time-position of information from the audio sequence\ninto the embedding; alignment to the final text output is processed during\ndecoding. We discover that the transformer-based encoder adopted in recent\nyears is actually capable of performing the alignment internally during the\nforward pass, prior to decoding. This new phenomenon enables a simpler and more\nefficient model, the \"Aligner-Encoder\". To train it, we discard the dynamic\nprogramming of RNN-T in favor of the frame-wise cross-entropy loss of AED,\nwhile the decoder employs the lighter text-only recurrence of RNN-T without\nlearned cross-attention -- it simply scans embedding frames in order from the\nbeginning, producing one token each until predicting the end-of-message. We\nconduct experiments demonstrating performance remarkably close to the state of\nthe art, including a special inference configuration enabling long-form\nrecognition. In a representative comparison, we measure the total inference\ntime for our model to be 2x faster than RNN-T and 16x faster than AED. Lastly,\nwe find that the audio-text alignment is clearly visible in the self-attention\nweights of a certain layer, which could be said to perform \"self-transduction\".", "published": "2025-02-06 22:09:52", "link": "http://arxiv.org/abs/2502.05232v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Hybrid Model for Weakly-Supervised Speech Dereverberation", "abstract": "This paper introduces a new training strategy to improve speech\ndereverberation systems using minimal acoustic information and reverberant\n(wet) speech. Most existing algorithms rely on paired dry/wet data, which is\ndifficult to obtain, or on target metrics that may not adequately capture\nreverberation characteristics and can lead to poor results on non-target\nmetrics. Our approach uses limited acoustic information, like the reverberation\ntime (RT60), to train a dereverberation system. The system's output is\nresynthesized using a generated room impulse response and compared with the\noriginal reverberant speech, providing a novel reverberation matching loss\nreplacing the standard target metrics. During inference, only the trained\ndereverberation model is used. Experimental results demonstrate that our method\nachieves more consistent performance across various objective metrics used in\nspeech dereverberation than the state-of-the-art.", "published": "2025-02-06 09:21:22", "link": "http://arxiv.org/abs/2502.06839v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Musical Score Following using Statistical Inference", "abstract": "Musical score following is the real-time mapping of a performance to\ncorresponding locations in a musical score. Score following can be used in a\nvariety of applications including automatic page turning and real-time\naccompaniment. This report presents a novel approach for score following\nmotivated by Wilson and Adams's 2013 paper, which introduces Spectral Mixture\n(SM) kernels for Gaussian Process (GP) regression. Since the SM kernel is\nderived from a Mixture of Gaussians in the frequency domain, it is particularly\nsuitable for modelling the superposed power spectra of musical notes, in which\nenergy is concentrated at multiples of the fundamental frequency of each note.\nOur score follower begins by using a GP to statistically infer the musical\nnotes played during 800-sample 'audioframes' (~18 ms) of solo piano music.\nThese predictions are then used in a duration-dependent Hidden Markov Model to\npredict the most likely score positions in real time. Our two-stage approach\nachieves successful score following not only on four-part hymns arranged for\nkeyboard, but also on pieces for the violin, oboe, and flute. This showcases\nthe powerful and flexible nature of GPs for statistical inference on musical\naudio signals. Given the success of this project, we contribute to the\nliterature a first proof of concept of the application of GPs in score\nfollowing, and more broadly, in online Music Information Retrieval (MIR) tasks.\nThis project also contributes a working score follower product that renders\nscore position in real time using an adapted open-source user interface. Areas\nfor future work include improving accuracy on repeated notes and during heavy\nuse of sustain pedal, adapting to minor deviations from the score, and\nmodelling multi-instrument works.", "published": "2025-02-06 08:24:30", "link": "http://arxiv.org/abs/2502.10426v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "UniForm: A Unified Diffusion Transformer for Audio-Video Generation", "abstract": "As a natural multimodal content, audible video delivers an immersive sensory\nexperience. Consequently, audio-video generation systems have substantial\npotential. However, existing diffusion-based studies mainly employ relatively\nindependent modules for generating each modality, which lack exploration of\nshared-weight generative modules. This approach may under-use the intrinsic\ncorrelations between audio and visual modalities, potentially resulting in\nsub-optimal generation quality. To address this, we propose UniForm, a unified\ndiffusion transformer designed to enhance cross-modal consistency. By\nconcatenating auditory and visual information, UniForm learns to generate audio\nand video simultaneously within a unified latent space, facilitating the\ncreation of high-quality and well-aligned audio-visual pairs. Extensive\nexperiments demonstrate the superior performance of our method in joint\naudio-video generation, audio-guided video generation, and video-guided audio\ngeneration tasks. Our demos are available at https://uniform-t2av.github.io/.", "published": "2025-02-06 09:18:30", "link": "http://arxiv.org/abs/2502.03897v2", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "XAttnMark: Learning Robust Audio Watermarking with Cross-Attention", "abstract": "The rapid proliferation of generative audio synthesis and editing\ntechnologies has raised significant concerns about copyright infringement, data\nprovenance, and the spread of misinformation through deepfake audio.\nWatermarking offers a proactive solution by embedding imperceptible,\nidentifiable, and traceable marks into audio content. While recent neural\nnetwork-based watermarking methods like WavMark and AudioSeal have improved\nrobustness and quality, they struggle to achieve both robust detection and\naccurate attribution simultaneously. This paper introduces Cross-Attention\nRobust Audio Watermark (XAttnMark), which bridges this gap by leveraging\npartial parameter sharing between the generator and the detector, a\ncross-attention mechanism for efficient message retrieval, and a temporal\nconditioning module for improved message distribution. Additionally, we propose\na psychoacoustic-aligned temporal-frequency masking loss that captures\nfine-grained auditory masking effects, enhancing watermark imperceptibility.\nOur approach achieves state-of-the-art performance in both detection and\nattribution, demonstrating superior robustness against a wide range of audio\ntransformations, including challenging generative editing with strong editing\nstrength. The project webpage is available at\nhttps://liuyixin-louis.github.io/xattnmark/.", "published": "2025-02-06 17:15:08", "link": "http://arxiv.org/abs/2502.04230v2", "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
