{"title": "Automated Phrase Mining from Massive Text Corpora", "abstract": "As one of the fundamental tasks in text analysis, phrase mining aims at\nextracting quality phrases from a text corpus. Phrase mining is important in\nvarious tasks such as information extraction/retrieval, taxonomy construction,\nand topic modeling. Most existing methods rely on complex, trained linguistic\nanalyzers, and thus likely have unsatisfactory performance on text corpora of\nnew domains and genres without extra but expensive adaption. Recently, a few\ndata-driven methods have been developed successfully for extraction of phrases\nfrom massive domain-specific text. However, none of the state-of-the-art models\nis fully automated because they require human experts for designing rules or\nlabeling phrases.\n  Since one can easily obtain many quality phrases from public knowledge bases\nto a scale that is much larger than that produced by human experts, in this\npaper, we propose a novel framework for automated phrase mining, AutoPhrase,\nwhich leverages this large amount of high-quality phrases in an effective way\nand achieves better performance compared to limited human labeled phrases. In\naddition, we develop a POS-guided phrasal segmentation model, which\nincorporates the shallow syntactic information in part-of-speech (POS) tags to\nfurther enhance the performance, when a POS tagger is available. Note that,\nAutoPhrase can support any language as long as a general knowledge base (e.g.,\nWikipedia) in that language is available, while benefiting from, but not\nrequiring, a POS tagger. Compared to the state-of-the-art methods, the new\nmethod has shown significant improvements in effectiveness on five real-world\ndatasets across different domains and languages.", "published": "2017-02-15 03:35:03", "link": "http://arxiv.org/abs/1702.04457v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Deep Learning for Low-Resource Chinese Word Segmentation with a\n  Novel Neural Network", "abstract": "Recent studies have shown effectiveness in using neural networks for Chinese\nword segmentation. However, these models rely on large-scale data and are less\neffective for low-resource datasets because of insufficient training data. We\npropose a transfer learning method to improve low-resource word segmentation by\nleveraging high-resource corpora. First, we train a teacher model on\nhigh-resource corpora and then use the learned knowledge to initialize a\nstudent model. Second, a weighted data similarity method is proposed to train\nthe student model on low-resource data. Experiment results show that our work\nsignificantly improves the performance on low-resource datasets: 2.3% and 1.5%\nF-score on PKU and CTB datasets. Furthermore, this paper achieves\nstate-of-the-art results: 96.1%, and 96.2% F-score on PKU and CTB datasets.", "published": "2017-02-15 07:37:55", "link": "http://arxiv.org/abs/1702.04488v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dependency-Based Neural Reordering Model for Statistical Machine\n  Translation", "abstract": "In machine translation (MT) that involves translating between two languages\nwith significant differences in word order, determining the correct word order\nof translated words is a major challenge. The dependency parse tree of a source\nsentence can help to determine the correct word order of the translated words.\nIn this paper, we present a novel reordering approach utilizing a neural\nnetwork and dependency-based embeddings to predict whether the translations of\ntwo source words linked by a dependency relation should remain in the same\norder or should be swapped in the translated sentence. Experiments on\nChinese-to-English translation show that our approach yields a statistically\nsignificant improvement of 0.57 BLEU point on benchmark NIST test sets,\ncompared to our prior state-of-the-art statistical MT system that uses sparse\ndependency-based reordering features.", "published": "2017-02-15 09:08:21", "link": "http://arxiv.org/abs/1702.04510v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Deep Learning Performance through an Examination of Test\n  Set Difficulty: A Psychometric Case Study", "abstract": "Interpreting the performance of deep learning models beyond test set accuracy\nis challenging. Characteristics of individual data points are often not\nconsidered during evaluation, and each data point is treated equally. We\nexamine the impact of a test set question's difficulty to determine if there is\na relationship between difficulty and performance. We model difficulty using\nwell-studied psychometric methods on human response patterns. Experiments on\nNatural Language Inference (NLI) and Sentiment Analysis (SA) show that the\nlikelihood of answering a question correctly is impacted by the question's\ndifficulty. As DNNs are trained with more data, easy examples are learned more\nquickly than hard examples.", "published": "2017-02-15 23:04:09", "link": "http://arxiv.org/abs/1702.04811v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Frustratingly Short Attention Spans in Neural Language Modeling", "abstract": "Neural language models predict the next token using a latent representation\nof the immediate token history. Recently, various methods for augmenting neural\nlanguage models with an attention mechanism over a differentiable memory have\nbeen proposed. For predicting the next token, these models query information\nfrom a memory of the recent history which can facilitate learning mid- and\nlong-range dependencies. However, conventional attention mechanisms used in\nmemory-augmented neural language models produce a single output vector per time\nstep. This vector is used both for predicting the next token as well as for the\nkey and value of a differentiable memory of a token history. In this paper, we\npropose a neural language model with a key-value attention mechanism that\noutputs separate representations for the key and value of a differentiable\nmemory, as well as for encoding the next-word distribution. This model\noutperforms existing memory-augmented neural language models on two corpora.\nYet, we found that our method mainly utilizes a memory of the five most recent\noutput representations. This led to the unexpected main finding that a much\nsimpler model based only on the concatenation of recent output representations\nfrom previous time steps is on par with more sophisticated memory-augmented\nneural language models.", "published": "2017-02-15 09:45:23", "link": "http://arxiv.org/abs/1702.04521v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Training Language Models Using Target-Propagation", "abstract": "While Truncated Back-Propagation through Time (BPTT) is the most popular\napproach to training Recurrent Neural Networks (RNNs), it suffers from being\ninherently sequential (making parallelization difficult) and from truncating\ngradient flow between distant time-steps. We investigate whether Target\nPropagation (TPROP) style approaches can address these shortcomings.\nUnfortunately, extensive experiments suggest that TPROP generally underperforms\nBPTT, and we end with an analysis of this phenomenon, and suggestions for\nfuture work.", "published": "2017-02-15 20:56:30", "link": "http://arxiv.org/abs/1702.04770v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
