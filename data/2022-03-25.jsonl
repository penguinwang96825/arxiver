{"title": "GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate\n  Degradation of Artificial Neural Language Models", "abstract": "Deep learning (DL) techniques involving fine-tuning large numbers of model\nparameters have delivered impressive performance on the task of discriminating\nbetween language produced by cognitively healthy individuals, and those with\nAlzheimer's disease (AD). However, questions remain about their ability to\ngeneralize beyond the small reference sets that are publicly available for\nresearch. As an alternative to fitting model parameters directly, we propose a\nnovel method by which a Transformer DL model (GPT-2) pre-trained on general\nEnglish text is paired with an artificially degraded version of itself (GPT-D),\nto compute the ratio between these two models' \\textit{perplexities} on\nlanguage from cognitively healthy and impaired individuals. This technique\napproaches state-of-the-art performance on text data from a widely used \"Cookie\nTheft\" picture description task, and unlike established alternatives also\ngeneralizes well to spontaneous conversations. Furthermore, GPT-D generates\ntext with characteristics known to be associated with AD, demonstrating the\ninduction of dementia-related linguistic anomalies. Our study is a step toward\nbetter understanding of the relationships between the inner workings of\ngenerative neural language models, the language that they produce, and the\ndeleterious effects of dementia on human speech and language characteristics.", "published": "2022-03-25 00:25:42", "link": "http://arxiv.org/abs/2203.13397v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Plagiarism Detection in the Bengali Language: A Text Similarity-Based\n  Approach", "abstract": "Plagiarism means taking another person's work and not giving any credit to\nthem for it. Plagiarism is one of the most serious problems in academia and\namong researchers. Even though there are multiple tools available to detect\nplagiarism in a document but most of them are domain-specific and designed to\nwork in English texts, but plagiarism is not limited to a single language only.\nBengali is the most widely spoken language of Bangladesh and the second most\nspoken language in India with 300 million native speakers and 37 million\nsecond-language speakers. Plagiarism detection requires a large corpus for\ncomparison. Bengali Literature has a history of 1300 years. Hence most Bengali\nLiterature books are not yet digitalized properly. As there was no such corpus\npresent for our purpose so we have collected Bengali Literature books from the\nNational Digital Library of India and with a comprehensive methodology\nextracted texts from it and constructed our corpus. Our experimental results\nfind out average accuracy between 72.10 % - 79.89 % in text extraction using\nOCR. Levenshtein Distance algorithm is used for determining Plagiarism. We have\nbuilt a web application for end-user and successfully tested it for Plagiarism\ndetection in Bengali texts. In future, we aim to construct a corpus with more\nbooks for more accurate detection.", "published": "2022-03-25 03:11:00", "link": "http://arxiv.org/abs/2203.13430v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Striking a Balance: Alleviating Inconsistency in Pre-trained Models for\n  Symmetric Classification Tasks", "abstract": "While fine-tuning pre-trained models for downstream classification is the\nconventional paradigm in NLP, often task-specific nuances may not get captured\nin the resultant models. Specifically, for tasks that take two inputs and\nrequire the output to be invariant of the order of the inputs, inconsistency is\noften observed in the predicted labels or confidence scores. We highlight this\nmodel shortcoming and apply a consistency loss function to alleviate\ninconsistency in symmetric classification. Our results show an improved\nconsistency in predictions for three paraphrase detection datasets without a\nsignificant drop in the accuracy scores. We examine the classification\nperformance of six datasets (both symmetric and non-symmetric) to showcase the\nstrengths and limitations of our approach.", "published": "2022-03-25 07:55:39", "link": "http://arxiv.org/abs/2203.13491v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Single Model Ensemble for Subword Regularized Models in Low-Resource\n  Machine Translation", "abstract": "Subword regularizations use multiple subword segmentations during training to\nimprove the robustness of neural machine translation models. In previous\nsubword regularizations, we use multiple segmentations in the training process\nbut use only one segmentation in the inference. In this study, we propose an\ninference strategy to address this discrepancy. The proposed strategy\napproximates the marginalized likelihood by using multiple segmentations\nincluding the most plausible segmentation and several sampled segmentations.\nBecause the proposed strategy aggregates predictions from several\nsegmentations, we can regard it as a single model ensemble that does not\nrequire any additional cost for training. Experimental results show that the\nproposed strategy improves the performance of models trained with subword\nregularization in low-resource machine translation tasks.", "published": "2022-03-25 09:25:47", "link": "http://arxiv.org/abs/2203.13528v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Target-Side Morphology in Neural Machine Translation: A\n  Comparison of Strategies", "abstract": "Morphologically rich languages pose difficulties to machine translation.\nMachine translation engines that rely on statistical learning from parallel\ntraining data, such as state-of-the-art neural systems, face challenges\nespecially with rich morphology on the output language side. Key challenges of\nrich target-side morphology in data-driven machine translation include: (1) A\nlarge amount of differently inflected word surface forms entails a larger\nvocabulary and thus data sparsity. (2) Some inflected forms of infrequent terms\ntypically do not appear in the training corpus, which makes closed-vocabulary\nsystems unable to generate these unobserved variants. (3) Linguistic agreement\nrequires the system to correctly match the grammatical categories between\ninflected word forms in the output sentence, both in terms of target-side\nmorpho-syntactic wellformedness and semantic adequacy with respect to the\ninput.\n  In this paper, we re-investigate two target-side linguistic processing\ntechniques: a lemma-tag strategy and a linguistically informed word\nsegmentation strategy. Our experiments are conducted on a English-German\ntranslation task under three training corpus conditions of different\nmagnitudes. We find that a stronger Transformer baseline leaves less room for\nimprovement than a shallow-RNN encoder-decoder model when translating\nin-domain. However, we find that linguistic modeling of target-side morphology\ndoes benefit the Transformer model when the same system is applied to\nout-of-domain input text. We also successfully apply our approach to English to\nCzech translation.", "published": "2022-03-25 10:13:20", "link": "http://arxiv.org/abs/2203.13550v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MISC: A MIxed Strategy-Aware Model Integrating COMET for Emotional\n  Support Conversation", "abstract": "Applying existing methods to emotional support conversation -- which provides\nvaluable assistance to people who are in need -- has two major limitations: (a)\nthey generally employ a conversation-level emotion label, which is too\ncoarse-grained to capture user's instant mental state; (b) most of them focus\non expressing empathy in the response(s) rather than gradually reducing user's\ndistress. To address the problems, we propose a novel model \\textbf{MISC},\nwhich firstly infers the user's fine-grained emotional status, and then\nresponds skillfully using a mixture of strategy. Experimental results on the\nbenchmark dataset demonstrate the effectiveness of our method and reveal the\nbenefits of fine-grained emotion understanding as well as mixed-up strategy\nmodeling. Our code and data could be found in\n\\url{https://github.com/morecry/MISC}.", "published": "2022-03-25 10:32:04", "link": "http://arxiv.org/abs/2203.13560v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ZS4IE: A toolkit for Zero-Shot Information Extraction with simple\n  Verbalizations", "abstract": "The current workflow for Information Extraction (IE) analysts involves the\ndefinition of the entities/relations of interest and a training corpus with\nannotated examples. In this demonstration we introduce a new workflow where the\nanalyst directly verbalizes the entities/relations, which are then used by a\nTextual Entailment model to perform zero-shot IE. We present the design and\nimplementation of a toolkit with a user interface, as well as experiments on\nfour IE tasks that show that the system achieves very good performance at\nzero-shot learning using only 5--15 minutes per type of a user's effort. Our\ndemonstration system is open-sourced at https://github.com/BBN-E/ZS4IE . A\ndemonstration video is available at https://vimeo.com/676138340 .", "published": "2022-03-25 12:02:53", "link": "http://arxiv.org/abs/2203.13602v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Formality Style Transfer with Consistency Training", "abstract": "Formality style transfer (FST) is a task that involves paraphrasing an\ninformal sentence into a formal one without altering its meaning. To address\nthe data-scarcity problem of existing parallel datasets, previous studies tend\nto adopt a cycle-reconstruction scheme to utilize additional unlabeled data,\nwhere the FST model mainly benefits from target-side unlabeled sentences. In\nthis work, we propose a simple yet effective semi-supervised framework to\nbetter utilize source-side unlabeled sentences based on consistency training.\nSpecifically, our approach augments pseudo-parallel data obtained from a\nsource-side informal sentence by enforcing the model to generate similar\noutputs for its perturbed version. Moreover, we empirically examined the\neffects of various data perturbation methods and propose effective data\nfiltering strategies to improve our framework. Experimental results on the\nGYAFC benchmark demonstrate that our approach can achieve state-of-the-art\nresults, even with less than 40% of the parallel data.", "published": "2022-03-25 12:40:36", "link": "http://arxiv.org/abs/2203.13620v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Pre-Trained Language Models for Cross-Cultural Differences in\n  Values", "abstract": "Language embeds information about social, cultural, and political values\npeople hold. Prior work has explored social and potentially harmful biases\nencoded in Pre-Trained Language models (PTLMs). However, there has been no\nsystematic study investigating how values embedded in these models vary across\ncultures. In this paper, we introduce probes to study which values across\ncultures are embedded in these models, and whether they align with existing\ntheories and cross-cultural value surveys. We find that PTLMs capture\ndifferences in values across cultures, but those only weakly align with\nestablished value surveys. We discuss implications of using mis-aligned models\nin cross-cultural settings, as well as ways of aligning PTLMs with value\nsurveys.", "published": "2022-03-25 15:45:49", "link": "http://arxiv.org/abs/2203.13722v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AUTOLEX: An Automatic Framework for Linguistic Exploration", "abstract": "Each language has its own complex systems of word, phrase, and sentence\nconstruction, the guiding principles of which are often summarized in grammar\ndescriptions for the consumption of linguists or language learners. However,\nmanual creation of such descriptions is a fraught process, as creating\ndescriptions which describe the language in \"its own terms\" without bias or\nerror requires both a deep understanding of the language at hand and\nlinguistics as a whole. We propose an automatic framework AutoLEX that aims to\nease linguists' discovery and extraction of concise descriptions of linguistic\nphenomena. Specifically, we apply this framework to extract descriptions for\nthree phenomena: morphological agreement, case marking, and word order, across\nseveral languages. We evaluate the descriptions with the help of language\nexperts and propose a method for automated evaluation when human evaluation is\ninfeasible.", "published": "2022-03-25 20:37:30", "link": "http://arxiv.org/abs/2203.13901v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What is wrong with you?: Leveraging User Sentiment for Automatic Dialog\n  Evaluation", "abstract": "Accurate automatic evaluation metrics for open-domain dialogs are in high\ndemand. Existing model-based metrics for system response evaluation are trained\non human annotated data, which is cumbersome to collect. In this work, we\npropose to use information that can be automatically extracted from the next\nuser utterance, such as its sentiment or whether the user explicitly ends the\nconversation, as a proxy to measure the quality of the previous system\nresponse. This allows us to train on a massive set of dialogs with weak\nsupervision, without requiring manual system turn quality annotations.\nExperiments show that our model is comparable to models trained on human\nannotated data. Furthermore, our model generalizes across both spoken and\nwritten open-domain dialog corpora collected from real and paid users.", "published": "2022-03-25 22:09:52", "link": "http://arxiv.org/abs/2203.13927v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Intrinsic and Extrinsic Fairness Evaluation Metrics for\n  Contextualized Language Representations", "abstract": "Multiple metrics have been introduced to measure fairness in various natural\nlanguage processing tasks. These metrics can be roughly categorized into two\ncategories: 1) \\emph{extrinsic metrics} for evaluating fairness in downstream\napplications and 2) \\emph{intrinsic metrics} for estimating fairness in\nupstream contextualized language representation models. In this paper, we\nconduct an extensive correlation study between intrinsic and extrinsic metrics\nacross bias notions using 19 contextualized language models. We find that\nintrinsic and extrinsic metrics do not necessarily correlate in their original\nsetting, even when correcting for metric misalignments, noise in evaluation\ndatasets, and confounding factors such as experiment configuration for\nextrinsic metrics. %al", "published": "2022-03-25 22:17:43", "link": "http://arxiv.org/abs/2203.13928v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UKP-SQUARE: An Online Platform for Question Answering Research", "abstract": "Recent advances in NLP and information retrieval have given rise to a diverse\nset of question answering tasks that are of different formats (e.g.,\nextractive, abstractive), require different model architectures (e.g.,\ngenerative, discriminative), and setups (e.g., with or without retrieval).\nDespite having a large number of powerful, specialized QA pipelines (which we\nrefer to as Skills) that consider a single domain, model or setup, there exists\nno framework where users can easily explore and compare such pipelines and can\nextend them according to their needs. To address this issue, we present\nUKP-SQUARE, an extensible online QA platform for researchers which allows users\nto query and analyze a large collection of modern Skills via a user-friendly\nweb interface and integrated behavioural tests. In addition, QA researchers can\ndevelop, manage, and share their custom Skills using our microservices that\nsupport a wide range of models (Transformers, Adapters, ONNX), datastores and\nretrieval techniques (e.g., sparse and dense). UKP-SQUARE is available on\nhttps://square.ukp-lab.de.", "published": "2022-03-25 15:00:24", "link": "http://arxiv.org/abs/2203.13693v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "L3Cube-MahaHate: A Tweet-based Marathi Hate Speech Detection Dataset and\n  BERT models", "abstract": "Social media platforms are used by a large number of people prominently to\nexpress their thoughts and opinions. However, these platforms have contributed\nto a substantial amount of hateful and abusive content as well. Therefore, it\nis important to curb the spread of hate speech on these platforms. In India,\nMarathi is one of the most popular languages used by a wide audience. In this\nwork, we present L3Cube-MahaHate, the first major Hate Speech Dataset in\nMarathi. The dataset is curated from Twitter, annotated manually. Our dataset\nconsists of over 25000 distinct tweets labeled into four major classes i.e\nhate, offensive, profane, and not. We present the approaches used for\ncollecting and annotating the data and the challenges faced during the process.\nFinally, we present baseline classification results using deep learning models\nbased on CNN, LSTM, and Transformers. We explore mono-lingual and multi-lingual\nvariants of BERT like MahaBERT, IndicBERT, mBERT, and xlm-RoBERTa and show that\nmono-lingual models perform better than their multi-lingual counterparts. The\nMahaBERT model provides the best results on L3Cube-MahaHate Corpus. The data\nand models are available at https://github.com/l3cube-pune/MarathiNLP .", "published": "2022-03-25 17:00:33", "link": "http://arxiv.org/abs/2203.13778v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Data Selection Curriculum for Neural Machine Translation", "abstract": "Neural Machine Translation (NMT) models are typically trained on\nheterogeneous data that are concatenated and randomly shuffled. However, not\nall of the training data are equally useful to the model. Curriculum training\naims to present the data to the NMT models in a meaningful order. In this work,\nwe introduce a two-stage curriculum training framework for NMT where we\nfine-tune a base NMT model on subsets of data, selected by both deterministic\nscoring using pre-trained methods and online scoring that considers prediction\nscores of the emerging NMT model. Through comprehensive experiments on six\nlanguage pairs comprising low- and high-resource languages from WMT'21, we have\nshown that our curriculum strategies consistently demonstrate better quality\n(up to +2.2 BLEU improvement) and faster convergence (approximately 50% fewer\nupdates).", "published": "2022-03-25 19:08:30", "link": "http://arxiv.org/abs/2203.13867v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues", "abstract": "This paper addresses the problem of dialogue reasoning with contextualized\ncommonsense inference. We curate CICERO, a dataset of dyadic conversations with\nfive types of utterance-level reasoning-based inferences: cause, subsequent\nevent, prerequisite, motivation, and emotional reaction. The dataset contains\n53,105 of such inferences from 5,672 dialogues. We use this dataset to solve\nrelevant generative and discriminative tasks: generation of cause and\nsubsequent event; generation of prerequisite, motivation, and listener's\nemotional reaction; and selection of plausible alternatives. Our results\nascertain the value of such dialogue-centric commonsense knowledge datasets. It\nis our hope that CICERO will open new research avenues into commonsense-based\ndialogue reasoning.", "published": "2022-03-25 22:08:50", "link": "http://arxiv.org/abs/2203.13926v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Predicting Clinical Intent from Free Text Electronic Health Records", "abstract": "After a patient consultation, a clinician determines the steps in the\nmanagement of the patient. A clinician may for example request to see the\npatient again or refer them to a specialist. Whilst most clinicians will record\ntheir intent as \"next steps\" in the patient's clinical notes, in some cases the\nclinician may forget to indicate their intent as an order or request, e.g.\nfailure to place the follow-up order. This consequently results in patients\nbecoming lost-to-follow up and may in some cases lead to adverse consequences.\nIn this paper we train a machine learning model to detect a clinician's intent\nto follow up with a patient from the patient's clinical notes. Annotators\nsystematically identified 22 possible types of clinical intent and annotated\n3000 Bariatric clinical notes. The annotation process revealed a class\nimbalance in the labeled data and we found that there was only sufficient\nlabeled data to train 11 out of the 22 intents. We used the data to train a\nBERT based multilabel classification model and reported the following average\naccuracy metrics for all intents: macro-precision: 0.91, macro-recall: 0.90,\nmacro-f1: 0.90.", "published": "2022-03-25 04:27:00", "link": "http://arxiv.org/abs/2204.09594v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Song Translation for Tonal Languages", "abstract": "This paper develops automatic song translation (AST) for tonal languages and\naddresses the unique challenge of aligning words' tones with melody of a song\nin addition to conveying the original meaning. We propose three criteria for\neffective AST -- preserving meaning, singability and intelligibility -- and\ndesign metrics for these criteria. We develop a new benchmark for\nEnglish--Mandarin song translation and develop an unsupervised AST system,\nGuided AliGnment for Automatic Song Translation (GagaST), which combines\npre-training with three decoding constraints. Both automatic and human\nevaluations show GagaST successfully balances semantics and singability.", "published": "2022-03-25 02:25:33", "link": "http://arxiv.org/abs/2203.13420v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program\n  Synthesis", "abstract": "Program synthesis strives to generate a computer program as a solution to a\ngiven problem specification, expressed with input-output examples or natural\nlanguage descriptions. The prevalence of large language models advances the\nstate-of-the-art for program synthesis, though limited training resources and\ndata impede open access to such models. To democratize this, we train and\nrelease a family of large language models up to 16.1B parameters, called\nCODEGEN, on natural language and programming language data, and open source the\ntraining library JAXFORMER. We show the utility of the trained model by\ndemonstrating that it is competitive with the previous state-of-the-art on\nzero-shot Python code generation on HumanEval. We further investigate the\nmulti-step paradigm for program synthesis, where a single program is factorized\ninto multiple prompts specifying subproblems. To this end, we construct an open\nbenchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse\nproblem sets that are factorized into multi-turn prompts. Our analysis on MTPB\nshows that the same intent provided to CODEGEN in multi-turn fashion\nsignificantly improves program synthesis over that provided as a single turn.\nWe make the training library JAXFORMER and model checkpoints available as open\nsource contribution: https://github.com/salesforce/CodeGen.", "published": "2022-03-25 06:55:15", "link": "http://arxiv.org/abs/2203.13474v5", "categories": ["cs.LG", "cs.CL", "cs.PL"], "primary_category": "cs.LG"}
{"title": "EmoCaps: Emotion Capsule based Model for Conversational Emotion\n  Recognition", "abstract": "Emotion recognition in conversation (ERC) aims to analyze the speaker's state\nand identify their emotion in the conversation. Recent works in ERC focus on\ncontext modeling but ignore the representation of contextual emotional\ntendency. In order to extract multi-modal information and the emotional\ntendency of the utterance effectively, we propose a new structure named\nEmoformer to extract multi-modal emotion vectors from different modalities and\nfuse them with sentence vector to be an emotion capsule. Furthermore, we design\nan end-to-end ERC model called EmoCaps, which extracts emotion vectors through\nthe Emoformer structure and obtain the emotion classification results from a\ncontext analysis model. Through the experiments with two benchmark datasets,\nour model shows better performance than the existing state-of-the-art models.", "published": "2022-03-25 08:42:57", "link": "http://arxiv.org/abs/2203.13504v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Impact of Dataset on Acoustic Models for Automatic Speech Recognition", "abstract": "In Automatic Speech Recognition, GMM-HMM had been widely used for acoustic\nmodelling. With the current advancement of deep learning, the Gaussian Mixture\nModel (GMM) from acoustic models has been replaced with Deep Neural Network,\nnamely DNN-HMM Acoustic Models. The GMM models are widely used to create the\nalignments of the training data for the hybrid deep neural network model, thus\nmaking it an important task to create accurate alignments. Many factors such as\ntraining dataset size, training data augmentation, model hyperparameters, etc.,\naffect the model learning. Traditionally in machine learning, larger datasets\ntend to have better performance, while smaller datasets tend to trigger\nover-fitting. The collection of speech data and their accurate transcriptions\nis a significant challenge that varies over different languages, and in most\ncases, it might be limited to big organizations. Moreover, in the case of\navailable large datasets, training a model using such data requires additional\ntime and computing resources, which may not be available. While the data about\nthe accuracy of state-of-the-art ASR models on open-source datasets are\npublished, the study about the impact of the size of a dataset on acoustic\nmodels is not readily available. This work aims to investigate the impact of\ndataset size variations on the performance of various GMM-HMM Acoustic Models\nand their respective computational costs.", "published": "2022-03-25 11:41:49", "link": "http://arxiv.org/abs/2203.13590v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "DeLoRes: Decorrelating Latent Spaces for Low-Resource Audio\n  Representation Learning", "abstract": "Inspired by the recent progress in self-supervised learning for computer\nvision, in this paper we introduce DeLoRes, a new general-purpose audio\nrepresentation learning approach. Our main objective is to make our network\nlearn representations in a resource-constrained setting (both data and\ncompute), that can generalize well across a diverse set of downstream tasks.\nInspired from the Barlow Twins objective function, we propose to learn\nembeddings that are invariant to distortions of an input audio sample, while\nmaking sure that they contain non-redundant information about the sample. To\nachieve this, we measure the cross-correlation matrix between the outputs of\ntwo identical networks fed with distorted versions of an audio segment sampled\nfrom an audio file and make it as close to the identity matrix as possible. We\nuse a combination of a small subset of the large-scale AudioSet dataset and\nFSD50K for self-supervised learning and are able to learn with less than half\nthe parameters compared to state-of-the-art algorithms. For evaluation, we\ntransfer these learned representations to 9 downstream classification tasks,\nincluding speech, music, and animal sounds, and show competitive results under\ndifferent evaluation setups. In addition to being simple and intuitive, our\npre-training algorithm is amenable to compute through its inherent nature of\nconstruction and does not require careful implementation details to avoid\ntrivial or degenerate solutions. Furthermore, we conduct ablation studies on\nour results and make all our code and pre-trained models publicly available\nhttps://github.com/Speech-Lab-IITM/DeLoRes.", "published": "2022-03-25 12:59:55", "link": "http://arxiv.org/abs/2203.13628v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-text Retrieval in Context", "abstract": "Audio-text retrieval based on natural language descriptions is a challenging\ntask. It involves learning cross-modality alignments between long sequences\nunder inadequate data conditions. In this work, we investigate several audio\nfeatures as well as sequence aggregation methods for better audio-text\nalignment. Moreover, through a qualitative analysis we observe that semantic\nmapping is more important than temporal relations in contextual retrieval.\nUsing pre-trained audio features and a descriptor-based aggregation method, we\nbuild our contextual audio-text retrieval system. Specifically, we utilize\nPANNs features pre-trained on a large sound event dataset and NetRVLAD pooling,\nwhich directly works with averaged descriptors. Experiments are conducted on\nthe AudioCaps and CLOTHO datasets, and results are compared with the previous\nstate-of-the-art system. With our proposed system, a significant improvement\nhas been achieved on bidirectional audio-text retrieval, on all metrics\nincluding recall, median and mean rank.", "published": "2022-03-25 13:41:17", "link": "http://arxiv.org/abs/2203.13645v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning to Mediate Disparities Towards Pragmatic Communication", "abstract": "Human communication is a collaborative process. Speakers, on top of conveying\ntheir own intent, adjust the content and language expressions by taking the\nlisteners into account, including their knowledge background, personalities,\nand physical capabilities. Towards building AI agents with similar abilities in\nlanguage communication, we propose Pragmatic Rational Speaker (PRS), a\nframework extending Rational Speech Act (RSA). The PRS attempts to learn the\nspeaker-listener disparity and adjust the speech accordingly, by adding a\nlight-weighted disparity adjustment layer into working memory on top of\nspeaker's long-term memory system. By fixing the long-term memory, the PRS only\nneeds to update its working memory to learn and adapt to different types of\nlisteners. To validate our framework, we create a dataset that simulates\ndifferent types of speaker-listener disparities in the context of referential\ngames. Our empirical results demonstrate that the PRS is able to shift its\noutput towards the language that listener are able to understand, significantly\nimprove the collaborative task outcome.", "published": "2022-03-25 14:46:43", "link": "http://arxiv.org/abs/2203.13685v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Analyzing Generalization of Vision and Language Navigation to Unseen\n  Outdoor Areas", "abstract": "Vision and language navigation (VLN) is a challenging visually-grounded\nlanguage understanding task. Given a natural language navigation instruction, a\nvisual agent interacts with a graph-based environment equipped with panorama\nimages and tries to follow the described route. Most prior work has been\nconducted in indoor scenarios where best results were obtained for navigation\non routes that are similar to the training routes, with sharp drops in\nperformance when testing on unseen environments. We focus on VLN in outdoor\nscenarios and find that in contrast to indoor VLN, most of the gain in outdoor\nVLN on unseen data is due to features like junction type embedding or heading\ndelta that are specific to the respective environment graph, while image\ninformation plays a very minor role in generalizing VLN to unseen outdoor\nareas. These findings show a bias to specifics of graph representations of\nurban environments, demanding that VLN tasks grow in scale and diversity of\ngeographical environments.", "published": "2022-03-25 18:06:14", "link": "http://arxiv.org/abs/2203.13838v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Canary Extraction in Natural Language Understanding Models", "abstract": "Natural Language Understanding (NLU) models can be trained on sensitive\ninformation such as phone numbers, zip-codes etc. Recent literature has focused\non Model Inversion Attacks (ModIvA) that can extract training data from model\nparameters. In this work, we present a version of such an attack by extracting\ncanaries inserted in NLU training data. In the attack, an adversary with\nopen-box access to the model reconstructs the canaries contained in the model's\ntraining set. We evaluate our approach by performing text completion on\ncanaries and demonstrate that by using the prefix (non-sensitive) tokens of the\ncanary, we can generate the full canary. As an example, our attack is able to\nreconstruct a four digit code in the training dataset of the NLU model with a\nprobability of 0.5 in its best configuration. As countermeasures, we identify\nseveral defense mechanisms that, when combined, effectively eliminate the risk\nof ModIvA in our experiments.", "published": "2022-03-25 21:47:59", "link": "http://arxiv.org/abs/2203.13920v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Probabilistic Embeddings with Laplacian Graph Priors", "abstract": "We introduce probabilistic embeddings using Laplacian priors (PELP). The\nproposed model enables incorporating graph side-information into static word\nembeddings. We theoretically show that the model unifies several previously\nproposed embedding methods under one umbrella. PELP generalises graph-enhanced,\ngroup, dynamic, and cross-lingual static word embeddings. PELP also enables any\ncombination of these previous models in a straightforward fashion. Furthermore,\nwe empirically show that our model matches the performance of previous models\nas special cases. In addition, we demonstrate its flexibility by applying it to\nthe comparison of political sociolects over time. Finally, we provide code as a\nTensorFlow implementation enabling flexible estimation in different settings.", "published": "2022-03-25 13:33:51", "link": "http://arxiv.org/abs/2204.01846v1", "categories": ["cs.CL", "cs.LG", "stat.ME", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Comparative Evaluation Of Transformer Models For De-Identification Of\n  Clinical Text Data", "abstract": "Objective: To comparatively evaluate several transformer model architectures\nat identifying protected health information (PHI) in the i2b2/UTHealth 2014\nclinical text de-identification challenge corpus.\n  Methods: The i2b2/UTHealth 2014 corpus contains N=1304 clinical notes\nobtained from N=296 patients. Using a transfer learning framework, we fine-tune\nseveral transformer model architectures on the corpus, including: BERT-base,\nBERT-large, ROBERTA-base, ROBERTA-large, ALBERT-base and ALBERT-xxlarge. During\nfine-tuning we vary the following model hyper-parameters: batch size, number\ntraining epochs, learning rate and weight decay. We fine tune models on a\ntraining data set, we evaluate and select optimally performing models on an\nindependent validation dataset, and lastly assess generalization performance on\na held-out test dataset. We assess model performance in terms of accuracy,\nprecision (positive predictive value), recall (sensitivity) and F1 score\n(harmonic mean of precision and recall). We are interested in overall model\nperformance (PHI identified vs. PHI not identified), as well as PHI-specific\nmodel performance.\n  Results: We observe that the ROBERTA-large models perform best at identifying\nPHI in the i2b2/UTHealth 2014 corpus, achieving >99% overall accuracy and 96.7%\nrecall/precision on the heldout test corpus. Performance was good across many\nPHI classes; however, accuracy/precision/recall decreased for identification of\nthe following entity classes: professions, organizations, ages, and certain\nlocations.\n  Conclusions: Transformers are a promising model class/architecture for\nclinical text de-identification. With minimal hyper-parameter tuning\ntransformers afford researchers/clinicians the opportunity to obtain (near)\nstate-of-the-art performance.", "published": "2022-03-25 19:42:03", "link": "http://arxiv.org/abs/2204.07056v1", "categories": ["cs.CL", "stat.AP", "stat.CO"], "primary_category": "cs.CL"}
{"title": "hate-alert@DravidianLangTech-ACL2022: Ensembling Multi-Modalities for\n  Tamil TrollMeme Classification", "abstract": "Social media platforms often act as breeding grounds for various forms of\ntrolling or malicious content targeting users or communities. One way of\ntrolling users is by creating memes, which in most cases unites an image with a\nshort piece of text embedded on top of it. The situation is more complex for\nmultilingual(e.g., Tamil) memes due to the lack of benchmark datasets and\nmodels. We explore several models to detect Troll memes in Tamil based on the\nshared task, \"Troll Meme Classification in DravidianLangTech2022\" at ACL-2022.\nWe observe while the text-based model MURIL performs better for Non-troll meme\nclassification, the image-based model VGG16 performs better for Troll-meme\nclassification. Further fusing these two modalities help us achieve stable\noutcomes in both classes. Our fusion model achieved a 0.561 weighted average F1\nscore and ranked second in this task.", "published": "2022-03-25 17:53:39", "link": "http://arxiv.org/abs/2204.12587v1", "categories": ["cs.MM", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.MM"}
{"title": "Chain-based Discriminative Autoencoders for Speech Recognition", "abstract": "In our previous work, we proposed a discriminative autoencoder (DcAE) for\nspeech recognition. DcAE combines two training schemes into one. First, since\nDcAE aims to learn encoder-decoder mappings, the squared error between the\nreconstructed speech and the input speech is minimized. Second, in the code\nlayer, frame-based phonetic embeddings are obtained by minimizing the\ncategorical cross-entropy between ground truth labels and predicted\ntriphone-state scores. DcAE is developed based on the Kaldi toolkit by treating\nvarious TDNN models as encoders. In this paper, we further propose three new\nversions of DcAE. First, a new objective function that considers both\ncategorical cross-entropy and mutual information between ground truth and\npredicted triphone-state sequences is used. The resulting DcAE is called a\nchain-based DcAE (c-DcAE). For application to robust speech recognition, we\nfurther extend c-DcAE to hierarchical and parallel structures, resulting in\nhc-DcAE and pc-DcAE. In these two models, both the error between the\nreconstructed noisy speech and the input noisy speech and the error between the\nenhanced speech and the reference clean speech are taken into the objective\nfunction. Experimental results on the WSJ and Aurora-4 corpora show that our\nDcAE models outperform baseline systems.", "published": "2022-03-25 14:51:48", "link": "http://arxiv.org/abs/2203.13687v3", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech-enhanced and Noise-aware Networks for Robust Speech Recognition", "abstract": "Compensation for channel mismatch and noise interference is essential for\nrobust automatic speech recognition. Enhanced speech has been introduced into\nthe multi-condition training of acoustic models to improve their generalization\nability. In this paper, a noise-aware training framework based on two cascaded\nneural structures is proposed to jointly optimize speech enhancement and speech\nrecognition. The feature enhancement module is composed of a multi-task\nautoencoder, where noisy speech is decomposed into clean speech and noise. By\nconcatenating its enhanced, noise-aware, and noisy features for each frame, the\nacoustic-modeling module maps each feature-augmented frame into a triphone\nstate by optimizing the lattice-free maximum mutual information and cross\nentropy between the predicted and actual state sequences. On top of the\nfactorized time delay neural network (TDNN-F) and its convolutional variant\n(CNN-TDNNF), both with SpecAug, the two proposed systems achieve word error\nrate (WER) of 3.90% and 3.55%, respectively, on the Aurora-4 task. Compared\nwith the best existing systems that use bigram and trigram language models for\ndecoding, the proposed CNN-TDNNF-based system achieves a relative WER reduction\nof 15.20% and 33.53%, respectively. In addition, the proposed CNN-TDNNF-based\nsystem also outperforms the baseline CNN-TDNNF system on the AMI task.", "published": "2022-03-25 15:04:51", "link": "http://arxiv.org/abs/2203.13696v3", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pseudo-Label Transfer from Frame-Level to Note-Level in a\n  Teacher-Student Framework for Singing Transcription from Polyphonic Music", "abstract": "Lack of large-scale note-level labeled data is the major obstacle to singing\ntranscription from polyphonic music. We address the issue by using pseudo\nlabels from vocal pitch estimation models given unlabeled data. The proposed\nmethod first converts the frame-level pseudo labels to note-level through pitch\nand rhythm quantization steps. Then, it further improves the label quality\nthrough self-training in a teacher-student framework. To validate the method,\nwe conduct various experiment settings by investigating two vocal pitch\nestimation models as pseudo-label generators, two setups of teacher-student\nframeworks, and the number of iterations in self-training. The results show\nthat the proposed method can effectively leverage large-scale unlabeled audio\ndata and self-training with the noisy student model helps to improve\nperformance. Finally, we show that the model trained with only unlabeled data\nhas comparable performance to previous works and the model trained with\nadditional labeled data achieves higher accuracy than the model trained with\nonly labeled data.", "published": "2022-03-25 02:29:25", "link": "http://arxiv.org/abs/2203.13422v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AudioTagging Done Right: 2nd comparison of deep learning methods for\n  environmental sound classification", "abstract": "After its sweeping success in vision and language tasks, pure attention-based\nneural architectures (e.g. DeiT) are emerging to the top of audio tagging (AT)\nleaderboards, which seemingly obsoletes traditional convolutional neural\nnetworks (CNNs), feed-forward networks or recurrent networks. However, taking a\ncloser look, there is great variability in published research, for instance,\nperformances of models initialized with pretrained weights differ drastically\nfrom without pretraining, training time for a model varies from hours to weeks,\nand often, essences are hidden in seemingly trivial details. This urgently\ncalls for a comprehensive study since our 1st comparison is half-decade old. In\nthis work, we perform extensive experiments on AudioSet which is the largest\nweakly-labeled sound event dataset available, we also did an analysis based on\nthe data quality and efficiency. We compare a few state-of-the-art baselines on\nthe AT task, and study the performance and efficiency of 2 major categories of\nneural architectures: CNN variants and attention-based variants. We also\nclosely examine their optimization procedures. Our opensourced experimental\nresults provide insights to trade-off between performance, efficiency,\noptimization process, for both practitioners and researchers. Implementation:\nhttps://github.com/lijuncheng16/AudioTaggingDoneRight", "published": "2022-03-25 05:03:31", "link": "http://arxiv.org/abs/2203.13448v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Embedding Recurrent Layers with Dual-Path Strategy in a Variant of\n  Convolutional Network for Speaker-Independent Speech Separation", "abstract": "Speaker-independent speech separation has achieved remarkable performance in\nrecent years with the development of deep neural network (DNN). Various network\narchitectures, from traditional convolutional neural network (CNN) and\nrecurrent neural network (RNN) to advanced transformer, have been designed\nsophistically to improve separation performance. However, the state-of-the-art\nmodels usually suffer from several flaws related to the computation, such as\nlarge model size, huge memory consumption and computational complexity. To find\nthe balance between the performance and computational efficiency and to further\nexplore the modeling ability of traditional network structure, we combine RNN\nand a newly proposed variant of convolutional network to cope with speech\nseparation problem. By embedding two RNNs into basic block of this variant with\nthe help of dual-path strategy, the proposed network can effectively learn the\nlocal information and global dependency. Besides, a four-staged structure\nenables the separation procedure to be performed gradually at finer and finer\nscales as the feature dimension increases. The experimental results on various\ndatasets have proven the effectiveness of the proposed method and shown that a\ntrade-off between the separation performance and computational efficiency is\nwell achieved.", "published": "2022-03-25 11:01:52", "link": "http://arxiv.org/abs/2203.13574v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Spatial Processing Front-End For Distant ASR Exploiting Self-Attention\n  Channel Combinator", "abstract": "We present a novel multi-channel front-end based on channel shortening with\ntheWeighted Prediction Error (WPE) method followed by a fixed MVDR beamformer\nused in combination with a recently proposed self-attention-based channel\ncombination (SACC) scheme, for tackling the distant ASR problem. We show that\nthe proposed system used as part of a ContextNet based end-to-end (E2E) ASR\nsystem outperforms leading ASR systems as demonstrated by a 21.6% reduction in\nrelative WER on a multi-channel LibriSpeech playback dataset. We also show how\ndereverberation prior to beamforming is beneficial and compare the WPE method\nwith a modified neural channel shortening approach. An analysis of the\nnon-intrusive estimate of the signal C50 confirms that the 8 channel WPE method\nprovides significant dereverberation of the signals (13.6 dB improvement). We\nalso show how the weights of the SACC system allow the extraction of accurate\nspatial information which can be beneficial for other speech processing\napplications like diarization.", "published": "2022-03-25 21:43:15", "link": "http://arxiv.org/abs/2203.13919v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "WaveFuzz: A Clean-Label Poisoning Attack to Protect Your Voice", "abstract": "People are not always receptive to their voice data being collected and\nmisused. Training the audio intelligence systems needs these data to build\nuseful features, but the cost for getting permissions or purchasing data is\nvery high, which inevitably encourages hackers to collect these voice data\nwithout people's awareness. To discourage the hackers from proactively\ncollecting people's voice data, we are the first to propose a clean-label\npoisoning attack, called WaveFuzz, which can prevent intelligence audio models\nfrom building useful features from protected (poisoned) voice data but still\npreserve the semantic information to the humans. Specifically, WaveFuzz\nperturbs the voice data to cause Mel Frequency Cepstral Coefficients (MFCC)\n(typical representations of audio signals) to generate the poisoned frequency\nfeatures. These poisoned features are then fed to audio prediction models,\nwhich degrades the performance of audio intelligence systems. Empirically, we\nshow the efficacy of WaveFuzz by attacking two representative types of\nintelligent audio systems, i.e., speaker recognition system (SR) and speech\ncommand recognition system (SCR). For example, the accuracies of models are\ndeclined by $19.78\\%$ when only $10\\%$ of the poisoned voice data is to\nfine-tune models, and the accuracies of models declined by $6.07\\%$ when only\n$10\\%$ of the training voice data is poisoned. Consequently, WaveFuzz is an\neffective technique that enables people to fight back to protect their own\nvoice data, which sheds new light on ameliorating privacy issues.", "published": "2022-03-25 08:14:37", "link": "http://arxiv.org/abs/2203.13497v1", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SeCo: Separating Unknown Musical Visual Sounds with Consistency Guidance", "abstract": "Recent years have witnessed the success of deep learning on the visual sound\nseparation task. However, existing works follow similar settings where the\ntraining and testing datasets share the same musical instrument categories,\nwhich to some extent limits the versatility of this task. In this work, we\nfocus on a more general and challenging scenario, namely the separation of\nunknown musical instruments, where the categories in training and testing\nphases have no overlap with each other. To tackle this new setting, we propose\nthe Separation-with-Consistency (SeCo) framework, which can accomplish the\nseparation on unknown categories by exploiting the consistency constraints.\nFurthermore, to capture richer characteristics of the novel melodies, we devise\nan online matching strategy, which can bring stable enhancements with no cost\nof extra parameters. Experiments demonstrate that our SeCo framework exhibits\nstrong adaptation ability on the novel musical categories and outperforms the\nbaseline methods by a significant margin.", "published": "2022-03-25 09:42:11", "link": "http://arxiv.org/abs/2203.13535v1", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "EmotionNAS: Two-stream Neural Architecture Search for Speech Emotion\n  Recognition", "abstract": "Speech emotion recognition (SER) is an important research topic in\nhuman-computer interaction. Existing works mainly rely on human expertise to\ndesign models. Despite their success, different datasets often require distinct\nstructures and hyperparameters. Searching for an optimal model for each dataset\nis time-consuming and labor-intensive. To address this problem, we propose a\ntwo-stream neural architecture search (NAS) based framework, called\n\\enquote{EmotionNAS}. Specifically, we take two-stream features (i.e.,\nhandcrafted and deep features) as the inputs, followed by NAS to search for the\noptimal structure for each stream. Furthermore, we incorporate complementary\ninformation in different streams through an efficient information supplement\nmodule. Experimental results demonstrate that our method outperforms existing\nmanually-designed and NAS-based models, setting the new state-of-the-art\nrecord.", "published": "2022-03-25 12:35:44", "link": "http://arxiv.org/abs/2203.13617v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Cross-Domain Approach for Continuous Impression Recognition from\n  Dyadic Audio-Visual-Physio Signals", "abstract": "The impression we make on others depends not only on what we say, but also,\nto a large extent, on how we say it. As a sub-branch of affective computing and\nsocial signal processing, impression recognition has proven critical in both\nhuman-human conversations and spoken dialogue systems. However, most research\nhas studied impressions only from the signals expressed by the emitter,\nignoring the response from the receiver. In this paper, we perform impression\nrecognition using a proposed cross-domain architecture on the dyadic IMPRESSION\ndataset. This improved architecture makes use of cross-domain attention and\nregularization. The cross-domain attention consists of intra- and\ninter-attention mechanisms, which capture intra- and inter-domain relatedness,\nrespectively. The cross-domain regularization includes knowledge distillation\nand similarity enhancement losses, which strengthen the feature connections\nbetween the emitter and receiver. The experimental evaluation verified the\neffectiveness of our approach. Our approach achieved a concordance correlation\ncoefficient of 0.770 in competence dimension and 0.748 in warmth dimension.", "published": "2022-03-25 22:40:53", "link": "http://arxiv.org/abs/2203.13932v1", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "BDDM: Bilateral Denoising Diffusion Models for Fast and High-Quality\n  Speech Synthesis", "abstract": "Diffusion probabilistic models (DPMs) and their extensions have emerged as\ncompetitive generative models yet confront challenges of efficient sampling. We\npropose a new bilateral denoising diffusion model (BDDM) that parameterizes\nboth the forward and reverse processes with a schedule network and a score\nnetwork, which can train with a novel bilateral modeling objective. We show\nthat the new surrogate objective can achieve a lower bound of the log marginal\nlikelihood tighter than a conventional surrogate. We also find that BDDM allows\ninheriting pre-trained score network parameters from any DPMs and consequently\nenables speedy and stable learning of the schedule network and optimization of\na noise schedule for sampling. Our experiments demonstrate that BDDMs can\ngenerate high-fidelity audio samples with as few as three sampling steps.\nMoreover, compared to other state-of-the-art diffusion-based neural vocoders,\nBDDMs produce comparable or higher quality samples indistinguishable from human\nspeech, notably with only seven sampling steps (143x faster than WaveGrad and\n28.6x faster than DiffWave). We release our code at\nhttps://github.com/tencent-ailab/bddm.", "published": "2022-03-25 08:53:12", "link": "http://arxiv.org/abs/2203.13508v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
