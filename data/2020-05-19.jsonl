{"title": "Matching Questions and Answers in Dialogues from Online Forums", "abstract": "Matching question-answer relations between two turns in conversations is not\nonly the first step in analyzing dialogue structures, but also valuable for\ntraining dialogue systems. This paper presents a QA matching model considering\nboth distance information and dialogue history by two simultaneous attention\nmechanisms called mutual attention. Given scores computed by the trained model\nbetween each non-question turn with its candidate questions, a greedy matching\nstrategy is used for final predictions. Because existing dialogue datasets such\nas the Ubuntu dataset are not suitable for the QA matching task, we further\ncreate a dataset with 1,000 labeled dialogues and demonstrate that our proposed\nmodel outperforms the state-of-the-art and other strong baselines, particularly\nfor matching long-distance QA pairs.", "published": "2020-05-19 08:18:52", "link": "http://arxiv.org/abs/2005.09276v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Staying True to Your Word: (How) Can Attention Become Explanation?", "abstract": "The attention mechanism has quickly become ubiquitous in NLP. In addition to\nimproving performance of models, attention has been widely used as a glimpse\ninto the inner workings of NLP models. The latter aspect has in the recent\nyears become a common topic of discussion, most notably in work of Jain and\nWallace, 2019; Wiegreffe and Pinter, 2019. With the shortcomings of using\nattention weights as a tool of transparency revealed, the attention mechanism\nhas been stuck in a limbo without concrete proof when and whether it can be\nused as an explanation. In this paper, we provide an explanation as to why\nattention has seen rightful critique when used with recurrent networks in\nsequence classification tasks. We propose a remedy to these issues in the form\nof a word level objective and our findings give credibility for attention to\nprovide faithful interpretations of recurrent models.", "published": "2020-05-19 11:55:11", "link": "http://arxiv.org/abs/2005.09379v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human Instruction-Following with Deep Reinforcement Learning via\n  Transfer-Learning from Text", "abstract": "Recent work has described neural-network-based agents that are trained with\nreinforcement learning (RL) to execute language-like commands in simulated\nworlds, as a step towards an intelligent agent or robot that can be instructed\nby human users. However, the optimisation of multi-goal motor policies via deep\nRL from scratch requires many episodes of experience. Consequently,\ninstruction-following with deep RL typically involves language generated from\ntemplates (by an environment simulator), which does not reflect the varied or\nambiguous expressions of real users. Here, we propose a conceptually simple\nmethod for training instruction-following agents with deep RL that are robust\nto natural human instructions. By applying our method with a state-of-the-art\npre-trained text-based language model (BERT), on tasks requiring agents to\nidentify and position everyday objects relative to other objects in a\nnaturalistic 3D simulated room, we demonstrate substantially-above-chance\nzero-shot transfer from synthetic template commands to natural instructions\ngiven by humans. Our approach is a general recipe for training any deep\nRL-based system to interface with human users, and bridges the gap between two\nresearch directions of notable recent success: agent-centric motor behavior and\ntext-based representation learning.", "published": "2020-05-19 12:16:58", "link": "http://arxiv.org/abs/2005.09382v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human Sentence Processing: Recurrence or Attention?", "abstract": "Recurrent neural networks (RNNs) have long been an architecture of interest\nfor computational models of human sentence processing. The recently introduced\nTransformer architecture outperforms RNNs on many natural language processing\ntasks but little is known about its ability to model human language processing.\nWe compare Transformer- and RNN-based language models' ability to account for\nmeasures of human reading effort. Our analysis shows Transformers to outperform\nRNNs in explaining self-paced reading times and neural activity during reading\nEnglish sentences, challenging the widely held idea that human sentence\nprocessing involves recurrent and immediate processing and provides evidence\nfor cue-based retrieval.", "published": "2020-05-19 14:17:49", "link": "http://arxiv.org/abs/2005.09471v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks", "abstract": "Many high-level procedural tasks can be decomposed into sequences of\ninstructions that vary in their order and choice of tools. In the cooking\ndomain, the web offers many partially-overlapping text and video recipes (i.e.\nprocedures) that describe how to make the same dish (i.e. high-level task).\nAligning instructions for the same dish across different sources can yield\ndescriptive visual explanations that are far richer semantically than\nconventional textual instructions, providing commonsense insight into how\nreal-world procedures are structured. Learning to align these different\ninstruction sets is challenging because: a) different recipes vary in their\norder of instructions and use of ingredients; and b) video instructions can be\nnoisy and tend to contain far more information than text instructions. To\naddress these challenges, we first use an unsupervised alignment algorithm that\nlearns pairwise alignments between instructions of different recipes for the\nsame dish. We then use a graph algorithm to derive a joint alignment between\nmultiple text and multiple video recipes for the same dish. We release the\nMicrosoft Research Multimodal Aligned Recipe Corpus containing 150K pairwise\nalignments between recipes across 4,262 dishes with rich commonsense\ninformation.", "published": "2020-05-19 17:27:00", "link": "http://arxiv.org/abs/2005.09606v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Faster, Simpler and More Accurate Hybrid ASR Systems Using Wordpieces", "abstract": "In this work, we first show that on the widely used LibriSpeech benchmark,\nour transformer-based context-dependent connectionist temporal classification\n(CTC) system produces state-of-the-art results. We then show that using\nwordpieces as modeling units combined with CTC training, we can greatly\nsimplify the engineering pipeline compared to conventional frame-based\ncross-entropy training by excluding all the GMM bootstrapping, decision tree\nbuilding and force alignment steps, while still achieving very competitive\nword-error-rate. Additionally, using wordpieces as modeling units can\nsignificantly improve runtime efficiency since we can use larger stride without\nlosing accuracy. We further confirm these findings on two internal VideoASR\ndatasets: German, which is similar to English as a fusional language, and\nTurkish, which is an agglutinative language.", "published": "2020-05-19 00:43:17", "link": "http://arxiv.org/abs/2005.09150v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Generative Adversarial Training Data Adaptation for Very Low-resource\n  Automatic Speech Recognition", "abstract": "It is important to transcribe and archive speech data of endangered languages\nfor preserving heritages of verbal culture and automatic speech recognition\n(ASR) is a powerful tool to facilitate this process. However, since endangered\nlanguages do not generally have large corpora with many speakers, the\nperformance of ASR models trained on them are considerably poor in general.\nNevertheless, we are often left with a lot of recordings of spontaneous speech\ndata that have to be transcribed. In this work, for mitigating this speaker\nsparsity problem, we propose to convert the whole training speech data and make\nit sound like the test speaker in order to develop a highly accurate ASR system\nfor this speaker. For this purpose, we utilize a CycleGAN-based non-parallel\nvoice conversion technology to forge a labeled training data that is close to\nthe test speaker's speech. We evaluated this speaker adaptation approach on two\nlow-resource corpora, namely, Ainu and Mboshi. We obtained 35-60% relative\nimprovement in phone error rate on the Ainu corpus, and 40% relative\nimprovement was attained on the Mboshi corpus. This approach outperformed two\nconventional methods namely unsupervised adaptation and multilingual training\nwith these two corpora.", "published": "2020-05-19 07:35:14", "link": "http://arxiv.org/abs/2005.09256v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Cross-lingual Approaches for Task-specific Dialogue Act Recognition", "abstract": "In this paper we exploit cross-lingual models to enable dialogue act\nrecognition for specific tasks with a small number of annotations. We design a\ntransfer learning approach for dialogue act recognition and validate it on two\ndifferent target languages and domains. We compute dialogue turn embeddings\nwith both a CNN and multi-head self-attention model and show that the best\nresults are obtained by combining all sources of transferred information. We\nfurther demonstrate that the proposed methods significantly outperform related\ncross-lingual DA recognition approaches.", "published": "2020-05-19 07:44:48", "link": "http://arxiv.org/abs/2005.09260v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Choice of Auxiliary Languages for Improved Sequence Tagging", "abstract": "Recent work showed that embeddings from related languages can improve the\nperformance of sequence tagging, even for monolingual models. In this analysis\npaper, we investigate whether the best auxiliary language can be predicted\nbased on language distances and show that the most related language is not\nalways the best auxiliary language. Further, we show that attention-based\nmeta-embeddings can effectively combine pre-trained embeddings from different\nlanguages for sequence tagging and set new state-of-the-art results for\npart-of-speech tagging in five languages.", "published": "2020-05-19 12:32:20", "link": "http://arxiv.org/abs/2005.09389v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adversarial Alignment of Multilingual Models for Extracting Temporal\n  Expressions from Text", "abstract": "Although temporal tagging is still dominated by rule-based systems, there\nhave been recent attempts at neural temporal taggers. However, all of them\nfocus on monolingual settings. In this paper, we explore multilingual methods\nfor the extraction of temporal expressions from text and investigate\nadversarial training for aligning embedding spaces to one common space. With\nthis, we create a single multilingual model that can also be transferred to\nunseen languages and set the new state of the art in those cross-lingual\ntransfer experiments.", "published": "2020-05-19 12:37:04", "link": "http://arxiv.org/abs/2005.09392v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Closing the Gap: Joint De-Identification and Concept Extraction in the\n  Clinical Domain", "abstract": "Exploiting natural language processing in the clinical domain requires\nde-identification, i.e., anonymization of personal information in texts.\nHowever, current research considers de-identification and downstream tasks,\nsuch as concept extraction, only in isolation and does not study the effects of\nde-identification on other tasks. In this paper, we close this gap by reporting\nconcept extraction performance on automatically anonymized data and\ninvestigating joint models for de-identification and concept extraction. In\nparticular, we propose a stacked model with restricted access to\nprivacy-sensitive information and a multitask model. We set the new state of\nthe art on benchmark datasets in English (96.1% F1 for de-identification and\n88.9% F1 for concept extraction) and Spanish (91.4% F1 for concept extraction).", "published": "2020-05-19 12:44:41", "link": "http://arxiv.org/abs/2005.09397v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Embeddings as representation for symbolic music", "abstract": "A representation technique that allows encoding music in a way that contains\nmusical meaning would improve the results of any model trained for computer\nmusic tasks like generation of melodies and harmonies of better quality. The\nfield of natural language processing has done a lot of work in finding a way to\ncapture the semantic meaning of words and sentences, and word embeddings have\nsuccessfully shown the capabilities for such a task. In this paper, we\nexperiment with embeddings to represent musical notes from 3 different\nvariations of a dataset and analyze if the model can capture useful musical\npatterns. To do this, the resulting embeddings are visualized in projections\nusing the t-SNE technique.", "published": "2020-05-19 13:04:02", "link": "http://arxiv.org/abs/2005.09406v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Vector-quantized neural networks for acoustic unit discovery in the\n  ZeroSpeech 2020 challenge", "abstract": "In this paper, we explore vector quantization for acoustic unit discovery.\nLeveraging unlabelled data, we aim to learn discrete representations of speech\nthat separate phonetic content from speaker-specific details. We propose two\nneural models to tackle this challenge - both use vector quantization to map\ncontinuous features to a finite set of codes. The first model is a type of\nvector-quantized variational autoencoder (VQ-VAE). The VQ-VAE encodes speech\ninto a sequence of discrete units before reconstructing the audio waveform. Our\nsecond model combines vector quantization with contrastive predictive coding\n(VQ-CPC). The idea is to learn a representation of speech by predicting future\nacoustic units. We evaluate the models on English and Indonesian data for the\nZeroSpeech 2020 challenge. In ABX phone discrimination tests, both models\noutperform all submissions to the 2019 and 2020 challenges, with a relative\nimprovement of more than 30%. The models also perform competitively on a\ndownstream voice conversion task. Of the two, VQ-CPC performs slightly better\nin general and is simpler and faster to train. Finally, probing experiments\nshow that vector quantization is an effective bottleneck, forcing the models to\ndiscard speaker information.", "published": "2020-05-19 13:06:17", "link": "http://arxiv.org/abs/2005.09409v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Functorial Language Games for Question Answering", "abstract": "We present some categorical investigations into Wittgenstein's\nlanguage-games, with applications to game-theoretic pragmatics and\nquestion-answering in natural language processing.", "published": "2020-05-19 13:35:13", "link": "http://arxiv.org/abs/2005.09439v2", "categories": ["cs.CL", "cs.GT"], "primary_category": "cs.CL"}
{"title": "GLEAKE: Global and Local Embedding Automatic Keyphrase Extraction", "abstract": "Automated methods for granular categorization of large corpora of text\ndocuments have become increasingly more important with the rate scientific,\nnews, medical, and web documents are growing in the last few years. Automatic\nkeyphrase extraction (AKE) aims to automatically detect a small set of single\nor multi-words from within a single textual document that captures the main\ntopics of the document. AKE plays an important role in various NLP and\ninformation retrieval tasks such as document summarization and categorization,\nfull-text indexing, and article recommendation. Due to the lack of sufficient\nhuman-labeled data in different textual contents, supervised learning\napproaches are not ideal for automatic detection of keyphrases from the content\nof textual bodies. With the state-of-the-art advances in text embedding\ntechniques, NLP researchers have focused on developing unsupervised methods to\nobtain meaningful insights from raw datasets. In this work, we introduce Global\nand Local Embedding Automatic Keyphrase Extractor (GLEAKE) for the task of AKE.\nGLEAKE utilizes single and multi-word embedding techniques to explore the\nsyntactic and semantic aspects of the candidate phrases and then combines them\ninto a series of embedding-based graphs. Moreover, GLEAKE applies network\nanalysis techniques on each embedding-based graph to refine the most\nsignificant phrases as a final set of keyphrases. We demonstrate the high\nperformance of GLEAKE by evaluating its results on five standard AKE datasets\nfrom different domains and writing styles and by showing its superiority with\nregards to other state-of-the-art methods.", "published": "2020-05-19 20:24:02", "link": "http://arxiv.org/abs/2005.09740v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Prototypical Q Networks for Automatic Conversational Diagnosis and\n  Few-Shot New Disease Adaption", "abstract": "Spoken dialog systems have seen applications in many domains, including\nmedical for automatic conversational diagnosis. State-of-the-art dialog\nmanagers are usually driven by deep reinforcement learning models, such as deep\nQ networks (DQNs), which learn by interacting with a simulator to explore the\nentire action space since real conversations are limited. However, the\nDQN-based automatic diagnosis models do not achieve satisfying performances\nwhen adapted to new, unseen diseases with only a few training samples. In this\nwork, we propose the Prototypical Q Networks (ProtoQN) as the dialog manager\nfor the automatic diagnosis systems. The model calculates prototype embeddings\nwith real conversations between doctors and patients, learning from them and\nsimulator-augmented dialogs more efficiently. We create both supervised and\nfew-shot learning tasks with the Muzhi corpus. Experiments showed that the\nProtoQN significantly outperformed the baseline DQN model in both supervised\nand few-shot learning scenarios, and achieves state-of-the-art few-shot\nlearning performances.", "published": "2020-05-19 19:10:49", "link": "http://arxiv.org/abs/2005.11153v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Retrieving and Highlighting Action with Spatiotemporal Reference", "abstract": "In this paper, we present a framework that jointly retrieves and\nspatiotemporally highlights actions in videos by enhancing current deep\ncross-modal retrieval methods. Our work takes on the novel task of action\nhighlighting, which visualizes where and when actions occur in an untrimmed\nvideo setting. Action highlighting is a fine-grained task, compared to\nconventional action recognition tasks which focus on classification or\nwindow-based localization. Leveraging weak supervision from annotated captions,\nour framework acquires spatiotemporal relevance maps and generates local\nembeddings which relate to the nouns and verbs in captions. Through\nexperiments, we show that our model generates various maps conditioned on\ndifferent actions, in which conventional visual reasoning methods only go as\nfar as to show a single deterministic saliency map. Also, our model improves\nretrieval recall over our baseline without alignment by 2-3% on the MSR-VTT\ndataset.", "published": "2020-05-19 03:12:31", "link": "http://arxiv.org/abs/2005.09183v1", "categories": ["cs.CV", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Quantifying the Uncertainty of Precision Estimates for Rule based Text\n  Classifiers", "abstract": "Rule based classifiers that use the presence and absence of key sub-strings\nto make classification decisions have a natural mechanism for quantifying the\nuncertainty of their precision. For a binary classifier, the key insight is to\ntreat partitions of the sub-string set induced by the documents as Bernoulli\nrandom variables. The mean value of each random variable is an estimate of the\nclassifier's precision when presented with a document inducing that partition.\nThese means can be compared, using standard statistical tests, to a desired or\nexpected classifier precision. A set of binary classifiers can be combined into\na single, multi-label classifier by an application of the Dempster-Shafer\ntheory of evidence. The utility of this approach is demonstrated with a\nbenchmark problem.", "published": "2020-05-19 03:51:47", "link": "http://arxiv.org/abs/2005.09198v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Table Search Using a Deep Contextualized Language Model", "abstract": "Pretrained contextualized language models such as BERT have achieved\nimpressive results on various natural language processing benchmarks.\nBenefiting from multiple pretraining tasks and large scale training corpora,\npretrained models can capture complex syntactic word relations. In this paper,\nwe use the deep contextualized language model BERT for the task of ad hoc table\nretrieval. We investigate how to encode table content considering the table\nstructure and input length limit of BERT. We also propose an approach that\nincorporates features from prior literature on table retrieval and jointly\ntrains them with BERT. In experiments on public datasets, we show that our best\napproach can outperform the previous state-of-the-art method and BERT baselines\nwith a large margin under different evaluation metrics.", "published": "2020-05-19 04:18:04", "link": "http://arxiv.org/abs/2005.09207v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "The Effect of Moderation on Online Mental Health Conversations", "abstract": "Many people struggling with mental health issues are unable to access\nadequate care due to high costs and a shortage of mental health professionals,\nleading to a global mental health crisis. Online mental health communities can\nhelp mitigate this crisis by offering a scalable, easily accessible alternative\nto in-person sessions with therapists or support groups. However, people\nseeking emotional or psychological support online may be especially vulnerable\nto the kinds of antisocial behavior that sometimes occur in online discussions.\nModeration can improve online discourse quality, but we lack an understanding\nof its effects on online mental health conversations. In this work, we\nleveraged a natural experiment, occurring across 200,000 messages from 7,000\nonline mental health conversations, to evaluate the effects of moderation on\nonline mental health discussions. We found that participation in group mental\nhealth discussions led to improvements in psychological perspective, and that\nthese improvements were larger in moderated conversations. The presence of a\nmoderator increased user engagement, encouraged users to discuss negative\nemotions more candidly, and dramatically reduced bad behavior among chat\nparticipants. Moderation also encouraged stronger linguistic coordination,\nwhich is indicative of trust building. In addition, moderators who remained\nactive in conversations were especially successful in keeping conversations on\ntopic. Our findings suggest that moderation can serve as a valuable tool to\nimprove the efficacy and safety of online mental health conversations. Based on\nthese findings, we discuss implications and trade-offs involved in designing\neffective online spaces for mental health support.", "published": "2020-05-19 05:40:59", "link": "http://arxiv.org/abs/2005.09225v7", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "Assertion Detection in Multi-Label Clinical Text using Scope\n  Localization", "abstract": "Multi-label sentences (text) in the clinical domain result from the rich\ndescription of scenarios during patient care. The state-of-theart methods for\nassertion detection mostly address this task in the setting of a single\nassertion label per sentence (text). In addition, few rules based and deep\nlearning methods perform negation/assertion scope detection on single-label\ntext. It is a significant challenge extending these methods to address\nmulti-label sentences without diminishing performance. Therefore, we developed\na convolutional neural network (CNN) architecture to localize multiple labels\nand their scopes in a single stage end-to-end fashion, and demonstrate that our\nmodel performs atleast 12% better than the state-of-the-art on multi-label\nclinical text.", "published": "2020-05-19 06:56:02", "link": "http://arxiv.org/abs/2005.09246v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Iterative Pseudo-Labeling for Speech Recognition", "abstract": "Pseudo-labeling has recently shown promise in end-to-end automatic speech\nrecognition (ASR). We study Iterative Pseudo-Labeling (IPL), a semi-supervised\nalgorithm which efficiently performs multiple iterations of pseudo-labeling on\nunlabeled data as the acoustic model evolves. In particular, IPL fine-tunes an\nexisting model at each iteration using both labeled data and a subset of\nunlabeled data. We study the main components of IPL: decoding with a language\nmodel and data augmentation. We then demonstrate the effectiveness of IPL by\nachieving state-of-the-art word-error rate on the Librispeech test sets in both\nstandard and low-resource setting. We also study the effect of language models\ntrained on different corpora to show IPL can effectively utilize additional\ntext. Finally, we release a new large in-domain text corpus which does not\noverlap with the Librispeech training transcriptions to foster research in\nlow-resource, semi-supervised ASR", "published": "2020-05-19 07:56:21", "link": "http://arxiv.org/abs/2005.09267v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving Accent Conversion with Reference Encoder and End-To-End\n  Text-To-Speech", "abstract": "Accent conversion (AC) transforms a non-native speaker's accent into a native\naccent while maintaining the speaker's voice timbre. In this paper, we propose\napproaches to improving accent conversion applicability, as well as quality.\nFirst of all, we assume no reference speech is available at the conversion\nstage, and hence we employ an end-to-end text-to-speech system that is trained\non native speech to generate native reference speech. To improve the quality\nand accent of the converted speech, we introduce reference encoders which make\nus capable of utilizing multi-source information. This is motivated by acoustic\nfeatures extracted from native reference and linguistic information, which are\ncomplementary to conventional phonetic posteriorgrams (PPGs), so they can be\nconcatenated as features to improve a baseline system based only on PPGs.\nMoreover, we optimize model architecture using GMM-based attention instead of\nwindowed attention to elevate synthesized performance. Experimental results\nindicate when the proposed techniques are applied the integrated system\nsignificantly raises the scores of acoustic quality (30$\\%$ relative increase\nin mean opinion score) and native accent (68$\\%$ relative preference) while\nretaining the voice identity of the non-native speaker.", "published": "2020-05-19 08:09:58", "link": "http://arxiv.org/abs/2005.09271v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Bayesian Subspace HMM for the Zerospeech 2020 Challenge", "abstract": "In this paper we describe our submission to the Zerospeech 2020 challenge,\nwhere the participants are required to discover latent representations from\nunannotated speech, and to use those representations to perform speech\nsynthesis, with synthesis quality used as a proxy metric for the unit quality.\nIn our system, we use the Bayesian Subspace Hidden Markov Model (SHMM) for unit\ndiscovery. The SHMM models each unit as an HMM whose parameters are constrained\nto lie in a low dimensional subspace of the total parameter space which is\ntrained to model phonetic variability. Our system compares favorably with the\nbaseline on the human-evaluated character error rate while maintaining\nsignificantly lower unit bitrate.", "published": "2020-05-19 08:28:38", "link": "http://arxiv.org/abs/2005.09282v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "eess.AS"}
{"title": "ISeeU2: Visually Interpretable ICU mortality prediction using deep\n  learning and free-text medical notes", "abstract": "Accurate mortality prediction allows Intensive Care Units (ICUs) to\nadequately benchmark clinical practice and identify patients with unexpected\noutcomes. Traditionally, simple statistical models have been used to assess\npatient death risk, many times with sub-optimal performance. On the other hand\ndeep learning holds promise to positively impact clinical practice by\nleveraging medical data to assist diagnosis and prediction, including mortality\nprediction. However, as the question of whether powerful Deep Learning models\nattend correlations backed by sound medical knowledge when generating\npredictions remains open, additional interpretability tools are needed to\nfoster trust and encourage the use of AI by clinicians. In this work we show a\nDeep Learning model trained on MIMIC-III to predict mortality using raw nursing\nnotes, together with visual explanations for word importance. Our model reaches\na ROC of 0.8629 (+/-0.0058), outperforming the traditional SAPS-II score and\nproviding enhanced interpretability when compared with similar Deep Learning\napproaches.", "published": "2020-05-19 08:30:34", "link": "http://arxiv.org/abs/2005.09284v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A systematic comparison of grapheme-based vs. phoneme-based label units\n  for encoder-decoder-attention models", "abstract": "Following the rationale of end-to-end modeling, CTC, RNN-T or\nencoder-decoder-attention models for automatic speech recognition (ASR) use\ngraphemes or grapheme-based subword units based on e.g. byte-pair encoding\n(BPE). The mapping from pronunciation to spelling is learned completely from\ndata. In contrast to this, classical approaches to ASR employ secondary\nknowledge sources in the form of phoneme lists to define phonetic output labels\nand pronunciation lexica. In this work, we do a systematic comparison between\ngrapheme- and phoneme-based output labels for an encoder-decoder-attention ASR\nmodel. We investigate the use of single phonemes as well as BPE-based phoneme\ngroups as output labels of our model. To preserve a simplified and efficient\ndecoder design, we also extend the phoneme set by auxiliary units to be able to\ndistinguish homophones. Experiments performed on the Switchboard 300h and\nLibriSpeech benchmarks show that phoneme-based modeling is competitive to\ngrapheme-based encoder-decoder-attention modeling.", "published": "2020-05-19 09:54:17", "link": "http://arxiv.org/abs/2005.09336v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.NE"], "primary_category": "eess.AS"}
{"title": "Enhancing Monotonic Multihead Attention for Streaming ASR", "abstract": "We investigate a monotonic multihead attention (MMA) by extending hard\nmonotonic attention to Transformer-based automatic speech recognition (ASR) for\nonline streaming applications. For streaming inference, all monotonic attention\n(MA) heads should learn proper alignments because the next token is not\ngenerated until all heads detect the corresponding token boundaries. However,\nwe found not all MA heads learn alignments with a na\\\"ive implementation. To\nencourage every head to learn alignments properly, we propose HeadDrop\nregularization by masking out a part of heads stochastically during training.\nFurthermore, we propose to prune redundant heads to improve consensus among\nheads for boundary detection and prevent delayed token generation caused by\nsuch heads. Chunkwise attention on each MA head is extended to the multihead\ncounterpart. Finally, we propose head-synchronous beam search decoding to\nguarantee stable streaming inference.", "published": "2020-05-19 12:39:38", "link": "http://arxiv.org/abs/2005.09394v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Embeddings-Based Clustering for Target Specific Stances: The Case of a\n  Polarized Turkey", "abstract": "On June 24, 2018, Turkey conducted a highly consequential election in which\nthe Turkish people elected their president and parliament in the first election\nunder a new presidential system. During the election period, the Turkish people\nextensively shared their political opinions on Twitter. One aspect of\npolarization among the electorate was support for or opposition to the\nreelection of Recep Tayyip Erdo\\u{g}an. In this paper, we present an\nunsupervised method for target-specific stance detection in a polarized\nsetting, specifically Turkish politics, achieving 90% precision in identifying\nuser stances, while maintaining more than 80% recall. The method involves\nrepresenting users in an embedding space using Google's Convolutional Neural\nNetwork (CNN) based multilingual universal sentence encoder. The\nrepresentations are then projected onto a lower dimensional space in a manner\nthat reflects similarities and are consequently clustered. We show the\neffectiveness of our method in properly clustering users of divergent groups\nacross multiple targets that include political figures, different groups, and\nparties. We perform our analysis on a large dataset of 108M Turkish\nelection-related tweets along with the timeline tweets of 168k Turkish users,\nwho authored 213M tweets. Given the resultant user stances, we are able to\nobserve correlations between topics and compute topic polarization.", "published": "2020-05-19 13:52:15", "link": "http://arxiv.org/abs/2005.09649v2", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "Exploring Transformers for Large-Scale Speech Recognition", "abstract": "While recurrent neural networks still largely define state-of-the-art speech\nrecognition systems, the Transformer network has been proven to be a\ncompetitive alternative, especially in the offline condition. Most studies with\nTransformers have been constrained in a relatively small scale setting, and\nsome forms of data argumentation approaches are usually applied to combat the\ndata sparsity issue. In this paper, we aim at understanding the behaviors of\nTransformers in the large-scale speech recognition setting, where we have used\naround 65,000 hours of training data. We investigated various aspects on\nscaling up Transformers, including model initialization, warmup training as\nwell as different Layer Normalization strategies. In the streaming condition,\nwe compared the widely used attention mask based future context lookahead\napproach to the Transformer-XL network. From our experiments, we show that\nTransformers can achieve around 6% relative word error rate (WER) reduction\ncompared to the BLSTM baseline in the offline fashion, while in the streaming\nfashion, Transformer-XL is comparable to LC-BLSTM with 800 millisecond latency\nconstraint.", "published": "2020-05-19 18:07:14", "link": "http://arxiv.org/abs/2005.09684v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Word-Emoji Embeddings from large scale Messaging Data reflect real-world\n  Semantic Associations of Expressive Icons", "abstract": "We train word-emoji embeddings on large scale messaging data obtained from\nthe Jodel online social network. Our data set contains more than 40 million\nsentences, of which 11 million sentences are annotated with a subset of the\nUnicode 13.0 standard Emoji list. We explore semantic emoji associations\ncontained in this embedding by analyzing associations between emojis, between\nemojis and text, and between text and emojis. Our investigations demonstrate\nanecdotally that word-emoji embeddings trained on large scale messaging data\ncan reflect real-world semantic associations. To enable further research we\nrelease the Jodel Emoji Embedding Dataset (JEED1488) containing 1488 emojis and\ntheir embeddings along 300 dimensions.", "published": "2020-05-19 19:55:56", "link": "http://arxiv.org/abs/2006.01207v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Transferring Source Style in Non-Parallel Voice Conversion", "abstract": "Voice conversion (VC) techniques aim to modify speaker identity of an\nutterance while preserving the underlying linguistic information. Most VC\napproaches ignore modeling of the speaking style (e.g. emotion and emphasis),\nwhich may contain the factors intentionally added by the speaker and should be\nretained during conversion. This study proposes a sequence-to-sequence based\nnon-parallel VC approach, which has the capability of transferring the speaking\nstyle from the source speech to the converted speech by explicitly modeling.\nObjective evaluation and subjective listening tests show superiority of the\nproposed VC approach in terms of speech naturalness and speaker similarity of\nthe converted speech. Experiments are also conducted to show the source-style\ntransferability of the proposed approach.", "published": "2020-05-19 02:58:58", "link": "http://arxiv.org/abs/2005.09178v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Robust Beam Search for Encoder-Decoder Attention Based Speech\n  Recognition without Length Bias", "abstract": "As one popular modeling approach for end-to-end speech recognition,\nattention-based encoder-decoder models are known to suffer the length bias and\ncorresponding beam problem. Different approaches have been applied in simple\nbeam search to ease the problem, most of which are heuristic-based and require\nconsiderable tuning. We show that heuristics are not proper modeling\nrefinement, which results in severe performance degradation with largely\nincreased beam sizes. We propose a novel beam search derived from\nreinterpreting the sequence posterior with an explicit length modeling. By\napplying the reinterpreted probability together with beam pruning, the obtained\nfinal probability leads to a robust model modification, which allows reliable\ncomparison among output sequences of different lengths. Experimental\nverification on the LibriSpeech corpus shows that the proposed approach solves\nthe length bias problem without heuristics or additional tuning effort. It\nprovides robust decision making and consistently good performance under both\nsmall and very large beam sizes. Compared with the best results of the\nheuristic baseline, the proposed approach achieves the same WER on the 'clean'\nsets and 4% relative improvement on the 'other' sets. We also show that it is\nmore efficient with the additional derived early stopping criterion.", "published": "2020-05-19 07:48:18", "link": "http://arxiv.org/abs/2005.09265v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "GEV Beamforming Supported by DOA-based Masks Generated on Pairs of\n  Microphones", "abstract": "Distant speech processing is a challenging task, especially when dealing with\nthe cocktail party effect. Sound source separation is thus often required as a\npreprocessing step prior to speech recognition to improve the signal to\ndistortion ratio (SDR). Recently, a combination of beamforming and speech\nseparation networks have been proposed to improve the target source quality in\nthe direction of arrival of interest. However, with this type of approach, the\nneural network needs to be trained in advance for a specific microphone array\ngeometry, which limits versatility when adding/removing microphones, or\nchanging the shape of the array. The solution presented in this paper is to\ntrain a neural network on pairs of microphones with different spacing and\nacoustic environmental conditions, and then use this network to estimate a\ntime-frequency mask from all the pairs of microphones forming the array with an\narbitrary shape. Using this mask, the target and noise covariance matrices can\nbe estimated, and then used to perform generalized eigenvalue (GEV)\nbeamforming. Results show that the proposed approach improves the SDR from 4.78\ndB to 7.69 dB on average, for various microphone array geometries that\ncorrespond to commercially available hardware.", "published": "2020-05-19 17:08:29", "link": "http://arxiv.org/abs/2005.09587v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Atss-Net: Target Speaker Separation via Attention-based Neural Network", "abstract": "Recently, Convolutional Neural Network (CNN) and Long short-term memory\n(LSTM) based models have been introduced to deep learning-based target speaker\nseparation. In this paper, we propose an Attention-based neural network\n(Atss-Net) in the spectrogram domain for the task. It allows the network to\ncompute the correlation between each feature parallelly, and using shallower\nlayers to extract more features, compared with the CNN-LSTM architecture.\nExperimental results show that our Atss-Net yields better performance than the\nVoiceFilter, although it only contains half of the parameters. Furthermore, our\nproposed model also demonstrates promising performance in speech enhancement.", "published": "2020-05-19 03:58:27", "link": "http://arxiv.org/abs/2005.09200v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Should we hard-code the recurrence concept or learn it instead ?\n  Exploring the Transformer architecture for Audio-Visual Speech Recognition", "abstract": "The audio-visual speech fusion strategy AV Align has shown significant\nperformance improvements in audio-visual speech recognition (AVSR) on the\nchallenging LRS2 dataset. Performance improvements range between 7% and 30%\ndepending on the noise level when leveraging the visual modality of speech in\naddition to the auditory one. This work presents a variant of AV Align where\nthe recurrent Long Short-term Memory (LSTM) computation block is replaced by\nthe more recently proposed Transformer block. We compare the two methods,\ndiscussing in greater detail their strengths and weaknesses. We find that\nTransformers also learn cross-modal monotonic alignments, but suffer from the\nsame visual convergence problems as the LSTM model, calling for a deeper\ninvestigation into the dominant modality problem in machine learning.", "published": "2020-05-19 09:06:39", "link": "http://arxiv.org/abs/2005.09297v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "The Privacy ZEBRA: Zero Evidence Biometric Recognition Assessment", "abstract": "Mounting privacy legislation calls for the preservation of privacy in speech\ntechnology, though solutions are gravely lacking. While evaluation campaigns\nare long-proven tools to drive progress, the need to consider a privacy\nadversary implies that traditional approaches to evaluation must be adapted to\nthe assessment of privacy and privacy preservation solutions. This paper\npresents the first step in this direction: metrics.\n  We introduce the zero evidence biometric recognition assessment (ZEBRA)\nframework and propose two new privacy metrics. They measure the average level\nof privacy preservation afforded by a given safeguard for a population and the\nworst-case privacy disclosure for an individual. The paper demonstrates their\napplication to privacy preservation assessment within the scope of the\nVoicePrivacy challenge. While the ZEBRA framework is designed with speech\napplications in mind, it is a candidate for incorporation into biometric\ninformation protection standards and is readily extendable to the study of\nprivacy in applications even beyond speech and biometrics.", "published": "2020-05-19 13:09:34", "link": "http://arxiv.org/abs/2005.09413v2", "categories": ["cs.CR", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Improved Noisy Student Training for Automatic Speech Recognition", "abstract": "Recently, a semi-supervised learning method known as \"noisy student training\"\nhas been shown to improve image classification performance of deep networks\nsignificantly. Noisy student training is an iterative self-training method that\nleverages augmentation to improve network performance. In this work, we adapt\nand improve noisy student training for automatic speech recognition, employing\n(adaptive) SpecAugment as the augmentation method. We find effective methods to\nfilter, balance and augment the data generated in between self-training\niterations. By doing so, we are able to obtain word error rates (WERs)\n4.2%/8.6% on the clean/noisy LibriSpeech test sets by only using the clean 100h\nsubset of LibriSpeech as the supervised set and the rest (860h) as the\nunlabeled set. Furthermore, we are able to achieve WERs 1.7%/3.4% on the\nclean/noisy LibriSpeech test sets by using the unlab-60k subset of LibriLight\nas the unlabeled set for LibriSpeech 960h. We are thus able to improve upon the\nprevious state-of-the-art clean/noisy test WERs achieved on LibriSpeech 100h\n(4.74%/12.20%) and LibriSpeech (1.9%/4.1%).", "published": "2020-05-19 17:57:29", "link": "http://arxiv.org/abs/2005.09629v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Perceptual similarity between piano notes: Simulations with a\n  template-based perception model", "abstract": "In this paper the auditory model developed by Dau et al. [J. Acoust. Soc. Am.\n102, 2892-2905 (1997)] was used to simulate the perceptual similarity between\ncomplex sounds. For this purpose, a central processor stage was developed and\nattached as a back-end module to the auditory model. As complex sounds, a set\nof recordings of one note played on seven different pianos was used, whose\nsimilarity has been recently measured by Osses et al. [J. Acoust. Soc. Am. 146,\n1024-1035 (2019)] using a 3-AFC discrimination task in noise. The auditory\nmodel has several processing stages that are to a greater or lesser extent\ninspired by physiological aspects of the human normal-hearing system. A set of\nconfigurable parameters in each stage affects directly the sound (internal)\nrepresentations that are further processed in the developed central processor.\nTherefore, a comprehensive review of the model parameters is given, indicating\nthe configuration we chose. This includes an in-depth description of the\nauditory adaptation stage, the adaptation loops. Simulations of the similarity\ntask were compared with (1) existing experimental data, where they had a\nmoderate to high correlation, and with (2) simulations using an alternative but\nsimilar background noise to that of the experiments, which were used to obtain\nfurther information about how the participants' responses were weighted as a\nfunction of frequency.", "published": "2020-05-19 21:44:51", "link": "http://arxiv.org/abs/2005.09768v1", "categories": ["eess.AS", "cs.SD", "68U20"], "primary_category": "eess.AS"}
{"title": "Anomalous sound detection based on interpolation deep neural network", "abstract": "As the labor force decreases, the demand for labor-saving automatic anomalous\nsound detection technology that conducts maintenance of industrial equipment\nhas grown. Conventional approaches detect anomalies based on the reconstruction\nerrors of an autoencoder. However, when the target machine sound is\nnon-stationary, a reconstruction error tends to be large independent of an\nanomaly, and its variations increased because of the difficulty of predicting\nthe edge frames. To solve the issue, we propose an approach to anomalous\ndetection in which the model utilizes multiple frames of a spectrogram whose\ncenter frame is removed as an input, and it predicts an interpolation of the\nremoved frame as an output. Rather than predicting the edge frames, the\nproposed approach makes the reconstruction error consistent with the anomaly.\nExperimental results showed that the proposed approach achieved 27% improvement\nbased on the standard AUC score, especially against non-stationary machinery\nsounds.", "published": "2020-05-19 06:12:41", "link": "http://arxiv.org/abs/2005.09234v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Acoustic Echo Cancellation by Combining Adaptive Digital Filter and\n  Recurrent Neural Network", "abstract": "Acoustic Echo Cancellation (AEC) plays a key role in voice interaction. Due\nto the explicit mathematical principle and intelligent nature to accommodate\nconditions, adaptive filters with different types of implementations are always\nused for AEC, giving considerable performance. However, there would be some\nkinds of residual echo in the results, including linear residue introduced by\nmismatching between estimation and the reality and non-linear residue mostly\ncaused by non-linear components on the audio devices. The linear residue can be\nreduced with elaborate structure and methods, leaving the non-linear residue\nintractable for suppression. Though, some non-linear processing methods have\nalready be raised, they are complicated and inefficient for suppression, and\nwould bring damage to the speech audio. In this paper, a fusion scheme by\ncombining adaptive filter and neural network is proposed for AEC. The echo\ncould be reduced in a large scale by adaptive filtering, resulting in little\nresidual echo. Though it is much smaller than speech audio, it could also be\nperceived by human ear and would make communication annoy. The neural network\nis elaborately designed and trained for suppressing such residual echo.\nExperiments compared with prevailing methods are conducted, validating the\neffectiveness and superiority of the proposed combination scheme.", "published": "2020-05-19 06:25:52", "link": "http://arxiv.org/abs/2005.09237v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Lite Microphone Array Beamforming Scheme with Maximum Signal-to-Noise\n  Ratio Filter", "abstract": "Since space-domain information can be utilized, microphone array beamforming\nis often used to enhance the quality of the speech by suppressing directional\ndisturbance. However, with the increasing number of microphone, the complexity\nwould be increased. In this paper, a concise beamforming scheme using Maximum\nSignal-to-Noise Ratio (SNR) filter is proposed to reduce the beamforming\ncomplexity. The maximum SNR filter is implemented by using the estimated\ndirection-of-arrival (DOA) of the speech source localization (SSL) and the\nsolving method of independent vector analysis (IVA). Our experiments show that\nwhen compared with other widely-used algorithms, the proposed algorithm obtain\nhigher gain of signal-to-interference and noise ratio (SINR).", "published": "2020-05-19 06:35:41", "link": "http://arxiv.org/abs/2005.09238v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Competitive Wakeup Scheme for Distributed Devices", "abstract": "Wakeup is the primary function in voice interaction which is the mainstream\nscheme in man-machine interaction (HMI) applications for smart home. All\ndevices will response if the same wake-up word is used for all devices. This\nwill bring chaos and reduce user quality of experience (QoE). The only way to\nsolve this problem is to make all the devices in the same wireless local area\nnetwork (WLAN) competing to wake-up based on the same scoring rule. The one\nclosest to the user would be selected for response. To this end, a competitive\nwakeup scheme is proposed in this paper with elaborately designed calibration\nmethod for receiving energy of microphones. Moreover, the user orientation is\nassisted to determine the optimal device. Experiments reveal the feasibility\nand validity of this scheme.", "published": "2020-05-19 06:46:02", "link": "http://arxiv.org/abs/2005.09242v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Distilling Knowledge from Ensembles of Acoustic Models for Joint\n  CTC-Attention End-to-End Speech Recognition", "abstract": "Knowledge distillation has been widely used to compress existing deep\nlearning models while preserving the performance on a wide range of\napplications. In the specific context of Automatic Speech Recognition (ASR),\ndistillation from ensembles of acoustic models has recently shown promising\nresults in increasing recognition performance. In this paper, we propose an\nextension of multi-teacher distillation methods to joint CTC-attention\nend-to-end ASR systems. We also introduce three novel distillation strategies.\nThe core intuition behind them is to integrate the error rate metric to the\nteacher selection rather than solely focusing on the observed losses. In this\nway, we directly distill and optimize the student toward the relevant metric\nfor speech recognition. We evaluate these strategies under a selection of\ntraining procedures on different datasets (TIMIT, Librispeech, Common Voice)\nand various languages (English, French, Italian). In particular,\nstate-of-the-art error rates are reported on the Common Voice French, Italian\nand TIMIT datasets.", "published": "2020-05-19 09:24:54", "link": "http://arxiv.org/abs/2005.09310v3", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A New Training Pipeline for an Improved Neural Transducer", "abstract": "The RNN transducer is a promising end-to-end model candidate. We compare the\noriginal training criterion with the full marginalization over all alignments,\nto the commonly used maximum approximation, which simplifies, improves and\nspeeds up our training. We also generalize from the original neural network\nmodel and study more powerful models, made possible due to the maximum\napproximation. We further generalize the output label topology to cover RNN-T,\nRNA and CTC. We perform several studies among all these aspects, including a\nstudy on the effect of external alignments. We find that the transducer model\ngeneralizes much better on longer sequences than the attention model. Our final\ntransducer model outperforms our attention model on Switchboard 300h by over 6%\nrelative WER.", "published": "2020-05-19 09:35:38", "link": "http://arxiv.org/abs/2005.09319v2", "categories": ["eess.AS", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Toward Automated Classroom Observation: Multimodal Machine Learning to\n  Estimate CLASS Positive Climate and Negative Climate", "abstract": "In this work we present a multi-modal machine learning-based system, which we\ncall ACORN, to analyze videos of school classrooms for the Positive Climate\n(PC) and Negative Climate (NC) dimensions of the CLASS observation protocol\nthat is widely used in educational research. ACORN uses convolutional neural\nnetworks to analyze spectral audio features, the faces of teachers and\nstudents, and the pixels of each image frame, and then integrates this\ninformation over time using Temporal Convolutional Networks. The audiovisual\nACORN's PC and NC predictions have Pearson correlations of $0.55$ and $0.63$\nwith ground-truth scores provided by expert CLASS coders on the UVA Toddler\ndataset (cross-validation on $n=300$ 15-min video segments), and a purely\nauditory ACORN predicts PC and NC with correlations of $0.36$ and $0.41$ on the\nMET dataset (test set of $n=2000$ videos segments). These numbers are similar\nto inter-coder reliability of human coders. Finally, using Graph Convolutional\nNetworks we make early strides (AUC=$0.70$) toward predicting the specific\nmoments (45-90sec clips) when the PC is particularly weak/strong. Our findings\ninform the design of automatic classroom observation and also more general\nvideo activity recognition and summary recognition systems.", "published": "2020-05-19 15:36:32", "link": "http://arxiv.org/abs/2005.09525v3", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Improving Proper Noun Recognition in End-to-End ASR By Customization of\n  the MWER Loss Criterion", "abstract": "Proper nouns present a challenge for end-to-end (E2E) automatic speech\nrecognition (ASR) systems in that a particular name may appear only rarely\nduring training, and may have a pronunciation similar to that of a more common\nword. Unlike conventional ASR models, E2E systems lack an explicit\npronounciation model that can be specifically trained with proper noun\npronounciations and a language model that can be trained on a large text-only\ncorpus. Past work has addressed this issue by incorporating additional training\ndata or additional models. In this paper, we instead build on recent advances\nin minimum word error rate (MWER) training to develop two new loss criteria\nthat specifically emphasize proper noun recognition. Unlike past work on this\nproblem, this method requires no new data during training or external models\nduring inference. We see improvements ranging from 2% to 7% relative on several\nrelevant benchmarks.", "published": "2020-05-19 21:10:50", "link": "http://arxiv.org/abs/2005.09756v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sparsity-based audio declipping methods: selected overview, new\n  algorithms, and large-scale evaluation", "abstract": "Recent advances in audio declipping have substantially improved the state of\nthe art.% in certain saturation regimes. Yet, practitioners need guidelines to\nchoose a method, and while existing benchmarks have been instrumental in\nadvancing the field, larger-scale experiments are needed to guide such choices.\nFirst, we show that the clipping levels in existing small-scale benchmarks are\nmoderate and call for benchmarks with more perceptually significant clipping\nlevels. We then propose a general algorithmic framework for declipping that\ncovers existing and new combinations of variants of state-of-the-art techniques\nexploiting time-frequency sparsity: synthesis vs. analysis sparsity, with plain\nor structured sparsity. Finally, we systematically compare these combinations\nand a selection of state-of-the-art methods. Using a large-scale numerical\nbenchmark and a smaller scale formal listening test, we provide guidelines for\nvarious clipping levels, both for speech and various musical genres. The code\nis made publicly available for the purpose of reproducible research and\nbenchmarking.", "published": "2020-05-19 07:08:18", "link": "http://arxiv.org/abs/2005.10228v2", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
