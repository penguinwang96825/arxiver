{"title": "Learning Multimodal Word Representation via Dynamic Fusion Methods", "abstract": "Multimodal models have been proven to outperform text-based models on\nlearning semantic word representations. Almost all previous multimodal models\ntypically treat the representations from different modalities equally. However,\nit is obvious that information from different modalities contributes\ndifferently to the meaning of words. This motivates us to build a multimodal\nmodel that can dynamically fuse the semantic representations from different\nmodalities according to different types of words. To that end, we propose three\nnovel dynamic fusion methods to assign importance weights to each modality, in\nwhich weights are learned under the weak supervision of word association pairs.\nThe extensive experiments have demonstrated that the proposed methods\noutperform strong unimodal baselines and state-of-the-art multimodal models.", "published": "2018-01-02 00:32:29", "link": "http://arxiv.org/abs/1801.00532v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Attentive Sequence Model for Adverse Drug Event Extraction from\n  Biomedical Text", "abstract": "Adverse reaction caused by drugs is a potentially dangerous problem which may\nlead to mortality and morbidity in patients. Adverse Drug Event (ADE)\nextraction is a significant problem in biomedical research. We model ADE\nextraction as a Question-Answering problem and take inspiration from Machine\nReading Comprehension (MRC) literature, to design our model. Our objective in\ndesigning such a model, is to exploit the local linguistic context in clinical\ntext and enable intra-sequence interaction, in order to jointly learn to\nclassify drug and disease entities, and to extract adverse reactions caused by\na given drug. Our model makes use of a self-attention mechanism to facilitate\nintra-sequence interaction in a text sequence. This enables us to visualize and\nunderstand how the network makes use of the local and wider context for\nclassification.", "published": "2018-01-02 12:19:08", "link": "http://arxiv.org/abs/1801.00625v1", "categories": ["cs.CL", "68T50 (Primary), 68T10 (Secondary)"], "primary_category": "cs.CL"}
{"title": "Identifying emergency stages in Facebook posts of police departments\n  with convolutional and recurrent neural networks and support vector machines", "abstract": "Classification of social media posts in emergency response is an important\npractical problem: accurate classification can help automate processing of such\nmessages and help other responders and the public react to emergencies in a\ntimely fashion. This research focused on classifying Facebook messages of US\npolice departments. Randomly selected 5,000 messages were used to train\nclassifiers that distinguished between four categories of messages: emergency\npreparedness, response and recovery, as well as general engagement messages.\nFeatures were represented with bag-of-words and word2vec, and models were\nconstructed using support vector machines (SVMs) and convolutional (CNNs) and\nrecurrent neural networks (RNNs). The best performing classifier was an RNN\nwith a custom-trained word2vec model to represent features, which achieved the\nF1 measure of 0.839.", "published": "2018-01-02 19:09:47", "link": "http://arxiv.org/abs/1801.00801v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Did you hear that? Adversarial Examples Against Automatic Speech\n  Recognition", "abstract": "Speech is a common and effective way of communication between humans, and\nmodern consumer devices such as smartphones and home hubs are equipped with\ndeep learning based accurate automatic speech recognition to enable natural\ninteraction between humans and machines. Recently, researchers have\ndemonstrated powerful attacks against machine learning models that can fool\nthem to produceincorrect results. However, nearly all previous research in\nadversarial attacks has focused on image recognition and object detection\nmodels. In this short paper, we present a first of its kind demonstration of\nadversarial attacks against speech classification model. Our algorithm performs\ntargeted attacks with 87% success by adding small background noise without\nhaving to know the underlying model parameter and architecture. Our attack only\nchanges the least significant bits of a subset of audio clip samples, and the\nnoise does not change 89% the human listener's perception of the audio clip as\nevaluated in our human study.", "published": "2018-01-02 05:24:30", "link": "http://arxiv.org/abs/1801.00554v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Matching with Text Data: An Experimental Evaluation of Methods for\n  Matching Documents and of Measuring Match Quality", "abstract": "Matching for causal inference is a well-studied problem, but standard methods\nfail when the units to match are text documents: the high-dimensional and rich\nnature of the data renders exact matching infeasible, causes propensity scores\nto produce incomparable matches, and makes assessing match quality difficult.\nIn this paper, we characterize a framework for matching text documents that\ndecomposes existing methods into: (1) the choice of text representation, and\n(2) the choice of distance metric. We investigate how different choices within\nthis framework affect both the quantity and quality of matches identified\nthrough a systematic multifactor evaluation experiment using human subjects.\nAltogether we evaluate over 100 unique text matching methods along with 5\ncomparison methods taken from the literature. Our experimental results identify\nmethods that generate matches with higher subjective match quality than current\nstate-of-the-art techniques. We enhance the precision of these results by\ndeveloping a predictive model to estimate the match quality of pairs of text\ndocuments as a function of our various distance scores. This model, which we\nfind successfully mimics human judgment, also allows for approximate and\nunsupervised evaluation of new procedures. We then employ the identified best\nmethod to illustrate the utility of text matching in two applications. First,\nwe engage with a substantive debate in the study of media bias by using text\nmatching to control for topic selection when comparing news articles from\nthirteen news sources. We then show how conditioning on text data leads to more\nprecise causal inferences in an observational study examining the effects of a\nmedical intervention.", "published": "2018-01-02 13:47:43", "link": "http://arxiv.org/abs/1801.00644v7", "categories": ["stat.ME", "cs.CL"], "primary_category": "stat.ME"}
{"title": "Character-level Recurrent Neural Networks in Practice: Comparing\n  Training and Sampling Schemes", "abstract": "Recurrent neural networks are nowadays successfully used in an abundance of\napplications, going from text, speech and image processing to recommender\nsystems. Backpropagation through time is the algorithm that is commonly used to\ntrain these networks on specific tasks. Many deep learning frameworks have\ntheir own implementation of training and sampling procedures for recurrent\nneural networks, while there are in fact multiple other possibilities to choose\nfrom and other parameters to tune. In existing literature this is very often\noverlooked or ignored. In this paper we therefore give an overview of possible\ntraining and sampling schemes for character-level recurrent neural networks to\nsolve the task of predicting the next token in a given sequence. We test these\ndifferent schemes on a variety of datasets, neural network architectures and\nparameter settings, and formulate a number of take-home recommendations. The\nchoice of training and sampling scheme turns out to be subject to a number of\ntrade-offs, such as training stability, sampling time, model performance and\nimplementation effort, but is largely independent of the data. Perhaps the most\nsurprising result is that transferring hidden states for correctly initializing\nthe model on subsequences often leads to unstable training behavior depending\non the dataset.", "published": "2018-01-02 12:50:12", "link": "http://arxiv.org/abs/1801.00632v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Exploring Architectures, Data and Units For Streaming End-to-End Speech\n  Recognition with RNN-Transducer", "abstract": "We investigate training end-to-end speech recognition models with the\nrecurrent neural network transducer (RNN-T): a streaming, all-neural,\nsequence-to-sequence architecture which jointly learns acoustic and language\nmodel components from transcribed acoustic data. We explore various model\narchitectures and demonstrate how the model can be improved further if\nadditional text or pronunciation data are available. The model consists of an\n`encoder', which is initialized from a connectionist temporal\nclassification-based (CTC) acoustic model, and a `decoder' which is partially\ninitialized from a recurrent neural network language model trained on text data\nalone. The entire neural network is trained with the RNN-T loss and directly\noutputs the recognized transcript as a sequence of graphemes, thus performing\nend-to-end speech recognition. We find that performance can be improved further\nthrough the use of sub-word units (`wordpieces') which capture longer context\nand significantly reduce substitution errors. The best RNN-T system, a\ntwelve-layer LSTM encoder with a two-layer LSTM decoder trained with 30,000\nwordpieces as output targets achieves a word error rate of 8.5\\% on\nvoice-search and 5.2\\% on voice-dictation tasks and is comparable to a\nstate-of-the-art baseline at 8.3\\% on voice-search and 5.4\\% voice-dictation.", "published": "2018-01-02 21:29:41", "link": "http://arxiv.org/abs/1801.00841v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning audio and image representations with bio-inspired trainable\n  feature extractors", "abstract": "Recent advancements in pattern recognition and signal processing concern the\nautomatic learning of data representations from labeled training samples.\nTypical approaches are based on deep learning and convolutional neural\nnetworks, which require large amount of labeled training samples. In this work,\nwe propose novel feature extractors that can be used to learn the\nrepresentation of single prototype samples in an automatic configuration\nprocess. We employ the proposed feature extractors in applications of audio and\nimage processing, and show their effectiveness on benchmark data sets.", "published": "2018-01-02 15:34:03", "link": "http://arxiv.org/abs/1801.00688v1", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
