{"title": "From Review to Rating: Exploring Dependency Measures for Text\n  Classification", "abstract": "Various text analysis techniques exist, which attempt to uncover unstructured\ninformation from text. In this work, we explore using statistical dependence\nmeasures for textual classification, representing text as word vectors. Student\nsatisfaction scores on a 3-point scale and their free text comments written\nabout university subjects are used as the dataset. We have compared two textual\nrepresentations: a frequency word representation and term frequency\nrelationship to word vectors, and found that word vectors provide a greater\naccuracy. However, these word vectors have a large number of features which\naggravates the burden of computational complexity. Thus, we explored using a\nnon-linear dependency measure for feature selection by maximizing the\ndependence between the text reviews and corresponding scores. Our quantitative\nand qualitative analysis on a student satisfaction dataset shows that our\napproach achieves comparable accuracy to the full feature vector, while being\nan order of magnitude faster in testing. These text analysis and feature\nreduction techniques can be used for other textual data applications such as\nsentiment analysis.", "published": "2017-09-04 05:38:35", "link": "http://arxiv.org/abs/1709.00813v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hypothesis Testing based Intrinsic Evaluation of Word Embeddings", "abstract": "We introduce the cross-match test - an exact, distribution free,\nhigh-dimensional hypothesis test as an intrinsic evaluation metric for word\nembeddings. We show that cross-match is an effective means of measuring\ndistributional similarity between different vector representations and of\nevaluating the statistical significance of different vector embedding models.\nAdditionally, we find that cross-match can be used to provide a quantitative\nmeasure of linguistic similarity for selecting bridge languages for machine\ntranslation. We demonstrate that the results of the hypothesis test align with\nour expectations and note that the framework of two sample hypothesis testing\nis not limited to word embeddings and can be extended to all vector\nrepresentations.", "published": "2017-09-04 06:29:36", "link": "http://arxiv.org/abs/1709.00831v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Getting Reliable Annotations for Sarcasm in Online Dialogues", "abstract": "The language used in online forums differs in many ways from that of\ntraditional language resources such as news. One difference is the use and\nfrequency of nonliteral, subjective dialogue acts such as sarcasm. Whether the\naim is to develop a theory of sarcasm in dialogue, or engineer automatic\nmethods for reliably detecting sarcasm, a major challenge is simply the\ndifficulty of getting enough reliably labelled examples. In this paper we\ndescribe our work on methods for achieving highly reliable sarcasm annotations\nfrom untrained annotators on Mechanical Turk. We explore the use of a number of\ncommon statistical reliability measures, such as Kappa, Karger's, Majority\nClass, and EM. We show that more sophisticated measures do not appear to yield\nbetter results for our data than simple measures such as assuming that the\ncorrect label is the one that a majority of Turkers apply.", "published": "2017-09-04 16:54:35", "link": "http://arxiv.org/abs/1709.01042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Query-based Generative Model for Question Generation and\n  Question Answering", "abstract": "We propose a query-based generative model for solving both tasks of question\ngeneration (QG) and question an- swering (QA). The model follows the classic\nencoder- decoder framework. The encoder takes a passage and a query as input\nthen performs query understanding by matching the query with the passage from\nmultiple per- spectives. The decoder is an attention-based Long Short Term\nMemory (LSTM) model with copy and coverage mechanisms. In the QG task, a\nquestion is generated from the system given the passage and the target answer,\nwhereas in the QA task, the answer is generated given the question and the\npassage. During the training stage, we leverage a policy-gradient reinforcement\nlearning algorithm to overcome exposure bias, a major prob- lem resulted from\nsequence learning with cross-entropy loss. For the QG task, our experiments\nshow higher per- formances than the state-of-the-art results. When used as\nadditional training data, the automatically generated questions even improve\nthe performance of a strong ex- tractive QA system. In addition, our model\nshows bet- ter performance than the state-of-the-art baselines of the\ngenerative QA task.", "published": "2017-09-04 17:54:49", "link": "http://arxiv.org/abs/1709.01058v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do latent tree learning models identify meaningful structure in\n  sentences?", "abstract": "Recent work on the problem of latent tree learning has made it possible to\ntrain neural networks that learn to both parse a sentence and use the resulting\nparse to interpret the sentence, all without exposure to ground-truth parse\ntrees at training time. Surprisingly, these models often perform better at\nsentence understanding tasks than models that use parse trees from conventional\nparsers. This paper aims to investigate what these latent tree learning models\nlearn. We replicate two such models in a shared codebase and find that (i) only\none of these models outperforms conventional tree-structured models on sentence\nclassification, (ii) its parsing strategies are not especially consistent\nacross random restarts, (iii) the parses it produces tend to be shallower than\nstandard Penn Treebank (PTB) parses, and (iv) they do not resemble those of PTB\nor any other semantic or syntactic formalism that the authors are aware of.", "published": "2017-09-04 19:05:39", "link": "http://arxiv.org/abs/1709.01121v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Neural Word Salience Scores", "abstract": "Measuring the salience of a word is an essential step in numerous NLP tasks.\nHeuristic approaches such as tfidf have been used so far to estimate the\nsalience of words. We propose \\emph{Neural Word Salience} (NWS) scores, unlike\nheuristics, are learnt from a corpus. Specifically, we learn word salience\nscores such that, using pre-trained word embeddings as the input, can\naccurately predict the words that appear in a sentence, given the words that\nappear in the sentences preceding or succeeding that sentence. Experimental\nresults on sentence similarity prediction show that the learnt word salience\nscores perform comparably or better than some of the state-of-the-art\napproaches for representing sentences on benchmark datasets for sentence\nsimilarity, while using only a fraction of the training and prediction times\nrequired by prior methods. Moreover, our NWS scores positively correlate with\npsycholinguistic measures such as concreteness, and imageability implying a\nclose connection to the salience as perceived by humans.", "published": "2017-09-04 22:52:59", "link": "http://arxiv.org/abs/1709.01186v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Satirical News Detection and Analysis using Attention Mechanism and\n  Linguistic Features", "abstract": "Satirical news is considered to be entertainment, but it is potentially\ndeceptive and harmful. Despite the embedded genre in the article, not everyone\ncan recognize the satirical cues and therefore believe the news as true news.\nWe observe that satirical cues are often reflected in certain paragraphs rather\nthan the whole document. Existing works only consider document-level features\nto detect the satire, which could be limited. We consider paragraph-level\nlinguistic features to unveil the satire by incorporating neural network and\nattention mechanism. We investigate the difference between paragraph-level\nfeatures and document-level features, and analyze them on a large satirical\nnews dataset. The evaluation shows that the proposed model detects satirical\nnews effectively and reveals what features are important at which level.", "published": "2017-09-04 23:06:36", "link": "http://arxiv.org/abs/1709.01189v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compositional Approaches for Representing Relations Between Words: A\n  Comparative Study", "abstract": "Identifying the relations that exist between words (or entities) is important\nfor various natural language processing tasks such as, relational search,\nnoun-modifier classification and analogy detection. A popular approach to\nrepresent the relations between a pair of words is to extract the patterns in\nwhich the words co-occur with from a corpus, and assign each word-pair a vector\nof pattern frequencies. Despite the simplicity of this approach, it suffers\nfrom data sparseness, information scalability and linguistic creativity as the\nmodel is unable to handle previously unseen word pairs in a corpus. In\ncontrast, a compositional approach for representing relations between words\novercomes these issues by using the attributes of each individual word to\nindirectly compose a representation for the common relations that hold between\nthe two words. This study aims to compare different operations for creating\nrelation representations from word-level representations. We investigate the\nperformance of the compositional methods by measuring the relational\nsimilarities using several benchmark datasets for word analogy. Moreover, we\nevaluate the different relation representations in a knowledge base completion\ntask.", "published": "2017-09-04 23:30:22", "link": "http://arxiv.org/abs/1709.01193v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interactive Attention Networks for Aspect-Level Sentiment Classification", "abstract": "Aspect-level sentiment classification aims at identifying the sentiment\npolarity of specific target in its context. Previous approaches have realized\nthe importance of targets in sentiment classification and developed various\nmethods with the goal of precisely modeling their contexts via generating\ntarget-specific representations. However, these studies always ignore the\nseparate modeling of targets. In this paper, we argue that both targets and\ncontexts deserve special treatment and need to be learned their own\nrepresentations via interactive learning. Then, we propose the interactive\nattention networks (IAN) to interactively learn attentions in the contexts and\ntargets, and generate the representations for targets and contexts separately.\nWith this design, the IAN model can well represent a target and its collocative\ncontext, which is helpful to sentiment classification. Experimental results on\nSemEval 2014 Datasets demonstrate the effectiveness of our model.", "published": "2017-09-04 10:34:34", "link": "http://arxiv.org/abs/1709.00893v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Using Optimal Ratio Mask as Training Target for Supervised Speech\n  Separation", "abstract": "Supervised speech separation uses supervised learning algorithms to learn a\nmapping from an input noisy signal to an output target. With the fast\ndevelopment of deep learning, supervised separation has become the most\nimportant direction in speech separation area in recent years. For the\nsupervised algorithm, training target has a significant impact on the\nperformance. Ideal ratio mask is a commonly used training target, which can\nimprove the speech intelligibility and quality of the separated speech.\nHowever, it does not take into account the correlation between noise and clean\nspeech. In this paper, we use the optimal ratio mask as the training target of\nthe deep neural network (DNN) for speech separation. The experiments are\ncarried out under various noise environments and signal to noise ratio (SNR)\nconditions. The results show that the optimal ratio mask outperforms other\ntraining targets in general.", "published": "2017-09-04 12:25:18", "link": "http://arxiv.org/abs/1709.00917v1", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Learning Word Embeddings from the Portuguese Twitter Stream: A Study of\n  some Practical Aspects", "abstract": "This paper describes a preliminary study for producing and distributing a\nlarge-scale database of embeddings from the Portuguese Twitter stream. We start\nby experimenting with a relatively small sample and focusing on three\nchallenges: volume of training data, vocabulary size and intrinsic evaluation\nmetrics. Using a single GPU, we were able to scale up vocabulary size from 2048\nwords embedded and 500K training examples to 32768 words over 10M training\nexamples while keeping a stable validation loss and approximately linear trend\non training time per epoch. We also observed that using less than 50\\% of the\navailable training examples for each vocabulary size might result in\noverfitting. Results on intrinsic evaluation show promising performance for a\nvocabulary size of 32768 words. Nevertheless, intrinsic evaluation metrics\nsuffer from over-sensitivity to their corresponding cosine similarity\nthresholds, indicating that a wider range of metrics need to be developed to\ntrack progress.", "published": "2017-09-04 13:30:23", "link": "http://arxiv.org/abs/1709.00947v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Storytelling Agents with Personality and Adaptivity", "abstract": "We explore the expression of personality and adaptivity through the gestures\nof virtual agents in a storytelling task. We conduct two experiments using four\ndifferent dialogic stories. We manipulate agent personality on the extraversion\nscale, whether the agents adapt to one another in their gestural performance\nand agent gender. Our results show that subjects are able to perceive the\nintended variation in extraversion between different virtual agents,\nindependently of the story they are telling and the gender of the agent. A\nsecond study shows that subjects also prefer adaptive to nonadaptive virtual\nagents.", "published": "2017-09-04 23:06:05", "link": "http://arxiv.org/abs/1709.01188v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
