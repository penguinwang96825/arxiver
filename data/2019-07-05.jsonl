{"title": "Improving Chemical Named Entity Recognition in Patents with\n  Contextualized Word Embeddings", "abstract": "Chemical patents are an important resource for chemical information. However,\nfew chemical Named Entity Recognition (NER) systems have been evaluated on\npatent documents, due in part to their structural and linguistic complexity. In\nthis paper, we explore the NER performance of a BiLSTM-CRF model utilising\npre-trained word embeddings, character-level word representations and\ncontextualized ELMo word representations for chemical patents. We compare word\nembeddings pre-trained on biomedical and chemical patent corpora. The effect of\ntokenizers optimized for the chemical domain on NER performance in chemical\npatents is also explored. The results on two patent corpora show that\ncontextualized word representations generated from ELMo substantially improve\nchemical NER performance w.r.t. the current state-of-the-art. We also show that\ndomain-specific resources such as word embeddings trained on chemical patents\nand chemical-specific tokenizers have a positive impact on NER performance.", "published": "2019-07-05 05:19:46", "link": "http://arxiv.org/abs/1907.02679v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Head-Driven Phrase Structure Grammar Parsing on Penn Treebank", "abstract": "Head-driven phrase structure grammar (HPSG) enjoys a uniform formalism\nrepresenting rich contextual syntactic and even semantic meanings. This paper\nmakes the first attempt to formulate a simplified HPSG by integrating\nconstituent and dependency formal representations into head-driven phrase\nstructure. Then two parsing algorithms are respectively proposed for two\nconverted tree representations, division span and joint span. As HPSG encodes\nboth constituent and dependency structure information, the proposed HPSG\nparsers may be regarded as a sort of joint decoder for both types of structures\nand thus are evaluated in terms of extracted or converted constituent and\ndependency parsing trees. Our parser achieves new state-of-the-art performance\nfor both parsing tasks on Penn Treebank (PTB) and Chinese Penn Treebank,\nverifying the effectiveness of joint learning constituent and dependency\nstructures. In details, we report 96.33 F1 of constituent parsing and 97.20\\%\nUAS of dependency parsing on PTB.", "published": "2019-07-05 05:44:21", "link": "http://arxiv.org/abs/1907.02684v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional\n  Encoder Representations from Transformer", "abstract": "An important yet rarely tackled problem in dialogue state tracking (DST) is\nscalability for dynamic ontology (e.g., movie, restaurant) and unseen slot\nvalues. We focus on a specific condition, where the ontology is unknown to the\nstate tracker, but the target slot value (except for none and dontcare),\npossibly unseen during training, can be found as word segment in the dialogue\ncontext. Prior approaches often rely on candidate generation from n-gram\nenumeration or slot tagger outputs, which can be inefficient or suffer from\nerror propagation. We propose BERT-DST, an end-to-end dialogue state tracker\nwhich directly extracts slot values from the dialogue context. We use BERT as\ndialogue context encoder whose contextualized language representations are\nsuitable for scalable DST to identify slot values from their semantic context.\nFurthermore, we employ encoder parameter sharing across all slots with two\nadvantages: (1) Number of parameters does not grow linearly with the ontology.\n(2) Language representation knowledge can be transferred among slots. Empirical\nevaluation shows BERT-DST with cross-slot parameter sharing outperforms prior\nwork on the benchmark scalable DST datasets Sim-M and Sim-R, and achieves\ncompetitive performance on the standard DSTC2 and WOZ 2.0 datasets.", "published": "2019-07-05 22:41:02", "link": "http://arxiv.org/abs/1907.03040v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Study of the Effect of Resolving Negation and Sentiment Analysis in\n  Recognizing Text Entailment for Arabic", "abstract": "Recognizing the entailment relation showed that its influence to extract the\nsemantic inferences in wide-ranging natural language processing domains (text\nsummarization, question answering, etc.) and enhanced the results of their\noutput. For Arabic language, few attempts concerns with Arabic entailment\nproblem. This paper aims to increase the entailment accuracy for Arabic texts\nby resolving negation of the text-hypothesis pair and determining the polarity\nof the text-hypothesis pair whether it is Positive, Negative or Neutral. It is\nnoticed that the absence of negation detection feature gives inaccurate results\nwhen detecting the entailment relation since the negation revers the truth. The\nnegation words are considered stop words and removed from the text-hypothesis\npair which may lead wrong entailment decision. Another case not solved\npreviously, it is impossible that the positive text entails negative text and\nvice versa. In this paper, in order to classify the text-hypothesis pair\npolarity, a sentiment analysis tool is used. We show that analyzing the\npolarity of the text-hypothesis pair increases the entailment accuracy. to\nevaluate our approach we used a dataset for Arabic textual entailment (ArbTEDS)\nconsisted of 618 text-hypothesis pairs and showed that the Arabic entailment\naccuracy is increased by resolving negation for entailment relation and\nanalyzing the polarity of the text-hypothesis pair.", "published": "2019-07-05 12:29:46", "link": "http://arxiv.org/abs/1907.03871v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Reinforcement Learning For Modeling Chit-Chat Dialog With Discrete\n  Attributes", "abstract": "Open domain dialog systems face the challenge of being repetitive and\nproducing generic responses. In this paper, we demonstrate that by conditioning\nthe response generation on interpretable discrete dialog attributes and\ncomposed attributes, it helps improve the model perplexity and results in\ndiverse and interesting non-redundant responses. We propose to formulate the\ndialog attribute prediction as a reinforcement learning (RL) problem and use\npolicy gradients methods to optimize utterance generation using long-term\nrewards. Unlike existing RL approaches which formulate the token prediction as\na policy, our method reduces the complexity of the policy optimization by\nlimiting the action space to dialog attributes, thereby making the policy\noptimization more practical and sample efficient. We demonstrate this with\nexperimental and human evaluations.", "published": "2019-07-05 14:19:15", "link": "http://arxiv.org/abs/1907.02848v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multi-lingual Intent Detection and Slot Filling in a Joint BERT-based\n  Model", "abstract": "Intent Detection and Slot Filling are two pillar tasks in Spoken Natural\nLanguage Understanding. Common approaches adopt joint Deep Learning\narchitectures in attention-based recurrent frameworks. In this work, we aim at\nexploiting the success of \"recurrence-less\" models for these tasks. We\nintroduce Bert-Joint, i.e., a multi-lingual joint text classification and\nsequence labeling framework. The experimental evaluation over two well-known\nEnglish benchmarks demonstrates the strong performances that can be obtained\nwith this model, even when few annotated data is available. Moreover, we\nannotated a new dataset for the Italian language, and we observed similar\nperformances without the need for changing the model.", "published": "2019-07-05 15:11:29", "link": "http://arxiv.org/abs/1907.02884v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MigrationMiner: An Automated Detection Tool of Third-Party Java Library\n  Migration at the Method Level", "abstract": "In this paper we introduce, MigrationMiner, an automated tool that detects\ncode migrations performed between Java third-party library. Given a list of\nopen source projects, the tool detects potential library migration code changes\nand collects the specific code fragments in which the developer replaces\nmethods from the retired library with methods from the new library. To support\nthe migration process, MigrationMiner collects the library documentation that\nis associated with every method involved in the migration. We evaluate our tool\non a benchmark of manually validated library migrations. Results show that\nMigrationMiner achieves an accuracy of 100%. A demo video of MigrationMiner is\navailable at https://youtu.be/sAlR1HNetXc.", "published": "2019-07-05 18:53:20", "link": "http://arxiv.org/abs/1907.02997v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Towards Universal Dialogue Act Tagging for Task-Oriented Dialogues", "abstract": "Machine learning approaches for building task-oriented dialogue systems\nrequire large conversational datasets with labels to train on. We are\ninterested in building task-oriented dialogue systems from human-human\nconversations, which may be available in ample amounts in existing customer\ncare center logs or can be collected from crowd workers. Annotating these\ndatasets can be prohibitively expensive. Recently multiple annotated\ntask-oriented human-machine dialogue datasets have been released, however their\nannotation schema varies across different collections, even for well-defined\ncategories such as dialogue acts (DAs). We propose a Universal DA schema for\ntask-oriented dialogues and align existing annotated datasets with our schema.\nOur aim is to train a Universal DA tagger (U-DAT) for task-oriented dialogues\nand use it for tagging human-human conversations. We investigate multiple\ndatasets, propose manual and automated approaches for aligning the different\nschema, and present results on a target corpus of human-human dialogues. In\nunsupervised learning experiments we achieve an F1 score of 54.1% on system\nturns in human-human dialogues. In a semi-supervised setup, the F1 score\nincreases to 57.7% which would otherwise require at least 1.7K manually\nannotated turns. For new domains, we show further improvements when unlabeled\nor labeled target domain data is available.", "published": "2019-07-05 20:43:30", "link": "http://arxiv.org/abs/1907.03020v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Video Question Generation via Cross-Modal Self-Attention Networks\n  Learning", "abstract": "We introduce a novel task, Video Question Generation (Video QG). A Video QG\nmodel automatically generates questions given a video clip and its\ncorresponding dialogues. Video QG requires a range of skills -- sentence\ncomprehension, temporal relation, the interplay between vision and language,\nand the ability to ask meaningful questions. To address this, we propose a\nnovel semantic rich cross-modal self-attention (SRCMSA) network to aggregate\nthe multi-modal and diverse features. To be more precise, we enhance the video\nframes semantic by integrating the object-level information, and we jointly\nconsider the cross-modal attention for the video question generation task.\nExcitingly, our proposed model remarkably improves the baseline from 7.58 to\n14.48 in the BLEU-4 score on the TVQA dataset. Most of all, we arguably pave a\nnovel path toward understanding the challenging video input and we provide\ndetailed analysis in terms of diversity, which ushers the avenues for future\ninvestigations.", "published": "2019-07-05 23:47:04", "link": "http://arxiv.org/abs/1907.03049v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Methodology for Controlling the Emotional Expressiveness in Synthetic\n  Speech -- a Deep Learning approach", "abstract": "In this project, we aim to build a Text-to-Speech system able to produce\nspeech with a controllable emotional expressiveness. We propose a methodology\nfor solving this problem in three main steps. The first is the collection of\nemotional speech data. We discuss the various formats of existing datasets and\ntheir usability in speech generation. The second step is the development of a\nsystem to automatically annotate data with emotion/expressiveness features. We\ncompare several techniques using transfer learning to extract such a\nrepresentation through other tasks and propose a method to visualize and\ninterpret the correlation between vocal and emotional features. The third step\nis the development of a deep learning-based system taking text and\nemotion/expressiveness as input and producing speech as output. We study the\nimpact of fine tuning from a neutral TTS towards an emotional TTS in terms of\nintelligibility and perception of the emotion.", "published": "2019-07-05 12:00:53", "link": "http://arxiv.org/abs/1907.02784v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Neural Baselines for Computational Paralinguistics", "abstract": "Detecting sleepiness from spoken language is an ambitious task, which is\naddressed by the Interspeech 2019 Computational Paralinguistics Challenge\n(ComParE). We propose an end-to-end deep learning approach to detect and\nclassify patterns reflecting sleepiness in the human voice. Our approach is\nbased solely on a moderately complex deep neural network architecture. It may\nbe applied directly on the audio data without requiring any specific feature\nengineering, thus remaining transferable to other audio classification tasks.\nNevertheless, our approach performs similar to state-of-the-art machine\nlearning models.", "published": "2019-07-05 14:43:55", "link": "http://arxiv.org/abs/1907.02864v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NeuType: A Simple and Effective Neural Network Approach for Predicting\n  Missing Entity Type Information in Knowledge Bases", "abstract": "Knowledge bases store information about the semantic types of entities, which\ncan be utilized in a range of information access tasks. This information,\nhowever, is often incomplete, due to new entities emerging on a daily basis. We\naddress the task of automatically assigning types to entities in a knowledge\nbase from a type taxonomy. Specifically, we present two neural network\narchitectures, which take short entity descriptions and, optionally,\ninformation about related entities as input. Using the DBpedia knowledge base\nfor experimental evaluation, we demonstrate that these simple architectures\nyield significant improvements over the current state of the art.", "published": "2019-07-05 19:47:10", "link": "http://arxiv.org/abs/1907.03007v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Extraction and Analysis of Fictional Character Networks: A Survey", "abstract": "A character network is a graph extracted from a narrative, in which vertices\nrepresent characters and edges correspond to interactions between them. A\nnumber of narrative-related problems can be addressed automatically through the\nanalysis of character networks, such as summarization, classification, or role\ndetection. Character networks are particularly relevant when considering works\nof fictions (e.g. novels, plays, movies, TV series), as their exploitation\nallows developing information retrieval and recommendation systems. However,\nworks of fiction possess specific properties making these tasks harder. This\nsurvey aims at presenting and organizing the scientific literature related to\nthe extraction of character networks from works of fiction, as well as their\nanalysis. We first describe the extraction process in a generic way, and\nexplain how its constituting steps are implemented in practice, depending on\nthe medium of the narrative, the goal of the network analysis, and other\nfactors. We then review the descriptive tools used to characterize character\nnetworks, with a focus on the way they are interpreted in this context. We\nillustrate the relevance of character networks by also providing a review of\napplications derived from their analysis. Finally, we identify the limitations\nof the existing approaches, and the most promising perspectives.", "published": "2019-07-05 07:27:31", "link": "http://arxiv.org/abs/1907.02704v5", "categories": ["cs.SI", "cs.CL", "cs.CV", "cs.IR", "cs.MM"], "primary_category": "cs.SI"}
{"title": "Zero-shot Learning for Audio-based Music Classification and Tagging", "abstract": "Audio-based music classification and tagging is typically based on\ncategorical supervised learning with a fixed set of labels. This intrinsically\ncannot handle unseen labels such as newly added music genres or semantic words\nthat users arbitrarily choose for music retrieval. Zero-shot learning can\naddress this problem by leveraging an additional semantic space of labels where\nside information about the labels is used to unveil the relationship between\neach other. In this work, we investigate the zero-shot learning in the music\ndomain and organize two different setups of side information. One is using\nhuman-labeled attribute information based on Free Music Archive and\nOpenMIC-2018 datasets. The other is using general word semantic information\nbased on Million Song Dataset and Last.fm tag annotations. Considering a music\ntrack is usually multi-labeled in music classification and tagging datasets, we\nalso propose a data split scheme and associated evaluation settings for the\nmulti-label zero-shot learning. Finally, we report experimental results and\ndiscuss the effectiveness and new possibilities of zero-shot learning in the\nmusic domain.", "published": "2019-07-05 04:19:37", "link": "http://arxiv.org/abs/1907.02670v2", "categories": ["cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "A Bi-directional Transformer for Musical Chord Recognition", "abstract": "Chord recognition is an important task since chords are highly abstract and\ndescriptive features of music. For effective chord recognition, it is essential\nto utilize relevant context in audio sequence. While various machine learning\nmodels such as convolutional neural networks (CNNs) and recurrent neural\nnetworks (RNNs) have been employed for the task, most of them have limitations\nin capturing long-term dependency or require training of an additional model.\nIn this work, we utilize a self-attention mechanism for chord recognition to\nfocus on certain regions of chords. Training of the proposed bi-directional\nTransformer for chord recognition (BTC) consists of a single phase while\nshowing competitive performance. Through an attention map analysis, we have\nvisualized how attention was performed. It turns out that the model was able to\ndivide segments of chords by utilizing adaptive receptive field of the\nattention mechanism. Furthermore, it was observed that the model was able to\neffectively capture long-term dependencies, making use of essential information\nregardless of distance.", "published": "2019-07-05 07:00:38", "link": "http://arxiv.org/abs/1907.02698v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Jointly Aligning and Predicting Continuous Emotion Annotations", "abstract": "Time-continuous dimensional descriptions of emotions (e.g., arousal, valence)\nallow researchers to characterize short-time changes and to capture long-term\ntrends in emotion expression. However, continuous emotion labels are generally\nnot synchronized with the input speech signal due to delays caused by\nreaction-time, which is inherent in human evaluations. To deal with this\nchallenge, we introduce a new convolutional neural network (multi-delay sinc\nnetwork) that is able to simultaneously align and predict labels in an\nend-to-end manner. The proposed network is a stack of convolutional layers\nfollowed by an aligner network that aligns the speech signal and emotion\nlabels. This network is implemented using a new convolutional layer that we\nintroduce, the delayed sinc layer. It is a time-shifted low-pass (sinc) filter\nthat uses a gradient-based algorithm to learn a single delay. Multiple delayed\nsinc layers can be used to compensate for a non-stationary delay that is a\nfunction of the acoustic space. We test the efficacy of this system on two\ncommon emotion datasets, RECOLA and SEWA, and show that this approach obtains\nstate-of-the-art speech-only results by learning time-varying delays while\npredicting dimensional descriptors of emotions.", "published": "2019-07-05 23:49:49", "link": "http://arxiv.org/abs/1907.03050v2", "categories": ["cs.LG", "cs.HC", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Synchronizing Audio-Visual Film Stimuli in Unity (version 5.5.1f1): Game\n  Engines as a Tool for Research", "abstract": "Unity is a software specifically designed for the development of video games.\nHowever, due to its programming possibilities and the polyvalence of its\narchitecture, it can prove to be a versatile tool for stimuli presentation in\nresearch experiments. Nevertheless, it also has some limitations and conditions\nthat need to be taken into account to ensure optimal performance in particular\nexperimental situations. Such is the case if we want to use it in an\nexperimental design that includes the acquisition of biometric signals\nsynchronized with the broadcasting of video and audio in real time. In the\npresent paper, we analyse how Unity (version 5.5.1f1) reacts in one such\nexperimental design that requires the execution of audio-visual material. From\nthe analysis of an experimental procedure in which the video was executed\nfollowing the standard software specifications, we have detected the following\nproblems desynchronization between the emission of the video and the audio;\ndesynchronization between the temporary counter and the video; a delay in the\nexecution of the screenshot; and depending on the encoding of the video a bad\nfluency in the video playback, which even though it maintains the total\nplayback time, it causes Unity to freeze frames and proceed to compensate with\nlittle temporary jumps in the video. Finally, having detected all the problems,\na compensation and verification process is designed to be able to work with\naudio-visual material in Unity (version 5.5.1f1) in an accurate way. We present\na protocol for checks and compensations that allows solving these problems to\nensure the execution of robust experiments in terms of reliability.", "published": "2019-07-05 09:05:37", "link": "http://arxiv.org/abs/1907.04926v1", "categories": ["eess.AS", "cs.MM", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Speech bandwidth extension with WaveNet", "abstract": "Large-scale mobile communication systems tend to contain legacy transmission\nchannels with narrowband bottlenecks, resulting in characteristic\n\"telephone-quality\" audio. While higher quality codecs exist, due to the scale\nand heterogeneity of the networks, transmitting higher sample rate audio with\nmodern high-quality audio codecs can be difficult in practice. This paper\nproposes an approach where a communication node can instead extend the\nbandwidth of a band-limited incoming speech signal that may have been passed\nthrough a low-rate codec. To this end, we propose a WaveNet-based model\nconditioned on a log-mel spectrogram representation of a bandwidth-constrained\nspeech audio signal of 8 kHz and audio with artifacts from GSM full-rate (FR)\ncompression to reconstruct the higher-resolution signal. In our experimental\nMUSHRA evaluation, we show that a model trained to upsample to 24kHz speech\nsignals from audio passed through the 8kHz GSM-FR codec is able to reconstruct\naudio only slightly lower in quality to that of the Adaptive Multi-Rate\nWideband audio codec (AMR-WB) codec at 16kHz, and closes around half the gap in\nperceptual quality between the original encoded signal and the original speech\nsampled at 24kHz. We further show that when the same model is passed 8kHz audio\nthat has not been compressed, is able to again reconstruct audio of slightly\nbetter quality than 16kHz AMR-WB, in the same MUSHRA evaluation.", "published": "2019-07-05 20:17:31", "link": "http://arxiv.org/abs/1907.04927v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "The DKU Replay Detection System for the ASVspoof 2019 Challenge: On Data\n  Augmentation, Feature Representation, Classification, and Fusion", "abstract": "This paper describes our DKU replay detection system for the ASVspoof 2019\nchallenge. The goal is to develop spoofing countermeasure for automatic speaker\nrecognition in physical access scenario. We leverage the countermeasure system\npipeline from four aspects, including the data augmentation, feature\nrepresentation, classification, and fusion. First, we introduce an\nutterance-level deep learning framework for anti-spoofing. It receives the\nvariable-length feature sequence and outputs the utterance-level scores\ndirectly. Based on the framework, we try out various kinds of input feature\nrepresentations extracted from either the magnitude spectrum or phase spectrum.\nBesides, we also perform the data augmentation strategy by applying the speed\nperturbation on the raw waveform. Our best single system employs a residual\nneural network trained by the speed-perturbed group delay gram. It achieves EER\nof 1.04% on the development set, as well as EER of 1.08% on the evaluation set.\nFinally, using the simple average score from several single systems can further\nimprove the performance. EER of 0.24% on the development set and 0.66% on the\nevaluation set is obtained for our primary system.", "published": "2019-07-05 03:00:05", "link": "http://arxiv.org/abs/1907.02663v1", "categories": ["eess.AS", "cs.CR", "cs.LG", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
