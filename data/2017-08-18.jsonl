{"title": "A Question Answering Approach to Emotion Cause Extraction", "abstract": "Emotion cause extraction aims to identify the reasons behind a certain\nemotion expressed in text. It is a much more difficult task compared to emotion\nclassification. Inspired by recent advances in using deep memory networks for\nquestion answering (QA), we propose a new approach which considers emotion\ncause identification as a reading comprehension task in QA. Inspired by\nconvolutional neural networks, we propose a new mechanism to store relevant\ncontext in different memory slots to model context information. Our proposed\napproach can extract both word level sequence features and lexical features.\nPerformance evaluation shows that our method achieves the state-of-the-art\nperformance on a recently released emotion cause dataset, outperforming a\nnumber of competitive baselines by at least 3.01% in F-measure.", "published": "2017-08-18 02:07:36", "link": "http://arxiv.org/abs/1708.05482v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syllable-level Neural Language Model for Agglutinative Language", "abstract": "Language models for agglutinative languages have always been hindered in past\ndue to myriad of agglutinations possible to any given word through various\naffixes. We propose a method to diminish the problem of out-of-vocabulary words\nby introducing an embedding derived from syllables and morphemes which\nleverages the agglutinative property. Our model outperforms character-level\nembedding in perplexity by 16.87 with 9.50M parameters. Proposed method\nachieves state of the art performance over existing input prediction methods in\nterms of Key Stroke Saving and has been commercialized.", "published": "2017-08-18 06:02:16", "link": "http://arxiv.org/abs/1708.05515v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EmoAtt at EmoInt-2017: Inner attention sentence embedding for Emotion\n  Intensity", "abstract": "In this paper we describe a deep learning system that has been designed and\nbuilt for the WASSA 2017 Emotion Intensity Shared Task. We introduce a\nrepresentation learning approach based on inner attention on top of an RNN.\nResults show that our model offers good capabilities and is able to\nsuccessfully identify emotion-bearing words to predict intensity without\nleveraging on lexicons, obtaining the 13th place among 22 shared task\ncompetitors.", "published": "2017-08-18 06:59:16", "link": "http://arxiv.org/abs/1708.05521v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Stylistic Properties of Neurally Generated Text in\n  Authorship Attribution", "abstract": "Recent applications of neural language models have led to an increased\ninterest in the automatic generation of natural language. However impressive,\nthe evaluation of neurally generated text has so far remained rather informal\nand anecdotal. Here, we present an attempt at the systematic assessment of one\naspect of the quality of neurally generated text. We focus on a specific aspect\nof neural language generation: its ability to reproduce authorial writing\nstyles. Using established models for authorship attribution, we empirically\nassess the stylistic qualities of neurally generated text. In comparison to\nconventional language models, neural models generate fuzzier text that is\nrelatively harder to attribute correctly. Nevertheless, our results also\nsuggest that neurally generated text offers more valuable perspectives for the\naugmentation of training data.", "published": "2017-08-18 08:43:52", "link": "http://arxiv.org/abs/1708.05536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Agree to Disagree: Improving Disagreement Detection with Dual GRUs", "abstract": "This paper presents models for detecting agreement/disagreement in online\ndiscussions. In this work we show that by using a Siamese inspired architecture\nto encode the discussions, we no longer need to rely on hand-crafted features\nto exploit the meta thread structure. We evaluate our model on existing online\ndiscussion corpora - ABCD, IAC and AWTP. Experimental results on ABCD dataset\nshow that by fusing lexical and word embedding features, our model achieves the\nstate of the art performance of 0.804 average F1 score. We also show that the\nmodel trained on ABCD dataset performs competitively on relatively smaller\nannotated datasets (IAC and AWTP).", "published": "2017-08-18 12:34:11", "link": "http://arxiv.org/abs/1708.05582v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Future Word Contexts in Neural Network Language Models", "abstract": "Recently, bidirectional recurrent network language models (bi-RNNLMs) have\nbeen shown to outperform standard, unidirectional, recurrent neural network\nlanguage models (uni-RNNLMs) on a range of speech recognition tasks. This\nindicates that future word context information beyond the word history can be\nuseful. However, bi-RNNLMs pose a number of challenges as they make use of the\ncomplete previous and future word context information. This impacts both\ntraining efficiency and their use within a lattice rescoring framework. In this\npaper these issues are addressed by proposing a novel neural network structure,\nsucceeding word RNNLMs (su-RNNLMs). Instead of using a recurrent unit to\ncapture the complete future word contexts, a feedforward unit is used to model\na finite number of succeeding, future, words. This model can be trained much\nmore efficiently than bi-RNNLMs and can also be used for lattice rescoring.\nExperimental results on a meeting transcription task (AMI) show the proposed\nmodel consistently outperformed uni-RNNLMs and yield only a slight degradation\ncompared to bi-RNNLMs in N-best rescoring. Additionally, performance\nimprovements can be obtained using lattice rescoring and subsequent confusion\nnetwork decoding.", "published": "2017-08-18 13:11:22", "link": "http://arxiv.org/abs/1708.05592v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Dependency Parsing for Closely Related Languages -\n  Helsinki's Submission to VarDial 2017", "abstract": "This paper describes the submission from the University of Helsinki to the\nshared task on cross-lingual dependency parsing at VarDial 2017. We present\nwork on annotation projection and treebank translation that gave good results\nfor all three target languages in the test set. In particular, Slovak seems to\nwork well with information coming from the Czech treebank, which is in line\nwith related work. The attachment scores for cross-lingual models even surpass\nthe fully supervised models trained on the target language treebank. Croatian\nis the most difficult language in the test set and the improvements over the\nbaseline are rather modest. Norwegian works best with information coming from\nSwedish whereas Danish contributes surprisingly little.", "published": "2017-08-18 18:00:05", "link": "http://arxiv.org/abs/1708.05719v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural machine translation for low-resource languages", "abstract": "Neural machine translation (NMT) approaches have improved the state of the\nart in many machine translation settings over the last couple of years, but\nthey require large amounts of training data to produce sensible output. We\ndemonstrate that NMT can be used for low-resource languages as well, by\nintroducing more local dependencies and using word alignments to learn sentence\nreordering during translation. In addition to our novel model, we also present\nan empirical evaluation of low-resource phrase-based statistical machine\ntranslation (SMT) and NMT to investigate the lower limits of the respective\ntechnologies. We find that while SMT remains the best option for low-resource\nsettings, our method can produce acceptable translations with only 70000 tokens\nof training data, a level where the baseline NMT system fails completely.", "published": "2017-08-18 18:16:23", "link": "http://arxiv.org/abs/1708.05729v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Natural Stories Corpus", "abstract": "It is now a common practice to compare models of human language processing by\npredicting participant reactions (such as reading times) to corpora consisting\nof rich naturalistic linguistic materials. However, many of the corpora used in\nthese studies are based on naturalistic text and thus do not contain many of\nthe low-frequency syntactic constructions that are often required to\ndistinguish processing theories. Here we describe a new corpus consisting of\nEnglish texts edited to contain many low-frequency syntactic constructions\nwhile still sounding fluent to native speakers. The corpus is annotated with\nhand-corrected parse trees and includes self-paced reading time data. Here we\ngive an overview of the content of the corpus and release the data.", "published": "2017-08-18 21:27:34", "link": "http://arxiv.org/abs/1708.05763v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LADDER: A Human-Level Bidding Agent for Large-Scale Real-Time Online\n  Auctions", "abstract": "We present LADDER, the first deep reinforcement learning agent that can\nsuccessfully learn control policies for large-scale real-world problems\ndirectly from raw inputs composed of high-level semantic information. The agent\nis based on an asynchronous stochastic variant of DQN (Deep Q Network) named\nDASQN. The inputs of the agent are plain-text descriptions of states of a game\nof incomplete information, i.e. real-time large scale online auctions, and the\nrewards are auction profits of very large scale. We apply the agent to an\nessential portion of JD's online RTB (real-time bidding) advertising business\nand find that it easily beats the former state-of-the-art bidding policy that\nhad been carefully engineered and calibrated by human experts: during JD.com's\nJune 18th anniversary sale, the agent increased the company's ads revenue from\nthe portion by more than 50%, while the advertisers' ROI (return on investment)\nalso improved significantly.", "published": "2017-08-18 11:25:30", "link": "http://arxiv.org/abs/1708.05565v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.GT"], "primary_category": "cs.LG"}
