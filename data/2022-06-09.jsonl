{"title": "Crosslinguistic word order variation reflects evolutionary pressures of\n  dependency and information locality", "abstract": "Languages vary considerably in syntactic structure. About 40% of the world's\nlanguages have subject-verb-object order, and about 40% have\nsubject-object-verb order. Extensive work has sought to explain this word order\nvariation across languages. However, the existing approaches are not able to\nexplain coherently the frequency distribution and evolution of word order in\nindividual languages. We propose that variation in word order reflects\ndifferent ways of balancing competing pressures of dependency locality and\ninformation locality, whereby languages favor placing elements together when\nthey are syntactically related or contextually informative about each other.\nUsing data from 80 languages in 17 language families and phylogenetic modeling,\nwe demonstrate that languages evolve to balance these pressures, such that word\norder change is accompanied by change in the frequency distribution of the\nsyntactic structures which speakers communicate to maintain overall efficiency.\nVariability in word order thus reflects different ways in which languages\nresolve these evolutionary pressures. We identify relevant characteristics that\nresult from this joint optimization, particularly the frequency with which\nsubjects and objects are expressed together for the same verb. Our findings\nsuggest that syntactic structure and usage across languages co-adapt to support\nefficient communication under limited cognitive resources.", "published": "2022-06-09 02:56:53", "link": "http://arxiv.org/abs/2206.04239v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FOAM: A Follower-aware Speaker Model For Vision-and-Language Navigation", "abstract": "The speaker-follower models have proven to be effective in\nvision-and-language navigation, where a speaker model is used to synthesize new\ninstructions to augment the training data for a follower navigation model.\nHowever, in many of the previous methods, the generated instructions are not\ndirectly trained to optimize the performance of the follower. In this paper, we\npresent \\textsc{foam}, a \\textsc{Fo}llower-\\textsc{a}ware speaker\n\\textsc{M}odel that is constantly updated given the follower feedback, so that\nthe generated instructions can be more suitable to the current learning state\nof the follower. Specifically, we optimize the speaker using a bi-level\noptimization framework and obtain its training signals by evaluating the\nfollower on labeled data. Experimental results on the Room-to-Room and\nRoom-across-Room datasets demonstrate that our methods can outperform strong\nbaseline models across settings. Analyses also reveal that our generated\ninstructions are of higher quality than the baselines.", "published": "2022-06-09 06:11:07", "link": "http://arxiv.org/abs/2206.04294v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic-Controllable Summarization: Topic-Aware Evaluation and Transformer\n  Methods", "abstract": "Topic-controllable summarization is an emerging research area with a wide\nrange of potential applications. However, existing approaches suffer from\nsignificant limitations. For example, the majority of existing methods built\nupon recurrent architectures, which can significantly limit their performance\ncompared to more recent Transformer-based architectures, while they also\nrequire modifications to the model's architecture for controlling the topic. At\nthe same time, there is currently no established evaluation metric designed\nspecifically for topic-controllable summarization. This work proposes a new\ntopic-oriented evaluation measure to automatically evaluate the generated\nsummaries based on the topic affinity between the generated summary and the\ndesired topic. The reliability of the proposed measure is demonstrated through\nappropriately designed human evaluation. In addition, we adapt topic embeddings\nto work with powerful Transformer architectures and propose a novel and\nefficient approach for guiding the summary generation through control tokens.\nExperimental results reveal that control tokens can achieve better performance\ncompared to more complicated embedding-based approaches while also being\nsignificantly faster.", "published": "2022-06-09 07:28:16", "link": "http://arxiv.org/abs/2206.04317v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Identification for Austronesian Languages", "abstract": "This paper provides language identification models for low- and\nunder-resourced languages in the Pacific region with a focus on previously\nunavailable Austronesian languages. Accurate language identification is an\nimportant part of developing language resources. The approach taken in this\npaper combines 29 Austronesian languages with 171 non-Austronesian languages to\ncreate an evaluation set drawn from eight data sources. After evaluating six\napproaches to language identification, we find that a classifier based on\nskip-gram embeddings reaches a significantly higher performance than alternate\nmethods. We then systematically increase the number of non-Austronesian\nlanguages in the model up to a total of 800 languages to evaluate whether an\nincreased language inventory leads to less precise predictions for the\nAustronesian languages of interest. This evaluation finds that there is only a\nminimal impact on accuracy caused by increasing the inventory of\nnon-Austronesian languages. Further experiments adapt these language\nidentification models for code-switching detection, achieving high accuracy\nacross all 29 languages.", "published": "2022-06-09 08:08:18", "link": "http://arxiv.org/abs/2206.04327v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Embedding Reliability in Low-Resource Settings Using Corpus\n  Similarity Measures", "abstract": "This paper simulates a low-resource setting across 17 languages in order to\nevaluate embedding similarity, stability, and reliability under different\nconditions. The goal is to use corpus similarity measures before training to\npredict properties of embeddings after training. The main contribution of the\npaper is to show that it is possible to predict downstream embedding similarity\nusing upstream corpus similarity measures. This finding is then applied to\nlow-resource settings by modelling the reliability of embeddings created from\nvery limited training data. Results show that it is possible to estimate the\nreliability of low-resource embeddings using corpus similarity measures that\nremain robust on small amounts of data. These findings have significant\nimplications for the evaluation of truly low-resource languages in which such\nsystematic downstream validation methods are not possible because of data\nlimitations.", "published": "2022-06-09 08:13:29", "link": "http://arxiv.org/abs/2206.04330v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Corpus Similarity Measures Remain Robust Across Diverse Languages", "abstract": "This paper experiments with frequency-based corpus similarity measures across\n39 languages using a register prediction task. The goal is to quantify (i) the\ndistance between different corpora from the same language and (ii) the\nhomogeneity of individual corpora. Both of these goals are essential for\nmeasuring how well corpus-based linguistic analysis generalizes from one\ndataset to another. The problem is that previous work has focused on\nIndo-European languages, raising the question of whether these measures are\nable to provide robust generalizations across diverse languages. This paper\nuses a register prediction task to evaluate competing measures across 39\nlanguages: how well are they able to distinguish between corpora representing\ndifferent contexts of production? Each experiment compares three corpora from a\nsingle language, with the same three digital registers shared across all\nlanguages: social media, web pages, and Wikipedia. Results show that measures\nof corpus similarity retain their validity across different language families,\nwriting systems, and types of morphology. Further, the measures remain robust\nwhen evaluated on out-of-domain corpora, when applied to low-resource\nlanguages, and when applied to different sets of registers. These findings are\nsignificant given our need to make generalizations across the rapidly\nincreasing number of corpora available for analysis.", "published": "2022-06-09 08:17:16", "link": "http://arxiv.org/abs/2206.04332v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ancestor-to-Creole Transfer is Not a Walk in the Park", "abstract": "We aim to learn language models for Creole languages for which large volumes\nof data are not readily available, and therefore explore the potential transfer\nfrom ancestor languages (the 'Ancestry Transfer Hypothesis'). We find that\nstandard transfer methods do not facilitate ancestry transfer. Surprisingly,\ndifferent from other non-Creole languages, a very distinct two-phase pattern\nemerges for Creoles: As our training losses plateau, and language models begin\nto overfit on their source languages, perplexity on the Creoles drop. We\nexplore if this compression phase can lead to practically useful language\nmodels (the 'Ancestry Bottleneck Hypothesis'), but also falsify this. Moreover,\nwe show that Creoles even exhibit this two-phase pattern even when training on\nrandom, unrelated languages. Thus Creoles seem to be typological outliers and\nwe speculate whether there is a link between the two observations.", "published": "2022-06-09 09:28:10", "link": "http://arxiv.org/abs/2206.04371v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Encoder-Decoder Self-Supervised Pre-training for ASR", "abstract": "Self-supervised learning (SSL) has shown tremendous success in various\nspeech-related downstream tasks, including Automatic Speech Recognition (ASR).\nThe output embeddings of the SSL model are treated as powerful short-time\nrepresentations of the speech signal. However, in the ASR task, the main\nobjective is to get the correct sequence of acoustic units, characters, or\nbyte-pair encodings (BPEs). Usually, encoder-decoder architecture works\nexceptionally well for a sequence-to-sequence task like ASR. Therefore, in this\npaper, we propose a new paradigm that exploits the power of a decoder during\nself-supervised learning. We use Hidden Unit BERT (HuBERT) SSL framework to\ncompute the conventional masked prediction loss for the encoder. In addition,\nwe have introduced a decoder in the SSL framework and proposed a target\npreparation strategy for the decoder. Finally, we use a multitask SSL setup\nwherein we jointly optimize both the encoder and decoder losses. We hypothesize\nthat the presence of a decoder in the SSL model helps it learn an acoustic\nunit-based language model, which might improve the performance of an ASR\ndownstream task. We compare our proposed SSL model with HuBERT and show up to\n25% relative improvement in performance on ASR by finetuning on various\nLibriSpeech subsets.", "published": "2022-06-09 12:45:29", "link": "http://arxiv.org/abs/2206.04465v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SsciBERT: A Pre-trained Language Model for Social Science Texts", "abstract": "The academic literature of social sciences records human civilization and\nstudies human social problems. With its large-scale growth, the ways to quickly\nfind existing research on relevant issues have become an urgent demand for\nresearchers. Previous studies, such as SciBERT, have shown that pre-training\nusing domain-specific texts can improve the performance of natural language\nprocessing tasks. However, the pre-trained language model for social sciences\nis not available so far. In light of this, the present research proposes a\npre-trained model based on the abstracts published in the Social Science\nCitation Index (SSCI) journals. The models, which are available on GitHub\n(https://github.com/S-T-Full-Text-Knowledge-Mining/SSCI-BERT), show excellent\nperformance on discipline classification, abstract structure-function\nrecognition, and named entity recognition tasks with the social sciences\nliterature.", "published": "2022-06-09 13:49:04", "link": "http://arxiv.org/abs/2206.04510v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Defending Compositionality in Emergent Languages", "abstract": "Compositionality has traditionally been understood as a major factor in\nproductivity of language and, more broadly, human cognition. Yet, recently,\nsome research started to question its status, showing that artificial neural\nnetworks are good at generalization even without noticeable compositional\nbehavior. We argue that some of these conclusions are too strong and/or\nincomplete. In the context of a two-agent communication game, we show that\ncompositionality indeed seems essential for successful generalization when the\nevaluation is done on a proper dataset.", "published": "2022-06-09 20:13:46", "link": "http://arxiv.org/abs/2206.04751v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Case for a Single Model that can Both Generate Continuations and\n  Fill in the Blank", "abstract": "The task of inserting text into a specified position in a passage, known as\nfill in the blank (FitB), is useful for a variety of applications where writers\ninteract with a natural language generation (NLG) system to craft text. While\nprevious work has tackled this problem with models trained specifically to do\nthe fill-in-the-blank task, a more useful model is one that can effectively\nperform _both_ FitB and continuation. In this work, we evaluate the feasibility\nof using a single model to do both tasks. We show that models pre-trained with\na FitB-style objective are capable of both tasks, while models pre-trained for\ncontinuation are not. Finally, we show how FitB models can be easily finetuned\nto allow for fine-grained control over the length and word choice of the\ngeneration.", "published": "2022-06-09 23:39:19", "link": "http://arxiv.org/abs/2206.04812v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Folktales of Different Regions Using Topic Modeling and\n  Clustering", "abstract": "This paper employs two major natural language processing techniques, topic\nmodeling and clustering, to find patterns in folktales and reveal cultural\nrelationships between regions. In particular, we used Latent Dirichlet\nAllocation and BERTopic to extract the recurring elements as well as K-means\nclustering to group folktales. Our paper tries to answer the question what are\nthe similarities and differences between folktales, and what do they say about\nculture. Here we show that the common trends between folktales are family,\nfood, traditional gender roles, mythological figures, and animals. Also,\nfolktales topics differ based on geographical location with folktales found in\ndifferent regions having different animals and environment. We were not\nsurprised to find that religious figures and animals are some of the common\ntopics in all cultures. However, we were surprised that European and Asian\nfolktales were often paired together. Our results demonstrate the prevalence of\ncertain elements in cultures across the world. We anticipate our work to be a\nresource to future research of folktales and an example of using natural\nlanguage processing to analyze documents in specific domains. Furthermore,\nsince we only analyzed the documents based on their topics, more work could be\ndone in analyzing the structure, sentiment, and the characters of these\nfolktales.", "published": "2022-06-09 02:04:18", "link": "http://arxiv.org/abs/2206.04221v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "CLTS+: A New Chinese Long Text Summarization Dataset with Abstractive\n  Summaries", "abstract": "The abstractive methods lack of creative ability is particularly a problem in\nautomatic text summarization. The summaries generated by models are mostly\nextracted from the source articles. One of the main causes for this problem is\nthe lack of dataset with abstractiveness, especially for Chinese. In order to\nsolve this problem, we paraphrase the reference summaries in CLTS, the Chinese\nLong Text Summarization dataset, correct errors of factual inconsistencies, and\npropose the first Chinese Long Text Summarization dataset with a high level of\nabstractiveness, CLTS+, which contains more than 180K article-summary pairs and\nis available online. Additionally, we introduce an intrinsic metric based on\nco-occurrence words to evaluate the dataset we constructed. We analyze the\nextraction strategies used in CLTS+ summaries against other datasets to\nquantify the abstractiveness and difficulty of our new data and train several\nbaselines on CLTS+ to verify the utility of it for improving the creative\nability of models.", "published": "2022-06-09 03:53:52", "link": "http://arxiv.org/abs/2206.04253v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dict-NMT: Bilingual Dictionary based NMT for Extremely Low Resource\n  Languages", "abstract": "Neural Machine Translation (NMT) models have been effective on large\nbilingual datasets. However, the existing methods and techniques show that the\nmodel's performance is highly dependent on the number of examples in training\ndata. For many languages, having such an amount of corpora is a far-fetched\ndream. Taking inspiration from monolingual speakers exploring new languages\nusing bilingual dictionaries, we investigate the applicability of bilingual\ndictionaries for languages with extremely low, or no bilingual corpus. In this\npaper, we explore methods using bilingual dictionaries with an NMT model to\nimprove translations for extremely low resource languages. We extend this work\nto multilingual systems, exhibiting zero-shot properties. We present a detailed\nanalysis of the effects of the quality of dictionaries, training dataset size,\nlanguage family, etc., on the translation quality. Results on multiple\nlow-resource test languages show a clear advantage of our bilingual\ndictionary-based method over the baselines.", "published": "2022-06-09 12:03:29", "link": "http://arxiv.org/abs/2206.04439v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Extracting Zero-shot Common Sense from Large Language Models for Robot\n  3D Scene Understanding", "abstract": "Semantic 3D scene understanding is a problem of critical importance in\nrobotics. While significant advances have been made in simultaneous\nlocalization and mapping algorithms, robots are still far from having the\ncommon sense knowledge about household objects and their locations of an\naverage human. We introduce a novel method for leveraging common sense embedded\nwithin large language models for labelling rooms given the objects contained\nwithin. This algorithm has the added benefits of (i) requiring no task-specific\npre-training (operating entirely in the zero-shot regime) and (ii) generalizing\nto arbitrary room and object labels, including previously-unseen ones -- both\nof which are highly desirable traits in robotic scene understanding algorithms.\nThe proposed algorithm operates on 3D scene graphs produced by modern spatial\nperception systems, and we hope it will pave the way to more generalizable and\nscalable high-level 3D scene understanding for robotics.", "published": "2022-06-09 16:05:35", "link": "http://arxiv.org/abs/2206.04585v2", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Jewelry Shop Conversational Chatbot", "abstract": "Since the advent of chatbots in the commercial sector, they have been widely\nemployed in the customer service department. Typically, these commercial\nchatbots are retrieval-based, so they are unable to respond to queries absent\nin the provided dataset. On the contrary, generative chatbots try to create the\nmost appropriate response, but are mostly unable to create a smooth flow in the\ncustomer-bot dialog. Since the client has few options left for continuing after\nreceiving a response, the dialog becomes short. Through our work, we try to\nmaximize the intelligence of a simple conversational agent so it can answer\nunseen queries, and generate follow-up questions or remarks. We have built a\nchatbot for a jewelry shop that finds the underlying objective of the\ncustomer's query by finding similarity of the input to patterns in the corpus.\nOur system features an audio input interface for clients, so they may speak to\nit in natural language. After converting the audio to text, we trained the\nmodel to extract the intent of the query, to find an appropriate response and\nto speak to the client in a natural human voice. To gauge the system's\nperformance, we used performance metrics such as Recall, Precision and F1\nscore.", "published": "2022-06-09 17:56:51", "link": "http://arxiv.org/abs/2206.04659v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Process Knowledge-Infused AI: Towards User-level Explainability,\n  Interpretability, and Safety", "abstract": "AI systems have been widely adopted across various domains in the real world.\nHowever, in high-value, sensitive, or safety-critical applications such as\nself-management for personalized health or food recommendation with a specific\npurpose (e.g., allergy-aware recipe recommendations), their adoption is\nunlikely. Firstly, the AI system needs to follow guidelines or well-defined\nprocesses set by experts; the data alone will not be adequate. For example, to\ndiagnose the severity of depression, mental healthcare providers use Patient\nHealth Questionnaire (PHQ-9). So if an AI system were to be used for diagnosis,\nthe medical guideline implied by the PHQ-9 needs to be used. Likewise, a\nnutritionist's knowledge and steps would need to be used for an AI system that\nguides a diabetic patient in developing a food plan. Second, the BlackBox\nnature typical of many current AI systems will not work; the user of an AI\nsystem will need to be able to give user-understandable explanations,\nexplanations constructed using concepts that humans can understand and are\nfamiliar with. This is the key to eliciting confidence and trust in the AI\nsystem. For such applications, in addition to data and domain knowledge, the AI\nsystems need to have access to and use the Process Knowledge, an ordered set of\nsteps that the AI system needs to use or adhere to.", "published": "2022-06-09 14:09:37", "link": "http://arxiv.org/abs/2206.13349v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Unveiling Transformers with LEGO: a synthetic reasoning task", "abstract": "We propose a synthetic reasoning task, LEGO (Learning Equality and Group\nOperations), that encapsulates the problem of following a chain of reasoning,\nand we study how the Transformer architectures learn this task. We pay special\nattention to data effects such as pretraining (on seemingly unrelated NLP\ntasks) and dataset composition (e.g., differing chain length at training and\ntest time), as well as architectural variants such as weight-tied layers or\nadding convolutional components. We study how the trained models eventually\nsucceed at the task, and in particular, we manage to understand some of the\nattention heads as well as how the information flows in the network. In\nparticular, we have identified a novel \\emph{association} pattern that globally\nattends only to identical tokens. Based on these observations we propose a\nhypothesis that here pretraining helps for LEGO tasks due to certain structured\nattention patterns, and we experimentally verify this hypothesis. We also\nobserve that in some data regime the trained transformer finds ``shortcut\"\nsolutions to follow the chain of reasoning, which impedes the model's\nrobustness, and moreover we propose ways to prevent it. Motivated by our\nfindings on structured attention patterns, we propose the LEGO attention\nmodule, a drop-in replacement for vanilla attention heads. This architectural\nchange significantly reduces Flops and maintains or even \\emph{improves} the\nmodel's performance at large-scale pretraining.", "published": "2022-06-09 06:30:17", "link": "http://arxiv.org/abs/2206.04301v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Context-based out-of-vocabulary word recovery for ASR systems in Indian\n  languages", "abstract": "Detecting and recovering out-of-vocabulary (OOV) words is always challenging\nfor Automatic Speech Recognition (ASR) systems. Many existing methods focus on\nmodeling OOV words by modifying acoustic and language models and integrating\ncontext words cleverly into models. To train such complex models, we need a\nlarge amount of data with context words, additional training time, and\nincreased model size. However, after getting the ASR transcription to recover\ncontext-based OOV words, the post-processing method has not been explored much.\nIn this work, we propose a post-processing technique to improve the performance\nof context-based OOV recovery. We created an acoustically boosted language\nmodel with a sub-graph made at phone level with an OOV words list. We proposed\ntwo methods to determine a suitable cost function to retrieve the OOV words\nbased on the context. The cost function is defined based on phonetic and\nacoustic knowledge for matching and recovering the correct context words in the\ndecode. The effectiveness of the proposed cost function is evaluated at both\nword-level and sentence-level. The evaluation results show that this approach\ncan recover an average of 50% context-based OOV words across multiple\ncategories.", "published": "2022-06-09 06:51:31", "link": "http://arxiv.org/abs/2206.04305v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Revisiting End-to-End Speech-to-Text Translation From Scratch", "abstract": "End-to-end (E2E) speech-to-text translation (ST) often depends on pretraining\nits encoder and/or decoder using source transcripts via speech recognition or\ntext translation tasks, without which translation performance drops\nsubstantially. However, transcripts are not always available, and how\nsignificant such pretraining is for E2E ST has rarely been studied in the\nliterature. In this paper, we revisit this question and explore the extent to\nwhich the quality of E2E ST trained on speech-translation pairs alone can be\nimproved. We reexamine several techniques proven beneficial to ST previously,\nand offer a set of best practices that biases a Transformer-based E2E ST system\ntoward training from scratch. Besides, we propose parameterized distance\npenalty to facilitate the modeling of locality in the self-attention model for\nspeech. On four benchmarks covering 23 languages, our experiments show that,\nwithout using any transcripts or pretraining, the proposed system reaches and\neven outperforms previous studies adopting pretraining, although the gap\nremains in (extremely) low-resource settings. Finally, we discuss neural\nacoustic feature modeling, where a neural model is designed to extract acoustic\nfeatures from raw speech signals directly, with the goal to simplify inductive\nbiases and add freedom to the model in describing speech. For the first time,\nwe demonstrate its feasibility and show encouraging results on ST tasks.", "published": "2022-06-09 15:39:19", "link": "http://arxiv.org/abs/2206.04571v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Privacy Leakage in Text Classification: A Data Extraction Approach", "abstract": "Recent work has demonstrated the successful extraction of training data from\ngenerative language models. However, it is not evident whether such extraction\nis feasible in text classification models since the training objective is to\npredict the class label as opposed to next-word prediction. This poses an\ninteresting challenge and raises an important question regarding the privacy of\ntraining data in text classification settings. Therefore, we study the\npotential privacy leakage in the text classification domain by investigating\nthe problem of unintended memorization of training data that is not pertinent\nto the learning task. We propose an algorithm to extract missing tokens of a\npartial text by exploiting the likelihood of the class label provided by the\nmodel. We test the effectiveness of our algorithm by inserting canaries into\nthe training set and attempting to extract tokens in these canaries\npost-training. In our experiments, we demonstrate that successful extraction is\npossible to some extent. This can also be used as an auditing strategy to\nassess any potential unauthorized use of personal data without consent.", "published": "2022-06-09 16:14:26", "link": "http://arxiv.org/abs/2206.04591v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Factuality Enhanced Language Models for Open-Ended Text Generation", "abstract": "Pretrained language models (LMs) are susceptible to generate text with\nnonfactual information. In this work, we measure and improve the factual\naccuracy of large-scale LMs for open-ended text generation. We design the\nFactualityPrompts test set and metrics to measure the factuality of LM\ngenerations. Based on that, we study the factual accuracy of LMs with parameter\nsizes ranging from 126M to 530B. Interestingly, we find that larger LMs are\nmore factual than smaller ones, although a previous study suggests that larger\nLMs can be less truthful in terms of misconceptions. In addition, popular\nsampling algorithms (e.g., top-p) in open-ended text generation can harm the\nfactuality due to the ''uniform randomness'' introduced at every sampling step.\nWe propose the factual-nucleus sampling algorithm that dynamically adapts the\nrandomness to improve the factuality of generation while maintaining quality.\nFurthermore, we analyze the inefficiencies of the standard training method in\nlearning correct associations between entities from factual text corpus (e.g.,\nWikipedia). We propose a factuality-enhanced training method that uses\nTopicPrefix for better awareness of facts and sentence completion as the\ntraining objective, which can vastly reduce the factual errors. We release our\ncode and FactualityPrompts benchmark at:\nhttps://github.com/nayeon7lee/FactualityPrompt.", "published": "2022-06-09 17:16:43", "link": "http://arxiv.org/abs/2206.04624v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BigVGAN: A Universal Neural Vocoder with Large-Scale Training", "abstract": "Despite recent progress in generative adversarial network (GAN)-based\nvocoders, where the model generates raw waveform conditioned on acoustic\nfeatures, it is challenging to synthesize high-fidelity audio for numerous\nspeakers across various recording environments. In this work, we present\nBigVGAN, a universal vocoder that generalizes well for various\nout-of-distribution scenarios without fine-tuning. We introduce periodic\nactivation function and anti-aliased representation into the GAN generator,\nwhich brings the desired inductive bias for audio synthesis and significantly\nimproves audio quality. In addition, we train our GAN vocoder at the largest\nscale up to 112M parameters, which is unprecedented in the literature. We\nidentify and address the failure modes in large-scale GAN training for audio,\nwhile maintaining high-fidelity output without over-regularization. Our\nBigVGAN, trained only on clean speech (LibriTTS), achieves the state-of-the-art\nperformance for various zero-shot (out-of-distribution) conditions, including\nunseen speakers, languages, recording environments, singing voices, music, and\ninstrumental audio. We release our code and model at:\nhttps://github.com/NVIDIA/BigVGAN", "published": "2022-06-09 17:56:10", "link": "http://arxiv.org/abs/2206.04658v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Face-Dubbing++: Lip-Synchronous, Voice Preserving Translation of Videos", "abstract": "In this paper, we propose a neural end-to-end system for voice preserving,\nlip-synchronous translation of videos. The system is designed to combine\nmultiple component models and produces a video of the original speaker speaking\nin the target language that is lip-synchronous with the target speech, yet\nmaintains emphases in speech, voice characteristics, face video of the original\nspeaker. The pipeline starts with automatic speech recognition including\nemphasis detection, followed by a translation model. The translated text is\nthen synthesized by a Text-to-Speech model that recreates the original emphases\nmapped from the original sentence. The resulting synthetic voice is then mapped\nback to the original speakers' voice using a voice conversion model. Finally,\nto synchronize the lips of the speaker with the translated audio, a conditional\ngenerative adversarial network-based model generates frames of adapted lip\nmovements with respect to the input face image as well as the output of the\nvoice conversion model. In the end, the system combines the generated video\nwith the converted audio to produce the final output. The result is a video of\na speaker speaking in another language without actually knowing it. To evaluate\nour design, we present a user study of the complete system as well as separate\nevaluations of the single components. Since there is no available dataset to\nevaluate our whole system, we collect a test set and evaluate our system on\nthis test set. The results indicate that our system is able to generate\nconvincing videos of the original speaker speaking the target language while\npreserving the original speaker's characteristics. The collected dataset will\nbe shared.", "published": "2022-06-09 14:15:37", "link": "http://arxiv.org/abs/2206.04523v1", "categories": ["cs.CL", "cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CL"}
{"title": "Beyond the Imitation Game: Quantifying and extrapolating the\n  capabilities of language models", "abstract": "Language models demonstrate both quantitative improvement and new qualitative\ncapabilities with increasing scale. Despite their potentially transformative\nimpact, these new capabilities are as yet poorly characterized. In order to\ninform future research, prepare for disruptive new model capabilities, and\nameliorate socially harmful effects, it is vital that we understand the present\nand near-future capabilities and limitations of language models. To address\nthis challenge, we introduce the Beyond the Imitation Game benchmark\n(BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450\nauthors across 132 institutions. Task topics are diverse, drawing problems from\nlinguistics, childhood development, math, common-sense reasoning, biology,\nphysics, social bias, software development, and beyond. BIG-bench focuses on\ntasks that are believed to be beyond the capabilities of current language\nmodels. We evaluate the behavior of OpenAI's GPT models, Google-internal dense\ntransformer architectures, and Switch-style sparse transformers on BIG-bench,\nacross model sizes spanning millions to hundreds of billions of parameters. In\naddition, a team of human expert raters performed all tasks in order to provide\na strong baseline. Findings include: model performance and calibration both\nimprove with scale, but are poor in absolute terms (and when compared with\nrater performance); performance is remarkably similar across model classes,\nthough with benefits from sparsity; tasks that improve gradually and\npredictably commonly involve a large knowledge or memorization component,\nwhereas tasks that exhibit \"breakthrough\" behavior at a critical scale often\ninvolve multiple steps or components, or brittle metrics; social bias typically\nincreases with scale in settings with ambiguous context, but this can be\nimproved with prompting.", "published": "2022-06-09 17:05:34", "link": "http://arxiv.org/abs/2206.04615v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "CLAP: Learning Audio Concepts From Natural Language Supervision", "abstract": "Mainstream Audio Analytics models are trained to learn under the paradigm of\none class label to many recordings focusing on one task. Learning under such\nrestricted supervision limits the flexibility of models because they require\nlabeled audio for training and can only predict the predefined categories.\nInstead, we propose to learn audio concepts from natural language supervision.\nWe call our approach Contrastive Language-Audio Pretraining (CLAP), which\nlearns to connect language and audio by using two encoders and a contrastive\nlearning to bring audio and text descriptions into a joint multimodal space. We\ntrained CLAP with 128k audio and text pairs and evaluated it on 16 downstream\ntasks across 8 domains, such as Sound Event Classification, Music tasks, and\nSpeech-related tasks. Although CLAP was trained with significantly less pairs\nthan similar computer vision models, it establishes SoTA for Zero-Shot\nperformance. Additionally, we evaluated CLAP in a supervised learning setup and\nachieve SoTA in 5 tasks. Hence, CLAP's Zero-Shot capability removes the need of\ntraining with class labels, enables flexible class prediction at inference\ntime, and generalizes to multiple downstream tasks.", "published": "2022-06-09 21:16:10", "link": "http://arxiv.org/abs/2206.04769v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speak Like a Dog: Human to Non-human creature Voice Conversion", "abstract": "This paper proposes a new voice conversion (VC) task from human speech to\ndog-like speech while preserving linguistic information as an example of human\nto non-human creature voice conversion (H2NH-VC) tasks. Although most VC\nstudies deal with human to human VC, H2NH-VC aims to convert human speech into\nnon-human creature-like speech. Non-parallel VC allows us to develop H2NH-VC,\nbecause we cannot collect a parallel dataset that non-human creatures speak\nhuman language. In this study, we propose to use dogs as an example of a\nnon-human creature target domain and define the \"speak like a dog\" task. To\nclarify the possibilities and characteristics of the \"speak like a dog\" task,\nwe conducted a comparative experiment using existing representative\nnon-parallel VC methods in acoustic features (Mel-cepstral coefficients and\nMel-spectrograms), network architectures (five different kernel-size settings),\nand training criteria (variational autoencoder (VAE)- based and generative\nadversarial network-based). Finally, the converted voices were evaluated using\nmean opinion scores: dog-likeness, sound quality and intelligibility, and\ncharacter error rate (CER). The experiment showed that the employment of the\nMel-spectrogram improved the dog-likeness of the converted speech, while it is\nchallenging to preserve linguistic information. Challenges and limitations of\nthe current VC methods for H2NH-VC are highlighted.", "published": "2022-06-09 22:10:43", "link": "http://arxiv.org/abs/2206.04780v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-video fusion strategies for active speaker detection in meetings", "abstract": "Meetings are a common activity in professional contexts, and it remains\nchallenging to endow vocal assistants with advanced functionalities to\nfacilitate meeting management. In this context, a task like active speaker\ndetection can provide useful insights to model interaction between meeting\nparticipants. Motivated by our application context related to advanced meeting\nassistant, we want to combine audio and visual information to achieve the best\npossible performance. In this paper, we propose two different types of fusion\nfor the detection of the active speaker, combining two visual modalities and an\naudio modality through neural networks. For comparison purpose, classical\nunsupervised approaches for audio feature extraction are also used. We expect\nvisual data centered on the face of each participant to be very appropriate for\ndetecting voice activity, based on the detection of lip and facial gestures.\nThus, our baseline system uses visual data and we chose a 3D Convolutional\nNeural Network architecture, which is effective for simultaneously encoding\nappearance and movement. To improve this system, we supplemented the visual\ninformation by processing the audio stream with a CNN or an unsupervised\nspeaker diarization system. We have further improved this system by adding\nvisual modality information using motion through optical flow. We evaluated our\nproposal with a public and state-of-the-art benchmark: the AMI corpus. We\nanalysed the contribution of each system to the merger carried out in order to\ndetermine if a given participant is currently speaking. We also discussed the\nresults we obtained. Besides, we have shown that, for our application context,\nadding motion information greatly improves performance. Finally, we have shown\nthat attention-based fusion improves performance while reducing the standard\ndeviation.", "published": "2022-06-09 08:20:52", "link": "http://arxiv.org/abs/2206.10411v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Coswara: A website application enabling COVID-19 screening by analysing\n  respiratory sound samples and health symptoms", "abstract": "The COVID-19 pandemic has accelerated research on design of alternative,\nquick and effective COVID-19 diagnosis approaches. In this paper, we describe\nthe Coswara tool, a website application designed to enable COVID-19 detection\nby analysing respiratory sound samples and health symptoms. A user using this\nservice can log into a website using any device connected to the internet,\nprovide there current health symptom information and record few sound sampled\ncorresponding to breathing, cough, and speech. Within a minute of analysis of\nthis information on a cloud server the website tool will output a COVID-19\nprobability score to the user. As the COVID-19 pandemic continues to demand\nmassive and scalable population level testing, we hypothesize that the proposed\ntool provides a potential solution towards this.", "published": "2022-06-09 05:50:18", "link": "http://arxiv.org/abs/2206.05053v1", "categories": ["cs.HC", "cs.LG", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.HC"}
