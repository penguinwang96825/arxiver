{"title": "Joint Learning Templates and Slots for Event Schema Induction", "abstract": "Automatic event schema induction (AESI) means to extract meta-event from raw\ntext, in other words, to find out what types (templates) of event may exist in\nthe raw text and what roles (slots) may exist in each event type. In this\npaper, we propose a joint entity-driven model to learn templates and slots\nsimultaneously based on the constraints of templates and slots in the same\nsentence. In addition, the entities' semantic information is also considered\nfor the inner connectivity of the entities. We borrow the normalized cut\ncriteria in image segmentation to divide the entities into more accurate\ntemplate clusters and slot clusters. The experiment shows that our model gains\na relatively higher result than previous work.", "published": "2016-03-04 02:15:11", "link": "http://arxiv.org/abs/1603.01333v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Architectures for Named Entity Recognition", "abstract": "State-of-the-art named entity recognition systems rely heavily on\nhand-crafted features and domain-specific knowledge in order to learn\neffectively from the small, supervised training corpora that are available. In\nthis paper, we introduce two new neural architectures---one based on\nbidirectional LSTMs and conditional random fields, and the other that\nconstructs and labels segments using a transition-based approach inspired by\nshift-reduce parsers. Our models rely on two sources of information about\nwords: character-based word representations learned from the supervised corpus\nand unsupervised word representations learned from unannotated corpora. Our\nmodels obtain state-of-the-art performance in NER in four languages without\nresorting to any language-specific knowledge or resources such as gazetteers.", "published": "2016-03-04 06:36:29", "link": "http://arxiv.org/abs/1603.01360v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Bayesian Model of Multilingual Unsupervised Semantic Role Induction", "abstract": "We propose a Bayesian model of unsupervised semantic role induction in\nmultiple languages, and use it to explore the usefulness of parallel corpora\nfor this task. Our joint Bayesian model consists of individual models for each\nlanguage plus additional latent variables that capture alignments between roles\nacross languages. Because it is a generative Bayesian model, we can do\nevaluations in a variety of scenarios just by varying the inference procedure,\nwithout changing the model, thereby comparing the scenarios directly. We\ncompare using only monolingual data, using a parallel corpus, using a parallel\ncorpus with annotations in the other language, and using small amounts of\nannotation in the target language. We find that the biggest impact of adding a\nparallel corpus to training is actually the increase in mono-lingual data, with\nthe alignments to another language resulting in small improvements, even with\nlabeled data for the other language.", "published": "2016-03-04 16:03:53", "link": "http://arxiv.org/abs/1603.01514v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parallel Texts in the Hebrew Bible, New Methods and Visualizations", "abstract": "In this article we develop an algorithm to detect parallel texts in the\nMasoretic Text of the Hebrew Bible. The results are presented online and\nchapters in the Hebrew Bible containing parallel passages can be inspected\nsynoptically. Differences between parallel passages are highlighted. In a\nsimilar way the MT of Isaiah is presented synoptically with 1QIsaa. We also\ninvestigate how one can investigate the degree of similarity between parallel\npassages with the help of a case study of 2 Kings 19-25 and its parallels in\nIsaiah, Jeremiah and 2 Chronicles.", "published": "2016-03-04 17:11:51", "link": "http://arxiv.org/abs/1603.01541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Understanding with the Attention Sum Reader Network", "abstract": "Several large cloze-style context-question-answer datasets have been\nintroduced recently: the CNN and Daily Mail news data and the Children's Book\nTest. Thanks to the size of these datasets, the associated text comprehension\ntask is well suited for deep-learning techniques that currently seem to\noutperform all alternative approaches. We present a new, simple model that uses\nattention to directly pick the answer from the context as opposed to computing\nthe answer using a blended representation of words in the document as is usual\nin similar models. This makes the model particularly suitable for\nquestion-answering problems where the answer is a single word from the\ndocument. Ensemble of our models sets new state of the art on all evaluated\ndatasets.", "published": "2016-03-04 17:32:42", "link": "http://arxiv.org/abs/1603.01547v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Getting More Out Of Syntax with PropS", "abstract": "Semantic NLP applications often rely on dependency trees to recognize major\nelements of the proposition structure of sentences. Yet, while much semantic\nstructure is indeed expressed by syntax, many phenomena are not easily read out\nof dependency trees, often leading to further ad-hoc heuristic post-processing\nor to information loss. To directly address the needs of semantic applications,\nwe present PropS -- an output representation designed to explicitly and\nuniformly express much of the proposition structure which is implied from\nsyntax, and an associated tool for extracting it from dependency trees.", "published": "2016-03-04 22:47:46", "link": "http://arxiv.org/abs/1603.01648v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimized Polynomial Evaluation with Semantic Annotations", "abstract": "In this paper we discuss how semantic annotations can be used to introduce\nmathematical algorithmic information of the underlying imperative code to\nenable compilers to produce code transformations that will enable better\nperformance. By using this approaches not only good performance is achieved,\nbut also better programmability, maintainability and portability across\ndifferent hardware architectures. To exemplify this we will use polynomial\nequations of different degrees.", "published": "2016-03-04 16:13:24", "link": "http://arxiv.org/abs/1603.01520v3", "categories": ["cs.PL", "cs.CL", "B.1.4"], "primary_category": "cs.PL"}
{"title": "Sentiment Analysis in Scholarly Book Reviews", "abstract": "So far different studies have tackled the sentiment analysis in several\ndomains such as restaurant and movie reviews. But, this problem has not been\nstudied in scholarly book reviews which is different in terms of review style\nand size. In this paper, we propose to combine different features in order to\nbe presented to a supervised classifiers which extract the opinion target\nexpressions and detect their polarities in scholarly book reviews. We construct\na labeled corpus for training and evaluating our methods in French book\nreviews. We also evaluate them on English restaurant reviews in order to\nmeasure their robustness across the domains and languages. The evaluation shows\nthat our methods are enough robust for English restaurant reviews and French\nbook reviews.", "published": "2016-03-04 20:04:31", "link": "http://arxiv.org/abs/1603.01595v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF", "abstract": "State-of-the-art sequence labeling systems traditionally require large\namounts of task-specific knowledge in the form of hand-crafted features and\ndata pre-processing. In this paper, we introduce a novel neutral network\narchitecture that benefits from both word- and character-level representations\nautomatically, by using combination of bidirectional LSTM, CNN and CRF. Our\nsystem is truly end-to-end, requiring no feature engineering or data\npre-processing, thus making it applicable to a wide range of sequence labeling\ntasks. We evaluate our system on two data sets for two sequence labeling tasks\n--- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003\ncorpus for named entity recognition (NER). We obtain state-of-the-art\nperformance on both the two data --- 97.55\\% accuracy for POS tagging and\n91.21\\% F1 for NER.", "published": "2016-03-04 05:55:02", "link": "http://arxiv.org/abs/1603.01354v5", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Dynamic Memory Networks for Visual and Textual Question Answering", "abstract": "Neural network architectures with memory and attention mechanisms exhibit\ncertain reasoning capabilities required for question answering. One such\narchitecture, the dynamic memory network (DMN), obtained high accuracy on a\nvariety of language tasks. However, it was not shown whether the architecture\nachieves strong results for question answering when supporting facts are not\nmarked during training or whether it could be applied to other modalities such\nas images. Based on an analysis of the DMN, we propose several improvements to\nits memory and input modules. Together with these changes we introduce a novel\ninput module for images in order to be able to answer visual questions. Our new\nDMN+ model improves the state of the art on both the Visual Question Answering\ndataset and the \\babi-10k text question-answering dataset without supporting\nfact supervision.", "published": "2016-03-04 10:40:28", "link": "http://arxiv.org/abs/1603.01417v1", "categories": ["cs.NE", "cs.CL", "cs.CV"], "primary_category": "cs.NE"}
{"title": "Integrated Sequence Tagging for Medieval Latin Using Deep Representation\n  Learning", "abstract": "In this paper we consider two sequence tagging tasks for medieval Latin:\npart-of-speech tagging and lemmatization. These are both basic, yet\nfoundational preprocessing steps in applications such as text re-use detection.\nNevertheless, they are generally complicated by the considerable orthographic\nvariation which is typical of medieval Latin. In Digital Classics, these tasks\nare traditionally solved in a (i) cascaded and (ii) lexicon-dependent fashion.\nFor example, a lexicon is used to generate all the potential lemma-tag pairs\nfor a token, and next, a context-aware PoS-tagger is used to select the most\nappropriate tag-lemma pair. Apart from the problems with out-of-lexicon items,\nerror percolation is a major downside of such approaches. In this paper we\nexplore the possibility to elegantly solve these tasks using a single,\nintegrated approach. For this, we make use of a layered neural network\narchitecture from the field of deep representation learning.", "published": "2016-03-04 20:13:56", "link": "http://arxiv.org/abs/1603.01597v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
