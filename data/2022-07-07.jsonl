{"title": "Sensitivity Analysis on Transferred Neural Architectures of BERT and\n  GPT-2 for Financial Sentiment Analysis", "abstract": "The explosion in novel NLP word embedding and deep learning techniques has\ninduced significant endeavors into potential applications. One of these\ndirections is in the financial sector. Although there is a lot of work done in\nstate-of-the-art models like GPT and BERT, there are relatively few works on\nhow well these methods perform through fine-tuning after being pre-trained, as\nwell as info on how sensitive their parameters are. We investigate the\nperformance and sensitivity of transferred neural architectures from\npre-trained GPT-2 and BERT models. We test the fine-tuning performance based on\nfreezing transformer layers, batch size, and learning rate. We find the\nparameters of BERT are hypersensitive to stochasticity in fine-tuning and that\nGPT-2 is more stable in such practice. It is also clear that the earlier layers\nof GPT-2 and BERT contain essential word pattern information that should be\nmaintained.", "published": "2022-07-07 01:38:07", "link": "http://arxiv.org/abs/2207.03037v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Active Learning and Multi-label Classification for Ellipsis and\n  Coreference Detection in Conversational Question-Answering", "abstract": "In human conversations, ellipsis and coreference are commonly occurring\nlinguistic phenomena. Although these phenomena are a mean of making\nhuman-machine conversations more fluent and natural, only few dialogue corpora\ncontain explicit indications on which turns contain ellipses and/or\ncoreferences. In this paper we address the task of automatically detecting\nellipsis and coreferences in conversational question answering. We propose to\nuse a multi-label classifier based on DistilBERT. Multi-label classification\nand active learning are employed to compensate the limited amount of labeled\ndata. We show that these methods greatly enhance the performance of the\nclassifier for detecting these phenomena on a manually labeled dataset.", "published": "2022-07-07 08:14:54", "link": "http://arxiv.org/abs/2207.03145v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoQAR: Question Rewriting on CoQA", "abstract": "Questions asked by humans during a conversation often contain contextual\ndependencies, i.e., explicit or implicit references to previous dialogue turns.\nThese dependencies take the form of coreferences (e.g., via pronoun use) or\nellipses, and can make the understanding difficult for automated systems. One\nway to facilitate the understanding and subsequent treatments of a question is\nto rewrite it into an out-of-context form, i.e., a form that can be understood\nwithout the conversational context. We propose CoQAR, a corpus containing\n$4.5$K conversations from the Conversational Question-Answering dataset CoQA,\nfor a total of $53$K follow-up question-answer pairs. Each original question\nwas manually annotated with at least 2 at most 3 out-of-context rewritings.\nCoQAR can be used in the supervised learning of three tasks: question\nparaphrasing, question rewriting and conversational question answering. In\norder to assess the quality of CoQAR's rewritings, we conduct several\nexperiments consisting in training and evaluating models for these three tasks.\nOur results support the idea that question rewriting can be used as a\npreprocessing step for question answering models, thereby increasing their\nperformances.", "published": "2022-07-07 11:47:22", "link": "http://arxiv.org/abs/2207.03240v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Part-of-Speech Tagging of Odia Language Using statistical and Deep\n  Learning-Based Approaches", "abstract": "Automatic Part-of-speech (POS) tagging is a preprocessing step of many\nnatural language processing (NLP) tasks such as name entity recognition (NER),\nspeech processing, information extraction, word sense disambiguation, and\nmachine translation. It has already gained a promising result in English and\nEuropean languages, but in Indian languages, particularly in Odia language, it\nis not yet well explored because of the lack of supporting tools, resources,\nand morphological richness of language. Unfortunately, we were unable to locate\nan open source POS tagger for Odia, and only a handful of attempts have been\nmade to develop POS taggers for Odia language. The main contribution of this\nresearch work is to present a conditional random field (CRF) and deep\nlearning-based approaches (CNN and Bidirectional Long Short-Term Memory) to\ndevelop Odia part-of-speech tagger. We used a publicly accessible corpus and\nthe dataset is annotated with the Bureau of Indian Standards (BIS) tagset.\nHowever, most of the languages around the globe have used the dataset annotated\nwith Universal Dependencies (UD) tagset. Hence, to maintain uniformity Odia\ndataset should use the same tagset. So we have constructed a simple mapping\nfrom BIS tagset to UD tagset. We experimented with various feature set inputs\nto the CRF model, observed the impact of constructed feature set. The deep\nlearning-based model includes Bi-LSTM network, CNN network, CRF layer,\ncharacter sequence information, and pre-trained word vector. Character sequence\ninformation was extracted by using convolutional neural network (CNN) and\nBi-LSTM network. Six different combinations of neural sequence labelling models\nare implemented, and their performance measures are investigated. It has been\nobserved that Bi-LSTM model with character sequence feature and pre-trained\nword vector achieved a significant state-of-the-art result.", "published": "2022-07-07 12:15:23", "link": "http://arxiv.org/abs/2207.03256v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Win-Win Cooperation: Bundling Sequence and Span Models for Named Entity\n  Recognition", "abstract": "For Named Entity Recognition (NER), sequence labeling-based and span-based\nparadigms are quite different. Previous research has demonstrated that the two\nparadigms have clear complementary advantages, but few models have attempted to\nleverage these advantages in a single NER model as far as we know. In our\nprevious work, we proposed a paradigm known as Bundling Learning (BL) to\naddress the above problem. The BL paradigm bundles the two NER paradigms,\nenabling NER models to jointly tune their parameters by weighted summing each\nparadigm's training loss. However, three critical issues remain unresolved:\nWhen does BL work? Why does BL work? Can BL enhance the existing\nstate-of-the-art (SOTA) NER models? To address the first two issues, we\nimplement three NER models, involving a sequence labeling-based model--SeqNER,\na span-based NER model--SpanNER, and BL-NER that bundles SeqNER and SpanNER\ntogether. We draw two conclusions regarding the two issues based on the\nexperimental results on eleven NER datasets from five domains. We then apply BL\nto five existing SOTA NER models to investigate the third issue, consisting of\nthree sequence labeling-based models and two span-based models. Experimental\nresults indicate that BL consistently enhances their performance, suggesting\nthat it is possible to construct a new SOTA NER system by incorporating BL into\nthe current SOTA system. Moreover, we find that BL reduces both entity boundary\nand type prediction errors. In addition, we compare two commonly used labeling\ntagging methods as well as three types of span semantic representations.", "published": "2022-07-07 13:52:06", "link": "http://arxiv.org/abs/2207.03300v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VeriDark: A Large-Scale Benchmark for Authorship Verification on the\n  Dark Web", "abstract": "The DarkWeb represents a hotbed for illicit activity, where users communicate\non different market forums in order to exchange goods and services. Law\nenforcement agencies benefit from forensic tools that perform authorship\nanalysis, in order to identify and profile users based on their textual\ncontent. However, authorship analysis has been traditionally studied using\ncorpora featuring literary texts such as fragments from novels or fan fiction,\nwhich may not be suitable in a cybercrime context. Moreover, the few works that\nemploy authorship analysis tools for cybercrime prevention usually employ\nad-hoc experimental setups and datasets. To address these issues, we release\nVeriDark: a benchmark comprised of three large scale authorship verification\ndatasets and one authorship identification dataset obtained from user activity\nfrom either Dark Web related Reddit communities or popular illicit Dark Web\nmarket forums. We evaluate competitive NLP baselines on the three datasets and\nperform an analysis of the predictions to better understand the limitations of\nsuch approaches. We make the datasets and baselines publicly available at\nhttps://github.com/bit-ml/VeriDark", "published": "2022-07-07 17:57:11", "link": "http://arxiv.org/abs/2207.03477v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta-Learning the Difference: Preparing Large Language Models for\n  Efficient Adaptation", "abstract": "Large pretrained language models (PLMs) are often domain- or task-adapted via\nfine-tuning or prompting. Finetuning requires modifying all of the parameters\nand having enough data to avoid overfitting while prompting requires no\ntraining and few examples but limits performance. Instead, we prepare PLMs for\ndata- and parameter-efficient adaptation by learning to learn the difference\nbetween general and adapted PLMs. This difference is expressed in terms of\nmodel weights and sublayer structure through our proposed dynamic low-rank\nreparameterization and learned architecture controller. Experiments on few-shot\ndialogue completion, low-resource abstractive summarization, and multi-domain\nlanguage modeling show improvements in adaptation time and performance over\ndirect finetuning or preparation via domain-adaptive pretraining. Ablations\nshow our task-adaptive reparameterization (TARP) and model search (TAMS)\ncomponents individually improve on other parameter-efficient transfer like\nadapters and structure-learning methods like learned sparsification.", "published": "2022-07-07 18:00:22", "link": "http://arxiv.org/abs/2207.03509v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling", "abstract": "This paper studies multi-task training of retrieval-augmented generation\nmodels for knowledge-intensive tasks. We propose to clean the training set by\nutilizing a distinct property of knowledge-intensive generation: The connection\nof query-answer pairs to items in the knowledge base. We filter training\nexamples via a threshold of confidence on the relevance labels, whether a pair\nis answerable by the knowledge base or not. We train a single Fusion-in-Decoder\n(FiD) generator on seven combined tasks of the KILT benchmark. The experimental\nresults suggest that our simple yet effective approach substantially improves\ncompetitive baselines on two strongly imbalanced tasks; and shows either\nsmaller improvements or no significant regression on the remaining tasks.\nFurthermore, we demonstrate our multi-task training with relevance label\nsampling scales well with increased model capacity and achieves\nstate-of-the-art results in five out of seven KILT tasks.", "published": "2022-07-07 00:57:02", "link": "http://arxiv.org/abs/2207.03030v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Dual-Stream Transformer for Generic Event Boundary Captioning", "abstract": "This paper describes our champion solution for the CVPR2022 Generic Event\nBoundary Captioning (GEBC) competition. GEBC requires the captioning model to\nhave a comprehension of instantaneous status changes around the given video\nboundary, which makes it much more challenging than conventional video\ncaptioning task. In this paper, a Dual-Stream Transformer with improvements on\nboth video content encoding and captions generation is proposed: (1) We utilize\nthree pre-trained models to extract the video features from different\ngranularities. Moreover, we exploit the types of boundary as hints to help the\nmodel generate captions. (2) We particularly design an model, termed as\nDual-Stream Transformer, to learn discriminative representations for boundary\ncaptioning. (3) Towards generating content-relevant and human-like captions, we\nimprove the description quality by designing a word-level ensemble strategy.\nThe promising results on the GEBC test split demonstrate the efficacy of our\nproposed model.", "published": "2022-07-07 01:47:19", "link": "http://arxiv.org/abs/2207.03038v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving Few-Shot Image Classification Using Machine- and\n  User-Generated Natural Language Descriptions", "abstract": "Humans can obtain the knowledge of novel visual concepts from language\ndescriptions, and we thus use the few-shot image classification task to\ninvestigate whether a machine learning model can have this capability. Our\nproposed model, LIDE (Learning from Image and DEscription), has a text decoder\nto generate the descriptions and a text encoder to obtain the text\nrepresentations of machine- or user-generated descriptions. We confirmed that\nLIDE with machine-generated descriptions outperformed baseline models.\nMoreover, the performance was improved further with high-quality user-generated\ndescriptions. The generated descriptions can be viewed as the explanations of\nthe model's predictions, and we observed that such explanations were consistent\nwith prediction results. We also investigated why the language description\nimproved the few-shot image classification performance by comparing the image\nrepresentations and the text representations in the feature spaces.", "published": "2022-07-07 07:48:06", "link": "http://arxiv.org/abs/2207.03133v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Neural Language Models are not Born Equal to Fit Brain Data, but\n  Training Helps", "abstract": "Neural Language Models (NLMs) have made tremendous advances during the last\nyears, achieving impressive performance on various linguistic tasks.\nCapitalizing on this, studies in neuroscience have started to use NLMs to study\nneural activity in the human brain during language processing. However, many\nquestions remain unanswered regarding which factors determine the ability of a\nneural language model to capture brain activity (aka its 'brain score'). Here,\nwe make first steps in this direction and examine the impact of test loss,\ntraining corpus and model architecture (comparing GloVe, LSTM, GPT-2 and BERT),\non the prediction of functional Magnetic Resonance Imaging timecourses of\nparticipants listening to an audiobook. We find that (1) untrained versions of\neach model already explain significant amount of signal in the brain by\ncapturing similarity in brain responses across identical words, with the\nuntrained LSTM outperforming the transformerbased models, being less impacted\nby the effect of context; (2) that training NLP models improves brain scores in\nthe same brain regions irrespective of the model's architecture; (3) that\nPerplexity (test loss) is not a good predictor of brain score; (4) that\ntraining data have a strong influence on the outcome and, notably, that\noff-the-shelf models may lack statistical power to detect brain activations.\nOverall, we outline the impact of modeltraining choices, and suggest good\npractices for future studies aiming at explaining the human language system\nusing neural language models.", "published": "2022-07-07 15:37:17", "link": "http://arxiv.org/abs/2207.03380v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Investigating the Impact of Cross-lingual Acoustic-Phonetic Similarities\n  on Multilingual Speech Recognition", "abstract": "Multilingual automatic speech recognition (ASR) systems mostly benefit low\nresource languages but suffer degradation in performance across several\nlanguages relative to their monolingual counterparts. Limited studies have\nfocused on understanding the languages behaviour in the multilingual speech\nrecognition setups. In this paper, a novel data-driven approach is proposed to\ninvestigate the cross-lingual acoustic-phonetic similarities. This technique\nmeasures the similarities between posterior distributions from various\nmonolingual acoustic models against a target speech signal. Deep neural\nnetworks are trained as mapping networks to transform the distributions from\ndifferent acoustic models into a directly comparable form. The analysis\nobserves that the languages closeness can not be truly estimated by the volume\nof overlapping phonemes set. Entropy analysis of the proposed mapping networks\nexhibits that a language with lesser overlap can be more amenable to\ncross-lingual transfer, and hence more beneficial in the multilingual setup.\nFinally, the proposed posterior transformation approach is leveraged to fuse\nmonolingual models for a target language. A relative improvement of ~8% over\nmonolingual counterpart is achieved.", "published": "2022-07-07 15:55:41", "link": "http://arxiv.org/abs/2207.03390v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Non-Linear Pairwise Language Mappings for Low-Resource Multilingual\n  Acoustic Model Fusion", "abstract": "Multilingual speech recognition has drawn significant attention as an\neffective way to compensate data scarcity for low-resource languages.\nEnd-to-end (e2e) modelling is preferred over conventional hybrid systems,\nmainly because of no lexicon requirement. However, hybrid DNN-HMMs still\noutperform e2e models in limited data scenarios. Furthermore, the problem of\nmanual lexicon creation has been alleviated by publicly available trained\nmodels of grapheme-to-phoneme (G2P) and text to IPA transliteration for a lot\nof languages. In this paper, a novel approach of hybrid DNN-HMM acoustic models\nfusion is proposed in a multilingual setup for the low-resource languages.\nPosterior distributions from different monolingual acoustic models, against a\ntarget language speech signal, are fused together. A separate regression neural\nnetwork is trained for each source-target language pair to transform posteriors\nfrom source acoustic model to the target language. These networks require very\nlimited data as compared to the ASR training. Posterior fusion yields a\nrelative gain of 14.65% and 6.5% when compared with multilingual and\nmonolingual baselines respectively. Cross-lingual model fusion shows that the\ncomparable results can be achieved without using posteriors from the language\ndependent ASR.", "published": "2022-07-07 15:56:50", "link": "http://arxiv.org/abs/2207.03391v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "AsNER -- Annotated Dataset and Baseline for Assamese Named Entity\n  recognition", "abstract": "We present the AsNER, a named entity annotation dataset for low resource\nAssamese language with a baseline Assamese NER model. The dataset contains\nabout 99k tokens comprised of text from the speech of the Prime Minister of\nIndia and Assamese play. It also contains person names, location names and\naddresses. The proposed NER dataset is likely to be a significant resource for\ndeep neural based Assamese language processing. We benchmark the dataset by\ntraining NER models and evaluating using state-of-the-art architectures for\nsupervised named entity recognition (NER) such as Fasttext, BERT, XLM-R, FLAIR,\nMuRIL etc. We implement several baseline approaches with state-of-the-art\nsequence tagging Bi-LSTM-CRF architecture. The highest F1-score among all\nbaselines achieves an accuracy of 80.69% when using MuRIL as a word embedding\nmethod. The annotated dataset and the top performing model are made publicly\navailable.", "published": "2022-07-07 16:45:55", "link": "http://arxiv.org/abs/2207.03422v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "End-to-end Speech-to-Punctuated-Text Recognition", "abstract": "Conventional automatic speech recognition systems do not produce punctuation\nmarks which are important for the readability of the speech recognition\nresults. They are also needed for subsequent natural language processing tasks\nsuch as machine translation. There have been a lot of works on punctuation\nprediction models that insert punctuation marks into speech recognition results\nas post-processing. However, these studies do not utilize acoustic information\nfor punctuation prediction and are directly affected by speech recognition\nerrors. In this study, we propose an end-to-end model that takes speech as\ninput and outputs punctuated texts. This model is expected to predict\npunctuation robustly against speech recognition errors while using acoustic\ninformation. We also propose to incorporate an auxiliary loss to train the\nmodel using the output of the intermediate layer and unpunctuated texts.\nThrough experiments, we compare the performance of the proposed model to that\nof a cascaded system. The proposed model achieves higher punctuation prediction\naccuracy than the cascaded system without sacrificing the speech recognition\nerror rate. It is also demonstrated that the multi-task learning using the\nintermediate output against the unpunctuated text is effective. Moreover, the\nproposed model has only about 1/7th of the parameters compared to the cascaded\nsystem.", "published": "2022-07-07 08:58:01", "link": "http://arxiv.org/abs/2207.03169v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Bayesian Modeling of Language-Evoked Event-Related Potentials", "abstract": "Bayesian hierarchical models are well-suited to analyzing the often noisy\ndata from electroencephalography experiments in cognitive neuroscience: these\nmodels provide an intuitive framework to account for structures and\ncorrelations in the data, and they allow a straightforward handling of\nuncertainty. In a typical neurolinguistic experiment, event-related potentials\nshow only very small effect sizes and frequentist approaches to data analysis\nfail to establish the significance of some of these effects. Here, we present a\nBayesian approach to analyzing event-related potentials using as an example\ndata from an experiment which relates word surprisal and neural response. Our\nmodel is able to estimate the effect of word surprisal on most components of\nthe event-related potential and provides a richer description of the data. The\nBayesian framework also allows easier comparison between estimates based on\nsurprisal values calculated using different language models.", "published": "2022-07-07 15:58:17", "link": "http://arxiv.org/abs/2207.03392v1", "categories": ["q-bio.QM", "cs.CL", "q-bio.NC"], "primary_category": "q-bio.QM"}
{"title": "BibleTTS: a large, high-fidelity, multilingual, and uniquely African\n  speech corpus", "abstract": "BibleTTS is a large, high-quality, open speech dataset for ten languages\nspoken in Sub-Saharan Africa. The corpus contains up to 86 hours of aligned,\nstudio quality 48kHz single speaker recordings per language, enabling the\ndevelopment of high-quality text-to-speech models. The ten languages\nrepresented are: Akuapem Twi, Asante Twi, Chichewa, Ewe, Hausa, Kikuyu,\nLingala, Luganda, Luo, and Yoruba. This corpus is a derivative work of Bible\nrecordings made and released by the Open.Bible project from Biblica. We have\naligned, cleaned, and filtered the original recordings, and additionally\nhand-checked a subset of the alignments for each language. We present results\nfor text-to-speech models with Coqui TTS. The data is released under a\ncommercial-friendly CC-BY-SA license.", "published": "2022-07-07 19:35:43", "link": "http://arxiv.org/abs/2207.03546v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Quote Erat Demonstrandum: A Web Interface for Exploring the Quotebank\n  Corpus", "abstract": "The use of attributed quotes is the most direct and least filtered pathway of\ninformation propagation in news. Consequently, quotes play a central role in\nthe conception, reception, and analysis of news stories. Since quotes provide a\nmore direct window into a speaker's mind than regular reporting, they are a\nvaluable resource for journalists and researchers alike. While substantial\nresearch efforts have been devoted to methods for the automated extraction of\nquotes from news and their attribution to speakers, few comprehensive corpora\nof attributed quotes from contemporary sources are available to the public.\nHere, we present an adaptive web interface for searching Quotebank, a massive\ncollection of quotes from the news, which we make available at\nhttps://quotebank.dlab.tools.", "published": "2022-07-07 21:41:03", "link": "http://arxiv.org/abs/2207.03592v1", "categories": ["cs.IR", "cs.CL", "cs.DB"], "primary_category": "cs.IR"}
{"title": "Can Language Models perform Abductive Commonsense Reasoning?", "abstract": "Abductive Reasoning is a task of inferring the most plausible hypothesis\ngiven a set of observations. In literature, the community has approached to\nsolve this challenge by classifying/generating a likely hypothesis that does\nnot contradict with a past observation and future observation. Some of the most\nwell-known benchmarks that tackle this problem are aNLI and aNLG (pronounced as\nalpha-NLI and alpha-NLG). In this report, I review over some of the\nmethodologies that were attempted to solve this challenge, re-implement the\nbaseline models, and analyze some of the weaknesses that current approaches\nhave. The code and the re-implemented results are available at this link.", "published": "2022-07-07 15:52:24", "link": "http://arxiv.org/abs/2207.05155v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Predicting Word Learning in Children from the Performance of Computer\n  Vision Systems", "abstract": "For human children as well as machine learning systems, a key challenge in\nlearning a word is linking the word to the visual phenomena it describes. We\nexplore this aspect of word learning by using the performance of computer\nvision systems as a proxy for the difficulty of learning a word from visual\ncues. We show that the age at which children acquire different categories of\nwords is correlated with the performance of visual classification and\ncaptioning systems, over and above the expected effects of word frequency. The\nperformance of the computer vision systems is correlated with human judgments\nof the concreteness of words, which are in turn a predictor of children's word\nlearning, suggesting that these models are capturing the relationship between\nwords and visual phenomena.", "published": "2022-07-07 22:49:32", "link": "http://arxiv.org/abs/2207.09847v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Cross-Scale Vector Quantization for Scalable Neural Speech Coding", "abstract": "Bitrate scalability is a desirable feature for audio coding in real-time\ncommunications. Existing neural audio codecs usually enforce a specific bitrate\nduring training, so different models need to be trained for each target\nbitrate, which increases the memory footprint at the sender and the receiver\nside and transcoding is often needed to support multiple receivers. In this\npaper, we introduce a cross-scale scalable vector quantization scheme (CSVQ),\nin which multi-scale features are encoded progressively with stepwise feature\nfusion and refinement. In this way, a coarse-level signal is reconstructed if\nonly a portion of the bitstream is received, and progressively improves the\nquality as more bits are available. The proposed CSVQ scheme can be flexibly\napplied to any neural audio coding network with a mirrored auto-encoder\nstructure to achieve bitrate scalability. Subjective results show that the\nproposed scheme outperforms the classical residual VQ (RVQ) with scalability.\nMoreover, the proposed CSVQ at 3 kbps outperforms Opus at 9 kbps and Lyra at\n3kbps and it could provide a graceful quality boost with bitrate increase.", "published": "2022-07-07 03:23:25", "link": "http://arxiv.org/abs/2207.03067v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Visual-Assisted Sound Source Depth Estimation in the Wild", "abstract": "Depth estimation enables a wide variety of 3D applications, such as robotics,\nautonomous driving, and virtual reality. Despite significant work in this area,\nit remains open how to enable accurate, low-cost, high-resolution, and\nlarge-range depth estimation. Inspired by the flash-to-bang phenomenon (i.e.\nhearing the thunder after seeing the lightning), this paper develops FBDepth,\nthe first audio-visual depth estimation framework. It takes the difference\nbetween the time-of-flight (ToF) of the light and the sound to infer the sound\nsource depth. FBDepth is the first to incorporate video and audio with both\nsemantic features and spatial hints for range estimation. It first aligns\ncorrespondence between the video track and audio track to locate the target\nobject and target sound in a coarse granularity. Based on the observation of\nmoving objects' trajectories, FBDepth proposes to estimate the intersection of\noptical flow before and after the sound production to locate video events in\ntime. FBDepth feeds the estimated timestamp of the video event and the audio\nclip for the final depth estimation. We use a mobile phone to collect 3000+\nvideo clips with 20 different objects at up to $50m$. FBDepth decreases the\nAbsolute Relative error (AbsRel) by 55\\% compared to RGB-based methods.", "published": "2022-07-07 03:58:19", "link": "http://arxiv.org/abs/2207.03074v2", "categories": ["cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.SD"}
{"title": "Learning Music-Dance Representations through Explicit-Implicit Rhythm\n  Synchronization", "abstract": "Although audio-visual representation has been proved to be applicable in many\ndownstream tasks, the representation of dancing videos, which is more specific\nand always accompanied by music with complex auditory contents, remains\nchallenging and uninvestigated. Considering the intrinsic alignment between the\ncadent movement of dancer and music rhythm, we introduce MuDaR, a novel\nMusic-Dance Representation learning framework to perform the synchronization of\nmusic and dance rhythms both in explicit and implicit ways. Specifically, we\nderive the dance rhythms based on visual appearance and motion cues inspired by\nthe music rhythm analysis. Then the visual rhythms are temporally aligned with\nthe music counterparts, which are extracted by the amplitude of sound\nintensity. Meanwhile, we exploit the implicit coherence of rhythms implied in\naudio and visual streams by contrastive learning. The model learns the joint\nembedding by predicting the temporal consistency between audio-visual pairs.\nThe music-dance representation, together with the capability of detecting audio\nand visual rhythms, can further be applied to three downstream tasks: (a) dance\nclassification, (b) music-dance retrieval, and (c) music-dance retargeting.\nExtensive experiments demonstrate that our proposed framework outperforms other\nself-supervised methods by a large margin.", "published": "2022-07-07 09:44:44", "link": "http://arxiv.org/abs/2207.03190v2", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NESC: Robust Neural End-2-End Speech Coding with GANs", "abstract": "Neural networks have proven to be a formidable tool to tackle the problem of\nspeech coding at very low bit rates. However, the design of a neural coder that\ncan be operated robustly under real-world conditions remains a major challenge.\nTherefore, we present Neural End-2-End Speech Codec (NESC) a robust, scalable\nend-to-end neural speech codec for high-quality wideband speech coding at 3\nkbps. The encoder uses a new architecture configuration, which relies on our\nproposed Dual-PathConvRNN (DPCRNN) layer, while the decoder architecture is\nbased on our previous work Streamwise-StyleMelGAN. Our subjective listening\ntests on clean and noisy speech show that NESC is particularly robust to unseen\nconditions and signal perturbations.", "published": "2022-07-07 13:23:58", "link": "http://arxiv.org/abs/2207.03282v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "The ACII 2022 Affective Vocal Bursts Workshop & Competition:\n  Understanding a critically understudied modality of emotional expression", "abstract": "The ACII Affective Vocal Bursts Workshop & Competition is focused on\nunderstanding multiple affective dimensions of vocal bursts: laughs, gasps,\ncries, screams, and many other non-linguistic vocalizations central to the\nexpression of emotion and to human communication more generally. This year's\ncompetition comprises four tracks using a large-scale and in-the-wild dataset\nof 59,299 vocalizations from 1,702 speakers. The first, the A-VB-High task,\nrequires competition participants to perform a multi-label regression on a\nnovel model for emotion, utilizing ten classes of richly annotated emotional\nexpression intensities, including; Awe, Fear, and Surprise. The second, the\nA-VB-Two task, utilizes the more conventional 2-dimensional model for emotion,\narousal, and valence. The third, the A-VB-Culture task, requires participants\nto explore the cultural aspects of the dataset, training native-country\ndependent models. Finally, for the fourth task, A-VB-Type, participants should\nrecognize the type of vocal burst (e.g., laughter, cry, grunt) as an 8-class\nclassification. This paper describes the four tracks and baseline systems,\nwhich use state-of-the-art machine learning methods. The baseline performance\nfor each track is obtained by utilizing an end-to-end deep learning model and\nis as follows: for A-VB-High, a mean (over the 10-dimensions) Concordance\nCorrelation Coefficient (CCC) of 0.5687 CCC is obtained; for A-VB-Two, a mean\n(over the 2-dimensions) CCC of 0.5084 is obtained; for A-VB-Culture, a mean CCC\nfrom the four cultures of 0.4401 is obtained; and for A-VB-Type, the baseline\nUnweighted Average Recall (UAR) from the 8-classes is 0.4172 UAR.", "published": "2022-07-07 21:09:35", "link": "http://arxiv.org/abs/2207.03572v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Rhythm and form in music: a complex systems approach", "abstract": "There has been an everlasting discussion around the concept of form in music.\nThis work is motivated by such debate by using a complex systems framework in\nwhich we study the form as an emergent property of rhythm. Such a framework\ncorresponds with the traditional notion of musical form and allows us to\ngeneralize this concept to more general shapes and structures in music. We\ndevelop the three following metrics of the rhythmic complexity of a musical\npiece and its parts: 1) the rhythmic heterogeneity, based on the permutation\nentropy, where high values indicate a wide variety of rhythmic patterns; 2) the\nsyncopation, based on the distribution of on-beat onsets, where high values\nindicate a high proportion of off-the-beat notes; and 3) the component\nextractor, based on the communities of a visibility graph of the rhythmic\nfigures over time, where we identify structural components that constitute the\npiece at a (to be explained) perceptual level. With the same parameters, our\nmetrics are comparable within a piece or between pieces.", "published": "2022-07-07 22:03:56", "link": "http://arxiv.org/abs/2207.03602v1", "categories": ["eess.AS", "cs.IT", "cs.SD", "math.IT", "00A65, 94A17, 05C82", "J.5"], "primary_category": "eess.AS"}
{"title": "Domain Adapting Deep Reinforcement Learning for Real-world Speech\n  Emotion Recognition", "abstract": "Computers can understand and then engage with people in an emotionally\nintelligent way thanks to speech-emotion recognition (SER). However, the\nperformance of SER in cross-corpus and real-world live data feed scenarios can\nbe significantly improved. The inability to adapt an existing model to a new\ndomain is one of the shortcomings of SER methods. To address this challenge,\nresearchers have developed domain adaptation techniques that transfer knowledge\nlearnt by a model across the domain. Although existing domain adaptation\ntechniques have improved performances across domains, they can be improved to\nadapt to a real-world live data feed situation where a model can self-tune\nwhile deployed. In this paper, we present a deep reinforcement learning-based\nstrategy (RL-DA) for adapting a pre-trained model to a real-world live data\nfeed setting while interacting with the environment and collecting continual\nfeedback. RL-DA is evaluated on SER tasks, including cross-corpus and\ncross-language domain adaption schema. Evaluation results show that in a live\ndata feed setting, RL-DA outperforms a baseline strategy by 11% and 14% in\ncross-corpus and cross-language scenarios, respectively.", "published": "2022-07-07 02:53:39", "link": "http://arxiv.org/abs/2207.12248v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Finding Fallen Objects Via Asynchronous Audio-Visual Integration", "abstract": "The way an object looks and sounds provide complementary reflections of its\nphysical properties. In many settings cues from vision and audition arrive\nasynchronously but must be integrated, as when we hear an object dropped on the\nfloor and then must find it. In this paper, we introduce a setting in which to\nstudy multi-modal object localization in 3D virtual environments. An object is\ndropped somewhere in a room. An embodied robot agent, equipped with a camera\nand microphone, must determine what object has been dropped -- and where -- by\ncombining audio and visual signals with knowledge of the underlying physics. To\nstudy this problem, we have generated a large-scale dataset -- the Fallen\nObjects dataset -- that includes 8000 instances of 30 physical object\ncategories in 64 rooms. The dataset uses the ThreeDWorld platform which can\nsimulate physics-based impact sounds and complex physical interactions between\nobjects in a photorealistic setting. As a first step toward addressing this\nchallenge, we develop a set of embodied agent baselines, based on imitation\nlearning, reinforcement learning, and modular planning, and perform an in-depth\nanalysis of the challenge of this new task.", "published": "2022-07-07 17:59:59", "link": "http://arxiv.org/abs/2207.03483v1", "categories": ["cs.CV", "cs.LG", "cs.RO", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
