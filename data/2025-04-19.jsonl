{"title": "Density Measures for Language Generation", "abstract": "The recent successes of large language models (LLMs) have led to a surge of\ntheoretical research into language generation. A recent line of work proposes\nan abstract view, called language generation in the limit, where generation is\nseen as a game between an adversary and an algorithm: the adversary generates\nstrings from an unknown language $K$, chosen from a countable collection of\ncandidate languages, and after seeing a finite set of these strings, the\nalgorithm must generate new strings from $K$ that it has not seen before. This\nformalism highlights a key tension: the trade-off between validity (the\nalgorithm should only produce strings from the language) and breadth (it should\nbe able to produce many strings from the language). This trade-off is central\nin applied language generation as well, where it appears as a balance between\nhallucination (generating invalid utterances) and mode collapse (generating\nonly a restricted set of outputs). Despite its importance, this trade-off has\nbeen challenging to study quantitatively. We develop ways to quantify this\ntrade-off by formalizing breadth using measures of density. Existing algorithms\nfor language generation in the limit produce output sets that can have zero\ndensity in the true language, and this important failure of breadth might seem\nunavoidable. We show, however, that such a failure is not necessary: we provide\nan algorithm for language generation in the limit whose outputs have strictly\npositive density in $K$. We also study the internal representations built by\nthese algorithms, specifically the sequence of hypothesized candidate languages\nthey consider, and show that achieving the strongest form of breadth may\nrequire oscillating indefinitely between high- and low-density representations.\nOur analysis introduces a novel topology on language families, with notions of\nconvergence and limit points playing a key role.", "published": "2025-04-19 18:08:18", "link": "http://arxiv.org/abs/2504.14370v1", "categories": ["math.CO", "cs.CL", "cs.DM", "cs.LG"], "primary_category": "math.CO"}
{"title": "Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites", "abstract": "Prompt engineering is essential for optimizing large language models (LLMs),\nyet the link between prompt structures and task performance remains\nunderexplored. This work introduces an evolutionary approach that combines\ncontext-free grammar (CFG) with the MAP-Elites algorithm to systematically\nexplore the prompt space. Our method prioritizes quality and diversity,\ngenerating high-performing and structurally varied prompts while analyzing\ntheir alignment with diverse tasks by varying traits such as the number of\nexamples (shots) and reasoning depth. By systematically mapping the phenotypic\nspace, we reveal how structural variations influence LLM performance, offering\nactionable insights for task-specific and adaptable prompt design. Evaluated on\nseven BigBench Lite tasks across multiple LLMs, our results underscore the\ncritical interplay of quality and diversity, advancing the effectiveness and\nversatility of LLMs.", "published": "2025-04-19 17:50:34", "link": "http://arxiv.org/abs/2504.14367v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models", "abstract": "Knowledge distillation is a widely used technique for compressing large\nlanguage models (LLMs) by training a smaller student model to mimic a larger\nteacher model. Typically, both the teacher and student are Transformer-based\narchitectures, leveraging softmax attention for sequence modeling. However, the\nquadratic complexity of self-attention at inference time remains a significant\nbottleneck, motivating the exploration of subquadratic alternatives such as\nstructured state-space models (SSMs), linear attention, and recurrent\narchitectures. In this work, we systematically evaluate the transferability of\nknowledge distillation from a Transformer teacher to nine subquadratic student\narchitectures. Our study aims to determine which subquadratic model best aligns\nwith the teacher's learned representations and how different architectural\nconstraints influence the distillation process. We also investigate the impact\nof intelligent initialization strategies, including matrix mixing and\nquery-key-value (QKV) copying, on the adaptation process. Our empirical results\non multiple NLP benchmarks provide insights into the trade-offs between\nefficiency and performance, highlighting key factors for successful knowledge\ntransfer to subquadratic architectures.", "published": "2025-04-19 17:49:52", "link": "http://arxiv.org/abs/2504.14366v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving RL Exploration for LLM Reasoning through Retrospective Replay", "abstract": "Reinforcement learning (RL) has increasingly become a pivotal technique in\nthe post-training of large language models (LLMs). The effective exploration of\nthe output space is essential for the success of RL. We observe that for\ncomplex problems, during the early stages of training, the model exhibits\nstrong exploratory capabilities and can identify promising solution ideas.\nHowever, its limited capability at this stage prevents it from successfully\nsolving these problems. The early suppression of these potentially valuable\nsolution ideas by the policy gradient hinders the model's ability to revisit\nand re-explore these ideas later. Consequently, although the LLM's capabilities\nimprove in the later stages of training, it still struggles to effectively\naddress these complex problems. To address this exploration issue, we propose a\nnovel algorithm named Retrospective Replay-based Reinforcement Learning (RRL),\nwhich introduces a dynamic replay mechanism throughout the training process.\nRRL enables the model to revisit promising states identified in the early\nstages, thereby improving its efficiency and effectiveness in exploration. To\nevaluate the effectiveness of RRL, we conduct extensive experiments on complex\nreasoning tasks, including mathematical reasoning and code generation, and\ngeneral dialogue tasks. The results indicate that RRL maintains high\nexploration efficiency throughout the training period, significantly enhancing\nthe effectiveness of RL in optimizing LLMs for complicated reasoning tasks.\nMoreover, it also improves the performance of RLHF, making the model both safer\nand more helpful.", "published": "2025-04-19 17:40:04", "link": "http://arxiv.org/abs/2504.14363v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction", "abstract": "In this study, we propose an innovative methodology for predicting Cancer\nDrug Response (CDR) through the integration of the scGPT foundation model\nwithin the DeepCDR model. Our approach utilizes scGPT to generate embeddings\nfrom gene expression data, which are then used as gene expression input data\nfor DeepCDR. The experimental findings demonstrate the efficacy of this\nscGPT-based method in outperforming previous related works, including the\noriginal DeepCDR model and the scFoundation-based model. This study highlights\nthe potential of scGPT embeddings to enhance the accuracy of CDR predictions\nand offers a promising alternative to existing approaches.", "published": "2025-04-19 17:35:54", "link": "http://arxiv.org/abs/2504.14361v1", "categories": ["cs.LG", "cs.CL", "q-bio.QM"], "primary_category": "cs.LG"}
{"title": "A Multimodal Recaptioning Framework to Account for Perceptual Diversity in Multilingual Vision-Language Modeling", "abstract": "There are many ways to describe, name, and group objects when captioning an\nimage. Differences are evident when speakers come from diverse cultures due to\nthe unique experiences that shape perception. Machine translation of captions\nhas pushed multilingual capabilities in vision-language models (VLMs), but data\ncomes mainly from English speakers, indicating a perceptual bias and lack of\nmodel flexibility. In this work, we address this challenge and outline a\ndata-efficient framework to instill multilingual VLMs with greater\nunderstanding of perceptual diversity. We specifically propose an LLM-based,\nmultimodal recaptioning strategy that alters the object descriptions of English\ncaptions before translation. The greatest benefits are demonstrated in a\ntargeted multimodal mechanism guided by native speaker data. By adding produced\nrewrites as augmentations in training, we improve on German and Japanese\ntext-image retrieval cases studies (up to +3.5 mean recall overall, +4.7 on\nnon-native error cases). We further propose a mechanism to analyze the specific\nobject description differences across datasets, and we offer insights into\ncross-dataset and cross-language generalization.", "published": "2025-04-19 17:23:12", "link": "http://arxiv.org/abs/2504.14359v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach", "abstract": "Multimodal coreference resolution (MCR) aims to identify mentions referring\nto the same entity across different modalities, such as text and visuals, and\nis essential for understanding multimodal content. In the era of rapidly\ngrowing mutimodal content and social media, MCR is particularly crucial for\ninterpreting user interactions and bridging text-visual references to improve\ncommunication and personalization. However, MCR research for real-world\ndialogues remains unexplored due to the lack of sufficient data resources.To\naddress this gap, we introduce TikTalkCoref, the first Chinese multimodal\ncoreference dataset for social media in real-world scenarios, derived from the\npopular Douyin short-video platform. This dataset pairs short videos with\ncorresponding textual dialogues from user comments and includes manually\nannotated coreference clusters for both person mentions in the text and the\ncoreferential person head regions in the corresponding video frames. We also\npresent an effective benchmark approach for MCR, focusing on the celebrity\ndomain, and conduct extensive experiments on our dataset, providing reliable\nbenchmark results for this newly constructed dataset. We will release the\nTikTalkCoref dataset to facilitate future research on MCR for real-world social\nmedia dialogues.", "published": "2025-04-19 15:15:59", "link": "http://arxiv.org/abs/2504.14321v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing the Subtle Ideological Manipulation of Large Language Models", "abstract": "Large Language Models (LLMs) have transformed natural language processing,\nbut concerns have emerged about their susceptibility to ideological\nmanipulation, particularly in politically sensitive areas. Prior work has\nfocused on binary Left-Right LLM biases, using explicit prompts and fine-tuning\non political QA datasets. In this work, we move beyond this binary approach to\nexplore the extent to which LLMs can be influenced across a spectrum of\npolitical ideologies, from Progressive-Left to Conservative-Right. We introduce\na novel multi-task dataset designed to reflect diverse ideological positions\nthrough tasks such as ideological QA, statement ranking, manifesto cloze\ncompletion, and Congress bill comprehension. By fine-tuning three LLMs-Phi-2,\nMistral, and Llama-3-on this dataset, we evaluate their capacity to adopt and\nexpress these nuanced ideologies. Our findings indicate that fine-tuning\nsignificantly enhances nuanced ideological alignment, while explicit prompts\nprovide only minor refinements. This highlights the models' susceptibility to\nsubtle ideological manipulation, suggesting a need for more robust safeguards\nto mitigate these risks.", "published": "2025-04-19 13:11:50", "link": "http://arxiv.org/abs/2504.14287v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Cross-attention for State-based model RWKV-7", "abstract": "We introduce CrossWKV, a novel cross-attention mechanism for the state-based\nRWKV-7 model, designed to enhance the expressive power of text-to-image\ngeneration. Leveraging RWKV-7's linear-complexity Weighted Key-Value (WKV)\narchitecture, CrossWKV integrates text and image modalities in a single pass,\nutilizing a generalized delta rule with vector-valued gating and low-rank\nadaptations (LoRA) to achieve superior cross-modal alignment. Unlike\nTransformer-based models, CrossWKV's non-diagonal, input-dependent transition\nmatrix enables it to represent complex functions beyond the $\\mathrm{TC}^0$\ncomplexity class, including all regular languages, as demonstrated by its\nability to perform state-tracking tasks like $S_5$ permutation modeling.\nEvaluated within the Diffusion in RWKV-7 (DIR-7) on datasets such as LAION-5B\nand ImageNet, CrossWKV achieves a Frechet Inception Distance (FID) of 2.88 and\na CLIP score of 0.33 on ImageNet 256x256, matching state-of-the-art performance\nwhile offering robust generalization across diverse prompts. The model's\nenhanced expressivity, combined with constant memory usage and linear scaling,\npositions it as a powerful solution for advanced cross-modal tasks, with\npotential applications in high-resolution generation and dynamic state\nmanipulation.Code at https://github.com/TorchRWKV/flash-linear-attention", "published": "2025-04-19 10:47:51", "link": "http://arxiv.org/abs/2504.14260v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards Explainable Fake Image Detection with Multi-Modal Large Language Models", "abstract": "Progress in image generation raises significant public security concerns. We\nargue that fake image detection should not operate as a \"black box\". Instead,\nan ideal approach must ensure both strong generalization and transparency.\nRecent progress in Multi-modal Large Language Models (MLLMs) offers new\nopportunities for reasoning-based AI-generated image detection. In this work,\nwe evaluate the capabilities of MLLMs in comparison to traditional detection\nmethods and human evaluators, highlighting their strengths and limitations.\nFurthermore, we design six distinct prompts and propose a framework that\nintegrates these prompts to develop a more robust, explainable, and\nreasoning-driven detection system. The code is available at\nhttps://github.com/Gennadiyev/mllm-defake.", "published": "2025-04-19 09:42:25", "link": "http://arxiv.org/abs/2504.14245v1", "categories": ["cs.CV", "cs.CL", "I.2.7; I.2.10"], "primary_category": "cs.CV"}
{"title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners", "abstract": "Multimodal Large Language Models (MLLMs) have powered Graphical User\nInterface (GUI) Agents, showing promise in automating tasks on computing\ndevices. Recent works have begun exploring reasoning in GUI tasks with\nencouraging results. However, many current approaches rely on manually designed\nreasoning templates, which may result in reasoning that is not sufficiently\nrobust and adaptive for complex GUI environments. Meanwhile, some existing\nagents continue to operate as Reactive Actors, relying primarily on implicit\nreasoning that may lack sufficient depth for GUI tasks demanding planning and\nerror recovery. We argue that advancing these agents requires a shift from\nreactive acting towards acting based on deliberate reasoning. To facilitate\nthis transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed\nthrough our Actor2Reasoner framework, a reasoning-centric, two-stage training\napproach designed to progressively evolve agents from Reactive Actors to\nDeliberative Reasoners. The first stage, Reasoning Injection, focuses on\nestablishing a basic reasoner. We employ Spatial Reasoning Distillation to\ntransfer cross-modal spatial reasoning capabilities from teacher models to\nMLLMs through trajectories with explicit reasoning steps, enabling models to\nintegrate GUI visual-spatial information with logical reasoning before action\ngeneration. The second stage, Deliberation Enhancement, refines the basic\nreasoner into a deliberative one using Reinforcement Learning. This stage\nintroduces two approaches: Sub-goal Guidance, which rewards models for\ngenerating accurate intermediate sub-goals, and Error Recovery Scenario\nConstruction, which creates failure-and-recovery training scenarios from\nidentified prone-to-error steps. Experimental results show InfiGUI-R1 achieves\nstrong performance in GUI grounding and trajectory tasks. Resources at\nhttps://github.com/Reallm-Labs/InfiGUI-R1.", "published": "2025-04-19 09:25:55", "link": "http://arxiv.org/abs/2504.14239v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment", "abstract": "This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz,\nan Artificial Intelligence (AI) driven plugin for automating Multiple-Choice\nQuestion (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured\nframework for categorizing educational objectives into hierarchical cognitive\nlevels. Our research investigates whether incorporating this taxonomy can\nimprove the alignment of AI-generated questions with specific cognitive\nobjectives. We developed a dataset of 3691 questions categorized according to\nBloom's levels and employed various classification models-Multinomial Logistic\nRegression, Naive Bayes, Linear Support Vector Classification (SVC), and a\nTransformer-based model (DistilBERT)-to evaluate their effectiveness in\ncategorizing questions. Our results indicate that higher Bloom's levels\ngenerally correlate with increased question length, Flesch-Kincaid Grade Level\n(FKGL), and Lexical Density (LD), reflecting the increased complexity of higher\ncognitive demands. Multinomial Logistic Regression showed varying accuracy\nacross Bloom's levels, performing best for \"Knowledge\" and less accurately for\nhigher-order levels. Merging higher-level categories improved accuracy for\ncomplex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective\nclassification for lower levels but struggled with higher-order tasks.\nDistilBERT achieved the highest performance, significantly improving\nclassification of both lower and higher-order cognitive levels, achieving an\noverall validation accuracy of 91%. This study highlights the potential of\nintegrating Bloom's Taxonomy into AI-driven assessment tools and underscores\nthe advantages of advanced models like DistilBERT for enhancing educational\ncontent generation.", "published": "2025-04-19 09:03:39", "link": "http://arxiv.org/abs/2504.14232v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale", "abstract": "Large Language Models (LLMs) have emerged as personalized assistants for\nusers across a wide range of tasks -- from offering writing support to\ndelivering tailored recommendations or consultations. Over time, the\ninteraction history between a user and an LLM can provide extensive information\nabout an individual's traits and preferences. However, open questions remain on\nhow well LLMs today can effectively leverage such history to (1) internalize\nthe user's inherent traits and preferences, (2) track how the user profiling\nand preferences evolve over time, and (3) generate personalized responses\naccordingly in new scenarios.\n  In this work, we introduce the PERSONAMEM benchmark. PERSONAMEM features\ncurated user profiles with over 180 simulated user-LLM interaction histories,\neach containing up to 60 sessions of multi-turn conversations across 15\nreal-world tasks that require personalization. Given an in-situ user query,\ni.e. query issued by the user from the first-person perspective, we evaluate\nLLM chatbots' ability to identify the most suitable response according to the\ncurrent state of the user's profile. We observe that current LLMs still\nstruggle to recognize the dynamic evolution in users' profiles over time\nthrough direct prompting approaches. As a consequence, LLMs often fail to\ndeliver responses that align with users' current situations and preferences,\nwith frontier models such as GPT-4.1, o4-mini, GPT-4.5, o1, or Gemini-2.0\nachieving only around 50% overall accuracy, suggesting room for improvement. We\nhope that PERSONAMEM, along with the user profile and conversation simulation\npipeline, can facilitate future research in the development of truly user-aware\nchatbots. Code and data are available at github.com/bowen-upenn/PersonaMem.", "published": "2025-04-19 08:16:10", "link": "http://arxiv.org/abs/2504.14225v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification", "abstract": "Text simplification is essential for making complex content accessible to\ndiverse audiences who face comprehension challenges. Yet, the limited\navailability of simplified materials creates significant barriers to personal\nand professional growth and hinders social inclusion. Although researchers have\nexplored various methods for automatic text simplification, none fully leverage\nlarge language models (LLMs) to offer tailored customization for different\ntarget groups and varying levels of simplicity. Moreover, despite its proven\nbenefits for both consumers and organizations, the well-established practice of\nplain language remains underutilized. In this paper, we\nhttps://simplifymytext.org, the first system designed to produce plain language\ncontent from multiple input formats, including typed text and file uploads,\nwith flexible customization options for diverse audiences. We employ GPT-4 and\nLlama-3 and evaluate outputs across multiple metrics. Overall, our work\ncontributes to research on automatic text simplification and highlights the\nimportance of tailored communication in promoting inclusivity.", "published": "2025-04-19 08:07:53", "link": "http://arxiv.org/abs/2504.14223v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Understanding the Repeat Curse in Large Language Models from a Feature Perspective", "abstract": "Large language models (LLMs) have made remarkable progress in various\ndomains, yet they often suffer from repetitive text generation, a phenomenon we\nrefer to as the \"Repeat Curse\". While previous studies have proposed decoding\nstrategies to mitigate repetition, the underlying mechanism behind this issue\nremains insufficiently explored. In this work, we investigate the root causes\nof repetition in LLMs through the lens of mechanistic interpretability.\nInspired by recent advances in Sparse Autoencoders (SAEs), which enable\nmonosemantic feature extraction, we propose a novel approach, \"Duplicatus\nCharm\", to induce and analyze the Repeat Curse. Our method systematically\nidentifies \"Repetition Features\" -the key model activations responsible for\ngenerating repetitive outputs. First, we locate the layers most involved in\nrepetition through logit analysis. Next, we extract and stimulate relevant\nfeatures using SAE-based activation manipulation. To validate our approach, we\nconstruct a repetition dataset covering token and paragraph level repetitions\nand introduce an evaluation pipeline to quantify the influence of identified\nrepetition features. Furthermore, by deactivating these features, we have\neffectively mitigated the Repeat Curse.", "published": "2025-04-19 07:53:37", "link": "http://arxiv.org/abs/2504.14218v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification", "abstract": "Large language models (LLMs) acquire general linguistic knowledge from\nmassive-scale pretraining. However, pretraining data mainly comprised of\nweb-crawled texts contain undesirable social biases which can be perpetuated or\neven amplified by LLMs. In this study, we propose an efficient yet effective\nannotation pipeline to investigate social biases in the pretraining corpora.\nOur pipeline consists of protected attribute detection to identify diverse\ndemographics, followed by regard classification to analyze the language\npolarity towards each attribute. Through our experiments, we demonstrate the\neffect of our bias analysis and mitigation measures, focusing on Common Crawl\nas the most representative pretraining corpus.", "published": "2025-04-19 07:36:02", "link": "http://arxiv.org/abs/2504.14212v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EIoU-EMC: A Novel Loss for Domain-specific Nested Entity Recognition", "abstract": "In recent years, research has mainly focused on the general NER task. There\nstill have some challenges with nested NER task in the specific domains.\nSpecifically, the scenarios of low resource and class imbalance impede the wide\napplication for biomedical and industrial domains. In this study, we design a\nnovel loss EIoU-EMC, by enhancing the implement of Intersection over Union loss\nand Multiclass loss. Our proposed method specially leverages the information of\nentity boundary and entity classification, thereby enhancing the model's\ncapacity to learn from a limited number of data samples. To validate the\nperformance of this innovative method in enhancing NER task, we conducted\nexperiments on three distinct biomedical NER datasets and one dataset\nconstructed by ourselves from industrial complex equipment maintenance\ndocuments. Comparing to strong baselines, our method demonstrates the\ncompetitive performance across all datasets. During the experimental analysis,\nour proposed method exhibits significant advancements in entity boundary\nrecognition and entity classification. Our code are available here.", "published": "2025-04-19 06:31:54", "link": "http://arxiv.org/abs/2504.14203v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "abstract": "The composition of pre-training datasets for large language models (LLMs)\nremains largely undisclosed, hindering transparency and efforts to optimize\ndata quality, a critical driver of model performance. Current data selection\nmethods, such as natural language quality assessments, diversity-based filters,\nand classifier-based approaches, are limited by single-dimensional evaluation\nor redundancy-focused strategies. To address these gaps, we propose PRRC to\nevaluate data quality across Professionalism, Readability, Reasoning, and\nCleanliness. We further introduce Meta-rater, a multi-dimensional data\nselection method that integrates these dimensions with existing quality metrics\nthrough learned optimal weightings. Meta-rater employs proxy models to train a\nregression model that predicts validation loss, enabling the identification of\noptimal combinations of quality scores. Experiments demonstrate that Meta-rater\ndoubles convergence speed for 1.3B parameter models and improves downstream\ntask performance by 3.23, with scalable benefits observed in 3.3B models\ntrained on 100B tokens. Additionally, we release the annotated SlimPajama-627B\ndataset, labeled across 25 quality metrics (including PRRC), to advance\nresearch in data-centric LLM development. Our work establishes that holistic,\nmulti-dimensional quality integration significantly outperforms conventional\nsingle-dimension approaches, offering a scalable paradigm for enhancing\npre-training efficiency and model capability.", "published": "2025-04-19 06:12:33", "link": "http://arxiv.org/abs/2504.14194v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AI Idea Bench 2025: AI Research Idea Generation Benchmark", "abstract": "Large-scale Language Models (LLMs) have revolutionized human-AI interaction\nand achieved significant success in the generation of novel ideas. However,\ncurrent assessments of idea generation overlook crucial factors such as\nknowledge leakage in LLMs, the absence of open-ended benchmarks with grounded\ntruth, and the limited scope of feasibility analysis constrained by prompt\ndesign. These limitations hinder the potential of uncovering groundbreaking\nresearch ideas. In this paper, we present AI Idea Bench 2025, a framework\ndesigned to quantitatively evaluate and compare the ideas generated by LLMs\nwithin the domain of AI research from diverse perspectives. The framework\ncomprises a comprehensive dataset of 3,495 AI papers and their associated\ninspired works, along with a robust evaluation methodology. This evaluation\nsystem gauges idea quality in two dimensions: alignment with the ground-truth\ncontent of the original papers and judgment based on general reference\nmaterial. AI Idea Bench 2025's benchmarking system stands to be an invaluable\nresource for assessing and comparing idea-generation techniques, thereby\nfacilitating the automation of scientific discovery.", "published": "2025-04-19 05:35:45", "link": "http://arxiv.org/abs/2504.14191v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "The First VoicePrivacy Attacker Challenge", "abstract": "The First VoicePrivacy Attacker Challenge is an ICASSP 2025 SP Grand\nChallenge which focuses on evaluating attacker systems against a set of voice\nanonymization systems submitted to the VoicePrivacy 2024 Challenge. Training,\ndevelopment, and evaluation datasets were provided along with a baseline\nattacker. Participants developed their attacker systems in the form of\nautomatic speaker verification systems and submitted their scores on the\ndevelopment and evaluation data. The best attacker systems reduced the equal\nerror rate (EER) by 25-44% relative w.r.t. the baseline.", "published": "2025-04-19 05:02:46", "link": "http://arxiv.org/abs/2504.14183v1", "categories": ["eess.AS", "cs.CL", "cs.CR"], "primary_category": "eess.AS"}
{"title": "Direct Advantage Regression: Aligning LLMs with Online AI Reward", "abstract": "Online AI Feedback (OAIF) presents a promising alternative to Reinforcement\nLearning from Human Feedback (RLHF) by utilizing online AI preference in\naligning language models (LLMs). However, the straightforward replacement of\nhumans with AI deprives LLMs from learning more fine-grained AI supervision\nbeyond binary signals. In this paper, we propose Direct Advantage Regression\n(DAR), a simple alignment algorithm using online AI reward to optimize policy\nimprovement through weighted supervised fine-tuning. As an RL-free approach,\nDAR maintains theoretical consistency with online RLHF pipelines while\nsignificantly reducing implementation complexity and improving learning\nefficiency. Our empirical results underscore that AI reward is a better form of\nAI supervision consistently achieving higher human-AI agreement as opposed to\nAI preference. Additionally, evaluations using GPT-4-Turbo and MT-bench show\nthat DAR outperforms both OAIF and online RLHF baselines.", "published": "2025-04-19 04:44:32", "link": "http://arxiv.org/abs/2504.14177v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion", "abstract": "Query expansion methods powered by large language models (LLMs) have\ndemonstrated effectiveness in zero-shot retrieval tasks. These methods assume\nthat LLMs can generate hypothetical documents that, when incorporated into a\nquery vector, enhance the retrieval of real evidence. However, we challenge\nthis assumption by investigating whether knowledge leakage in benchmarks\ncontributes to the observed performance gains. Using fact verification as a\ntestbed, we analyzed whether the generated documents contained information\nentailed by ground truth evidence and assessed their impact on performance. Our\nfindings indicate that performance improvements occurred consistently only for\nclaims whose generated documents included sentences entailed by ground truth\nevidence. This suggests that knowledge leakage may be present in these\nbenchmarks, inflating the perceived performance of LLM-based query expansion\nmethods, particularly in real-world scenarios that require retrieving niche or\nnovel knowledge.", "published": "2025-04-19 04:32:38", "link": "http://arxiv.org/abs/2504.14175v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Self-Correction Makes LLMs Better Parsers", "abstract": "Large language models (LLMs) have achieved remarkable success across various\nnatural language processing (NLP) tasks. However, recent studies suggest that\nthey still face challenges in performing fundamental NLP tasks essential for\ndeep language understanding, particularly syntactic parsing. In this paper, we\nconduct an in-depth analysis of LLM parsing capabilities, delving into the\nspecific shortcomings of their parsing results. We find that LLMs may stem from\nlimitations to fully leverage grammar rules in existing treebanks, which\nrestricts their capability to generate valid syntactic structures. To help LLMs\nacquire knowledge without additional training, we propose a self-correction\nmethod that leverages grammar rules from existing treebanks to guide LLMs in\ncorrecting previous errors. Specifically, we automatically detect potential\nerrors and dynamically search for relevant rules, offering hints and examples\nto guide LLMs in making corrections themselves. Experimental results on three\ndatasets with various LLMs, demonstrate that our method significantly improves\nperformance in both in-domain and cross-domain settings on the English and\nChinese datasets.", "published": "2025-04-19 03:50:59", "link": "http://arxiv.org/abs/2504.14165v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SConU: Selective Conformal Uncertainty in Large Language Models", "abstract": "As large language models are increasingly utilized in real-world\napplications, guarantees of task-specific metrics are essential for their\nreliable deployment. Previous studies have introduced various criteria of\nconformal uncertainty grounded in split conformal prediction, which offer\nuser-specified correctness coverage. However, existing frameworks often fail to\nidentify uncertainty data outliers that violate the exchangeability assumption,\nleading to unbounded miscoverage rates and unactionable prediction sets. In\nthis paper, we propose a novel approach termed Selective Conformal Uncertainty\n(SConU), which, for the first time, implements significance tests, by\ndeveloping two conformal p-values that are instrumental in determining whether\na given sample deviates from the uncertainty distribution of the calibration\nset at a specific manageable risk level. Our approach not only facilitates\nrigorous management of miscoverage rates across both single-domain and\ninterdisciplinary contexts, but also enhances the efficiency of predictions.\nFurthermore, we comprehensively analyze the components of the conformal\nprocedures, aiming to approximate conditional coverage, particularly in\nhigh-stakes question-answering tasks.", "published": "2025-04-19 03:01:45", "link": "http://arxiv.org/abs/2504.14154v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations", "abstract": "Large language models (LLMs) are capable of generating plausible explanations\nof how they arrived at an answer to a question. However, these explanations can\nmisrepresent the model's \"reasoning\" process, i.e., they can be unfaithful.\nThis, in turn, can lead to over-trust and misuse. We introduce a new approach\nfor measuring the faithfulness of LLM explanations. First, we provide a\nrigorous definition of faithfulness. Since LLM explanations mimic human\nexplanations, they often reference high-level concepts in the input question\nthat purportedly influenced the model. We define faithfulness in terms of the\ndifference between the set of concepts that LLM explanations imply are\ninfluential and the set that truly are. Second, we present a novel method for\nestimating faithfulness that is based on: (1) using an auxiliary LLM to modify\nthe values of concepts within model inputs to create realistic counterfactuals,\nand (2) using a Bayesian hierarchical model to quantify the causal effects of\nconcepts at both the example- and dataset-level. Our experiments show that our\nmethod can be used to quantify and discover interpretable patterns of\nunfaithfulness. On a social bias task, we uncover cases where LLM explanations\nhide the influence of social bias. On a medical question answering task, we\nuncover cases where LLM explanations provide misleading claims about which\npieces of evidence influenced the model's decisions.", "published": "2025-04-19 02:51:20", "link": "http://arxiv.org/abs/2504.14150v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "HF4Rec: Human-Like Feedback-Driven Optimization Framework for Explainable Recommendation", "abstract": "Recent advancements in explainable recommendation have greatly bolstered user\nexperience by elucidating the decision-making rationale. However, the existing\nmethods actually fail to provide effective feedback signals for potentially\nbetter or worse generated explanations due to their reliance on traditional\nsupervised learning paradigms in sparse interaction data. To address these\nissues, we propose a novel human-like feedback-driven optimization framework.\nThis framework employs a dynamic interactive optimization mechanism for\nachieving human-centered explainable requirements without incurring high labor\ncosts. Specifically, we propose to utilize large language models (LLMs) as\nhuman simulators to predict human-like feedback for guiding the learning\nprocess. To enable the LLMs to deeply understand the task essence and meet\nuser's diverse personalized requirements, we introduce a human-induced\ncustomized reward scoring method, which helps stimulate the language\nunderstanding and logical reasoning capabilities of LLMs. Furthermore,\nconsidering the potential conflicts between different perspectives of\nexplanation quality, we introduce a principled Pareto optimization that\ntransforms the multi-perspective quality enhancement task into a\nmulti-objective optimization problem for improving explanation performance. At\nlast, to achieve efficient model training, we design an off-policy optimization\npipeline. By incorporating a replay buffer and addressing the data distribution\nbiases, we can effectively improve data utilization and enhance model\ngenerality. Extensive experiments on four datasets demonstrate the superiority\nof our approach.", "published": "2025-04-19 02:46:10", "link": "http://arxiv.org/abs/2504.14147v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "TALES: Text Adventure Learning Environment Suite", "abstract": "Reasoning is an essential skill to enable Large Language Models (LLMs) to\ninteract with the world. As tasks become more complex, they demand increasingly\nsophisticated and diverse reasoning capabilities for sequential\ndecision-making, requiring structured reasoning over the context history to\ndetermine the next best action. We introduce TALES, a diverse collection of\nsynthetic and human-written text-adventure games designed to challenge and\nevaluate diverse reasoning capabilities. We present results over a range of\nLLMs, open- and closed-weights, performing a qualitative analysis on the top\nperforming models. Despite an impressive showing on synthetic games, even the\ntop LLM-driven agents fail to achieve 15% on games designed for human\nenjoyment. Code and visualization of the experiments can be found at\nhttps://microsoft.github.io/tales.", "published": "2025-04-19 01:02:42", "link": "http://arxiv.org/abs/2504.14128v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Large Language Model Enhanced Particle Swarm Optimization for Hyperparameter Tuning for Deep Learning Models", "abstract": "Determining the ideal architecture for deep learning models, such as the\nnumber of layers and neurons, is a difficult and resource-intensive process\nthat frequently relies on human tuning or computationally costly optimization\napproaches. While Particle Swarm Optimization (PSO) and Large Language Models\n(LLMs) have been individually applied in optimization and deep learning, their\ncombined use for enhancing convergence in numerical optimization tasks remains\nunderexplored. Our work addresses this gap by integrating LLMs into PSO to\nreduce model evaluations and improve convergence for deep learning\nhyperparameter tuning. The proposed LLM-enhanced PSO method addresses the\ndifficulties of efficiency and convergence by using LLMs (particularly\nChatGPT-3.5 and Llama3) to improve PSO performance, allowing for faster\nachievement of target objectives. Our method speeds up search space exploration\nby substituting underperforming particle placements with best suggestions\noffered by LLMs. Comprehensive experiments across three scenarios -- (1)\noptimizing the Rastrigin function, (2) using Long Short-Term Memory (LSTM)\nnetworks for time series regression, and (3) using Convolutional Neural\nNetworks (CNNs) for material classification -- show that the method\nsignificantly improves convergence rates and lowers computational costs.\nDepending on the application, computational complexity is lowered by 20% to 60%\ncompared to traditional PSO methods. Llama3 achieved a 20% to 40% reduction in\nmodel calls for regression tasks, whereas ChatGPT-3.5 reduced model calls by\n60% for both regression and classification tasks, all while preserving accuracy\nand error rates. This groundbreaking methodology offers a very efficient and\neffective solution for optimizing deep learning models, leading to substantial\ncomputational performance improvements across a wide range of applications.", "published": "2025-04-19 00:54:59", "link": "http://arxiv.org/abs/2504.14126v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Bayesian Principles Improve Prompt Learning In Vision-Language Models", "abstract": "Prompt learning is a popular fine-tuning method for vision-language models\ndue to its efficiency. It requires a small number of additional learnable\nparameters while significantly enhancing performance on target tasks. However,\nmost existing methods suffer from overfitting to fine-tuning data, yielding\npoor generalizability. To address this, we propose a new training objective\nfunction based on a Bayesian learning principle to balance adaptability and\ngeneralizability. We derive a prior over the logits, where the mean function is\nparameterized by the pre-trained model, while the posterior corresponds to the\nfine-tuned model. This objective establishes a balance by allowing the\nfine-tuned model to adapt to downstream tasks while remaining close to the\npre-trained model.", "published": "2025-04-19 00:48:09", "link": "http://arxiv.org/abs/2504.14123v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision Models", "abstract": "Large models such as Large Language Models (LLMs) and Vision Language Models\n(VLMs) have transformed artificial intelligence, powering applications in\nnatural language processing, computer vision, and multimodal learning. However,\nfully fine-tuning these models remains expensive, requiring extensive\ncomputational resources, memory, and task-specific data. Parameter-Efficient\nFine-Tuning (PEFT) has emerged as a promising solution that allows adapting\nlarge models to downstream tasks by updating only a small portion of\nparameters. This survey presents a comprehensive overview of PEFT techniques,\nfocusing on their motivations, design principles, and effectiveness. We begin\nby analyzing the resource and accessibility challenges posed by traditional\nfine-tuning and highlight key issues, such as overfitting, catastrophic\nforgetting, and parameter inefficiency. We then introduce a structured taxonomy\nof PEFT methods -- grouped into additive, selective, reparameterized, hybrid,\nand unified frameworks -- and systematically compare their mechanisms and\ntrade-offs. Beyond taxonomy, we explore the impact of PEFT across diverse\ndomains, including language, vision, and generative modeling, showing how these\ntechniques offer strong performance with lower resource costs. We also discuss\nimportant open challenges in scalability, interpretability, and robustness, and\nsuggest future directions such as federated learning, domain adaptation, and\ntheoretical grounding. Our goal is to provide a unified understanding of PEFT\nand its growing role in enabling practical, efficient, and sustainable use of\nlarge models.", "published": "2025-04-19 00:33:16", "link": "http://arxiv.org/abs/2504.14117v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Algebraic Barriers to Halving Algorithmic Information Quantities in Correlated Strings", "abstract": "We study the possibility of scaling down algorithmic information quantities\nin tuples of correlated strings. In particular, we address a question raised by\nAlexander Shen: whether, for any triple of strings \\((a, b, c)\\), there exists\na string \\(z\\) such that each of the values of conditional Kolmogorov\ncomplexity \\(C(a|z), C(b|z), C(c|z)\\) is approximately half of the\ncorresponding unconditional Kolmogorov complexity. We provide a negative answer\nto this question by constructing a triple \\((a, b, c)\\) for which no such\nstring \\(z\\) exists. Our construction is based on combinatorial properties of\nincidences in finite projective planes and relies on recent bounds on\npoint-line incidences over prime fields. As an application, we show that this\nimpossibility implies lower bounds on the communication complexity of secret\nkey agreement protocols in certain settings. These results reveal algebraic\nobstructions to efficient information exchange and highlight a separation in\nthe information-theoretic behavior of projective planes over fields with and\nwithout proper subfields.", "published": "2025-04-19 21:34:16", "link": "http://arxiv.org/abs/2504.14408v1", "categories": ["cs.IT", "cs.DM", "math.IT"], "primary_category": "cs.IT"}
{"title": "Temporal Graph Realization With Bounded Stretch", "abstract": "A periodic temporal graph, in its simplest form, is a graph in which every\nedge appears exactly once in the first $\\Delta$ time steps, and then it\nreappears recurrently every $\\Delta$ time steps, where $\\Delta$ is a given\nperiod length. This model offers a natural abstraction of transportation\nnetworks where each transportation link connects two destinations periodically.\nFrom a network design perspective, a crucial task is to assign the time-labels\non the edges in a way that optimizes some criterion. In this paper we introduce\na very natural optimality criterion that captures how the temporal distances of\nall vertex pairs are `stretched', compared to their physical distances, i.e.\ntheir distances in the underlying static (non-temporal) graph. Given a static\ngraph $G$, the task is to assign to each edge one time-label between 1 and\n$\\Delta$ such that, in the resulting periodic temporal graph with\nperiod~$\\Delta$, the duration of the fastest temporal path from any vertex $u$\nto any other vertex $v$ is at most $\\alpha$ times the distance between $u$ and\n$v$ in $G$. Here, the value of $\\alpha$ measures how much the shortest paths\nare allowed to be \\emph{stretched} once we assign the periodic time-labels.\n  Our results span three different directions: First, we provide a series of\napproximation and NP-hardness results. Second, we provide approximation and\nfixed-parameter algorithms. Among them, we provide a simple polynomial-time\nalgorithm (the \\textit{radius-algorithm}) which always guarantees an\napproximation strictly smaller than $\\Delta$, and which also computes the\noptimum stretch in some cases. Third, we consider a parameterized local search\nextension of the problem where we are given the temporal labeling of the graph,\nbut we are allowed to change the time-labels of at most $k$ edges; for this\nproblem we prove that it is W[2]-hard but admits an XP algorithm with respect\nto $k$.", "published": "2025-04-19 10:35:43", "link": "http://arxiv.org/abs/2504.14258v1", "categories": ["cs.DS", "cs.DM"], "primary_category": "cs.DS"}
{"title": "Maker-Maker games of rank 4 are PSPACE-complete", "abstract": "The Maker-Maker convention of positional games is played on a hypergraph\nwhose edges are interpreted as winning sets. Two players take turns picking a\npreviously unpicked vertex, aiming at being first to pick all the vertices of\nsome edge. Optimal play can only lead to a first player win or a draw, and\ndeciding between the two is known to be PSPACE-complete even for 6-uniform\nhypergraphs. We establish PSPACE-completeness for hypergraphs of rank 4. As an\nintermediary, we use the recently introduced achievement positional games, a\nmore general convention in which each player has their own winning sets (blue\nand red). We show that deciding whether the blue player has a winning strategy\nas the first player is PSPACE-complete even with blue edges of size 2 or 3 and\npairwise disjoint red edges of size 2. The result for hypergraphs of rank 4 in\nthe Maker-Maker convention follows as a simple corollary.", "published": "2025-04-19 10:20:40", "link": "http://arxiv.org/abs/2504.14256v1", "categories": ["cs.DM", "cs.CC", "math.CO"], "primary_category": "cs.DM"}
{"title": "Progress on Self Identifying Codes", "abstract": "The concept of an identifying code for a graph was introduced by Karpovsky,\nChakrabarty, and Levitin in 1998 as the problem of covering the vertices of a\ngraph such that we can uniquely identify any vertex in the graph by examining\nthe vertices that cover it. An application of an identifying code would be to\ndetect a faulty processor in a multiprocessor system. In 2020, a variation of\nidentify code called \"self-identifying code\" was introduced by Junnila and\nLaihonen, which simplifies the task of locating the malfunctioning processor.\nIn this paper, we continue to explore self-identifying codes. In particular, we\nprove the problem of determining the minimum cardinality of a self-identifying\ncode for an arbitrary graph is NP-complete and we investigate minimum-sized\nself-identifying code in several classes of graphs, including cubic graphs and\ninfinite grids.", "published": "2025-04-19 00:49:49", "link": "http://arxiv.org/abs/2504.14124v1", "categories": ["cs.DM", "math.CO"], "primary_category": "cs.DM"}
{"title": "LLM-Driven Usefulness Judgment for Web Search Evaluation", "abstract": "Evaluation is fundamental in optimizing search experiences and supporting\ndiverse user intents in Information Retrieval (IR). Traditional search\nevaluation methods primarily rely on relevance labels, which assess how well\nretrieved documents match a user's query. However, relevance alone fails to\ncapture a search system's effectiveness in helping users achieve their search\ngoals, making usefulness a critical evaluation criterion. In this paper, we\nexplore an alternative approach: LLM-generated usefulness labels, which\nincorporate both implicit and explicit user behavior signals to evaluate\ndocument usefulness. We propose Task-aware Rubric-based Usefulness Evaluation\n(TRUE), a rubric-driven evaluation method that employs iterative sampling and\nreasoning to model complex search behavior patterns. Our findings show that (i)\nLLMs can generate moderate usefulness labels by leveraging comprehensive search\nsession history incorporating personalization and contextual understanding, and\n(ii) fine-tuned LLMs improve usefulness judgments when provided with structured\nsearch session contexts. Additionally, we examine whether LLMs can distinguish\nbetween relevance and usefulness, particularly in cases where this divergence\nimpacts search success. We also conduct an ablation study to identify key\nmetrics for accurate usefulness label generation, optimizing for token\nefficiency and cost-effectiveness in real-world applications. This study\nadvances LLM-based usefulness evaluation by refining key user metrics,\nexploring LLM-generated label reliability, and ensuring feasibility for\nlarge-scale search systems.", "published": "2025-04-19 20:38:09", "link": "http://arxiv.org/abs/2504.14401v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Unconstrained Monotonic Calibration of Predictions in Deep Ranking Systems", "abstract": "Ranking models primarily focus on modeling the relative order of predictions\nwhile often neglecting the significance of the accuracy of their absolute\nvalues. However, accurate absolute values are essential for certain downstream\ntasks, necessitating the calibration of the original predictions. To address\nthis, existing calibration approaches typically employ predefined\ntransformation functions with order-preserving properties to adjust the\noriginal predictions. Unfortunately, these functions often adhere to fixed\nforms, such as piece-wise linear functions, which exhibit limited\nexpressiveness and flexibility, thereby constraining their effectiveness in\ncomplex calibration scenarios. To mitigate this issue, we propose implementing\na calibrator using an Unconstrained Monotonic Neural Network (UMNN), which can\nlearn arbitrary monotonic functions with great modeling power. This approach\nsignificantly relaxes the constraints on the calibrator, improving its\nflexibility and expressiveness while avoiding excessively distorting the\noriginal predictions by requiring monotonicity. Furthermore, to optimize this\nhighly flexible network for calibration, we introduce a novel additional loss\nfunction termed Smooth Calibration Loss (SCLoss), which aims to fulfill a\nnecessary condition for achieving the ideal calibration state. Extensive\noffline experiments confirm the effectiveness of our method in achieving\nsuperior calibration performance. Moreover, deployment in Kuaishou's\nlarge-scale online video ranking system demonstrates that the method's\ncalibration improvements translate into enhanced business metrics. The source\ncode is available at https://github.com/baiyimeng/UMC.", "published": "2025-04-19 09:35:11", "link": "http://arxiv.org/abs/2504.14243v1", "categories": ["cs.IR", "H.3.3; H.3.5"], "primary_category": "cs.IR"}
{"title": "Template-Based Financial Report Generation in Agentic and Decomposed Information Retrieval", "abstract": "Tailoring structured financial reports from companies' earnings releases is\ncrucial for understanding financial performance and has been widely adopted in\nreal-world analytics. However, existing summarization methods often generate\nbroad, high-level summaries, which may lack the precision and detail required\nfor financial reports that typically focus on specific, structured sections.\nWhile Large Language Models (LLMs) hold promise, generating reports adhering to\npredefined multi-section templates remains challenging. This paper investigates\ntwo LLM-based approaches popular in industry for generating templated financial\nreports: an agentic information retrieval (IR) framework and a decomposed IR\napproach, namely AgenticIR and DecomposedIR. The AgenticIR utilizes\ncollaborative agents prompted with the full template. In contrast, the\nDecomposedIR approach applies a prompt chaining workflow to break down the\ntemplate and reframe each section as a query answered by the LLM using the\nearnings release. To quantitatively assess the generated reports, we evaluated\nboth methods in two scenarios: one using a financial dataset without direct\nhuman references, and another with a weather-domain dataset featuring\nexpert-written reports. Experimental results show that while AgenticIR may\nexcel in orchestrating tasks and generating concise reports through agent\ncollaboration, DecomposedIR statistically significantly outperforms AgenticIR\napproach in providing broader and more detailed coverage in both scenarios,\noffering reflection on the utilization of the agentic framework in real-world\napplications.", "published": "2025-04-19 09:06:13", "link": "http://arxiv.org/abs/2504.14233v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Teach Me How to Denoise: A Universal Framework for Denoising Multi-modal Recommender Systems via Guided Calibration", "abstract": "The surge in multimedia content has led to the development of Multi-Modal\nRecommender Systems (MMRecs), which use diverse modalities such as text,\nimages, videos, and audio for more personalized recommendations. However,\nMMRecs struggle with noisy data caused by misalignment among modal content and\nthe gap between modal semantics and recommendation semantics. Traditional\ndenoising methods are inadequate due to the complexity of multi-modal data. To\naddress this, we propose a universal guided in-sync distillation denoising\nframework for multi-modal recommendation (GUIDER), designed to improve MMRecs\nby denoising user feedback. Specifically, GUIDER uses a re-calibration strategy\nto identify clean and noisy interactions from modal content. It incorporates a\nDenoising Bayesian Personalized Ranking (DBPR) loss function to handle implicit\nuser feedback. Finally, it applies a denoising knowledge distillation objective\nbased on Optimal Transport distance to guide the alignment from modality\nrepresentations to recommendation semantics. GUIDER can be seamlessly\nintegrated into existing MMRecs methods as a plug-and-play solution.\nExperimental results on four public datasets demonstrate its effectiveness and\ngeneralizability. Our source code is available at\nhttps://github.com/Neon-Jing/Guider", "published": "2025-04-19 07:37:03", "link": "http://arxiv.org/abs/2504.14214v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "FedCIA: Federated Collaborative Information Aggregation for Privacy-Preserving Recommendation", "abstract": "Recommendation algorithms rely on user historical interactions to deliver\npersonalized suggestions, which raises significant privacy concerns. Federated\nrecommendation algorithms tackle this issue by combining local model training\nwith server-side model aggregation, where most existing algorithms use a\nuniform weighted summation to aggregate item embeddings from different client\nmodels. This approach has three major limitations: 1) information loss during\naggregation, 2) failure to retain personalized local features, and 3)\nincompatibility with parameter-free recommendation algorithms. To address these\nlimitations, we first review the development of recommendation algorithms and\nrecognize that their core function is to share collaborative information,\nspecifically the global relationship between users and items. With this\nunderstanding, we propose a novel aggregation paradigm named collaborative\ninformation aggregation, which focuses on sharing collaborative information\nrather than item parameters. Based on this new paradigm, we introduce the\nfederated collaborative information aggregation (FedCIA) method for\nprivacy-preserving recommendation. This method requires each client to upload\nitem similarity matrices for aggregation, which allows clients to align their\nlocal models without constraining embeddings to a unified vector space. As a\nresult, it mitigates information loss caused by direct summation, preserves the\npersonalized embedding distributions of individual clients, and supports the\naggregation of parameter-free models. Theoretical analysis and experimental\nresults on real-world datasets demonstrate the superior performance of FedCIA\ncompared with the state-of-the-art federated recommendation algorithms. Code is\navailable at https://github.com/Mingzhe-Han/FedCIA.", "published": "2025-04-19 06:59:34", "link": "http://arxiv.org/abs/2504.14208v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Personalized News Recommendation with Multi-granularity Candidate-aware User Modeling", "abstract": "Matching candidate news with user interests is crucial for personalized news\nrecommendations. Most existing methods can represent a user's reading interests\nthrough a single profile based on clicked news, which may not fully capture the\ndiversity of user interests. Although some approaches incorporate candidate\nnews or topic information, they remain insufficient because they neglect the\nmulti-granularity relatedness between candidate news and user interests. To\naddress this, this study proposed a multi-granularity candidate-aware user\nmodeling framework that integrated user interest features across various levels\nof granularity. It consisted of two main components: candidate news encoding\nand user modeling. A news textual information extractor and a\nknowledge-enhanced entity information extractor can capture candidate news\nfeatures, and word-level, entity-level, and news-level candidate-aware\nmechanisms can provide a comprehensive representation of user interests.\nExtensive experiments on a real-world dataset demonstrated that the proposed\nmodel could significantly outperform baseline models.", "published": "2025-04-19 01:14:55", "link": "http://arxiv.org/abs/2504.14130v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "On the Redundancy of Function-Correcting Codes over Finite Fields", "abstract": "Function-correcting codes (FCCs) are a class of codes introduced by Lenz et\nal. (2023) that protect specific function evaluations of a message against\nerrors, imposing a less stringent distance requirement than classical\nerror-correcting codes (ECCs) and thereby allowing for reduced redundancy. For\nFCCs over binary field, a lower bound on the optimal redundancy for function\ncorrection was established by Lenz et al., and we derive an upper bound that\nremains within a logarithmic factor of this lower bound. We extend this result\nby proving that the same lower bound holds for any q-ary finite field.\nFurthermore, we show that for sufficiently large fields, this bound is tight by\nproving it also serves as an upper bound. In addition, we construct an encoding\nscheme that achieves this optimal redundancy. Finally, motivated by these two\nextremal regimes, we conjecture that our bound continues to serve as a valid\nupper bound across all finite fields.", "published": "2025-04-19 21:50:21", "link": "http://arxiv.org/abs/2504.14410v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Publicly Verifiable Secret Sharing: Generic Constructions and Lattice-Based Instantiations in the Standard Model", "abstract": "Publicly verifiable secret sharing (PVSS) allows a dealer to share a secret\namong a set of shareholders so that the secret can be reconstructed later from\nany set of qualified participants. In addition, any public verifier should be\nable to check the correctness of the sharing and reconstruction process. PVSS\nhas been demonstrated to yield various applications, such as e-voting,\ndistributed key generation, decentralized random number generation protocols,\nand multi-party computation. Although many concrete PVSS protocols have been\nproposed, their security is either proven in the random oracle model or relies\non quantum-vulnerable assumptions such as factoring or discrete logarithm. In\nthis work, we put forward a generic construction for PVSS that can be\ninstantiated in the standard model under the Learning With Errors (LWE)\nassumption. Our instantiation provides the first post-quantum PVSS in the\nstandard model, with a reasonable level of asymptotic efficiency.", "published": "2025-04-19 19:05:41", "link": "http://arxiv.org/abs/2504.14381v1", "categories": ["cs.CR", "cs.IT", "math.IT", "math.NT", "94A60"], "primary_category": "cs.CR"}
{"title": "DLW-CI: A Dynamic Likelihood-Weighted Cooperative Infotaxis Approach for Multi-Source Search in Urban Environments Using Consumer Drone Networks", "abstract": "Consumer-grade drones equipped with low-cost sensors have emerged as a\ncornerstone of Autonomous Intelligent Systems (AISs) for environmental\nmonitoring and hazardous substance detection in urban environments. However,\nexisting research primarily addresses single-source search problems,\noverlooking the complexities of real-world urban scenarios where both the\nlocation and quantity of hazardous sources remain unknown. To address this\nissue, we propose the Dynamic Likelihood-Weighted Cooperative Infotaxis\n(DLW-CI) approach for consumer drone networks. Our approach enhances\nmulti-drone collaboration in AISs by combining infotaxis (a cognitive search\nstrategy) with optimized source term estimation and an innovative cooperative\nmechanism. Specifically, we introduce a novel source term estimation method\nthat utilizes multiple parallel particle filters, with each filter dedicated to\nestimating the parameters of a potentially unknown source within the search\nscene. Furthermore, we develop a cooperative mechanism based on dynamic\nlikelihood weights to prevent multiple drones from simultaneously estimating\nand searching for the same source, thus optimizing the energy efficiency and\nsearch coverage of the consumer AIS. Experimental results demonstrate that the\nDLW-CI approach significantly outperforms baseline methods regarding success\nrate, accuracy, and root mean square error, particularly in scenarios with\nrelatively few sources, regardless of the presence of obstacles. Also, the\neffectiveness of the proposed approach is verified in a diffusion scenario\ngenerated by the computational fluid dynamics (CFD) model. Research findings\nindicate that our approach could improve source estimation accuracy and search\nefficiency by consumer drone-based AISs, making a valuable contribution to\nenvironmental safety monitoring applications within smart city infrastructure.", "published": "2025-04-19 15:44:09", "link": "http://arxiv.org/abs/2504.14330v1", "categories": ["cs.IT", "cs.RO", "math.IT"], "primary_category": "cs.IT"}
{"title": "Sparse Superposition Codes with Binomial Dictionary are Capacity-Achieving with Maximum Likelihood Decoding", "abstract": "It is known that sparse superposition codes asymptotically achieve the\nchannel capacity over the additive white Gaussian noise channel with both\nmaximum likelihood decoding and efficient decoding (Joseph and Barron in 2012,\n2014). Takeishi et al. (in 2014, 2019) demonstrated that these codes can also\nasymptotically achieve the channel capacity with maximum likelihood decoding\nwhen the dictionary is drawn from a Bernoulli distribution. In this paper, we\nextend these results by showing that the dictionary distribution can be\nnaturally generalized to the binomial distribution.", "published": "2025-04-19 11:07:16", "link": "http://arxiv.org/abs/2504.14262v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models", "abstract": "To develop trustworthy Vision-Language Models (VLMs), it is essential to\naddress adversarial robustness and hallucination mitigation, both of which\nimpact factual accuracy in high-stakes applications such as defense and\nhealthcare. Existing methods primarily focus on either adversarial defense or\nhallucination post-hoc correction, leaving a gap in unified robustness\nstrategies. We introduce \\textbf{Hydra}, an adaptive agentic framework that\nenhances plug-in VLMs through iterative reasoning, structured critiques, and\ncross-model verification, improving both resilience to adversarial\nperturbations and intrinsic model errors. Hydra employs an Action-Critique\nLoop, where it retrieves and critiques visual information, leveraging\nChain-of-Thought (CoT) and In-Context Learning (ICL) techniques to refine\noutputs dynamically. Unlike static post-hoc correction methods, Hydra adapts to\nboth adversarial manipulations and intrinsic model errors, making it robust to\nmalicious perturbations and hallucination-related inaccuracies. We evaluate\nHydra on four VLMs, three hallucination benchmarks, two adversarial attack\nstrategies, and two adversarial defense methods, assessing performance on both\nclean and adversarial inputs. Results show that Hydra surpasses plug-in VLMs\nand state-of-the-art (SOTA) dehallucination methods, even without explicit\nadversarial defenses, demonstrating enhanced robustness and factual\nconsistency. By bridging adversarial resistance and hallucination mitigation,\nHydra provides a scalable, training-free solution for improving the reliability\nof VLMs in real-world applications.", "published": "2025-04-19 19:51:20", "link": "http://arxiv.org/abs/2504.14395v1", "categories": ["cs.CV", "cs.AI", "cs.MA"], "primary_category": "cs.CV"}
{"title": "Decentralized Signaling Mechanisms", "abstract": "We study a system composed of multiple distinct service locations that aims\nto convince customers to join the system by providing information to customers.\nWe cast the system's information design problem in the framework of Bayesian\npersuasion and describe centralized and decentralized signaling. We provide\nefficient methods for computing the system's optimal centralized and\ndecentralized signaling mechanisms and derive a performance guarantee for\ndecentralized signaling when the locations' states are independent. The\nguarantee states that the probability that a customer joins under optimal\ndecentralized signaling is bounded below by the product of a strictly positive\nconstant and the probability that a customer joins under optimal centralized\nsignaling. The constant depends only on the number of service locations. We\nprovide an example that shows that the constant cannot be improved. We consider\nan extension to more-general objectives for the system and establish that the\nsame guarantee continues to hold. We also extend our analysis to systems where\nthe locations' states are correlated, and again derive a performance guarantee\nfor decentralized signaling in that setting. For the correlated setting, we\nprove that the guarantee's asymptotic dependence upon the number of locations\ncannot be substantially improved. A comparison of our guarantees for\nindependent locations and for correlated locations reveals the influence of\ndependence on the performance of decentralized signaling.", "published": "2025-04-19 03:36:53", "link": "http://arxiv.org/abs/2504.14163v1", "categories": ["cs.GT", "cs.MA", "econ.TH", "91A28, 90B99, 68W25"], "primary_category": "cs.GT"}
{"title": "Numerical analysis of a particle system for the calibrated Heston-type local stochastic volatility model", "abstract": "We analyse a Monte Carlo particle method for the simulation of the calibrated\nHeston-type local stochastic volatility (H-LSV) model. The common application\nof a kernel estimator for a conditional expectation in the calibration\ncondition results in a McKean-Vlasov (MV) stochastic differential equation\n(SDE) with non-standard coefficients. The primary challenges lie in certain\nmean-field terms in the drift and diffusion coefficients and the\n$1/2$-H\\\"{o}lder regularity of the diffusion coefficient. We establish the\nwell-posedness of this equation for a fixed but arbitrarily small bandwidth of\nthe kernel estimator. Moreover, we prove a strong propagation of chaos result,\nensuring convergence of the particle system under a condition on the Feller\nratio and up to a critical time. For the numerical simulation, we employ an\nEuler-Maruyama scheme for the log-spot process and a full truncation Euler\nscheme for the CIR volatility process. Under certain conditions on the inputs\nand the Feller ratio, we prove strong convergence of the Euler-Maruyama scheme\nwith rate $1/2$ in time, up to a logarithmic factor. Numerical experiments\nillustrate the convergence of the discretisation scheme and validate the\npropagation of chaos in practice.", "published": "2025-04-19 16:21:28", "link": "http://arxiv.org/abs/2504.14343v1", "categories": ["q-fin.CP", "cs.NA", "math.NA"], "primary_category": "q-fin.CP"}
{"title": "The Schur complements for $SDD_{1}$ matrices and their application to linear complementarity problems", "abstract": "In this paper we propose a new scaling method to study the Schur complements\nof $SDD_{1}$ matrices. Its core is related to the non-negative property of the\ninverse $M$-matrix, while numerically improving the Quotient formula. Based on\nthe Schur complement and a novel norm splitting manner, we establish an upper\nbound for the infinity norm of the inverse of $SDD_{1}$ matrices, which depends\nsolely on the original matrix entries. We apply the new bound to derive an\nerror bound for linear complementarity problems of $B_{1}$-matrices.\nAdditionally, new lower and upper bounds for the determinant of $SDD_{1}$\nmatrices are presented. Numerical experiments validate the effectiveness and\nsuperiority of our results.", "published": "2025-04-19 14:13:06", "link": "http://arxiv.org/abs/2504.14308v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Time discretization in convected linearized thermo-visco-elastodynamics at large displacements", "abstract": "The fully-implicit time discretization (i.e. the backward Euler formula) is\napplied to compressible nonlinear dynamical models of thermo-viscoelastic\nsolids in the Eulerian description, i.e. in the actual deforming configuration,\nformulated in terms of rates. The Kelvin-Voigt rheology or also, in the\ndeviatoric part, the Jeffreys rheology (covering creep or plasticity) are\nconsidered, using the additive Green-Naghdi's decomposition of total strain\ninto the elastic and the inelastic strains formulated in terms of (objective)\nrates exploiting the Zaremba-Jaumann time derivative. A linearized convective\nmodel at large displacements is considered, focusing on the case where the\ninternal energy additively splits the (convex) mechanical and the thermal\nparts. The time-discrete suitably regularized scheme is devised. The numerical\nstability and, considering the multipolar 2nd-grade viscosity, also convergence\ntowards weak solutions are proved, exploiting the convexity of the kinetic\nenergy when written in terms of linear momentum instead of velocity and\nestimating the temperature gradient from the entropy-like inequality.", "published": "2025-04-19 13:48:55", "link": "http://arxiv.org/abs/2504.14297v1", "categories": ["math.NA", "cs.NA", "math.AP", "35Q74, 35Q79, 65M99, 74A15, 74A30, 74C20, 80A20"], "primary_category": "math.NA"}
{"title": "Analysis of a finite element method for PDEs in evolving domains with topological changes", "abstract": "The paper presents the first rigorous error analysis of an unfitted finite\nelement method for a linear parabolic problem posed on an evolving domain\n$\\Omega(t)$ that may undergo a topological change, such as, for example, a\ndomain splitting. The domain evolution is assumed to be $C^2$-smooth away from\na critical time $t_c$, at which the topology may change instantaneously. To\naccommodate such topological transitions in the error analysis, we introduce\nseveral structural assumptions on the evolution of $\\Omega(t)$ in the vicinity\nof the critical time. These assumptions allow a specific stability estimate\neven across singularities. Based on this stability result we derive\noptimal-order discretization error bounds, provided the continuous solution is\nsufficiently smooth. We demonstrate the applicability of our assumptions with\nexamples of level-set domains undergoing topological transitions and discuss\ncases where the analysis fails. The theoretical error estimate is confirmed by\nthe results of a numerical experiment.", "published": "2025-04-19 00:23:09", "link": "http://arxiv.org/abs/2504.14116v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Optimal Scheduling of Dynamic Transport", "abstract": "Flow-based methods for sampling and generative modeling use continuous-time\ndynamical systems to represent a {transport map} that pushes forward a source\nmeasure to a target measure. The introduction of a time axis provides\nconsiderable design freedom, and a central question is how to exploit this\nfreedom. Though many popular methods seek straight line (i.e., zero\nacceleration) trajectories, we show here that a specific class of ``curved''\ntrajectories can significantly improve approximation and learning. In\nparticular, we consider the unit-time interpolation of any given transport map\n$T$ and seek the schedule $\\tau: [0,1] \\to [0,1]$ that minimizes the spatial\nLipschitz constant of the corresponding velocity field over all times $t \\in\n[0,1]$. This quantity is crucial as it allows for control of the approximation\nerror when the velocity field is learned from data. We show that, for a broad\nclass of source/target measures and transport maps $T$, the \\emph{optimal\nschedule} can be computed in closed form, and that the resulting optimal\nLipschitz constant is \\emph{exponentially smaller} than that induced by an\nidentity schedule (corresponding to, for instance, the Wasserstein geodesic).\nOur proof technique relies on the calculus of variations and\n$\\Gamma$-convergence, allowing us to approximate the aforementioned degenerate\nobjective by a family of smooth, tractable problems.", "published": "2025-04-19 23:40:54", "link": "http://arxiv.org/abs/2504.14425v1", "categories": ["stat.ML", "cs.LG", "math.CA", "math.FA"], "primary_category": "stat.ML"}
{"title": "Optimal Lattice Boltzmann Closures through Multi-Agent Reinforcement Learning", "abstract": "The Lattice Boltzmann method (LBM) offers a powerful and versatile approach\nto simulating diverse hydrodynamic phenomena, spanning microfluidics to\naerodynamics. The vast range of spatiotemporal scales inherent in these systems\ncurrently renders full resolution impractical, necessitating the development of\neffective closure models for under-resolved simulations. Under-resolved LBMs\nare unstable, and while there is a number of important efforts to stabilize\nthem, they often face limitations in generalizing across scales and physical\nsystems. We present a novel, data-driven, multiagent reinforcement learning\n(MARL) approach that drastically improves stability and accuracy of\ncoarse-grained LBM simulations. The proposed method uses a convolutional neural\nnetwork to dynamically control the local relaxation parameter for the LB across\nthe simulation grid. The LB-MARL framework is showcased in turbulent Kolmogorov\nflows. We find that the MARL closures stabilize the simulations and recover the\nenergy spectra of significantly more expensive fully resolved simulations while\nmaintaining computational efficiency. The learned closure model can be\ntransferred to flow scenarios unseen during training and has improved\nrobustness and spectral accuracy compared to traditional LBM models. We believe\nthat MARL closures open new frontiers for efficient and accurate simulations of\na multitude of complex problems not accessible to present-day LB methods alone.", "published": "2025-04-19 23:31:29", "link": "http://arxiv.org/abs/2504.14422v1", "categories": ["physics.flu-dyn", "cs.LG", "physics.comp-ph", "stat.ML"], "primary_category": "physics.flu-dyn"}
{"title": "Unlearning Works Better Than You Think: Local Reinforcement-Based Selection of Auxiliary Objectives", "abstract": "We introduce Local Reinforcement-Based Selection of Auxiliary Objectives\n(LRSAO), a novel approach that selects auxiliary objectives using reinforcement\nlearning (RL) to support the optimization process of an evolutionary algorithm\n(EA) as in EA+RL framework and furthermore incorporates the ability to unlearn\npreviously used objectives. By modifying the reward mechanism to penalize moves\nthat do no increase the fitness value and relying on the local auxiliary\nobjectives, LRSAO dynamically adapts its selection strategy to optimize\nperformance according to the landscape and unlearn previous objectives when\nnecessary.\n  We analyze and evaluate LRSAO on the black-box complexity version of the\nnon-monotonic Jump function, with gap parameter $\\ell$, where each auxiliary\nobjective is beneficial at specific stages of optimization. The Jump function\nis hard to optimize for evolutionary-based algorithms and the best-known\ncomplexity for reinforcement-based selection on Jump was $O(n^2 \\log(n) /\n\\ell)$. Our approach improves over this result to achieve a complexity of\n$\\Theta(n^2 / \\ell^2 + n \\log(n))$ resulting in a significant improvement,\nwhich demonstrates the efficiency and adaptability of LRSAO, highlighting its\npotential to outperform traditional methods in complex optimization scenarios.", "published": "2025-04-19 23:00:24", "link": "http://arxiv.org/abs/2504.14418v1", "categories": ["cs.NE", "stat.ML"], "primary_category": "cs.NE"}
{"title": "Local distribution-based adaptive oversampling for imbalanced regression", "abstract": "Imbalanced regression occurs when continuous target variables have skewed\ndistributions, creating sparse regions that are difficult for machine learning\nmodels to predict accurately. This issue particularly affects neural networks,\nwhich often struggle with imbalanced data. While class imbalance in\nclassification has been extensively studied, imbalanced regression remains\nrelatively unexplored, with few effective solutions. Existing approaches often\nrely on arbitrary thresholds to categorize samples as rare or frequent,\nignoring the continuous nature of target distributions. These methods can\nproduce synthetic samples that fail to improve model performance and may\ndiscard valuable information through undersampling. To address these\nlimitations, we propose LDAO (Local Distribution-based Adaptive Oversampling),\na novel data-level approach that avoids categorizing individual samples as rare\nor frequent. Instead, LDAO learns the global distribution structure by\ndecomposing the dataset into a mixture of local distributions, each preserving\nits statistical characteristics. LDAO then models and samples from each local\ndistribution independently before merging them into a balanced training set.\nLDAO achieves a balanced representation across the entire target range while\npreserving the inherent statistical structure within each local distribution.\nIn extensive evaluations on 45 imbalanced datasets, LDAO outperforms\nstate-of-the-art oversampling methods on both frequent and rare target values,\ndemonstrating its effectiveness for addressing the challenge of imbalanced\nregression.", "published": "2025-04-19 14:36:41", "link": "http://arxiv.org/abs/2504.14316v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Generative emulation of chaotic dynamics with coherent prior", "abstract": "Data-driven emulation of nonlinear dynamics is challenging due to long-range\nskill decay that often produces physically unrealistic outputs. Recent advances\nin generative modeling aim to address these issues by providing uncertainty\nquantification and correction. However, the quality of generated simulation\nremains heavily dependent on the choice of conditioning priors. In this work,\nwe present an efficient generative framework for dynamics emulation, unifying\nprinciples of turbulence with diffusion-based modeling: Cohesion. Specifically,\nour method estimates large-scale coherent structure of the underlying dynamics\nas guidance during the denoising process, where small-scale fluctuation in the\nflow is then resolved. These coherent priors are efficiently approximated using\nreduced-order models, such as deep Koopman operators, that allow for rapid\ngeneration of long prior sequences while maintaining stability over extended\nforecasting horizon. With this gain, we can reframe forecasting as trajectory\nplanning, a common task in reinforcement learning, where conditional denoising\nis performed once over entire sequences, minimizing the computational cost of\nautoregressive-based generative methods. Empirical evaluations on chaotic\nsystems of increasing complexity, including Kolmogorov flow, shallow water\nequations, and subseasonal-to-seasonal climate dynamics, demonstrate Cohesion\nsuperior long-range forecasting skill that can efficiently generate\nphysically-consistent simulations, even in the presence of partially-observed\nguidance.", "published": "2025-04-19 11:14:40", "link": "http://arxiv.org/abs/2504.14264v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning over von Mises-Fisher Distributions via a Wasserstein-like Geometry", "abstract": "We introduce a novel, geometry-aware distance metric for the family of von\nMises-Fisher (vMF) distributions, which are fundamental models for directional\ndata on the unit hypersphere. Although the vMF distribution is widely employed\nin a variety of probabilistic learning tasks involving spherical data,\nprincipled tools for comparing vMF distributions remain limited, primarily due\nto the intractability of normalization constants and the absence of suitable\ngeometric metrics. Motivated by the theory of optimal transport, we propose a\nWasserstein-like distance that decomposes the discrepancy between two vMF\ndistributions into two interpretable components: a geodesic term capturing the\nangular separation between mean directions, and a variance-like term\nquantifying differences in concentration parameters. The derivation leverages a\nGaussian approximation in the high-concentration regime to yield a tractable,\nclosed-form expression that respects the intrinsic spherical geometry. We show\nthat the proposed distance exhibits desirable theoretical properties and\ninduces a latent geometric structure on the space of non-degenerate vMF\ndistributions. As a primary application, we develop the efficient algorithms\nfor vMF mixture reduction, enabling structure-preserving compression of mixture\nmodels in high-dimensional settings. Empirical results on synthetic datasets\nand real-world high-dimensional embeddings, including biomedical sentence\nrepresentations and deep visual features, demonstrate the effectiveness of the\nproposed geometry in distinguishing distributions and supporting interpretable\ninference. This work expands the statistical toolbox for directional data\nanalysis by introducing a tractable, transport-inspired distance tailored to\nthe geometry of the hypersphere.", "published": "2025-04-19 03:38:15", "link": "http://arxiv.org/abs/2504.14164v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Robust Estimation in metric spaces: Achieving Exponential Concentration with a Fr\u00e9chet Median", "abstract": "There is growing interest in developing statistical estimators that achieve\nexponential concentration around a population target even when the data\ndistribution has heavier than exponential tails. More recent activity has\nfocused on extending such ideas beyond Euclidean spaces to Hilbert spaces and\nRiemannian manifolds. In this work, we show that such exponential concentration\nin presence of heavy tails can be achieved over a broader class of parameter\nspaces called CAT($\\kappa$) spaces, a very general metric space equipped with\nthe minimal essential geometric structure for our purpose, while being\nsufficiently broad to encompass most typical examples encountered in statistics\nand machine learning. The key technique is to develop and exploit a general\nconcentration bound for the Fr\\'echet median in CAT($\\kappa$) spaces. We\nillustrate our theory through a number of examples, and provide empirical\nsupport through simulation studies.", "published": "2025-04-19 03:19:51", "link": "http://arxiv.org/abs/2504.14161v1", "categories": ["math.ST", "math.MG", "stat.ML", "stat.TH"], "primary_category": "math.ST"}
{"title": "Data Augmentation Using Neural Acoustic Fields With Retrieval-Augmented Pre-training", "abstract": "This report details MERL's system for room impulse response (RIR) estimation\nsubmitted to the Generative Data Augmentation Workshop at ICASSP 2025 for\nAugmenting RIR Data (Task 1) and Improving Speaker Distance Estimation (Task\n2). We first pre-train a neural acoustic field conditioned by room geometry on\nan external large-scale dataset in which pairs of RIRs and the geometries are\nprovided. The neural acoustic field is then adapted to each target room by\nusing the enrollment data, where we leverage either the provided room\ngeometries or geometries retrieved from the external dataset, depending on\navailability. Lastly, we predict the RIRs for each pair of source and receiver\nlocations specified by Task 1, and use these RIRs to train the speaker distance\nestimation model in Task 2.", "published": "2025-04-19 21:43:56", "link": "http://arxiv.org/abs/2504.14409v1", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Iterative Polynomial Approximation Algorithms for Inverse Graph Filters", "abstract": "Chebyshev interpolation polynomials exhibit the exponential approximation\nproperty to analytic functions on a cube. Based on the Chebyshev interpolation\npolynomial approximation, we propose\n  iterative polynomial approximation algorithms to implement the inverse filter\nwith a polynomial graph filter of commutative graph shifts in a distributed\nmanner. The proposed algorithms exhibit exponential convergence properties, and\nthey can be implemented on distributed networks in which agents are equipped\nwith a data processing subsystem for limited data storage and computation\npower, and with a one-hop communication subsystem for direct data exchange only\nwith their adjacent agents. Our simulations show that the proposed polynomial\napproximation algorithms may converge faster than the Chebyshev polynomial\napproximation algorithm\n  and the conventional gradient descent algorithm\n  do.", "published": "2025-04-19 16:18:16", "link": "http://arxiv.org/abs/2504.14341v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Exploiting Symmetric Non-Convexity for Multi-Objective Symbol-Level DFRC Signal Design", "abstract": "Symbol-level precoding (SLP) is a promising solution for addressing the\ninherent interference problem in dual-functional radar-communication (DFRC)\nsignal designs. This paper considers an SLP-DFRC signal design problem which\noptimizes the radar performance under communication performance constraints. We\nshow that a common phase modulation applied to the transmit signals from an\nantenna array does not affect the performance of different radar sensing\nmetrics, including beampattern similarity, signal-to-interference-plus-noise\nratio (SINR), and Cram\\'er-Rao lower bound (CRLB). We refer to this as\nsymmetric-rotation invariance, upon which we develop low-complexity yet\nefficient DFRC signal design algorithms. More specifically, we propose a\nsymmetric non-convexity (SNC)-based DFRC algorithm that relies on the\nnon-convexity of the radar sensing metrics to identify a set of radar-only\nsolutions. Based on these solutions, we further exploit the symmetry property\nof the radar sensing metrics to efficiently design the DFRC signal. We show\nthat the proposed SNC-based algorithm is versatile in the sense that it can be\napplied to the DFRC signal optimization of all three sensing metrics mentioned\nabove (beampattern, SINR, and CRLB). In addition, since the radar sensing\nmetrics are independent of the communication channel and data symbols, the set\nof radar-only solutions can be constructed offline, thereby reducing the\ncomputational complexity. We also develop an accelerated SNC-based algorithm\nthat further reduces the complexity. Finally, we numerically demonstrate the\nsuperiority of the proposed algorithms compared to existing methods in terms of\nsensing and communication performance as well as computational requirements.", "published": "2025-04-19 12:29:11", "link": "http://arxiv.org/abs/2504.14281v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Real-time and Hardware Efficient Artfecat-free Spike Sorting Using Deep Spike Detection", "abstract": "Spike sorting is a valuable tool in understanding brain regions. It assigns\ndetected spike waveforms to their origins, helping to research the mechanism of\nthe human brain and the development of implantable brain-machine interfaces\n(iBMIs). The presence of noise and artefacts will adversely affect the efficacy\nof spike sorting. This paper proposes a framework for low-cost and real-time\nimplementation of deep spike detection, which consists of two one-dimensional\n(1-D) convolutional neural network (CNN) model for channel selection and\nartefact removal. The framework utilizes simulation and hardware layers, and it\napplies several low-power techniques to optimise the implementation cost of a\n1-D CNN model. A compact CNN model with 210 bytes memory size is achieved using\nstructured pruning, network projection and quantization in the simulation\nlayer. The hardware layer also accommodates various techniques including a\ncustomized multiply-accumulate (MAC) engine, novel fused layers in the\nconvolution pipeline and proposing flexible resource allocation for a\npower-efficient and low-delay design. The optimized 1-D CNN significantly\ndecreases both computational complexity and model size, with only a minimal\nreduction in accuracy. Classification of 1-D CNN on the Cyclone V 5CSEMA5F31C6\nFPGA evaluation platform is accomplished in just 16.8 microseconds at a\nfrequency of 2.5 MHz. The FPGA prototype achieves an accuracy rate of 97.14% on\na standard dataset and operates with a power consumption of 2.67mW from a\nsupply voltage of 1.1 volts. An accuracy of 95.05% is achieved with a power of\n5.6mW when deep spike detection is implemented using two optimized 1-D CNNs on\nan FPGA board.", "published": "2025-04-19 12:19:09", "link": "http://arxiv.org/abs/2504.14279v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "An Artificial Intelligence Enabled Signature Estimation of Dual Wideband Systems in Ultra-Low Signal-to-Noise Ratio", "abstract": "Millimeter-wave (mmWave) massive Multiple Input Multiple Output (MIMO)\nsystems encounter both spatial wideband spreading and temporal wideband effects\nin the communication channels of individual users. Accurate estimation of a\nuser's channel signature -- specifically, the direction of arrival and time of\narrival -- is crucial for designing efficient beamforming transceivers,\nespecially under noisy observations. In this work, we propose an Artificial\nIntelligence (AI)-enabled framework for estimating the channel signature of a\nuser's location in mmWave massive MIMO systems. Our approach explicitly\naccounts for spatial wideband spreading, finite basis leakage effects, and\nsignificant unknown receiver noise. We demonstrate the effectiveness of a\ndenoising convolutional neural network with residual learning for recovering\nchannel responses, even when channel gains are of extremely low amplitude and\nembedded in ultra-high receiver noise environments. Notably, our method\nsuccessfully recovers spatio-temporal diversity branches at signal-to-noise\nratios as low as -20 dB. Furthermore, we introduce a local gravitation-based\nclustering algorithm to infer the number of physical propagation paths (unknown\na priori) and to identify their respective support in the delay-angle domain of\nthe denoised response. To complement our approach, we design tailored metrics\nfor evaluating denoising and clustering performance within the context of\nwireless communications. We validate our framework through system-level\nsimulations using Orthogonal Frequency Division Multiplexing (OFDM) with a\nQuadrature Phase Shift Keying (QPSK) modulation scheme over mmWave fading\nchannels, highlighting the necessity and robustness of the proposed methods in\nultra-low SNR scenarios.", "published": "2025-04-19 08:20:16", "link": "http://arxiv.org/abs/2504.14226v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
