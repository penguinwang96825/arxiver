{"title": "Towards Neural Machine Translation with Partially Aligned Corpora", "abstract": "While neural machine translation (NMT) has become the new paradigm, the\nparameter optimization requires large-scale parallel data which is scarce in\nmany domains and language pairs. In this paper, we address a new translation\nscenario in which there only exists monolingual corpora and phrase pairs. We\npropose a new method towards translation with partially aligned sentence pairs\nwhich are derived from the phrase pairs and monolingual corpora. To make full\nuse of the partially aligned corpora, we adapt the conventional NMT training\nmethod in two aspects. On one hand, different generation strategies are\ndesigned for aligned and unaligned target words. On the other hand, a different\nobjective function is designed to model the partially aligned parts. The\nexperiments demonstrate that our method can achieve a relatively good result in\nsuch a translation scenario, and tiny bitexts can boost translation quality to\na large extent.", "published": "2017-11-03 03:15:44", "link": "http://arxiv.org/abs/1711.01006v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual Language Models for Code Switched Speech Recognition", "abstract": "In this work, we present a simple and elegant approach to language modeling\nfor bilingual code-switched text. Since code-switching is a blend of two or\nmore different languages, a standard bilingual language model can be improved\nupon by using structures of the monolingual language models. We propose a novel\ntechnique called dual language models, which involves building two\ncomplementary monolingual language models and combining them using a\nprobabilistic model for switching between the two. We evaluate the efficacy of\nour approach using a conversational Mandarin-English speech corpus. We prove\nthe robustness of our model by showing significant improvements in perplexity\nmeasures over the standard bilingual language model without the use of any\nexternal information. Similar consistent improvements are also reflected in\nautomatic speech recognition error rates.", "published": "2017-11-03 07:56:31", "link": "http://arxiv.org/abs/1711.01048v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compressing Word Embeddings via Deep Compositional Code Learning", "abstract": "Natural language processing (NLP) models often require a massive number of\nparameters for word embeddings, resulting in a large storage or memory\nfootprint. Deploying neural NLP models to mobile devices requires compressing\nthe word embeddings without any significant sacrifices in performance. For this\npurpose, we propose to construct the embeddings with few basis vectors. For\neach word, the composition of basis vectors is determined by a hash code. To\nmaximize the compression rate, we adopt the multi-codebook quantization\napproach instead of binary coding scheme. Each code is composed of multiple\ndiscrete numbers, such as (3, 2, 1, 8), where the value of each component is\nlimited to a fixed range. We propose to directly learn the discrete codes in an\nend-to-end neural network by applying the Gumbel-softmax trick. Experiments\nshow the compression rate achieves 98% in a sentiment analysis task and 94% ~\n99% in machine translation tasks without performance loss. In both tasks, the\nproposed method can improve the model performance by slightly lowering the\ncompression rate. Compared to other approaches such as character-level\nsegmentation, the proposed method is language-independent and does not require\nmodifications to the network architecture.", "published": "2017-11-03 09:05:44", "link": "http://arxiv.org/abs/1711.01068v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One Model to Rule them all: Multitask and Multilingual Modelling for\n  Lexical Analysis", "abstract": "When learning a new skill, you take advantage of your preexisting skills and\nknowledge. For instance, if you are a skilled violinist, you will likely have\nan easier time learning to play cello. Similarly, when learning a new language\nyou take advantage of the languages you already speak. For instance, if your\nnative language is Norwegian and you decide to learn Dutch, the lexical overlap\nbetween these two languages will likely benefit your rate of language\nacquisition. This thesis deals with the intersection of learning multiple tasks\nand learning multiple languages in the context of Natural Language Processing\n(NLP), which can be defined as the study of computational processing of human\nlanguage. Although these two types of learning may seem different on the\nsurface, we will see that they share many similarities.\n  The traditional approach in NLP is to consider a single task for a single\nlanguage at a time. However, recent advances allow for broadening this\napproach, by considering data for multiple tasks and languages simultaneously.\nThis is an important approach to explore further as the key to improving the\nreliability of NLP, especially for low-resource languages, is to take advantage\nof all relevant data whenever possible. In doing so, the hope is that in the\nlong term, low-resource languages can benefit from the advances made in NLP\nwhich are currently to a large extent reserved for high-resource languages.\nThis, in turn, may then have positive consequences for, e.g., language\npreservation, as speakers of minority languages will have a lower degree of\npressure to using high-resource languages. In the short term, answering the\nspecific research questions posed should be of use to NLP researchers working\ntowards the same goal.", "published": "2017-11-03 10:53:05", "link": "http://arxiv.org/abs/1711.01100v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Filterbanks from Raw Speech for Phone Recognition", "abstract": "We train a bank of complex filters that operates on the raw waveform and is\nfed into a convolutional neural network for end-to-end phone recognition. These\ntime-domain filterbanks (TD-filterbanks) are initialized as an approximation of\nmel-filterbanks, and then fine-tuned jointly with the remaining convolutional\narchitecture. We perform phone recognition experiments on TIMIT and show that\nfor several architectures, models trained on TD-filterbanks consistently\noutperform their counterparts trained on comparable mel-filterbanks. We get our\nbest performance by learning all front-end steps, from pre-emphasis up to\naveraging. Finally, we observe that the filters at convergence have an\nasymmetric impulse response, and that some of them remain almost analytic.", "published": "2017-11-03 13:56:53", "link": "http://arxiv.org/abs/1711.01161v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Attention\" for Detecting Unreliable News in the Information Age", "abstract": "An Unreliable news is any piece of information which is false or misleading,\ndeliberately spread to promote political, ideological and financial agendas.\nRecently the problem of unreliable news has got a lot of attention as the\nnumber instances of using news and social media outlets for propaganda have\nincreased rapidly. This poses a serious threat to society, which calls for\ntechnology to automatically and reliably identify unreliable news sources. This\npaper is an effort made in this direction to build systems for detecting\nunreliable news articles. In this paper, various NLP algorithms were built and\nevaluated on Unreliable News Data 2017 dataset. Variants of hierarchical\nattention networks (HAN) are presented for encoding and classifying news\narticles which achieve the best results of 0.944 ROC-AUC. Finally, Attention\nlayer weights are visualized to understand and give insight into the decisions\nmade by HANs. The results obtained are very promising and encouraging to deploy\nand use these systems in the real world to mitigate the problem of unreliable\nnews.", "published": "2017-11-03 23:48:18", "link": "http://arxiv.org/abs/1711.01362v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-tuning Tree-LSTM for phrase-level sentiment classification on a\n  Polish dependency treebank. Submission to PolEval task 2", "abstract": "We describe a variant of Child-Sum Tree-LSTM deep neural network (Tai et al,\n2015) fine-tuned for working with dependency trees and morphologically rich\nlanguages using the example of Polish. Fine-tuning included applying a custom\nregularization technique (zoneout, described by (Krueger et al., 2016), and\nfurther adapted for Tree-LSTMs) as well as using pre-trained word embeddings\nenhanced with sub-word information (Bojanowski et al., 2016). The system was\nimplemented in PyTorch and evaluated on phrase-level sentiment labeling task as\npart of the PolEval competition.", "published": "2017-11-03 14:52:08", "link": "http://arxiv.org/abs/1711.01985v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
