{"title": "TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real\n  World", "abstract": "To facilitate the research on intelligent and human-like chatbots with\nmulti-modal context, we introduce a new video-based multi-modal dialogue\ndataset, called TikTalk. We collect 38K videos from a popular video-sharing\nplatform, along with 367K conversations posted by users beneath them. Users\nengage in spontaneous conversations based on their multi-modal experiences from\nwatching videos, which helps recreate real-world chitchat context. Compared to\nprevious multi-modal dialogue datasets, the richer context types in TikTalk\nlead to more diverse conversations, but also increase the difficulty in\ncapturing human interests from intricate multi-modal information to generate\npersonalized responses. Moreover, external knowledge is more frequently evoked\nin our dataset. These facts reveal new challenges for multi-modal dialogue\nmodels. We quantitatively demonstrate the characteristics of TikTalk, propose a\nvideo-based multi-modal chitchat task, and evaluate several dialogue baselines.\nExperimental results indicate that the models incorporating large language\nmodels (LLM) can generate more diverse responses, while the model utilizing\nknowledge graphs to introduce external knowledge performs the best overall.\nFurthermore, no existing model can solve all the above challenges well. There\nis still a large room for future improvements, even for LLM with visual\nextensions. Our dataset is available at\n\\url{https://ruc-aimind.github.io/projects/TikTalk/}.", "published": "2023-01-14 10:18:22", "link": "http://arxiv.org/abs/2301.05880v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "tasksource: A Dataset Harmonization Framework for Streamlined NLP\n  Multi-Task Learning and Evaluation", "abstract": "The HuggingFace Datasets Hub hosts thousands of datasets, offering exciting\nopportunities for language model training and evaluation. However, datasets for\na specific task type often have different schemas, making harmonization\nchallenging. Multi-task training or evaluation necessitates manual work to fit\ndata into task templates. Several initiatives independently tackle this issue\nby releasing harmonized datasets or providing harmonization codes to preprocess\ndatasets into a consistent format. We identify patterns across previous\npreprocessing efforts, such as column name mapping and extracting specific\nsub-fields from structured data in a column. We then propose a structured\nannotation framework that ensures our annotations are fully exposed and not\nhidden within unstructured code. We release a dataset annotation framework and\ndataset annotations for more than 500 English\ntasks\\footnote{\\url{https://github.com/sileod/tasksource}}. These annotations\ninclude metadata, such as the names of columns to be used as input or labels\nfor all datasets, which can save time for future dataset preprocessing,\nregardless of whether our framework is utilized. We fine-tune a multi-task text\nencoder on all tasksource tasks, outperforming every publicly available text\nencoder of comparable size in an external evaluation.", "published": "2023-01-14 16:38:04", "link": "http://arxiv.org/abs/2301.05948v3", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models to Power Chatbots for Collecting User\n  Self-Reported Data", "abstract": "Large language models (LLMs) provide a new way to build chatbots by accepting\nnatural language prompts. Yet, it is unclear how to design prompts to power\nchatbots to carry on naturalistic conversations while pursuing a given goal,\nsuch as collecting self-report data from users. We explore what design factors\nof prompts can help steer chatbots to talk naturally and collect data reliably.\nTo this aim, we formulated four prompt designs with different structures and\npersonas. Through an online study (N = 48) where participants conversed with\nchatbots driven by different designs of prompts, we assessed how prompt designs\nand conversation topics affected the conversation flows and users' perceptions\nof chatbots. Our chatbots covered 79% of the desired information slots during\nconversations, and the designs of prompts and topics significantly influenced\nthe conversation flows and the data collection performance. We discuss the\nopportunities and challenges of building chatbots with LLMs.", "published": "2023-01-14 07:29:36", "link": "http://arxiv.org/abs/2301.05843v2", "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "primary_category": "cs.HC"}
{"title": "Detecting Stance of Authorities towards Rumors in Arabic Tweets: A\n  Preliminary Study", "abstract": "A myriad of studies addressed the problem of rumor verification in Twitter by\neither utilizing evidence from the propagation networks or external evidence\nfrom the Web. However, none of these studies exploited evidence from trusted\nauthorities. In this paper, we define the task of detecting the stance of\nauthorities towards rumors in tweets, i.e., whether a tweet from an authority\nagrees, disagrees, or is unrelated to the rumor. We believe the task is useful\nto augment the sources of evidence utilized by existing rumor verification\nsystems. We construct and release the first Authority STance towards Rumors\n(AuSTR) dataset, where evidence is retrieved from authority timelines in Arabic\nTwitter. Due to the relatively limited size of our dataset, we study the\nusefulness of existing datasets for stance detection in our task. We show that\nexisting datasets are somewhat useful for the task; however, they are clearly\ninsufficient, which motivates the need to augment them with annotated data\nconstituting stance of authorities from Twitter.", "published": "2023-01-14 09:28:43", "link": "http://arxiv.org/abs/2301.05863v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Music Playlist Title Generation Using Artist Information", "abstract": "Automatically generating or captioning music playlist titles given a set of\ntracks is of significant interest in music streaming services as customized\nplaylists are widely used in personalized music recommendation, and\nwell-composed text titles attract users and help their music discovery. We\npresent an encoder-decoder model that generates a playlist title from a\nsequence of music tracks. While previous work takes track IDs as tokenized\ninput for playlist title generation, we use artist IDs corresponding to the\ntracks to mitigate the issue from the long-tail distribution of tracks included\nin the playlist dataset. Also, we introduce a chronological data split method\nto deal with newly-released tracks in real-world scenarios. Comparing the track\nIDs and artist IDs as input sequences, we show that the artist-based approach\nsignificantly enhances the performance in terms of word overlap, semantic\nrelevance, and diversity.", "published": "2023-01-14 00:19:39", "link": "http://arxiv.org/abs/2301.08145v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Modulation spectral features for speech emotion recognition using deep\n  neural networks", "abstract": "This work explores the use of constant-Q transform based modulation spectral\nfeatures (CQT-MSF) for speech emotion recognition (SER). The human perception\nand analysis of sound comprise of two important cognitive parts: early auditory\nanalysis and cortex-based processing. The early auditory analysis considers\nspectrogram-based representation whereas cortex-based analysis includes\nextraction of temporal modulations from the spectrogram. This temporal\nmodulation representation of spectrogram is called modulation spectral feature\n(MSF). As the constant-Q transform (CQT) provides higher resolution at emotion\nsalient low-frequency regions of speech, we find that CQT-based spectrogram,\ntogether with its temporal modulations, provides a representation enriched with\nemotion-specific information. We argue that CQT-MSF when used with a\n2-dimensional convolutional network can provide a time-shift invariant and\ndeformation insensitive representation for SER. Our results show that CQT-MSF\noutperforms standard mel-scale based spectrogram and its modulation features on\ntwo popular SER databases, Berlin EmoDB and RAVDESS. We also show that our\nproposed feature outperforms the shift and deformation invariant scattering\ntransform coefficients, hence, showing the importance of joint hand-crafted and\nself-learned feature extraction instead of reliance on complete hand-crafted\nfeatures. Finally, we perform Grad-CAM analysis to visually inspect the\ncontribution of constant-Q modulation features over SER.", "published": "2023-01-14 09:36:49", "link": "http://arxiv.org/abs/2301.05868v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Acoustic correlates of the syllabic rhythm of speech: Modulation\n  spectrum or local features of the temporal envelope", "abstract": "The syllable is a perceptually salient unit in speech. Since both the\nsyllable and its acoustic correlate, i.e., the speech envelope, have a\npreferred range of rhythmicity between 4 and 8 Hz, it is hypothesized that\ntheta-band neural oscillations play a major role in extracting syllables based\non the envelope. A literature survey, however, reveals inconsistent evidence\nabout the relationship between speech envelope and syllables, and the current\nstudy revisits this question by analyzing large speech corpora. It is shown\nthat the center frequency of speech envelope, characterized by the modulation\nspectrum, reliably correlates with the rate of syllables only when the analysis\nis pooled over minutes of speech recordings. In contrast, in the time domain, a\ncomponent of the speech envelope is reliably phase-locked to syllable onsets.\nBased on a speaker-independent model, the timing of syllable onsets explains\nabout 24% variance of the speech envelope. These results indicate that local\nfeatures in the speech envelope, instead of the modulation spectrum, are a more\nreliable acoustic correlate of syllables.", "published": "2023-01-14 11:32:52", "link": "http://arxiv.org/abs/2301.05898v1", "categories": ["cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "cs.SD"}
{"title": "An Order-Complexity Model for Aesthetic Quality Assessment of Symbolic\n  Homophony Music Scores", "abstract": "Computational aesthetics evaluation has made great achievements in the field\nof visual arts, but the research work on music still needs to be explored.\nAlthough the existing work of music generation is very substantial, the quality\nof music score generated by AI is relatively poor compared with that created by\nhuman composers. The music scores created by AI are usually monotonous and\ndevoid of emotion. Based on Birkhoff's aesthetic measure, this paper proposes\nan objective quantitative evaluation method for homophony music score aesthetic\nquality assessment. The main contributions of our work are as follows: first,\nwe put forward a homophony music score aesthetic model to objectively evaluate\nthe quality of music score as a baseline model; second, we put forward eight\nbasic music features and four music aesthetic features.", "published": "2023-01-14 12:30:16", "link": "http://arxiv.org/abs/2301.05908v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
