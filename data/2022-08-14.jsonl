{"title": "Text Difficulty Study: Do machines behave the same as humans regarding\n  text difficulty?", "abstract": "Given a task, human learns from easy to hard, whereas the model learns\nrandomly. Undeniably, difficulty insensitive learning leads to great success in\nNLP, but little attention has been paid to the effect of text difficulty in\nNLP. In this research, we propose the Human Learning Matching Index (HLM Index)\nto investigate the effect of text difficulty. Experiment results show: (1) LSTM\nhas more human-like learning behavior than BERT. (2) UID-SuperLinear gives the\nbest evaluation of text difficulty among four text difficulty criteria. (3)\nAmong nine tasks, some tasks' performance is related to text difficulty,\nwhereas some are not. (4) Model trained on easy data performs best in easy and\nmedium data, whereas trains on a hard level only perform well on hard data. (5)\nTraining the model from easy to hard leads to fast convergence.", "published": "2022-08-14 06:12:08", "link": "http://arxiv.org/abs/2208.14509v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast Vocabulary Projection Method via Clustering for Multilingual\n  Machine Translation on GPU", "abstract": "Multilingual Neural Machine Translation has been showing great success using\ntransformer models. Deploying these models is challenging because they usually\nrequire large vocabulary (vocab) sizes for various languages. This limits the\nspeed of predicting the output tokens in the last vocab projection layer. To\nalleviate these challenges, this paper proposes a fast vocabulary projection\nmethod via clustering which can be used for multilingual transformers on GPUs.\nFirst, we offline split the vocab search space into disjoint clusters given the\nhidden context vector of the decoder output, which results in much smaller\nvocab columns for vocab projection. Second, at inference time, the proposed\nmethod predicts the clusters and candidate active tokens for hidden context\nvectors at the vocab projection. This paper also includes analysis of different\nways of building these clusters in multilingual settings. Our results show\nend-to-end speed gains in float16 GPU inference up to 25% while maintaining the\nBLEU score and slightly increasing memory cost. The proposed method speeds up\nthe vocab projection step itself by up to 2.6x. We also conduct an extensive\nhuman evaluation to verify the proposed method preserves the quality of the\ntranslations from the original model.", "published": "2022-08-14 16:10:14", "link": "http://arxiv.org/abs/2208.06874v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Models of Music Cognition and Composition", "abstract": "Much like most of cognition research, music cognition is an interdisciplinary\nfield, which attempts to apply methods of cognitive science (neurological,\ncomputational and experimental) to understand the perception and process of\ncomposition of music. In this paper, we first motivate why music is relevant to\ncognitive scientists and give an overview of the approaches to computational\nmodelling of music cognition. We then review literature on the various models\nof music perception, including non-computational models, computational\nnon-cognitive models and computational cognitive models. Lastly, we review\nliterature on modelling the creative behaviour and on computer systems capable\nof composing music. Since a lot of technical terms from music theory have been\nused, we have appended a list of relevant terms and their definitions at the\nend.", "published": "2022-08-14 16:27:59", "link": "http://arxiv.org/abs/2208.06878v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
