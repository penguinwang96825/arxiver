{"title": "A Roadmap towards Machine Intelligence", "abstract": "The development of intelligent machines is one of the biggest unsolved\nchallenges in computer science. In this paper, we propose some fundamental\nproperties these machines should have, focusing in particular on communication\nand learning. We discuss a simple environment that could be used to\nincrementally teach a machine the basics of natural-language-based\ncommunication, as a prerequisite to more complex interaction with human users.\nWe also present some conjectures on the sort of algorithms the machine should\nsupport in order to profitably learn from the environment.", "published": "2015-11-25 17:32:18", "link": "http://arxiv.org/abs/1511.08130v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Towards Universal Paraphrastic Sentence Embeddings", "abstract": "We consider the problem of learning general-purpose, paraphrastic sentence\nembeddings based on supervision from the Paraphrase Database (Ganitkevitch et\nal., 2013). We compare six compositional architectures, evaluating them on\nannotated textual similarity datasets drawn both from the same distribution as\nthe training data and from a wide range of other domains. We find that the most\ncomplex architectures, such as long short-term memory (LSTM) recurrent neural\nnetworks, perform best on the in-domain data. However, in out-of-domain\nscenarios, simple architectures such as word averaging vastly outperform LSTMs.\nOur simplest averaging model is even competitive with systems tuned for the\nparticular tasks while also being extremely efficient and easy to use.\n  In order to better understand how these architectures compare, we conduct\nfurther experiments on three supervised NLP tasks: sentence similarity,\nentailment, and sentiment classification. We again find that the word averaging\nmodels perform well for sentence similarity and entailment, outperforming\nLSTMs. However, on sentiment classification, we find that the LSTM performs\nvery strongly-even recording new state-of-the-art performance on the Stanford\nSentiment Treebank.\n  We then demonstrate how to combine our pretrained sentence embeddings with\nthese supervised tasks, using them both as a prior and as a black box feature\nextractor. This leads to performance rivaling the state of the art on the SICK\nsimilarity and entailment tasks. We release all of our resources to the\nresearch community with the hope that they can serve as the new baseline for\nfurther work on universal sentence embeddings.", "published": "2015-11-25 20:52:15", "link": "http://arxiv.org/abs/1511.08198v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning with Memory Embeddings", "abstract": "Embedding learning, a.k.a. representation learning, has been shown to be able\nto model large-scale semantic knowledge graphs. A key concept is a mapping of\nthe knowledge graph to a tensor representation whose entries are predicted by\nmodels using latent representations of generalized entities. Latent variable\nmodels are well suited to deal with the high dimensionality and sparsity of\ntypical knowledge graphs. In recent publications the embedding models were\nextended to also consider time evolutions, time patterns and subsymbolic\nrepresentations. In this paper we map embedding models, which were developed\npurely as solutions to technical problems for modelling temporal knowledge\ngraphs, to various cognitive memory functions, in particular to semantic and\nconcept memory, episodic memory, sensory memory, short-term memory, and working\nmemory. We discuss learning, query answering, the path from sensory input to\nsemantic decoding, and the relationship between episodic memory and semantic\nmemory. We introduce a number of hypotheses on human memory that can be derived\nfrom the developed mathematical models.", "published": "2015-11-25 07:06:09", "link": "http://arxiv.org/abs/1511.07972v9", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
