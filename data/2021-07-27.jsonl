{"title": "Measuring daily-life fear perception change: a computational study in\n  the context of COVID-19", "abstract": "COVID-19, as a global health crisis, has triggered the fear emotion with\nunprecedented intensity. Besides the fear of getting infected, the outbreak of\nCOVID-19 also created significant disruptions in people's daily life and thus\nevoked intensive psychological responses indirect to COVID-19 infections. Here,\nwe construct an expressed fear database using 16 million social media posts\ngenerated by 536 thousand users between January 1st, 2019 and August 31st, 2020\nin China. We employ deep learning techniques to detect the fear emotion within\neach post and apply topic models to extract the central fear topics. Based on\nthis database, we find that sleep disorders (\"nightmare\" and \"insomnia\") take\nup the largest share of fear-labeled posts in the pre-pandemic period (January\n2019-December 2019), and significantly increase during the COVID-19. We\nidentify health and work-related concerns are the two major sources of fear\ninduced by the COVID-19. We also detect gender differences, with females\ngenerating more posts containing the daily-life fear sources during the\nCOVID-19 period. This research adopts a data-driven approach to trace back\npublic emotion, which can be used to complement traditional surveys to achieve\nreal-time emotion monitoring to discern societal concerns and support policy\ndecision-making.", "published": "2021-07-27 05:17:09", "link": "http://arxiv.org/abs/2107.12606v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Transferring of Pre-trained Contextualized Language Models", "abstract": "Though the pre-trained contextualized language model (PrLM) has made a\nsignificant impact on NLP, training PrLMs in languages other than English can\nbe impractical for two reasons: other languages often lack corpora sufficient\nfor training powerful PrLMs, and because of the commonalities among human\nlanguages, computationally expensive PrLM training for different languages is\nsomewhat redundant. In this work, building upon the recent works connecting\ncross-lingual model transferring and neural machine translation, we thus\npropose a novel cross-lingual model transferring framework for PrLMs: TreLM. To\nhandle the symbol order and sequence length differences between languages, we\npropose an intermediate ``TRILayer\" structure that learns from these\ndifferences and creates a better transfer in our primary translation direction,\nas well as a new cross-lingual language modeling objective for transfer\ntraining. Additionally, we showcase an embedding aligning that adversarially\nadapts a PrLM's non-contextualized embedding space and the TRILayer structure\nto learn a text transformation network across languages, which addresses the\nvocabulary difference between languages. Experiments on both language\nunderstanding and structure parsing tasks show the proposed framework\nsignificantly outperforms language models trained from scratch with limited\ndata in both performance and efficiency. Moreover, despite an insignificant\nperformance loss compared to pre-training from scratch in resource-rich\nscenarios, our cross-lingual model transferring framework is significantly more\neconomical.", "published": "2021-07-27 06:51:13", "link": "http://arxiv.org/abs/2107.12627v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Domain Adaptation for Hate Speech Detection Using a Data\n  Augmentation Approach", "abstract": "Online harassment in the form of hate speech has been on the rise in recent\nyears. Addressing the issue requires a combination of content moderation by\npeople, aided by automatic detection methods. As content moderation is itself\nharmful to the people doing it, we desire to reduce the burden by improving the\nautomatic detection of hate speech. Hate speech presents a challenge as it is\ndirected at different target groups using a completely different vocabulary.\nFurther the authors of the hate speech are incentivized to disguise their\nbehavior to avoid being removed from a platform. This makes it difficult to\ndevelop a comprehensive data set for training and evaluating hate speech\ndetection models because the examples that represent one hate speech domain do\nnot typically represent others, even within the same language or culture. We\npropose an unsupervised domain adaptation approach to augment labeled data for\nhate speech detection. We evaluate the approach with three different models\n(character CNNs, BiLSTMs and BERT) on three different collections. We show our\napproach improves Area under the Precision/Recall curve by as much as 42% and\nrecall by as much as 278%, with no loss (and in some cases a significant gain)\nin precision.", "published": "2021-07-27 15:01:22", "link": "http://arxiv.org/abs/2107.12866v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion Stimulus Detection in German News Headlines", "abstract": "Emotion stimulus extraction is a fine-grained subtask of emotion analysis\nthat focuses on identifying the description of the cause behind an emotion\nexpression from a text passage (e.g., in the sentence \"I am happy that I passed\nmy exam\" the phrase \"passed my exam\" corresponds to the stimulus.). Previous\nwork mainly focused on Mandarin and English, with no resources or models for\nGerman. We fill this research gap by developing a corpus of 2006 German news\nheadlines annotated with emotions and 811 instances with annotations of\nstimulus phrases. Given that such corpus creation efforts are time-consuming\nand expensive, we additionally work on an approach for projecting the existing\nEnglish GoodNewsEveryone (GNE) corpus to a machine-translated German version.\nWe compare the performance of a conditional random field (CRF) model (trained\nmonolingually on German and cross-lingually via projection) with a multilingual\nXLM-RoBERTa (XLM-R) model. Our results show that training with the German\ncorpus achieves higher F1 scores than projection. Experiments with XLM-R\noutperform their respective CRF counterparts.", "published": "2021-07-27 16:22:04", "link": "http://arxiv.org/abs/2107.12920v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "gaBERT -- an Irish Language Model", "abstract": "The BERT family of neural language models have become highly popular due to\ntheir ability to provide sequences of text with rich context-sensitive token\nencodings which are able to generalise well to many NLP tasks. We introduce\ngaBERT, a monolingual BERT model for the Irish language. We compare our gaBERT\nmodel to multilingual BERT and the monolingual Irish WikiBERT, and we show that\ngaBERT provides better representations for a downstream parsing task. We also\nshow how different filtering criteria, vocabulary size and the choice of\nsubword tokenisation model affect downstream performance. We compare the\nresults of fine-tuning a gaBERT model with an mBERT model for the task of\nidentifying verbal multiword expressions, and show that the fine-tuned gaBERT\nmodel also performs better at this task. We release gaBERT and related code to\nthe community.", "published": "2021-07-27 16:38:53", "link": "http://arxiv.org/abs/2107.12930v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Rule-Execution Tracking Machine For Transformer-Based Text\n  Generation", "abstract": "Sequence-to-Sequence (S2S) neural text generation models, especially the\npre-trained ones (e.g., BART and T5), have exhibited compelling performance on\nvarious natural language generation tasks. However, the black-box nature of\nthese models limits their application in tasks where specific rules (e.g.,\ncontrollable constraints, prior knowledge) need to be executed. Previous works\neither design specific model structure (e.g., Copy Mechanism corresponding to\nthe rule \"the generated output should include certain words in the source\ninput\") or implement specialized inference algorithm (e.g., Constrained Beam\nSearch) to execute particular rules through the text generation. These methods\nrequire careful design case-by-case and are difficult to support multiple rules\nconcurrently. In this paper, we propose a novel module named Neural\nRule-Execution Tracking Machine that can be equipped into various\ntransformer-based generators to leverage multiple rules simultaneously to guide\nthe neural generation model for superior generation performance in a unified\nand scalable way. Extensive experimental results on several benchmarks verify\nthe effectiveness of our proposed model in both controllable and general text\ngeneration.", "published": "2021-07-27 20:41:05", "link": "http://arxiv.org/abs/2107.13077v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual Slot Selector via Local Reliability Verification for Dialogue State\n  Tracking", "abstract": "The goal of dialogue state tracking (DST) is to predict the current dialogue\nstate given all previous dialogue contexts. Existing approaches generally\npredict the dialogue state at every turn from scratch. However, the\noverwhelming majority of the slots in each turn should simply inherit the slot\nvalues from the previous turn. Therefore, the mechanism of treating slots\nequally in each turn not only is inefficient but also may lead to additional\nerrors because of the redundant slot value generation. To address this problem,\nwe devise the two-stage DSS-DST which consists of the Dual Slot Selector based\non the current turn dialogue, and the Slot Value Generator based on the\ndialogue history. The Dual Slot Selector determines each slot whether to update\nslot value or to inherit the slot value from the previous turn from two\naspects: (1) if there is a strong relationship between it and the current turn\ndialogue utterances; (2) if a slot value with high reliability can be obtained\nfor it through the current turn dialogue. The slots selected to be updated are\npermitted to enter the Slot Value Generator to update values by a hybrid\nmethod, while the other slots directly inherit the values from the previous\nturn. Empirical results show that our method achieves 56.93%, 60.73%, and\n58.04% joint accuracy on MultiWOZ 2.0, MultiWOZ 2.1, and MultiWOZ 2.2 datasets\nrespectively and achieves a new state-of-the-art performance with significant\nimprovements.", "published": "2021-07-27 03:40:05", "link": "http://arxiv.org/abs/2107.12578v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering\n  and Reading Comprehension", "abstract": "Alongside huge volumes of research on deep learning models in NLP in the\nrecent years, there has been also much work on benchmark datasets needed to\ntrack modeling progress. Question answering and reading comprehension have been\nparticularly prolific in this regard, with over 80 new datasets appearing in\nthe past two years. This study is the largest survey of the field to date. We\nprovide an overview of the various formats and domains of the current\nresources, highlighting the current lacunae for future work. We further discuss\nthe current classifications of \"skills\" that question answering/reading\ncomprehension systems are supposed to acquire, and propose a new taxonomy. The\nsupplementary materials survey the current multilingual resources and\nmonolingual resources for languages other than English, and we discuss the\nimplications of over-focusing on English. The study is aimed at both\npractitioners looking for pointers to the wealth of existing data, and at\nresearchers working on new resources.", "published": "2021-07-27 10:09:13", "link": "http://arxiv.org/abs/2107.12708v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Energy-based Unknown Intent Detection with Data Manipulation", "abstract": "Unknown intent detection aims to identify the out-of-distribution (OOD)\nutterance whose intent has never appeared in the training set. In this paper,\nwe propose using energy scores for this task as the energy score is\ntheoretically aligned with the density of the input and can be derived from any\nclassifier. However, high-quality OOD utterances are required during the\ntraining stage in order to shape the energy gap between OOD and in-distribution\n(IND), and these utterances are difficult to collect in practice. To tackle\nthis problem, we propose a data manipulation framework to Generate high-quality\nOOD utterances with importance weighTs (GOT). Experimental results show that\nthe energy-based detector fine-tuned by GOT can achieve state-of-the-art\nresults on two benchmark datasets.", "published": "2021-07-27 01:32:23", "link": "http://arxiv.org/abs/2107.12542v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Federated Learning Meets Natural Language Processing: A Survey", "abstract": "Federated Learning aims to learn machine learning models from multiple\ndecentralized edge devices (e.g. mobiles) or servers without sacrificing local\ndata privacy. Recent Natural Language Processing techniques rely on deep\nlearning and large pre-trained language models. However, both big deep neural\nand language models are trained with huge amounts of data which often lies on\nthe server side. Since text data is widely originated from end users, in this\nwork, we look into recent NLP models and techniques which use federated\nlearning as the learning framework. Our survey discusses major challenges in\nfederated natural language processing, including the algorithm challenges,\nsystem challenges as well as the privacy issues. We also provide a critical\nreview of the existing Federated NLP evaluation methods and tools. Finally, we\nhighlight the current research gaps and future directions.", "published": "2021-07-27 05:07:48", "link": "http://arxiv.org/abs/2107.12603v1", "categories": ["cs.CL", "cs.AI", "cs.DC"], "primary_category": "cs.CL"}
{"title": "Greedy Gradient Ensemble for Robust Visual Question Answering", "abstract": "Language bias is a critical issue in Visual Question Answering (VQA), where\nmodels often exploit dataset biases for the final decision without considering\nthe image information. As a result, they suffer from performance drop on\nout-of-distribution data and inadequate visual explanation. Based on\nexperimental analysis for existing robust VQA methods, we stress the language\nbias in VQA that comes from two aspects, i.e., distribution bias and shortcut\nbias. We further propose a new de-bias framework, Greedy Gradient Ensemble\n(GGE), which combines multiple biased models for unbiased base model learning.\nWith the greedy strategy, GGE forces the biased models to over-fit the biased\ndata distribution in priority, thus makes the base model pay more attention to\nexamples that are hard to solve by biased models. The experiments demonstrate\nthat our method makes better use of visual information and achieves\nstate-of-the-art performance on diagnosing dataset VQA-CP without using extra\nannotations.", "published": "2021-07-27 08:02:49", "link": "http://arxiv.org/abs/2107.12651v4", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Emotion Recognition under Consideration of the Emotion Component Process\n  Model", "abstract": "Emotion classification in text is typically performed with neural network\nmodels which learn to associate linguistic units with emotions. While this\noften leads to good predictive performance, it does only help to a limited\ndegree to understand how emotions are communicated in various domains. The\nemotion component process model (CPM) by Scherer (2005) is an interesting\napproach to explain emotion communication. It states that emotions are a\ncoordinated process of various subcomponents, in reaction to an event, namely\nthe subjective feeling, the cognitive appraisal, the expression, a\nphysiological bodily reaction, and a motivational action tendency. We\nhypothesize that these components are associated with linguistic realizations:\nan emotion can be expressed by describing a physiological bodily reaction (\"he\nwas trembling\"), or the expression (\"she smiled\"), etc. We annotate existing\nliterature and Twitter emotion corpora with emotion component classes and find\nthat emotions on Twitter are predominantly expressed by event descriptions or\nsubjective reports of the feeling, while in literature, authors prefer to\ndescribe what characters do, and leave the interpretation to the reader. We\nfurther include the CPM in a multitask learning model and find that this\nsupports the emotion categorization. The annotated corpora are available at\nhttps://www.ims.uni-stuttgart.de/data/emotion.", "published": "2021-07-27 15:53:25", "link": "http://arxiv.org/abs/2107.12895v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Red Dragon AI at TextGraphs 2021 Shared Task: Multi-Hop Inference\n  Explanation Regeneration by Matching Expert Ratings", "abstract": "Creating explanations for answers to science questions is a challenging task\nthat requires multi-hop inference over a large set of fact sentences. This\nyear, to refocus the Textgraphs Shared Task on the problem of gathering\nrelevant statements (rather than solely finding a single 'correct path'), the\nWorldTree dataset was augmented with expert ratings of 'relevance' of\nstatements to each overall explanation. Our system, which achieved second place\non the Shared Task leaderboard, combines initial statement retrieval; language\nmodels trained to predict the relevance scores; and ensembling of a number of\nthe resulting rankings. Our code implementation is made available at\nhttps://github.com/mdda/worldtree_corpus/tree/textgraphs_2021", "published": "2021-07-27 18:29:51", "link": "http://arxiv.org/abs/2107.13031v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Exceeding the Limits of Visual-Linguistic Multi-Task Learning", "abstract": "By leveraging large amounts of product data collected across hundreds of live\ne-commerce websites, we construct 1000 unique classification tasks that share\nsimilarly-structured input data, comprised of both text and images. These\nclassification tasks focus on learning the product hierarchy of different\ne-commerce websites, causing many of them to be correlated. Adopting a\nmulti-modal transformer model, we solve these tasks in unison using multi-task\nlearning (MTL). Extensive experiments are presented over an initial 100-task\ndataset to reveal best practices for \"large-scale MTL\" (i.e., MTL with more\nthan 100 tasks). From these experiments, a final, unified methodology is\nderived, which is composed of both best practices and new proposals such as\nDyPa, a simple heuristic for automatically allocating task-specific parameters\nto tasks that could benefit from extra capacity. Using our large-scale MTL\nmethodology, we successfully train a single model across all 1000 tasks in our\ndataset while using minimal task specific parameters, thereby showing that it\nis possible to extend several orders of magnitude beyond current efforts in\nMTL.", "published": "2021-07-27 19:42:14", "link": "http://arxiv.org/abs/2107.13054v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "68T07", "I.2.6; I.2.7; I.2.10"], "primary_category": "cs.AI"}
{"title": "Microphone Array Generalization for Multichannel Narrowband Deep Speech\n  Enhancement", "abstract": "This paper addresses the problem of microphone array generalization for\ndeep-learning-based end-to-end multichannel speech enhancement. We aim to train\na unique deep neural network (DNN) potentially performing well on unseen\nmicrophone arrays. The microphone array geometry shapes the network's\nparameters when training on a fixed microphone array, and thus restricts the\ngeneralization of the trained network to another microphone array. To resolve\nthis problem, a single network is trained using data recorded by various\nmicrophone arrays of different geometries. We design three variants of our\nrecently proposed narrowband network to cope with the agnostic number of\nmicrophones. Overall, the goal is to make the network learn the universal\ninformation for speech enhancement that is available for any array geometry,\nrather than learn the one-array-dedicated characteristics. The experiments on\nboth simulated and real room impulse responses (RIR) demonstrate the excellent\nacross-array generalization capability of the proposed networks, in the sense\nthat their performance measures are very close to, or even exceed the network\ntrained with test arrays. Moreover, they notably outperform various beamforming\nmethods and other advanced deep-learning-based methods.", "published": "2021-07-27 05:03:18", "link": "http://arxiv.org/abs/2107.12601v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-End Spectro-Temporal Graph Attention Networks for Speaker\n  Verification Anti-Spoofing and Speech Deepfake Detection", "abstract": "Artefacts that serve to distinguish bona fide speech from spoofed or deepfake\nspeech are known to reside in specific subbands and temporal segments. Various\napproaches can be used to capture and model such artefacts, however, none works\nwell across a spectrum of diverse spoofing attacks. Reliable detection then\noften depends upon the fusion of multiple detection systems, each tuned to\ndetect different forms of attack. In this paper we show that better performance\ncan be achieved when the fusion is performed within the model itself and when\nthe representation is learned automatically from raw waveform inputs. The\nprincipal contribution is a spectro-temporal graph attention network (GAT)\nwhich learns the relationship between cues spanning different sub-bands and\ntemporal intervals. Using a model-level graph fusion of spectral (S) and\ntemporal (T) sub-graphs and a graph pooling strategy to improve discrimination,\nthe proposed RawGAT-ST model achieves an equal error rate of 1.06 % for the\nASVspoof 2019 logical access database. This is one of the best results reported\nto date and is reproducible using an open source implementation.", "published": "2021-07-27 10:11:41", "link": "http://arxiv.org/abs/2107.12710v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PKSpell: Data-Driven Pitch Spelling and Key Signature Estimation", "abstract": "We present PKSpell: a data-driven approach for the joint estimation of pitch\nspelling and key signatures from MIDI files. Both elements are fundamental for\nthe production of a full-fledged musical score and facilitate many MIR tasks\nsuch as harmonic analysis, section identification, melodic similarity, and\nsearch in a digital music library. We design a deep recurrent neural network\nmodel that only requires information readily available in all kinds of MIDI\nfiles, including performances, or other symbolic encodings. We release a model\ntrained on the ASAP dataset. Our system can be used with these pre-trained\nparameters and is easy to integrate into a MIR pipeline. We also propose a data\naugmentation procedure that helps retraining on small datasets. PKSpell\nachieves strong key signature estimation performance on a challenging dataset.\nMost importantly, this model establishes a new state-of-the-art performance on\nthe MuseData pitch spelling dataset without retraining.", "published": "2021-07-27 13:34:47", "link": "http://arxiv.org/abs/2107.14009v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cross-speaker Style Transfer with Prosody Bottleneck in Neural Speech\n  Synthesis", "abstract": "Cross-speaker style transfer is crucial to the applications of multi-style\nand expressive speech synthesis at scale. It does not require the target\nspeakers to be experts in expressing all styles and to collect corresponding\nrecordings for model training. However, the performances of existing style\ntransfer methods are still far behind real application needs. The root causes\nare mainly twofold. Firstly, the style embedding extracted from single\nreference speech can hardly provide fine-grained and appropriate prosody\ninformation for arbitrary text to synthesize. Secondly, in these models the\ncontent/text, prosody, and speaker timbre are usually highly entangled, it's\ntherefore not realistic to expect a satisfied result when freely combining\nthese components, such as to transfer speaking style between speakers. In this\npaper, we propose a cross-speaker style transfer text-to-speech (TTS) model\nwith explicit prosody bottleneck. The prosody bottleneck builds up the kernels\naccounting for speaking style robustly, and disentangles the prosody from\ncontent and speaker timbre, therefore guarantees high quality cross-speaker\nstyle transfer. Evaluation result shows the proposed method even achieves\non-par performance with source speaker's speaker-dependent (SD) model in\nobjective measurement of prosody, and significantly outperforms the cycle\nconsistency and GMVAE-based baselines in objective and subjective evaluations.", "published": "2021-07-27 02:43:57", "link": "http://arxiv.org/abs/2107.12562v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Optimal Piezoelectric Beam for Acoustic Energy Harvesting", "abstract": "This study presents a novel piezoelectric beam structure for acoustic energy\nharvesting. The beams have been designed to maximize output energy in areas\nwhere the noise level is loud such as highway traffic. The beam consists of two\nlayers of copper and polyvinylidene fluoride that convert the ambient noise's\nvibration energy to electrical energy. The piezoelectric material's optimum\nplacement has been studied, and its best position is obtained on the substrate\nfor the maximum yield. Unlike previous studies, in which the entire beam\nsubstrate used to be covered by a material, this study presents a modest\nmaterial usage and contributes to lowering the harvester's final production\ncost. Additionally, in this study, an electrical model was developed for the\nsensor and a read-out circuitry was proposed for the converter. Moreover, the\nsensor was validated at different noise levels at various lengths and\nlocations. The simulations were performed in COMSOL Multiphysics and MATLAB and\nreport a maximum sound pressure of 140 dB from 100 dB point sources in an\nenclosed air-filled cubic meter chamber.", "published": "2021-07-27 08:37:32", "link": "http://arxiv.org/abs/2107.12671v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "The cyclotactor: towards a tactile platform for musical interaction", "abstract": "This paper reports on work in progress on a finger-based tactile I/O device\nfor musical interaction. Central to the device is the ability to set up\ncyclical relationships between tactile input and output. A direct practical\napplication of this to musical interaction is given, using the idea to\nmultiplex two degrees of freedom on a single tactile loop.", "published": "2021-07-27 10:02:57", "link": "http://arxiv.org/abs/2107.12704v1", "categories": ["cs.HC", "cs.SD", "eess.AS", "H.5.5"], "primary_category": "cs.HC"}
{"title": "Making grains tangible: microtouch for microsound", "abstract": "This paper proposes a new research direction for the large family of\ninstrumental musical interfaces where sound is generated using digital granular\nsynthesis, and where interaction and control involve the (fine) operation of\nstiff, flat contact surfaces. First, within a historical context, a general\nabsence of, and clear need for, tangible output that is dynamically\ninstantiated by the grain-generating process itself is identified. Second, to\nfill this gap, a concrete general approach is proposed based on the careful\nconstruction of non-vibratory and vibratory force pulses, in a one-to-one\nrelationship with sonic grains. An informal pilot psychophysics experiment\ninitiating the approach was conducted, which took into account the two main\ncases for applying forces to the human skin: perpendicular, and lateral.\nInitial results indicate that the force pulse approach can enable perceivably\nmultidimensional, tangible display of the ongoing grain-generating process.\nMoreover, it was found that this can be made to meaningfully happen (in real\ntime) in the same timescale of basic sonic grain generation. This is not a\ntrivial property, and provides an important and positive fundament for further\ndeveloping this type of enhanced display. It also leads to the exciting\nprospect of making arbitrary sonic grains actual physical manipulanda.", "published": "2021-07-27 10:18:56", "link": "http://arxiv.org/abs/2107.12714v1", "categories": ["cs.HC", "cs.SD", "eess.AS", "H.5.5"], "primary_category": "cs.HC"}
{"title": "The CORSMAL benchmark for the prediction of the properties of containers", "abstract": "The contactless estimation of the weight of a container and the amount of its\ncontent manipulated by a person are key pre-requisites for safe human-to-robot\nhandovers. However, opaqueness and transparencies of the container and the\ncontent, and variability of materials, shapes, and sizes, make this estimation\ndifficult. In this paper, we present a range of methods and an open framework\nto benchmark acoustic and visual perception for the estimation of the capacity\nof a container, and the type, mass, and amount of its content. The framework\nincludes a dataset, specific tasks and performance measures. We conduct an\nin-depth comparative analysis of methods that used this framework and\naudio-only or vision-only baselines designed from related works. Based on this\nanalysis, we can conclude that audio-only and audio-visual classifiers are\nsuitable for the estimation of the type and amount of the content using\ndifferent types of convolutional neural networks, combined with either\nrecurrent neural networks or a majority voting strategy, whereas computer\nvision methods are suitable to determine the capacity of the container using\nregression and geometric approaches. Classifying the content type and level\nusing only audio achieves a weighted average F1-score up to 81% and 97%,\nrespectively. Estimating the container capacity with vision-only approaches and\nestimating the filling mass with audio-visual multi-stage approaches reach up\nto 65% weighted average capacity and mass scores. These results show that there\nis still room for improvement on the design of new methods. These new methods\ncan be ranked and compared on the individual leaderboards provided by our open\nframework.", "published": "2021-07-27 10:36:19", "link": "http://arxiv.org/abs/2107.12719v3", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Audio-to-Score Alignment Using Deep Automatic Music Transcription", "abstract": "Audio-to-score alignment (A2SA) is a multimodal task consisting in the\nalignment of audio signals to music scores. Recent literature confirms the\nbenefits of Automatic Music Transcription (AMT) for A2SA at the frame-level. In\nthis work, we aim to elaborate on the exploitation of AMT Deep Learning (DL)\nmodels for achieving alignment at the note-level. We propose a method which\nbenefits from HMM-based score-to-score alignment and AMT, showing a remarkable\nadvancement beyond the state-of-the-art. We design a systematic procedure to\ntake advantage of large datasets which do not offer an aligned score. Finally,\nwe perform a thorough comparison and extensive tests on multiple datasets.", "published": "2021-07-27 14:41:41", "link": "http://arxiv.org/abs/2107.12854v3", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
