{"title": "Minimally-Supervised Structure-Rich Text Categorization via Learning on\n  Text-Rich Networks", "abstract": "Text categorization is an essential task in Web content analysis. Considering\nthe ever-evolving Web data and new emerging categories, instead of the\nlaborious supervised setting, in this paper, we focus on the\nminimally-supervised setting that aims to categorize documents effectively,\nwith a couple of seed documents annotated per category. We recognize that texts\ncollected from the Web are often structure-rich, i.e., accompanied by various\nmetadata. One can easily organize the corpus into a text-rich network, joining\nraw text documents with document attributes, high-quality phrases, label\nsurface names as nodes, and their associations as edges. Such a network\nprovides a holistic view of the corpus' heterogeneous data sources and enables\na joint optimization for network-based analysis and deep textual model\ntraining. We therefore propose a novel framework for minimally supervised\ncategorization by learning from the text-rich network. Specifically, we jointly\ntrain two modules with different inductive biases -- a text analysis module for\ntext understanding and a network learning module for class-discriminative,\nscalable network learning. Each module generates pseudo training labels from\nthe unlabeled document set, and both modules mutually enhance each other by\nco-training using pooled pseudo labels. We test our model on two real-world\ndatasets. On the challenging e-commerce product categorization dataset with 683\ncategories, our experiments show that given only three seed documents per\ncategory, our framework can achieve an accuracy of about 92%, significantly\noutperforming all compared methods; our accuracy is only less than 2% away from\nthe supervised BERT model trained on about 50K labeled documents.", "published": "2021-02-23 04:14:34", "link": "http://arxiv.org/abs/2102.11479v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Quality Assessment of Cognitive Behavioral Therapy Sessions\n  Through Highly Contextualized Language Representations", "abstract": "During a psychotherapy session, the counselor typically adopts techniques\nwhich are codified along specific dimensions (e.g., 'displays warmth and\nconfidence', or 'attempts to set up collaboration') to facilitate the\nevaluation of the session. Those constructs, traditionally scored by trained\nhuman raters, reflect the complex nature of psychotherapy and highly depend on\nthe context of the interaction. Recent advances in deep contextualized language\nmodels offer an avenue for accurate in-domain linguistic representations which\ncan lead to robust recognition and scoring of such psychotherapy-relevant\nbehavioral constructs, and support quality assurance and supervision. In this\nwork, we propose a BERT-based model for automatic behavioral scoring of a\nspecific type of psychotherapy, called Cognitive Behavioral Therapy (CBT),\nwhere prior work is limited to frequency-based language features and/or short\ntext excerpts which do not capture the unique elements involved in a\nspontaneous long conversational interaction. The model focuses on the\nclassification of therapy sessions with respect to the overall score achieved\non the widely-used Cognitive Therapy Rating Scale (CTRS), but is trained in a\nmulti-task manner in order to achieve higher interpretability. BERT-based\nrepresentations are further augmented with available therapy metadata,\nproviding relevant non-linguistic context and leading to consistent performance\nimprovements. We train and evaluate our models on a set of 1,118 real-world\ntherapy sessions, recorded and automatically transcribed. Our best model\nachieves an F1 score equal to 72.61% on the binary classification task of low\nvs. high total CTRS.", "published": "2021-02-23 09:22:29", "link": "http://arxiv.org/abs/2102.11573v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Model Robustness By Incorporating Adversarial Knowledge Into\n  Semantic Representation", "abstract": "Despite that deep neural networks (DNNs) have achieved enormous success in\nmany domains like natural language processing (NLP), they have also been proven\nto be vulnerable to maliciously generated adversarial examples. Such inherent\nvulnerability has threatened various real-world deployed DNNs-based\napplications. To strength the model robustness, several countermeasures have\nbeen proposed in the English NLP domain and obtained satisfactory performance.\nHowever, due to the unique language properties of Chinese, it is not trivial to\nextend existing defenses to the Chinese domain. Therefore, we propose AdvGraph,\na novel defense which enhances the robustness of Chinese-based NLP models by\nincorporating adversarial knowledge into the semantic representation of the\ninput. Extensive experiments on two real-world tasks show that AdvGraph\nexhibits better performance compared with previous work: (i) effective - it\nsignificantly strengthens the model robustness even under the adaptive attacks\nsetting without negative impact on model performance over legitimate input;\n(ii) generic - its key component, i.e., the representation of connotative\nadversarial knowledge is task-agnostic, which can be reused in any\nChinese-based NLP models without retraining; and (iii) efficient - it is a\nlight-weight defense with sub-linear computational complexity, which can\nguarantee the efficiency required in practical scenarios.", "published": "2021-02-23 09:47:45", "link": "http://arxiv.org/abs/2102.11584v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Factorization of Fact-Checks for Low Resource Indian Languages", "abstract": "The advancement in technology and accessibility of internet to each\nindividual is revolutionizing the real time information. The liberty to express\nyour thoughts without passing through any credibility check is leading to\ndissemination of fake content in the ecosystem. It can have disastrous effects\non both individuals and society as a whole. The amplification of fake news is\nbecoming rampant in India too. Debunked information often gets republished with\na replacement description, claiming it to depict some different incidence. To\ncurb such fabricated stories, it is necessary to investigate such deduplicates\nand false claims made in public. The majority of studies on automatic\nfact-checking and fake news detection is restricted to English only. But for a\ncountry like India where only 10% of the literate population speak English,\nrole of regional languages in spreading falsity cannot be undermined. In this\npaper, we introduce FactDRIL: the first large scale multilingual Fact-checking\nDataset for Regional Indian Languages. We collect an exhaustive dataset across\n7 months covering 11 low-resource languages. Our propose dataset consists of\n9,058 samples belonging to English, 5,155 samples to Hindi and remaining 8,222\nsamples are distributed across various regional languages, i.e. Bangla,\nMarathi, Malayalam, Telugu, Tamil, Oriya, Assamese, Punjabi, Urdu, Sinhala and\nBurmese. We also present the detailed characterization of three M's\n(multi-lingual, multi-media, multi-domain) in the FactDRIL accompanied with the\ncomplete list of other varied attributes making it a unique dataset to study.\nLastly, we present some potential use cases of the dataset. We expect this\ndataset will be a valuable resource and serve as a starting point to fight\nproliferation of fake news in low resource languages.", "published": "2021-02-23 16:47:41", "link": "http://arxiv.org/abs/2102.11276v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "A Novel Deep Learning Method for Textual Sentiment Analysis", "abstract": "Sentiment analysis is known as one of the most crucial tasks in the field of\nnatural language processing and Convolutional Neural Network (CNN) is one of\nthose prominent models that is commonly used for this aim. Although\nconvolutional neural networks have obtained remarkable results in recent years,\nthey are still confronted with some limitations. Firstly, they consider that\nall words in a sentence have equal contributions in the sentence meaning\nrepresentation and are not able to extract informative words. Secondly, they\nrequire a large number of training data to obtain considerable results while\nthey have many parameters that must be accurately adjusted. To this end, a\nconvolutional neural network integrated with a hierarchical attention layer is\nproposed which is able to extract informative words and assign them higher\nweight. Moreover, the effect of transfer learning that transfers knowledge\nlearned in the source domain to the target domain with the aim of improving the\nperformance is also explored. Based on the empirical results, the proposed\nmodel not only has higher classification accuracy and can extract informative\nwords but also applying incremental transfer learning can significantly enhance\nthe classification performance.", "published": "2021-02-23 12:11:36", "link": "http://arxiv.org/abs/2102.11651v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Paraphrases do not explain word analogies", "abstract": "Many types of distributional word embeddings (weakly) encode linguistic\nregularities as directions (the difference between \"jump\" and \"jumped\" will be\nin a similar direction to that of \"walk\" and \"walked,\" and so on). Several\nattempts have been made to explain this fact. We respond to Allen and\nHospedales' recent (ICML, 2019) theoretical explanation, which claims that\nword2vec and GloVe will encode linguistic regularities whenever a specific\nrelation of paraphrase holds between the four words involved in the regularity.\nWe demonstrate that the explanation does not go through: the paraphrase\nrelations needed under this explanation do not hold empirically.", "published": "2021-02-23 15:25:10", "link": "http://arxiv.org/abs/2102.11749v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Sensitivity of Word Embeddings-based Author Detection Models to\n  Semantic-preserving Adversarial Perturbations", "abstract": "Authorship analysis is an important subject in the field of natural language\nprocessing. It allows the detection of the most likely writer of articles,\nnews, books, or messages. This technique has multiple uses in tasks related to\nauthorship attribution, detection of plagiarism, style analysis, sources of\nmisinformation, etc. The focus of this paper is to explore the limitations and\nsensitiveness of established approaches to adversarial manipulations of inputs.\nTo this end, and using those established techniques, we first developed an\nexperimental frame-work for author detection and input perturbations. Next, we\nexperimentally evaluated the performance of the authorship detection model to a\ncollection of semantic-preserving adversarial perturbations of input\nnarratives. Finally, we compare and analyze the effects of different\nperturbation strategies, input and model configurations, and the effects of\nthese on the author detection model.", "published": "2021-02-23 19:55:45", "link": "http://arxiv.org/abs/2102.11917v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do Transformer Modifications Transfer Across Implementations and\n  Applications?", "abstract": "The research community has proposed copious modifications to the Transformer\narchitecture since it was introduced over three years ago, relatively few of\nwhich have seen widespread adoption. In this paper, we comprehensively evaluate\nmany of these modifications in a shared experimental setting that covers most\nof the common uses of the Transformer in natural language processing.\nSurprisingly, we find that most modifications do not meaningfully improve\nperformance. Furthermore, most of the Transformer variants we found beneficial\nwere either developed in the same codebase that we used or are relatively minor\nchanges. We conjecture that performance improvements may strongly depend on\nimplementation details and correspondingly make some recommendations for\nimproving the generality of experimental results.", "published": "2021-02-23 22:44:54", "link": "http://arxiv.org/abs/2102.11972v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Evolutionary optimization of contexts for phonetic correction in speech\n  recognition systems", "abstract": "Automatic Speech Recognition (ASR) is an area of growing academic and\ncommercial interest due to the high demand for applications that use it to\nprovide a natural communication method. It is common for general purpose ASR\nsystems to fail in applications that use a domain-specific language. Various\nstrategies have been used to reduce the error, such as providing a context that\nmodifies the language model and post-processing correction methods. This\narticle explores the use of an evolutionary process to generate an optimized\ncontext for a specific application domain, as well as different correction\ntechniques based on phonetic distance metrics. The results show the viability\nof a genetic algorithm as a tool for context optimization, which, added to a\npost-processing correction based on phonetic representations, can reduce the\nerrors on the recognized speech.", "published": "2021-02-23 04:14:51", "link": "http://arxiv.org/abs/2102.11480v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Memory-efficient Speech Recognition on Smart Devices", "abstract": "Recurrent transducer models have emerged as a promising solution for speech\nrecognition on the current and next generation smart devices. The transducer\nmodels provide competitive accuracy within a reasonable memory footprint\nalleviating the memory capacity constraints in these devices. However, these\nmodels access parameters from off-chip memory for every input time step which\nadversely effects device battery life and limits their usability on low-power\ndevices.\n  We address transducer model's memory access concerns by optimizing their\nmodel architecture and designing novel recurrent cell designs. We demonstrate\nthat i) model's energy cost is dominated by accessing model weights from\noff-chip memory, ii) transducer model architecture is pivotal in determining\nthe number of accesses to off-chip memory and just model size is not a good\nproxy, iii) our transducer model optimizations and novel recurrent cell reduces\noff-chip memory accesses by 4.5x and model size by 2x with minimal accuracy\nimpact.", "published": "2021-02-23 07:43:45", "link": "http://arxiv.org/abs/2102.11531v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Robust and Transferable Anomaly Detection in Log Data using Pre-Trained\n  Language Models", "abstract": "Anomalies or failures in large computer systems, such as the cloud, have an\nimpact on a large number of users that communicate, compute, and store\ninformation. Therefore, timely and accurate anomaly detection is necessary for\nreliability, security, safe operation, and mitigation of losses in these\nincreasingly important systems. Recently, the evolution of the software\nindustry opens up several problems that need to be tackled including (1)\naddressing the software evolution due software upgrades, and (2) solving the\ncold-start problem, where data from the system of interest is not available. In\nthis paper, we propose a framework for anomaly detection in log data, as a\nmajor troubleshooting source of system information. To that end, we utilize\npre-trained general-purpose language models to preserve the semantics of log\nmessages and map them into log vector embeddings. The key idea is that these\nrepresentations for the logs are robust and less invariant to changes in the\nlogs, and therefore, result in a better generalization of the anomaly detection\nmodels. We perform several experiments on a cloud dataset evaluating different\nlanguage models for obtaining numerical log representations such as BERT,\nGPT-2, and XL. The robustness is evaluated by gradually altering log messages,\nto simulate a change in semantics. Our results show that the proposed approach\nachieves high performance and robustness, which opens up possibilities for\nfuture research in this direction.", "published": "2021-02-23 09:17:05", "link": "http://arxiv.org/abs/2102.11570v1", "categories": ["cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.AI"}
{"title": "Data Fusion for Audiovisual Speaker Localization: Extending Dynamic\n  Stream Weights to the Spatial Domain", "abstract": "Estimating the positions of multiple speakers can be helpful for tasks like\nautomatic speech recognition or speaker diarization. Both applications benefit\nfrom a known speaker position when, for instance, applying beamforming or\nassigning unique speaker identities. Recently, several approaches utilizing\nacoustic signals augmented with visual data have been proposed for this task.\nHowever, both the acoustic and the visual modality may be corrupted in specific\nspatial regions, for instance due to poor lighting conditions or to the\npresence of background noise. This paper proposes a novel audiovisual data\nfusion framework for speaker localization by assigning individual dynamic\nstream weights to specific regions in the localization space. This fusion is\nachieved via a neural network, which combines the predictions of individual\naudio and video trackers based on their time- and location-dependent\nreliability. A performance evaluation using audiovisual recordings yields\npromising results, with the proposed fusion approach outperforming all baseline\nmodels.", "published": "2021-02-23 09:59:31", "link": "http://arxiv.org/abs/2102.11588v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.CV", "cs.LG", "eess.AS", "eess.IV"], "primary_category": "cs.SD"}
{"title": "Investigating Local and Global Information for Automated Audio\n  Captioning with Transfer Learning", "abstract": "Automated audio captioning (AAC) aims at generating summarizing descriptions\nfor audio clips. Multitudinous concepts are described in an audio caption,\nranging from local information such as sound events to global information like\nacoustic scenery. Currently, the mainstream paradigm for AAC is the end-to-end\nencoder-decoder architecture, expecting the encoder to learn all levels of\nconcepts embedded in the audio automatically. This paper first proposes a topic\nmodel for audio descriptions, comprehensively analyzing the hierarchical audio\ntopics that are commonly covered. We then explore a transfer learning scheme to\naccess local and global information. Two source tasks are identified to\nrespectively represent local and global information, being Audio Tagging (AT)\nand Acoustic Scene Classification (ASC). Experiments are conducted on the AAC\nbenchmark dataset Clotho and Audiocaps, amounting to a vast increase in all\neight metrics with topic transfer learning. Further, it is discovered that\nlocal information and abstract representation learning are more crucial to AAC\nthan global information and temporal relationship learning.", "published": "2021-02-23 02:09:49", "link": "http://arxiv.org/abs/2102.11457v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Text-to-Audio Grounding: Building Correspondence Between Captions and\n  Sound Events", "abstract": "Automated Audio Captioning is a cross-modal task, generating natural language\ndescriptions to summarize the audio clips' sound events. However, grounding the\nactual sound events in the given audio based on its corresponding caption has\nnot been investigated. This paper contributes an AudioGrounding dataset, which\nprovides the correspondence between sound events and the captions provided in\nAudiocaps, along with the location (timestamps) of each present sound event.\nBased on such, we propose the text-to-audio grounding (TAG) task, which\ninteractively considers the relationship between audio processing and language\nunderstanding. A baseline approach is provided, resulting in an event-F1 score\nof 28.3% and a Polyphonic Sound Detection Score (PSDS) score of 14.7%.", "published": "2021-02-23 03:44:14", "link": "http://arxiv.org/abs/2102.11474v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-to-End Dereverberation, Beamforming, and Speech Recognition with\n  Improved Numerical Stability and Advanced Frontend", "abstract": "Recently, the end-to-end approach has been successfully applied to\nmulti-speaker speech separation and recognition in both single-channel and\nmultichannel conditions. However, severe performance degradation is still\nobserved in the reverberant and noisy scenarios, and there is still a large\nperformance gap between anechoic and reverberant conditions. In this work, we\nfocus on the multichannel multi-speaker reverberant condition, and propose to\nextend our previous framework for end-to-end dereverberation, beamforming, and\nspeech recognition with improved numerical stability and advanced frontend\nsubnetworks including voice activity detection like masks. The techniques\nsignificantly stabilize the end-to-end training process. The experiments on the\nspatialized wsj1-2mix corpus show that the proposed system achieves about 35%\nWER relative reduction compared to our conventional multi-channel E2E ASR\nsystem, and also obtains decent speech dereverberation and separation\nperformance (SDR=12.5 dB) in the reverberant multi-speaker condition while\ntrained only with the ASR criterion.", "published": "2021-02-23 07:16:02", "link": "http://arxiv.org/abs/2102.11525v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unidirectional Memory-Self-Attention Transducer for Online Speech\n  Recognition", "abstract": "Self-attention models have been successfully applied in end-to-end speech\nrecognition systems, which greatly improve the performance of recognition\naccuracy. However, such attention-based models cannot be used in online speech\nrecognition, because these models usually have to utilize a whole acoustic\nsequences as inputs. A common method is restricting the field of attention\nsights by a fixed left and right window, which makes the computation costs\nmanageable yet also introduces performance degradation. In this paper, we\npropose Memory-Self-Attention (MSA), which adds history information into the\nRestricted-Self-Attention unit. MSA only needs localtime features as inputs,\nand efficiently models long temporal contexts by attending memory states.\nMeanwhile, recurrent neural network transducer (RNN-T) has proved to be a great\napproach for online ASR tasks, because the alignments of RNN-T are local and\nmonotonic. We propose a novel network structure, called Memory-Self-Attention\n(MSA) Transducer. Both encoder and decoder of the MSA Transducer contain the\nproposed MSA unit. The experiments demonstrate that our proposed models improve\nWER results than Restricted-Self-Attention models by $13.5 on WSJ and $7.1 on\nSWBD datasets relatively, and without much computation costs increase.", "published": "2021-02-23 10:14:30", "link": "http://arxiv.org/abs/2102.11594v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dual-Path Modeling for Long Recording Speech Separation in Meetings", "abstract": "The continuous speech separation (CSS) is a task to separate the speech\nsources from a long, partially overlapped recording, which involves a varying\nnumber of speakers. A straightforward extension of conventional utterance-level\nspeech separation to the CSS task is to segment the long recording with a\nsize-fixed window and process each window separately. Though effective, this\nextension fails to model the long dependency in speech and thus leads to\nsub-optimum performance. The recent proposed dual-path modeling could be a\nremedy to this problem, thanks to its capability in jointly modeling the\ncross-window dependency and the local-window processing. In this work, we\nfurther extend the dual-path modeling framework for CSS task. A\ntransformer-based dual-path system is proposed, which integrates transform\nlayers for global modeling. The proposed models are applied to LibriCSS, a real\nrecorded multi-talk dataset, and consistent WER reduction can be observed in\nthe ASR evaluation for separated speech. Also, a dual-path transformer equipped\nwith convolutional layers is proposed. It significantly reduces the computation\namount by 30% with better WER evaluation. Furthermore, the online processing\ndual-path models are investigated, which shows 10% relative WER reduction\ncompared to the baseline.", "published": "2021-02-23 11:23:45", "link": "http://arxiv.org/abs/2102.11634v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Handling Background Noise in Neural Speech Generation", "abstract": "Recent advances in neural-network based generative modeling of speech has\nshown great potential for speech coding. However, the performance of such\nmodels drops when the input is not clean speech, e.g., in the presence of\nbackground noise, preventing its use in practical applications. In this paper\nwe examine the reason and discuss methods to overcome this issue. Placing a\ndenoising preprocessing stage when extracting features and target clean speech\nduring training is shown to be the best performing strategy.", "published": "2021-02-23 19:36:05", "link": "http://arxiv.org/abs/2102.11906v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Senone-aware Adversarial Multi-task Training for Unsupervised Child to\n  Adult Speech Adaptation", "abstract": "Acoustic modeling for child speech is challenging due to the high acoustic\nvariability caused by physiological differences in the vocal tract. The dearth\nof publicly available datasets makes the task more challenging. In this work,\nwe propose a feature adaptation approach by exploiting adversarial multi-task\ntraining to minimize acoustic mismatch at the senone (tied triphone states)\nlevel between adult and child speech and leverage large amounts of transcribed\nadult speech. We validate the proposed method on three tasks: child speech\nrecognition, child pronunciation assessment, and child fluency score\nprediction. Empirical results indicate that our proposed approach consistently\noutperforms competitive baselines, achieving 7.7% relative error reduction on\nspeech recognition and up to 25.2% relative gains on the evaluation tasks.", "published": "2021-02-23 04:49:27", "link": "http://arxiv.org/abs/2102.11488v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Deep Learning Sound Events Classifiers using Gram Matrix\n  Feature-wise Correlations", "abstract": "In this paper, we propose a new Sound Event Classification (SEC) method which\nis inspired in recent works for out-of-distribution detection. In our method,\nwe analyse all the activations of a generic CNN in order to produce feature\nrepresentations using Gram Matrices. The similarity metrics are evaluated\nconsidering all possible classes, and the final prediction is defined as the\nclass that minimizes the deviation with respect to the features seeing during\ntraining. The proposed approach can be applied to any CNN and our experimental\nevaluation of four different architectures on two datasets demonstrated that\nour method consistently improves the baseline models.", "published": "2021-02-23 16:08:02", "link": "http://arxiv.org/abs/2102.11771v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
