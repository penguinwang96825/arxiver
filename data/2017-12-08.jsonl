{"title": "Sequence to Sequence Networks for Roman-Urdu to Urdu Transliteration", "abstract": "Neural Machine Translation models have replaced the conventional phrase based\nstatistical translation methods since the former takes a generic, scalable,\ndata-driven approach rather than relying on manual, hand-crafted features. The\nneural machine translation system is based on one neural network that is\ncomposed of two parts, one that is responsible for input language sentence and\nother part that handles the desired output language sentence. This model based\non encoder-decoder architecture also takes as input the distributed\nrepresentations of the source language which enriches the learnt dependencies\nand gives a warm start to the network. In this work, we transform Roman-Urdu to\nUrdu transliteration into sequence to sequence learning problem. To this end,\nwe make the following contributions. We create the first ever parallel corpora\nof Roman-Urdu to Urdu, create the first ever distributed representation of\nRoman-Urdu and present the first neural machine translation model that\ntransliterates text from Roman-Urdu to Urdu language. Our model has achieved\nthe state-of-the-art results using BLEU as the evaluation metric. Precisely,\nour model is able to correctly predict sentences up to length 10 while\nachieving BLEU score of 48.6 on the test set. We are hopeful that our model and\nour results shall serve as the baseline for further work in the domain of\nneural machine translation for Roman-Urdu to Urdu using distributed\nrepresentation.", "published": "2017-12-08 06:36:54", "link": "http://arxiv.org/abs/1712.02959v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Characterizing the hyper-parameter space of LSTM language models for\n  mixed context applications", "abstract": "Applying state of the art deep learning models to novel real world datasets\ngives a practical evaluation of the generalizability of these models. Of\nimportance in this process is how sensitive the hyper parameters of such models\nare to novel datasets as this would affect the reproducibility of a model. We\npresent work to characterize the hyper parameter space of an LSTM for language\nmodeling on a code-mixed corpus. We observe that the evaluated model shows\nminimal sensitivity to our novel dataset bar a few hyper parameters.", "published": "2017-12-08 17:52:32", "link": "http://arxiv.org/abs/1712.03199v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Audio-Visual Sentiment Analysis for Learning Emotional Arcs in Movies", "abstract": "Stories can have tremendous power -- not only useful for entertainment, they\ncan activate our interests and mobilize our actions. The degree to which a\nstory resonates with its audience may be in part reflected in the emotional\njourney it takes the audience upon. In this paper, we use machine learning\nmethods to construct emotional arcs in movies, calculate families of arcs, and\ndemonstrate the ability for certain arcs to predict audience engagement. The\nsystem is applied to Hollywood films and high quality shorts found on the web.\nWe begin by using deep convolutional neural networks for audio and visual\nsentiment analysis. These models are trained on both new and existing\nlarge-scale datasets, after which they can be used to compute separate audio\nand visual emotional arcs. We then crowdsource annotations for 30-second video\nclips extracted from highs and lows in the arcs in order to assess the\nmicro-level precision of the system, with precision measured in terms of\nagreement in polarity between the system's predictions and annotators' ratings.\nThese annotations are also used to combine the audio and visual predictions.\nNext, we look at macro-level characterizations of movies by investigating\nwhether there exist `universal shapes' of emotional arcs. In particular, we\ndevelop a clustering approach to discover distinct classes of emotional arcs.\nFinally, we show on a sample corpus of short web videos that certain emotional\narcs are statistically significant predictors of the number of comments a video\nreceives. These results suggest that the emotional arcs learned by our approach\nsuccessfully represent macroscopic aspects of a video story that drive audience\nengagement. Such machine understanding could be used to predict audience\nreactions to video stories, ultimately improving our ability as storytellers to\ncommunicate with each other.", "published": "2017-12-08 00:27:08", "link": "http://arxiv.org/abs/1712.02896v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Building competitive direct acoustics-to-word models for English\n  conversational speech recognition", "abstract": "Direct acoustics-to-word (A2W) models in the end-to-end paradigm have\nreceived increasing attention compared to conventional sub-word based automatic\nspeech recognition models using phones, characters, or context-dependent hidden\nMarkov model states. This is because A2W models recognize words from speech\nwithout any decoder, pronunciation lexicon, or externally-trained language\nmodel, making training and decoding with such models simple. Prior work has\nshown that A2W models require orders of magnitude more training data in order\nto perform comparably to conventional models. Our work also showed this\naccuracy gap when using the English Switchboard-Fisher data set. This paper\ndescribes a recipe to train an A2W model that closes this gap and is at-par\nwith state-of-the-art sub-word based models. We achieve a word error rate of\n8.8%/13.9% on the Hub5-2000 Switchboard/CallHome test sets without any decoder\nor language model. We find that model initialization, training data order, and\nregularization have the most impact on the A2W model performance. Next, we\npresent a joint word-character A2W model that learns to first spell the word\nand then recognize it. This model provides a rich output to the user instead of\nsimple word hypotheses, making it especially useful in the case of words unseen\nor rarely-seen during training.", "published": "2017-12-08 15:43:21", "link": "http://arxiv.org/abs/1712.03133v1", "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Social Emotion Mining Techniques for Facebook Posts Reaction Prediction", "abstract": "As of February 2016 Facebook allows users to express their experienced\nemotions about a post by using five so-called `reactions'. This research paper\nproposes and evaluates alternative methods for predicting these reactions to\nuser posts on public pages of firms/companies (like supermarket chains). For\nthis purpose, we collected posts (and their reactions) from Facebook pages of\nlarge supermarket chains and constructed a dataset which is available for other\nresearches. In order to predict the distribution of reactions of a new post,\nneural network architectures (convolutional and recurrent neural networks) were\ntested using pretrained word embeddings. Results of the neural networks were\nimproved by introducing a bootstrapping approach for sentiment and emotion\nmining on the comments for each post. The final model (a combination of neural\nnetwork and a baseline emotion miner) is able to predict the reaction\ndistribution on Facebook posts with a mean squared error (or misclassification\nrate) of 0.135.", "published": "2017-12-08 19:05:50", "link": "http://arxiv.org/abs/1712.03249v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Representations of Sound in Deep Learning of Audio Features from Music", "abstract": "The work of a single musician, group or composer can vary widely in terms of\nmusical style. Indeed, different stylistic elements, from performance medium\nand rhythm to harmony and texture, are typically exploited and developed across\nan artist's lifetime. Yet, there is often a discernable character to the work\nof, for instance, individual composers at the perceptual level - an experienced\nlistener can often pick up on subtle clues in the music to identify the\ncomposer or performer. Here we suggest that a convolutional network may learn\nthese subtle clues or features given an appropriate representation of the\nmusic. In this paper, we apply a deep convolutional neural network to a large\naudio dataset and empirically evaluate its performance on audio classification\ntasks. Our trained network demonstrates accurate performance on such\nclassification tasks when presented with 5 s examples of music obtained by\nsimple transformations of the raw audio waveform. A particularly interesting\nexample is the spectral representation of music obtained by application of a\nlogarithmically spaced filter bank, mirroring the early stages of auditory\nsignal transduction in mammals. The most successful representation of music to\nfacilitate discrimination was obtained via a random matrix transform (RMT).\nNetworks based on logarithmic filter banks and RMT were able to correctly guess\nthe one composer out of 31 possibilities in 68 and 84 percent of cases\nrespectively.", "published": "2017-12-08 00:37:23", "link": "http://arxiv.org/abs/1712.02898v1", "categories": ["cs.SD", "cs.CV", "eess.AS", "q-bio.NC"], "primary_category": "cs.SD"}
{"title": "Music Transcription by Deep Learning with Data and \"Artificial Semantic\"\n  Augmentation", "abstract": "In this progress paper the previous results of the single note recognition by\ndeep learning are presented. The several ways for data augmentation and\n\"artificial semantic\" augmentation are proposed to enhance efficiency of deep\nlearning approaches for monophonic and polyphonic note recognition by increase\nof dimensions of training data, their lossless and lossy transformations.", "published": "2017-12-08 11:18:22", "link": "http://arxiv.org/abs/1712.03228v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
