{"title": "Conceptual and Unbiased Reasoning in Language Models", "abstract": "Conceptual reasoning, the ability to reason in abstract and high-level\nperspectives, is key to generalization in human cognition. However, limited\nstudy has been done on large language models' capability to perform conceptual\nreasoning. In this work, we bridge this gap and propose a novel\nconceptualization framework that forces models to perform conceptual reasoning\non abstract questions and generate solutions in a verifiable symbolic space.\nUsing this framework as an analytical tool, we show that existing large\nlanguage models fall short on conceptual reasoning, dropping 9% to 28% on\nvarious benchmarks compared to direct inference methods. We then discuss how\nmodels can improve since high-level abstract reasoning is key to unbiased and\ngeneralizable decision-making. We propose two techniques to add trustworthy\ninduction signals by generating familiar questions with similar underlying\nreasoning paths and asking models to perform self-refinement. Experiments show\nthat our proposed techniques improve models' conceptual reasoning performance\nby 8% to 11%, achieving a more robust reasoning system that relies less on\ninductive biases.", "published": "2024-03-30 00:53:53", "link": "http://arxiv.org/abs/2404.00205v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EventGround: Narrative Reasoning by Grounding to Eventuality-centric\n  Knowledge Graphs", "abstract": "Narrative reasoning relies on the understanding of eventualities in story\ncontexts, which requires a wealth of background world knowledge. To help\nmachines leverage such knowledge, existing solutions can be categorized into\ntwo groups. Some focus on implicitly modeling eventuality knowledge by\npretraining language models (LMs) with eventuality-aware objectives. However,\nthis approach breaks down knowledge structures and lacks interpretability.\nOthers explicitly collect world knowledge of eventualities into structured\neventuality-centric knowledge graphs (KGs). However, existing research on\nleveraging these knowledge sources for free-texts is limited. In this work, we\npropose an initial comprehensive framework called EventGround, which aims to\ntackle the problem of grounding free-texts to eventuality-centric KGs for\ncontextualized narrative reasoning. We identify two critical problems in this\ndirection: the event representation and sparsity problems. We provide simple\nyet effective parsing and partial information extraction methods to tackle\nthese problems. Experimental results demonstrate that our approach consistently\noutperforms baseline models when combined with graph neural network (GNN) or\nlarge language model (LLM) based graph reasoning models. Our framework,\nincorporating grounded knowledge, achieves state-of-the-art performance while\nproviding interpretable evidence.", "published": "2024-03-30 01:16:37", "link": "http://arxiv.org/abs/2404.00209v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Injecting New Knowledge into Large Language Models via Supervised\n  Fine-Tuning", "abstract": "In recent years, Large Language Models (LLMs) have shown remarkable\nperformance in generating human-like text, proving to be a valuable asset\nacross various applications. However, adapting these models to incorporate new,\nout-of-domain knowledge remains a challenge, particularly for facts and events\nthat occur after the model's knowledge cutoff date. This paper investigates the\neffectiveness of Supervised Fine-Tuning (SFT) as a method for knowledge\ninjection in LLMs, specifically focusing on the domain of recent sporting\nevents. We compare different dataset generation strategies -- token-based and\nfact-based scaling -- to create training data that helps the model learn new\ninformation. Our experiments on GPT-4 demonstrate that while token-based\nscaling can lead to improvements in Q&A accuracy, it may not provide uniform\ncoverage of new knowledge. Fact-based scaling, on the other hand, offers a more\nsystematic approach to ensure even coverage across all facts. We present a\nnovel dataset generation process that leads to more effective knowledge\ningestion through SFT, and our results show considerable performance\nimprovements in Q&A tasks related to out-of-domain knowledge. This study\ncontributes to the understanding of domain adaptation for LLMs and highlights\nthe potential of SFT in enhancing the factuality of LLM responses in specific\nknowledge domains.", "published": "2024-03-30 01:56:07", "link": "http://arxiv.org/abs/2404.00213v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rationale-based Opinion Summarization", "abstract": "Opinion summarization aims to generate concise summaries that present popular\nopinions of a large group of reviews. However, these summaries can be too\ngeneric and lack supporting details. To address these issues, we propose a new\nparadigm for summarizing reviews, rationale-based opinion summarization.\nRationale-based opinion summaries output the representative opinions as well as\none or more corresponding rationales. To extract good rationales, we define\nfour desirable properties: relatedness, specificity, popularity, and diversity\nand present a Gibbs-sampling-based method to extract rationales. Overall, we\npropose RATION, an unsupervised extractive system that has two components: an\nOpinion Extractor (to extract representative opinions) and Rationales Extractor\n(to extract corresponding rationales). We conduct automatic and human\nevaluations to show that rationales extracted by RATION have the proposed\nproperties and its summaries are more useful than conventional summaries. The\nimplementation of our work is available at\nhttps://github.com/leehaoyuan/RATION.", "published": "2024-03-30 02:22:57", "link": "http://arxiv.org/abs/2404.00217v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classification and Clustering of Sentence-Level Embeddings of Scientific\n  Articles Generated by Contrastive Learning", "abstract": "Scientific articles are long text documents organized into sections, each\ndescribing aspects of the research. Analyzing scientific production has become\nprogressively challenging due to the increase in the number of available\narticles. Within this scenario, our approach consisted of fine-tuning\ntransformer language models to generate sentence-level embeddings from\nscientific articles, considering the following labels: background, objective,\nmethods, results, and conclusion. We trained our models on three datasets with\ncontrastive learning. Two datasets are from the article's abstracts in the\ncomputer science and medical domains. Also, we introduce PMC-Sents-FULL, a\nnovel dataset of sentences extracted from the full texts of medical articles.\nWe compare the fine-tuned and baseline models in clustering and classification\ntasks to evaluate our approach. On average, clustering agreement measures\nvalues were five times higher. For the classification measures, in the\nbest-case scenario, we had an average improvement in F1-micro of 30.73\\%.\nResults show that fine-tuning sentence transformers with contrastive learning\nand using the generated embeddings in downstream tasks is a feasible approach\nto sentence classification in scientific articles. Our experiment codes are\navailable on GitHub.", "published": "2024-03-30 02:52:14", "link": "http://arxiv.org/abs/2404.00224v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal\n  Traits", "abstract": "Prior research has established associations between individuals' language\nusage and their personal traits; our linguistic patterns reveal information\nabout our personalities, emotional states, and beliefs. However, with the\nincreasing adoption of Large Language Models (LLMs) as writing assistants in\neveryday writing, a critical question emerges: are authors' linguistic patterns\nstill predictive of their personal traits when LLMs are involved in the writing\nprocess? We investigate the impact of LLMs on the linguistic markers of\ndemographic and psychological traits, specifically examining three LLMs -\nGPT3.5, Llama 2, and Gemini - across six different traits: gender, age,\npolitical affiliation, personality, empathy, and morality. Our findings\nindicate that although the use of LLMs slightly reduces the predictive power of\nlinguistic patterns over authors' personal traits, the significant changes are\ninfrequent, and the use of LLMs does not fully diminish the predictive power of\nauthors' linguistic patterns over their personal traits. We also note that some\ntheoretically established lexical-based linguistic markers lose their\nreliability as predictors when LLMs are used in the writing process. Our\nfindings have important implications for the study of linguistic markers of\npersonal traits in the age of LLMs.", "published": "2024-03-30 06:49:17", "link": "http://arxiv.org/abs/2404.00267v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Likelihood Ratio Test of Genetic Relationship among Languages", "abstract": "Lexical resemblances among a group of languages indicate that the languages\ncould be genetically related, i.e., they could have descended from a common\nancestral language. However, such resemblances can arise by chance and, hence,\nneed not always imply an underlying genetic relationship. Many tests of\nsignificance based on permutation of wordlists and word similarity measures\nappeared in the past to determine the statistical significance of such\nrelationships. We demonstrate that although existing tests may work well for\nbilateral comparisons, i.e., on pairs of languages, they are either infeasible\nby design or are prone to yield false positives when applied to groups of\nlanguages or language families. To this end, inspired by molecular\nphylogenetics, we propose a likelihood ratio test to determine if given\nlanguages are related based on the proportion of invariant character sites in\nthe aligned wordlists applied during tree inference. Further, we evaluate some\nlanguage families and show that the proposed test solves the problem of false\npositives. Finally, we demonstrate that the test supports the existence of\nmacro language families such as Nostratic and Macro-Mayan.", "published": "2024-03-30 08:35:08", "link": "http://arxiv.org/abs/2404.00284v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Study on NLP Data Augmentation for Hate Speech\n  Detection: Legacy Methods, BERT, and LLMs", "abstract": "The surge of interest in data augmentation within the realm of NLP has been\ndriven by the need to address challenges posed by hate speech domains, the\ndynamic nature of social media vocabulary, and the demands for large-scale\nneural networks requiring extensive training data. However, the prevalent use\nof lexical substitution in data augmentation has raised concerns, as it may\ninadvertently alter the intended meaning, thereby impacting the efficacy of\nsupervised machine learning models. In pursuit of suitable data augmentation\nmethods, this study explores both established legacy approaches and\ncontemporary practices such as Large Language Models (LLM), including GPT in\nHate Speech detection. Additionally, we propose an optimized utilization of\nBERT-based encoder models with contextual cosine similarity filtration,\nexposing significant limitations in prior synonym substitution methods. Our\ncomparative analysis encompasses five popular augmentation techniques: WordNet\nand Fast-Text synonym replacement, Back-translation, BERT-mask contextual\naugmentation, and LLM. Our analysis across five benchmarked datasets revealed\nthat while traditional methods like back-translation show low label alteration\nrates (0.3-1.5%), and BERT-based contextual synonym replacement offers sentence\ndiversity but at the cost of higher label alteration rates (over 6%). Our\nproposed BERT-based contextual cosine similarity filtration markedly reduced\nlabel alteration to just 0.05%, demonstrating its efficacy in 0.7% higher F1\nperformance. However, augmenting data with GPT-3 not only avoided overfitting\nwith up to sevenfold data increase but also improved embedding space coverage\nby 15% and classification F1 score by 1.4% over traditional methods, and by\n0.8% over our method.", "published": "2024-03-30 09:55:58", "link": "http://arxiv.org/abs/2404.00303v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controllable and Diverse Data Augmentation with Large Language Model for\n  Low-Resource Open-Domain Dialogue Generation", "abstract": "Data augmentation (DA) is crucial to mitigate model training instability and\nover-fitting problems in low-resource open-domain dialogue generation. However,\ntraditional DA methods often neglect semantic data diversity, restricting the\noverall quality. Recently, large language models (LLM) have been used for DA to\ngenerate diversified dialogues. However, they have limited controllability and\ntend to generate dialogues with a distribution shift compared to the seed\ndialogues. To maximize the augmentation diversity and address the\ncontrollability problem, we propose \\textbf{S}ummary-based \\textbf{D}ialogue\n\\textbf{A}ugmentation with LLM (SDA). Our approach enhances the controllability\nof LLM by using dialogue summaries as a planning tool. Based on summaries, SDA\ncan generate high-quality and diverse dialogue data even with a small seed\ndataset. To evaluate the efficacy of data augmentation methods for open-domain\ndialogue, we designed a clustering-based metric to characterize the semantic\ndiversity of the augmented dialogue data. The experimental results show that\nSDA can augment high-quality and semantically diverse dialogues given a small\nseed dataset and an LLM, and the augmented data can boost the performance of\nopen-domain dialogue models.", "published": "2024-03-30 13:28:51", "link": "http://arxiv.org/abs/2404.00361v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Small Language Models Learn Enhanced Reasoning Skills from Medical\n  Textbooks", "abstract": "While recent advancements in commercial large language models (LM) have shown\npromising results in medical tasks, their closed-source nature poses\nsignificant privacy and security concerns, hindering their widespread use in\nthe medical field. Despite efforts to create open-source models, their limited\nparameters often result in insufficient multi-step reasoning capabilities\nrequired for solving complex medical problems. To address this, we introduce\nMeerkat, a new family of medical AI systems ranging from 7 to 70 billion\nparameters. The models were trained using our new synthetic dataset consisting\nof high-quality chain-of-thought reasoning paths sourced from 18 medical\ntextbooks, along with diverse instruction-following datasets. Our systems\nachieved remarkable accuracy across six medical benchmarks, surpassing the\nprevious best models such as MediTron and BioMistral, and GPT-3.5 by a large\nmargin. Notably, Meerkat-7B surpassed the passing threshold of the United\nStates Medical Licensing Examination (USMLE) for the first time for a\n7B-parameter model, while Meerkat-70B outperformed GPT-4 by an average of 1.3%.\nAdditionally, Meerkat-70B correctly diagnosed 21 out of 38 complex clinical\ncases, outperforming humans' 13.8 and closely matching GPT-4's 21.8. Our\nsystems offered more detailed free-form responses to clinical queries compared\nto existing small models, approaching the performance level of large commercial\nmodels. This significantly narrows the performance gap with large LMs,\nshowcasing its effectiveness in addressing complex medical challenges.", "published": "2024-03-30 14:09:00", "link": "http://arxiv.org/abs/2404.00376v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jetsons at FinNLP 2024: Towards Understanding the ESG Impact of a News\n  Article using Transformer-based Models", "abstract": "In this paper, we describe the different approaches explored by the Jetsons\nteam for the Multi-Lingual ESG Impact Duration Inference (ML-ESG-3) shared\ntask. The shared task focuses on predicting the duration and type of the ESG\nimpact of a news article. The shared task dataset consists of 2,059 news titles\nand articles in English, French, Korean, and Japanese languages. For the impact\nduration classification task, we fine-tuned XLM-RoBERTa with a custom\nfine-tuning strategy and using self-training and DeBERTa-v3 using only English\ntranslations. These models individually ranked first on the leaderboard for\nKorean and Japanese and in an ensemble for the English language, respectively.\nFor the impact type classification task, our XLM-RoBERTa model fine-tuned using\na custom fine-tuning strategy ranked first for the English language.", "published": "2024-03-30 14:58:44", "link": "http://arxiv.org/abs/2404.00386v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Analysis of BPE Vocabulary Trimming in Neural Machine Translation", "abstract": "We explore threshold vocabulary trimming in Byte-Pair Encoding subword\ntokenization, a postprocessing step that replaces rare subwords with their\ncomponent subwords. The technique is available in popular tokenization\nlibraries but has not been subjected to rigorous scientific scrutiny. While the\nremoval of rare subwords is suggested as best practice in machine translation\nimplementations, both as a means to reduce model size and for improving model\nperformance through robustness, our experiments indicate that, across a large\nspace of hyperparameter settings, vocabulary trimming fails to improve\nperformance, and is even prone to incurring heavy degradation.", "published": "2024-03-30 15:29:49", "link": "http://arxiv.org/abs/2404.00397v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Robust are the Tabular QA Models for Scientific Tables? A Study\n  using Customized Dataset", "abstract": "Question-answering (QA) on hybrid scientific tabular and textual data deals\nwith scientific information, and relies on complex numerical reasoning. In\nrecent years, while tabular QA has seen rapid progress, understanding their\nrobustness on scientific information is lacking due to absence of any benchmark\ndataset. To investigate the robustness of the existing state-of-the-art QA\nmodels on scientific hybrid tabular data, we propose a new dataset, \"SciTabQA\",\nconsisting of 822 question-answer pairs from scientific tables and their\ndescriptions. With the help of this dataset, we assess the state-of-the-art\nTabular QA models based on their ability (i) to use heterogeneous information\nrequiring both structured data (table) and unstructured data (text) and (ii) to\nperform complex scientific reasoning tasks. In essence, we check the capability\nof the models to interpret scientific tables and text. Our experiments show\nthat \"SciTabQA\" is an innovative dataset to study question-answering over\nscientific heterogeneous data. We benchmark three state-of-the-art Tabular QA\nmodels, and find that the best F1 score is only 0.462.", "published": "2024-03-30 15:48:49", "link": "http://arxiv.org/abs/2404.00401v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion\n  Cause", "abstract": "Multimodal emotion recognition in conversation (MERC) and multimodal\nemotion-cause pair extraction (MECPE) have recently garnered significant\nattention. Emotions are the expression of affect or feelings; responses to\nspecific events, or situations -- known as emotion causes. Both collectively\nexplain the causality between human emotion and intents. However, existing\nworks treat emotion recognition and emotion cause extraction as two individual\nproblems, ignoring their natural causality. In this paper, we propose a Unified\nMultimodal Emotion recognition and Emotion-Cause analysis framework (UniMEEC)\nto explore the causality between emotion and emotion cause. Concretely, UniMEEC\nreformulates the MERC and MECPE tasks as mask prediction problems and unifies\nthem with a causal prompt template. To differentiate the modal effects, UniMEEC\nproposes a multimodal causal prompt to probe the pre-trained knowledge\nspecified to modality and implements cross-task and cross-modality interactions\nunder task-oriented settings. Experiment results on four public benchmark\ndatasets verify the model performance on MERC and MECPE tasks and achieve\nconsistent improvements compared with the previous state-of-the-art methods.", "published": "2024-03-30 15:59:17", "link": "http://arxiv.org/abs/2404.00403v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoDa: Constrained Generation based Data Augmentation for Low-Resource\n  NLP", "abstract": "We present CoDa (Constrained Generation based Data Augmentation), a\ncontrollable, effective, and training-free data augmentation technique for\nlow-resource (data-scarce) NLP. Our approach is based on prompting\noff-the-shelf instruction-following Large Language Models (LLMs) for generating\ntext that satisfies a set of constraints. Precisely, we extract a set of simple\nconstraints from every instance in the low-resource dataset and verbalize them\nto prompt an LLM to generate novel and diverse training instances. Our findings\nreveal that synthetic data that follows simple constraints in the downstream\ndataset act as highly effective augmentations, and CoDa can achieve this\nwithout intricate decoding-time constrained generation techniques or\nfine-tuning with complex algorithms that eventually make the model biased\ntoward the small number of training instances. Additionally, CoDa is the first\nframework that provides users explicit control over the augmentation generation\nprocess, thereby also allowing easy adaptation to several domains. We\ndemonstrate the effectiveness of CoDa across 11 datasets spanning 3 tasks and 3\nlow-resource settings. CoDa outperforms all our baselines, qualitatively and\nquantitatively, with improvements of 0.12%-7.19%. Code is available here:\nhttps://github.com/Sreyan88/CoDa", "published": "2024-03-30 16:47:06", "link": "http://arxiv.org/abs/2404.00415v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DOCMASTER: A Unified Platform for Annotation, Training, & Inference in\n  Document Question-Answering", "abstract": "The application of natural language processing models to PDF documents is\npivotal for various business applications yet the challenge of training models\nfor this purpose persists in businesses due to specific hurdles. These include\nthe complexity of working with PDF formats that necessitate parsing text and\nlayout information for curating training data and the lack of\nprivacy-preserving annotation tools. This paper introduces DOCMASTER, a unified\nplatform designed for annotating PDF documents, model training, and inference,\ntailored to document question-answering. The annotation interface enables users\nto input questions and highlight text spans within the PDF file as answers,\nsaving layout information and text spans accordingly. Furthermore, DOCMASTER\nsupports both state-of-the-art layout-aware and text models for comprehensive\ntraining purposes. Importantly, as annotations, training, and inference occur\non-device, it also safeguards privacy. The platform has been instrumental in\ndriving several research prototypes concerning document analysis such as the AI\nassistant utilized by University of California San Diego's (UCSD) International\nServices and Engagement Office (ISEO) for processing a substantial volume of\nPDF documents.", "published": "2024-03-30 18:11:39", "link": "http://arxiv.org/abs/2404.00439v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MetaIE: Distilling a Meta Model from LLM for All Kinds of Information\n  Extraction Tasks", "abstract": "Information extraction (IE) is a fundamental area in natural language\nprocessing where prompting large language models (LLMs), even with in-context\nexamples, cannot defeat small LMs tuned on very small IE datasets. We observe\nthat IE tasks, such as named entity recognition and relation extraction, all\nfocus on extracting important information, which can be formalized as a\nlabel-to-span matching. In this paper, we propose a novel framework MetaIE to\nbuild a small LM as meta-model by learning to extract \"important information\",\ni.e., the meta-understanding of IE, so that this meta-model can be adapted to\nall kind of IE tasks effectively and efficiently. Specifically, MetaIE obtains\nthe small LM via a symbolic distillation from an LLM following the\nlabel-to-span scheme. We construct the distillation dataset via sampling\nsentences from language model pre-training datasets (e.g., OpenWebText in our\nimplementation) and prompting an LLM to identify the typed spans of \"important\ninformation\". We evaluate the meta-model under the few-shot adaptation setting.\nExtensive results on 13 datasets from 6 IE tasks confirm that MetaIE can offer\na better starting point for few-shot tuning on IE datasets and outperform other\nmeta-models from (1) vanilla language model pre-training, (2) multi-IE-task\npre-training with human annotations, and (3) single-IE-task symbolic\ndistillation from LLM. Moreover, we provide comprehensive analyses of MetaIE,\nsuch as the size of the distillation dataset, the meta-model architecture, and\nthe size of the meta-model.", "published": "2024-03-30 19:43:45", "link": "http://arxiv.org/abs/2404.00457v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning", "abstract": "Language models struggle with handling numerical data and performing\narithmetic operations. We hypothesize that this limitation can be partially\nattributed to non-intuitive textual numbers representation. When a digit is\nread or generated by a causal language model it does not know its place value\n(e.g. thousands vs. hundreds) until the entire number is processed. To address\nthis issue, we propose a simple adjustment to how numbers are represented by\nincluding the count of digits before each number. For instance, instead of\n\"42\", we suggest using \"{2:42}\" as the new format. This approach, which we term\nNumeroLogic, offers an added advantage in number generation by serving as a\nChain of Thought (CoT). By requiring the model to consider the number of digits\nfirst, it enhances the reasoning process before generating the actual number.\nWe use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic\nformatting. We further demonstrate NumeroLogic applicability to general natural\nlanguage modeling, improving language understanding performance in the MMLU\nbenchmark.", "published": "2024-03-30 19:46:59", "link": "http://arxiv.org/abs/2404.00459v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model\n  unless you have access to GPT-4", "abstract": "The NLI4CT task assesses Natural Language Inference systems in predicting\nwhether hypotheses entail or contradict evidence from Clinical Trial Reports.\nIn this study, we evaluate various Large Language Models (LLMs) with multiple\nstrategies, including Chain-of-Thought, In-Context Learning, and\nParameter-Efficient Fine-Tuning (PEFT). We propose a PEFT method to improve the\nconsistency of LLMs by merging adapters that were fine-tuned separately using\ntriplet and language modelling objectives. We found that merging the two PEFT\nadapters improves the F1 score (+0.0346) and consistency (+0.152) of the LLMs.\nHowever, our novel methods did not produce more accurate results than GPT-4 in\nterms of faithfulness and consistency. Averaging the three metrics, GPT-4 ranks\njoint-first in the competition with 0.8328. Finally, our contamination analysis\nwith GPT-4 indicates that there was no test data leakage.", "published": "2024-03-30 22:27:21", "link": "http://arxiv.org/abs/2404.00484v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Conditional Ranking with Large Language Models", "abstract": "Utilizing large language models (LLMs) to rank a set of items has become a\ncommon approach in recommendation and retrieval systems. Typically, these\nsystems focus on ordering a substantial number of documents in a monotonic\norder based on a given query. However, real-world scenarios often present a\ndifferent challenge: ranking a comparatively smaller set of items, but\naccording to a variety of diverse and occasionally conflicting conditions. In\nthis paper, we define and explore the task of multi-conditional ranking by\nintroducing MCRank, a benchmark tailored for assessing multi-conditional\nranking across various item types and conditions. Our analysis of LLMs using\nMCRank indicates a significant decrease in performance as the number and\ncomplexity of items and conditions grow. To overcome this limitation, we\npropose a novel decomposed reasoning method, consisting of EXtracting and\nSorting the conditions, and then Iteratively Ranking the items (EXSIR). Our\nextensive experiments show that this decomposed reasoning method enhances LLMs'\nperformance significantly, achieving up to a 14.4% improvement over existing\nLLMs. We also provide a detailed analysis of LLMs performance across various\ncondition categories, and examine the effectiveness of decomposition step.\nFurthermore, we compare our method with existing approaches such as\nChain-of-Thought and existing ranking models, demonstrating the superiority of\nour approach and complexity of MCR task. We released our dataset and code.", "published": "2024-03-30 01:26:05", "link": "http://arxiv.org/abs/2404.00211v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is Factuality Enhancement a Free Lunch For LLMs? Better Factuality Can\n  Lead to Worse Context-Faithfulness", "abstract": "As the modern tools of choice for text understanding and generation, large\nlanguage models (LLMs) are expected to accurately output answers by leveraging\nthe input context. This requires LLMs to possess both context-faithfulness and\nfactual accuracy. Extensive efforts have been made to enable better outputs\nfrom LLMs by mitigating hallucinations through factuality enhancement methods.\nHowever, they also pose risks of hindering context-faithfulness, as factuality\nenhancement can lead LLMs to become overly confident in their parametric\nknowledge, causing them to overlook the relevant input context. In this work,\nwe argue that current factuality enhancement methods can significantly\nundermine the context-faithfulness of LLMs. We first revisit the current\nfactuality enhancement methods and evaluate their effectiveness in enhancing\nfactual accuracy. Next, we evaluate their performance on knowledge editing\ntasks to assess the potential impact on context-faithfulness. The experimental\nresults reveal that while these methods may yield inconsistent improvements in\nfactual accuracy, they also cause a more severe decline in\ncontext-faithfulness, with the largest decrease reaching a striking 69.7\\%. To\nexplain these declines, we analyze the hidden states and logit distributions\nfor the tokens representing new knowledge and parametric knowledge\nrespectively, highlighting the limitations of current approaches. Our finding\nhighlights the complex trade-offs inherent in enhancing LLMs. Therefore, we\nrecommend that more research on LLMs' factuality enhancement make efforts to\nreduce the sacrifice of context-faithfulness.", "published": "2024-03-30 02:08:28", "link": "http://arxiv.org/abs/2404.00216v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Design as Desired: Utilizing Visual Question Answering for Multimodal\n  Pre-training", "abstract": "Multimodal pre-training demonstrates its potential in the medical domain,\nwhich learns medical visual representations from paired medical reports.\nHowever, many pre-training tasks require extra annotations from clinicians, and\nmost of them fail to explicitly guide the model to learn the desired features\nof different pathologies. In this paper, we utilize Visual Question Answering\n(VQA) for multimodal pre-training to guide the framework focusing on targeted\npathological features. We leverage descriptions in medical reports to design\nmulti-granular question-answer pairs associated with different diseases, which\nassist the framework in pre-training without requiring extra annotations from\nexperts. We also propose a novel pre-training framework with a quasi-textual\nfeature transformer, a module designed to transform visual features into a\nquasi-textual space closer to the textual domain via a contrastive learning\nstrategy. This narrows the vision-language gap and facilitates modality\nalignment. Our framework is applied to four downstream tasks: report\ngeneration, classification, segmentation, and detection across five datasets.\nExtensive experiments demonstrate the superiority of our framework compared to\nother state-of-the-art methods. Our code is available at\nhttps://github.com/MoramiSu/QFT-MICCAI2024.", "published": "2024-03-30 02:56:54", "link": "http://arxiv.org/abs/2404.00226v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Survey of using Large Language Models for Generating Infrastructure as\n  Code", "abstract": "Infrastructure as Code (IaC) is a revolutionary approach which has gained\nsignificant prominence in the Industry. IaC manages and provisions IT\ninfrastructure using machine-readable code by enabling automation, consistency\nacross the environments, reproducibility, version control, error reduction and\nenhancement in scalability. However, IaC orchestration is often a painstaking\neffort which requires specialised skills as well as a lot of manual effort.\nAutomation of IaC is a necessity in the present conditions of the Industry and\nin this survey, we study the feasibility of applying Large Language Models\n(LLM) to address this problem. LLMs are large neural network-based models which\nhave demonstrated significant language processing abilities and shown to be\ncapable of following a range of instructions within a broad scope. Recently,\nthey have also been adapted for code understanding and generation tasks\nsuccessfully, which makes them a promising choice for the automatic generation\nof IaC configurations. In this survey, we delve into the details of IaC, usage\nof IaC in different platforms, their challenges, LLMs in terms of\ncode-generation aspects and the importance of LLMs in IaC along with our own\nexperiments. Finally, we conclude by presenting the challenges in this area and\nhighlighting the scope for future research.", "published": "2024-03-30 02:57:55", "link": "http://arxiv.org/abs/2404.00227v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Enhancing Content-based Recommendation via Large Language Model", "abstract": "In real-world applications, users express different behaviors when they\ninteract with different items, including implicit click/like interactions, and\nexplicit comments/reviews interactions. Nevertheless, almost all recommender\nworks are focused on how to describe user preferences by the implicit\nclick/like interactions, to find the synergy of people. For the content-based\nexplicit comments/reviews interactions, some works attempt to utilize them to\nmine the semantic knowledge to enhance recommender models. However, they still\nneglect the following two points: (1) The content semantic is a universal world\nknowledge; how do we extract the multi-aspect semantic information to empower\ndifferent domains? (2) The user/item ID feature is a fundamental element for\nrecommender models; how do we align the ID and content semantic feature space?\nIn this paper, we propose a `plugin' semantic knowledge transferring method\n\\textbf{LoID}, which includes two major components: (1) LoRA-based large\nlanguage model pretraining to extract multi-aspect semantic information; (2)\nID-based contrastive objective to align their feature spaces. We conduct\nextensive experiments with SOTA baselines on real-world datasets, the detailed\nresults demonstrating significant improvements of our method LoID.", "published": "2024-03-30 03:56:53", "link": "http://arxiv.org/abs/2404.00236v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference", "abstract": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT.", "published": "2024-03-30 04:34:54", "link": "http://arxiv.org/abs/2404.00242v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DiLM: Distilling Dataset into Language Model for Text-level Dataset\n  Distillation", "abstract": "Dataset distillation aims to compress a training dataset by creating a small\nnumber of informative synthetic samples such that neural networks trained on\nthem perform as well as those trained on the original training dataset. Current\ntext dataset distillation methods create each synthetic sample as a sequence of\nword embeddings instead of a text to apply gradient-based optimization;\nhowever, such embedding-level distilled datasets cannot be used for training\nother models whose word embedding weights are different from the model used for\ndistillation. To address this issue, we propose a novel text dataset\ndistillation approach, called Distilling dataset into Language Model (DiLM),\nwhich trains a language model to generate informative synthetic training\nsamples as text data, instead of directly optimizing synthetic samples. We\nevaluated DiLM on various text classification datasets and showed that\ndistilled synthetic datasets from DiLM outperform those from current coreset\nselection methods. DiLM achieved remarkable generalization performance in\ntraining different types of models and in-context learning of large language\nmodels. Our code will be available at https://github.com/arumaekawa/DiLM.", "published": "2024-03-30 06:40:54", "link": "http://arxiv.org/abs/2404.00264v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A hybrid transformer and attention based recurrent neural network for\n  robust and interpretable sentiment analysis of tweets", "abstract": "Sentiment analysis is crucial for understanding public opinion and consumer\nbehavior. Existing models face challenges with linguistic diversity,\ngeneralizability, and explainability. We propose TRABSA, a hybrid framework\nintegrating transformer-based architectures, attention mechanisms, and BiLSTM\nnetworks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge\ngaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy.\nAugmenting datasets with tweets from 32 countries and US states, we compare six\nword-embedding techniques and three lexicon-based labeling techniques,\nselecting the best for optimal sentiment analysis. TRABSA outperforms\ntraditional ML and deep learning models with 94% accuracy and significant\nprecision, recall, and F1-score gains. Evaluation across diverse datasets\ndemonstrates consistent superiority and generalizability. SHAP and LIME\nanalyses enhance interpretability, improving confidence in predictions. Our\nstudy facilitates pandemic resource management, aiding resource planning,\npolicy formation, and vaccination tactics.", "published": "2024-03-30 09:20:43", "link": "http://arxiv.org/abs/2404.00297v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TACO -- Twitter Arguments from COnversations", "abstract": "Twitter has emerged as a global hub for engaging in online conversations and\nas a research corpus for various disciplines that have recognized the\nsignificance of its user-generated content. Argument mining is an important\nanalytical task for processing and understanding online discourse.\nSpecifically, it aims to identify the structural elements of arguments, denoted\nas information and inference. These elements, however, are not static and may\nrequire context within the conversation they are in, yet there is a lack of\ndata and annotation frameworks addressing this dynamic aspect on Twitter. We\ncontribute TACO, the first dataset of Twitter Arguments utilizing 1,814 tweets\ncovering 200 entire conversations spanning six heterogeneous topics annotated\nwith an agreement of 0.718 Krippendorff's alpha among six experts. Second, we\nprovide our annotation framework, incorporating definitions from the Cambridge\nDictionary, to define and identify argument components on Twitter. Our\ntransformer-based classifier achieves an 85.06\\% macro F1 baseline score in\ndetecting arguments. Moreover, our data reveals that Twitter users tend to\nengage in discussions involving informed inferences and information. TACO\nserves multiple purposes, such as training tweet classifiers to manage tweets\nbased on inference and information elements, while also providing valuable\ninsights into the conversational reply patterns of tweets.", "published": "2024-03-30 16:14:46", "link": "http://arxiv.org/abs/2404.00406v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do Vision-Language Models Understand Compound Nouns?", "abstract": "Open-vocabulary vision-language models (VLMs) like CLIP, trained using\ncontrastive loss, have emerged as a promising new paradigm for text-to-image\nretrieval. However, do VLMs understand compound nouns (CNs) (e.g., lab coat) as\nwell as they understand nouns (e.g., lab)? We curate Compun, a novel benchmark\nwith 400 unique and commonly used CNs, to evaluate the effectiveness of VLMs in\ninterpreting CNs. The Compun benchmark challenges a VLM for text-to-image\nretrieval where, given a text prompt with a CN, the task is to select the\ncorrect image that shows the CN among a pair of distractor images that show the\nconstituent nouns that make up the CN. Next, we perform an in-depth analysis to\nhighlight CLIPs' limited understanding of certain types of CNs. Finally, we\npresent an alternative framework that moves beyond hand-written templates for\ntext prompts widely used by CLIP-like models. We employ a Large Language Model\nto generate multiple diverse captions that include the CN as an object in the\nscene described by the caption. Our proposed method improves CN understanding\nof CLIP by 8.25% on Compun. Code and benchmark are available at:\nhttps://github.com/sonalkum/Compun", "published": "2024-03-30 16:54:45", "link": "http://arxiv.org/abs/2404.00419v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Beyond One-Size-Fits-All: Multi-Domain, Multi-Task Framework for\n  Embedding Model Selection", "abstract": "This position paper proposes a systematic approach towards developing a\nframework to help select the most effective embedding models for natural\nlanguage processing (NLP) tasks, addressing the challenge posed by the\nproliferation of both proprietary and open-source encoder models.", "published": "2024-03-30 19:45:04", "link": "http://arxiv.org/abs/2404.00458v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Dialectical Alignment: Resolving the Tension of 3H and Security Threats\n  of LLMs", "abstract": "With the rise of large language models (LLMs), ensuring they embody the\nprinciples of being helpful, honest, and harmless (3H), known as Human\nAlignment, becomes crucial. While existing alignment methods like RLHF, DPO,\netc., effectively fine-tune LLMs to match preferences in the preference\ndataset, they often lead LLMs to highly receptive human input and external\nevidence, even when this information is poisoned. This leads to a tendency for\nLLMs to be Adaptive Chameleons when external evidence conflicts with their\nparametric memory. This exacerbates the risk of LLM being attacked by external\npoisoned data, which poses a significant security risk to LLM system\napplications such as Retrieval-augmented generation (RAG). To address the\nchallenge, we propose a novel framework: Dialectical Alignment (DA), which (1)\nutilizes AI feedback to identify optimal strategies for LLMs to navigate\ninter-context conflicts and context-memory conflicts with different external\nevidence in context window (i.e., different ratios of poisoned factual\ncontexts); (2) constructs the SFT dataset as well as the preference dataset\nbased on the AI feedback and strategies above; (3) uses the above datasets for\nLLM alignment to defense poisoned context attack while preserving the\neffectiveness of in-context knowledge editing. Our experiments show that the\ndialectical alignment model improves poisoned data attack defense by 20 and\ndoes not require any additional prompt engineering or prior declaration of\n``you may be attacked`` to the LLMs' context window.", "published": "2024-03-30 22:41:05", "link": "http://arxiv.org/abs/2404.00486v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Configurable Safety Tuning of Language Models with Synthetic Preference\n  Data", "abstract": "State-of-the-art language model fine-tuning techniques, such as Direct\nPreference Optimization (DPO), restrict user control by hard-coding predefined\nbehaviors into the model. To address this, we propose a novel method,\nConfigurable Safety Tuning (CST), that augments DPO using synthetic preference\ndata to facilitate flexible safety configuration of LLMs at inference time. CST\novercomes the constraints of vanilla DPO by introducing a system prompt\nspecifying safety configurations, enabling LLM deployers to disable/enable\nsafety preferences based on their need, just changing the system prompt. Our\nexperimental evaluations indicate that CST successfully manages different\nsafety configurations and retains the original functionality of LLMs, showing\nit is a robust method for configurable deployment. Data and models available at\nhttps://github.com/vicgalle/configurable-safety-tuning", "published": "2024-03-30 23:28:05", "link": "http://arxiv.org/abs/2404.00495v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Shape of Word Embeddings: Quantifying Non-Isometry With Topological\n  Data Analysis", "abstract": "Word embeddings represent language vocabularies as clouds of $d$-dimensional\npoints. We investigate how information is conveyed by the general shape of\nthese clouds, instead of representing the semantic meaning of each token.\nSpecifically, we use the notion of persistent homology from topological data\nanalysis (TDA) to measure the distances between language pairs from the shape\nof their unlabeled embeddings. These distances quantify the degree of\nnon-isometry of the embeddings. To distinguish whether these differences are\nrandom training errors or capture real information about the languages, we use\nthe computed distance matrices to construct language phylogenetic trees over 81\nIndo-European languages. Careful evaluation shows that our reconstructed trees\nexhibit strong and statistically-significant similarities to the reference.", "published": "2024-03-30 23:51:25", "link": "http://arxiv.org/abs/2404.00500v2", "categories": ["cs.CL", "math.AT"], "primary_category": "cs.CL"}
{"title": "Augmenting NER Datasets with LLMs: Towards Automated and Refined\n  Annotation", "abstract": "In the field of Natural Language Processing (NLP), Named Entity Recognition\n(NER) is recognized as a critical technology, employed across a wide array of\napplications. Traditional methodologies for annotating datasets for NER models\nare challenged by high costs and variations in dataset quality. This research\nintroduces a novel hybrid annotation approach that synergizes human effort with\nthe capabilities of Large Language Models (LLMs). This approach not only aims\nto ameliorate the noise inherent in manual annotations, such as omissions,\nthereby enhancing the performance of NER models, but also achieves this in a\ncost-effective manner. Additionally, by employing a label mixing strategy, it\naddresses the issue of class imbalance encountered in LLM-based annotations.\nThrough an analysis across multiple datasets, this method has been consistently\nshown to provide superior performance compared to traditional annotation\nmethods, even under constrained budget conditions. This study illuminates the\npotential of leveraging LLMs to improve dataset quality, introduces a novel\ntechnique to mitigate class imbalances, and demonstrates the feasibility of\nachieving high-performance NER in a cost-effective way.", "published": "2024-03-30 12:13:57", "link": "http://arxiv.org/abs/2404.01334v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Causal Inference for Human-Language Model Collaboration", "abstract": "In this paper, we examine the collaborative dynamics between humans and\nlanguage models (LMs), where the interactions typically involve LMs proposing\ntext segments and humans editing or responding to these proposals. Productive\nengagement with LMs in such scenarios necessitates that humans discern\neffective text-based interaction strategies, such as editing and response\nstyles, from historical human-LM interactions. This objective is inherently\ncausal, driven by the counterfactual `what-if' question: how would the outcome\nof collaboration change if humans employed a different text editing/refinement\nstrategy? A key challenge in answering this causal inference question is\nformulating an appropriate causal estimand: the conventional average treatment\neffect (ATE) estimand is inapplicable to text-based treatments due to their\nhigh dimensionality. To address this concern, we introduce a new causal\nestimand -- Incremental Stylistic Effect (ISE) -- which characterizes the\naverage impact of infinitesimally shifting a text towards a specific style,\nsuch as increasing formality. We establish the conditions for the\nnon-parametric identification of ISE. Building on this, we develop\nCausalCollab, an algorithm designed to estimate the ISE of various interaction\nstrategies in dynamic human-LM collaborations. Our empirical investigations\nacross three distinct human-LM collaboration scenarios reveal that CausalCollab\neffectively reduces confounding and significantly improves counterfactual\nestimation over a set of competitive baselines.", "published": "2024-03-30 01:08:25", "link": "http://arxiv.org/abs/2404.00207v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Your Co-Workers Matter: Evaluating Collaborative Capabilities of\n  Language Models in Blocks World", "abstract": "Language agents that interact with the world on their own have great\npotential for automating digital tasks. While large language model (LLM) agents\nhave made progress in understanding and executing tasks such as textual games\nand webpage control, many real-world tasks also require collaboration with\nhumans or other LLMs in equal roles, which involves intent understanding, task\ncoordination, and communication. To test LLM's ability to collaborate, we\ndesign a blocks-world environment, where two agents, each having unique goals\nand skills, build a target structure together. To complete the goals, they can\nact in the world and communicate in natural language. Under this environment,\nwe design increasingly challenging settings to evaluate different collaboration\nperspectives, from independent to more complex, dependent tasks. We further\nadopt chain-of-thought prompts that include intermediate reasoning steps to\nmodel the partner's state and identify and correct execution errors. Both\nhuman-machine and machine-machine experiments show that LLM agents have strong\ngrounding capacities, and our approach significantly improves the evaluation\nmetric.", "published": "2024-03-30 04:48:38", "link": "http://arxiv.org/abs/2404.00246v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Survey on Large Language Model-Enhanced Reinforcement Learning: Concept,\n  Taxonomy, and Methods", "abstract": "With extensive pre-trained knowledge and high-level general capabilities,\nlarge language models (LLMs) emerge as a promising avenue to augment\nreinforcement learning (RL) in aspects such as multi-task learning, sample\nefficiency, and high-level task planning. In this survey, we provide a\ncomprehensive review of the existing literature in LLM-enhanced RL and\nsummarize its characteristics compared to conventional RL methods, aiming to\nclarify the research scope and directions for future studies. Utilizing the\nclassical agent-environment interaction paradigm, we propose a structured\ntaxonomy to systematically categorize LLMs' functionalities in RL, including\nfour roles: information processor, reward designer, decision-maker, and\ngenerator. For each role, we summarize the methodologies, analyze the specific\nRL challenges that are mitigated, and provide insights into future directions.\nLastly, a comparative analysis of each role, potential applications,\nprospective opportunities, and challenges of the LLM-enhanced RL are discussed.\nBy proposing this taxonomy, we aim to provide a framework for researchers to\neffectively leverage LLMs in the RL field, potentially accelerating RL\napplications in complex applications such as robotics, autonomous driving, and\nenergy systems.", "published": "2024-03-30 08:28:08", "link": "http://arxiv.org/abs/2404.00282v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.LG"}
{"title": "Can LLMs Master Math? Investigating Large Language Models on Math Stack\n  Exchange", "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities in\nvarious natural language tasks, often achieving performances that surpass those\nof humans. Despite these advancements, the domain of mathematics presents a\ndistinctive challenge, primarily due to its specialized structure and the\nprecision it demands. In this study, we adopted a two-step approach for\ninvestigating the proficiency of LLMs in answering mathematical questions.\nFirst, we employ the most effective LLMs, as identified by their performance on\nmath question-answer benchmarks, to generate answers to 78 questions from the\nMath Stack Exchange (MSE). Second, a case analysis is conducted on the LLM that\nshowed the highest performance, focusing on the quality and accuracy of its\nanswers through manual evaluation. We found that GPT-4 performs best (nDCG of\n0.48 and P@10 of 0.37) amongst existing LLMs fine-tuned for answering\nmathematics questions and outperforms the current best approach on ArqMATH3\nTask1, considering P@10. Our Case analysis indicates that while the GPT-4 can\ngenerate relevant responses in certain instances, it does not consistently\nanswer all questions accurately. This paper explores the current limitations of\nLLMs in navigating complex mathematical problem-solving. Through case analysis,\nwe shed light on the gaps in LLM capabilities within mathematics, thereby\nsetting the stage for future research and advancements in AI-driven\nmathematical reasoning. We make our code and findings publicly available for\nresearch: \\url{https://github.com/gipplab/LLM-Investig-MathStackExchange}", "published": "2024-03-30 12:48:31", "link": "http://arxiv.org/abs/2404.00344v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Aurora-M: Open Source Continual Pre-training for Multilingual Language\n  and Code", "abstract": "Pretrained language models are an integral part of AI applications, but their\nhigh computational cost for training limits accessibility. Initiatives such as\nBloom and StarCoder aim to democratize access to pretrained models for\ncollaborative community development. Despite these efforts, such models\nencounter challenges such as limited multilingual capabilities, risks of\ncatastrophic forgetting during continual pretraining, and the high costs of\ntraining models from scratch, alongside the need to align with AI safety\nstandards and regulatory frameworks.\n  This paper presents Aurora-M, a 15B parameter multilingual open-source model\ntrained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually\npretrained from StarCoderPlus on 435B additional tokens, Aurora-M surpasses 2T\ntokens in total training token count. It is the first open-source multilingual\nmodel fine-tuned on human-reviewed safety instructions, thus aligning its\ndevelopment not only with conventional red-teaming considerations, but also\nwith the specific concerns articulated in the Biden-Harris Executive Order on\nthe Safe, Secure, and Trustworthy Development and Use of Artificial\nIntelligence.\n  We evaluate Aurora-M across a wide range of tasks and languages, showcasing\nits robustness against catastrophic forgetting and its superior performance in\nmultilingual settings, particularly in safety evaluations. We open-source\nAurora-M and its variants to encourage responsible open-source development of\nlarge language models at https://huggingface.co/aurora-m.", "published": "2024-03-30 15:38:54", "link": "http://arxiv.org/abs/2404.00399v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic explanation of the classification of Spanish legal judgments\n  in jurisdiction-dependent law categories with tree estimators", "abstract": "Automatic legal text classification systems have been proposed in the\nliterature to address knowledge extraction from judgments and detect their\naspects. However, most of these systems are black boxes even when their models\nare interpretable. This may raise concerns about their trustworthiness.\nAccordingly, this work contributes with a system combining Natural Language\nProcessing (NLP) with Machine Learning (ML) to classify legal texts in an\nexplainable manner. We analyze the features involved in the decision and the\nthreshold bifurcation values of the decision paths of tree structures and\npresent this information to the users in natural language. This is the first\nwork on automatic analysis of legal texts combining NLP and ML along with\nExplainable Artificial Intelligence techniques to automatically make the\nmodels' decisions understandable to end users. Furthermore, legal experts have\nvalidated our solution, and this knowledge has also been incorporated into the\nexplanation process as \"expert-in-the-loop\" dictionaries. Experimental results\non an annotated data set in law categories by jurisdiction demonstrate that our\nsystem yields competitive classification performance, with accuracy values well\nabove 90%, and that its automatic explanations are easily understandable even\nto non-expert users.", "published": "2024-03-30 17:59:43", "link": "http://arxiv.org/abs/2404.00437v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Planning and Editing What You Retrieve for Enhanced Tool Learning", "abstract": "Recent advancements in integrating external tools with Large Language Models\n(LLMs) have opened new frontiers, with applications in mathematical reasoning,\ncode generators, and smart assistants. However, existing methods, relying on\nsimple one-time retrieval strategies, fall short on effectively and accurately\nshortlisting relevant tools. This paper introduces a novel PLUTO (Planning,\nLearning, and Understanding for TOols) approach, encompassing\n`Plan-and-Retrieve (P&R)` and `Edit-and-Ground (E&G)` paradigms. The P&R\nparadigm consists of a neural retrieval module for shortlisting relevant tools\nand an LLM-based query planner that decomposes complex queries into actionable\ntasks, enhancing the effectiveness of tool utilization. The E&G paradigm\nutilizes LLMs to enrich tool descriptions based on user scenarios, bridging the\ngap between user queries and tool functionalities. Experiment results\ndemonstrate that these paradigms significantly improve the recall and NDCG in\ntool retrieval tasks, significantly surpassing current state-of-the-art models.", "published": "2024-03-30 18:41:51", "link": "http://arxiv.org/abs/2404.00450v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Shortcuts Arising from Contrast: Effective and Covert Clean-Label\n  Attacks in Prompt-Based Learning", "abstract": "Prompt-based learning paradigm has demonstrated remarkable efficacy in\nenhancing the adaptability of pretrained language models (PLMs), particularly\nin few-shot scenarios. However, this learning paradigm has been shown to be\nvulnerable to backdoor attacks. The current clean-label attack, employing a\nspecific prompt as a trigger, can achieve success without the need for external\ntriggers and ensure correct labeling of poisoned samples, which is more\nstealthy compared to the poisoned-label attack, but on the other hand, it faces\nsignificant issues with false activations and poses greater challenges,\nnecessitating a higher rate of poisoning. Using conventional negative data\naugmentation methods, we discovered that it is challenging to trade off between\neffectiveness and stealthiness in a clean-label setting. In addressing this\nissue, we are inspired by the notion that a backdoor acts as a shortcut and\nposit that this shortcut stems from the contrast between the trigger and the\ndata utilized for poisoning. In this study, we propose a method named\nContrastive Shortcut Injection (CSI), by leveraging activation values,\nintegrates trigger design and data selection strategies to craft stronger\nshortcut features. With extensive experiments on full-shot and few-shot text\nclassification tasks, we empirically validate CSI's high effectiveness and high\nstealthiness at low poisoning rates. Notably, we found that the two approaches\nplay leading roles in full-shot and few-shot settings, respectively.", "published": "2024-03-30 20:02:36", "link": "http://arxiv.org/abs/2404.00461v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "68T50", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Addressing Both Statistical and Causal Gender Fairness in NLP Models", "abstract": "Statistical fairness stipulates equivalent outcomes for every protected\ngroup, whereas causal fairness prescribes that a model makes the same\nprediction for an individual regardless of their protected characteristics.\nCounterfactual data augmentation (CDA) is effective for reducing bias in NLP\nmodels, yet models trained with CDA are often evaluated only on metrics that\nare closely tied to the causal fairness notion; similarly, sampling-based\nmethods designed to promote statistical fairness are rarely evaluated for\ncausal fairness. In this work, we evaluate both statistical and causal\ndebiasing methods for gender bias in NLP models, and find that while such\nmethods are effective at reducing bias as measured by the targeted metric, they\ndo not necessarily improve results on other bias metrics. We demonstrate that\ncombinations of statistical and causal debiasing techniques are able to reduce\nbias measured through both types of metrics.", "published": "2024-03-30 20:05:41", "link": "http://arxiv.org/abs/2404.00463v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Linguistic Calibration of Long-Form Generations", "abstract": "Language models (LMs) may lead their users to make suboptimal downstream\ndecisions when they confidently hallucinate. This issue can be mitigated by\nhaving the LM verbally convey the probability that its claims are correct, but\nexisting models cannot produce long-form text with calibrated confidence\nstatements. Through the lens of decision-making, we define linguistic\ncalibration for long-form generations: an LM is linguistically calibrated if\nits generations enable its users to make calibrated probabilistic predictions.\nThis definition enables a training framework where a supervised finetuning step\nbootstraps an LM to emit long-form generations with confidence statements such\nas \"I estimate a 30% chance of...\" or \"I am certain that...\", followed by a\nreinforcement learning step which rewards generations that enable a user to\nprovide calibrated answers to related questions. We linguistically calibrate\nLlama 2 7B and find in automated and human evaluations of long-form generations\nthat it is significantly more calibrated than strong finetuned factuality\nbaselines with comparable accuracy. These findings generalize under significant\ndomain shifts to scientific and biomedical questions and to an entirely\nheld-out person biography generation task. Our results demonstrate that\nlong-form generations may be calibrated end-to-end by constructing an objective\nin the space of the predictions that users make in downstream decision-making.", "published": "2024-03-30 20:47:55", "link": "http://arxiv.org/abs/2404.00474v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Cross-lingual Named Entity Corpus for Slavic Languages", "abstract": "This paper presents a corpus manually annotated with named entities for six\nSlavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian.\nThis work is the result of a series of shared tasks, conducted in 2017-2023 as\na part of the Workshops on Slavic Natural Language Processing. The corpus\nconsists of 5 017 documents on seven topics. The documents are annotated with\nfive classes of named entities. Each entity is described by a category, a\nlemma, and a unique cross-lingual identifier. We provide two train-tune dataset\nsplits - single topic out and cross topics. For each split, we set benchmarks\nusing a transformer-based neural network architecture with the pre-trained\nmultilingual models - XLM-RoBERTa-large for named entity mention recognition\nand categorization, and mT5-large for named entity lemmatization and linking.", "published": "2024-03-30 22:20:08", "link": "http://arxiv.org/abs/2404.00482v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Noise-Aware Training of Layout-Aware Language Models", "abstract": "A visually rich document (VRD) utilizes visual features along with linguistic\ncues to disseminate information. Training a custom extractor that identifies\nnamed entities from a document requires a large number of instances of the\ntarget document type annotated at textual and visual modalities. This is an\nexpensive bottleneck in enterprise scenarios, where we want to train custom\nextractors for thousands of different document types in a scalable way.\nPre-training an extractor model on unlabeled instances of the target document\ntype, followed by a fine-tuning step on human-labeled instances does not work\nin these scenarios, as it surpasses the maximum allowable training time\nallocated for the extractor. We address this scenario by proposing a\nNoise-Aware Training method or NAT in this paper. Instead of acquiring\nexpensive human-labeled documents, NAT utilizes weakly labeled documents to\ntrain an extractor in a scalable way. To avoid degradation in the model's\nquality due to noisy, weakly labeled samples, NAT estimates the confidence of\neach training sample and incorporates it as uncertainty measure during\ntraining. We train multiple state-of-the-art extractor models using NAT.\nExperiments on a number of publicly available and in-house datasets show that\nNAT-trained models are not only robust in performance -- it outperforms a\ntransfer-learning baseline by up to 6% in terms of macro-F1 score, but it is\nalso more label-efficient -- it reduces the amount of human-effort required to\nobtain comparable performance by up to 73%.", "published": "2024-03-30 23:06:34", "link": "http://arxiv.org/abs/2404.00488v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prompt-SAW: Leveraging Relation-Aware Graphs for Textual Prompt\n  Compression", "abstract": "Large Language Models (LLMs) have shown exceptional abilities for multiple\ndifferent natural language processing tasks. While prompting is a crucial tool\nfor LLM inference, we observe that there is a significant cost associated with\nexceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead\nto substandard results in terms of readability/interpretability of the\ncompressed prompt, with a detrimental impact on prompt utility. To address\nthis, we propose PromptSAW: Prompt compresSion via Relation AWare graphs, an\neffective strategy for prompt compression over task-agnostic and task-aware\nprompts. Prompt-SAW uses the prompt's textual information to build a graph and\nlater extracts key information elements in the graph to come up with the\ncompressed prompt. We also propose GSM8K-aug, i.e., an extended version of the\nexisting GSM8K benchmark for task-agnostic prompts in order to provide a\ncomprehensive evaluation platform. Experimental evaluation using benchmark\ndatasets shows that prompts compressed by Prompt-SAW are not only better in\nterms of readability, but they also outperform the best-performing baseline\nmodels by up to 10.1 and 77.1, respectively, for task-agnostic and task-aware\nsettings while compressing the original prompt text by 34.9 and 56.7.", "published": "2024-03-30 23:07:58", "link": "http://arxiv.org/abs/2404.00489v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-hop Question Answering under Temporal Knowledge Editing", "abstract": "Multi-hop question answering (MQA) under knowledge editing (KE) has garnered\nsignificant attention in the era of large language models. However, existing\nmodels for MQA under KE exhibit poor performance when dealing with questions\ncontaining explicit temporal contexts. To address this limitation, we propose a\nnovel framework, namely TEMPoral knowLEdge augmented Multi-hop Question\nAnswering (TEMPLE-MQA). Unlike previous methods, TEMPLE-MQA first constructs a\ntime-aware graph (TAG) to store edit knowledge in a structured manner. Then,\nthrough our proposed inference path, structural retrieval, and joint reasoning\nstages, TEMPLE-MQA effectively discerns temporal contexts within the question\nquery. Experiments on benchmark datasets demonstrate that TEMPLE-MQA\nsignificantly outperforms baseline models. Additionally, we contribute a new\ndataset, namely TKEMQA, which serves as the inaugural benchmark tailored\nspecifically for MQA with temporal scopes.", "published": "2024-03-30 23:22:51", "link": "http://arxiv.org/abs/2404.00492v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain\n  Fake News Detection", "abstract": "Existing benchmarks for fake news detection have significantly contributed to\nthe advancement of models in assessing the authenticity of news content.\nHowever, these benchmarks typically focus solely on news pertaining to a single\nsemantic topic or originating from a single platform, thereby failing to\ncapture the diversity of multi-domain news in real scenarios. In order to\nunderstand fake news across various domains, the external knowledge and\nfine-grained annotations are indispensable to provide precise evidence and\nuncover the diverse underlying strategies for fabrication, which are also\nignored by existing benchmarks. To address this gap, we introduce a novel\nmulti-domain knowledge-enhanced benchmark with fine-grained annotations, named\n\\textbf{FineFake}. FineFake encompasses 16,909 data samples spanning six\nsemantic topics and eight platforms. Each news item is enriched with\nmulti-modal content, potential social context, semi-manually verified common\nknowledge, and fine-grained annotations that surpass conventional binary\nlabels. Furthermore, we formulate three challenging tasks based on FineFake and\npropose a knowledge-enhanced domain adaptation network. Extensive experiments\nare conducted on FineFake under various scenarios, providing accurate and\nreliable benchmarks for future endeavors. The entire FineFake project is\npublicly accessible as an open-source repository at\n\\url{https://github.com/Accuser907/FineFake}.", "published": "2024-03-30 14:39:09", "link": "http://arxiv.org/abs/2404.01336v3", "categories": ["cs.CL", "cs.AI", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Detection of Temporality at Discourse Level on Financial News by\n  Combining Natural Language Processing and Machine Learning", "abstract": "Finance-related news such as Bloomberg News, CNN Business and Forbes are\nvaluable sources of real data for market screening systems. In news, an expert\nshares opinions beyond plain technical analyses that include context such as\npolitical, sociological and cultural factors. In the same text, the expert\noften discusses the performance of different assets. Some key statements are\nmere descriptions of past events while others are predictions. Therefore,\nunderstanding the temporality of the key statements in a text is essential to\nseparate context information from valuable predictions. We propose a novel\nsystem to detect the temporality of finance-related news at discourse level\nthat combines Natural Language Processing and Machine Learning techniques, and\nexploits sophisticated features such as syntactic and semantic dependencies.\nMore specifically, we seek to extract the dominant tenses of the main\nstatements, which may be either explicit or implicit. We have tested our system\non a labelled dataset of finance-related news annotated by researchers with\nknowledge in the field. Experimental results reveal a high detection precision\ncompared to an alternative rule-based baseline approach. Ultimately, this\nresearch contributes to the state-of-the-art of market screening by identifying\npredictive knowledge for financial decision making.", "published": "2024-03-30 16:40:10", "link": "http://arxiv.org/abs/2404.01337v1", "categories": ["cs.CL", "cs.CE", "cs.IR", "cs.LG", "q-fin.ST"], "primary_category": "cs.CL"}
{"title": "Automatic detection of relevant information, predictions and forecasts\n  in financial news through topic modelling with Latent Dirichlet Allocation", "abstract": "Financial news items are unstructured sources of information that can be\nmined to extract knowledge for market screening applications. Manual extraction\nof relevant information from the continuous stream of finance-related news is\ncumbersome and beyond the skills of many investors, who, at most, can follow a\nfew sources and authors. Accordingly, we focus on the analysis of financial\nnews to identify relevant text and, within that text, forecasts and\npredictions. We propose a novel Natural Language Processing (NLP) system to\nassist investors in the detection of relevant financial events in unstructured\ntextual sources by considering both relevance and temporality at the discursive\nlevel. Firstly, we segment the text to group together closely related text.\nSecondly, we apply co-reference resolution to discover internal dependencies\nwithin segments. Finally, we perform relevant topic modelling with Latent\nDirichlet Allocation (LDA) to separate relevant from less relevant text and\nthen analyse the relevant text using a Machine Learning-oriented temporal\napproach to identify predictions and speculative statements. We created an\nexperimental data set composed of 2,158 financial news items that were manually\nlabelled by NLP researchers to evaluate our solution. The ROUGE-L values for\nthe identification of relevant text and predictions/forecasts were 0.662 and\n0.982, respectively. To our knowledge, this is the first work to jointly\nconsider relevance and temporality at the discursive level. It contributes to\nthe transfer of human associative discourse capabilities to expert systems\nthrough the combination of multi-paragraph topic segmentation and co-reference\nresolution to separate author expression patterns, topic modelling with LDA to\ndetect relevant text, and discursive temporality analysis to identify forecasts\nand predictions within this text.", "published": "2024-03-30 17:49:34", "link": "http://arxiv.org/abs/2404.01338v1", "categories": ["cs.CL", "cs.CE", "cs.IR", "cs.LG", "q-fin.ST"], "primary_category": "cs.CL"}
{"title": "Targeted aspect-based emotion analysis to detect opportunities and\n  precaution in financial Twitter messages", "abstract": "Microblogging platforms, of which Twitter is a representative example, are\nvaluable information sources for market screening and financial models. In\nthem, users voluntarily provide relevant information, including educated\nknowledge on investments, reacting to the state of the stock markets in\nreal-time and, often, influencing this state. We are interested in the user\nforecasts in financial, social media messages expressing opportunities and\nprecautions about assets. We propose a novel Targeted Aspect-Based Emotion\nAnalysis (TABEA) system that can individually discern the financial emotions\n(positive and negative forecasts) on the different stock market assets in the\nsame tweet (instead of making an overall guess about that whole tweet). It is\nbased on Natural Language Processing (NLP) techniques and Machine Learning\nstreaming algorithms. The system comprises a constituency parsing module for\nparsing the tweets and splitting them into simpler declarative clauses; an\noffline data processing module to engineer textual, numerical and categorical\nfeatures and analyse and select them based on their relevance; and a stream\nclassification module to continuously process tweets on-the-fly. Experimental\nresults on a labelled data set endorse our solution. It achieves over 90%\nprecision for the target emotions, financial opportunity, and precaution on\nTwitter. To the best of our knowledge, no prior work in the literature has\naddressed this problem despite its practical interest in decision-making, and\nwe are not aware of any previous NLP nor online Machine Learning approaches to\nTABEA.", "published": "2024-03-30 16:46:25", "link": "http://arxiv.org/abs/2404.08665v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "cs.SI", "q-fin.TR"], "primary_category": "cs.IR"}
{"title": "Classification of Short Segment Pediatric Heart Sounds Based on a\n  Transformer-Based Convolutional Neural Network", "abstract": "Congenital anomalies arising as a result of a defect in the structure of the\nheart and great vessels are known as congenital heart diseases or CHDs. A PCG\ncan provide essential details about the mechanical conduction system of the\nheart and point out specific patterns linked to different kinds of CHD. This\nstudy aims to investigate the minimum signal duration required for the\nautomatic classification of heart sounds. This study also investigated the\noptimum signal quality assessment indicator (Root Mean Square of Successive\nDifferences) RMSSD and (Zero Crossings Rate) ZCR value. Mel-frequency cepstral\ncoefficients (MFCCs) based feature is used as an input to build a\nTransformer-Based residual one-dimensional convolutional neural network, which\nis then used for classifying the heart sound. The study showed that 0.4 is the\nideal threshold for getting suitable signals for the RMSSD and ZCR indicators.\nMoreover, a minimum signal length of 5s is required for effective heart sound\nclassification. It also shows that a shorter signal (3 s heart sound) does not\nhave enough information to categorize heart sounds accurately, and the longer\nsignal (15 s heart sound) may contain more noise. The best accuracy, 93.69%, is\nobtained for the 5s signal to distinguish the heart sound.", "published": "2024-03-30 20:32:35", "link": "http://arxiv.org/abs/2404.00470v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
