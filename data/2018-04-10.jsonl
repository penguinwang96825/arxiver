{"title": "Who framed Roger Reindeer? De-censorship of Facebook posts by snippet\n  classification", "abstract": "This paper considers online news censorship and it concentrates on censorship\nof identities. Obfuscating identities may occur for disparate reasons, from\nmilitary to judiciary ones. In the majority of cases, this happens to protect\nindividuals from being identified and persecuted by hostile people. However,\nbeing the collaborative web characterised by a redundancy of information, it is\nnot unusual that the same fact is reported by multiple sources, which may not\napply the same restriction policies in terms of censorship. Also, the proven\naptitude of social network users to disclose personal information leads to the\nphenomenon that comments to news can reveal the data withheld in the news\nitself. This gives us a mean to figure out who the subject of the censored news\nis. We propose an adaptation of a text analysis approach to unveil censored\nidentities. The approach is tested on a synthesised scenario, which however\nresembles a real use case. Leveraging a text analysis based on a context\nclassifier trained over snippets from posts and comments of Facebook pages, we\nachieve promising results. Despite the quite constrained settings in which we\noperate -- such as considering only snippets of very short length -- our system\nsuccessfully detects the censored name, choosing among 10 different candidate\nnames, in more than 50\\% of the investigated cases. This outperforms the\nresults of two reference baselines. The findings reported in this paper, other\nthan being supported by a thorough experimental methodology and interesting on\ntheir own, also pave the way for further investigation on the insidious issues\nof censorship on the web.", "published": "2018-04-10 10:22:19", "link": "http://arxiv.org/abs/1804.03433v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Learning for Digital Text Analytics: Sentiment Analysis", "abstract": "In today's scenario, imagining a world without negativity is something very\nunrealistic, as bad NEWS spreads more virally than good ones. Though it seems\nimpractical in real life, this could be implemented by building a system using\nMachine Learning and Natural Language Processing techniques in identifying the\nnews datum with negative shade and filter them by taking only the news with\npositive shade (good news) to the end user. In this work, around two lakhs\ndatum have been trained and tested using a combination of rule-based and data\ndriven approaches. VADER along with a filtration method has been used as an\nannotating tool followed by statistical Machine Learning approach that have\nused Document Term Matrix (representation) and Support Vector Machine\n(classification). Deep Learning algorithms then came into picture to make this\nsystem reliable (Doc2Vec) which finally ended up with Convolutional Neural\nNetwork(CNN) that yielded better results than the other experimented modules.\nIt showed up a training accuracy of 96%, while a test accuracy of (internal and\nexternal news datum) above 85% was obtained.", "published": "2018-04-10 18:10:33", "link": "http://arxiv.org/abs/1804.03673v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Sentiment Transfer using Seq2Seq Adversarial Autoencoders", "abstract": "Expressing in language is subjective. Everyone has a different style of\nreading and writing, apparently it all boil downs to the way their mind\nunderstands things (in a specific format). Language style transfer is a way to\npreserve the meaning of a text and change the way it is expressed. Progress in\nlanguage style transfer is lagged behind other domains, such as computer\nvision, mainly because of the lack of parallel data, use cases, and reliable\nevaluation metrics. In response to the challenge of lacking parallel data, we\nexplore learning style transfer from non-parallel data. We propose a model\ncombining seq2seq, autoencoders, and adversarial loss to achieve this goal. The\nkey idea behind the proposed models is to learn separate content\nrepresentations and style representations using adversarial networks.\nConsidering the problem of evaluating style transfer tasks, we frame the\nproblem as sentiment transfer and evaluation using a sentiment classifier to\ncalculate how many sentiments was the model able to transfer. We report our\nresults on several kinds of models.", "published": "2018-04-10 16:28:33", "link": "http://arxiv.org/abs/1804.04003v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Natural Language Statistical Features of LSTM-generated Texts", "abstract": "Long Short-Term Memory (LSTM) networks have recently shown remarkable\nperformance in several tasks dealing with natural language generation, such as\nimage captioning or poetry composition. Yet, only few works have analyzed text\ngenerated by LSTMs in order to quantitatively evaluate to which extent such\nartificial texts resemble those generated by humans. We compared the\nstatistical structure of LSTM-generated language to that of written natural\nlanguage, and to those produced by Markov models of various orders. In\nparticular, we characterized the statistical structure of language by assessing\nword-frequency statistics, long-range correlations, and entropy measures. Our\nmain finding is that while both LSTM and Markov-generated texts can exhibit\nfeatures similar to real ones in their word-frequency statistics and entropy\nmeasures, LSTM-texts are shown to reproduce long-range correlations at scales\ncomparable to those found in natural language. Moreover, for LSTM networks a\ntemperature-like parameter controlling the generation process shows an optimal\nvalue---for which the produced texts are closest to real language---consistent\nacross all the different statistical features investigated.", "published": "2018-04-10 13:17:36", "link": "http://arxiv.org/abs/1804.04087v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Question Answering over Freebase via Attentive RNN with Similarity\n  Matrix based CNN", "abstract": "With the rapid growth of knowledge bases (KBs), question answering over\nknowledge base, a.k.a. KBQA has drawn huge attention in recent years. Most of\nthe existing KBQA methods follow so called encoder-compare framework. They map\nthe question and the KB facts to a common embedding space, in which the\nsimilarity between the question vector and the fact vectors can be conveniently\ncomputed. This, however, inevitably loses original words interaction\ninformation. To preserve more original information, we propose an attentive\nrecurrent neural network with similarity matrix based convolutional neural\nnetwork (AR-SMCNN) model, which is able to capture comprehensive hierarchical\ninformation utilizing the advantages of both RNN and CNN. We use RNN to capture\nsemantic-level correlation by its sequential modeling nature, and use an\nattention mechanism to keep track of the entities and relations simultaneously.\nMeanwhile, we use a similarity matrix based CNN with two-directions pooling to\nextract literal-level words interaction matching utilizing CNNs strength of\nmodeling spatial correlation among data. Moreover, we have developed a new\nheuristic extension method for entity detection, which significantly decreases\nthe effect of noise. Our method has outperformed the state-of-the-arts on\nSimpleQuestion benchmark in both accuracy and efficiency.", "published": "2018-04-10 02:39:41", "link": "http://arxiv.org/abs/1804.03317v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QA4IE: A Question Answering based Framework for Information Extraction", "abstract": "Information Extraction (IE) refers to automatically extracting structured\nrelation tuples from unstructured texts. Common IE solutions, including\nRelation Extraction (RE) and open IE systems, can hardly handle cross-sentence\ntuples, and are severely restricted by limited relation types as well as\ninformal relation specifications (e.g., free-text based relation tuples). In\norder to overcome these weaknesses, we propose a novel IE framework named\nQA4IE, which leverages the flexible question answering (QA) approaches to\nproduce high quality relation triples across sentences. Based on the framework,\nwe develop a large IE benchmark with high quality human evaluation. This\nbenchmark contains 293K documents, 2M golden relation triples, and 636 relation\ntypes. We compare our system with some IE baselines on our benchmark and the\nresults show that our system achieves great improvements.", "published": "2018-04-10 08:31:03", "link": "http://arxiv.org/abs/1804.03396v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Hierarchical Latent Structure for Variational Conversation Modeling", "abstract": "Variational autoencoders (VAE) combined with hierarchical RNNs have emerged\nas a powerful framework for conversation modeling. However, they suffer from\nthe notorious degeneration problem, where the decoders learn to ignore latent\nvariables and reduce to vanilla RNNs. We empirically show that this degeneracy\noccurs mostly due to two reasons. First, the expressive power of hierarchical\nRNN decoders is often high enough to model the data using only its decoding\ndistributions without relying on the latent variables. Second, the conditional\nVAE structure whose generation process is conditioned on a context, makes the\nrange of training targets very sparse; that is, the RNN decoders can easily\noverfit to the training data ignoring the latent variables. To solve the\ndegeneration problem, we propose a novel model named Variational Hierarchical\nConversation RNNs (VHCR), involving two key ideas of (1) using a hierarchical\nstructure of latent variables, and (2) exploiting an utterance drop\nregularization. With evaluations on two datasets of Cornell Movie Dialog and\nUbuntu Dialog Corpus, we show that our VHCR successfully utilizes latent\nvariables and outperforms state-of-the-art models for conversation generation.\nMoreover, it can perform several new utterance control tasks, thanks to its\nhierarchical latent structure.", "published": "2018-04-10 10:00:36", "link": "http://arxiv.org/abs/1804.03424v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mining Social Media for Newsgathering: A Review", "abstract": "Social media is becoming an increasingly important data source for learning\nabout breaking news and for following the latest developments of ongoing news.\nThis is in part possible thanks to the existence of mobile devices, which\nallows anyone with access to the Internet to post updates from anywhere,\nleading in turn to a growing presence of citizen journalism. Consequently,\nsocial media has become a go-to resource for journalists during the process of\nnewsgathering. Use of social media for newsgathering is however challenging,\nand suitable tools are needed in order to facilitate access to useful\ninformation for reporting. In this paper, we provide an overview of research in\ndata mining and natural language processing for mining social media for\nnewsgathering. We discuss five different areas that researchers have worked on\nto mitigate the challenges inherent to social media newsgathering: news\ndiscovery, curation of news, validation and verification of content,\nnewsgathering dashboards, and other tasks. We outline the progress made so far\nin the field, summarise the current challenges as well as discuss future\ndirections in the use of computational journalism to assist with social media\nnewsgathering. This review is relevant to computer scientists researching news\nin social media as well as for interdisciplinary researchers interested in the\nintersection of computer science and journalism.", "published": "2018-04-10 13:54:05", "link": "http://arxiv.org/abs/1804.03540v2", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Imagine This! Scripts to Compositions to Videos", "abstract": "Imagining a scene described in natural language with realistic layout and\nappearance of entities is the ultimate test of spatial, visual, and semantic\nworld knowledge. Towards this goal, we present the Composition, Retrieval, and\nFusion Network (CRAFT), a model capable of learning this knowledge from\nvideo-caption data and applying it while generating videos from novel captions.\nCRAFT explicitly predicts a temporal-layout of mentioned entities (characters\nand objects), retrieves spatio-temporal entity segments from a video database\nand fuses them to generate scene videos. Our contributions include sequential\ntraining of components of CRAFT while jointly modeling layout and appearances,\nand losses that encourage learning compositional representations for retrieval.\nWe evaluate CRAFT on semantic fidelity to caption, composition consistency, and\nvisual quality. CRAFT outperforms direct pixel generation approaches and\ngeneralizes well to unseen captions and to unseen video databases with no text\nannotations. We demonstrate CRAFT on FLINTSTONES, a new richly annotated\nvideo-caption dataset with over 25000 videos. For a glimpse of videos generated\nby CRAFT, see https://youtu.be/688Vv86n0z8.", "published": "2018-04-10 15:59:45", "link": "http://arxiv.org/abs/1804.03608v1", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Realtime Active Sound Source Localization for Unmanned Ground Robots\n  Using a Self-Rotational Bi-Microphone Array", "abstract": "This work presents a novel technique that performs both orientation and\ndistance localization of a sound source in a three-dimensional (3D) space using\nonly the interaural time difference (ITD) cue, generated by a newly-developed\nself-rotational bi-microphone robotic platform. The system dynamics is\nestablished in the spherical coordinate frame using a state-space model. The\nobservability analysis of the state-space model shows that the system is\nunobservable when the sound source is placed with elevation angles of $90$ and\n$0$ degree. The proposed method utilizes the difference between the azimuth\nestimates resulting from respectively the 3D and the two-dimensional models to\ncheck the zero-degree-elevation condition and further estimates the elevation\nangle using a polynomial curve fitting approach. Also, the proposed method is\ncapable of detecting a $90$-degree elevation by extracting the zero-ITD signal\n'buried' in noise. Additionally, a distance localization is performed by first\nrotating the microphone array to face toward the sound source and then shifting\nthe microphone perpendicular to the source-robot vector by a predefined\ndistance of a fixed number of steps. The integrated rotational and\ntranslational motions of the microphone array provide a complete orientation\nand distance localization using only the ITD cue. A novel robotic platform\nusing a self-rotational bi-microphone array was also developed for unmanned\nground robots performing sound source localization. The proposed technique was\nfirst tested in simulation and was then verified on the newly-developed robotic\nplatform. Experimental data collected by the microphones installed on a KEMAR\ndummy head were also used to test the proposed technique. All results show the\neffectiveness of the proposed technique.", "published": "2018-04-10 07:04:33", "link": "http://arxiv.org/abs/1804.03372v1", "categories": ["cs.SD", "cs.RO", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Looking to Listen at the Cocktail Party: A Speaker-Independent\n  Audio-Visual Model for Speech Separation", "abstract": "We present a joint audio-visual model for isolating a single speech signal\nfrom a mixture of sounds such as other speakers and background noise. Solving\nthis task using only audio as input is extremely challenging and does not\nprovide an association of the separated speech signals with speakers in the\nvideo. In this paper, we present a deep network-based model that incorporates\nboth visual and auditory signals to solve this task. The visual features are\nused to \"focus\" the audio on desired speakers in a scene and to improve the\nspeech separation quality. To train our joint audio-visual model, we introduce\nAVSpeech, a new dataset comprised of thousands of hours of video segments from\nthe Web. We demonstrate the applicability of our method to classic speech\nseparation tasks, as well as real-world scenarios involving heated interviews,\nnoisy bars, and screaming children, only requiring the user to specify the face\nof the person in the video whose speech they want to isolate. Our method shows\nclear advantage over state-of-the-art audio-only speech separation in cases of\nmixed speech. In addition, our model, which is speaker-independent (trained\nonce, applicable to any speaker), produces better results than recent\naudio-visual speech separation methods that are speaker-dependent (require\ntraining a separate model for each speaker of interest).", "published": "2018-04-10 16:28:59", "link": "http://arxiv.org/abs/1804.03619v2", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-Visual Scene Analysis with Self-Supervised Multisensory Features", "abstract": "The thud of a bouncing ball, the onset of speech as lips open -- when visual\nand audio events occur together, it suggests that there might be a common,\nunderlying event that produced both signals. In this paper, we argue that the\nvisual and audio components of a video signal should be modeled jointly using a\nfused multisensory representation. We propose to learn such a representation in\na self-supervised way, by training a neural network to predict whether video\nframes and audio are temporally aligned. We use this learned representation for\nthree applications: (a) sound source localization, i.e. visualizing the source\nof sound in a video; (b) audio-visual action recognition; and (c) on/off-screen\naudio source separation, e.g. removing the off-screen translator's voice from a\nforeign official's speech. Code, models, and video results are available on our\nwebpage: http://andrewowens.com/multisensory", "published": "2018-04-10 17:36:50", "link": "http://arxiv.org/abs/1804.03641v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
