{"title": "SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with\n  Structured Semantics for Medical Text Mining", "abstract": "Recently, the performance of Pre-trained Language Models (PLMs) has been\nsignificantly improved by injecting knowledge facts to enhance their abilities\nof language understanding. For medical domains, the background knowledge\nsources are especially useful, due to the massive medical terms and their\ncomplicated relations are difficult to understand in text. In this work, we\nintroduce SMedBERT, a medical PLM trained on large-scale medical corpora,\nincorporating deep structured semantic knowledge from neighbors of\nlinked-entity.In SMedBERT, the mention-neighbor hybrid attention is proposed to\nlearn heterogeneous-entity information, which infuses the semantic\nrepresentations of entity types into the homogeneous neighboring entity\nstructure. Apart from knowledge integration as external features, we propose to\nemploy the neighbors of linked-entities in the knowledge graph as additional\nglobal contexts of text mentions, allowing them to communicate via shared\nneighbors, thus enrich their semantic representations. Experiments demonstrate\nthat SMedBERT significantly outperforms strong baselines in various\nknowledge-intensive Chinese medical tasks. It also improves the performance of\nother tasks such as question answering, question matching and natural language\ninference.", "published": "2021-08-20 03:32:01", "link": "http://arxiv.org/abs/2108.08983v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fastformer: Additive Attention Can Be All You Need", "abstract": "Transformer is a powerful model for text understanding. However, it is\ninefficient due to its quadratic complexity to input sequence length. Although\nthere are many methods on Transformer acceleration, they are still either\ninefficient on long sequences or not effective enough. In this paper, we\npropose Fastformer, which is an efficient Transformer model based on additive\nattention. In Fastformer, instead of modeling the pair-wise interactions\nbetween tokens, we first use additive attention mechanism to model global\ncontexts, and then further transform each token representation based on its\ninteraction with global context representations. In this way, Fastformer can\nachieve effective context modeling with linear complexity. Extensive\nexperiments on five datasets show that Fastformer is much more efficient than\nmany existing Transformer models and can meanwhile achieve comparable or even\nbetter long text modeling performance.", "published": "2021-08-20 09:44:44", "link": "http://arxiv.org/abs/2108.09084v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GEDIT: Geographic-Enhanced and Dependency-Guided Tagging for Joint POI\n  and Accessibility Extraction at Baidu Maps", "abstract": "Providing timely accessibility reminders of a point-of-interest (POI) plays a\nvital role in improving user satisfaction of finding places and making visiting\ndecisions. However, it is difficult to keep the POI database in sync with the\nreal-world counterparts due to the dynamic nature of business changes. To\nalleviate this problem, we formulate and present a practical solution that\njointly extracts POI mentions and identifies their coupled accessibility labels\nfrom unstructured text. We approach this task as a sequence tagging problem,\nwhere the goal is to produce <POI name, accessibility label> pairs from\nunstructured text. This task is challenging because of two main issues: (1) POI\nnames are often newly-coined words so as to successfully register new entities\nor brands and (2) there may exist multiple pairs in the text, which\nnecessitates dealing with one-to-many or many-to-one mapping to make each POI\ncoupled with its accessibility label. To this end, we propose a\nGeographic-Enhanced and Dependency-guIded sequence Tagging (GEDIT) model to\nconcurrently address the two challenges. First, to alleviate challenge #1, we\ndevelop a geographic-enhanced pre-trained model to learn the text\nrepresentations. Second, to mitigate challenge #2, we apply a relational graph\nconvolutional network to learn the tree node representations from the parsed\ndependency tree. Finally, we construct a neural sequence tagging model by\nintegrating and feeding the previously pre-learned representations into a CRF\nlayer. Extensive experiments conducted on a real-world dataset demonstrate the\nsuperiority and effectiveness of GEDIT. In addition, it has already been\ndeployed in production at Baidu Maps. Statistics show that the proposed\nsolution can save significant human effort and labor costs to deal with the\nsame amount of documents, which confirms that it is a practical way for POI\naccessibility maintenance.", "published": "2021-08-20 10:55:48", "link": "http://arxiv.org/abs/2108.09104v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Neural Conversation Generation Model via Equivalent Shared Memory\n  Investigation", "abstract": "Conversation generation as a challenging task in Natural Language Generation\n(NLG) has been increasingly attracting attention over the last years. A number\nof recent works adopted sequence-to-sequence structures along with external\nknowledge, which successfully enhanced the quality of generated conversations.\nNevertheless, few works utilized the knowledge extracted from similar\nconversations for utterance generation. Taking conversations in customer\nservice and court debate domains as examples, it is evident that essential\nentities/phrases, as well as their associated logic and inter-relationships can\nbe extracted and borrowed from similar conversation instances. Such information\ncould provide useful signals for improving conversation generation. In this\npaper, we propose a novel reading and memory framework called Deep Reading\nMemory Network (DRMN) which is capable of remembering useful information of\nsimilar conversations for improving utterance generation. We apply our model to\ntwo large-scale conversation datasets of justice and e-commerce fields.\nExperiments prove that the proposed model outperforms the state-of-the-art\napproaches.", "published": "2021-08-20 13:20:14", "link": "http://arxiv.org/abs/2108.09164v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective\n  Transformer", "abstract": "Transformer has achieved great success in NLP. However, the quadratic\ncomplexity of the self-attention mechanism in Transformer makes it inefficient\nin handling long sequences. Many existing works explore to accelerate\nTransformers by computing sparse self-attention instead of a dense one, which\nusually attends to tokens at certain positions or randomly selected tokens.\nHowever, manually selected or random tokens may be uninformative for context\nmodeling. In this paper, we propose Smart Bird, which is an efficient and\neffective Transformer with learnable sparse attention. In Smart Bird, we first\ncompute a sketched attention matrix with a single-head low-dimensional\nTransformer, which aims to find potential important interactions between\ntokens. We then sample token pairs based on their probability scores derived\nfrom the sketched attention matrix to generate different sparse attention index\nmatrices for different attention heads. Finally, we select token embeddings\naccording to the index matrices to form the input of sparse attention networks.\nExtensive experiments on six benchmark datasets for different tasks validate\nthe efficiency and effectiveness of Smart Bird in text modeling.", "published": "2021-08-20 14:22:00", "link": "http://arxiv.org/abs/2108.09193v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Radiological Findings With Normalized Anatomical Information\n  Using a Span-Based BERT Relation Extraction Model", "abstract": "Medical imaging is critical to the diagnosis and treatment of numerous\nmedical problems, including many forms of cancer. Medical imaging reports\ndistill the findings and observations of radiologists, creating an unstructured\ntextual representation of unstructured medical images. Large-scale use of this\ntext-encoded information requires converting the unstructured text to a\nstructured, semantic representation. We explore the extraction and\nnormalization of anatomical information in radiology reports that is associated\nwith radiological findings. We investigate this extraction and normalization\ntask using a span-based relation extraction model that jointly extracts\nentities and relations using BERT. This work examines the factors that\ninfluence extraction and normalization performance, including the body\npart/organ system, frequency of occurrence, span length, and span diversity. It\ndiscusses approaches for improving performance and creating high-quality\nsemantic representations of radiological phenomena.", "published": "2021-08-20 15:02:59", "link": "http://arxiv.org/abs/2108.09211v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CIGLI: Conditional Image Generation from Language & Image", "abstract": "Multi-modal generation has been widely explored in recent years. Current\nresearch directions involve generating text based on an image or vice versa. In\nthis paper, we propose a new task called CIGLI: Conditional Image Generation\nfrom Language and Image. Instead of generating an image based on text as in\ntext-image generation, this task requires the generation of an image from a\ntextual description and an image prompt. We designed a new dataset to ensure\nthat the text description describes information from both images, and that\nsolely analyzing the description is insufficient to generate an image. We then\npropose a novel language-image fusion model which improves the performance over\ntwo established baseline methods, as evaluated by quantitative (automatic) and\nqualitative (human) evaluations. The code and dataset is available at\nhttps://github.com/vincentlux/CIGLI.", "published": "2021-08-20 00:58:42", "link": "http://arxiv.org/abs/2108.08955v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Localize, Group, and Select: Boosting Text-VQA by Scene Text Modeling", "abstract": "As an important task in multimodal context understanding, Text-VQA (Visual\nQuestion Answering) aims at question answering through reading text information\nin images. It differentiates from the original VQA task as Text-VQA requires\nlarge amounts of scene-text relationship understanding, in addition to the\ncross-modal grounding capability. In this paper, we propose Localize, Group,\nand Select (LOGOS), a novel model which attempts to tackle this problem from\nmultiple aspects. LOGOS leverages two grounding tasks to better localize the\nkey information of the image, utilizes scene text clustering to group\nindividual OCR tokens, and learns to select the best answer from different\nsources of OCR (Optical Character Recognition) texts. Experiments show that\nLOGOS outperforms previous state-of-the-art methods on two Text-VQA benchmarks\nwithout using additional OCR annotation data. Ablation studies and analysis\ndemonstrate the capability of LOGOS to bridge different modalities and better\nunderstand scene text.", "published": "2021-08-20 01:31:51", "link": "http://arxiv.org/abs/2108.08965v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SoMeSci- A 5 Star Open Data Gold Standard Knowledge Graph of Software\n  Mentions in Scientific Articles", "abstract": "Knowledge about software used in scientific investigations is important for\nseveral reasons, for instance, to enable an understanding of provenance and\nmethods involved in data handling. However, software is usually not formally\ncited, but rather mentioned informally within the scholarly description of the\ninvestigation, raising the need for automatic information extraction and\ndisambiguation. Given the lack of reliable ground truth data, we present\nSoMeSci (Software Mentions in Science) a gold standard knowledge graph of\nsoftware mentions in scientific articles. It contains high quality annotations\n(IRR: $\\kappa{=}.82$) of 3756 software mentions in 1367 PubMed Central\narticles. Besides the plain mention of the software, we also provide relation\nlabels for additional information, such as the version, the developer, a URL or\ncitations. Moreover, we distinguish between different types, such as\napplication, plugin or programming environment, as well as different types of\nmentions, such as usage or creation. To the best of our knowledge, SoMeSci is\nthe most comprehensive corpus about software mentions in scientific articles,\nproviding training samples for Named Entity Recognition, Relation Extraction,\nEntity Disambiguation, and Entity Linking. Finally, we sketch potential use\ncases and provide baseline results.", "published": "2021-08-20 08:53:03", "link": "http://arxiv.org/abs/2108.09070v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Semantic Communication with Adaptive Universal Transformer", "abstract": "With the development of deep learning (DL), natural language processing (NLP)\nmakes it possible for us to analyze and understand a large amount of language\ntexts. Accordingly, we can achieve a semantic communication in terms of joint\nsemantic source and channel coding over a noisy channel with the help of NLP.\nHowever, the existing method to realize this goal is to use a fixed transformer\nof NLP while ignoring the difference of semantic information contained in each\nsentence. To solve this problem, we propose a new semantic communication system\nbased on Universal Transformer. Compared with the traditional transformer, an\nadaptive circulation mechanism is introduced in the Universal Transformer.\nThrough the introduction of the circulation mechanism, the new semantic\ncommunication system can be more flexible to transmit sentences with different\nsemantic information, and achieve better end-to-end performance under various\nchannel conditions.", "published": "2021-08-20 11:36:24", "link": "http://arxiv.org/abs/2108.09119v3", "categories": ["cs.CL", "eess.SP"], "primary_category": "cs.CL"}
{"title": "Open Relation Modeling: Learning to Define Relations between Entities", "abstract": "Relations between entities can be represented by different instances, e.g., a\nsentence containing both entities or a fact in a Knowledge Graph (KG). However,\nthese instances may not well capture the general relations between entities,\nmay be difficult to understand by humans, even may not be found due to the\nincompleteness of the knowledge source. In this paper, we introduce the Open\nRelation Modeling problem - given two entities, generate a coherent sentence\ndescribing the relation between them. To solve this problem, we propose to\nteach machines to generate definition-like relation descriptions by letting\nthem learn from defining entities. Specifically, we fine-tune Pre-trained\nLanguage Models (PLMs) to produce definitions conditioned on extracted entity\npairs. To help PLMs reason between entities and provide additional relational\nknowledge to PLMs for open relation modeling, we incorporate reasoning paths in\nKGs and include a reasoning path selection mechanism. Experimental results show\nthat our model can generate concise but informative relation descriptions that\ncapture the representative characteristics of entities.", "published": "2021-08-20 16:03:23", "link": "http://arxiv.org/abs/2108.09241v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "One Chatbot Per Person: Creating Personalized Chatbots based on Implicit\n  User Profiles", "abstract": "Personalized chatbots focus on endowing chatbots with a consistent\npersonality to behave like real users, give more informative responses, and\nfurther act as personal assistants. Existing personalized approaches tried to\nincorporate several text descriptions as explicit user profiles. However, the\nacquisition of such explicit profiles is expensive and time-consuming, thus\nbeing impractical for large-scale real-world applications. Moreover, the\nrestricted predefined profile neglects the language behavior of a real user and\ncannot be automatically updated together with the change of user interests. In\nthis paper, we propose to learn implicit user profiles automatically from\nlarge-scale user dialogue history for building personalized chatbots.\nSpecifically, leveraging the benefits of Transformer on language understanding,\nwe train a personalized language model to construct a general user profile from\nthe user's historical responses. To highlight the relevant historical responses\nto the input post, we further establish a key-value memory network of\nhistorical post-response pairs, and build a dynamic post-aware user profile.\nThe dynamic profile mainly describes what and how the user has responded to\nsimilar posts in history. To explicitly utilize users' frequently used words,\nwe design a personalized decoder to fuse two decoding strategies, including\ngenerating a word from the generic vocabulary and copying one word from the\nuser's personalized vocabulary. Experiments on two real-world datasets show the\nsignificant improvement of our model compared with existing methods. Our code\nis available at https://github.com/zhengyima/DHAP", "published": "2021-08-20 20:33:12", "link": "http://arxiv.org/abs/2108.09355v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Conditional Cascade Model for Relational Triple Extraction", "abstract": "Tagging based methods are one of the mainstream methods in relational triple\nextraction. However, most of them suffer from the class imbalance issue\ngreatly. Here we propose a novel tagging based model that addresses this issue\nfrom following two aspects. First, at the model level, we propose a three-step\nextraction framework that can reduce the total number of samples greatly, which\nimplicitly decreases the severity of the mentioned issue. Second, at the\nintra-model level, we propose a confidence threshold based cross entropy loss\nthat can directly neglect some samples in the major classes. We evaluate the\nproposed model on NYT and WebNLG. Extensive experiments show that it can\naddress the mentioned issue effectively and achieves state-of-the-art results\non both datasets. The source code of our model is available at:\nhttps://github.com/neukg/ConCasRTE.", "published": "2021-08-20 03:00:59", "link": "http://arxiv.org/abs/2108.13303v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Extracting Qualitative Causal Structure with Transformer-Based NLP", "abstract": "Qualitative causal relationships compactly express the direction, dependency,\ntemporal constraints, and monotonicity constraints of discrete or continuous\ninteractions in the world. In everyday or academic language, we may express\ninteractions between quantities (e.g., sleep decreases stress), between\ndiscrete events or entities (e.g., a protein inhibits another protein's\ntranscription), or between intentional or functional factors (e.g., hospital\npatients pray to relieve their pain). This paper presents a transformer-based\nNLP architecture that jointly identifies and extracts (1) variables or factors\ndescribed in language, (2) qualitative causal relationships over these\nvariables, and (3) qualifiers and magnitudes that constrain these causal\nrelationships. We demonstrate this approach and include promising results from\nin two use cases, processing textual inputs from academic publications, news\narticles, and social media.", "published": "2021-08-20 20:15:13", "link": "http://arxiv.org/abs/2108.13304v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Group-based Distinctive Image Captioning with Memory Attention", "abstract": "Describing images using natural language is widely known as image captioning,\nwhich has made consistent progress due to the development of computer vision\nand natural language generation techniques. Though conventional captioning\nmodels achieve high accuracy based on popular metrics, i.e., BLEU, CIDEr, and\nSPICE, the ability of captions to distinguish the target image from other\nsimilar images is under-explored. To generate distinctive captions, a few\npioneers employ contrastive learning or re-weighted the ground-truth captions,\nwhich focuses on one single input image. However, the relationships between\nobjects in a similar image group (e.g., items or properties within the same\nalbum or fine-grained events) are neglected. In this paper, we improve the\ndistinctiveness of image captions using a Group-based Distinctive Captioning\nModel (GdisCap), which compares each image with other images in one similar\ngroup and highlights the uniqueness of each image. In particular, we propose a\ngroup-based memory attention (GMA) module, which stores object features that\nare unique among the image group (i.e., with low similarity to objects in other\nimages). These unique object features are highlighted when generating captions,\nresulting in more distinctive captions. Furthermore, the distinctive words in\nthe ground-truth captions are selected to supervise the language decoder and\nGMA. Finally, we propose a new evaluation metric, distinctive word rate\n(DisWordRate) to measure the distinctiveness of captions. Quantitative results\nindicate that the proposed method significantly improves the distinctiveness of\nseveral baseline models, and achieves the state-of-the-art performance on both\naccuracy and distinctiveness. Results of a user study agree with the\nquantitative evaluation and demonstrate the rationality of the new metric\nDisWordRate.", "published": "2021-08-20 12:46:36", "link": "http://arxiv.org/abs/2108.09151v4", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Knowledge Perceived Multi-modal Pretraining in E-commerce", "abstract": "In this paper, we address multi-modal pretraining of product data in the\nfield of E-commerce. Current multi-modal pretraining methods proposed for image\nand text modalities lack robustness in the face of modality-missing and\nmodality-noise, which are two pervasive problems of multi-modal product data in\nreal E-commerce scenarios. To this end, we propose a novel method, K3M, which\nintroduces knowledge modality in multi-modal pretraining to correct the noise\nand supplement the missing of image and text modalities. The modal-encoding\nlayer extracts the features of each modality. The modal-interaction layer is\ncapable of effectively modeling the interaction of multiple modalities, where\nan initial-interactive feature fusion model is designed to maintain the\nindependence of image modality and text modality, and a structure aggregation\nmodule is designed to fuse the information of image, text, and knowledge\nmodalities. We pretrain K3M with three pretraining tasks, including masked\nobject modeling (MOM), masked language modeling (MLM), and link prediction\nmodeling (LPM). Experimental results on a real-world E-commerce dataset and a\nseries of product-based downstream tasks demonstrate that K3M achieves\nsignificant improvements in performances than the baseline and state-of-the-art\nmethods when modality-noise or modality-missing exists.", "published": "2021-08-20 08:01:28", "link": "http://arxiv.org/abs/2109.00895v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Twitter User Representation Using Weakly Supervised Graph Embedding", "abstract": "Social media platforms provide convenient means for users to participate in\nmultiple online activities on various contents and create fast widespread\ninteractions. However, this rapidly growing access has also increased the\ndiverse information, and characterizing user types to understand people's\nlifestyle decisions shared in social media is challenging. In this paper, we\npropose a weakly supervised graph embedding based framework for understanding\nuser types. We evaluate the user embedding learned using weak supervision over\nwell-being related tweets from Twitter, focusing on 'Yoga', 'Keto diet'.\nExperiments on real-world datasets demonstrate that the proposed framework\noutperforms the baselines for detecting user types. Finally, we illustrate data\nanalysis on different types of users (e.g., practitioner vs. promotional) from\nour dataset. While we focus on lifestyle-related tweets (i.e., yoga, keto), our\nmethod for constructing user representation readily generalizes to other\ndomains.", "published": "2021-08-20 03:54:29", "link": "http://arxiv.org/abs/2108.08988v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Airbert: In-domain Pretraining for Vision-and-Language Navigation", "abstract": "Vision-and-language navigation (VLN) aims to enable embodied agents to\nnavigate in realistic environments using natural language instructions. Given\nthe scarcity of domain-specific training data and the high diversity of image\nand language inputs, the generalization of VLN agents to unseen environments\nremains challenging. Recent methods explore pretraining to improve\ngeneralization, however, the use of generic image-caption datasets or existing\nsmall-scale VLN environments is suboptimal and results in limited improvements.\nIn this work, we introduce BnB, a large-scale and diverse in-domain VLN\ndataset. We first collect image-caption (IC) pairs from hundreds of thousands\nof listings from online rental marketplaces. Using IC pairs we next propose\nautomatic strategies to generate millions of VLN path-instruction (PI) pairs.\nWe further propose a shuffling loss that improves the learning of temporal\norder inside PI pairs. We use BnB pretrain our Airbert model that can be\nadapted to discriminative and generative settings and show that it outperforms\nstate of the art for Room-to-Room (R2R) navigation and Remote Referring\nExpression (REVERIE) benchmarks. Moreover, our in-domain pretraining\nsignificantly increases performance on a challenging few-shot VLN evaluation,\nwhere we train the model only on VLN instructions from a few houses.", "published": "2021-08-20 10:58:09", "link": "http://arxiv.org/abs/2108.09105v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Investigation of the Assessment of Infant Vocalizations by Laypersons", "abstract": "The goal of this investigation was the assessment of acoustic infant\nvocalizations by laypersons. More specifically, the goal was to identify (1)\nthe set of most salient classes for infant vocalizations, (2) their\nrelationship to each other and to affective ratings, and (3) proposals for\nclassification schemes based on these labels and relationships. The assessment\nbehavior of laypersons has not yet been investigated, as current infant\nvocalization classification schemes have been aimed at professional and\nscientific applications. The study methodology was based on the Nijmegen\nprotocol, in which participants rated vocalization recordings regarding\nacoustic class labels, and continuous affective scales valence, tense arousal\nand energetic arousal. We determined consensus stimuli ratings as well as\nstimuli similarities based on participant ratings. Our main findings are: (1)\nwe identified 9 salient labels, (2) valence has the overall greatest\nassociation to label ratings, (3) there is a strong association between label\nand valence ratings in the negative valence space, but low association for\nneutral labels, and (4) stimuli separability is highest when grouping labels\ninto 3 - 5 classes. We finally propose two classification schemes based on\nthese findings.", "published": "2021-08-20 14:52:23", "link": "http://arxiv.org/abs/2108.09205v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Sparse Array Capon Beamformer Design Availing Deep Learning", "abstract": "The paper considers sparse array design for receive beamforming achieving\nmaximum signal-to-interference plus noise ratio (MaxSINR). We develop a design\napproach based on supervised neural network where class labels are generated\nusing an efficient sparse beamformer spectral analysis (SBSA) approach. SBSA\nuses explicit information of the unknown narrowband interference environment\nfor training the network and bears close performance to training using\nenumerations, i.e., exhaustive search which is computationally prohibitive for\nlarge arrays. The employed DNN effectively approximates the unknown mapping\nfrom the input received data spatial correlations to the output of sparse\nconfiguration with effective interference mitigation capability. The problem is\nposed as a multi-label classification problem where the selected antenna\nlocations achieving MaxSINR are indicated by the output layer of DNN. In\naddition to evaluating the performance of the DNN in terms of the\nclassification accuracy, we evaluate the performance in terms of the the\nability of the classified sparse array to mitigate interference and maximize\nsignal power. It is shown that even in the case of miss-classification, where\nat least one sensor location doesn't match the optimal locations, the DNN\neffectively learns the sub-optimal sparse configuration which has desirable\nSINR characteristics. This shows the ability of the DNN to learn the proposed\noptimization algorithms, hence paving the way for efficient real-time\nimplementation.", "published": "2021-08-20 01:21:38", "link": "http://arxiv.org/abs/2108.08962v1", "categories": ["eess.SP", "eess.AS", "eess.IV"], "primary_category": "eess.SP"}
{"title": "Parsing Birdsong with Deep Audio Embeddings", "abstract": "Monitoring of bird populations has played a vital role in conservation\nefforts and in understanding biodiversity loss. The automation of this process\nhas been facilitated by both sensing technologies, such as passive acoustic\nmonitoring, and accompanying analytical tools, such as deep learning. However,\nmachine learning models frequently have difficulty generalizing to examples not\nencountered in the training data. In our work, we present a semi-supervised\napproach to identify characteristic calls and environmental noise. We utilize\nseveral methods to learn a latent representation of audio samples, including a\nconvolutional autoencoder and two pre-trained networks, and group the resulting\nembeddings for a domain expert to identify cluster labels. We show that our\napproach can improve classification precision and provide insight into the\nlatent structure of environmental acoustic datasets.", "published": "2021-08-20 14:45:44", "link": "http://arxiv.org/abs/2108.09203v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
