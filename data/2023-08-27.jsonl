{"title": "Situated Natural Language Explanations", "abstract": "Natural language is among the most accessible tools for explaining decisions\nto humans, and large pretrained language models (PLMs) have demonstrated\nimpressive abilities to generate coherent natural language explanations (NLE).\nThe existing NLE research perspectives do not take the audience into account.\nAn NLE can have high textual quality, but it might not accommodate audiences'\nneeds and preference. To address this limitation, we propose an alternative\nperspective, \\textit{situated} NLE. On the evaluation side, we set up automated\nevaluation scores. These scores describe the properties of NLEs in lexical,\nsemantic, and pragmatic categories. On the generation side, we identify three\nprompt engineering techniques and assess their applicability on the situations.\nSituated NLE provides a perspective and facilitates further research on the\ngeneration and evaluation of explanations.", "published": "2023-08-27 14:14:28", "link": "http://arxiv.org/abs/2308.14115v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative AI for Business Strategy: Using Foundation Models to Create\n  Business Strategy Tools", "abstract": "Generative models (foundation models) such as LLMs (large language models)\nare having a large impact on multiple fields. In this work, we propose the use\nof such models for business decision making. In particular, we combine\nunstructured textual data sources (e.g., news data) with multiple foundation\nmodels (namely, GPT4, transformer-based Named Entity Recognition (NER) models\nand Entailment-based Zero-shot Classifiers (ZSC)) to derive IT (information\ntechnology) artifacts in the form of a (sequence of) signed business networks.\nWe posit that such artifacts can inform business stakeholders about the state\nof the market and their own positioning as well as provide quantitative\ninsights into improving their future outlook.", "published": "2023-08-27 19:03:12", "link": "http://arxiv.org/abs/2308.14182v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Confucius: Iterative Tool Learning from Introspection Feedback by\n  Easy-to-Difficult Curriculum", "abstract": "Augmenting large language models (LLMs) with external tools has emerged as a\npromising approach to extending the capability of LLMs. Although some works\nemploy open-source LLMs for the tool learning task, most of them are trained in\na controlled environment in which LLMs only learn to execute the human-provided\ntools. However, selecting proper tools from the large toolset is also a crucial\nability for the tool learning model to be applied in real-world applications.\nExisting methods usually directly employ self-instruction methods to train the\nmodel, which ignores differences in tool complexity. In this paper, we propose\nthe Confucius, a novel tool learning framework to train LLM to use complicated\ntools in real-world scenarios, which contains two main phases: (1) We first\npropose a multi-stage learning method to teach the LLM to use various tools\nfrom an easy-to-difficult curriculum; (2) thenceforth, we propose the Iterative\nSelf-instruct from Introspective Feedback (ISIF) to dynamically construct the\ndataset to improve the ability to use the complicated tool. Extensive\nexperiments conducted on both controlled and real-world settings demonstrate\nthe superiority of our tool learning framework in the real-world application\nscenarios compared to both tuning-free (e.g. ChatGPT, Claude) and tuning-based\nbaselines (e.g. GPT4Tools).", "published": "2023-08-27 07:53:00", "link": "http://arxiv.org/abs/2308.14034v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "An Analysis of On-the-fly Determinization of Finite-state Automata", "abstract": "In this paper we establish an abstraction of on-the-fly determinization of\nfinite-state automata using transition monoids and demonstrate how it can be\napplied to bound the asymptotics. We present algebraic and combinatorial\nproperties that are sufficient for a polynomial state complexity of the\ndeterministic automaton constructed on-the-fly. A special case of our findings\nis that automata with many non-deterministic transitions almost always admit a\ndeterminization of polynomial complexity. Furthermore, we extend our ideas to\nweighted finite-state automata.", "published": "2023-08-27 11:51:27", "link": "http://arxiv.org/abs/2308.14077v1", "categories": ["cs.FL", "cs.CL"], "primary_category": "cs.FL"}
{"title": "Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on\n  Language, Multimodal, and Scientific GPT Models", "abstract": "Generative pre-trained transformer (GPT) models have revolutionized the field\nof natural language processing (NLP) with remarkable performance in various\ntasks and also extend their power to multimodal domains. Despite their success,\nlarge GPT models like GPT-4 face inherent limitations such as considerable\nsize, high computational requirements, complex deployment processes, and closed\ndevelopment loops. These constraints restrict their widespread adoption and\nraise concerns regarding their responsible development and usage. The need for\nuser-friendly, relatively small, and open-sourced alternative GPT models arises\nfrom the desire to overcome these limitations while retaining high performance.\nIn this survey paper, we provide an examination of alternative open-sourced\nmodels of large GPTs, focusing on user-friendly and relatively small models\nthat facilitate easier deployment and accessibility. Through this extensive\nsurvey, we aim to equip researchers, practitioners, and enthusiasts with a\nthorough understanding of user-friendly and relatively small open-sourced\nmodels of large GPTs, their current state, challenges, and future research\ndirections, inspiring the development of more efficient, accessible, and\nversatile GPT models that cater to the broader scientific community and advance\nthe field of general artificial intelligence. The source contents are\ncontinuously updating in https://github.com/GPT-Alternatives/gpt_alternatives.", "published": "2023-08-27 16:14:19", "link": "http://arxiv.org/abs/2308.14149v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Empowering Cross-lingual Abilities of Instruction-tuned Large Language\n  Models by Translation-following demonstrations", "abstract": "The language ability of Large Language Models (LLMs) is often unbalanced\ntowards English because of the imbalance in the distribution of the\npre-training data. This disparity is demanded in further fine-tuning and\naffecting the cross-lingual abilities of LLMs. In this paper, we propose to\nempower Instructiontuned LLMs (It-LLMs) in languages other than English by\nbuilding semantic alignment between them. Hence, we propose CrossAlpaca, an\nIt-LLM with cross-lingual instruction-following and Translation-following\ndemonstrations to improve semantic alignment between languages. We validate our\napproach on the multilingual Question Answering (QA) benchmarks XQUAD and MLQA\nand adapted versions of MMLU and BBH. Our models, tested over six different\nlanguages, outperform the It-LLMs tuned on monolingual data. The final results\nshow that instruction tuning on non-English data is not enough and that\nsemantic alignment can be further improved by Translation-following\ndemonstrations.", "published": "2023-08-27 19:22:12", "link": "http://arxiv.org/abs/2308.14186v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Symbolic and Language Agnostic Large Language Models", "abstract": "We argue that the relative success of large language models (LLMs) is not a\nreflection on the symbolic vs. subsymbolic debate but a reflection on employing\nan appropriate strategy of bottom-up reverse engineering of language at scale.\nHowever, due to the subsymbolic nature of these models whatever knowledge these\nsystems acquire about language will always be buried in millions of\nmicrofeatures (weights) none of which is meaningful on its own. Moreover, and\ndue to their stochastic nature, these models will often fail in capturing\nvarious inferential aspects that are prevalent in natural language. What we\nsuggest here is employing the successful bottom-up strategy in a symbolic\nsetting, producing symbolic, language agnostic and ontologically grounded large\nlanguage models.", "published": "2023-08-27 20:24:33", "link": "http://arxiv.org/abs/2308.14199v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "MedAlign: A Clinician-Generated Dataset for Instruction Following with\n  Electronic Medical Records", "abstract": "The ability of large language models (LLMs) to follow natural language\ninstructions with human-level fluency suggests many opportunities in healthcare\nto reduce administrative burden and improve quality of care. However,\nevaluating LLMs on realistic text generation tasks for healthcare remains\nchallenging. Existing question answering datasets for electronic health record\n(EHR) data fail to capture the complexity of information needs and\ndocumentation burdens experienced by clinicians. To address these challenges,\nwe introduce MedAlign, a benchmark dataset of 983 natural language instructions\nfor EHR data. MedAlign is curated by 15 clinicians (7 specialities), includes\nclinician-written reference responses for 303 instructions, and provides 276\nlongitudinal EHRs for grounding instruction-response pairs. We used MedAlign to\nevaluate 6 general domain LLMs, having clinicians rank the accuracy and quality\nof each LLM response. We found high error rates, ranging from 35% (GPT-4) to\n68% (MPT-7B-Instruct), and an 8.3% drop in accuracy moving from 32k to 2k\ncontext lengths for GPT-4. Finally, we report correlations between clinician\nrankings and automated natural language generation metrics as a way to rank\nLLMs without human review. We make MedAlign available under a research data use\nagreement to enable LLM evaluations on tasks aligned with clinician needs and\npreferences.", "published": "2023-08-27 12:24:39", "link": "http://arxiv.org/abs/2308.14089v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models Streamline Automated Machine Learning for Clinical\n  Studies", "abstract": "A knowledge gap persists between machine learning (ML) developers (e.g., data\nscientists) and practitioners (e.g., clinicians), hampering the full\nutilization of ML for clinical data analysis. We investigated the potential of\nthe ChatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this\ngap and perform ML analyses efficiently. Real-world clinical datasets and study\ndetails from large trials across various medical specialties were presented to\nChatGPT ADA without specific guidance. ChatGPT ADA autonomously developed\nstate-of-the-art ML models based on the original study's training data to\npredict clinical outcomes such as cancer development, cancer progression,\ndisease complications, or biomarkers such as pathogenic gene sequences.\nFollowing the re-implementation and optimization of the published models, the\nhead-to-head comparison of the ChatGPT ADA-crafted ML models and their\nrespective manually crafted counterparts revealed no significant differences in\ntraditional performance metrics (P>0.071). Strikingly, the ChatGPT ADA-crafted\nML models often outperformed their counterparts. In conclusion, ChatGPT ADA\noffers a promising avenue to democratize ML in medicine by simplifying complex\ndata analyses, yet should enhance, not replace, specialized training and\nresources, to promote broader applications in medical research and practice.", "published": "2023-08-27 14:28:38", "link": "http://arxiv.org/abs/2308.14120v5", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Detecting Language Model Attacks with Perplexity", "abstract": "A novel hack involving Large Language Models (LLMs) has emerged, exploiting\nadversarial suffixes to deceive models into generating perilous responses. Such\njailbreaks can trick LLMs into providing intricate instructions to a malicious\nuser for creating explosives, orchestrating a bank heist, or facilitating the\ncreation of offensive content. By evaluating the perplexity of queries with\nadversarial suffixes using an open-source LLM (GPT-2), we found that they have\nexceedingly high perplexity values. As we explored a broad range of regular\n(non-adversarial) prompt varieties, we concluded that false positives are a\nsignificant challenge for plain perplexity filtering. A Light-GBM trained on\nperplexity and token length resolved the false positives and correctly detected\nmost adversarial attacks in the test set.", "published": "2023-08-27 15:20:06", "link": "http://arxiv.org/abs/2308.14132v3", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Vision-Language Mechanistic Interpretability: A Causal Tracing\n  Tool for BLIP", "abstract": "Mechanistic interpretability seeks to understand the neural mechanisms that\nenable specific behaviors in Large Language Models (LLMs) by leveraging\ncausality-based methods. While these approaches have identified neural circuits\nthat copy spans of text, capture factual knowledge, and more, they remain\nunusable for multimodal models since adapting these tools to the\nvision-language domain requires considerable architectural changes. In this\nwork, we adapt a unimodal causal tracing tool to BLIP to enable the study of\nthe neural mechanisms underlying image-conditioned text generation. We\ndemonstrate our approach on a visual question answering dataset, highlighting\nthe causal relevance of later layer representations for all tokens.\nFurthermore, we release our BLIP causal tracing tool as open source to enable\nfurther experimentation in vision-language mechanistic interpretability by the\ncommunity. Our code is available at\nhttps://github.com/vedantpalit/Towards-Vision-Language-Mechanistic-Interpretability.", "published": "2023-08-27 18:46:47", "link": "http://arxiv.org/abs/2308.14179v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Generations of Knowledge Graphs: The Crazy Ideas and the Business Impact", "abstract": "Knowledge Graphs (KGs) have been used to support a wide range of\napplications, from web search to personal assistant. In this paper, we describe\nthree generations of knowledge graphs: entity-based KGs, which have been\nsupporting general search and question answering (e.g., at Google and Bing);\ntext-rich KGs, which have been supporting search and recommendations for\nproducts, bio-informatics, etc. (e.g., at Amazon and Alibaba); and the emerging\nintegration of KGs and LLMs, which we call dual neural KGs. We describe the\ncharacteristics of each generation of KGs, the crazy ideas behind the scenes in\nconstructing such KGs, and the techniques developed over time to enable\nindustry impact. In addition, we use KGs as examples to demonstrate a recipe to\nevolve research ideas from innovations to production practice, and then to the\nnext level of innovations, to advance both science and business.", "published": "2023-08-27 22:35:27", "link": "http://arxiv.org/abs/2308.14217v1", "categories": ["cs.DB", "cs.AI", "cs.CL"], "primary_category": "cs.DB"}
{"title": "VoiceBank-2023: A Multi-Speaker Mandarin Speech Corpus for Constructing\n  Personalized TTS Systems for the Speech Impaired", "abstract": "Services of personalized TTS systems for the Mandarin-speaking speech\nimpaired are rarely mentioned. Taiwan started the VoiceBanking project in 2020,\naiming to build a complete set of services to deliver personalized Mandarin TTS\nsystems to amyotrophic lateral sclerosis patients. This paper reports the\ncorpus design, corpus recording, data purging and correction for the corpus,\nand evaluations of the developed personalized TTS systems, for the VoiceBanking\nproject. The developed corpus is named after the VoiceBank-2023 speech corpus\nbecause of its release year. The corpus contains 29.78 hours of utterances with\nprompts of short paragraphs and common phrases spoken by 111 native Mandarin\nspeakers. The corpus is labeled with information about gender, degree of speech\nimpairment, types of users, transcription, SNRs, and speaking rates. The\nVoiceBank-2023 is available by request for non-commercial use and welcomes all\nparties to join the VoiceBanking project to improve the services for the speech\nimpaired.", "published": "2023-08-27 07:35:30", "link": "http://arxiv.org/abs/2308.14763v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fairness and Privacy in Voice Biometrics:A Study of Gender Influences\n  Using wav2vec 2.0", "abstract": "This study investigates the impact of gender information on utility, privacy,\nand fairness in voice biometric systems, guided by the General Data Protection\nRegulation (GDPR) mandates, which underscore the need for minimizing the\nprocessing and storage of private and sensitive data, and ensuring fairness in\nautomated decision-making systems. We adopt an approach that involves the\nfine-tuning of the wav2vec 2.0 model for speaker verification tasks, evaluating\npotential gender-related privacy vulnerabilities in the process. Gender\ninfluences during the fine-tuning process were employed to enhance fairness and\nprivacy in order to emphasise or obscure gender information within the\nspeakers' embeddings. Results from VoxCeleb datasets indicate our adversarial\nmodel increases privacy against uninformed attacks, yet slightly diminishes\nspeaker verification performance compared to the non-adversarial model.\nHowever, the model's efficacy reduces against informed attacks. Analysis of\nsystem performance was conducted to identify potential gender biases, thus\nhighlighting the need for further research to understand and improve the\ndelicate interplay between utility, privacy, and equity in voice biometric\nsystems.", "published": "2023-08-27 09:04:54", "link": "http://arxiv.org/abs/2308.14049v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Anomalous Sound Detection Using Self-Attention-Based Frequency Pattern\n  Analysis of Machine Sounds", "abstract": "Different machines can exhibit diverse frequency patterns in their emitted\nsound. This feature has been recently explored in anomaly sound detection and\nreached state-of-the-art performance. However, existing methods rely on the\nmanual or empirical determination of the frequency filter by observing the\neffective frequency range in the training data, which may be impractical for\ngeneral application. This paper proposes an anomalous sound detection method\nusing self-attention-based frequency pattern analysis and spectral-temporal\ninformation fusion. Our experiments demonstrate that the self-attention module\nautomatically and adaptively analyses the effective frequencies of a machine\nsound and enhances that information in the spectral feature representation.\nWith spectral-temporal information fusion, the obtained audio feature\neventually improves the anomaly detection performance on the DCASE 2020\nChallenge Task 2 dataset.", "published": "2023-08-27 10:21:32", "link": "http://arxiv.org/abs/2308.14063v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Subdomain Adversarial Network for Cross-Subject EEG-based Emotion\n  Recognition", "abstract": "The individual difference between subjects is significant in EEG-based\nemotion recognition, resulting in the difficulty of sharing the model across\nsubjects. Previous studies use domain adaptation algorithms to minimize the\nglobal domain discrepancy while ignoring the class information, which may cause\nmisalignment of subdomains and reduce model performance. This paper proposes a\nmulti-subdomain adversarial network (MSAN) for cross-subject EEG-based emotion\nrecognition. MSAN uses adversarial training to model the discrepancy in the\nglobal domain and subdomain to reduce the intra-class distance and enlarge the\ninter-class distance. In addition, MSAN initializes parameters through a\npre-trained autoencoder to ensure the stability and convertibility of the\nmodel. The experimental results show that the accuracy of MSAN is improved by\n30.02\\% on the SEED dataset comparing with the nontransfer method.", "published": "2023-08-27 09:57:46", "link": "http://arxiv.org/abs/2308.14059v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
