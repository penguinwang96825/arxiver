{"title": "X-METRA-ADA: Cross-lingual Meta-Transfer Learning Adaptation to Natural\n  Language Understanding and Question Answering", "abstract": "Multilingual models, such as M-BERT and XLM-R, have gained increasing\npopularity, due to their zero-shot cross-lingual transfer learning\ncapabilities. However, their generalization ability is still inconsistent for\ntypologically diverse languages and across different benchmarks. Recently,\nmeta-learning has garnered attention as a promising technique for enhancing\ntransfer learning under low-resource scenarios: particularly for cross-lingual\ntransfer in Natural Language Understanding (NLU). In this work, we propose\nX-METRA-ADA, a cross-lingual MEta-TRAnsfer learning ADAptation approach for\nNLU. Our approach adapts MAML, an optimization-based meta-learning approach, to\nlearn to adapt to new languages. We extensively evaluate our framework on two\nchallenging cross-lingual NLU tasks: multilingual task-oriented dialog and\ntypologically diverse question answering. We show that our approach outperforms\nnaive fine-tuning, reaching competitive performance on both tasks for most\nlanguages. Our analysis reveals that X-METRA-ADA can leverage limited data for\nfaster adaptation.", "published": "2021-04-20 00:13:35", "link": "http://arxiv.org/abs/2104.09696v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Problems and Countermeasures in Natural Language Processing Evaluation", "abstract": "Evaluation in natural language processing guides and promotes research on\nmodels and methods. In recent years, new evalua-tion data sets and evaluation\ntasks have been continuously proposed. At the same time, a series of problems\nexposed by ex-isting evaluation have also restricted the progress of natural\nlanguage processing technology. Starting from the concept, com-position,\ndevelopment and meaning of natural language evaluation, this article classifies\nand summarizes the tasks and char-acteristics of mainstream natural language\nevaluation, and then summarizes the problems and causes of natural language\npro-cessing evaluation. Finally, this article refers to the human language\nability evaluation standard, puts forward the concept of human-like machine\nlanguage ability evaluation, and proposes a series of basic principles and\nimplementation ideas for hu-man-like machine language ability evaluation from\nthe three aspects of reliability, difficulty and validity.", "published": "2021-04-20 01:35:16", "link": "http://arxiv.org/abs/2104.09712v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Temporal-Drift: A Simple Approach to Keep NER Models Crisp", "abstract": "Performance of neural models for named entity recognition degrades over time,\nbecoming stale. This degradation is due to temporal drift, the change in our\ntarget variables' statistical properties over time. This issue is especially\nproblematic for social media data, where topics change rapidly. In order to\nmitigate the problem, data annotation and retraining of models is common.\nDespite its usefulness, this process is expensive and time-consuming, which\nmotivates new research on efficient model updating. In this paper, we propose\nan intuitive approach to measure the potential trendiness of tweets and use\nthis metric to select the most informative instances to use for training. We\nconduct experiments on three state-of-the-art models on the Temporal Twitter\nDataset. Our approach shows larger increases in prediction accuracy with less\ntraining data than the alternatives, making it an attractive, practical\nsolution.", "published": "2021-04-20 03:35:25", "link": "http://arxiv.org/abs/2104.09742v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seed Word Selection for Weakly-Supervised Text Classification with\n  Unsupervised Error Estimation", "abstract": "Weakly-supervised text classification aims to induce text classifiers from\nonly a few user-provided seed words. The vast majority of previous work assumes\nhigh-quality seed words are given. However, the expert-annotated seed words are\nsometimes non-trivial to come up with. Furthermore, in the weakly-supervised\nlearning setting, we do not have any labeled document to measure the seed\nwords' efficacy, making the seed word selection process \"a walk in the dark\".\nIn this work, we remove the need for expert-curated seed words by first mining\n(noisy) candidate seed words associated with the category names. We then train\ninterim models with individual candidate seed words. Lastly, we estimate the\ninterim models' error rate in an unsupervised manner. The seed words that yield\nthe lowest estimated error rates are added to the final seed word set. A\ncomprehensive evaluation of six binary classification tasks on four popular\ndatasets demonstrates that the proposed method outperforms a baseline using\nonly category name seed words and obtained comparable performance as a\ncounterpart using expert-annotated seed words.", "published": "2021-04-20 05:10:40", "link": "http://arxiv.org/abs/2104.09765v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Addressing the Vulnerability of NMT in Input Perturbations", "abstract": "Neural Machine Translation (NMT) has achieved significant breakthrough in\nperformance but is known to suffer vulnerability to input perturbations. As\nreal input noise is difficult to predict during training, robustness is a big\nissue for system deployment. In this paper, we improve the robustness of NMT\nmodels by reducing the effect of noisy words through a Context-Enhanced\nReconstruction (CER) approach. CER trains the model to resist noise in two\nsteps: (1) perturbation step that breaks the naturalness of input sequence with\nmade-up words; (2) reconstruction step that defends the noise propagation by\ngenerating better and more robust contextual representation. Experimental\nresults on Chinese-English (ZH-EN) and French-English (FR-EN) translation tasks\ndemonstrate robustness improvement on both news and social media text. Further\nfine-tuning experiments on social media text show our approach can converge at\na higher position and provide a better adaptation.", "published": "2021-04-20 07:52:58", "link": "http://arxiv.org/abs/2104.09810v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WASSA@IITK at WASSA 2021: Multi-task Learning and Transformer Finetuning\n  for Emotion Classification and Empathy Prediction", "abstract": "This paper describes our contribution to the WASSA 2021 shared task on\nEmpathy Prediction and Emotion Classification. The broad goal of this task was\nto model an empathy score, a distress score and the overall level of emotion of\nan essay written in response to a newspaper article associated with harm to\nsomeone. We have used the ELECTRA model abundantly and also advanced deep\nlearning approaches like multi-task learning. Additionally, we also leveraged\nstandard machine learning techniques like ensembling. Our system achieves a\nPearson Correlation Coefficient of 0.533 on sub-task I and a macro F1 score of\n0.5528 on sub-task II. We ranked 1st in Emotion Classification sub-task and 3rd\nin Empathy Prediction sub-task", "published": "2021-04-20 08:24:10", "link": "http://arxiv.org/abs/2104.09827v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Frustratingly Easy Edit-based Linguistic Steganography with a Masked\n  Language Model", "abstract": "With advances in neural language models, the focus of linguistic\nsteganography has shifted from edit-based approaches to generation-based ones.\nWhile the latter's payload capacity is impressive, generating genuine-looking\ntexts remains challenging. In this paper, we revisit edit-based linguistic\nsteganography, with the idea that a masked language model offers an\noff-the-shelf solution. The proposed method eliminates painstaking rule\nconstruction and has a high payload capacity for an edit-based model. It is\nalso shown to be more secure against automatic detection than a\ngeneration-based method while offering better control of the security/payload\ncapacity trade-off.", "published": "2021-04-20 08:35:53", "link": "http://arxiv.org/abs/2104.09833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HYPER^2: Hyperbolic Poincare Embedding for Hyper-Relational Link\n  Prediction", "abstract": "Link Prediction, addressing the issue of completing KGs with missing facts,\nhas been broadly studied. However, less light is shed on the ubiquitous\nhyper-relational KGs. Most existing hyper-relational KG embedding models still\ntear an n-ary fact into smaller tuples, neglecting the indecomposability of\nsome n-ary facts. While other frameworks work for certain arity facts only or\nignore the significance of primary triple. In this paper, we represent an n-ary\nfact as a whole, simultaneously keeping the integrity of n-ary fact and\nmaintaining the vital role that the primary triple plays. In addition, we\ngeneralize hyperbolic Poincar\\'e embedding from binary to arbitrary arity data,\nwhich has not been studied yet. To tackle the weak expressiveness and high\ncomplexity issue, we propose HYPER^2 which is qualified for capturing the\ninteraction between entities within and beyond triple through information\naggregation on the tangent space. Extensive experiments demonstrate HYPER^2\nachieves superior performance to its translational and deep analogues,\nimproving SOTA by up to 34.5\\% with relatively few dimensions. Moreover, we\nstudy the side effect of literals and we theoretically and experimentally\ncompare the computational complexity of HYPER^2 against several best performing\nbaselines, HYPER^2 is 49-61 times quicker than its counterparts.", "published": "2021-04-20 10:06:05", "link": "http://arxiv.org/abs/2104.09871v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grammatical Error Generation Based on Translated Fragments", "abstract": "We perform neural machine translation of sentence fragments in order to\ncreate large amounts of training data for English grammatical error correction.\nOur method aims at simulating mistakes made by second language learners, and\nproduces a wider range of non-native style language in comparison to\nstate-of-the-art synthetic data creation methods. In addition to purely\ngrammatical errors, our approach generates other types of errors, such as\nlexical errors. We perform grammatical error correction experiments using\nneural sequence-to-sequence models, and carry out quantitative and qualitative\nevaluation. A model trained on data created using our proposed method is shown\nto outperform a baseline model on test data with a high proportion of errors.", "published": "2021-04-20 12:43:40", "link": "http://arxiv.org/abs/2104.09933v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UIT-ISE-NLP at SemEval-2021 Task 5: Toxic Spans Detection with\n  BiLSTM-CRF and ToxicBERT Comment Classification", "abstract": "We present our works on SemEval-2021 Task 5 about Toxic Spans Detection. This\ntask aims to build a model for identifying toxic words in whole posts. We use\nthe BiLSTM-CRF model combining with ToxicBERT Classification to train the\ndetection model for identifying toxic words in posts. Our model achieves 62.23%\nby F1-score on the Toxic Spans Detection task.", "published": "2021-04-20 16:32:56", "link": "http://arxiv.org/abs/2104.10100v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Cognitive Models of Emotions with Representation Learning", "abstract": "We present a novel deep learning-based framework to generate embedding\nrepresentations of fine-grained emotions that can be used to computationally\ndescribe psychological models of emotions. Our framework integrates a\ncontextualized embedding encoder with a multi-head probing model that enables\nto interpret dynamically learned representations optimized for an emotion\nclassification task. Our model is evaluated on the Empathetic Dialogue dataset\nand shows the state-of-the-art result for classifying 32 emotions. Our layer\nanalysis can derive an emotion graph to depict hierarchical relations among the\nemotions. Our emotion representations can be used to generate an emotion wheel\ndirectly comparable to the one from Plutchik's\\LN model, and also augment the\nvalues of missing emotions in the PAD emotional state model.", "published": "2021-04-20 16:55:15", "link": "http://arxiv.org/abs/2104.10117v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Solving Multimodal Comprehension", "abstract": "This paper targets the problem of procedural multimodal machine comprehension\n(M3C). This task requires an AI to comprehend given steps of multimodal\ninstructions and then answer questions. Compared to vanilla machine\ncomprehension tasks where an AI is required only to understand a textual input,\nprocedural M3C is more challenging as the AI needs to comprehend both the\ntemporal and causal factors along with multimodal inputs. Recently Yagcioglu et\nal. [35] introduced RecipeQA dataset to evaluate M3C. Our first contribution is\nthe introduction of two new M3C datasets- WoodworkQA and DecorationQA with 16K\nand 10K instructional procedures, respectively. We then evaluate M3C using a\ntextual cloze style question-answering task and highlight an inherent bias in\nthe question answer generation method from [35] that enables a naive baseline\nto cheat by learning from only answer choices. This naive baseline performs\nsimilar to a popular method used in question answering- Impatient Reader [6]\nthat uses attention over both the context and the query. We hypothesized that\nthis naturally occurring bias present in the dataset affects even the best\nperforming model. We verify our proposed hypothesis and propose an algorithm\ncapable of modifying the given dataset to remove the bias elements. Finally, we\nreport our performance on the debiased dataset with several strong baselines.\nWe observe that the performance of all methods falls by a margin of 8% - 16%\nafter correcting for the bias. We hope these datasets and the analysis will\nprovide valuable benchmarks and encourage further research in this area.", "published": "2021-04-20 17:30:27", "link": "http://arxiv.org/abs/2104.10139v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Immediate Applicability of Pose Estimation for Sign\n  Language Recognition", "abstract": "Signed languages are visual languages produced by the movement of the hands,\nface, and body. In this paper, we evaluate representations based on skeleton\nposes, as these are explainable, person-independent, privacy-preserving,\nlow-dimensional representations. Basically, skeletal representations generalize\nover an individual's appearance and background, allowing us to focus on the\nrecognition of motion. But how much information is lost by the skeletal\nrepresentation? We perform two independent studies using two state-of-the-art\npose estimation systems. We analyze the applicability of the pose estimation\nsystems to sign language recognition by evaluating the failure cases of the\nrecognition models. Importantly, this allows us to characterize the current\nlimitations of skeletal pose estimation approaches in sign language\nrecognition.", "published": "2021-04-20 14:41:45", "link": "http://arxiv.org/abs/2104.10166v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Event Plausibility with Consistent Conceptual Abstraction", "abstract": "Understanding natural language requires common sense, one aspect of which is\nthe ability to discern the plausibility of events. While distributional models\n-- most recently pre-trained, Transformer language models -- have demonstrated\nimprovements in modeling event plausibility, their performance still falls\nshort of humans'. In this work, we show that Transformer-based plausibility\nmodels are markedly inconsistent across the conceptual classes of a lexical\nhierarchy, inferring that \"a person breathing\" is plausible while \"a dentist\nbreathing\" is not, for example. We find this inconsistency persists even when\nmodels are softly injected with lexical knowledge, and we present a simple\npost-hoc method of forcing model consistency that improves correlation with\nhuman plausibility judgements.", "published": "2021-04-20 21:08:32", "link": "http://arxiv.org/abs/2104.10247v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Subsentence Extraction from Text Using Coverage-Based Deep Learning\n  Language Models", "abstract": "Sentiment prediction remains a challenging and unresolved task in various\nresearch fields, including psychology, neuroscience, and computer science. This\nstems from its high degree of subjectivity and limited input sources that can\neffectively capture the actual sentiment. This can be even more challenging\nwith only text-based input. Meanwhile, the rise of deep learning and an\nunprecedented large volume of data have paved the way for artificial\nintelligence to perform impressively accurate predictions or even human-level\nreasoning. Drawing inspiration from this, we propose a coverage-based sentiment\nand subsentence extraction system that estimates a span of input text and\nrecursively feeds this information back to the networks. The predicted\nsubsentence consists of auxiliary information expressing a sentiment. This is\nan important building block for enabling vivid and epic sentiment delivery\n(within the scope of this paper) and for other natural language processing\ntasks such as text summarisation and Q&A. Our approach outperforms the\nstate-of-the-art approaches by a large margin in subsentence prediction (i.e.,\nAverage Jaccard scores from 0.72 to 0.89). For the evaluation, we designed\nrigorous experiments consisting of 24 ablation studies. Finally, our learned\nlessons are returned to the community by sharing software packages and a public\ndataset that can reproduce the results presented in this paper.", "published": "2021-04-20 06:24:49", "link": "http://arxiv.org/abs/2104.09777v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Identifying Helpful Sentences in Product Reviews", "abstract": "In recent years online shopping has gained momentum and became an important\nvenue for customers wishing to save time and simplify their shopping process. A\nkey advantage of shopping online is the ability to read what other customers\nare saying about products of interest. In this work, we aim to maintain this\nadvantage in situations where extreme brevity is needed, for example, when\nshopping by voice. We suggest a novel task of extracting a single\nrepresentative helpful sentence from a set of reviews for a given product. The\nselected sentence should meet two conditions: first, it should be helpful for a\npurchase decision and second, the opinion it expresses should be supported by\nmultiple reviewers. This task is closely related to the task of Multi Document\nSummarization in the product reviews domain but differs in its objective and\nits level of conciseness. We collect a dataset in English of sentence\nhelpfulness scores via crowd-sourcing and demonstrate its reliability despite\nthe inherent subjectivity involved. Next, we describe a complete model that\nextracts representative helpful sentences with positive and negative sentiment\ntowards the product and demonstrate that it outperforms several baselines.", "published": "2021-04-20 07:09:22", "link": "http://arxiv.org/abs/2104.09792v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring Shifts in Attitudes Towards COVID-19 Measures in Belgium Using\n  Multilingual BERT", "abstract": "We classify seven months' worth of Belgian COVID-related Tweets using\nmultilingual BERT and relate them to their governments' COVID measures. We\nclassify Tweets by their stated opinion on Belgian government curfew measures\n(too strict, ok, too loose). We examine the change in topics discussed and\nviews expressed over time and in reference to dates of related events such as\nimplementation of new measures or COVID-19 related announcements in the media.", "published": "2021-04-20 13:17:56", "link": "http://arxiv.org/abs/2104.09947v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Beyond Fair Pay: Ethical Implications of NLP Crowdsourcing", "abstract": "The use of crowdworkers in NLP research is growing rapidly, in tandem with\nthe exponential increase in research production in machine learning and AI.\nEthical discussion regarding the use of crowdworkers within the NLP research\ncommunity is typically confined in scope to issues related to labor conditions\nsuch as fair pay. We draw attention to the lack of ethical considerations\nrelated to the various tasks performed by workers, including labeling,\nevaluation, and production. We find that the Final Rule, the common ethical\nframework used by researchers, did not anticipate the use of online\ncrowdsourcing platforms for data collection, resulting in gaps between the\nspirit and practice of human-subjects ethics in NLP research. We enumerate\ncommon scenarios where crowdworkers performing NLP tasks are at risk of harm.\nWe thus recommend that researchers evaluate these risks by considering the\nthree ethical principles set up by the Belmont Report. We also clarify some\ncommon misconceptions regarding the Institutional Review Board (IRB)\napplication. We hope this paper will serve to reopen the discussion within our\ncommunity regarding the ethical use of crowdworkers.", "published": "2021-04-20 16:30:59", "link": "http://arxiv.org/abs/2104.10097v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Efficient Retrieval Optimized Multi-task Learning", "abstract": "Recently, there have been significant advances in neural methods for tackling\nknowledge-intensive tasks such as open domain question answering (QA). These\nadvances are fueled by combining large pre-trained language models with\nlearnable retrieval of documents. Majority of these models use separate\nencoders for learning query representation, passage representation for the\nretriever and an additional encoder for the downstream task. Using separate\nencoders for each stage/task occupies a lot of memory and makes it difficult to\nscale to a large number of tasks. In this paper, we propose a novel Retrieval\nOptimized Multi-task (ROM) framework for jointly training self-supervised\ntasks, knowledge retrieval, and extractive question answering. Our ROM approach\npresents a unified and generalizable framework that enables scaling efficiently\nto multiple tasks, varying levels of supervision, and optimization choices such\nas different learning schedules without changing the model architecture. It\nalso provides the flexibility of changing the encoders without changing the\narchitecture of the system. Using our framework, we achieve comparable or\nbetter performance than recent methods on QA, while drastically reducing the\nnumber of parameters.", "published": "2021-04-20 17:16:34", "link": "http://arxiv.org/abs/2104.10129v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Hidden Biases in Unreliable News Detection Datasets", "abstract": "Automatic unreliable news detection is a research problem with great\npotential impact. Recently, several papers have shown promising results on\nlarge-scale news datasets with models that only use the article itself without\nresorting to any fact-checking mechanism or retrieving any supporting evidence.\nIn this work, we take a closer look at these datasets. While they all provide\nvaluable resources for future research, we observe a number of problems that\nmay lead to results that do not generalize in more realistic settings.\nSpecifically, we show that selection bias during data collection leads to\nundesired artifacts in the datasets. In addition, while most systems train and\npredict at the level of individual articles, overlapping article sources in the\ntraining and evaluation data can provide a strong confounding factor that\nmodels can exploit. In the presence of this confounding factor, the models can\nachieve good performance by directly memorizing the site-label mapping instead\nof modeling the real task of unreliable news detection. We observed a\nsignificant drop (>10%) in accuracy for all models tested in a clean split with\nno train/test source overlap. Using the observations and experimental results,\nwe provide practical suggestions on how to create more reliable datasets for\nthe unreliable news detection task. We suggest future dataset creation include\na simple model as a difficulty/bias probe and future model development use a\nclean non-overlapping site and date split.", "published": "2021-04-20 17:16:41", "link": "http://arxiv.org/abs/2104.10130v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense\n  Reasoning Tasks", "abstract": "Integrating external knowledge into commonsense reasoning tasks has shown\nprogress in resolving some, but not all, knowledge gaps in these tasks. For\nknowledge integration to yield peak performance, it is critical to select a\nknowledge graph (KG) that is well-aligned with the given task's objective. We\npresent an approach to assess how well a candidate KG can correctly identify\nand accurately fill in gaps of reasoning for a task, which we call KG-to-task\nmatch. We show this KG-to-task match in 3 phases: knowledge-task\nidentification, knowledge-task alignment, and knowledge-task integration. We\nalso analyze our transformer-based KG-to-task models via commonsense probes to\nmeasure how much knowledge is captured in these models before and after KG\nintegration. Empirically, we investigate KG matches for the SocialIQA (SIQA)\n(Sap et al., 2019b), Physical IQA (PIQA) (Bisk et al., 2020), and MCScript2.0\n(Ostermann et al., 2019) datasets with 3 diverse KGs: ATOMIC (Sap et al.,\n2019a), ConceptNet (Speer et al., 2017), and an automatically constructed\ninstructional KG based on WikiHow (Koupaee and Wang, 2018). With our methods we\nare able to demonstrate that ATOMIC, an event-inference focused KG, is the best\nmatch for SIQA and MCScript2.0, and that the taxonomic ConceptNet and\nWikiHow-based KGs are the best matches for PIQA across all 3 analysis phases.\nWe verify our methods and findings with human evaluation.", "published": "2021-04-20 18:23:45", "link": "http://arxiv.org/abs/2104.10193v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analyzing COVID-19 Tweets with Transformer-based Language Models", "abstract": "This paper describes a method for using Transformer-based Language Models\n(TLMs) to understand public opinion from social media posts. In this approach,\nwe train a set of GPT models on several COVID-19 tweet corpora that reflect\npopulations of users with distinctive views. We then use prompt-based queries\nto probe these models to reveal insights into the biases and opinions of the\nusers. We demonstrate how this approach can be used to produce results which\nresemble polling the public on diverse social, political and public health\nissues. The results on the COVID-19 tweet data show that transformer language\nmodels are promising tools that can help us understand public opinions on\nsocial media at scale.", "published": "2021-04-20 21:45:33", "link": "http://arxiv.org/abs/2104.10259v3", "categories": ["cs.CL", "cs.CY", "J.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "Novel Aficionados and Doppelg\u00e4ngers: a referential task for semantic\n  representations of individual entities", "abstract": "In human semantic cognition, proper names (names which refer to individual\nentities) are harder to learn and retrieve than common nouns. This seems to be\nthe case for machine learning algorithms too, but the linguistic and\ndistributional reasons for this behaviour have not been investigated in depth\nso far. To tackle this issue, we show that the semantic distinction between\nproper names and common nouns is reflected in their linguistic distributions by\nemploying an original task for distributional semantics, the Doppelg\\\"anger\ntest, an extensive set of models, and a new dataset, the Novel Aficionados\ndataset. The results indicate that the distributional representations of\ndifferent individual entities are less clearly distinguishable from each other\nthan those of common nouns, an outcome which intriguingly mirrors human\ncognition.", "published": "2021-04-20 22:24:19", "link": "http://arxiv.org/abs/2104.10270v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GraghVQA: Language-Guided Graph Neural Networks for Graph-based Visual\n  Question Answering", "abstract": "Images are more than a collection of objects or attributes -- they represent\na web of relationships among interconnected objects. Scene Graph has emerged as\na new modality for a structured graphical representation of images. Scene Graph\nencodes objects as nodes connected via pairwise relations as edges. To support\nquestion answering on scene graphs, we propose GraphVQA, a language-guided\ngraph neural network framework that translates and executes a natural language\nquestion as multiple iterations of message passing among graph nodes. We\nexplore the design space of GraphVQA framework, and discuss the trade-off of\ndifferent design choices. Our experiments on GQA dataset show that GraphVQA\noutperforms the state-of-the-art model by a large margin (88.43% vs. 94.78%).", "published": "2021-04-20 23:54:41", "link": "http://arxiv.org/abs/2104.10283v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Interventional Aspect-Based Sentiment Analysis", "abstract": "Recent neural-based aspect-based sentiment analysis approaches, though\nachieving promising improvement on benchmark datasets, have reported suffering\nfrom poor robustness when encountering confounder such as non-target aspects.\nIn this paper, we take a causal view to addressing this issue. We propose a\nsimple yet effective method, namely, Sentiment Adjustment (SENTA), by applying\na backdoor adjustment to disentangle those confounding factors. Experimental\nresults on the Aspect Robustness Test Set (ARTS) dataset demonstrate that our\napproach improves the performance while maintaining accuracy in the original\ntest set.", "published": "2021-04-20 07:54:29", "link": "http://arxiv.org/abs/2104.11681v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient pre-training objectives for Transformers", "abstract": "The Transformer architecture deeply changed the natural language processing,\noutperforming all previous state-of-the-art models. However, well-known\nTransformer models like BERT, RoBERTa, and GPT-2 require a huge compute budget\nto create a high quality contextualised representation. In this paper, we study\nseveral efficient pre-training objectives for Transformers-based models. By\ntesting these objectives on different tasks, we determine which of the ELECTRA\nmodel's new features is the most relevant. We confirm that Transformers\npre-training is improved when the input does not contain masked tokens and that\nthe usage of the whole output to compute the loss reduces training time.\nMoreover, inspired by ELECTRA, we study a model composed of two blocks; a\ndiscriminator and a simple generator based on a statistical model with no\nimpact on the computational performances. Besides, we prove that eliminating\nthe MASK token and considering the whole output during the loss computation are\nessential choices to improve performance. Furthermore, we show that it is\npossible to efficiently train BERT-like models using a discriminative approach\nas in ELECTRA but without a complex generator, which is expensive. Finally, we\nshow that ELECTRA benefits heavily from a state-of-the-art hyper-parameters\nsearch.", "published": "2021-04-20 00:09:37", "link": "http://arxiv.org/abs/2104.09694v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AdaSpeech 2: Adaptive Text to Speech with Untranscribed Data", "abstract": "Text to speech (TTS) is widely used to synthesize personal voice for a target\nspeaker, where a well-trained source TTS model is fine-tuned with few paired\nadaptation data (speech and its transcripts) on this target speaker. However,\nin many scenarios, only untranscribed speech data is available for adaptation,\nwhich brings challenges to the previous TTS adaptation pipelines (e.g.,\nAdaSpeech). In this paper, we develop AdaSpeech 2, an adaptive TTS system that\nonly leverages untranscribed speech data for adaptation. Specifically, we\nintroduce a mel-spectrogram encoder to a well-trained TTS model to conduct\nspeech reconstruction, and at the same time constrain the output sequence of\nthe mel-spectrogram encoder to be close to that of the original phoneme\nencoder. In adaptation, we use untranscribed speech data for speech\nreconstruction and only fine-tune the TTS decoder. AdaSpeech 2 has two\nadvantages: 1) Pluggable: our system can be easily applied to existing trained\nTTS models without re-training. 2) Effective: our system achieves on-par voice\nquality with the transcribed TTS adaptation (e.g., AdaSpeech) with the same\namount of untranscribed data, and achieves better voice quality than previous\nuntranscribed adaptation methods. Synthesized speech samples can be found at\nhttps://speechresearch.github.io/adaspeech2/.", "published": "2021-04-20 01:53:30", "link": "http://arxiv.org/abs/2104.09715v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RoFormer: Enhanced Transformer with Rotary Position Embedding", "abstract": "Position encoding recently has shown effective in the transformer\narchitecture. It enables valuable supervision for dependency modeling between\nelements at different positions of the sequence. In this paper, we first\ninvestigate various methods to integrate positional information into the\nlearning process of transformer-based language models. Then, we propose a novel\nmethod named Rotary Position Embedding(RoPE) to effectively leverage the\npositional information. Specifically, the proposed RoPE encodes the absolute\nposition with a rotation matrix and meanwhile incorporates the explicit\nrelative position dependency in self-attention formulation. Notably, RoPE\nenables valuable properties, including the flexibility of sequence length,\ndecaying inter-token dependency with increasing relative distances, and the\ncapability of equipping the linear self-attention with relative position\nencoding. Finally, we evaluate the enhanced transformer with rotary position\nembedding, also called RoFormer, on various long text classification benchmark\ndatasets. Our experiments show that it consistently overcomes its alternatives.\nFurthermore, we provide a theoretical analysis to explain some experimental\nresults. RoFormer is already integrated into Huggingface:\n\\url{https://huggingface.co/docs/transformers/model_doc/roformer}.", "published": "2021-04-20 09:54:06", "link": "http://arxiv.org/abs/2104.09864v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robustness Tests of NLP Machine Learning Models: Search and Semantically\n  Replace", "abstract": "This paper proposes a strategy to assess the robustness of different machine\nlearning models that involve natural language processing (NLP). The overall\napproach relies upon a Search and Semantically Replace strategy that consists\nof two steps: (1) Search, which identifies important parts in the text; (2)\nSemantically Replace, which finds replacements for the important parts, and\nconstrains the replaced tokens with semantically similar words. We introduce\ndifferent types of Search and Semantically Replace methods designed\nspecifically for particular types of machine learning models. We also\ninvestigate the effectiveness of this strategy and provide a general framework\nto assess a variety of machine learning models. Finally, an empirical\ncomparison is provided of robustness performance among three different model\ntypes, each with a different text representation.", "published": "2021-04-20 14:05:36", "link": "http://arxiv.org/abs/2104.09978v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Review of end-to-end speech synthesis technology based on deep learning", "abstract": "As an indispensable part of modern human-computer interaction system, speech\nsynthesis technology helps users get the output of intelligent machine more\neasily and intuitively, thus has attracted more and more attention. Due to the\nlimitations of high complexity and low efficiency of traditional speech\nsynthesis technology, the current research focus is the deep learning-based\nend-to-end speech synthesis technology, which has more powerful modeling\nability and a simpler pipeline. It mainly consists of three modules: text\nfront-end, acoustic model, and vocoder. This paper reviews the research status\nof these three parts, and classifies and compares various methods according to\ntheir emphasis. Moreover, this paper also summarizes the open-source speech\ncorpus of English, Chinese and other languages that can be used for speech\nsynthesis tasks, and introduces some commonly used subjective and objective\nspeech quality evaluation method. Finally, some attractive future research\ndirections are pointed out.", "published": "2021-04-20 14:24:05", "link": "http://arxiv.org/abs/2104.09995v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the Impact of Word Error Rate on Acoustic-Linguistic Speech Emotion\n  Recognition: An Update for the Deep Learning Era", "abstract": "Text encodings from automatic speech recognition (ASR) transcripts and audio\nrepresentations have shown promise in speech emotion recognition (SER) ever\nsince. Yet, it is challenging to explain the effect of each information stream\non the SER systems. Further, more clarification is required for analysing the\nimpact of ASR's word error rate (WER) on linguistic emotion recognition per se\nand in the context of fusion with acoustic information exploitation in the age\nof deep ASR systems. In order to tackle the above issues, we create transcripts\nfrom the original speech by applying three modern ASR systems, including an\nend-to-end model trained with recurrent neural network-transducer loss, a model\nwith connectionist temporal classification loss, and a wav2vec framework for\nself-supervised learning. Afterwards, we use pre-trained textual models to\nextract text representations from the ASR outputs and the gold standard. For\nextraction and learning of acoustic speech features, we utilise openSMILE,\nopenXBoW, DeepSpectrum, and auDeep. Finally, we conduct decision-level fusion\non both information streams -- acoustics and linguistics. Using the best\ndevelopment configuration, we achieve state-of-the-art unweighted average\nrecall values of $73.6\\,\\%$ and $73.8\\,\\%$ on the speaker-independent\ndevelopment and test partitions of IEMOCAP, respectively.", "published": "2021-04-20 17:10:01", "link": "http://arxiv.org/abs/2104.10121v1", "categories": ["cs.SD", "cs.CL", "eess.AS", "I.2.7; I.5.0"], "primary_category": "cs.SD"}
{"title": "How individuals change language", "abstract": "Languages emerge and change over time at the population level though\ninteractions between individual speakers. It is, however, hard to directly\nobserve how a single speaker's linguistic innovation precipitates a\npopulation-wide change in the language, and many theoretical proposals exist.\nWe introduce a very general mathematical model that encompasses a wide variety\nof individual-level linguistic behaviours and provides statistical predictions\nfor the population-level changes that result from them. This model allows us to\ncompare the likelihood of empirically-attested changes in definite and\nindefinite articles in multiple languages under different assumptions on the\nway in which individuals learn and use language. We find that accounts of\nlanguage change that appeal primarily to errors in childhood language\nacquisition are very weakly supported by the historical data, whereas those\nthat allow speakers to change incrementally across the lifespan are more\nplausible, particularly when combined with social network effects.", "published": "2021-04-20 19:02:49", "link": "http://arxiv.org/abs/2104.10210v1", "categories": ["cs.CL", "physics.soc-ph", "q-bio.PE"], "primary_category": "cs.CL"}
{"title": "Evaluating the Impact of a Hierarchical Discourse Representation on\n  Entity Coreference Resolution Performance", "abstract": "Recent work on entity coreference resolution (CR) follows current trends in\nDeep Learning applied to embeddings and relatively simple task-related\nfeatures. SOTA models do not make use of hierarchical representations of\ndiscourse structure. In this work, we leverage automatically constructed\ndiscourse parse trees within a neural approach and demonstrate a significant\nimprovement on two benchmark entity coreference-resolution datasets. We explore\nhow the impact varies depending upon the type of mention.", "published": "2021-04-20 19:14:57", "link": "http://arxiv.org/abs/2104.10215v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "StateCensusLaws.org: A Web Application for Consuming and Annotating\n  Legal Discourse Learning", "abstract": "In this work, we create a web application to highlight the output of NLP\nmodels trained to parse and label discourse segments in law text. Our system is\nbuilt primarily with journalists and legal interpreters in mind, and we focus\non state-level law that uses U.S. Census population numbers to allocate\nresources and organize government.\n  Our system exposes a corpus we collect of 6,000 state-level laws that pertain\nto the U.S. census, using 25 scrapers we built to crawl state law websites,\nwhich we release. We also build a novel, flexible annotation framework that can\nhandle span-tagging and relation tagging on an arbitrary input text document\nand be embedded simply into any webpage. This framework allows journalists and\nresearchers to add to our annotation database by correcting and tagging new\ndata.", "published": "2021-04-20 22:00:54", "link": "http://arxiv.org/abs/2104.10263v2", "categories": ["cs.CL", "cs.DL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Waveform Phasicity Prediction from Arterial Sounds through Spectrogram\n  Analysis using Convolutional Neural Networks for Limb Perfusion Assessment", "abstract": "Peripheral Arterial Disease (PAD) is a common form of arterial occlusive\ndisease that is challenging to evaluate at the point-of-care. Hand-held\ndopplers are the most ubiquitous device used to evaluate circulation and allows\nproviders to audibly \"listen\" to the blood flow. Providers use the audible\nfeedback to subjectively assess whether the sound characteristics are\nconsistent with Monophasic, Biphasic, or Triphasic waveforms. Subjective\nassessment of doppler sounds raises suspicion of PAD and leads to further\ntesting, often delaying definitive treatment. Misdiagnoses are also possible\nwith subjective interpretation of doppler waveforms. This paper presents a Deep\nLearning system that has the ability to predict waveform phasicity through\nanalysis of hand-held doppler sounds. We collected 268 four-second recordings\non an iPhone taken during a formal vascular lab study in patients with\ncardiovascular disease. Our end-to-end system works by converting input sound\ninto a spectrogram which visually represents frequency changes in temporal\npatterns. This conversion enables visual differentiation between the phasicity\nclasses. With these changes present, a custom trained Convolutional Neural\nNetwork (CNN) is used for prediction through learned feature extraction. The\nperformance of the model was evaluated via calculation of the F1 score and\naccuracy metrics. The system received an F1 score of 90.57% and an accuracy of\n96.23%. Our Deep Learning system is not computationally expensive and has the\nability for integration within several applications. When used in a clinic,\nthis system has the capability of preventing misdiagnosis and gives\npractitioners a second opinion that can be useful in the evaluation of PAD.", "published": "2021-04-20 03:58:54", "link": "http://arxiv.org/abs/2104.09748v3", "categories": ["cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.SD"}
{"title": "Identification of fake stereo audio", "abstract": "Channel is one of the important criterions for digital audio quality.\nGeneral-ly, stereo audio two channels can provide better perceptual quality\nthan mono audio. To seek illegal commercial benefit, one might convert mono\naudio to stereo one with fake quality. Identifying of stereo faking audio is\nstill a less-investigated audio forensic issue. In this paper, a stereo faking\ncorpus is first present, which is created by Haas Effect technique. Then the\neffect of stereo faking on Mel Frequency Cepstral Coefficients (MFCC) is\nanalyzed to find the difference between the real and faked stereo audio.\nFi-nally, an effective algorithm for identifying stereo faking audio is\nproposed, in which 80-dimensional MFCC features and Support Vector Machine\n(SVM) classifier are adopted. The experimental results on three datasets with\nfive different cut-off frequencies show that the proposed algorithm can\nef-fectively detect stereo faking audio and achieve a good robustness.", "published": "2021-04-20 08:35:38", "link": "http://arxiv.org/abs/2104.09832v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A cappella: Audio-visual Singing Voice Separation", "abstract": "The task of isolating a target singing voice in music videos has useful\napplications. In this work, we explore the single-channel singing voice\nseparation problem from a multimodal perspective, by jointly learning from\naudio and visual modalities. To do so, we present Acappella, a dataset spanning\naround 46 hours of a cappella solo singing videos sourced from YouTube. We also\npropose an audio-visual convolutional network based on graphs which achieves\nstate-of-the-art singing voice separation results on our dataset and compare it\nagainst its audio-only counterpart, U-Net, and a state-of-the-art audio-visual\nspeech separation model. We evaluate the models in the following challenging\nsetups: i) presence of overlapping voices in the audio mixtures, ii) the target\nvoice set to lower volume levels in the mix, and iii) combination of i) and\nii). The third one being the most challenging evaluation setup. We demonstrate\nthat our model outperforms the baseline models in the singing voice separation\ntask in the most challenging evaluation setup. The code, the pre-trained\nmodels, and the dataset are publicly available at\nhttps://ipcv.github.io/Acappella/able at https://ipcv.github.io/Acappella/", "published": "2021-04-20 13:17:06", "link": "http://arxiv.org/abs/2104.09946v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Detection of Audio-Video Synchronization Errors Via Event Detection", "abstract": "We present a new method and a large-scale database to detect audio-video\nsynchronization(A/V sync) errors in tennis videos. A deep network is trained to\ndetect the visual signature of the tennis ball being hit by the racquet in the\nvideo stream. Another deep network is trained to detect the auditory signature\nof the same event in the audio stream. During evaluation, the audio stream is\nsearched by the audio network for the audio event of the ball being hit. If the\nevent is found in audio, the neighboring interval in video is searched for the\ncorresponding visual signature. If the event is not found in the video stream\nbut is found in the audio stream, A/V sync error is flagged. We developed a\nlarge-scaled database of 504,300 frames from 6 hours of videos of tennis\nevents, simulated A/V sync errors, and found our method achieves high accuracy\non the task.", "published": "2021-04-20 16:54:44", "link": "http://arxiv.org/abs/2104.10116v1", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Bias-Aware Loss for Training Image and Speech Quality Prediction Models\n  from Multiple Datasets", "abstract": "The ground truth used for training image, video, or speech quality prediction\nmodels is based on the Mean Opinion Scores (MOS) obtained from subjective\nexperiments. Usually, it is necessary to conduct multiple experiments, mostly\nwith different test participants, to obtain enough data to train quality models\nbased on machine learning. Each of these experiments is subject to an\nexperiment-specific bias, where the rating of the same file may be\nsubstantially different in two experiments (e.g. depending on the overall\nquality distribution). These different ratings for the same distortion levels\nconfuse neural networks during training and lead to lower performance. To\novercome this problem, we propose a bias-aware loss function that estimates\neach dataset's biases during training with a linear function and considers it\nwhile optimising the network weights. We prove the efficiency of the proposed\nmethod by training and validating quality prediction models on synthetic and\nsubjective image and speech quality datasets.", "published": "2021-04-20 19:20:11", "link": "http://arxiv.org/abs/2104.10217v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
