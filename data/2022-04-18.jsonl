{"title": "Less is More: Learning to Refine Dialogue History for Personalized\n  Dialogue Generation", "abstract": "Personalized dialogue systems explore the problem of generating responses\nthat are consistent with the user's personality, which has raised much\nattention in recent years. Existing personalized dialogue systems have tried to\nextract user profiles from dialogue history to guide personalized response\ngeneration. Since the dialogue history is usually long and noisy, most existing\nmethods truncate the dialogue history to model the user's personality. Such\nmethods can generate some personalized responses, but a large part of dialogue\nhistory is wasted, leading to sub-optimal performance of personalized response\ngeneration. In this work, we propose to refine the user dialogue history on a\nlarge scale, based on which we can handle more dialogue history and obtain more\nabundant and accurate persona information. Specifically, we design an MSP model\nwhich consists of three personal information refiners and a personalized\nresponse generator. With these multi-level refiners, we can sparsely extract\nthe most valuable information (tokens) from the dialogue history and leverage\nother similar users' data to enhance personalization. Experimental results on\ntwo real-world datasets demonstrate the superiority of our model in generating\nmore informative and personalized responses.", "published": "2022-04-18 02:02:56", "link": "http://arxiv.org/abs/2204.08128v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ingredient Extraction from Text in the Recipe Domain", "abstract": "In recent years, there has been an increase in the number of devices with\nvirtual assistants (e.g: Siri, Google Home, Alexa) in our living rooms and\nkitchens. As a result of this, these devices receive several queries about\nrecipes. All these queries will contain terms relating to a \"recipe-domain\"\ni.e: they will contain dish-names, ingredients, cooking times, dietary\npreferences etc. Extracting these recipe-relevant aspects from the query thus\nbecomes important when it comes to addressing the user's information need. Our\nproject focuses on extracting ingredients from such plain-text user utterances.\nOur best performing model was a fine-tuned BERT which achieved an F1-score of\n$95.01$. We have released all our code in a GitHub repository.", "published": "2022-04-18 02:54:54", "link": "http://arxiv.org/abs/2204.08137v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Position Encoding for Transformers", "abstract": "Recurrent models have been dominating the field of neural machine translation\n(NMT) for the past few years. Transformers \\citep{vaswani2017attention}, have\nradically changed it by proposing a novel architecture that relies on a\nfeed-forward backbone and self-attention mechanism. Although Transformers are\npowerful, they could fail to properly encode sequential/positional information\ndue to their non-recurrent nature. To solve this problem, position embeddings\nare defined exclusively for each time step to enrich word information. However,\nsuch embeddings are fixed after training regardless of the task and the word\nordering system of the source or target language.\n  In this paper, we propose a novel architecture with new position embeddings\ndepending on the input text to address this shortcoming by taking the order of\ntarget words into consideration. Instead of using predefined position\nembeddings, our solution generates new embeddings to refine each word's\nposition information. Since we do not dictate the position of source tokens and\nlearn them in an end-to-end fashion, we refer to our method as dynamic position\nencoding (DPE). We evaluated the impact of our model on multiple datasets to\ntranslate from English into German, French, and Italian and observed meaningful\nimprovements in comparison to the original Transformer.", "published": "2022-04-18 03:08:48", "link": "http://arxiv.org/abs/2204.08142v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Back to the Future: Bidirectional Information Decoupling Network for\n  Multi-turn Dialogue Modeling", "abstract": "Multi-turn dialogue modeling as a challenging branch of natural language\nunderstanding (NLU), aims to build representations for machines to understand\nhuman dialogues, which provides a solid foundation for multiple downstream\ntasks. Recent studies of dialogue modeling commonly employ pre-trained language\nmodels (PrLMs) to encode the dialogue history as successive tokens, which is\ninsufficient in capturing the temporal characteristics of dialogues. Therefore,\nwe propose Bidirectional Information Decoupling Network (BiDeN) as a universal\ndialogue encoder, which explicitly incorporates both the past and future\ncontexts and can be generalized to a wide range of dialogue-related tasks.\nExperimental results on datasets of different downstream tasks demonstrate the\nuniversality and effectiveness of our BiDeN.", "published": "2022-04-18 03:51:46", "link": "http://arxiv.org/abs/2204.08152v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UTNLP at SemEval-2022 Task 6: A Comparative Analysis of Sarcasm\n  Detection Using Generative-based and Mutation-based Data Augmentation", "abstract": "Sarcasm is a term that refers to the use of words to mock, irritate, or amuse\nsomeone. It is commonly used on social media. The metaphorical and creative\nnature of sarcasm presents a significant difficulty for sentiment analysis\nsystems based on affective computing. The methodology and results of our team,\nUTNLP, in the SemEval-2022 shared task 6 on sarcasm detection are presented in\nthis paper. We put different models, and data augmentation approaches to the\ntest and report on which one works best. The tests begin with traditional\nmachine learning models and progress to transformer-based and attention-based\nmodels. We employed data augmentation based on data mutation and data\ngeneration. Using RoBERTa and mutation-based data augmentation, our best\napproach achieved an F1-sarcastic of 0.38 in the competition's evaluation\nphase. After the competition, we fixed our model's flaws and achieved an\nF1-sarcastic of 0.414.", "published": "2022-04-18 07:25:27", "link": "http://arxiv.org/abs/2204.08198v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Factual Error Correction for Abstractive Summaries Using Entity\n  Retrieval", "abstract": "Despite the recent advancements in abstractive summarization systems\nleveraged from large-scale datasets and pre-trained language models, the\nfactual correctness of the summary is still insufficient. One line of trials to\nmitigate this problem is to include a post-editing process that can detect and\ncorrect factual errors in the summary. In building such a post-editing system,\nit is strongly required that 1) the process has a high success rate and\ninterpretability and 2) has a fast running time. Previous approaches focus on\nregeneration of the summary using the autoregressive models, which lack\ninterpretability and require high computing resources. In this paper, we\npropose an efficient factual error correction system RFEC based on entities\nretrieval post-editing process. RFEC first retrieves the evidence sentences\nfrom the original document by comparing the sentences with the target summary.\nThis approach greatly reduces the length of text for a system to analyze. Next,\nRFEC detects the entity-level errors in the summaries by considering the\nevidence sentences and substitutes the wrong entities with the accurate\nentities from the evidence sentences. Experimental results show that our\nproposed error correction system shows more competitive performance than\nbaseline methods in correcting the factual errors with a much faster speed.", "published": "2022-04-18 11:35:02", "link": "http://arxiv.org/abs/2204.08263v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UMass PCL at SemEval-2022 Task 4: Pre-trained Language Model Ensembles\n  for Detecting Patronizing and Condescending Language", "abstract": "Patronizing and condescending language (PCL) is everywhere, but rarely is the\nfocus on its use by media towards vulnerable communities. Accurately detecting\nPCL of this form is a difficult task due to limited labeled data and how subtle\nit can be. In this paper, we describe our system for detecting such language\nwhich was submitted to SemEval 2022 Task 4: Patronizing and Condescending\nLanguage Detection. Our approach uses an ensemble of pre-trained language\nmodels, data augmentation, and optimizing the threshold for detection.\nExperimental results on the evaluation dataset released by the competition\nhosts show that our work is reliably able to detect PCL, achieving an F1 score\nof 55.47% on the binary classification task and a macro F1 score of 36.25% on\nthe fine-grained, multi-label detection task.", "published": "2022-04-18 13:22:10", "link": "http://arxiv.org/abs/2204.08304v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GL-CLeF: A Global-Local Contrastive Learning Framework for Cross-lingual\n  Spoken Language Understanding", "abstract": "Due to high data demands of current methods, attention to zero-shot\ncross-lingual spoken language understanding (SLU) has grown, as such approaches\ngreatly reduce human annotation effort. However, existing models solely rely on\nshared parameters, which can only perform implicit alignment across languages.\nWe present Global--Local Contrastive Learning Framework (GL-CLeF) to address\nthis shortcoming. Specifically, we employ contrastive learning, leveraging\nbilingual dictionaries to construct multilingual views of the same utterance,\nthen encourage their representations to be more similar than negative example\npairs, which achieves to explicitly aligned representations of similar\nsentences across languages. In addition, a key step in GL-CLeF is a proposed\nLocal and Global component, which achieves a fine-grained cross-lingual\ntransfer (i.e., sentence-level Local intent transfer, token-level Local slot\ntransfer, and semantic-level Global transfer across intent and slot).\nExperiments on MultiATIS++ show that GL-CLeF achieves the best performance and\nsuccessfully pulls representations of similar sentences across languages\ncloser.", "published": "2022-04-18 13:56:58", "link": "http://arxiv.org/abs/2204.08325v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Execute Actions or Ask Clarification Questions", "abstract": "Collaborative tasks are ubiquitous activities where a form of communication\nis required in order to reach a joint goal. Collaborative building is one of\nsuch tasks. We wish to develop an intelligent builder agent in a simulated\nbuilding environment (Minecraft) that can build whatever users wish to build by\njust talking to the agent. In order to achieve this goal, such agents need to\nbe able to take the initiative by asking clarification questions when further\ninformation is needed. Existing works on Minecraft Corpus Dataset only learn to\nexecute instructions neglecting the importance of asking for clarifications. In\nthis paper, we extend the Minecraft Corpus Dataset by annotating all builder\nutterances into eight types, including clarification questions, and propose a\nnew builder agent model capable of determining when to ask or execute\ninstructions. Experimental results show that our model achieves\nstate-of-the-art performance on the collaborative building task with a\nsubstantial improvement. We also define two new tasks, the learning to ask task\nand the joint learning task. The latter consists of solving both collaborating\nbuilding and learning to ask tasks jointly.", "published": "2022-04-18 15:36:02", "link": "http://arxiv.org/abs/2204.08373v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TranS: Transition-based Knowledge Graph Embedding with Synthetic\n  Relation Representation", "abstract": "Knowledge graph embedding (KGE) aims to learn continuous vectors of relations\nand entities in knowledge graph. Recently, transition-based KGE methods have\nachieved promising performance, where the single relation vector learns to\ntranslate head entity to tail entity. However, this scoring pattern is not\nsuitable for complex scenarios where the same entity pair has different\nrelations. Previous models usually focus on the improvement of entity\nrepresentation for 1-to-N, N-to-1 and N-to-N relations, but ignore the single\nrelation vector. In this paper, we propose a novel transition-based method,\nTranS, for knowledge graph embedding. The single relation vector in traditional\nscoring patterns is replaced with synthetic relation representation, which can\nsolve these issues effectively and efficiently. Experiments on a large\nknowledge graph dataset, ogbl-wikikg2, show that our model achieves\nstate-of-the-art results.", "published": "2022-04-18 16:55:25", "link": "http://arxiv.org/abs/2204.08401v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Dimensionality Reduction Techniques in Multilingual\n  Transformers", "abstract": "Both in scientific literature and in industry,, Semantic and context-aware\nNatural Language Processing-based solutions have been gaining importance in\nrecent years. The possibilities and performance shown by these models when\ndealing with complex Language Understanding tasks is unquestionable, from\nconversational agents to the fight against disinformation in social networks.\nIn addition, considerable attention is also being paid to developing\nmultilingual models to tackle the language bottleneck. The growing need to\nprovide more complex models implementing all these features has been\naccompanied by an increase in their size, without being conservative in the\nnumber of dimensions required. This paper aims to give a comprehensive account\nof the impact of a wide variety of dimensional reduction techniques on the\nperformance of different state-of-the-art multilingual Siamese Transformers,\nincluding unsupervised dimensional reduction techniques such as linear and\nnonlinear feature extraction, feature selection, and manifold techniques. In\norder to evaluate the effects of these techniques, we considered the\nmultilingual extended version of Semantic Textual Similarity Benchmark (mSTSb)\nand two different baseline approaches, one using the pre-trained version of\nseveral models and another using their fine-tuned STS version. The results\nevidence that it is possible to achieve an average reduction in the number of\ndimensions of $91.58\\% \\pm 2.59\\%$ and $54.65\\% \\pm 32.20\\%$, respectively.\nThis work has also considered the consequences of dimensionality reduction for\nvisualization purposes. The results of this study will significantly contribute\nto the understanding of how different tuning approaches affect performance on\nsemantic-aware tasks and how dimensional reduction techniques deal with the\nhigh-dimensional embeddings computed for the STS task and their potential for\nhighly demanding NLP tasks", "published": "2022-04-18 17:20:55", "link": "http://arxiv.org/abs/2204.08415v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Imagination-Augmented Natural Language Understanding", "abstract": "Human brains integrate linguistic and perceptual information simultaneously\nto understand natural language, and hold the critical ability to render\nimaginations. Such abilities enable us to construct new abstract concepts or\nconcrete objects, and are essential in involving practical knowledge to solve\nproblems in low-resource scenarios. However, most existing methods for Natural\nLanguage Understanding (NLU) are mainly focused on textual signals. They do not\nsimulate human visual imagination ability, which hinders models from inferring\nand learning efficiently from limited data samples. Therefore, we introduce an\nImagination-Augmented Cross-modal Encoder (iACE) to solve natural language\nunderstanding tasks from a novel learning perspective -- imagination-augmented\ncross-modal understanding. iACE enables visual imagination with external\nknowledge transferred from the powerful generative and pre-trained\nvision-and-language models. Extensive experiments on GLUE and SWAG show that\niACE achieves consistent improvement over visually-supervised pre-trained\nmodels. More importantly, results in extreme and normal few-shot settings\nvalidate the effectiveness of iACE in low-resource natural language\nunderstanding circumstances.", "published": "2022-04-18 19:39:36", "link": "http://arxiv.org/abs/2204.08535v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HFT-ONLSTM: Hierarchical and Fine-Tuning Multi-label Text Classification", "abstract": "Many important classification problems in the real-world consist of a large\nnumber of closely related categories in a hierarchical structure or taxonomy.\nHierarchical multi-label text classification (HMTC) with higher accuracy over\nlarge sets of closely related categories organized in a hierarchy or taxonomy\nhas become a challenging problem. In this paper, we present a hierarchical and\nfine-tuning approach based on the Ordered Neural LSTM neural network,\nabbreviated as HFT-ONLSTM, for more accurate level-by-level HMTC. First, we\npresent a novel approach to learning the joint embeddings based on parent\ncategory labels and textual data for accurately capturing the joint features of\nboth category labels and texts. Second, a fine tuning technique is adopted for\ntraining parameters such that the text classification results in the upper\nlevel should contribute to the classification in the lower one. At last, the\ncomprehensive analysis is made based on extensive experiments in comparison\nwith the state-of-the-art hierarchical and flat multi-label text classification\napproaches over two benchmark datasets, and the experimental results show that\nour HFT-ONLSTM approach outperforms these approaches, in particular reducing\ncomputational costs while achieving superior performance.", "published": "2022-04-18 00:57:46", "link": "http://arxiv.org/abs/2204.08115v1", "categories": ["cs.CL", "cs.AI", "68T07", "I.2.7; I.2.4"], "primary_category": "cs.CL"}
{"title": "End-to-end Dense Video Captioning as Sequence Generation", "abstract": "Dense video captioning aims to identify the events of interest in an input\nvideo, and generate descriptive captions for each event. Previous approaches\nusually follow a two-stage generative process, which first proposes a segment\nfor each event, then renders a caption for each identified segment. Recent\nadvances in large-scale sequence generation pretraining have seen great success\nin unifying task formulation for a great variety of tasks, but so far, more\ncomplex tasks such as dense video captioning are not able to fully utilize this\npowerful paradigm. In this work, we show how to model the two subtasks of dense\nvideo captioning jointly as one sequence generation task, and simultaneously\npredict the events and the corresponding descriptions. Experiments on YouCook2\nand ViTT show encouraging results and indicate the feasibility of training\ncomplex tasks such as end-to-end dense video captioning integrated into\nlarge-scale pretrained models.", "published": "2022-04-18 01:30:54", "link": "http://arxiv.org/abs/2204.08121v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Detect Rumors in Microblog Posts for Low-Resource Domains via\n  Adversarial Contrastive Learning", "abstract": "Massive false rumors emerging along with breaking news or trending topics\nseverely hinder the truth. Existing rumor detection approaches achieve\npromising performance on the yesterday's news, since there is enough corpus\ncollected from the same domain for model training. However, they are poor at\ndetecting rumors about unforeseen events especially those propagated in\ndifferent languages due to the lack of training data and prior knowledge (i.e.,\nlow-resource regimes). In this paper, we propose an adversarial contrastive\nlearning framework to detect rumors by adapting the features learned from\nwell-resourced rumor data to that of the low-resourced. Our model explicitly\novercomes the restriction of domain and/or language usage via language\nalignment and a novel supervised contrastive training paradigm. Moreover, we\ndevelop an adversarial augmentation mechanism to further enhance the robustness\nof low-resource rumor representation. Extensive experiments conducted on two\nlow-resource datasets collected from real-world microblog platforms demonstrate\nthat our framework achieves much better performance than state-of-the-art\nmethods and exhibits a superior capacity for detecting rumors at early stages.", "published": "2022-04-18 03:10:34", "link": "http://arxiv.org/abs/2204.08143v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Study on Prompt-based Few-Shot Learning Methods for Belief State\n  Tracking in Task-oriented Dialog Systems", "abstract": "We tackle the Dialogue Belief State Tracking(DST) problem of task-oriented\nconversational systems. Recent approaches to this problem leveraging\nTransformer-based models have yielded great results. However, training these\nmodels is expensive, both in terms of computational resources and time.\nAdditionally, collecting high quality annotated dialogue datasets remains a\nchallenge for researchers because of the extensive annotation required for\ntraining these models. Driven by the recent success of pre-trained language\nmodels and prompt-based learning, we explore prompt-based few-shot learning for\nDialogue Belief State Tracking. We formulate the DST problem as a 2-stage\nprompt-based language modelling task and train language models for both tasks\nand present a comprehensive empirical analysis of their separate and joint\nperformance. We demonstrate the potential of prompt-based methods in few-shot\nlearning for DST and provide directions for future improvement.", "published": "2022-04-18 05:29:54", "link": "http://arxiv.org/abs/2204.08167v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TABi: Type-Aware Bi-Encoders for Open-Domain Entity Retrieval", "abstract": "Entity retrieval--retrieving information about entity mentions in a query--is\na key step in open-domain tasks, such as question answering or fact checking.\nHowever, state-of-the-art entity retrievers struggle to retrieve rare entities\nfor ambiguous mentions due to biases towards popular entities. Incorporating\nknowledge graph types during training could help overcome popularity biases,\nbut there are several challenges: (1) existing type-based retrieval methods\nrequire mention boundaries as input, but open-domain tasks run on unstructured\ntext, (2) type-based methods should not compromise overall performance, and (3)\ntype-based methods should be robust to noisy and missing types. In this work,\nwe introduce TABi, a method to jointly train bi-encoders on knowledge graph\ntypes and unstructured text for entity retrieval for open-domain tasks. TABi\nleverages a type-enforced contrastive loss to encourage entities and queries of\nsimilar types to be close in the embedding space. TABi improves retrieval of\nrare entities on the Ambiguous Entity Retrieval (AmbER) sets, while maintaining\nstrong overall retrieval performance on open-domain tasks in the KILT benchmark\ncompared to state-of-the-art retrievers. TABi is also robust to incomplete type\nsystems, improving rare entity retrieval over baselines with only 5% type\ncoverage of the training dataset. We make our code publicly available at\nhttps://github.com/HazyResearch/tabi.", "published": "2022-04-18 05:54:44", "link": "http://arxiv.org/abs/2204.08173v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in\n  Texts", "abstract": "Inferring spatial relations in natural language is a crucial ability an\nintelligent system should possess. The bAbI dataset tries to capture tasks\nrelevant to this domain (task 17 and 19). However, these tasks have several\nlimitations. Most importantly, they are limited to fixed expressions, they are\nlimited in the number of reasoning steps required to solve them, and they fail\nto test the robustness of models to input that contains irrelevant or redundant\ninformation. In this paper, we present a new Question-Answering dataset called\nStepGame for robust multi-hop spatial reasoning in texts. Our experiments\ndemonstrate that state-of-the-art models on the bAbI dataset struggle on the\nStepGame dataset. Moreover, we propose a Tensor-Product based Memory-Augmented\nNeural Network (TP-MANN) specialized for spatial reasoning tasks. Experimental\nresults on both datasets show that our model outperforms all the baselines with\nsuperior generalization and robustness performance.", "published": "2022-04-18 12:46:46", "link": "http://arxiv.org/abs/2204.08292v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LayoutLMv3: Pre-training for Document AI with Unified Text and Image\n  Masking", "abstract": "Self-supervised pre-training techniques have achieved remarkable progress in\nDocument AI. Most multimodal pre-trained models use a masked language modeling\nobjective to learn bidirectional representations on the text modality, but they\ndiffer in pre-training objectives for the image modality. This discrepancy adds\ndifficulty to multimodal representation learning. In this paper, we propose\n\\textbf{LayoutLMv3} to pre-train multimodal Transformers for Document AI with\nunified text and image masking. Additionally, LayoutLMv3 is pre-trained with a\nword-patch alignment objective to learn cross-modal alignment by predicting\nwhether the corresponding image patch of a text word is masked. The simple\nunified architecture and training objectives make LayoutLMv3 a general-purpose\npre-trained model for both text-centric and image-centric Document AI tasks.\nExperimental results show that LayoutLMv3 achieves state-of-the-art performance\nnot only in text-centric tasks, including form understanding, receipt\nunderstanding, and document visual question answering, but also in\nimage-centric tasks such as document image classification and document layout\nanalysis. The code and models are publicly available at\n\\url{https://aka.ms/layoutlmv3}.", "published": "2022-04-18 16:19:52", "link": "http://arxiv.org/abs/2204.08387v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "StableMoE: Stable Routing Strategy for Mixture of Experts", "abstract": "The Mixture-of-Experts (MoE) technique can scale up the model size of\nTransformers with an affordable computational overhead. We point out that\nexisting learning-to-route MoE methods suffer from the routing fluctuation\nissue, i.e., the target expert of the same input may change along with\ntraining, but only one expert will be activated for the input during inference.\nThe routing fluctuation tends to harm sample efficiency because the same input\nupdates different experts but only one is finally used. In this paper, we\npropose StableMoE with two training stages to address the routing fluctuation\nproblem. In the first training stage, we learn a balanced and cohesive routing\nstrategy and distill it into a lightweight router decoupled from the backbone\nmodel. In the second training stage, we utilize the distilled router to\ndetermine the token-to-expert assignment and freeze it for a stable routing\nstrategy. We validate our method on language modeling and multilingual machine\ntranslation. The results show that StableMoE outperforms existing MoE methods\nin terms of both convergence speed and performance.", "published": "2022-04-18 16:48:19", "link": "http://arxiv.org/abs/2204.08396v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "L3Cube-HingCorpus and HingBERT: A Code Mixed Hindi-English Dataset and\n  BERT Language Models", "abstract": "Code-switching occurs when more than one language is mixed in a given\nsentence or a conversation. This phenomenon is more prominent on social media\nplatforms and its adoption is increasing over time. Therefore code-mixed NLP\nhas been extensively studied in the literature. As pre-trained\ntransformer-based architectures are gaining popularity, we observe that real\ncode-mixing data are scarce to pre-train large language models. We present\nL3Cube-HingCorpus, the first large-scale real Hindi-English code mixed data in\na Roman script. It consists of 52.93M sentences and 1.04B tokens, scraped from\nTwitter. We further present HingBERT, HingMBERT, HingRoBERTa, and HingGPT. The\nBERT models have been pre-trained on codemixed HingCorpus using masked language\nmodelling objectives. We show the effectiveness of these BERT models on the\nsubsequent downstream tasks like code-mixed sentiment analysis, POS tagging,\nNER, and LID from the GLUECoS benchmark. The HingGPT is a GPT2 based generative\ntransformer model capable of generating full tweets. We also release\nL3Cube-HingLID Corpus, the largest code-mixed Hindi-English language\nidentification(LID) dataset and HingBERT-LID, a production-quality LID model to\nfacilitate capturing of more code-mixed data using the process outlined in this\nwork. The dataset and models are available at\nhttps://github.com/l3cube-pune/code-mixed-nlp .", "published": "2022-04-18 16:49:59", "link": "http://arxiv.org/abs/2204.08398v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zero-shot Entity and Tweet Characterization with Designed Conditional\n  Prompts and Contexts", "abstract": "Online news and social media have been the de facto mediums to disseminate\ninformation globally from the beginning of the last decade. However, bias in\ncontent and purpose of intentions are not regulated, and managing bias is the\nresponsibility of content consumers. In this regard, understanding the stances\nand biases of news sources towards specific entities becomes important. To\naddress this problem, we use pretrained language models, which have been shown\nto bring about good results with no task-specific training or few-shot\ntraining. In this work, we approach the problem of characterizing Named\nEntities and Tweets as an open-ended text classification and open-ended fact\nprobing problem.We evaluate the zero-shot language model capabilities of\nGenerative Pretrained Transformer 2 (GPT-2) to characterize Entities and Tweets\nsubjectively with human psychology-inspired and logical conditional prefixes\nand contexts. First, we fine-tune the GPT-2 model on a sufficiently large news\ncorpus and evaluate subjective characterization of popular entities in the\ncorpus by priming with prefixes. Second, we fine-tune GPT-2 with a Tweets\ncorpus from a few popular hashtags and evaluate characterizing tweets by\npriming the language model with prefixes, questions, and contextual synopsis\nprompts. Entity characterization results were positive across measures and\nhuman evaluation.", "published": "2022-04-18 17:01:49", "link": "http://arxiv.org/abs/2204.08405v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CHAI: A CHatbot AI for Task-Oriented Dialogue with Offline Reinforcement\n  Learning", "abstract": "Conventionally, generation of natural language for dialogue agents may be\nviewed as a statistical learning problem: determine the patterns in\nhuman-provided data and generate appropriate responses with similar statistical\nproperties. However, dialogue can also be regarded as a goal directed process,\nwhere speakers attempt to accomplish a specific task. Reinforcement learning\n(RL) algorithms are designed specifically for solving such goal-directed\nproblems, but the most direct way to apply RL -- through trial-and-error\nlearning in human conversations, -- is costly. In this paper, we study how\noffline reinforcement learning can instead be used to train dialogue agents\nentirely using static datasets collected from human speakers. Our experiments\nshow that recently developed offline RL methods can be combined with language\nmodels to yield realistic dialogue agents that better accomplish task goals.", "published": "2022-04-18 17:43:21", "link": "http://arxiv.org/abs/2204.08426v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Context-Aware Language Modeling for Goal-Oriented Dialogue Systems", "abstract": "Goal-oriented dialogue systems face a trade-off between fluent language\ngeneration and task-specific control. While supervised learning with large\nlanguage models is capable of producing realistic text, how to steer such\nresponses towards completing a specific task without sacrificing language\nquality remains an open question. In this work, we formulate goal-oriented\ndialogue as a partially observed Markov decision process, interpreting the\nlanguage model as a representation of both the dynamics and the policy. This\nview allows us to extend techniques from learning-based control, such as task\nrelabeling, to derive a simple and effective method to finetune language models\nin a goal-aware way, leading to significantly improved task performance. We\nadditionally introduce a number of training strategies that serve to better\nfocus the model on the task at hand. We evaluate our method, Context-Aware\nLanguage Models (CALM), on a practical flight-booking task using AirDialogue.\nEmpirically, CALM outperforms the state-of-the-art method by 7% in terms of\ntask success, matching human-level task performance.", "published": "2022-04-18 17:23:11", "link": "http://arxiv.org/abs/2204.10198v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Non-Parallel Text Style Transfer with Self-Parallel Supervision", "abstract": "The performance of existing text style transfer models is severely limited by\nthe non-parallel datasets on which the models are trained. In non-parallel\ndatasets, no direct mapping exists between sentences of the source and target\nstyle; the style transfer models thus only receive weak supervision of the\ntarget sentences during training, which often leads the model to discard too\nmuch style-independent information, or utterly fail to transfer the style. In\nthis work, we propose LaMer, a novel text style transfer framework based on\nlarge-scale language models. LaMer first mines the roughly parallel expressions\nin the non-parallel datasets with scene graphs, and then employs MLE training,\nfollowed by imitation learning refinement, to leverage the intrinsic\nparallelism within the data. On two benchmark tasks (sentiment & formality\ntransfer) and a newly proposed challenging task (political stance transfer),\nour model achieves qualitative advances in transfer accuracy, content\npreservation, and fluency. Further empirical and human evaluations demonstrate\nthat our model not only makes training more efficient, but also generates more\nreadable and diverse expressions than previous models.", "published": "2022-04-18 01:38:35", "link": "http://arxiv.org/abs/2204.08123v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Caption Feature Space Regularization for Audio Captioning", "abstract": "Audio captioning aims at describing the content of audio clips with human\nlanguage. Due to the ambiguity of audio, different people may perceive the same\naudio differently, resulting in caption disparities (i.e., one audio may\ncorrelate to several captions with diverse semantics). For that, general audio\ncaptioning models achieve the one-to-many training by randomly selecting a\ncorrelated caption as the ground truth for each audio. However, it leads to a\nsignificant variation in the optimization directions and weakens the model\nstability. To eliminate this negative effect, in this paper, we propose a\ntwo-stage framework for audio captioning: (i) in the first stage, via the\ncontrastive learning, we construct a proxy feature space to reduce the\ndistances between captions correlated to the same audio, and (ii) in the second\nstage, the proxy feature space is utilized as additional supervision to\nencourage the model to be optimized in the direction that benefits all the\ncorrelated captions. We conducted extensive experiments on two datasets using\nfour commonly used encoder and decoder architectures. Experimental results\ndemonstrate the effectiveness of the proposed method. The code is available at\nhttps://github.com/PRIS-CV/Caption-Feature-Space-Regularization.", "published": "2022-04-18 17:07:31", "link": "http://arxiv.org/abs/2204.08409v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Active Learning Helps Pretrained Models Learn the Intended Task", "abstract": "Models can fail in unpredictable ways during deployment due to task\nambiguity, when multiple behaviors are consistent with the provided training\ndata. An example is an object classifier trained on red squares and blue\ncircles: when encountering blue squares, the intended behavior is undefined. We\ninvestigate whether pretrained models are better active learners, capable of\ndisambiguating between the possible tasks a user may be trying to specify.\nIntriguingly, we find that better active learning is an emergent property of\nthe pretraining process: pretrained models require up to 5 times fewer labels\nwhen using uncertainty-based active learning, while non-pretrained models see\nno or even negative benefit. We find these gains come from an ability to select\nexamples with attributes that disambiguate the intended behavior, such as rare\nproduct categories or atypical backgrounds. These attributes are far more\nlinearly separable in pretrained model's representation spaces vs\nnon-pretrained models, suggesting a possible mechanism for this behavior.", "published": "2022-04-18 18:00:19", "link": "http://arxiv.org/abs/2204.08491v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "CBR-iKB: A Case-Based Reasoning Approach for Question Answering over\n  Incomplete Knowledge Bases", "abstract": "Knowledge bases (KBs) are often incomplete and constantly changing in\npractice. Yet, in many question answering applications coupled with knowledge\nbases, the sparse nature of KBs is often overlooked. To this end, we propose a\ncase-based reasoning approach, CBR-iKB, for knowledge base question answering\n(KBQA) with incomplete-KB as our main focus. Our method ensembles decisions\nfrom multiple reasoning chains with a novel nonparametric reasoning algorithm.\nBy design, CBR-iKB can seamlessly adapt to changes in KBs without any\ntask-specific training or fine-tuning. Our method achieves 100% accuracy on\nMetaQA and establishes new state-of-the-art on multiple benchmarks. For\ninstance, CBR-iKB achieves an accuracy of 70% on WebQSP under the incomplete-KB\nsetting, outperforming the existing state-of-the-art method by 22.3%.", "published": "2022-04-18 20:46:41", "link": "http://arxiv.org/abs/2204.08554v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MASSIVE: A 1M-Example Multilingual Natural Language Understanding\n  Dataset with 51 Typologically-Diverse Languages", "abstract": "We present the MASSIVE dataset--Multilingual Amazon Slu resource package\n(SLURP) for Slot-filling, Intent classification, and Virtual assistant\nEvaluation. MASSIVE contains 1M realistic, parallel, labeled virtual assistant\nutterances spanning 51 languages, 18 domains, 60 intents, and 55 slots. MASSIVE\nwas created by tasking professional translators to localize the English-only\nSLURP dataset into 50 typologically diverse languages from 29 genera. We also\npresent modeling results on XLM-R and mT5, including exact match accuracy,\nintent classification accuracy, and slot-filling F1 score. We have released our\ndataset, modeling code, and models publicly.", "published": "2022-04-18 22:40:52", "link": "http://arxiv.org/abs/2204.08582v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gated Multimodal Fusion with Contrastive Learning for Turn-taking\n  Prediction in Human-robot Dialogue", "abstract": "Turn-taking, aiming to decide when the next speaker can start talking, is an\nessential component in building human-robot spoken dialogue systems. Previous\nstudies indicate that multimodal cues can facilitate this challenging task.\nHowever, due to the paucity of public multimodal datasets, current methods are\nmostly limited to either utilizing unimodal features or simplistic multimodal\nensemble models. Besides, the inherent class imbalance in real scenario, e.g.\nsentence ending with short pause will be mostly regarded as the end of turn,\nalso poses great challenge to the turn-taking decision. In this paper, we first\ncollect a large-scale annotated corpus for turn-taking with over 5,000 real\nhuman-robot dialogues in speech and text modalities. Then, a novel gated\nmultimodal fusion mechanism is devised to utilize various information\nseamlessly for turn-taking prediction. More importantly, to tackle the data\nimbalance issue, we design a simple yet effective data augmentation method to\nconstruct negative instances without supervision and apply contrastive learning\nto obtain better feature representations. Extensive experiments are conducted\nand the results demonstrate the superiority and competitiveness of our model\nover several state-of-the-art baselines.", "published": "2022-04-18 05:18:00", "link": "http://arxiv.org/abs/2204.10172v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Visio-Linguistic Brain Encoding", "abstract": "Enabling effective brain-computer interfaces requires understanding how the\nhuman brain encodes stimuli across modalities such as visual, language (or\ntext), etc. Brain encoding aims at constructing fMRI brain activity given a\nstimulus. There exists a plethora of neural encoding models which study brain\nencoding for single mode stimuli: visual (pretrained CNNs) or text (pretrained\nlanguage models). Few recent papers have also obtained separate visual and text\nrepresentation models and performed late-fusion using simple heuristics.\nHowever, previous work has failed to explore: (a) the effectiveness of image\nTransformer models for encoding visual stimuli, and (b) co-attentive\nmulti-modal modeling for visual and text reasoning. In this paper, we\nsystematically explore the efficacy of image Transformers (ViT, DEiT, and BEiT)\nand multi-modal Transformers (VisualBERT, LXMERT, and CLIP) for brain encoding.\nExtensive experiments on two popular datasets, BOLD5000 and Pereira, provide\nthe following insights. (1) To the best of our knowledge, we are the first to\ninvestigate the effectiveness of image and multi-modal Transformers for brain\nencoding. (2) We find that VisualBERT, a multi-modal Transformer, significantly\noutperforms previously proposed single-mode CNNs, image Transformers as well as\nother previously proposed multi-modal models, thereby establishing new\nstate-of-the-art. The supremacy of visio-linguistic models raises the question\nof whether the responses elicited in the visual regions are affected implicitly\nby linguistic processing even when passively viewing images. Future fMRI tasks\ncan verify this computational insight in an appropriate experimental setting.", "published": "2022-04-18 11:28:18", "link": "http://arxiv.org/abs/2204.08261v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "q-bio.NC"], "primary_category": "cs.CV"}
{"title": "Cross-view Brain Decoding", "abstract": "How the brain captures the meaning of linguistic stimuli across multiple\nviews is still a critical open question in neuroscience. Consider three\ndifferent views of the concept apartment: (1) picture (WP) presented with the\ntarget word label, (2) sentence (S) using the target word, and (3) word cloud\n(WC) containing the target word along with other semantically related words.\nUnlike previous efforts, which focus only on single view analysis, in this\npaper, we study the effectiveness of brain decoding in a zero-shot cross-view\nlearning setup. Further, we propose brain decoding in the novel context of\ncross-view-translation tasks like image captioning (IC), image tagging (IT),\nkeyword extraction (KE), and sentence formation (SF). Using extensive\nexperiments, we demonstrate that cross-view zero-shot brain decoding is\npractical leading to ~0.68 average pairwise accuracy across view pairs. Also,\nthe decoded representations are sufficiently detailed to enable high accuracy\nfor cross-view-translation tasks with following pairwise accuracy: IC (78.0),\nIT (83.0), KE (83.7) and SF (74.5). Analysis of the contribution of different\nbrain networks reveals exciting cognitive insights: (1) A high percentage of\nvisual voxels are involved in image captioning and image tagging tasks, and a\nhigh percentage of language voxels are involved in the sentence formation and\nkeyword extraction tasks. (2) Zero-shot accuracy of the model trained on S view\nand tested on WC view is better than same-view accuracy of the model trained\nand tested on WC view.", "published": "2022-04-18 10:43:11", "link": "http://arxiv.org/abs/2204.09564v1", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.CV", "cs.LG", "eess.IV"], "primary_category": "q-bio.NC"}
{"title": "Robust End-to-end Speaker Diarization with Generic Neural Clustering", "abstract": "End-to-end speaker diarization approaches have shown exceptional performance\nover the traditional modular approaches. To further improve the performance of\nthe end-to-end speaker diarization for real speech recordings, recently works\nhave been proposed which integrate unsupervised clustering algorithms with the\nend-to-end neural diarization models. However, these methods have a number of\ndrawbacks: 1) The unsupervised clustering algorithms cannot leverage the\nsupervision from the available datasets; 2) The K-means-based unsupervised\nalgorithms that are explored often suffer from the constraint violation\nproblem; 3) There is unavoidable mismatch between the supervised training and\nthe unsupervised inference. In this paper, a robust generic neural clustering\napproach is proposed that can be integrated with any chunk-level predictor to\naccomplish a fully supervised end-to-end speaker diarization model. Also, by\nleveraging the sequence modelling ability of a recurrent neural network, the\nproposed neural clustering approach can dynamically estimate the number of\nspeakers during inference. Experimental show that when integrating an\nattractor-based chunk-level predictor, the proposed neural clustering approach\ncan yield better Diarization Error Rate (DER) than the constrained\nK-means-based clustering approaches under the mismatched conditions.", "published": "2022-04-18 05:02:00", "link": "http://arxiv.org/abs/2204.08164v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automated Audio Captioning using Audio Event Clues", "abstract": "Audio captioning is an important research area that aims to generate\nmeaningful descriptions for audio clips. Most of the existing research extracts\nacoustic features of audio clips as input to encoder-decoder and transformer\narchitectures to produce the captions in a sequence-to-sequence manner. Due to\ndata insufficiency and the architecture's inadequate learning capacity,\nadditional information is needed to generate natural language sentences, as\nwell as acoustic features. To address these problems, an encoder-decoder\narchitecture is proposed that learns from both acoustic features and extracted\naudio event labels as inputs. The proposed model is based on pre-trained\nacoustic features and audio event detection. Various experiments used different\nacoustic features, word embedding models, audio event label extraction methods,\nand implementation configurations to show which combinations have better\nperformance on the audio captioning task. Results of the extensive experiments\non multiple datasets show that using audio event labels with the acoustic\nfeatures improves the recognition performance and the proposed method either\noutperforms or achieves competitive results with the state-of-the-art models.", "published": "2022-04-18 21:30:42", "link": "http://arxiv.org/abs/2204.08567v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Differentiable Time-Frequency Scattering on GPU", "abstract": "Joint time-frequency scattering (JTFS) is a convolutional operator in the\ntime-frequency domain which extracts spectrotemporal modulations at various\nrates and scales. It offers an idealized model of spectrotemporal receptive\nfields (STRF) in the primary auditory cortex, and thus may serve as a\nbiological plausible surrogate for human perceptual judgments at the scale of\nisolated audio events. Yet, prior implementations of JTFS and STRF have\nremained outside of the standard toolkit of perceptual similarity measures and\nevaluation methods for audio generation. We trace this issue down to three\nlimitations: differentiability, speed, and flexibility. In this paper, we\npresent an implementation of time-frequency scattering in Python. Unlike prior\nimplementations, ours accommodates NumPy, PyTorch, and TensorFlow as backends\nand is thus portable on both CPU and GPU. We demonstrate the usefulness of JTFS\nvia three applications: unsupervised manifold learning of spectrotemporal\nmodulations, supervised classification of musical instruments, and texture\nresynthesis of bioacoustic sounds.", "published": "2022-04-18 12:02:08", "link": "http://arxiv.org/abs/2204.08269v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Extracting Targeted Training Data from ASR Models, and How to Mitigate\n  It", "abstract": "Recent work has designed methods to demonstrate that model updates in ASR\ntraining can leak potentially sensitive attributes of the utterances used in\ncomputing the updates. In this work, we design the first method to demonstrate\ninformation leakage about training data from trained ASR models. We design\nNoise Masking, a fill-in-the-blank style method for extracting targeted parts\nof training data from trained ASR models. We demonstrate the success of Noise\nMasking by using it in four settings for extracting names from the LibriSpeech\ndataset used for training a state-of-the-art Conformer model. In particular, we\nshow that we are able to extract the correct names from masked training\nutterances with 11.8% accuracy, while the model outputs some name from the\ntrain set 55.2% of the time. Further, we show that even in a setting that uses\nsynthetic audio and partial transcripts from the test set, our method achieves\n2.5% correct name accuracy (47.7% any name success rate). Lastly, we design\nWord Dropout, a data augmentation method that we show when used in training\nalong with Multistyle TRaining (MTR), provides comparable utility as the\nbaseline, along with significantly mitigating extraction via Noise Masking\nacross the four evaluated settings.", "published": "2022-04-18 14:43:17", "link": "http://arxiv.org/abs/2204.08345v2", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AB/BA analysis: A framework for estimating keyword spotting recall\n  improvement while maintaining audio privacy", "abstract": "Evaluation of keyword spotting (KWS) systems that detect keywords in speech\nis a challenging task under realistic privacy constraints. The KWS is designed\nto only collect data when the keyword is present, limiting the availability of\nhard samples that may contain false negatives, and preventing direct estimation\nof model recall from production data. Alternatively, complementary data\ncollected from other sources may not be fully representative of the real\napplication. In this work, we propose an evaluation technique which we call\nAB/BA analysis. Our framework evaluates a candidate KWS model B against a\nbaseline model A, using cross-dataset offline decoding for relative recall\nestimation, without requiring negative examples. Moreover, we propose a\nformulation with assumptions that allow estimation of relative false positive\nrate between models with low variance even when the number of false positives\nis small. Finally, we propose to leverage machine-generated soft labels, in a\ntechnique we call Semi-Supervised AB/BA analysis, that improves the analysis\ntime, privacy, and cost. Experiments with both simulation and real data show\nthat AB/BA analysis is successful at measuring recall improvement in\nconjunction with the trade-off in relative false positive rate.", "published": "2022-04-18 13:52:22", "link": "http://arxiv.org/abs/2204.08474v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Robust, Nonparametric, Efficient Decomposition of Spectral Peaks under\n  Distortion and Interference", "abstract": "We propose a decomposition method for the spectral peaks in an observed\nfrequency spectrum, which is efficiently acquired by utilizing the Fast Fourier\nTransform. In contrast to the traditional methods of waveform fitting on the\nspectrum, we optimize the problem from a more robust perspective. We model the\npeaks in spectrum as pseudo-symmetric functions, where the only constraint is a\nnonincreasing behavior around a central frequency when the distance increases.\nOur approach is more robust against arbitrary distortion, interference and\nnoise on the spectrum that may be caused by an observation system. The time\ncomplexity of our method is linear, i.e., $O(N)$ per extracted spectral peak.\nMoreover, the decomposed spectral peaks show a pseudo-orthogonal behavior,\nwhere they conform to a power preserving equality.", "published": "2022-04-18 17:08:37", "link": "http://arxiv.org/abs/2204.08411v1", "categories": ["eess.SP", "cs.LG", "eess.AS", "math.OC", "stat.ML"], "primary_category": "eess.SP"}
