{"title": "Strictly Breadth-First AMR Parsing", "abstract": "AMR parsing is the task that maps a sentence to an AMR semantic graph\nautomatically. We focus on the breadth-first strategy of this task, which was\nproposed recently and achieved better performance than other strategies.\nHowever, current models under this strategy only \\emph{encourage} the model to\nproduce the AMR graph in breadth-first order, but \\emph{cannot guarantee} this.\nTo solve this problem, we propose a new architecture that \\emph{guarantees}\nthat the parsing will strictly follow the breadth-first order. In each parsing\nstep, we introduce a \\textbf{focused parent} vertex and use this vertex to\nguide the generation. With the help of this new architecture and some other\nimprovements in the sentence and graph encoder, our model obtains better\nperformance on both the AMR 1.0 and 2.0 dataset.", "published": "2022-11-08 00:42:27", "link": "http://arxiv.org/abs/2211.03922v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tell Your Story: Task-Oriented Dialogs for Interactive Content Creation", "abstract": "People capture photos and videos to relive and share memories of personal\nsignificance. Recently, media montages (stories) have become a popular mode of\nsharing these memories due to their intuitive and powerful storytelling\ncapabilities. However, creating such montages usually involves a lot of manual\nsearches, clicks, and selections that are time-consuming and cumbersome,\nadversely affecting user experiences.\n  To alleviate this, we propose task-oriented dialogs for montage creation as a\nnovel interactive tool to seamlessly search, compile, and edit montages from a\nmedia collection. To the best of our knowledge, our work is the first to\nleverage multi-turn conversations for such a challenging application, extending\nthe previous literature studying simple media retrieval tasks. We collect a new\ndataset C3 (Conversational Content Creation), comprising 10k dialogs\nconditioned on media montages simulated from a large media collection.\n  We take a simulate-and-paraphrase approach to collect these dialogs to be\nboth cost and time efficient, while drawing from natural language distribution.\nOur analysis and benchmarking of state-of-the-art language models showcase the\nmultimodal challenges present in the dataset. Lastly, we present a real-world\nmobile demo application that shows the feasibility of the proposed work in\nreal-world applications. Our code and data will be made publicly available.", "published": "2022-11-08 01:23:59", "link": "http://arxiv.org/abs/2211.03940v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Unstructured Knowledge Access in Conversational Dialogue with ASR\n  Errors", "abstract": "Performance of spoken language understanding (SLU) can be degraded with\nautomatic speech recognition (ASR) errors. We propose a novel approach to\nimprove SLU robustness by randomly corrupting clean training text with an ASR\nerror simulator, followed by self-correcting the errors and minimizing the\ntarget classification loss in a joint manner. In the proposed error simulator,\nwe leverage confusion networks generated from an ASR decoder without human\ntranscriptions to generate a variety of error patterns for model training. We\nevaluate our approach on the DSTC10 challenge targeted for knowledge-grounded\ntask-oriented conversational dialogues with ASR errors. Experimental results\nshow the effectiveness of our proposed approach, boosting the knowledge-seeking\nturn detection (KTD) F1 significantly from 0.9433 to 0.9904. Knowledge cluster\nclassification is boosted from 0.7924 to 0.9333 in Recall@1. After knowledge\ndocument re-ranking, our approach shows significant improvement in all\nknowledge selection metrics, from 0.7358 to 0.7806 in Recall@1, from 0.8301 to\n0.9333 in Recall@5, and from 0.7798 to 0.8460 in MRR@5 on the test set. In the\nrecent DSTC10 evaluation, our approach demonstrates significant improvement in\nknowledge selection, boosting Recall@1 from 0.495 to 0.7144 compared to the\nofficial baseline. Our source code is released in GitHub\nhttps://github.com/yctam/dstc10_track2_task2.git.", "published": "2022-11-08 04:04:02", "link": "http://arxiv.org/abs/2211.03990v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Knowledge Is Needed? Towards Explainable Memory for kNN-MT Domain\n  Adaptation", "abstract": "kNN-MT presents a new paradigm for domain adaptation by building an external\ndatastore, which usually saves all target language token occurrences in the\nparallel corpus. As a result, the constructed datastore is usually large and\npossibly redundant. In this paper, we investigate the interpretability issue of\nthis approach: what knowledge does the NMT model need? We propose the notion of\nlocal correctness (LAC) as a new angle, which describes the potential\ntranslation correctness for a single entry and for a given neighborhood.\nEmpirical study shows that our investigation successfully finds the conditions\nwhere the NMT model could easily fail and need related knowledge. Experiments\non six diverse target domains and two language-pairs show that pruning\naccording to local correctness brings a light and more explainable memory for\nkNN-MT domain adaptation.", "published": "2022-11-08 07:23:09", "link": "http://arxiv.org/abs/2211.04052v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COPEN: Probing Conceptual Knowledge in Pre-trained Language Models", "abstract": "Conceptual knowledge is fundamental to human cognition and knowledge bases.\nHowever, existing knowledge probing works only focus on evaluating factual\nknowledge of pre-trained language models (PLMs) and ignore conceptual\nknowledge. Since conceptual knowledge often appears as implicit commonsense\nbehind texts, designing probes for conceptual knowledge is hard. Inspired by\nknowledge representation schemata, we comprehensively evaluate conceptual\nknowledge of PLMs by designing three tasks to probe whether PLMs organize\nentities by conceptual similarities, learn conceptual properties, and\nconceptualize entities in contexts, respectively. For the tasks, we collect and\nannotate 24k data instances covering 393 concepts, which is COPEN, a COnceptual\nknowledge Probing bENchmark. Extensive experiments on different sizes and types\nof PLMs show that existing PLMs systematically lack conceptual knowledge and\nsuffer from various spurious correlations. We believe this is a critical\nbottleneck for realizing human-like cognition in PLMs. COPEN and our codes are\npublicly released at https://github.com/THU-KEG/COPEN.", "published": "2022-11-08 08:18:06", "link": "http://arxiv.org/abs/2211.04079v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conciseness: An Overlooked Language Task", "abstract": "We report on novel investigations into training models that make sentences\nconcise. We define the task and show that it is different from related tasks\nsuch as summarization and simplification. For evaluation, we release two test\nsets, consisting of 2000 sentences each, that were annotated by two and five\nhuman annotators, respectively. We demonstrate that conciseness is a difficult\ntask for which zero-shot setups with large neural language models often do not\nperform well. Given the limitations of these approaches, we propose a synthetic\ndata generation method based on round-trip translations. Using this data to\neither train Transformers from scratch or fine-tune T5 models yields our\nstrongest baselines that can be further improved by fine-tuning on an\nartificial conciseness dataset that we derived from multi-annotator machine\ntranslation test sets.", "published": "2022-11-08 09:47:11", "link": "http://arxiv.org/abs/2211.04126v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Active Learning with Tabular Language Models", "abstract": "Despite recent advancements in tabular language model research, real-world\napplications are still challenging. In industry, there is an abundance of\ntables found in spreadsheets, but acquisition of substantial amounts of labels\nis expensive, since only experts can annotate the often highly technical and\ndomain-specific tables. Active learning could potentially reduce labeling\ncosts, however, so far there are no works related to active learning in\nconjunction with tabular language models. In this paper we investigate\ndifferent acquisition functions in a real-world industrial tabular language\nmodel use case for sub-cell named entity recognition. Our results show that\ncell-level acquisition functions with built-in diversity can significantly\nreduce the labeling effort, while enforced table diversity is detrimental. We\nfurther see open fundamental questions concerning computational efficiency and\nthe perspective of human annotators.", "published": "2022-11-08 09:50:30", "link": "http://arxiv.org/abs/2211.04128v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Perspectives on neural proof nets", "abstract": "In this paper I will present a novel way of combining proof net proof search\nwith neural networks. It contrasts with the 'standard' approach which has been\napplied to proof search in type-logical grammars in various different forms. In\nthe standard approach, we first transform words to formulas (supertagging) then\nmatch atomic formulas to obtain a proof. I will introduce an alternative way to\nsplit the task into two: first, we generate the graph structure in a way which\nguarantees it corresponds to a lambda-term, then we obtain the detailed\nstructure using vertex labelling. Vertex labelling is a well-studied task in\ngraph neural networks, and different ways of implementing graph generation\nusing neural networks will be explored.", "published": "2022-11-08 10:17:59", "link": "http://arxiv.org/abs/2211.04141v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Third-Party Aligner for Neural Word Alignments", "abstract": "Word alignment is to find translationally equivalent words between source and\ntarget sentences. Previous work has demonstrated that self-training can achieve\ncompetitive word alignment results. In this paper, we propose to use word\nalignments generated by a third-party word aligner to supervise the neural word\nalignment training. Specifically, source word and target word of each word pair\naligned by the third-party aligner are trained to be close neighbors to each\nother in the contextualized embedding space when fine-tuning a pre-trained\ncross-lingual language model. Experiments on the benchmarks of various language\npairs show that our approach can surprisingly do self-correction over the\nthird-party supervision by finding more accurate word alignments and deleting\nwrong word alignments, leading to better performance than various third-party\nword aligners, including the currently best one. When we integrate all\nsupervisions from various third-party aligners, we achieve state-of-the-art\nword alignment performances, with averagely more than two points lower\nalignment error rates than the best third-party aligner. We released our code\nat https://github.com/sdongchuanqi/Third-Party-Supervised-Aligner.", "published": "2022-11-08 12:30:08", "link": "http://arxiv.org/abs/2211.04198v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging Fairness and Environmental Sustainability in Natural Language\n  Processing", "abstract": "Fairness and environmental impact are important research directions for the\nsustainable development of artificial intelligence. However, while each topic\nis an active research area in natural language processing (NLP), there is a\nsurprising lack of research on the interplay between the two fields. This\nlacuna is highly problematic, since there is increasing evidence that an\nexclusive focus on fairness can actually hinder environmental sustainability,\nand vice versa. In this work, we shed light on this crucial intersection in NLP\nby (1) investigating the efficiency of current fairness approaches through\nsurveying example methods for reducing unfair stereotypical bias from the\nliterature, and (2) evaluating a common technique to reduce energy consumption\n(and thus environmental impact) of English NLP models, knowledge distillation\n(KD), for its impact on fairness. In this case study, we evaluate the effect of\nimportant KD factors, including layer and dimensionality reduction, with\nrespect to: (a) performance on the distillation task (natural language\ninference and semantic similarity prediction), and (b) multiple measures and\ndimensions of stereotypical bias (e.g., gender bias measured via the Word\nEmbedding Association Test). Our results lead us to clarify current assumptions\nregarding the effect of KD on unfair bias: contrary to other findings, we show\nthat KD can actually decrease model fairness.", "published": "2022-11-08 14:05:07", "link": "http://arxiv.org/abs/2211.04256v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SocioProbe: What, When, and Where Language Models Learn about\n  Sociodemographics", "abstract": "Pre-trained language models (PLMs) have outperformed other NLP models on a\nwide range of tasks. Opting for a more thorough understanding of their\ncapabilities and inner workings, researchers have established the extend to\nwhich they capture lower-level knowledge like grammaticality, and mid-level\nsemantic knowledge like factual understanding. However, there is still little\nunderstanding of their knowledge of higher-level aspects of language. In\nparticular, despite the importance of sociodemographic aspects in shaping our\nlanguage, the questions of whether, where, and how PLMs encode these aspects,\ne.g., gender or age, is still unexplored. We address this research gap by\nprobing the sociodemographic knowledge of different single-GPU PLMs on multiple\nEnglish data sets via traditional classifier probing and information-theoretic\nminimum description length probing. Our results show that PLMs do encode these\nsociodemographics, and that this knowledge is sometimes spread across the\nlayers of some of the tested PLMs. We further conduct a multilingual analysis\nand investigate the effect of supplementary training to further explore to what\nextent, where, and with what amount of pre-training data the knowledge is\nencoded. Our overall results indicate that sociodemographic knowledge is still\na major challenge for NLP. PLMs require large amounts of pre-training data to\nacquire the knowledge and models that excel in general language understanding\ndo not seem to own more knowledge about these aspects.", "published": "2022-11-08 14:37:45", "link": "http://arxiv.org/abs/2211.04281v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt-Based Metric Learning for Few-Shot NER", "abstract": "Few-shot named entity recognition (NER) targets generalizing to unseen labels\nand/or domains with few labeled examples. Existing metric learning methods\ncompute token-level similarities between query and support sets, but are not\nable to fully incorporate label semantics into modeling. To address this issue,\nwe propose a simple method to largely improve metric learning for NER: 1)\nmultiple prompt schemas are designed to enhance label semantics; 2) we propose\na novel architecture to effectively combine multiple prompt-based\nrepresentations. Empirically, our method achieves new state-of-the-art (SOTA)\nresults under 16 of the 18 considered settings, substantially outperforming the\nprevious SOTA by an average of 8.84% and a maximum of 34.51% in relative gains\nof micro F1. Our code is available at https://github.com/AChen-qaq/ProML.", "published": "2022-11-08 15:59:13", "link": "http://arxiv.org/abs/2211.04337v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NaturalAdversaries: Can Naturalistic Adversaries Be as Effective as\n  Artificial Adversaries?", "abstract": "While a substantial body of prior work has explored adversarial example\ngeneration for natural language understanding tasks, these examples are often\nunrealistic and diverge from the real-world data distributions. In this work,\nwe introduce a two-stage adversarial example generation framework\n(NaturalAdversaries), for designing adversaries that are effective at fooling a\ngiven classifier and demonstrate natural-looking failure cases that could\nplausibly occur during in-the-wild deployment of the models.\n  At the first stage a token attribution method is used to summarize a given\nclassifier's behaviour as a function of the key tokens in the input. In the\nsecond stage a generative model is conditioned on the key tokens from the first\nstage. NaturalAdversaries is adaptable to both black-box and white-box\nadversarial attacks based on the level of access to the model parameters. Our\nresults indicate these adversaries generalize across domains, and offer\ninsights for future research on improving robustness of neural text\nclassification models.", "published": "2022-11-08 16:37:34", "link": "http://arxiv.org/abs/2211.04364v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "nBIIG: A Neural BI Insights Generation System for Table Reporting", "abstract": "We present nBIIG, a neural Business Intelligence (BI) Insights Generation\nsystem. Given a table, our system applies various analyses to create\ncorresponding RDF representations, and then uses a neural model to generate\nfluent textual insights out of these representations. The generated insights\ncan be used by an analyst, via a human-in-the-loop paradigm, to enhance the\ntask of creating compelling table reports. The underlying generative neural\nmodel is trained over large and carefully distilled data, curated from multiple\nBI domains. Thus, the system can generate faithful and fluent insights over\nopen-domain tables, making it practical and useful.", "published": "2022-11-08 18:00:59", "link": "http://arxiv.org/abs/2211.04417v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hyperbolic Centroid Calculations for Text Classification", "abstract": "A new development in NLP is the construction of hyperbolic word embeddings.\nAs opposed to their Euclidean counterparts, hyperbolic embeddings are\nrepresented not by vectors, but by points in hyperbolic space. This makes the\nmost common basic scheme for constructing document representations, namely the\naveraging of word vectors, meaningless in the hyperbolic setting. We\nreinterpret the vector mean as the centroid of the points represented by the\nvectors, and investigate various hyperbolic centroid schemes and their\neffectiveness at text classification.", "published": "2022-11-08 18:55:09", "link": "http://arxiv.org/abs/2211.04462v1", "categories": ["cs.CL", "51M10", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Toward a Neural Semantic Parsing System for EHR Question Answering", "abstract": "Clinical semantic parsing (SP) is an important step toward identifying the\nexact information need (as a machine-understandable logical form) from a\nnatural language query aimed at retrieving information from electronic health\nrecords (EHRs). Current approaches to clinical SP are largely based on\ntraditional machine learning and require hand-building a lexicon. The recent\nadvancements in neural SP show a promise for building a robust and flexible\nsemantic parser without much human effort. Thus, in this paper, we aim to\nsystematically assess the performance of two such neural SP models for EHR\nquestion answering (QA). We found that the performance of these advanced neural\nmodels on two clinical SP datasets is promising given their ease of application\nand generalizability. Our error analysis surfaces the common types of errors\nmade by these models and has the potential to inform future research into\nimproving the performance of neural SP models for EHR QA.", "published": "2022-11-08 21:36:22", "link": "http://arxiv.org/abs/2211.04569v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Graph-aware Multi-View Fusion for Rumor Detection on Social\n  Media", "abstract": "Automatic detecting rumors on social media has become a challenging task.\nPrevious studies focus on learning indicative clues from conversation threads\nfor identifying rumorous information. However, these methods only model\nrumorous conversation threads from various views but fail to fuse multi-view\nfeatures very well. In this paper, we propose a novel multi-view fusion\nframework for rumor representation learning and classification. It encodes the\nmultiple views based on Graph Convolutional Networks (GCN), and leverages\nConvolutional Neural Networks (CNN) to capture the consistent and complementary\ninformation among all views and fuse them together. Experimental results on two\npublic datasets demonstrate that our method outperforms state-of-the-art\napproaches.", "published": "2022-11-08 13:27:43", "link": "http://arxiv.org/abs/2212.02419v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameter and Data Efficient Continual Pre-training for Robustness to\n  Dialectal Variance in Arabic", "abstract": "The use of multilingual language models for tasks in low and high-resource\nlanguages has been a success story in deep learning. In recent times, Arabic\nhas been receiving widespread attention on account of its dialectal variance.\nWhile prior research studies have tried to adapt these multilingual models for\ndialectal variants of Arabic, it still remains a challenging problem owing to\nthe lack of sufficient monolingual dialectal data and parallel translation data\nof such dialectal variants. It remains an open problem on whether the limited\ndialectical data can be used to improve the models trained in Arabic on its\ndialectal variants. First, we show that multilingual-BERT (mBERT) incrementally\npretrained on Arabic monolingual data takes less training time and yields\ncomparable accuracy when compared to our custom monolingual Arabic model and\nbeat existing models (by an avg metric of +$6.41$). We then explore two\ncontinual pre-training methods -- (1) using small amounts of dialectical data\nfor continual finetuning and (2) parallel Arabic to English data and a\nTranslation Language Modeling loss function. We show that both approaches help\nimprove performance on dialectal classification tasks ($+4.64$ avg. gain) when\nused on monolingual models.", "published": "2022-11-08 02:51:57", "link": "http://arxiv.org/abs/2211.03966v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Domain Adaptation for Sparse Retrieval by Filling\n  Vocabulary and Word Frequency Gaps", "abstract": "IR models using a pretrained language model significantly outperform lexical\napproaches like BM25. In particular, SPLADE, which encodes texts to sparse\nvectors, is an effective model for practical use because it shows robustness to\nout-of-domain datasets. However, SPLADE still struggles with exact matching of\nlow-frequency words in training data. In addition, domain shifts in vocabulary\nand word frequencies deteriorate the IR performance of SPLADE. Because\nsupervision data are scarce in the target domain, addressing the domain shifts\nwithout supervision data is necessary. This paper proposes an unsupervised\ndomain adaptation method by filling vocabulary and word-frequency gaps. First,\nwe expand a vocabulary and execute continual pretraining with a masked language\nmodel on a corpus of the target domain. Then, we multiply SPLADE-encoded sparse\nvectors by inverse document frequency weights to consider the importance of\ndocuments with lowfrequency words. We conducted experiments using our method on\ndatasets with a large vocabulary gap from a source domain. We show that our\nmethod outperforms the present stateof-the-art domain adaptation method. In\naddition, our method achieves state-of-the-art results, combined with BM25.", "published": "2022-11-08 03:58:26", "link": "http://arxiv.org/abs/2211.03988v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "COV19IR : COVID-19 Domain Literature Information Retrieval", "abstract": "Increasing number of COVID-19 research literatures cause new challenges in\neffective literature screening and COVID-19 domain knowledge aware Information\nRetrieval. To tackle the challenges, we demonstrate two tasks along\nwithsolutions, COVID-19 literature retrieval, and question answering. COVID-19\nliterature retrieval task screens matching COVID-19 literature documents for\ntextual user query, and COVID-19 question answering task predicts proper text\nfragments from text corpus as the answer of specific COVID-19 related\nquestions. Based on transformer neural network, we provided solutions to\nimplement the tasks on CORD-19 dataset, we display some examples to show the\neffectiveness of our proposed solutions.", "published": "2022-11-08 05:12:37", "link": "http://arxiv.org/abs/2211.04013v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Dynamic Graph Interactive Framework with Label-Semantic Injection for\n  Spoken Language Understanding", "abstract": "Multi-intent detection and slot filling joint models are gaining increasing\ntraction since they are closer to complicated real-world scenarios. However,\nexisting approaches (1) focus on identifying implicit correlations between\nutterances and one-hot encoded labels in both tasks while ignoring explicit\nlabel characteristics; (2) directly incorporate multi-intent information for\neach token, which could lead to incorrect slot prediction due to the\nintroduction of irrelevant intent. In this paper, we propose a framework termed\nDGIF, which first leverages the semantic information of labels to give the\nmodel additional signals and enriched priors. Then, a multi-grain interactive\ngraph is constructed to model correlations between intents and slots.\nSpecifically, we propose a novel approach to construct the interactive graph\nbased on the injection of label semantics, which can automatically update the\ngraph to better alleviate error propagation. Experimental results show that our\nframework significantly outperforms existing approaches, obtaining a relative\nimprovement of 13.7% over the previous best model on the MixATIS dataset in\noverall accuracy.", "published": "2022-11-08 05:57:46", "link": "http://arxiv.org/abs/2211.04023v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ConsPrompt: Exploiting Contrastive Samples for Fewshot Prompt Learning", "abstract": "The prompt has become an effective linguistic tool for utilizing pre-trained\nlanguage models. However, in few-shot scenarios, subtle changes in the prompt\ndesign always make the result widely different, and the prompt learning methods\nalso make it easy to overfit the limited samples. To alleviate this, we explore\nutilizing suitable contrastive samples and multi-degree contrastive learning\nmethods to improve the robustness of the prompt representation. Therefore, the\nproposed Consprompt combined with the prompt encoding network, contrastive\nsampling modules, and contrastive scoring modules, is introduced to realize\ndifferential contrastive learning. Our results exhibit state-of-the-art\nperformance in different few-shot settings, and the ablation experiments also\ncertify the effectiveness of utilizing multi-degree contrastive learning in the\nprompt-based fine-tuning process.", "published": "2022-11-08 09:29:45", "link": "http://arxiv.org/abs/2211.04118v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Query-Specific Knowledge Graphs for Complex Finance Topics", "abstract": "Across the financial domain, researchers answer complex questions by\nextensively \"searching\" for relevant information to generate long-form reports.\nThis workshop paper discusses automating the construction of query-specific\ndocument and entity knowledge graphs (KGs) for complex research topics. We\nfocus on the CODEC dataset, where domain experts (1) create challenging\nquestions, (2) construct long natural language narratives, and (3) iteratively\nsearch and assess the relevance of documents and entities. For the construction\nof query-specific KGs, we show that state-of-the-art ranking systems have\nheadroom for improvement, with specific failings due to a lack of context or\nexplicit knowledge representation. We demonstrate that entity and document\nrelevance are positively correlated, and that entity-based query feedback\nimproves document ranking effectiveness. Furthermore, we construct\nquery-specific KGs using retrieval and evaluate using CODEC's \"ground-truth\ngraphs\", showing the precision and recall trade-offs. Lastly, we point to\nfuture work, including adaptive KG retrieval algorithms and GNN-based weighting\nmethods, while highlighting key challenges such as high-quality data,\ninformation extraction recall, and the size and sparsity of complex topic\ngraphs.", "published": "2022-11-08 10:21:13", "link": "http://arxiv.org/abs/2211.04142v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Preserving Semantics in Textual Adversarial Attacks", "abstract": "The growth of hateful online content, or hate speech, has been associated\nwith a global increase in violent crimes against minorities [23]. Harmful\nonline content can be produced easily, automatically and anonymously. Even\nthough, some form of auto-detection is already achieved through text\nclassifiers in NLP, they can be fooled by adversarial attacks. To strengthen\nexisting systems and stay ahead of attackers, we need better adversarial\nattacks. In this paper, we show that up to 70% of adversarial examples\ngenerated by adversarial attacks should be discarded because they do not\npreserve semantics. We address this core weakness and propose a new, fully\nsupervised sentence embedding technique called Semantics-Preserving-Encoder\n(SPE). Our method outperforms existing sentence encoders used in adversarial\nattacks by achieving 1.2x - 5.1x better real attack success rate. We release\nour code as a plugin that can be used in any existing adversarial attack to\nimprove its quality and speed up its execution.", "published": "2022-11-08 12:40:07", "link": "http://arxiv.org/abs/2211.04205v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Active Relation Discovery: Towards General and Label-aware Open Relation\n  Extraction", "abstract": "Open Relation Extraction (OpenRE) aims to discover novel relations from open\ndomains. Previous OpenRE methods mainly suffer from two problems: (1)\nInsufficient capacity to discriminate between known and novel relations. When\nextending conventional test settings to a more general setting where test data\nmight also come from seen classes, existing approaches have a significant\nperformance decline. (2) Secondary labeling must be performed before practical\napplication. Existing methods cannot label human-readable and meaningful types\nfor novel relations, which is urgently required by the downstream tasks. To\naddress these issues, we propose the Active Relation Discovery (ARD) framework,\nwhich utilizes relational outlier detection for discriminating known and novel\nrelations and involves active learning for labeling novel relations. Extensive\nexperiments on three real-world datasets show that ARD significantly\noutperforms previous state-of-the-art methods on both conventional and our\nproposed general OpenRE settings. The source code and datasets will be\navailable for reproducibility.", "published": "2022-11-08 13:01:17", "link": "http://arxiv.org/abs/2211.04215v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-conditioned Embedding Diffusion for Text Generation", "abstract": "Can continuous diffusion models bring the same performance breakthrough on\nnatural language they did for image generation? To circumvent the discrete\nnature of text data, we can simply project tokens in a continuous space of\nembeddings, as is standard in language modeling. We propose Self-conditioned\nEmbedding Diffusion, a continuous diffusion mechanism that operates on token\nembeddings and allows to learn flexible and scalable diffusion models for both\nconditional and unconditional text generation. Through qualitative and\nquantitative evaluation, we show that our text diffusion models generate\nsamples comparable with those produced by standard autoregressive language\nmodels - while being in theory more efficient on accelerator hardware at\ninference time. Our work paves the way for scaling up diffusion models for\ntext, similarly to autoregressive models, and for improving performance with\nrecent refinements to continuous diffusion.", "published": "2022-11-08 13:30:27", "link": "http://arxiv.org/abs/2211.04236v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Review of coreference resolution in English and Persian", "abstract": "Coreference resolution (CR), identifying expressions referring to the same\nreal-world entity, is a fundamental challenge in natural language processing\n(NLP). This paper explores the latest advancements in CR, spanning coreference\nand anaphora resolution. We critically analyze the diverse corpora that have\nfueled CR research, highlighting their strengths, limitations, and suitability\nfor various tasks. We examine the spectrum of evaluation metrics used to assess\nCR systems, emphasizing their advantages, disadvantages, and the need for more\nnuanced, task-specific metrics. Tracing the evolution of CR algorithms, we\nprovide a detailed overview of methodologies, from rule-based approaches to\ncutting-edge deep learning architectures. We delve into mention-pair,\nentity-based, cluster-ranking, sequence-to-sequence, and graph neural network\nmodels, elucidating their theoretical foundations and performance on benchmark\ndatasets. Recognizing the unique challenges of Persian CR, we dedicate a\nfocused analysis to this under-resourced language. We examine existing Persian\nCR systems and highlight the emergence of end-to-end neural models leveraging\npre-trained language models like ParsBERT. This review is an essential resource\nfor researchers and practitioners, offering a comprehensive overview of the\ncurrent state-of-the-art in CR, identifying key challenges, and charting a\ncourse for future research in this rapidly evolving field.", "published": "2022-11-08 18:14:09", "link": "http://arxiv.org/abs/2211.04428v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SLATE: A Sequence Labeling Approach for Task Extraction from Free-form\n  Inked Content", "abstract": "We present SLATE, a sequence labeling approach for extracting tasks from\nfree-form content such as digitally handwritten (or \"inked\") notes on a virtual\nwhiteboard. Our approach allows us to create a single, low-latency model to\nsimultaneously perform sentence segmentation and classification of these\nsentences into task/non-task sentences. SLATE greatly outperforms a baseline\ntwo-model (sentence segmentation followed by classification model) approach,\nachieving a task F1 score of 84.4%, a sentence segmentation (boundary\nsimilarity) score of 88.4% and three times lower latency compared to the\nbaseline. Furthermore, we provide insights into tackling challenges of\nperforming NLP on the inking domain. We release both our code and dataset for\nthis novel task.", "published": "2022-11-08 18:46:21", "link": "http://arxiv.org/abs/2211.04454v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Active Example Selection for In-Context Learning", "abstract": "With a handful of demonstration examples, large-scale language models show\nstrong capability to perform various tasks by in-context learning from these\nexamples, without any fine-tuning. We demonstrate that in-context learning\nperformance can be highly unstable across samples of examples, indicating the\nidiosyncrasies of how language models acquire information. We formulate example\nselection for in-context learning as a sequential decision problem, and propose\na reinforcement learning algorithm for identifying generalizable policies to\nselect demonstration examples. For GPT-2, our learned policies demonstrate\nstrong abilities of generalizing to unseen tasks in training, with a $5.8\\%$\nimprovement on average. Examples selected from our learned policies can even\nachieve a small improvement on GPT-3 Ada. However, the improvement diminishes\non larger GPT-3 models, suggesting emerging capabilities of large language\nmodels.", "published": "2022-11-08 19:00:02", "link": "http://arxiv.org/abs/2211.04486v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Going for GOAL: A Resource for Grounded Football Commentaries", "abstract": "Recent video+language datasets cover domains where the interaction is highly\nstructured, such as instructional videos, or where the interaction is scripted,\nsuch as TV shows. Both of these properties can lead to spurious cues to be\nexploited by models rather than learning to ground language. In this paper, we\npresent GrOunded footbAlL commentaries (GOAL), a novel dataset of football (or\n`soccer') highlights videos with transcribed live commentaries in English. As\nthe course of a game is unpredictable, so are commentaries, which makes them a\nunique resource to investigate dynamic language grounding. We also provide\nstate-of-the-art baselines for the following tasks: frame reordering, moment\nretrieval, live commentary retrieval and play-by-play live commentary\ngeneration. Results show that SOTA models perform reasonably well in most\ntasks. We discuss the implications of these results and suggest new tasks for\nwhich GOAL can be used. Our codebase is available at:\nhttps://gitlab.com/grounded-sport-convai/goal-baselines.", "published": "2022-11-08 20:04:27", "link": "http://arxiv.org/abs/2211.04534v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Detecting Euphemisms with Literal Descriptions and Visual Imagery", "abstract": "This paper describes our two-stage system for the Euphemism Detection shared\ntask hosted by the 3rd Workshop on Figurative Language Processing in\nconjunction with EMNLP 2022. Euphemisms tone down expressions about sensitive\nor unpleasant issues like addiction and death. The ambiguous nature of\neuphemistic words or expressions makes it challenging to detect their actual\nmeaning within a context. In the first stage, we seek to mitigate this\nambiguity by incorporating literal descriptions into input text prompts to our\nbaseline model. It turns out that this kind of direct supervision yields\nremarkable performance improvement. In the second stage, we integrate visual\nsupervision into our system using visual imageries, two sets of images\ngenerated by a text-to-image model by taking terms and descriptions as input.\nOur experiments demonstrate that visual supervision also gives a statistically\nsignificant performance boost. Our system achieved the second place with an F1\nscore of 87.2%, only about 0.9% worse than the best submission.", "published": "2022-11-08 21:50:05", "link": "http://arxiv.org/abs/2211.04576v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Proactive Detractor Detection Framework Based on Message-Wise Sentiment\n  Analysis Over Customer Support Interactions", "abstract": "In this work, we propose a framework relying solely on chat-based customer\nsupport (CS) interactions for predicting the recommendation decision of\nindividual users. For our case study, we analyzed a total number of 16.4k users\nand 48.7k customer support conversations within the financial vertical of a\nlarge e-commerce company in Latin America. Consequently, our main contributions\nand objectives are to use Natural Language Processing (NLP) to assess and\npredict the recommendation behavior where, in addition to using static\nsentiment analysis, we exploit the predictive power of each user's sentiment\ndynamics. Our results show that, with respective feature interpretability, it\nis possible to predict the likelihood of a user to recommend a product or\nservice, based solely on the message-wise sentiment evolution of their CS\nconversations in a fully automated way.", "published": "2022-11-08 00:43:36", "link": "http://arxiv.org/abs/2211.03923v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Comparative layer-wise analysis of self-supervised speech models", "abstract": "Many self-supervised speech models, varying in their pre-training objective,\ninput modality, and pre-training data, have been proposed in the last few\nyears. Despite impressive successes on downstream tasks, we still have a\nlimited understanding of the properties encoded by the models and the\ndifferences across models. In this work, we examine the intermediate\nrepresentations for a variety of recent models. Specifically, we measure\nacoustic, phonetic, and word-level properties encoded in individual layers,\nusing a lightweight analysis tool based on canonical correlation analysis\n(CCA). We find that these properties evolve across layers differently depending\non the model, and the variations relate to the choice of pre-training\nobjective. We further investigate the utility of our analyses for downstream\ntasks by comparing the property trends with performance on speech recognition\nand spoken language understanding tasks. We discover that CCA trends provide\nreliable guidance to choose layers of interest for downstream tasks and that\nsingle-layer performance often matches or improves upon using all layers,\nsuggesting implications for more efficient use of pre-trained models.", "published": "2022-11-08 00:59:05", "link": "http://arxiv.org/abs/2211.03929v3", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ATCO2 corpus: A Large-Scale Dataset for Research on Automatic Speech\n  Recognition and Natural Language Understanding of Air Traffic Control\n  Communications", "abstract": "Personal assistants, automatic speech recognizers and dialogue understanding\nsystems are becoming more critical in our interconnected digital world. A clear\nexample is air traffic control (ATC) communications. ATC aims at guiding\naircraft and controlling the airspace in a safe and optimal manner. These\nvoice-based dialogues are carried between an air traffic controller (ATCO) and\npilots via very-high frequency radio channels. In order to incorporate these\nnovel technologies into ATC (low-resource domain), large-scale annotated\ndatasets are required to develop the data-driven AI systems. Two examples are\nautomatic speech recognition (ASR) and natural language understanding (NLU). In\nthis paper, we introduce the ATCO2 corpus, a dataset that aims at fostering\nresearch on the challenging ATC field, which has lagged behind due to lack of\nannotated data. The ATCO2 corpus covers 1) data collection and pre-processing,\n2) pseudo-annotations of speech data, and 3) extraction of ATC-related named\nentities. The ATCO2 corpus is split into three subsets. 1) ATCO2-test-set\ncorpus contains 4 hours of ATC speech with manual transcripts and a subset with\ngold annotations for named-entity recognition (callsign, command, value). 2)\nThe ATCO2-PL-set corpus consists of 5281 hours of unlabeled ATC data enriched\nwith automatic transcripts from an in-domain speech recognizer, contextual\ninformation, speaker turn information, signal-to-noise ratio estimate and\nEnglish language detection score per sample. Both available for purchase\nthrough ELDA at http://catalog.elra.info/en-us/repository/browse/ELRA-S0484. 3)\nThe ATCO2-test-set-1h corpus is a one-hour subset from the original test set\ncorpus, that we are offering for free at https://www.atco2.org/data. We expect\nthe ATCO2 corpus will foster research on robust ASR and NLU not only in the\nfield of ATC communications but also in the general research community.", "published": "2022-11-08 07:26:45", "link": "http://arxiv.org/abs/2211.04054v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "High-resolution embedding extractor for speaker diarisation", "abstract": "Speaker embedding extractors significantly influence the performance of\nclustering-based speaker diarisation systems. Conventionally, only one\nembedding is extracted from each speech segment. However, because of the\nsliding window approach, a segment easily includes two or more speakers owing\nto speaker change points. This study proposes a novel embedding extractor\narchitecture, referred to as a high-resolution embedding extractor (HEE), which\nextracts multiple high-resolution embeddings from each speech segment. Hee\nconsists of a feature-map extractor and an enhancer, where the enhancer with\nthe self-attention mechanism is the key to success. The enhancer of HEE\nreplaces the aggregation process; instead of a global pooling layer, the\nenhancer combines relative information to each frame via attention leveraging\nthe global context. Extracted dense frame-level embeddings can each represent a\nspeaker. Thus, multiple speakers can be represented by different frame-level\nfeatures in each segment. We also propose an artificially generating mixture\ndata training framework to train the proposed HEE. Through experiments on five\nevaluation sets, including four public datasets, the proposed HEE demonstrates\nat least 10% improvement on each evaluation set, except for one dataset, which\nwe analyse that rapid speaker changes less exist.", "published": "2022-11-08 07:41:18", "link": "http://arxiv.org/abs/2211.04060v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "BER: Balanced Error Rate For Speaker Diarization", "abstract": "DER is the primary metric to evaluate diarization performance while facing a\ndilemma: the errors in short utterances or segments tend to be overwhelmed by\nlonger ones. Short segments, e.g., `yes' or `no,' still have semantic\ninformation. Besides, DER overlooks errors in less-talked speakers. Although\nJER balances speaker errors, it still suffers from the same dilemma.\nConsidering all those aspects, duration error, segment error, and\nspeaker-weighted error constituting a complete diarization evaluation, we\npropose a Balanced Error Rate (BER) to evaluate speaker diarization. First, we\npropose a segment-level error rate (SER) via connected sub-graphs and adaptive\nIoU threshold to get accurate segment matching. Second, to evaluate diarization\nin a unified way, we adopt a speaker-specific harmonic mean between duration\nand segment, followed by a speaker-weighted average. Third, we analyze our\nmetric via the modularized system, EEND, and the multi-modal method on real\ndatasets. SER and BER are publicly available at https://github.com/X-LANCE/BER.", "published": "2022-11-08 15:17:39", "link": "http://arxiv.org/abs/2211.04304v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Multimodal Approach for Dementia Detection from Spontaneous Speech\n  with Tensor Fusion Layer", "abstract": "Alzheimer's disease (AD) is a progressive neurological disorder, meaning that\nthe symptoms develop gradually throughout the years. It is also the main cause\nof dementia, which affects memory, thinking skills, and mental abilities.\nNowadays, researchers have moved their interest towards AD detection from\nspontaneous speech, since it constitutes a time-effective procedure. However,\nexisting state-of-the-art works proposing multimodal approaches do not take\ninto consideration the inter- and intra-modal interactions and propose early\nand late fusion approaches. To tackle these limitations, we propose deep neural\nnetworks, which can be trained in an end-to-end trainable way and capture the\ninter- and intra-modal interactions. Firstly, each audio file is converted to\nan image consisting of three channels, i.e., log-Mel spectrogram, delta, and\ndelta-delta. Next, each transcript is passed through a BERT model followed by a\ngated self-attention layer. Similarly, each image is passed through a Swin\nTransformer followed by an independent gated self-attention layer. Acoustic\nfeatures are extracted also from each audio file. Finally, the representation\nvectors from the different modalities are fed to a tensor fusion layer for\ncapturing the inter-modal interactions. Extensive experiments conducted on the\nADReSS Challenge dataset indicate that our introduced approaches obtain\nvaluable advantages over existing research initiatives reaching Accuracy and\nF1-score up to 86.25% and 85.48% respectively.", "published": "2022-11-08 16:43:58", "link": "http://arxiv.org/abs/2211.04368v1", "categories": ["cs.CL", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Word Order Matters when you Increase Masking", "abstract": "Word order, an essential property of natural languages, is injected in\nTransformer-based neural language models using position encoding. However,\nrecent experiments have shown that explicit position encoding is not always\nuseful, since some models without such feature managed to achieve state-of-the\nart performance on some tasks. To understand better this phenomenon, we examine\nthe effect of removing position encodings on the pre-training objective itself\n(i.e., masked language modelling), to test whether models can reconstruct\nposition information from co-occurrences alone. We do so by controlling the\namount of masked tokens in the input sentence, as a proxy to affect the\nimportance of position information for the task. We find that the necessity of\nposition information increases with the amount of masking, and that masked\nlanguage models without position encodings are not able to reconstruct this\ninformation on the task. These findings point towards a direct relationship\nbetween the amount of masking and the ability of Transformers to capture\norder-sensitive aspects of language using position encoding.", "published": "2022-11-08 18:14:04", "link": "http://arxiv.org/abs/2211.04427v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Discover, Explanation, Improvement: An Automatic Slice Detection\n  Framework for Natural Language Processing", "abstract": "Pretrained natural language processing (NLP) models have achieved high\noverall performance, but they still make systematic errors. Instead of manual\nerror analysis, research on slice detection models (SDM), which automatically\nidentify underperforming groups of datapoints, has caught escalated attention\nin Computer Vision for both understanding model behaviors and providing\ninsights for future model training and designing. However, little research on\nSDM and quantitative evaluation of their effectiveness have been conducted on\nNLP tasks. Our paper fills the gap by proposing a benchmark named \"Discover,\nExplain, Improve (DEIM)\" for classification NLP tasks along with a new SDM\nEdisa. Edisa discovers coherent and underperforming groups of datapoints; DEIM\nthen unites them under human-understandable concepts and provides comprehensive\nevaluation tasks and corresponding quantitative metrics. The evaluation in DEIM\nshows that Edisa can accurately select error-prone datapoints with informative\nsemantic features that summarize error patterns. Detecting difficult datapoints\ndirectly boosts model performance without tuning any original model parameters,\nshowing that discovered slices are actionable for users.", "published": "2022-11-08 19:00:00", "link": "http://arxiv.org/abs/2211.04476v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SpeechMatrix: A Large-Scale Mined Corpus of Multilingual\n  Speech-to-Speech Translations", "abstract": "We present SpeechMatrix, a large-scale multilingual corpus of\nspeech-to-speech translations mined from real speech of European Parliament\nrecordings. It contains speech alignments in 136 language pairs with a total of\n418 thousand hours of speech. To evaluate the quality of this parallel speech,\nwe train bilingual speech-to-speech translation models on mined data only and\nestablish extensive baseline results on EuroParl-ST, VoxPopuli and FLEURS test\nsets. Enabled by the multilinguality of SpeechMatrix, we also explore\nmultilingual speech-to-speech translation, a topic which was addressed by few\nother works. We also demonstrate that model pre-training and sparse scaling\nusing Mixture-of-Experts bring large gains to translation performance. The\nmined data and models are freely available.", "published": "2022-11-08 19:09:27", "link": "http://arxiv.org/abs/2211.04508v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning to Follow Instructions in Text-Based Games", "abstract": "Text-based games present a unique class of sequential decision making problem\nin which agents interact with a partially observable, simulated environment via\nactions and observations conveyed through natural language. Such observations\ntypically include instructions that, in a reinforcement learning (RL) setting,\ncan directly or indirectly guide a player towards completing reward-worthy\ntasks. In this work, we study the ability of RL agents to follow such\ninstructions. We conduct experiments that show that the performance of\nstate-of-the-art text-based game agents is largely unaffected by the presence\nor absence of such instructions, and that these agents are typically unable to\nexecute tasks to completion. To further study and address the task of\ninstruction following, we equip RL agents with an internal structured\nrepresentation of natural language instructions in the form of Linear Temporal\nLogic (LTL), a formal language that is increasingly used for temporally\nextended reward specification in RL. Our framework both supports and highlights\nthe benefit of understanding the temporal semantics of instructions and in\nmeasuring progress towards achievement of such a temporally extended behaviour.\nExperiments with 500+ games in TextWorld demonstrate the superior performance\nof our approach.", "published": "2022-11-08 22:20:17", "link": "http://arxiv.org/abs/2211.04591v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "StructDiffusion: Language-Guided Creation of Physically-Valid Structures\n  using Unseen Objects", "abstract": "Robots operating in human environments must be able to rearrange objects into\nsemantically-meaningful configurations, even if these objects are previously\nunseen. In this work, we focus on the problem of building physically-valid\nstructures without step-by-step instructions. We propose StructDiffusion, which\ncombines a diffusion model and an object-centric transformer to construct\nstructures given partial-view point clouds and high-level language goals, such\nas \"set the table\". Our method can perform multiple challenging\nlanguage-conditioned multi-step 3D planning tasks using one model.\nStructDiffusion even improves the success rate of assembling physically-valid\nstructures out of unseen objects by on average 16% over an existing multi-modal\ntransformer model trained on specific structures. We show experiments on\nheld-out objects in both simulation and on real-world rearrangement tasks.\nImportantly, we show how integrating both a diffusion model and a\ncollision-discriminator model allows for improved generalization over other\nmethods when rearranging previously-unseen objects. For videos and additional\nresults, see our website: https://structdiffusion.github.io/.", "published": "2022-11-08 23:04:49", "link": "http://arxiv.org/abs/2211.04604v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "On Negative Sampling for Contrastive Audio-Text Retrieval", "abstract": "This paper investigates negative sampling for contrastive learning in the\ncontext of audio-text retrieval. The strategy for negative sampling refers to\nselecting negatives (either audio clips or textual descriptions) from a pool of\ncandidates for a positive audio-text pair. We explore sampling strategies via\nmodel-estimated within-modality and cross-modality relevance scores for audio\nand text samples. With a constant training setting on the retrieval system from\n[1], we study eight sampling strategies, including hard and semi-hard negative\nsampling. Experimental results show that retrieval performance varies\ndramatically among different strategies. Particularly, by selecting semi-hard\nnegatives with cross-modality scores, the retrieval system gains improved\nperformance in both text-to-audio and audio-to-text retrieval. Besides, we show\nthat feature collapse occurs while sampling hard negatives with cross-modality\nscores.", "published": "2022-11-08 08:03:05", "link": "http://arxiv.org/abs/2211.04070v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving performance of real-time full-band blind packet-loss\n  concealment with predictive network", "abstract": "Packet loss concealment (PLC) is a tool for enhancing speech degradation\ncaused by poor network conditions or underflow/overflow in audio processing\npipelines. We propose a real-time recurrent method that leverages previous\noutputs to mitigate artefact of lost packets without the prior knowledge of\nloss mask. The proposed full-band recurrent network (FRN) model operates at 48\nkHz, which is suitable for high-quality telecommunication applications.\nExperiment results highlight the superiority of FRN over an offline non-causal\nbaseline and a top performer in a recent PLC challenge.", "published": "2022-11-08 08:04:25", "link": "http://arxiv.org/abs/2211.04071v6", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pushing the limits of self-supervised speaker verification using\n  regularized distillation framework", "abstract": "Training robust speaker verification systems without speaker labels has long\nbeen a challenging task. Previous studies observed a large performance gap\nbetween self-supervised and fully supervised methods. In this paper, we apply a\nnon-contrastive self-supervised learning framework called DIstillation with NO\nlabels (DINO) and propose two regularization terms applied to embeddings in\nDINO. One regularization term guarantees the diversity of the embeddings, while\nthe other regularization term decorrelates the variables of each embedding. The\neffectiveness of various data augmentation techniques are explored, on both\ntime and frequency domain. A range of experiments conducted on the VoxCeleb\ndatasets demonstrate the superiority of the regularized DINO framework in\nspeaker verification. Our method achieves the state-of-the-art speaker\nverification performance under a single-stage self-supervised setting on\nVoxCeleb. Code has been made publicly available at\nhttps://github.com/alibaba-damo-academy/3D-Speaker.", "published": "2022-11-08 11:21:38", "link": "http://arxiv.org/abs/2211.04168v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cross-Attention is all you need: Real-Time Streaming Transformers for\n  Personalised Speech Enhancement", "abstract": "Personalised speech enhancement (PSE), which extracts only the speech of a\ntarget user and removes everything else from a recorded audio clip, can\npotentially improve users' experiences of audio AI modules deployed in the\nwild. To support a large variety of downstream audio tasks, such as real-time\nASR and audio-call enhancement, a PSE solution should operate in a streaming\nmode, i.e., input audio cleaning should happen in real-time with a small\nlatency and real-time factor. Personalisation is typically achieved by\nextracting a target speaker's voice profile from an enrolment audio, in the\nform of a static embedding vector, and then using it to condition the output of\na PSE model. However, a fixed target speaker embedding may not be optimal under\nall conditions. In this work, we present a streaming Transformer-based PSE\nmodel and propose a novel cross-attention approach that gives adaptive target\nspeaker representations. We present extensive experiments and show that our\nproposed cross-attention approach outperforms competitive baselines\nconsistently, even when our model is only approximately half the size.", "published": "2022-11-08 16:12:38", "link": "http://arxiv.org/abs/2211.04346v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised vocal dereverberation with diffusion-based generative\n  models", "abstract": "Removing reverb from reverberant music is a necessary technique to clean up\naudio for downstream music manipulations. Reverberation of music contains two\ncategories, natural reverb, and artificial reverb. Artificial reverb has a\nwider diversity than natural reverb due to its various parameter setups and\nreverberation types. However, recent supervised dereverberation methods may\nfail because they rely on sufficiently diverse and numerous pairs of\nreverberant observations and retrieved data for training in order to be\ngeneralizable to unseen observations during inference. To resolve these\nproblems, we propose an unsupervised method that can remove a general kind of\nartificial reverb for music without requiring pairs of data for training. The\nproposed method is based on diffusion models, where it initializes the unknown\nreverberation operator with a conventional signal processing technique and\nsimultaneously refines the estimate with the help of diffusion models. We show\nthrough objective and perceptual evaluations that our method outperforms the\ncurrent leading vocal dereverberation benchmarks.", "published": "2022-11-08 09:43:01", "link": "http://arxiv.org/abs/2211.04124v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DiffPhase: Generative Diffusion-based STFT Phase Retrieval", "abstract": "Diffusion probabilistic models have been recently used in a variety of tasks,\nincluding speech enhancement and synthesis. As a generative approach, diffusion\nmodels have been shown to be especially suitable for imputation problems, where\nmissing data is generated based on existing data. Phase retrieval is inherently\nan imputation problem, where phase information has to be generated based on the\ngiven magnitude. In this work we build upon previous work in the speech domain,\nadapting a speech enhancement diffusion model specifically for STFT phase\nretrieval. Evaluation using speech quality and intelligibility metrics shows\nthe diffusion approach is well-suited to the phase retrieval task, with\nperformance surpassing both classical and modern methods.", "published": "2022-11-08 15:50:35", "link": "http://arxiv.org/abs/2211.04332v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Towards Improved Room Impulse Response Estimation for Speech Recognition", "abstract": "We propose a novel approach for blind room impulse response (RIR) estimation\nsystems in the context of a downstream application scenario, far-field\nautomatic speech recognition (ASR). We first draw the connection between\nimproved RIR estimation and improved ASR performance, as a means of evaluating\nneural RIR estimators. We then propose a generative adversarial network (GAN)\nbased architecture that encodes RIR features from reverberant speech and\nconstructs an RIR from the encoded features, and uses a novel energy decay\nrelief loss to optimize for capturing energy-based properties of the input\nreverberant speech. We show that our model outperforms the state-of-the-art\nbaselines on acoustic benchmarks (by 17\\% on the energy decay relief and 22\\%\non an early-reflection energy metric), as well as in an ASR evaluation task (by\n6.9\\% in word error rate).", "published": "2022-11-08 00:40:27", "link": "http://arxiv.org/abs/2211.04473v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PhaseAug: A Differentiable Augmentation for Speech Synthesis to Simulate\n  One-to-Many Mapping", "abstract": "Previous generative adversarial network (GAN)-based neural vocoders are\ntrained to reconstruct the exact ground truth waveform from the paired\nmel-spectrogram and do not consider the one-to-many relationship of speech\nsynthesis. This conventional training causes overfitting for both the\ndiscriminators and the generator, leading to the periodicity artifacts in the\ngenerated audio signal. In this work, we present PhaseAug, the first\ndifferentiable augmentation for speech synthesis that rotates the phase of each\nfrequency bin to simulate one-to-many mapping. With our proposed method, we\noutperform baselines without any architecture modification. Code and audio\nsamples will be available at https://github.com/mindslab-ai/phaseaug.", "published": "2022-11-08 23:37:05", "link": "http://arxiv.org/abs/2211.04610v2", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
