{"title": "Static and Dynamic Vector Semantics for Lambda Calculus Models of\n  Natural Language", "abstract": "Vector models of language are based on the contextual aspects of language,\nthe distributions of words and how they co-occur in text. Truth conditional\nmodels focus on the logical aspects of language, compositional properties of\nwords and how they compose to form sentences. In the truth conditional\napproach, the denotation of a sentence determines its truth conditions, which\ncan be taken to be a truth value, a set of possible worlds, a context change\npotential, or similar. In the vector models, the degree of co-occurrence of\nwords in context determines how similar the meanings of words are. In this\npaper, we put these two models together and develop a vector semantics for\nlanguage based on the simply typed lambda calculus models of natural language.\nWe provide two types of vector semantics: a static one that uses techniques\nfamiliar from the truth conditional tradition and a dynamic one based on a form\nof dynamic interpretation inspired by Heim's context change potentials. We show\nhow the dynamic model can be applied to entailment between a corpus and a\nsentence and we provide examples.", "published": "2018-10-26 14:41:37", "link": "http://arxiv.org/abs/1810.11351v1", "categories": ["cs.CL", "03B65", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Named Person Coreference in English News", "abstract": "People are often entities of interest in tasks such as search and information\nextraction. In these tasks, the goal is to find as much information as possible\nabout people specified by their name. However in text, some of the references\nto people are by pronouns (she, his) or generic descriptions (the professor,\nthe German chancellor). It is therefore important that coreference resolution\nsystems are able to link these different types of mentions to the correct\nperson name. Here, we evaluate two state of the art coreference resolution\nsystems on the subtask of Named Person Coreference, in which we are interested\nin identifying a person mentioned by name, along with all other mentions of the\nperson, by pronoun or generic noun phrase. Our analysis reveals that standard\ncoreference metrics do not reflect adequately the requirements in this task:\nthey do not penalize systems for not identifying any mentions by name and they\nreward systems even if systems find correctly mentions to the same entity but\nfail to link these to a proper name (she--the student---no name). We introduce\nnew metrics for evaluating named person coreference that address these\ndiscrepancies. We present a simple rule-based named entity recognition driven\nsystem, which outperforms the current state-of-the-art systems on these\ntask-specific metrics and performs on par with them on traditional coreference\nevaluations. Finally, we present similar evaluation for coreference resolution\nof other named entities and show that the rule-based approach is effective only\nfor person named coreference, not other named entity types.", "published": "2018-10-26 18:10:03", "link": "http://arxiv.org/abs/1810.11476v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Entropy Explain Successor Surprisal Effects in Reading?", "abstract": "Human reading behavior is sensitive to surprisal: more predictable words tend\nto be read faster. Unexpectedly, this applies not only to the surprisal of the\nword that is currently being read, but also to the surprisal of upcoming\n(successor) words that have not been fixated yet. This finding has been\ninterpreted as evidence that readers can extract lexical information\nparafoveally. Calling this interpretation into question, Angele et al. (2015)\nshowed that successor effects appear even in contexts in which those successor\nwords are not yet visible. They hypothesized that successor surprisal predicts\nreading time because it approximates the reader's uncertainty about upcoming\nwords. We test this hypothesis on a reading time corpus using an LSTM language\nmodel, and find that successor surprisal and entropy are independent predictors\nof reading time. This independence suggests that entropy alone is unlikely to\nbe the full explanation for successor surprisal effects.", "published": "2018-10-26 18:19:29", "link": "http://arxiv.org/abs/1810.11481v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Magnitude: A Fast, Efficient Universal Vector Embedding Utility Package", "abstract": "Vector space embedding models like word2vec, GloVe, fastText, and ELMo are\nextremely popular representations in natural language processing (NLP)\napplications. We present Magnitude, a fast, lightweight tool for utilizing and\nprocessing embeddings. Magnitude is an open source Python package with a\ncompact vector storage file format that allows for efficient manipulation of\nhuge numbers of embeddings. Magnitude performs common operations up to 60 to\n6,000 times faster than Gensim. Magnitude introduces several novel features for\nimproved robustness like out-of-vocabulary lookups.", "published": "2018-10-26 05:04:07", "link": "http://arxiv.org/abs/1810.11190v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Integrating Transformer and Paraphrase Rules for Sentence Simplification", "abstract": "Sentence simplification aims to reduce the complexity of a sentence while\nretaining its original meaning. Current models for sentence simplification\nadopted ideas from ma- chine translation studies and implicitly learned\nsimplification mapping rules from normal- simple sentence pairs. In this paper,\nwe explore a novel model based on a multi-layer and multi-head attention\narchitecture and we pro- pose two innovative approaches to integrate the Simple\nPPDB (A Paraphrase Database for Simplification), an external paraphrase\nknowledge base for simplification that covers a wide range of real-world\nsimplification rules. The experiments show that the integration provides two\nmajor benefits: (1) the integrated model outperforms multiple state- of-the-art\nbaseline models for sentence simplification in the literature (2) through\nanalysis of the rule utilization, the model seeks to select more accurate\nsimplification rules. The code and models used in the paper are available at\nhttps://github.com/ Sanqiang/text_simplification.", "published": "2018-10-26 05:44:01", "link": "http://arxiv.org/abs/1810.11193v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "American Sign Language fingerspelling recognition in the wild", "abstract": "We address the problem of American Sign Language fingerspelling recognition\nin the wild, using videos collected from websites. We introduce the largest\ndata set available so far for the problem of fingerspelling recognition, and\nthe first using naturally occurring video data. Using this data set, we present\nthe first attempt to recognize fingerspelling sequences in this challenging\nsetting. Unlike prior work, our video data is extremely challenging due to low\nframe rates and visual variability. To tackle the visual challenges, we train a\nspecial-purpose signing hand detector using a small subset of our data. Given\nthe hand detector output, a sequence model decodes the hypothesized\nfingerspelled letter sequence. For the sequence model, we explore\nattention-based recurrent encoder-decoders and CTC-based approaches. As the\nfirst attempt at fingerspelling recognition in the wild, this work is intended\nto serve as a baseline for future work on sign language recognition in\nrealistic conditions. We find that, as expected, letter error rates are much\nhigher than in previous work on more controlled data, and we analyze the\nsources of error and effects of model variants.", "published": "2018-10-26 17:43:44", "link": "http://arxiv.org/abs/1810.11438v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Neural Modular Control for Embodied Question Answering", "abstract": "We present a modular approach for learning policies for navigation over long\nplanning horizons from language input. Our hierarchical policy operates at\nmultiple timescales, where the higher-level master policy proposes subgoals to\nbe executed by specialized sub-policies. Our choice of subgoals is\ncompositional and semantic, i.e. they can be sequentially combined in arbitrary\norderings, and assume human-interpretable descriptions (e.g. 'exit room', 'find\nkitchen', 'find refrigerator', etc.).\n  We use imitation learning to warm-start policies at each level of the\nhierarchy, dramatically increasing sample efficiency, followed by reinforcement\nlearning. Independent reinforcement learning at each level of hierarchy enables\nsub-policies to adapt to consequences of their actions and recover from errors.\nSubsequent joint hierarchical training enables the master policy to adapt to\nthe sub-policies.\n  On the challenging EQA (Das et al., 2018) benchmark in House3D (Wu et al.,\n2018), requiring navigating diverse realistic indoor environments, our approach\noutperforms prior work by a significant margin, both in terms of navigation and\nquestion answering.", "published": "2018-10-26 03:58:26", "link": "http://arxiv.org/abs/1810.11181v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Parsing Coordination for Spoken Language Understanding", "abstract": "Typical spoken language understanding systems provide narrow semantic parses\nusing a domain-specific ontology. The parses contain intents and slots that are\ndirectly consumed by downstream domain applications. In this work we discuss\nexpanding such systems to handle compound entities and intents by introducing a\ndomain-agnostic shallow parser that handles linguistic coordination. We show\nthat our model for parsing coordination learns domain-independent and\nslot-independent features and is able to segment conjunct boundaries of many\ndifferent phrasal categories. We also show that using adversarial training can\nbe effective for improving generalization across different slot types for\ncoordination parsing.", "published": "2018-10-26 18:44:52", "link": "http://arxiv.org/abs/1810.11497v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Extractive Summarization of EHR Discharge Notes", "abstract": "Patient summarization is essential for clinicians to provide coordinated care\nand practice effective communication. Automated summarization has the potential\nto save time, standardize notes, aid clinical decision making, and reduce\nmedical errors. Here we provide an upper bound on extractive summarization of\ndischarge notes and develop an LSTM model to sequentially label topics of\nhistory of present illness notes. We achieve an F1 score of 0.876, which\nindicates that this model can be employed to create a dataset for evaluation of\nextractive summarization methods.", "published": "2018-10-26 16:36:27", "link": "http://arxiv.org/abs/1810.12085v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Finding Answers from the Word of God: Domain Adaptation for Neural\n  Networks in Biblical Question Answering", "abstract": "Question answering (QA) has significantly benefitted from deep learning\ntechniques in recent years. However, domain-specific QA remains a challenge due\nto the significant amount of data required to train a neural network. This\npaper studies the answer sentence selection task in the Bible domain and answer\nquestions by selecting relevant verses from the Bible. For this purpose, we\ncreate a new dataset BibleQA based on bible trivia questions and propose three\nneural network models for our task. We pre-train our models on a large-scale QA\ndataset, SQuAD, and investigate the effect of transferring weights on model\naccuracy. Furthermore, we also measure the model accuracies with different\nanswer context lengths and different Bible translations. We affirm that\ntransfer learning has a noticeable improvement in the model accuracy. We\nachieve relatively good results with shorter context lengths, whereas longer\ncontext lengths decreased model accuracy. We also find that using a more modern\nBible translation in the dataset has a positive effect on the task.", "published": "2018-10-26 12:34:21", "link": "http://arxiv.org/abs/1810.12118v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "68T50", "I.2.7; I.2.6; J.5"], "primary_category": "cs.IR"}
{"title": "Concatenated Identical DNN (CI-DNN) to Reduce Noise-Type Dependence in\n  DNN-Based Speech Enhancement", "abstract": "Estimating time-frequency domain masks for speech enhancement using deep\nlearning approaches has recently become a popular field of research. In this\npaper, we propose a mask-based speech enhancement framework by using\nconcatenated identical deep neural networks (CI-DNNs). The idea is that a\nsingle DNN is trained under multiple input and output signal-to-noise power\nratio (SNR) conditions, using targets that provide a moderate SNR gain with\nrespect to the input and therefore achieve a balance between speech component\nquality and noise suppression. We concatenate this single DNN several times\nwithout any retraining to provide enough noise attenuation. Simulation results\nshow that our proposed CI-DNN outperforms enhancement methods using classical\nspectral weighting rules w.r.t. total speech quality and speech\nintelligibility. Moreover, our approach shows similar or even a little bit\nbetter performance with much fewer trainable parameters compared with a\nnoisy-target single DNN approach of the same size. A comparison to the\nconventional clean-target single DNN approach shows that our proposed CI-DNN is\nbetter in speech component quality and much better in residual noise component\nquality. Most importantly, our new CI-DNN generalized best to an unseen noise\ntype, if compared to the other tested deep learning approaches.", "published": "2018-10-26 07:52:26", "link": "http://arxiv.org/abs/1810.11217v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A novel pyramidal-FSMN architecture with lattice-free MMI for speech\n  recognition", "abstract": "Deep Feedforward Sequential Memory Network (DFSMN) has shown superior\nperformance on speech recognition tasks. Based on this work, we propose a novel\nnetwork architecture which introduces pyramidal memory structure to represent\nvarious context information in different layers. Additionally, res-CNN layers\nare added in the front to extract more sophisticated features as well. Together\nwith lattice-free maximum mutual information (LF-MMI) and cross entropy (CE)\njoint training criteria, experimental results show that this approach achieves\nword error rates (WERs) of 3.62% and 10.89% respectively on Librispeech and\nLDC97S62 (Switchboard 300 hours) corpora. Furthermore, Recurrent neural network\nlanguage model (RNNLM) rescoring is applied and a WER of 2.97% is obtained on\nLibrispeech.", "published": "2018-10-26 14:44:00", "link": "http://arxiv.org/abs/1810.11352v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "gpuRIR: A Python Library for Room Impulse Response Simulation with GPU\n  Acceleration", "abstract": "The Image Source Method (ISM) is one of the most employed techniques to\ncalculate acoustic Room Impulse Responses (RIRs), however, its computational\ncomplexity grows fast with the reverberation time of the room and its\ncomputation time can be prohibitive for some applications where a huge number\nof RIRs are needed. In this paper, we present a new implementation that\ndramatically improves the computation speed of the ISM by using Graphic\nProcessing Units (GPUs) to parallelize both the simulation of multiple RIRs and\nthe computation of the images inside each RIR. Additional speedups were\nachieved by exploiting the mixed precision capabilities of the newer GPUs and\nby using lookup tables. We provide a Python library under GNU license that can\nbe easily used without any knowledge about GPU programming and we show that it\nis about 100 times faster than other state of the art CPU libraries. It may\nbecome a powerful tool for many applications that need to perform a large\nnumber of acoustic simulations, such as training machine learning systems for\naudio signal processing, or for real-time room acoustics simulations for\nimmersive multimedia systems, such as augmented or virtual reality.", "published": "2018-10-26 15:05:04", "link": "http://arxiv.org/abs/1810.11359v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Scaling Speech Enhancement in Unseen Environments with Noise Embeddings", "abstract": "We address the problem of speech enhancement generalisation to unseen\nenvironments by performing two manipulations. First, we embed an additional\nrecording from the environment alone, and use this embedding to alter\nactivations in the main enhancement subnetwork. Second, we scale the number of\nnoise environments present at training time to 16,784 different environments.\nExperiment results show that both manipulations reduce word error rates of a\npretrained speech recognition system and improve enhancement quality according\nto a number of performance measures. Specifically, our best model reduces the\nword error rate from 34.04% on noisy speech to 15.46% on the enhanced speech.\nEnhanced audio samples can be found in\nhttps://speechenhancement.page.link/samples.", "published": "2018-10-26 13:05:54", "link": "http://arxiv.org/abs/1810.12757v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Spectrogram-channels u-net: a source separation model viewing each\n  channel as the spectrogram of each source", "abstract": "Sound source separation has attracted attention from Music Information\nRetrieval(MIR) researchers, since it is related to many MIR tasks such as\nautomatic lyric transcription, singer identification, and voice conversion. In\nthis paper, we propose an intuitive spectrogram-based model for source\nseparation by adapting U-Net. We call it Spectrogram-Channels U-Net, which\nmeans each channel of the output corresponds to the spectrogram of separated\nsource itself. The proposed model can be used for not only singing voice\nseparation but also multi-instrument separation by changing only the number of\noutput channels. In addition, we propose a loss function that balances volumes\nbetween different sources. Finally, we yield performance that is\nstate-of-the-art on both separation tasks.", "published": "2018-10-26 20:23:17", "link": "http://arxiv.org/abs/1810.11520v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP", "stat.ML"], "primary_category": "cs.SD"}
