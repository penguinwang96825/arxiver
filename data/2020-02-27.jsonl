{"title": "CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue\n  Dataset", "abstract": "To advance multi-domain (cross-domain) dialogue modeling as well as alleviate\nthe shortage of Chinese task-oriented datasets, we propose CrossWOZ, the first\nlarge-scale Chinese Cross-Domain Wizard-of-Oz task-oriented dataset. It\ncontains 6K dialogue sessions and 102K utterances for 5 domains, including\nhotel, restaurant, attraction, metro, and taxi. Moreover, the corpus contains\nrich annotation of dialogue states and dialogue acts at both user and system\nsides. About 60% of the dialogues have cross-domain user goals that favor\ninter-domain dependency and encourage natural transition across domains in\nconversation. We also provide a user simulator and several benchmark models for\npipelined task-oriented dialogue systems, which will facilitate researchers to\ncompare and evaluate their models on this corpus. The large size and rich\nannotation of CrossWOZ make it suitable to investigate a variety of tasks in\ncross-domain dialogue modeling, such as dialogue state tracking, policy\nlearning, user simulation, etc.", "published": "2020-02-27 03:06:35", "link": "http://arxiv.org/abs/2002.11893v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating Boundary Assembling into a DNN Framework for Named Entity\n  Recognition in Chinese Social Media Text", "abstract": "Named entity recognition is a challenging task in Natural Language\nProcessing, especially for informal and noisy social media text. Chinese word\nboundaries are also entity boundaries, therefore, named entity recognition for\nChinese text can benefit from word boundary detection, outputted by Chinese\nword segmentation. Yet Chinese word segmentation poses its own difficulty\nbecause it is influenced by several factors, e.g., segmentation criteria,\nemployed algorithm, etc. Dealt improperly, it may generate a cascading failure\nto the quality of named entity recognition followed. In this paper we integrate\na boundary assembling method with the state-of-the-art deep neural network\nmodel, and incorporate the updated word boundary information into a conditional\nrandom field model for named entity recognition. Our method shows a 2% absolute\nimprovement over previous state-of-the-art results.", "published": "2020-02-27 04:29:13", "link": "http://arxiv.org/abs/2002.11910v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving cross-lingual model transfer by chunking", "abstract": "We present a shallow parser guided cross-lingual model transfer approach in\norder to address the syntactic differences between source and target languages\nmore effectively. In this work, we assume the chunks or phrases in a sentence\nas transfer units in order to address the syntactic differences between the\nsource and target languages arising due to the differences in ordering of words\nin the phrases and the ordering of phrases in a sentence separately.", "published": "2020-02-27 14:02:31", "link": "http://arxiv.org/abs/2002.12097v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Primer in BERTology: What we know about how BERT works", "abstract": "Transformer-based models have pushed state of the art in many areas of NLP,\nbut our understanding of what is behind their success is still limited. This\npaper is the first survey of over 150 studies of the popular BERT model. We\nreview the current state of knowledge about how BERT works, what kind of\ninformation it learns and how it is represented, common modifications to its\ntraining objectives and architecture, the overparameterization issue and\napproaches to compression. We then outline directions for future research.", "published": "2020-02-27 18:46:42", "link": "http://arxiv.org/abs/2002.12327v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-shot Natural Language Generation for Task-Oriented Dialog", "abstract": "As a crucial component in task-oriented dialog systems, the Natural Language\nGeneration (NLG) module converts a dialog act represented in a semantic form\ninto a response in natural language. The success of traditional template-based\nor statistical models typically relies on heavily annotated data, which is\ninfeasible for new domains. Therefore, it is pivotal for an NLG system to\ngeneralize well with limited labelled data in real applications. To this end,\nwe present FewShotWoz, the first NLG benchmark to simulate the few-shot\nlearning setting in task-oriented dialog systems. Further, we develop the\nSC-GPT model. It is pre-trained on a large set of annotated NLG corpus to\nacquire the controllable generation ability, and fine-tuned with only a few\ndomain-specific labels to adapt to new domains. Experiments on FewShotWoz and\nthe large Multi-Domain-WOZ datasets show that the proposed SC-GPT significantly\noutperforms existing methods, measured by various automatic metrics and human\nevaluations.", "published": "2020-02-27 18:48:33", "link": "http://arxiv.org/abs/2002.12328v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Followup Questions for Interpretable Multi-hop Question\n  Answering", "abstract": "We propose a framework for answering open domain multi-hop questions in which\npartial information is read and used to generate followup questions, to finally\nbe answered by a pretrained single-hop answer extractor. This framework makes\neach hop interpretable, and makes the retrieval associated with later hops as\nflexible and specific as for the first hop. As a first instantiation of this\nframework, we train a pointer-generator network to predict followup questions\nbased on the question and partial information. This provides a novel\napplication of a neural question generation network, which is applied to give\nweak ground truth single-hop followup questions based on the final answers and\ntheir supporting facts. Learning to generate followup questions that select the\nrelevant answer spans against downstream supporting facts, while avoiding\ndistracting premises, poses an exciting semantic challenge for text generation.\nWe present an evaluation using the two-hop bridge questions of HotpotQA.", "published": "2020-02-27 18:58:15", "link": "http://arxiv.org/abs/2002.12344v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adv-BERT: BERT is not robust on misspellings! Generating nature\n  adversarial samples on BERT", "abstract": "There is an increasing amount of literature that claims the brittleness of\ndeep neural networks in dealing with adversarial examples that are created\nmaliciously. It is unclear, however, how the models will perform in realistic\nscenarios where \\textit{natural rather than malicious} adversarial instances\noften exist. This work systematically explores the robustness of BERT, the\nstate-of-the-art Transformer-style model in NLP, in dealing with noisy data,\nparticularly mistakes in typing the keyboard, that occur inadvertently.\nIntensive experiments on sentiment analysis and question answering benchmarks\nindicate that: (i) Typos in various words of a sentence do not influence\nequally. The typos in informative words make severer damages; (ii) Mistype is\nthe most damaging factor, compared with inserting, deleting, etc.; (iii) Humans\nand machines have different focuses on recognizing adversarial attacks.", "published": "2020-02-27 22:07:11", "link": "http://arxiv.org/abs/2003.04985v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis of diversity-accuracy tradeoff in image captioning", "abstract": "We investigate the effect of different model architectures, training\nobjectives, hyperparameter settings and decoding procedures on the diversity of\nautomatically generated image captions. Our results show that 1) simple\ndecoding by naive sampling, coupled with low temperature is a competitive and\nfast method to produce diverse and accurate caption sets; 2) training with\nCIDEr-based reward using Reinforcement learning harms the diversity properties\nof the resulting generator, which cannot be mitigated by manipulating decoding\nparameters. In addition, we propose a new metric AllSPICE for evaluating both\naccuracy and diversity of a set of captions by a single value.", "published": "2020-02-27 00:09:25", "link": "http://arxiv.org/abs/2002.11848v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Squashed Shifted PMI Matrix: Bridging Word Embeddings and Hyperbolic\n  Spaces", "abstract": "We show that removing sigmoid transformation in the skip-gram with negative\nsampling (SGNS) objective does not harm the quality of word vectors\nsignificantly and at the same time is related to factorizing a squashed shifted\nPMI matrix which, in turn, can be treated as a connection probabilities matrix\nof a random graph. Empirically, such graph is a complex network, i.e. it has\nstrong clustering and scale-free degree distribution, and is tightly connected\nwith hyperbolic spaces. In short, we show the connection between static word\nembeddings and hyperbolic spaces through the squashed shifted PMI matrix using\nanalytical and empirical methods.", "published": "2020-02-27 09:50:41", "link": "http://arxiv.org/abs/2002.12005v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Annotation of Emotion Carriers in Personal Narratives", "abstract": "We are interested in the problem of understanding personal narratives (PN) -\nspoken or written - recollections of facts, events, and thoughts. In PN,\nemotion carriers are the speech or text segments that best explain the\nemotional state of the user. Such segments may include entities, verb or noun\nphrases. Advanced automatic understanding of PNs requires not only the\nprediction of the user emotional state but also to identify which events (e.g.\n\"the loss of relative\" or \"the visit of grandpa\") or people ( e.g. \"the old\ngroup of high school mates\") carry the emotion manifested during the personal\nrecollection. This work proposes and evaluates an annotation model for\nidentifying emotion carriers in spoken personal narratives. Compared to other\ntext genres such as news and microblogs, spoken PNs are particularly\nchallenging because a narrative is usually unstructured, involving multiple\nsub-events and characters as well as thoughts and associated emotions perceived\nby the narrator. In this work, we experiment with annotating emotion carriers\nfrom speech transcriptions in the Ulm State-of-Mind in Speech (USoMS) corpus, a\ndataset of German PNs. We believe this resource could be used for experiments\nin the automatic extraction of emotion carriers from PN, a task that could\nprovide further advancements in narrative understanding.", "published": "2020-02-27 15:42:39", "link": "http://arxiv.org/abs/2002.12196v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comment Ranking Diversification in Forum Discussions", "abstract": "Viewing consumption of discussion forums with hundreds or more comments\ndepends on ranking because most users only view top-ranked comments. When\ncomments are ranked by an ordered score (e.g. number of replies or up-votes)\nwithout adjusting for semantic similarity of near-ranked comments, top-ranked\ncomments are more likely to emphasize the majority opinion and incur\nredundancy. In this paper, we propose a top K comment diversification\nre-ranking model using Maximal Marginal Relevance (MMR) and evaluate its impact\nin three categories: (1) semantic diversity, (2) inclusion of the semantics of\nlower-ranked comments, and (3) redundancy, within the context of a HarvardX\ncourse discussion forum. We conducted a double-blind, small-scale evaluation\nexperiment requiring subjects to select between the top 5 comments of a\ndiversified ranking and a baseline ranking ordered by score. For three\nsubjects, across 100 trials, subjects selected the diversified (75% score, 25%\ndiversification) ranking as significantly (1) more diverse, (2) more inclusive,\nand (3) less redundant. Within each category, inter-rater reliability showed\nmoderate consistency, with typical Cohen-Kappa scores near 0.2. Our findings\nsuggest that our model improves (1) diversification, (2) inclusion, and (3)\nredundancy, among top K ranked comments in online discussion forums.", "published": "2020-02-27 21:44:41", "link": "http://arxiv.org/abs/2002.12457v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Optimizing Memory-Access Patterns for Deep Learning Accelerators", "abstract": "Deep learning (DL) workloads are moving towards accelerators for faster\nprocessing and lower cost. Modern DL accelerators are good at handling the\nlarge-scale multiply-accumulate operations that dominate DL workloads; however,\nit is challenging to make full use of the compute power of an accelerator since\nthe data must be properly staged in a software-managed scratchpad memory.\nFailing to do so can result in significant performance loss. This paper\nproposes a systematic approach which leverages the polyhedral model to analyze\nall operators of a DL model together to minimize the number of memory accesses.\nExperiments show that our approach can substantially reduce the impact of\nmemory accesses required by common neural-network models on a homegrown AWS\nmachine-learning inference chip named Inferentia, which is available through\nAmazon EC2 Inf1 instances.", "published": "2020-02-27 05:06:19", "link": "http://arxiv.org/abs/2002.12798v1", "categories": ["cs.PF", "cs.CL"], "primary_category": "cs.PF"}
{"title": "Echo State Neural Machine Translation", "abstract": "We present neural machine translation (NMT) models inspired by echo state\nnetwork (ESN), named Echo State NMT (ESNMT), in which the encoder and decoder\nlayer weights are randomly generated then fixed throughout training. We show\nthat even with this extremely simple model construction and training procedure,\nESNMT can already reach 70-80% quality of fully trainable baselines. We examine\nhow spectral radius of the reservoir, a key quantity that characterizes the\nmodel, determines the model behavior. Our findings indicate that randomized\nnetworks can work well even for complicated sequence-to-sequence prediction NLP\ntasks.", "published": "2020-02-27 00:08:45", "link": "http://arxiv.org/abs/2002.11847v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "SkinAugment: Auto-Encoding Speaker Conversions for Automatic Speech\n  Translation", "abstract": "We propose autoencoding speaker conversion for training data augmentation in\nautomatic speech translation. This technique directly transforms an audio\nsequence, resulting in audio synthesized to resemble another speaker's voice.\nOur method compares favorably to SpecAugment on English$\\to$French and\nEnglish$\\to$Romanian automatic speech translation (AST) tasks as well as on a\nlow-resource English automatic speech recognition (ASR) task. Further, in\nablations, we show the benefits of both quantity and diversity in augmented\ndata. Finally, we show that we can combine our approach with augmentation by\nmachine-translated transcripts to obtain a competitive end-to-end AST model\nthat outperforms a very strong cascade model on an English$\\to$French AST task.\nOur method is sufficiently general that it can be applied to other speech\ngeneration and analysis tasks.", "published": "2020-02-27 16:22:42", "link": "http://arxiv.org/abs/2002.12231v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Masking Orchestration: Multi-task Pretraining for Multi-role Dialogue\n  Representation Learning", "abstract": "Multi-role dialogue understanding comprises a wide range of diverse tasks\nsuch as question answering, act classification, dialogue summarization etc.\nWhile dialogue corpora are abundantly available, labeled data, for specific\nlearning tasks, can be highly scarce and expensive. In this work, we\ninvestigate dialogue context representation learning with various types\nunsupervised pretraining tasks where the training objectives are given\nnaturally according to the nature of the utterance and the structure of the\nmulti-role conversation. Meanwhile, in order to locate essential information\nfor dialogue summarization/extraction, the pretraining process enables external\nknowledge integration. The proposed fine-tuned pretraining mechanism is\ncomprehensively evaluated via three different dialogue datasets along with a\nnumber of downstream dialogue-mining tasks. Result shows that the proposed\npretraining mechanism significantly contributes to all the downstream tasks\nwithout discrimination to different encoders.", "published": "2020-02-27 04:36:52", "link": "http://arxiv.org/abs/2003.04994v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Identification of Dementia Using Audio Biomarkers", "abstract": "Dementia is a syndrome, generally of a chronic nature characterized by a\ndeterioration in cognitive function, especially in the geriatric population and\nis severe enough to impact their daily activities. Early diagnosis of dementia\nis essential to provide timely treatment to alleviate the effects and sometimes\nto slow the progression of dementia. Speech has been known to provide an\nindication of a person's cognitive state. The objective of this work is to use\nspeech processing and machine learning techniques to automatically identify the\nstage of dementia such as mild cognitive impairment (MCI) or Alzheimers disease\n(AD). Non-linguistic acoustic parameters are used for this purpose, making this\na language independent approach. We analyze the patients audio excerpts from a\nclinician-participant conversations taken from the Pitt corpus of DementiaBank\ndatabase, to identify the speech parameters that best distinguish between MCI,\nAD and healthy (HC) speech. We analyze the contribution of various types of\nacoustic features such as spectral, temporal, cepstral their feature-level\nfusion and selection towards the identification of dementia stage.\nAdditionally, we compare the performance of using feature-level fusion and\nscore-level fusion. An accuracy of 82% is achieved using score-level fusion\nwith an absolute improvement of 5% over feature-level fusion.", "published": "2020-02-27 13:54:00", "link": "http://arxiv.org/abs/2002.12788v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Deep Residual-Dense Lattice Network for Speech Enhancement", "abstract": "Convolutional neural networks (CNNs) with residual links (ResNets) and causal\ndilated convolutional units have been the network of choice for deep learning\napproaches to speech enhancement. While residual links improve gradient flow\nduring training, feature diminution of shallow layer outputs can occur due to\nrepetitive summations with deeper layer outputs. One strategy to improve\nfeature re-usage is to fuse both ResNets and densely connected CNNs\n(DenseNets). DenseNets, however, over-allocate parameters for feature re-usage.\nMotivated by this, we propose the residual-dense lattice network (RDL-Net),\nwhich is a new CNN for speech enhancement that employs both residual and dense\naggregations without over-allocating parameters for feature re-usage. This is\nmanaged through the topology of the RDL blocks, which limit the number of\noutputs used for dense aggregations. Our extensive experimental investigation\nshows that RDL-Nets are able to achieve a higher speech enhancement performance\nthan CNNs that employ residual and/or dense aggregations. RDL-Nets also use\nsubstantially fewer parameters and have a lower computational requirement.\nFurthermore, we demonstrate that RDL-Nets outperform many state-of-the-art deep\nlearning approaches to speech enhancement.", "published": "2020-02-27 04:36:30", "link": "http://arxiv.org/abs/2002.12794v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
