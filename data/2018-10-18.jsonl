{"title": "Semantic Parsing for Task Oriented Dialog using Hierarchical\n  Representations", "abstract": "Task oriented dialog systems typically first parse user utterances to\nsemantic frames comprised of intents and slots. Previous work on task oriented\nintent and slot-filling work has been restricted to one intent per query and\none slot label per token, and thus cannot model complex compositional requests.\nAlternative semantic parsing systems have represented queries as logical forms,\nbut these are challenging to annotate and parse. We propose a hierarchical\nannotation scheme for semantic parsing that allows the representation of\ncompositional queries, and can be efficiently and accurately parsed by standard\nconstituency parsing models. We release a dataset of 44k annotated queries\n(fb.me/semanticparsingdialog), and show that parsing models outperform\nsequence-to-sequence approaches on this dataset.", "published": "2018-10-18 08:22:49", "link": "http://arxiv.org/abs/1810.07942v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Temporally Sensitive Submodularity Framework for Timeline\n  Summarization", "abstract": "Timeline summarization (TLS) creates an overview of long-running events via\ndated daily summaries for the most important dates. TLS differs from standard\nmulti-document summarization (MDS) in the importance of date selection,\ninterdependencies between summaries of different dates and by having very short\nsummaries compared to the number of corpus documents. However, we show that MDS\noptimization models using submodular functions can be adapted to yield\nwell-performing TLS models by designing objective functions and constraints\nthat model the temporal dimension inherent in TLS. Importantly, these\nadaptations retain the elegance and advantages of the original MDS models\n(clear separation of features and inference, performance guarantees and\nscalability, little need for supervision) that current TLS-specific models\nlack. An open-source implementation of the framework and all models described\nin this paper is available online.", "published": "2018-10-18 08:46:40", "link": "http://arxiv.org/abs/1810.07949v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial TableQA: Attention Supervision for Question Answering on\n  Tables", "abstract": "The task of answering a question given a text passage has shown great\ndevelopments on model performance thanks to community efforts in building\nuseful datasets. Recently, there have been doubts whether such rapid progress\nhas been based on truly understanding language. The same question has not been\nasked in the table question answering (TableQA) task, where we are tasked to\nanswer a query given a table. We show that existing efforts, of using \"answers\"\nfor both evaluation and supervision for TableQA, show deteriorating\nperformances in adversarial settings of perturbations that do not affect the\nanswer. This insight naturally motivates to develop new models that understand\nquestion and table more precisely. For this goal, we propose Neural Operator\n(NeOp), a multi-layer sequential network with attention supervision to answer\nthe query given a table. NeOp uses multiple Selective Recurrent Units (SelRUs)\nto further help the interpretability of the answers of the model. Experiments\nshow that the use of operand information to train the model significantly\nimproves the performance and interpretability of TableQA models. NeOp\noutperforms all the previous models by a big margin.", "published": "2018-10-18 15:35:07", "link": "http://arxiv.org/abs/1810.08113v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Topic Modeling For Dialog Systems", "abstract": "Accurate prediction of conversation topics can be a valuable signal for\ncreating coherent and engaging dialog systems. In this work, we focus on\ncontext-aware topic classification methods for identifying topics in free-form\nhuman-chatbot dialogs. We extend previous work on neural topic classification\nand unsupervised topic keyword detection by incorporating conversational\ncontext and dialog act features. On annotated data, we show that incorporating\ncontext and dialog acts leads to relative gains in topic classification\naccuracy by 35% and on unsupervised keyword detection recall by 11% for\nconversational interactions where topics frequently span multiple utterances.\nWe show that topical metrics such as topical depth is highly correlated with\ndialog evaluation metrics such as coherence and engagement implying that\nconversational topic models can predict user satisfaction. Our work for\ndetecting conversation topics and keywords can be used to guide chatbots\ntowards coherent dialog.", "published": "2018-10-18 16:19:16", "link": "http://arxiv.org/abs/1810.08135v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large-scale Hierarchical Alignment for Data-driven Text Rewriting", "abstract": "We propose a simple unsupervised method for extracting pseudo-parallel\nmonolingual sentence pairs from comparable corpora representative of two\ndifferent text styles, such as news articles and scientific papers. Our\napproach does not require a seed parallel corpus, but instead relies solely on\nhierarchical search over pre-trained embeddings of documents and sentences. We\ndemonstrate the effectiveness of our method through automatic and extrinsic\nevaluation on text simplification from the normal to the Simple Wikipedia. We\nshow that pseudo-parallel sentences extracted with our method not only\nsupplement existing parallel data, but can even lead to competitive performance\non their own.", "published": "2018-10-18 18:51:43", "link": "http://arxiv.org/abs/1810.08237v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reduction of Parameter Redundancy in Biaffine Classifiers with Symmetric\n  and Circulant Weight Matrices", "abstract": "Currently, the biaffine classifier has been attracting attention as a method\nto introduce an attention mechanism into the modeling of binary relations. For\ninstance, in the field of dependency parsing, the Deep Biaffine Parser by Dozat\nand Manning has achieved state-of-the-art performance as a graph-based\ndependency parser on the English Penn Treebank and CoNLL 2017 shared task. On\nthe other hand, it is reported that parameter redundancy in the weight matrix\nin biaffine classifiers, which has O(n^2) parameters, results in overfitting (n\nis the number of dimensions). In this paper, we attempted to reduce the\nparameter redundancy by assuming either symmetry or circularity of weight\nmatrices. In our experiments on the CoNLL 2017 shared task dataset, our model\nachieved better or comparable accuracy on most of the treebanks with more than\n16% parameter reduction.", "published": "2018-10-18 23:40:47", "link": "http://arxiv.org/abs/1810.08307v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Neural Text Simplification", "abstract": "The paper presents a first attempt towards unsupervised neural text\nsimplification that relies only on unlabeled text corpora. The core framework\nis composed of a shared encoder and a pair of attentional-decoders and gains\nknowledge of simplification through discrimination based-losses and denoising.\nThe framework is trained using unlabeled text collected from en-Wikipedia dump.\nOur analysis (both quantitative and qualitative involving human evaluators) on\na public test data shows that the proposed model can perform\ntext-simplification at both lexical and syntactic levels, competitive to\nexisting supervised methods. Addition of a few labelled pairs also improves the\nperformance further.", "published": "2018-10-18 07:43:12", "link": "http://arxiv.org/abs/1810.07931v6", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Discourse Embellishment Using a Deep Encoder-Decoder Network", "abstract": "We suggest a new NLG task in the context of the discourse generation pipeline\nof computational storytelling systems. This task, textual embellishment, is\ndefined by taking a text as input and generating a semantically equivalent\noutput with increased lexical and syntactic complexity. Ideally, this would\nallow the authors of computational storytellers to implement just lightweight\nNLG systems and use a domain-independent embellishment module to translate its\noutput into more literary text. We present promising first results on this task\nusing LSTM Encoder-Decoder networks trained on the WikiLarge dataset.\nFurthermore, we introduce \"Compiled Computer Tales\", a corpus of\ncomputationally generated stories, that can be used to test the capabilities of\nembellishment algorithms.", "published": "2018-10-18 14:29:50", "link": "http://arxiv.org/abs/1810.08076v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BabyAI: A Platform to Study the Sample Efficiency of Grounded Language\n  Learning", "abstract": "Allowing humans to interactively train artificial agents to understand\nlanguage instructions is desirable for both practical and scientific reasons,\nbut given the poor data efficiency of the current learning methods, this goal\nmay require substantial research efforts. Here, we introduce the BabyAI\nresearch platform to support investigations towards including humans in the\nloop for grounded language learning. The BabyAI platform comprises an\nextensible suite of 19 levels of increasing difficulty. The levels gradually\nlead the agent towards acquiring a combinatorially rich synthetic language\nwhich is a proper subset of English. The platform also provides a heuristic\nexpert agent for the purpose of simulating a human teacher. We report baseline\nresults and estimate the amount of human involvement that would be required to\ntrain a neural network-based agent on some of the BabyAI levels. We put forward\nstrong evidence that current deep learning methods are not yet sufficiently\nsample efficient when it comes to learning a language with compositional\nproperties.", "published": "2018-10-18 20:48:08", "link": "http://arxiv.org/abs/1810.08272v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "WikiHow: A Large Scale Text Summarization Dataset", "abstract": "Sequence-to-sequence models have recently gained the state of the art\nperformance in summarization. However, not too many large-scale high-quality\ndatasets are available and almost all the available ones are mainly news\narticles with specific writing style. Moreover, abstractive human-style systems\ninvolving description of the content at a deeper level require data with higher\nlevels of abstraction. In this paper, we present WikiHow, a dataset of more\nthan 230,000 article and summary pairs extracted and constructed from an online\nknowledge base written by different human authors. The articles span a wide\nrange of topics and therefore represent high diversity styles. We evaluate the\nperformance of the existing methods on WikiHow to present its challenges and\nset some baselines to further improve it.", "published": "2018-10-18 05:29:41", "link": "http://arxiv.org/abs/1810.09305v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analyzing and Interpreting Convolutional Neural Networks in NLP", "abstract": "Convolutional neural networks have been successfully applied to various NLP\ntasks. However, it is not obvious whether they model different linguistic\npatterns such as negation, intensification, and clause compositionality to help\nthe decision-making process. In this paper, we apply visualization techniques\nto observe how the model can capture different linguistic features and how\nthese features can affect the performance of the model. Later on, we try to\nidentify the model errors and their sources. We believe that interpreting CNNs\nis the first step to understand the underlying semantic features which can\nraise awareness to further improve the performance and explainability of CNN\nmodels.", "published": "2018-10-18 05:18:04", "link": "http://arxiv.org/abs/1810.09312v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "EdgeSpeechNets: Highly Efficient Deep Neural Networks for Speech\n  Recognition on the Edge", "abstract": "Despite showing state-of-the-art performance, deep learning for speech\nrecognition remains challenging to deploy in on-device edge scenarios such as\nmobile and other consumer devices. Recently, there have been greater efforts in\nthe design of small, low-footprint deep neural networks (DNNs) that are more\nappropriate for edge devices, with much of the focus on design principles for\nhand-crafting efficient network architectures. In this study, we explore a\nhuman-machine collaborative design strategy for building low-footprint DNN\narchitectures for speech recognition through a marriage of human-driven\nprincipled network design prototyping and machine-driven design exploration.\nThe efficacy of this design strategy is demonstrated through the design of a\nfamily of highly-efficient DNNs (nicknamed EdgeSpeechNets) for\nlimited-vocabulary speech recognition. Experimental results using the Google\nSpeech Commands dataset for limited-vocabulary speech recognition showed that\nEdgeSpeechNets have higher accuracies than state-of-the-art DNNs (with the best\nEdgeSpeechNet achieving ~97% accuracy), while achieving significantly smaller\nnetwork sizes (as much as 7.8x smaller) and lower computational cost (as much\nas 36x fewer multiply-add operations, 10x lower prediction latency, and 16x\nsmaller memory footprint on a Motorola Moto E phone), making them very\nwell-suited for on-device edge voice interface applications.", "published": "2018-10-18 00:47:20", "link": "http://arxiv.org/abs/1810.08559v2", "categories": ["eess.AS", "cs.LG", "cs.NE", "cs.SD", "eess.SP", "stat.ML"], "primary_category": "eess.AS"}
