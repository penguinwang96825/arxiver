{"title": "Reproducibility, Replicability and Beyond: Assessing Production\n  Readiness of Aspect Based Sentiment Analysis in the Wild", "abstract": "With the exponential growth of online marketplaces and user-generated content\ntherein, aspect-based sentiment analysis has become more important than ever.\nIn this work, we critically review a representative sample of the models\npublished during the past six years through the lens of a practitioner, with an\neye towards deployment in production. First, our rigorous empirical evaluation\nreveals poor reproducibility: an average 4-5% drop in test accuracy across the\nsample. Second, to further bolster our confidence in empirical evaluation, we\nreport experiments on two challenging data slices, and observe a consistent\n12-55% drop in accuracy. Third, we study the possibility of transfer across\ndomains and observe that as little as 10-25% of the domain-specific training\ndataset, when used in conjunction with datasets from other domains within the\nsame locale, largely closes the gap between complete cross-domain and complete\nin-domain predictive performance. Lastly, we open-source two large-scale\nannotated review corpora from a large e-commerce portal in India in order to\naid the study of replicability and transfer, with the hope that it will fuel\nfurther growth of the field.", "published": "2021-01-23 07:45:27", "link": "http://arxiv.org/abs/2101.09449v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "WebSRC: A Dataset for Web-Based Structural Reading Comprehension", "abstract": "Web search is an essential way for humans to obtain information, but it's\nstill a great challenge for machines to understand the contents of web pages.\nIn this paper, we introduce the task of structural reading comprehension (SRC)\non web. Given a web page and a question about it, the task is to find the\nanswer from the web page. This task requires a system not only to understand\nthe semantics of texts but also the structure of the web page. Moreover, we\nproposed WebSRC, a novel Web-based Structural Reading Comprehension dataset.\nWebSRC consists of 400K question-answer pairs, which are collected from 6.4K\nweb pages. Along with the QA pairs, corresponding HTML source code,\nscreenshots, and metadata are also provided in our dataset. Each question in\nWebSRC requires a certain structural understanding of a web page to answer, and\nthe answer is either a text span on the web page or yes/no. We evaluate various\nbaselines on our dataset to show the difficulty of our task. We also\ninvestigate the usefulness of structural information and visual features. Our\ndataset and baselines have been publicly available at\nhttps://x-lance.github.io/WebSRC/.", "published": "2021-01-23 09:43:44", "link": "http://arxiv.org/abs/2101.09465v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Multilingual Pre-trained Language Model with Byte-level\n  Subwords", "abstract": "The pre-trained language models have achieved great successes in various\nnatural language understanding (NLU) tasks due to its capacity to capture the\ndeep contextualized information in text by pre-training on large-scale corpora.\nOne of the fundamental components in pre-trained language models is the\nvocabulary, especially for training multilingual models on many different\nlanguages. In the technical report, we present our practices on training\nmultilingual pre-trained language models with BBPE: Byte-Level BPE (i.e., Byte\nPair Encoding). In the experiment, we adopted the architecture of NEZHA as the\nunderlying pre-trained language model and the results show that NEZHA trained\nwith byte-level subwords consistently outperforms Google multilingual BERT and\nvanilla NEZHA by a notable margin in several multilingual NLU tasks. We release\nthe source code of our byte-level vocabulary building tools and the\nmultilingual pre-trained language models.", "published": "2021-01-23 10:01:28", "link": "http://arxiv.org/abs/2101.09469v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Debiasing Pre-trained Contextualised Embeddings", "abstract": "In comparison to the numerous debiasing methods proposed for the static\nnon-contextualised word embeddings, the discriminative biases in contextualised\nembeddings have received relatively little attention. We propose a fine-tuning\nmethod that can be applied at token- or sentence-levels to debias pre-trained\ncontextualised embeddings. Our proposed method can be applied to any\npre-trained contextualised embedding model, without requiring to retrain those\nmodels. Using gender bias as an illustrative example, we then conduct a\nsystematic study using several state-of-the-art (SoTA) contextualised\nrepresentations on multiple benchmark datasets to evaluate the level of biases\nencoded in different contextualised embeddings before and after debiasing using\nthe proposed method. We find that applying token-level debiasing for all tokens\nand across all layers of a contextualised embedding model produces the best\nperformance. Interestingly, we observe that there is a trade-off between\ncreating an accurate vs. unbiased contextualised embedding model, and different\ncontextualised embedding models respond differently to this trade-off.", "published": "2021-01-23 15:28:48", "link": "http://arxiv.org/abs/2101.09523v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dictionary-based Debiasing of Pre-trained Word Embeddings", "abstract": "Word embeddings trained on large corpora have shown to encode high levels of\nunfair discriminatory gender, racial, religious and ethnic biases.\n  In contrast, human-written dictionaries describe the meanings of words in a\nconcise, objective and an unbiased manner.\n  We propose a method for debiasing pre-trained word embeddings using\ndictionaries, without requiring access to the original training resources or\nany knowledge regarding the word embedding algorithms used.\n  Unlike prior work, our proposed method does not require the types of biases\nto be pre-defined in the form of word lists, and learns the constraints that\nmust be satisfied by unbiased word embeddings automatically from dictionary\ndefinitions of the words.\n  Specifically, we learn an encoder to generate a debiased version of an input\nword embedding such that it\n  (a) retains the semantics of the pre-trained word embeddings,\n  (b) agrees with the unbiased definition of the word according to the\ndictionary, and\n  (c) remains orthogonal to the vector space spanned by any biased basis\nvectors in the pre-trained word embedding space.\n  Experimental results on standard benchmark datasets show that the proposed\nmethod can accurately remove unfair biases encoded in pre-trained word\nembeddings, while preserving useful semantics.", "published": "2021-01-23 15:44:23", "link": "http://arxiv.org/abs/2101.09525v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Evolution of Word Order", "abstract": "Most natural languages have a predominant or fixed word order. For example in\nEnglish the word order is usually Subject-Verb-Object. This work attempts to\nexplain this phenomenon as well as other typological findings regarding word\norder from a functional perspective. In particular, we examine whether fixed\nword order provides a functional advantage, explaining why these languages are\nprevalent. To this end, we consider an evolutionary model of language and\ndemonstrate, both theoretically and using genetic algorithms, that a language\nwith a fixed word order is optimal. We also show that adding information to the\nsentence, such as case markers and noun-verb distinction, reduces the need for\nfixed word order, in accordance with the typological findings.", "published": "2021-01-23 20:30:17", "link": "http://arxiv.org/abs/2101.09579v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arabic aspect based sentiment analysis using bidirectional GRU based\n  models", "abstract": "Aspect-based Sentiment analysis (ABSA) accomplishes a fine-grained analysis\nthat defines the aspects of a given document or sentence and the sentiments\nconveyed regarding each aspect. This level of analysis is the most detailed\nversion that is capable of exploring the nuanced viewpoints of the reviews. The\nbulk of study in ABSA focuses on English with very little work available in\nArabic. Most previous work in Arabic has been based on regular methods of\nmachine learning that mainly depends on a group of rare resources and tools for\nanalyzing and processing Arabic content such as lexicons, but the lack of those\nresources presents another challenge. In order to address these challenges,\nDeep Learning (DL)-based methods are proposed using two models based on Gated\nRecurrent Units (GRU) neural networks for ABSA. The first is a DL model that\ntakes advantage of word and character representations by combining\nbidirectional GRU, Convolutional Neural Network (CNN), and Conditional Random\nField (CRF) making up the (BGRU-CNN-CRF) model to extract the main opinionated\naspects (OTE). The second is an interactive attention network based on\nbidirectional GRU (IAN-BGRU) to identify sentiment polarity toward extracted\naspects. We evaluated our models using the benchmarked Arabic hotel reviews\ndataset. The results indicate that the proposed methods are better than\nbaseline research on both tasks having 39.7% enhancement in F1-score for\nopinion target extraction (T2) and 7.58% in accuracy for aspect-based sentiment\npolarity classification (T3). Achieving F1 score of 70.67% for T2, and accuracy\nof 83.98% for T3.", "published": "2021-01-23 02:54:30", "link": "http://arxiv.org/abs/2101.10539v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Team Performance with Embeddings from Multiparty Dialogues", "abstract": "Good communication is indubitably the foundation of effective teamwork. Over\ntime teams develop their own communication styles and often exhibit\nentrainment, a conversational phenomena in which humans synchronize their\nlinguistic choices. This paper examines the problem of predicting team\nperformance from embeddings learned from multiparty dialogues such that teams\nwith similar conflict scores lie close to one another in vector space.\nEmbeddings were extracted from three types of features: 1) dialogue acts 2)\nsentiment polarity 3) syntactic entrainment. Although all of these features can\nbe used to effectively predict team performance, their utility varies by the\nteamwork phase. We separate the dialogues of players playing a cooperative game\ninto stages: 1) early (knowledge building) 2) middle (problem-solving) and 3)\nlate (culmination). Unlike syntactic entrainment, both dialogue act and\nsentiment embeddings are effective for classifying team performance, even\nduring the initial phase. This finding has potential ramifications for the\ndevelopment of conversational agents that facilitate teaming.", "published": "2021-01-23 05:18:12", "link": "http://arxiv.org/abs/2101.09421v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Natural Language Question Answering over Earth Observation\n  Linked Data using Attention-based Neural Machine Translation", "abstract": "With an increase in Geospatial Linked Open Data being adopted and published\nover the web, there is a need to develop intuitive interfaces and systems for\nseamless and efficient exploratory analysis of such rich heterogeneous\nmulti-modal datasets. This work is geared towards improving the exploration\nprocess of Earth Observation (EO) Linked Data by developing a natural language\ninterface to facilitate querying. Questions asked over Earth Observation Linked\nData have an inherent spatio-temporal dimension and can be represented using\nGeoSPARQL. This paper seeks to study and analyze the use of RNN-based neural\nmachine translation with attention for transforming natural language questions\ninto GeoSPARQL queries. Specifically, it aims to assess the feasibility of a\nneural approach for identifying and mapping spatial predicates in natural\nlanguage to GeoSPARQL's topology vocabulary extension including - Egenhofer and\nRCC8 relations. The queries can then be executed over a triple store to yield\nanswers for the natural language questions. A dataset consisting of mappings\nfrom natural language questions to GeoSPARQL queries over the Corine Land\nCover(CLC) Linked Data has been created to train and validate the deep neural\nnetwork. From our experiments, it is evident that neural machine translation\nwith attention is a promising approach for the task of translating spatial\npredicates in natural language questions to GeoSPARQL queries.", "published": "2021-01-23 06:12:20", "link": "http://arxiv.org/abs/2101.09427v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Advances and Challenges in Conversational Recommender Systems: A Survey", "abstract": "Recommender systems exploit interaction history to estimate user preference,\nhaving been heavily used in a wide range of industry applications. However,\nstatic recommendation models are difficult to answer two important questions\nwell due to inherent shortcomings: (a) What exactly does a user like? (b) Why\ndoes a user like an item? The shortcomings are due to the way that static\nmodels learn user preference, i.e., without explicit instructions and active\nfeedback from users. The recent rise of conversational recommender systems\n(CRSs) changes this situation fundamentally. In a CRS, users and the system can\ndynamically communicate through natural language interactions, which provide\nunprecedented opportunities to explicitly obtain the exact preference of users.\n  Considerable efforts, spread across disparate settings and applications, have\nbeen put into developing CRSs. Existing models, technologies, and evaluation\nmethods for CRSs are far from mature. In this paper, we provide a systematic\nreview of the techniques used in current CRSs. We summarize the key challenges\nof developing CRSs in five directions: (1) Question-based user preference\nelicitation. (2) Multi-turn conversational recommendation strategies. (3)\nDialogue understanding and generation. (4) Exploitation-exploration trade-offs.\n(5) Evaluation and user simulation. These research directions involve multiple\nresearch fields like information retrieval (IR), natural language processing\n(NLP), and human-computer interaction (HCI). Based on these research\ndirections, we discuss some future challenges and opportunities. We provide a\nroad map for researchers from multiple communities to get started in this area.\nWe hope this survey can help to identify and address challenges in CRSs and\ninspire future research.", "published": "2021-01-23 08:53:15", "link": "http://arxiv.org/abs/2101.09459v7", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ARTH: Algorithm For Reading Text Handily -- An AI Aid for People having\n  Word Processing Issues", "abstract": "The objective of this project is to solve one of the major problems faced by\nthe people having word processing issues like trauma, or mild mental\ndisability. \"ARTH\" is the short form of Algorithm for Reading Handily. ARTH is\na self-learning set of algorithms that is an intelligent way of fulfilling the\nneed for \"reading and understanding the text effortlessly\" which adjusts\naccording to the needs of every user. The research project propagates in two\nsteps. In the first step, the algorithm tries to identify the difficult words\npresent in the text based on two features -- the number of syllables and usage\nfrequency -- using a clustering algorithm. After the analysis of the clusters,\nthe algorithm labels these clusters, according to their difficulty level. In\nthe second step, the algorithm interacts with the user. It aims to test the\nuser's comprehensibility of the text and his/her vocabulary level by taking an\nautomatically generated quiz. The algorithm identifies the clusters which are\ndifficult for the user, based on the result of the analysis. The meaning of\nperceived difficult words is displayed next to them. The technology \"ARTH\"\nfocuses on the revival of the joy of reading among those people, who have a\npoor vocabulary or any word processing issues.", "published": "2021-01-23 09:39:45", "link": "http://arxiv.org/abs/2101.09464v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
