{"title": "Optical Text Recognition in Nepali and Bengali: A Transformer-based\n  Approach", "abstract": "Efforts on the research and development of OCR systems for Low-Resource\nLanguages are relatively new. Low-resource languages have little training data\navailable for training Machine Translation systems or other systems. Even\nthough a vast amount of text has been digitized and made available on the\ninternet the text is still in PDF and Image format, which are not instantly\naccessible. This paper discusses text recognition for two scripts: Bengali and\nNepali; there are about 300 and 40 million Bengali and Nepali speakers\nrespectively. In this study, using encoder-decoder transformers, a model was\ndeveloped, and its efficacy was assessed using a collection of optical text\nimages, both handwritten and printed. The results signify that the suggested\ntechnique corresponds with current approaches and achieves high precision in\nrecognizing text in Bengali and Nepali. This study can pave the way for the\nadvanced and accessible study of linguistics in South East Asia.", "published": "2024-04-03 00:21:14", "link": "http://arxiv.org/abs/2404.02375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-resource neural machine translation with morphological modeling", "abstract": "Morphological modeling in neural machine translation (NMT) is a promising\napproach to achieving open-vocabulary machine translation for\nmorphologically-rich languages. However, existing methods such as sub-word\ntokenization and character-based models are limited to the surface forms of the\nwords. In this work, we propose a framework-solution for modeling complex\nmorphology in low-resource settings. A two-tier transformer architecture is\nchosen to encode morphological information at the inputs. At the target-side\noutput, a multi-task multi-label training scheme coupled with a beam\nsearch-based decoder are found to improve machine translation performance. An\nattention augmentation scheme to the transformer model is proposed in a generic\nform to allow integration of pre-trained language models and also facilitate\nmodeling of word order relationships between the source and target languages.\nSeveral data augmentation techniques are evaluated and shown to increase\ntranslation performance in low-resource settings. We evaluate our proposed\nsolution on Kinyarwanda - English translation using public-domain parallel\ntext. Our final models achieve competitive performance in relation to large\nmulti-lingual models. We hope that our results will motivate more use of\nexplicit morphological information and the proposed model and data\naugmentations in low-resource NMT.", "published": "2024-04-03 01:31:41", "link": "http://arxiv.org/abs/2404.02392v1", "categories": ["cs.CL", "I.2.7; I.2"], "primary_category": "cs.CL"}
{"title": "Backdoor Attack on Multilingual Machine Translation", "abstract": "While multilingual machine translation (MNMT) systems hold substantial\npromise, they also have security vulnerabilities. Our research highlights that\nMNMT systems can be susceptible to a particularly devious style of backdoor\nattack, whereby an attacker injects poisoned data into a low-resource language\npair to cause malicious translations in other languages, including\nhigh-resource languages. Our experimental results reveal that injecting less\nthan 0.01% poisoned data into a low-resource language pair can achieve an\naverage 20% attack success rate in attacking high-resource language pairs. This\ntype of attack is of particular concern, given the larger attack surface of\nlanguages inherent to low-resource settings. Our aim is to bring attention to\nthese vulnerabilities within MNMT systems with the hope of encouraging the\ncommunity to address security concerns in machine translation, especially in\nthe context of low-resource languages.", "published": "2024-04-03 01:32:31", "link": "http://arxiv.org/abs/2404.02393v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CMULAB: An Open-Source Framework for Training and Deployment of Natural\n  Language Processing Models", "abstract": "Effectively using Natural Language Processing (NLP) tools in under-resourced\nlanguages requires a thorough understanding of the language itself, familiarity\nwith the latest models and training methodologies, and technical expertise to\ndeploy these models. This could present a significant obstacle for language\ncommunity members and linguists to use NLP tools. This paper introduces the CMU\nLinguistic Annotation Backend, an open-source framework that simplifies model\ndeployment and continuous human-in-the-loop fine-tuning of NLP models. CMULAB\nenables users to leverage the power of multilingual models to quickly adapt and\nextend existing tools for speech recognition, OCR, translation, and syntactic\nanalysis to new languages, even with limited training data. We describe various\ntools and APIs that are currently available and how developers can easily add\nnew models/functionality to the framework. Code is available at\nhttps://github.com/neulab/cmulab along with a live demo at https://cmulab.dev", "published": "2024-04-03 02:21:46", "link": "http://arxiv.org/abs/2404.02408v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting subword tokenization: A case study on affixal negation in\n  large language models", "abstract": "In this work, we measure the impact of affixal negation on modern English\nlarge language models (LLMs). In affixal negation, the negated meaning is\nexpressed through a negative morpheme, which is potentially challenging for\nLLMs as their tokenizers are often not morphologically plausible. We conduct\nextensive experiments using LLMs with different subword tokenization methods,\nwhich lead to several insights on the interaction between tokenization\nperformance and negation sensitivity. Despite some interesting mismatches\nbetween tokenization accuracy and negation detection performance, we show that\nmodels can, on the whole, reliably recognize the meaning of affixal negation.", "published": "2024-04-03 03:14:27", "link": "http://arxiv.org/abs/2404.02421v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Multilingual Ability of Decoder-based Pre-trained Language\n  Models: Finding and Controlling Language-Specific Neurons", "abstract": "Current decoder-based pre-trained language models (PLMs) successfully\ndemonstrate multilingual capabilities. However, it is unclear how these models\nhandle multilingualism. We analyze the neuron-level internal behavior of\nmultilingual decoder-based PLMs, Specifically examining the existence of\nneurons that fire ``uniquely for each language'' within decoder-only\nmultilingual PLMs. We analyze six languages: English, German, French, Spanish,\nChinese, and Japanese, and show that language-specific neurons are unique, with\na slight overlap (< 5%) between languages. These neurons are mainly distributed\nin the models' first and last few layers. This trend remains consistent across\nlanguages and models. Additionally, we tamper with less than 1% of the total\nneurons in each model during inference and demonstrate that tampering with a\nfew language-specific neurons drastically changes the probability of target\nlanguage occurrence in text generation.", "published": "2024-04-03 03:37:22", "link": "http://arxiv.org/abs/2404.02431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Cross-lingual Text Classification through In-Context One-Shot\n  Demonstrations", "abstract": "Zero-Shot Cross-lingual Transfer (ZS-XLT) utilizes a model trained in a\nsource language to make predictions in another language, often with a\nperformance loss. To alleviate this, additional improvements can be achieved\nthrough subsequent adaptation using examples in the target language. In this\npaper, we exploit In-Context Tuning (ICT) for One-Shot Cross-lingual transfer\nin the classification task by introducing In-Context Cross-lingual Transfer\n(IC-XLT). The novel concept involves training a model to learn from context\nexamples and subsequently adapting it during inference to a target language by\nprepending a One-Shot context demonstration in that language. Our results show\nthat IC-XLT successfully leverages target-language examples to improve the\ncross-lingual capabilities of the evaluated mT5 model, outperforming\nprompt-based models in the Zero and Few-shot scenarios adapted through\nfine-tuning. Moreover, we show that when source-language data is limited, the\nfine-tuning framework employed for IC-XLT performs comparably to prompt-based\nfine-tuning with significantly more training data in the source language.", "published": "2024-04-03 04:40:57", "link": "http://arxiv.org/abs/2404.02452v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Cross-lingual Sentence Embedding for Low-resource Languages\n  with Word Alignment", "abstract": "The field of cross-lingual sentence embeddings has recently experienced\nsignificant advancements, but research concerning low-resource languages has\nlagged due to the scarcity of parallel corpora. This paper shows that\ncross-lingual word representation in low-resource languages is notably\nunder-aligned with that in high-resource languages in current models. To\naddress this, we introduce a novel framework that explicitly aligns words\nbetween English and eight low-resource languages, utilizing off-the-shelf word\nalignment models. This framework incorporates three primary training\nobjectives: aligned word prediction and word translation ranking, along with\nthe widely used translation ranking. We evaluate our approach through\nexperiments on the bitext retrieval task, which demonstrate substantial\nimprovements on sentence embeddings in low-resource languages. In addition, the\ncompetitive performance of the proposed model across a broader range of tasks\nin high-resource languages underscores its practicality.", "published": "2024-04-03 05:58:53", "link": "http://arxiv.org/abs/2404.02490v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lifelong Event Detection with Embedding Space Separation and Compaction", "abstract": "To mitigate forgetting, existing lifelong event detection methods typically\nmaintain a memory module and replay the stored memory data during the learning\nof a new task. However, the simple combination of memory data and new-task\nsamples can still result in substantial forgetting of previously acquired\nknowledge, which may occur due to the potential overlap between the feature\ndistribution of new data and the previously learned embedding space. Moreover,\nthe model suffers from overfitting on the few memory samples rather than\neffectively remembering learned patterns. To address the challenges of\nforgetting and overfitting, we propose a novel method based on embedding space\nseparation and compaction. Our method alleviates forgetting of previously\nlearned tasks by forcing the feature distribution of new data away from the\nprevious embedding space. It also mitigates overfitting by a memory calibration\nmechanism that encourages memory data to be close to its prototype to enhance\nintra-class compactness. In addition, the learnable parameters of the new task\nare initialized by drawing upon acquired knowledge from the previously learned\ntask to facilitate forward knowledge transfer. With extensive experiments, we\ndemonstrate that our method can significantly outperform previous\nstate-of-the-art approaches.", "published": "2024-04-03 06:51:49", "link": "http://arxiv.org/abs/2404.02507v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Large Language Model driven Reference-less Translation\n  Evaluation for English and Indian Languages", "abstract": "With the primary focus on evaluating the effectiveness of large language\nmodels for automatic reference-less translation assessment, this work presents\nour experiments on mimicking human direct assessment to evaluate the quality of\ntranslations in English and Indian languages. We constructed a translation\nevaluation task where we performed zero-shot learning, in-context\nexample-driven learning, and fine-tuning of large language models to provide a\nscore out of 100, where 100 represents a perfect translation and 1 represents a\npoor translation. We compared the performance of our trained systems with\nexisting methods such as COMET, BERT-Scorer, and LABSE, and found that the\nLLM-based evaluator (LLaMA-2-13B) achieves a comparable or higher overall\ncorrelation with human judgments for the considered Indian language pairs.", "published": "2024-04-03 06:57:45", "link": "http://arxiv.org/abs/2404.02512v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A School Student Essay Corpus for Analyzing Interactions of\n  Argumentative Structure and Quality", "abstract": "Learning argumentative writing is challenging. Besides writing fundamentals\nsuch as syntax and grammar, learners must select and arrange argument\ncomponents meaningfully to create high-quality essays. To support argumentative\nwriting computationally, one step is to mine the argumentative structure. When\ncombined with automatic essay scoring, interactions of the argumentative\nstructure and quality scores can be exploited for comprehensive writing\nsupport. Although studies have shown the usefulness of using information about\nthe argumentative structure for essay scoring, no argument mining corpus with\nground-truth essay quality annotations has been published yet. Moreover, none\nof the existing corpora contain essays written by school students specifically.\nTo fill this research gap, we present a German corpus of 1,320 essays from\nschool students of two age groups. Each essay has been manually annotated for\nargumentative structure and quality on multiple levels of granularity. We\npropose baseline approaches to argument mining and essay scoring, and we\nanalyze interactions between both tasks, thereby laying the ground for\nquality-oriented argumentative writing support.", "published": "2024-04-03 07:31:53", "link": "http://arxiv.org/abs/2404.02529v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CSEPrompts: A Benchmark of Introductory Computer Science Prompts", "abstract": "Recent advances in AI, machine learning, and NLP have led to the development\nof a new generation of Large Language Models (LLMs) that are trained on massive\namounts of data and often have trillions of parameters. Commercial applications\n(e.g., ChatGPT) have made this technology available to the general public, thus\nmaking it possible to use LLMs to produce high-quality texts for academic and\nprofessional purposes. Schools and universities are aware of the increasing use\nof AI-generated content by students and they have been researching the impact\nof this new technology and its potential misuse. Educational programs in\nComputer Science (CS) and related fields are particularly affected because LLMs\nare also capable of generating programming code in various programming\nlanguages. To help understand the potential impact of publicly available LLMs\nin CS education, we introduce CSEPrompts, a framework with hundreds of\nprogramming exercise prompts and multiple-choice questions retrieved from\nintroductory CS and programming courses. We also provide experimental results\non CSEPrompts to evaluate the performance of several LLMs with respect to\ngenerating Python code and answering basic computer science and programming\nquestions.", "published": "2024-04-03 07:55:57", "link": "http://arxiv.org/abs/2404.02540v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MaiNLP at SemEval-2024 Task 1: Analyzing Source Language Selection in\n  Cross-Lingual Textual Relatedness", "abstract": "This paper presents our system developed for the SemEval-2024 Task 1:\nSemantic Textual Relatedness (STR), on Track C: Cross-lingual. The task aims to\ndetect semantic relatedness of two sentences in a given target language without\naccess to direct supervision (i.e. zero-shot cross-lingual transfer). To this\nend, we focus on different source language selection strategies on two\ndifferent pre-trained languages models: XLM-R and Furina. We experiment with 1)\nsingle-source transfer and select source languages based on typological\nsimilarity, 2) augmenting English training data with the two nearest-neighbor\nsource languages, and 3) multi-source transfer where we compare selecting on\nall training languages against languages from the same family. We further study\nmachine translation-based data augmentation and the impact of script\ndifferences. Our submission achieved the first place in the C8 (Kinyarwanda)\ntest set.", "published": "2024-04-03 08:44:51", "link": "http://arxiv.org/abs/2404.02570v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models as Compilers: Simulating Pseudocode Execution Improves\n  Algorithmic Reasoning in Language Models", "abstract": "Algorithmic reasoning refers to the ability to understand the complex\npatterns behind the problem and decompose them into a sequence of reasoning\nsteps towards the solution. Such nature of algorithmic reasoning makes it a\nchallenge for large language models (LLMs), even though they have demonstrated\npromising performance in other reasoning tasks. Within this context, some\nrecent studies use programming languages (e.g., Python) to express the\nnecessary logic for solving a given instance/question (e.g.,\nProgram-of-Thought) as inspired by their strict and precise syntaxes. However,\nit is non-trivial to write an executable code that expresses the correct logic\non the fly within a single inference call. Also, the code generated\nspecifically for an instance cannot be reused for others, even if they are from\nthe same task and might require identical logic to solve. This paper presents\nThink-and-Execute, a novel framework that decomposes the reasoning process of\nlanguage models into two steps. (1) In Think, we discover a task-level logic\nthat is shared across all instances for solving a given task and then express\nthe logic with pseudocode; (2) In Execute, we further tailor the generated\npseudocode to each instance and simulate the execution of the code. With\nextensive experiments on seven algorithmic reasoning tasks, we demonstrate the\neffectiveness of Think-and-Execute. Our approach better improves LMs' reasoning\ncompared to several strong baselines performing instance-specific reasoning\n(e.g., CoT and PoT), suggesting the helpfulness of discovering task-level\nlogic. Also, we show that compared to natural language, pseudocode can better\nguide the reasoning of LMs, even though they are trained to follow natural\nlanguage instructions.", "published": "2024-04-03 08:49:11", "link": "http://arxiv.org/abs/2404.02575v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Expansion of Spoken Language Understanding\n  Systems to New Languages", "abstract": "Spoken Language Understanding (SLU) models are a core component of voice\nassistants (VA), such as Alexa, Bixby, and Google Assistant. In this paper, we\nintroduce a pipeline designed to extend SLU systems to new languages, utilizing\nLarge Language Models (LLMs) that we fine-tune for machine translation of\nslot-annotated SLU training data. Our approach improved on the MultiATIS++\nbenchmark, a primary multi-language SLU dataset, in the cloud scenario using an\nmBERT model. Specifically, we saw an improvement in the Overall Accuracy\nmetric: from 53% to 62.18%, compared to the existing state-of-the-art method,\nFine and Coarse-grained Multi-Task Learning Framework (FC-MTLF). In the\non-device scenario (tiny and not pretrained SLU), our method improved the\nOverall Accuracy from 5.31% to 22.06% over the baseline Global-Local\nContrastive Learning Framework (GL-CLeF) method. Contrary to both FC-MTLF and\nGL-CLeF, our LLM-based machine translation does not require changes in the\nproduction architecture of SLU. Additionally, our pipeline is slot-type\nindependent: it does not require any slot definitions or examples.", "published": "2024-04-03 09:13:26", "link": "http://arxiv.org/abs/2404.02588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adjusting Interpretable Dimensions in Embedding Space with Human\n  Judgments", "abstract": "Embedding spaces contain interpretable dimensions indicating gender,\nformality in style, or even object properties. This has been observed multiple\ntimes. Such interpretable dimensions are becoming valuable tools in different\nareas of study, from social science to neuroscience. The standard way to\ncompute these dimensions uses contrasting seed words and computes difference\nvectors over them. This is simple but does not always work well. We combine\nseed-based vectors with guidance from human ratings of where words fall along a\nspecific dimension, and evaluate on predicting both object properties like size\nand danger, and the stylistic properties of formality and complexity. We obtain\ninterpretable dimensions with markedly better performance especially in cases\nwhere seed-based dimensions do not work well.", "published": "2024-04-03 10:13:18", "link": "http://arxiv.org/abs/2404.02619v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Estimating the Causal Effects of Natural Logic Features in\n  Transformer-Based NLI Models", "abstract": "Rigorous evaluation of the causal effects of semantic features on language\nmodel predictions can be hard to achieve for natural language reasoning\nproblems. However, this is such a desirable form of analysis from both an\ninterpretability and model evaluation perspective, that it is valuable to\ninvestigate specific patterns of reasoning with enough structure and regularity\nto identify and quantify systematic reasoning failures in widely-used models.\nIn this vein, we pick a portion of the NLI task for which an explicit causal\ndiagram can be systematically constructed: the case where across two sentences\n(the premise and hypothesis), two related words/terms occur in a shared\ncontext. In this work, we apply causal effect estimation strategies to measure\nthe effect of context interventions (whose effect on the entailment label is\nmediated by the semantic monotonicity characteristic) and interventions on the\ninserted word-pair (whose effect on the entailment label is mediated by the\nrelation between these words). Extending related work on causal analysis of NLP\nmodels in different settings, we perform an extensive interventional study on\nthe NLI task to investigate robustness to irrelevant changes and sensitivity to\nimpactful changes of Transformers. The results strongly bolster the fact that\nsimilar benchmark accuracy scores may be observed for models that exhibit very\ndifferent behaviour. Moreover, our methodology reinforces previously suspected\nbiases from a causal perspective, including biases in favour of upward-monotone\ncontexts and ignoring the effects of negation markers.", "published": "2024-04-03 10:22:35", "link": "http://arxiv.org/abs/2404.02622v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Calibrating the Confidence of Large Language Models by Eliciting\n  Fidelity", "abstract": "Large language models optimized with techniques like RLHF have achieved good\nalignment in being helpful and harmless. However, post-alignment, these\nlanguage models often exhibit overconfidence, where the expressed confidence\ndoes not accurately calibrate with their correctness rate. In this paper, we\ndecompose the language model confidence into the \\textit{Uncertainty} about the\nquestion and the \\textit{Fidelity} to the answer generated by language models.\nThen, we propose a plug-and-play method to estimate the confidence of language\nmodels. Our method has shown good calibration performance by conducting\nexperiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two\nnovel metrics, IPR and CE, to evaluate the calibration of the model, and we\nhave conducted a detailed discussion on \\textit{Truly Well-Calibrated\nConfidence}. Our method could serve as a strong baseline, and we hope that this\nwork will provide some insights into the model confidence calibration.", "published": "2024-04-03 11:36:12", "link": "http://arxiv.org/abs/2404.02655v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scalable Model Editing via Customized Expert Networks", "abstract": "Addressing the issues of hallucinations and outdated knowledge in large\nlanguage models is critical for their reliable application. Model Editing\npresents a promising avenue for mitigating these challenges in a cost-effective\nmanner. However, existing methods often suffer from unsatisfactory\ngeneralization and unintended effects on non-edited samples. To overcome these\nlimitations, we introduce a novel approach: Scalable Model Editing via\nCustomized Expert Networks (SCEN), which is a two-stage continuous training\nparadigm. Specifically, in the first stage, we train lightweight expert\nnetworks individually for each piece of knowledge that needs to be updated.\nSubsequently, we train a corresponding indexing neuron for each expert to\ncontrol the activation state of that expert. We conducted a series of\nexperiments on the ZsRE and Hallucination benchmarks by tuning the advanced\nopen-source LLM, Llama2, achieving state-of-the-art results compared to current\nmainstream methods. Our code is available at\nhttps://github.com/TAL-auroraX/SCEN.", "published": "2024-04-03 12:57:19", "link": "http://arxiv.org/abs/2404.02699v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FPT: Feature Prompt Tuning for Few-shot Readability Assessment", "abstract": "Prompt-based methods have achieved promising results in most few-shot text\nclassification tasks. However, for readability assessment tasks, traditional\nprompt methods lackcrucial linguistic knowledge, which has already been proven\nto be essential. Moreover, previous studies on utilizing linguistic features\nhave shown non-robust performance in few-shot settings and may even impair\nmodel performance.To address these issues, we propose a novel prompt-based\ntuning framework that incorporates rich linguistic knowledge, called Feature\nPrompt Tuning (FPT). Specifically, we extract linguistic features from the text\nand embed them into trainable soft prompts. Further, we devise a new loss\nfunction to calibrate the similarity ranking order between categories.\nExperimental results demonstrate that our proposed method FTP not only exhibits\na significant performance improvement over the prior best prompt-based tuning\napproaches, but also surpasses the previous leading methods that incorporate\nlinguistic features. Also, our proposed model significantly outperforms the\nlarge language model gpt-3.5-turbo-16k in most cases. Our proposed method\nestablishes a new architecture for prompt tuning that sheds light on how\nlinguistic features can be easily adapted to linguistic-related tasks.", "published": "2024-04-03 14:39:47", "link": "http://arxiv.org/abs/2404.02772v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieving Examples from Memory for Retrieval Augmented Neural Machine\n  Translation: A Systematic Comparison", "abstract": "Retrieval-Augmented Neural Machine Translation (RAMT) architectures retrieve\nexamples from memory to guide the generation process. While most works in this\ntrend explore new ways to exploit the retrieved examples, the upstream\nretrieval step is mostly unexplored. In this paper, we study the effect of\nvarying retrieval methods for several translation architectures, to better\nunderstand the interplay between these two processes. We conduct experiments in\ntwo language pairs in a multi-domain setting and consider several downstream\narchitectures based on a standard autoregressive model, an edit-based model,\nand a large language model with in-context learning. Our experiments show that\nthe choice of the retrieval technique impacts the translation scores, with\nvariance across architectures. We also discuss the effects of increasing the\nnumber and diversity of examples, which are mostly positive across the board.", "published": "2024-04-03 16:13:29", "link": "http://arxiv.org/abs/2404.02835v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cherry on Top: Parameter Heterogeneity and Quantization in Large\n  Language Models", "abstract": "This paper reveals the phenomenon of parameter heterogeneity in large\nlanguage models (LLMs). We find that a small subset of \"cherry\" parameters\nexhibit a disproportionately large influence on model performance, while the\nvast majority of parameters have minimal impact. This heterogeneity is found to\nbe prevalent across different model families, scales, and types. Motivated by\nthis observation, we propose CherryQ, a novel quantization method that unifies\nthe optimization of mixed-precision parameters. CherryQ identifies and\npreserves the critical cherry parameters in high precision while aggressively\nquantizing the remaining parameters to low precision. Extensive experiments\ndemonstrate the effectiveness of CherryQ. CherryQ outperforms existing\nquantization approaches in terms of perplexity and downstream task performance.\nNotably, our 3-bit quantized Vicuna-1.5 exhibits competitive performance\ncompared to their 16-bit counterparts.", "published": "2024-04-03 16:16:31", "link": "http://arxiv.org/abs/2404.02837v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models\n  with a Self-Critique Pipeline", "abstract": "Large language models (LLMs) have shown excellent mastering of human\nlanguage, but still struggle in real-world applications that require\nmathematical problem-solving. While many strategies and datasets to enhance\nLLMs' mathematics are developed, it remains a challenge to simultaneously\nmaintain and improve both language and mathematical capabilities in deployed\nLLM systems.In this work, we tailor the Self-Critique pipeline, which addresses\nthe challenge in the feedback learning stage of LLM alignment. We first train a\ngeneral Math-Critique model from the LLM itself to provide feedback signals.\nThen, we sequentially employ rejective fine-tuning and direct preference\noptimization over the LLM's own generations for data collection. Based on\nChatGLM3-32B, we conduct a series of experiments on both academic and our newly\ncreated challenging dataset, MathUserEval. Results show that our pipeline\nsignificantly enhances the LLM's mathematical problem-solving while still\nimproving its language ability, outperforming LLMs that could be two times\nlarger. Related techniques have been deployed to\nChatGLM\\footnote{\\url{https://chatglm.cn}}, an online serving LLM. Related\nevaluation dataset and scripts are released at\n\\url{https://github.com/THUDM/ChatGLM-Math}.", "published": "2024-04-03 17:51:18", "link": "http://arxiv.org/abs/2404.02893v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a Fully Interpretable and More Scalable RSA Model for Metaphor\n  Understanding", "abstract": "The Rational Speech Act (RSA) model provides a flexible framework to model\npragmatic reasoning in computational terms. However, state-of-the-art RSA\nmodels are still fairly distant from modern machine learning techniques and\npresent a number of limitations related to their interpretability and\nscalability. Here, we introduce a new RSA framework for metaphor understanding\nthat addresses these limitations by providing an explicit formula - based on\nthe mutually shared information between the speaker and the listener - for the\nestimation of the communicative goal and by learning the rationality parameter\nusing gradient-based methods. The model was tested against 24 metaphors, not\nlimited to the conventional $\\textit{John-is-a-shark}$ type. Results suggest an\noverall strong positive correlation between the distributions generated by the\nmodel and the interpretations obtained from the human behavioral data, which\nincreased when the intended meaning capitalized on properties that were\ninherent to the vehicle concept. Overall, findings suggest that metaphor\nprocessing is well captured by a typicality-based Bayesian model, even when\nmore scalable and interpretable, opening up possible applications to other\npragmatic phenomena and novel uses for increasing Large Language Models\ninterpretability. Yet, results highlight that the more creative nuances of\nmetaphorical meaning, not strictly encoded in the lexical concepts, are a\nchallenging aspect for machines.", "published": "2024-04-03 18:09:33", "link": "http://arxiv.org/abs/2404.02983v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Incomplete Loop: Instruction Inference, Instruction Following, and\n  In-context Learning in Language Models", "abstract": "Modern language models (LMs) can learn to perform new tasks in different\nways: in instruction following, the target task is described explicitly in\nnatural language; in few-shot prompting, the task is specified implicitly with\na small number of examples; in instruction inference, LMs are presented with\nin-context examples and are then prompted to generate a natural language task\ndescription before making predictions. Each of these procedures may be thought\nof as invoking a different form of reasoning: instruction following involves\ndeductive reasoning, few-shot prompting involves inductive reasoning, and\ninstruction inference involves abductive reasoning. How do these different\ncapabilities relate? Across four LMs (from the gpt and llama families) and two\nlearning problems (involving arithmetic functions and machine translation) we\nfind a strong dissociation between the different types of reasoning: LMs can\nsometimes learn effectively from few-shot prompts even when they are unable to\nexplain their own prediction rules; conversely, they sometimes infer useful\ntask descriptions while completely failing to learn from human-generated\ndescriptions of the same task. Our results highlight the non-systematic nature\nof reasoning even in some of today's largest LMs, and underscore the fact that\nvery different learning mechanisms may be invoked by seemingly similar\nprompting procedures.", "published": "2024-04-03 19:31:56", "link": "http://arxiv.org/abs/2404.03028v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MuLan: A Study of Fact Mutability in Language Models", "abstract": "Facts are subject to contingencies and can be true or false in different\ncircumstances. One such contingency is time, wherein some facts mutate over a\ngiven period, e.g., the president of a country or the winner of a championship.\nTrustworthy language models ideally identify mutable facts as such and process\nthem accordingly. We create MuLan, a benchmark for evaluating the ability of\nEnglish language models to anticipate time-contingency, covering both 1:1 and\n1:N relations. We hypothesize that mutable facts are encoded differently than\nimmutable ones, hence being easier to update. In a detailed evaluation of six\npopular large language models, we consistently find differences in the LLMs'\nconfidence, representations, and update behavior, depending on the mutability\nof a fact. Our findings should inform future work on the injection of and\ninduction of time-contingent knowledge to/from LLMs.", "published": "2024-04-03 19:47:33", "link": "http://arxiv.org/abs/2404.03036v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language, Environment, and Robotic Navigation", "abstract": "This paper explores the integration of linguistic inputs within robotic\nnavigation systems, drawing upon the symbol interdependency hypothesis to\nbridge the divide between symbolic and embodied cognition. It examines previous\nwork incorporating language and semantics into Neural Network (NN) and\nSimultaneous Localization and Mapping (SLAM) approaches, highlighting how these\nintegrations have advanced the field. By contrasting abstract symbol\nmanipulation with sensory-motor grounding, we propose a unified framework where\nlanguage functions both as an abstract communicative system and as a grounded\nrepresentation of perceptual experiences. Our review of cognitive models of\ndistributional semantics and their application to autonomous agents underscores\nthe transformative potential of language-integrated systems.", "published": "2024-04-03 20:30:38", "link": "http://arxiv.org/abs/2404.03049v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GPT-DETOX: An In-Context Learning-Based Paraphraser for Text\n  Detoxification", "abstract": "Harmful and offensive communication or content is detrimental to social\nbonding and the mental state of users on social media platforms. Text\ndetoxification is a crucial task in natural language processing (NLP), where\nthe goal is removing profanity and toxicity from text while preserving its\ncontent. Supervised and unsupervised learning are common approaches for\ndesigning text detoxification solutions. However, these methods necessitate\nfine-tuning, leading to computational overhead. In this paper, we propose\nGPT-DETOX as a framework for prompt-based in-context learning for text\ndetoxification using GPT-3.5 Turbo. We utilize zero-shot and few-shot prompting\ntechniques for detoxifying input sentences. To generate few-shot prompts, we\npropose two methods: word-matching example selection (WMES) and\ncontext-matching example selection (CMES). We additionally take into account\nensemble in-context learning (EICL) where the ensemble is shaped by base\nprompts from zero-shot and all few-shot settings. We use ParaDetox and APPDIA\nas benchmark detoxification datasets. Our experimental results show that the\nzero-shot solution achieves promising performance, while our best few-shot\nsetting outperforms the state-of-the-art models on ParaDetox and shows\ncomparable results on APPDIA. Our EICL solutions obtain the greatest\nperformance, adding at least 10% improvement, against both datasets.", "published": "2024-04-03 20:35:36", "link": "http://arxiv.org/abs/2404.03052v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing ML Classification Algorithms and NLP Techniques for Depression\n  Detection: An Experimental Case Study", "abstract": "Depression has affected millions of people worldwide and has become one of\nthe most common mental disorders. Early mental disorder detection can reduce\ncosts for public health agencies and prevent other major comorbidities.\nAdditionally, the shortage of specialized personnel is very concerning since\nDepression diagnosis is highly dependent on expert professionals and is\ntime-consuming. Recent research has evidenced that machine learning (ML) and\nNatural Language Processing (NLP) tools and techniques have significantly bene\nted the diagnosis of depression. However, there are still several challenges in\nthe assessment of depression detection approaches in which other conditions\nsuch as post-traumatic stress disorder (PTSD) are present. These challenges\ninclude assessing alternatives in terms of data cleaning and pre-processing\ntechniques, feature selection, and appropriate ML classification algorithms.\nThis paper tackels such an assessment based on a case study that compares\ndifferent ML classifiers, specifically in terms of data cleaning and\npre-processing, feature selection, parameter setting, and model choices. The\ncase study is based on the Distress Analysis Interview Corpus - Wizard-of-Oz\n(DAIC-WOZ) dataset, which is designed to support the diagnosis of mental\ndisorders such as depression, anxiety, and PTSD. Besides the assessment of\nalternative techniques, we were able to build models with accuracy levels\naround 84% with Random Forest and XGBoost models, which is significantly higher\nthan the results from the comparable literature which presented the level of\naccuracy of 72% from the SVM model.", "published": "2024-04-03 19:45:40", "link": "http://arxiv.org/abs/2404.04284v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Linearizing Structured Data in Encoder-Decoder Language Models:\n  Insights from Text-to-SQL", "abstract": "Structured data, prevalent in tables, databases, and knowledge graphs, poses\na significant challenge in its representation. With the advent of large\nlanguage models (LLMs), there has been a shift towards linearization-based\nmethods, which process structured data as sequential token streams, diverging\nfrom approaches that explicitly model structure, often as a graph. Crucially,\nthere remains a gap in our understanding of how these linearization-based\nmethods handle structured data, which is inherently non-linear. This work\ninvestigates the linear handling of structured data in encoder-decoder language\nmodels, specifically T5. Our findings reveal the model's ability to mimic\nhuman-designed processes such as schema linking and syntax prediction,\nindicating a deep, meaningful learning of structure beyond simple token\nsequencing. We also uncover insights into the model's internal mechanisms,\nincluding the ego-centric nature of structure node encodings and the potential\nfor model compression due to modality fusion redundancy. Overall, this work\nsheds light on the inner workings of linearization-based methods and could\npotentially provide guidance for future research.", "published": "2024-04-03 01:16:20", "link": "http://arxiv.org/abs/2404.02389v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Benchmarking Large Language Models for Persian: A Preliminary Study\n  Focusing on ChatGPT", "abstract": "This paper explores the efficacy of large language models (LLMs) for Persian.\nWhile ChatGPT and consequent LLMs have shown remarkable performance in English,\ntheir efficiency for more low-resource languages remains an open question. We\npresent the first comprehensive benchmarking study of LLMs across diverse\nPersian language tasks. Our primary focus is on GPT-3.5-turbo, but we also\ninclude GPT-4 and OpenChat-3.5 to provide a more holistic evaluation. Our\nassessment encompasses a diverse set of tasks categorized into classic,\nreasoning, and knowledge-based domains. To enable a thorough comparison, we\nevaluate LLMs against existing task-specific fine-tuned models. Given the\nlimited availability of Persian datasets for reasoning tasks, we introduce two\nnew benchmarks: one based on elementary school math questions and another\nderived from the entrance exams for 7th and 10th grades. Our findings reveal\nthat while LLMs, especially GPT-4, excel in tasks requiring reasoning abilities\nand a broad understanding of general knowledge, they often lag behind smaller\npre-trained models fine-tuned specifically for particular tasks. Additionally,\nwe observe improved performance when test sets are translated to English before\ninputting them into GPT-3.5. These results highlight the significant potential\nfor enhancing LLM performance in the Persian language. This is particularly\nnoteworthy due to the unique attributes of Persian, including its distinct\nalphabet and writing styles.", "published": "2024-04-03 02:12:29", "link": "http://arxiv.org/abs/2404.02403v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Auxiliary task demands mask the capabilities of smaller language models", "abstract": "Developmental psychologists have argued about when cognitive capacities such\nas language understanding or theory of mind emerge. These debates often hinge\non the concept of \"task demands\" -- the auxiliary challenges associated with\nperforming a particular evaluation -- that may mask the child's underlying\nability. The same issues arise when measuring the capacities of language models\n(LMs): performance on a task is a function of the model's underlying knowledge,\ncombined with the model's ability to interpret and perform the task given its\navailable resources. Here, we show that for analogical reasoning, reflective\nreasoning, word prediction, and grammaticality judgments, evaluation methods\nwith greater task demands yield lower performance than evaluations with reduced\ndemands. This \"demand gap\" is most pronounced for models with fewer parameters\nand less training data. Our results illustrate that LM performance should not\nbe interpreted as a direct indication of intelligence (or lack thereof), but as\na reflection of capacities seen through the lens of researchers' design\nchoices.", "published": "2024-04-03 02:56:52", "link": "http://arxiv.org/abs/2404.02418v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data", "abstract": "Large Language Models (LLMs) operating in 0-shot or few-shot settings achieve\ncompetitive results in Text Classification tasks. In-Context Learning (ICL)\ntypically achieves better accuracy than the 0-shot setting, but it pays in\nterms of efficiency, due to the longer input prompt. In this paper, we propose\na strategy to make LLMs as efficient as 0-shot text classifiers, while getting\ncomparable or better accuracy than ICL. Our solution targets the low resource\nsetting, i.e., when only 4 examples per class are available. Using a single LLM\nand few-shot real data we perform a sequence of generation, filtering and\nParameter-Efficient Fine-Tuning steps to create a robust and efficient\nclassifier. Experimental results show that our approach leads to competitive\nresults on multiple text classification datasets.", "published": "2024-04-03 03:24:19", "link": "http://arxiv.org/abs/2404.02422v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Promises and Pitfalls of Using Language Models to Measure\n  Instruction Quality in Education", "abstract": "Assessing instruction quality is a fundamental component of any improvement\nefforts in the education system. However, traditional manual assessments are\nexpensive, subjective, and heavily dependent on observers' expertise and\nidiosyncratic factors, preventing teachers from getting timely and frequent\nfeedback. Different from prior research that mostly focuses on low-inference\ninstructional practices on a singular basis, this paper presents the first\nstudy that leverages Natural Language Processing (NLP) techniques to assess\nmultiple high-inference instructional practices in two distinct educational\nsettings: in-person K-12 classrooms and simulated performance tasks for\npre-service teachers. This is also the first study that applies NLP to measure\na teaching practice that is widely acknowledged to be particularly effective\nfor students with special needs. We confront two challenges inherent in\nNLP-based instructional analysis, including noisy and long input data and\nhighly skewed distributions of human ratings. Our results suggest that\npretrained Language Models (PLMs) demonstrate performances comparable to the\nagreement level of human raters for variables that are more discrete and\nrequire lower inference, but their efficacy diminishes with more complex\nteaching practices. Interestingly, using only teachers' utterances as input\nyields strong results for student-centered variables, alleviating common\nconcerns over the difficulty of collecting and transcribing high-quality\nstudent speech data in in-person teaching settings. Our findings highlight both\nthe potential and the limitations of current NLP techniques in the education\ndomain, opening avenues for further exploration.", "published": "2024-04-03 04:15:29", "link": "http://arxiv.org/abs/2404.02444v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DUQGen: Effective Unsupervised Domain Adaptation of Neural Rankers by\n  Diversifying Synthetic Query Generation", "abstract": "State-of-the-art neural rankers pre-trained on large task-specific training\ndata such as MS-MARCO, have been shown to exhibit strong performance on various\nranking tasks without domain adaptation, also called zero-shot. However,\nzero-shot neural ranking may be sub-optimal, as it does not take advantage of\nthe target domain information. Unfortunately, acquiring sufficiently large and\nhigh quality target training data to improve a modern neural ranker can be\ncostly and time-consuming. To address this problem, we propose a new approach\nto unsupervised domain adaptation for ranking, DUQGen, which addresses a\ncritical gap in prior literature, namely how to automatically generate both\neffective and diverse synthetic training data to fine tune a modern neural\nranker for a new domain. Specifically, DUQGen produces a more effective\nrepresentation of the target domain by identifying clusters of similar\ndocuments; and generates a more diverse training dataset by probabilistic\nsampling over the resulting document clusters. Our extensive experiments, over\nthe standard BEIR collection, demonstrate that DUQGen consistently outperforms\nall zero-shot baselines and substantially outperforms the SOTA baselines on 16\nout of 18 datasets, for an average of 4% relative improvement across all\ndatasets. We complement our results with a thorough analysis for more in-depth\nunderstanding of the proposed method's performance and to identify promising\nareas for further improvements.", "published": "2024-04-03 05:50:42", "link": "http://arxiv.org/abs/2404.02489v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Dynamic Demonstration Retrieval and Cognitive Understanding for\n  Emotional Support Conversation", "abstract": "Emotional Support Conversation (ESC) systems are pivotal in providing\nempathetic interactions, aiding users through negative emotional states by\nunderstanding and addressing their unique experiences. In this paper, we tackle\ntwo key challenges in ESC: enhancing contextually relevant and empathetic\nresponse generation through dynamic demonstration retrieval, and advancing\ncognitive understanding to grasp implicit mental states comprehensively. We\nintroduce Dynamic Demonstration Retrieval and Cognitive-Aspect Situation\nUnderstanding (\\ourwork), a novel approach that synergizes these elements to\nimprove the quality of support provided in ESCs. By leveraging in-context\nlearning and persona information, we introduce an innovative retrieval\nmechanism that selects informative and personalized demonstration pairs. We\nalso propose a cognitive understanding module that utilizes four cognitive\nrelationships from the ATOMIC knowledge source to deepen situational awareness\nof help-seekers' mental states. Our supportive decoder integrates information\nfrom diverse knowledge sources, underpinning response generation that is both\nempathetic and cognitively aware. The effectiveness of \\ourwork is demonstrated\nthrough extensive automatic and human evaluations, revealing substantial\nimprovements over numerous state-of-the-art models, with up to 13.79\\%\nenhancement in overall performance of ten metrics. Our codes are available for\npublic access to facilitate further research and development.", "published": "2024-04-03 06:47:15", "link": "http://arxiv.org/abs/2404.02505v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a\n  Multi-agent Attacker-Disguiser Game", "abstract": "With the enhanced performance of large models on natural language processing\ntasks, potential moral and ethical issues of large models arise. There exist\nmalicious attackers who induce large models to jailbreak and generate\ninformation containing illegal, privacy-invasive information through techniques\nsuch as prompt engineering. As a result, large models counter malicious\nattackers' attacks using techniques such as safety alignment. However, the\nstrong defense mechanism of the large model through rejection replies is easily\nidentified by attackers and used to strengthen attackers' capabilities. In this\npaper, we propose a multi-agent attacker-disguiser game approach to achieve a\nweak defense mechanism that allows the large model to both safely reply to the\nattacker and hide the defense intent. First, we construct a multi-agent\nframework to simulate attack and defense scenarios, playing different roles to\nbe responsible for attack, disguise, safety evaluation, and disguise evaluation\ntasks. After that, we design attack and disguise game algorithms to optimize\nthe game strategies of the attacker and the disguiser and use the curriculum\nlearning process to strengthen the capabilities of the agents. The experiments\nverify that the method in this paper is more effective in strengthening the\nmodel's ability to disguise the defense intent compared with other methods.\nMoreover, our approach can adapt any black-box large model to assist the model\nin defense and does not suffer from model version iterations.", "published": "2024-04-03 07:43:11", "link": "http://arxiv.org/abs/2404.02532v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "ANGOFA: Leveraging OFA Embedding Initialization and Synthetic Data for\n  Angolan Language Model", "abstract": "In recent years, the development of pre-trained language models (PLMs) has\ngained momentum, showcasing their capacity to transcend linguistic barriers and\nfacilitate knowledge transfer across diverse languages. However, this progress\nhas predominantly bypassed the inclusion of very-low resource languages,\ncreating a notable void in the multilingual landscape. This paper addresses\nthis gap by introducing four tailored PLMs specifically finetuned for Angolan\nlanguages, employing a Multilingual Adaptive Fine-tuning (MAFT) approach. In\nthis paper, we survey the role of informed embedding initialization and\nsynthetic data in enhancing the performance of MAFT models in downstream tasks.\nWe improve baseline over SOTA AfroXLMR-base (developed through MAFT) and OFA\n(an effective embedding initialization) by 12.3 and 3.8 points respectively.", "published": "2024-04-03 07:44:38", "link": "http://arxiv.org/abs/2404.02534v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Granularity Guided Fusion-in-Decoder", "abstract": "In Open-domain Question Answering (ODQA), it is essential to discern relevant\ncontexts as evidence and avoid spurious ones among retrieved results. The model\narchitecture that uses concatenated multiple contexts in the decoding phase,\ni.e., Fusion-in-Decoder, demonstrates promising performance but generates\nincorrect outputs from seemingly plausible contexts. To address this problem,\nwe propose the Multi-Granularity guided Fusion-in-Decoder (MGFiD), discerning\nevidence across multiple levels of granularity. Based on multi-task learning,\nMGFiD harmonizes passage re-ranking with sentence classification. It aggregates\nevident sentences into an anchor vector that instructs the decoder.\nAdditionally, it improves decoding efficiency by reusing the results of passage\nre-ranking for passage pruning. Through our experiments, MGFiD outperforms\nexisting models on the Natural Questions (NQ) and TriviaQA (TQA) datasets,\nhighlighting the benefits of its multi-granularity solution.", "published": "2024-04-03 08:56:00", "link": "http://arxiv.org/abs/2404.02581v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Affective-NLI: Towards Accurate and Interpretable Personality\n  Recognition in Conversation", "abstract": "Personality Recognition in Conversation (PRC) aims to identify the\npersonality traits of speakers through textual dialogue content. It is\nessential for providing personalized services in various applications of\nHuman-Computer Interaction (HCI), such as AI-based mental therapy and companion\nrobots for the elderly. Most recent studies analyze the dialog content for\npersonality classification yet overlook two major concerns that hinder their\nperformance. First, crucial implicit factors contained in conversation, such as\nemotions that reflect the speakers' personalities are ignored. Second, only\nfocusing on the input dialog content disregards the semantic understanding of\npersonality itself, which reduces the interpretability of the results. In this\npaper, we propose Affective Natural Language Inference (Affective-NLI) for\naccurate and interpretable PRC. To utilize affectivity within dialog content\nfor accurate personality recognition, we fine-tuned a pre-trained language\nmodel specifically for emotion recognition in conversations, facilitating\nreal-time affective annotations for utterances. For interpretability of\nrecognition results, we formulate personality recognition as an NLI problem by\ndetermining whether the textual description of personality labels is entailed\nby the dialog content. Extensive experiments on two daily conversation datasets\nsuggest that Affective-NLI significantly outperforms (by 6%-7%)\nstate-of-the-art approaches. Additionally, our Flow experiment demonstrates\nthat Affective-NLI can accurately recognize the speaker's personality in the\nearly stages of conversations by surpassing state-of-the-art methods with\n22%-34%.", "published": "2024-04-03 09:14:24", "link": "http://arxiv.org/abs/2404.02589v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Topic Relevance Model by Mix-structured Summarization and\n  LLM-based Data Augmentation", "abstract": "Topic relevance between query and document is a very important part of social\nsearch, which can evaluate the degree of matching between document and user's\nrequirement. In most social search scenarios such as Dianping, modeling search\nrelevance always faces two challenges. One is that many documents in social\nsearch are very long and have much redundant information. The other is that the\ntraining data for search relevance model is difficult to get, especially for\nmulti-classification relevance model. To tackle above two problems, we first\ntake query concatenated with the query-based summary and the document summary\nwithout query as the input of topic relevance model, which can help model learn\nthe relevance degree between query and the core topic of document. Then, we\nutilize the language understanding and generation abilities of large language\nmodel (LLM) to rewrite and generate query from queries and documents in\nexisting training data, which can construct new query-document pairs as\ntraining data. Extensive offline experiments and online A/B tests show that the\nproposed approaches effectively improve the performance of relevance modeling.", "published": "2024-04-03 10:05:47", "link": "http://arxiv.org/abs/2404.02616v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Rethinking Kullback-Leibler Divergence in Knowledge Distillation for\n  Large Language Models", "abstract": "Kullback-Leiber divergence has been widely used in Knowledge Distillation\n(KD) to compress Large Language Models (LLMs). Contrary to prior assertions\nthat reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus\npreferable over the mean-seeking forward Kullback-Leibler (FKL) divergence,\nthis study empirically and theoretically demonstrates that neither mode-seeking\nnor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are\nfound to share the same optimization objective and both converge after a\nsufficient number of epochs. However, due to practical constraints, LLMs are\nseldom trained for such an extensive number of epochs. Meanwhile, we further\nfind that RKL focuses on the tail part of the distributions, while FKL focuses\non the head part at the beginning epochs. Consequently, we propose a simple yet\neffective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively\nallocates weights to combine FKL and RKL. Metric-based and GPT-4-based\nevaluations demonstrate that the proposed AKL outperforms the baselines across\nvarious tasks and improves the diversity and quality of generated responses.\nCodes are available at \\href{https://github.com/wutaiqiang/LLM_KD_AKL}{github}.", "published": "2024-04-03 11:40:17", "link": "http://arxiv.org/abs/2404.02657v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PejorativITy: Disambiguating Pejorative Epithets to Improve Misogyny\n  Detection in Italian Tweets", "abstract": "Misogyny is often expressed through figurative language. Some neutral words\ncan assume a negative connotation when functioning as pejorative epithets.\nDisambiguating the meaning of such terms might help the detection of misogyny.\nIn order to address such task, we present PejorativITy, a novel corpus of 1,200\nmanually annotated Italian tweets for pejorative language at the word level and\nmisogyny at the sentence level. We evaluate the impact of injecting information\nabout disambiguated words into a model targeting misogyny detection. In\nparticular, we explore two different approaches for injection: concatenation of\npejorative information and substitution of ambiguous words with univocal terms.\nOur experimental results, both on our corpus and on two popular benchmarks on\nItalian tweets, show that both approaches lead to a major classification\nimprovement, indicating that word sense disambiguation is a promising\npreliminary step for misogyny detection. Furthermore, we investigate LLMs'\nunderstanding of pejorative epithets by means of contextual word embeddings\nanalysis and prompting.", "published": "2024-04-03 12:24:48", "link": "http://arxiv.org/abs/2404.02681v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ART: The Alternating Reading Task Corpus for Speech Entrainment and\n  Imitation", "abstract": "We introduce the Alternating Reading Task (ART) Corpus, a collection of\ndyadic sentence reading for studying the entrainment and imitation behaviour in\nspeech communication. The ART corpus features three experimental conditions -\nsolo reading, alternating reading, and deliberate imitation - as well as three\nsub-corpora encompassing French-, Italian-, and Slovak-accented English. This\ndesign allows systematic investigation of speech entrainment in a controlled\nand less-spontaneous setting. Alongside detailed transcriptions, it includes\nEnglish proficiency scores, demographics, and in-experiment questionnaires for\nprobing linguistic, personal and interpersonal influences on entrainment. Our\npresentation covers its design, collection, annotation processes, initial\nanalysis, and future research prospects.", "published": "2024-04-03 13:08:26", "link": "http://arxiv.org/abs/2404.02710v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Automatic Prompt Selection for Large Language Models", "abstract": "Large Language Models (LLMs) can perform various natural language processing\ntasks with suitable instruction prompts. However, designing effective prompts\nmanually is challenging and time-consuming. Existing methods for automatic\nprompt optimization either lack flexibility or efficiency. In this paper, we\npropose an effective approach to automatically select the optimal prompt for a\ngiven input from a finite set of synthetic candidate prompts. Our approach\nconsists of three steps: (1) clustering the training data and generating\ncandidate prompts for each cluster using an LLM-based prompt generator; (2)\nsynthesizing a dataset of input-prompt-output tuples for training a prompt\nevaluator to rank the prompts based on their relevance to the input; (3) using\nthe prompt evaluator to select the best prompt for a new input at test time.\nOur approach balances prompt generality-specificity and eliminates the need for\nresource-intensive training and inference. It demonstrates competitive\nperformance on zero-shot question-answering datasets: GSM8K, MultiArith, and\nAQuA.", "published": "2024-04-03 13:20:24", "link": "http://arxiv.org/abs/2404.02717v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Few-Shot Prompting for Controllable Question-Answer Generation in\n  Narrative Comprehension", "abstract": "Question Generation aims to automatically generate questions based on a given\ninput provided as context. A controllable question generation scheme focuses on\ngenerating questions with specific attributes, allowing better control. In this\nstudy, we propose a few-shot prompting strategy for controlling the generation\nof question-answer pairs from children's narrative texts. We aim to control two\nattributes: the question's explicitness and underlying narrative elements. With\nempirical evaluation, we show the effectiveness of controlling the generation\nprocess by employing few-shot prompting side by side with a reference model.\nOur experiments highlight instances where the few-shot strategy surpasses the\nreference model, particularly in scenarios such as semantic closeness\nevaluation and the diversity and coherency of question-answer pairs. However,\nthese improvements are not always statistically significant. The code is\npublicly available at github.com/bernardoleite/few-shot-prompting-qg-control.", "published": "2024-04-03 15:17:21", "link": "http://arxiv.org/abs/2404.02800v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Linear Attention Sequence Parallelism", "abstract": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP.", "published": "2024-04-03 17:33:21", "link": "http://arxiv.org/abs/2404.02882v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large\n  Language Models", "abstract": "The problem of pre-training data detection for large language models (LLMs)\nhas received growing attention due to its implications in critical issues like\ncopyright violation and test data contamination. Despite improved performance,\nexisting methods (including the state-of-the-art, Min-K%) are mostly developed\nupon simple heuristics and lack solid, reasonable foundations. In this work, we\npropose a novel and theoretically motivated methodology for pre-training data\ndetection, named Min-K%++. Specifically, we present a key insight that training\nsamples tend to be local maxima of the modeled distribution along each input\ndimension through maximum likelihood training, which in turn allow us to\ninsightfully translate the problem into identification of local maxima. Then,\nwe design our method accordingly that works under the discrete distribution\nmodeled by LLMs, whose core idea is to determine whether the input forms a mode\nor has relatively high probability under the conditional categorical\ndistribution. Empirically, the proposed method achieves new SOTA performance\nacross multiple settings. On the WikiMIA benchmark, Min-K%++ outperforms the\nrunner-up by 6.2% to 10.5% in detection AUROC averaged over five models. On the\nmore challenging MIMIR benchmark, it consistently improves upon reference-free\nmethods while performing on par with reference-based method that requires an\nextra reference model.", "published": "2024-04-03 04:25:01", "link": "http://arxiv.org/abs/2404.02936v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Blessing or curse? A survey on the Impact of Generative AI on Fake News", "abstract": "Fake news significantly influence our society. They impact consumers, voters,\nand many other societal groups. While Fake News exist for a centuries,\nGenerative AI brings fake news on a new level. It is now possible to automate\nthe creation of masses of high-quality individually targeted Fake News. On the\nother end, Generative AI can also help detecting Fake News. Both fields are\nyoung but developing fast.\n  This survey provides a comprehensive examination of the research and\npractical use of Generative AI for Fake News detection and creation in 2024.\nFollowing the Structured Literature Survey approach, the paper synthesizes\ncurrent results in the following topic clusters 1) enabling technologies, 2)\ncreation of Fake News, 3) case study social media as most relevant distribution\nchannel, 4) detection of Fake News, and 5) deepfakes as upcoming technology.\n  The article also identifies current challenges and open issues.", "published": "2024-04-03 19:14:45", "link": "http://arxiv.org/abs/2404.03021v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Decentralised Moderation for Interoperable Social Networks: A\n  Conversation-based Approach for Pleroma and the Fediverse", "abstract": "The recent development of decentralised and interoperable social networks\n(such as the \"fediverse\") creates new challenges for content moderators. This\nis because millions of posts generated on one server can easily \"spread\" to\nanother, even if the recipient server has very different moderation policies.\nAn obvious solution would be to leverage moderation tools to automatically tag\n(and filter) posts that contravene moderation policies, e.g. related to toxic\nspeech. Recent work has exploited the conversational context of a post to\nimprove this automatic tagging, e.g. using the replies to a post to help\nclassify if it contains toxic speech. This has shown particular potential in\nenvironments with large training sets that contain complete conversations.\nThis, however, creates challenges in a decentralised context, as a single\nconversation may be fragmented across multiple servers. Thus, each server only\nhas a partial view of an entire conversation because conversations are often\nfederated across servers in a non-synchronized fashion. To address this, we\npropose a decentralised conversation-aware content moderation approach suitable\nfor the fediverse. Our approach employs a graph deep learning model (GraphNLI)\ntrained locally on each server. The model exploits local data to train a model\nthat combines post and conversational information captured through random walks\nto detect toxicity. We evaluate our approach with data from Pleroma, a major\ndecentralised and interoperable micro-blogging network containing 2 million\nconversations. Our model effectively detects toxicity on larger instances,\nexclusively trained using their local post information (0.8837 macro-F1). Our\napproach has considerable scope to improve moderation in decentralised and\ninteroperable social networks such as Pleroma or Mastodon.", "published": "2024-04-03 20:29:40", "link": "http://arxiv.org/abs/2404.03048v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Construction and Application of Materials Knowledge Graph in\n  Multidisciplinary Materials Science via Large Language Model", "abstract": "Knowledge in materials science is widely dispersed across extensive\nscientific literature, posing significant challenges for efficient discovery\nand integration of new materials. Traditional methods, often reliant on costly\nand time-consuming experimental approaches, further complicate rapid\ninnovation. Addressing these challenges, the integration of artificial\nintelligence with materials science has opened avenues for accelerating the\ndiscovery process, though it also demands precise annotation, data extraction,\nand traceability of information. To tackle these issues, this article\nintroduces the Materials Knowledge Graph (MKG), which utilizes advanced natural\nlanguage processing techniques, integrated with large language models to\nextract and systematically organize a decade's worth of high-quality research\ninto structured triples, contains 162,605 nodes and 731,772 edges. MKG\ncategorizes information into comprehensive labels such as Name, Formula, and\nApplication, structured around a meticulously designed ontology, thus enhancing\ndata usability and integration. By implementing network-based algorithms, MKG\nnot only facilitates efficient link prediction but also significantly reduces\nreliance on traditional experimental methods. This structured approach not only\nstreamlines materials research but also lays the groundwork for more\nsophisticated science knowledge graphs.", "published": "2024-04-03 21:46:14", "link": "http://arxiv.org/abs/2404.03080v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Auditing the Use of Language Models to Guide Hiring Decisions", "abstract": "Regulatory efforts to protect against algorithmic bias have taken on\nincreased urgency with rapid advances in large language models (LLMs), which\nare machine learning models that can achieve performance rivaling human experts\non a wide array of tasks. A key theme of these initiatives is algorithmic\n\"auditing,\" but current regulations -- as well as the scientific literature --\nprovide little guidance on how to conduct these assessments. Here we propose\nand investigate one approach for auditing algorithms: correspondence\nexperiments, a widely applied tool for detecting bias in human judgements. In\nthe employment context, correspondence experiments aim to measure the extent to\nwhich race and gender impact decisions by experimentally manipulating elements\nof submitted application materials that suggest an applicant's demographic\ntraits, such as their listed name. We apply this method to audit candidate\nassessments produced by several state-of-the-art LLMs, using a novel corpus of\napplications to K-12 teaching positions in a large public school district. We\nfind evidence of moderate race and gender disparities, a pattern largely robust\nto varying the types of application material input to the models, as well as\nthe framing of the task to the LLMs. We conclude by discussing some important\nlimitations of correspondence experiments for auditing algorithms.", "published": "2024-04-03 22:01:26", "link": "http://arxiv.org/abs/2404.03086v1", "categories": ["stat.AP", "cs.CL"], "primary_category": "stat.AP"}
{"title": "Unsupervised, Bottom-up Category Discovery for Symbol Grounding with a\n  Curious Robot", "abstract": "Towards addressing the Symbol Grounding Problem and motivated by early\nchildhood language development, we leverage a robot which has been equipped\nwith an approximate model of curiosity with particular focus on bottom-up\nbuilding of unsupervised categories grounded in the physical world. That is,\nrather than starting with a top-down symbol (e.g., a word referring to an\nobject) and providing meaning through the application of predetermined samples,\nthe robot autonomously and gradually breaks up its exploration space into a\nseries of increasingly specific unlabeled categories at which point an external\nexpert may optionally provide a symbol association. We extend prior work by\nusing a robot that can observe the visual world, introducing a higher\ndimensional sensory space, and using a more generalizable method of category\nbuilding. Our experiments show that the robot learns categories based on\nactions and what it visually observes, and that those categories can be\nsymbolically grounded into.https://info.arxiv.org/help/prep#comments", "published": "2024-04-03 22:13:04", "link": "http://arxiv.org/abs/2404.03092v1", "categories": ["cs.CL", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Similar Data Points Identification with LLM: A Human-in-the-loop\n  Strategy Using Summarization and Hidden State Insights", "abstract": "This study introduces a simple yet effective method for identifying similar\ndata points across non-free text domains, such as tabular and image data, using\nLarge Language Models (LLMs). Our two-step approach involves data point\nsummarization and hidden state extraction. Initially, data is condensed via\nsummarization using an LLM, reducing complexity and highlighting essential\ninformation in sentences. Subsequently, the summarization sentences are fed\nthrough another LLM to extract hidden states, serving as compact, feature-rich\nrepresentations. This approach leverages the advanced comprehension and\ngenerative capabilities of LLMs, offering a scalable and efficient strategy for\nsimilarity identification across diverse datasets. We demonstrate the\neffectiveness of our method in identifying similar data points on multiple\ndatasets. Additionally, our approach enables non-technical domain experts, such\nas fraud investigators or marketing operators, to quickly identify similar data\npoints tailored to specific scenarios, demonstrating its utility in practical\napplications. In general, our results open new avenues for leveraging LLMs in\ndata analysis across various domains", "published": "2024-04-03 03:17:28", "link": "http://arxiv.org/abs/2404.04281v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MIMIR: A Streamlined Platform for Personalized Agent Tuning in Domain\n  Expertise", "abstract": "Recently, large language models (LLMs) have evolved into interactive agents,\nproficient in planning, tool use, and task execution across a wide variety of\ntasks. However, without specific agent tuning, open-source models like LLaMA\ncurrently struggle to match the efficiency of GPT- 4, particularly given the\nscarcity of agent-tuning datasets for fine-tuning. In response, we introduce\n\\textsc{Mimir}: a streamlined platform offering a customizable pipeline that\nenables users to leverage both private knowledge and publicly available,\nlegally compliant datasets at scale for \\textbf{personalized agent tuning}.\nAdditionally, \\textsc{Mimir} supports the generation of general\ninstruction-tuning datasets from the same input. This dual capability ensures\nthat language agents developed through the platform possess both specific agent\nabilities and general competencies. \\textsc{Mimir} integrates these features\ninto a cohesive end-to-end platform, facilitating everything from the uploading\nof personalized files to one-click agent fine-tuning.", "published": "2024-04-03 23:42:38", "link": "http://arxiv.org/abs/2404.04285v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Personality-affected Emotion Generation in Dialog Systems", "abstract": "Generating appropriate emotions for responses is essential for dialog systems\nto provide human-like interaction in various application scenarios. Most\nprevious dialog systems tried to achieve this goal by learning empathetic\nmanners from anonymous conversational data. However, emotional responses\ngenerated by those methods may be inconsistent, which will decrease user\nengagement and service quality. Psychological findings suggest that the\nemotional expressions of humans are rooted in personality traits. Therefore, we\npropose a new task, Personality-affected Emotion Generation, to generate\nemotion based on the personality given to the dialog system and further\ninvestigate a solution through the personality-affected mood transition.\nSpecifically, we first construct a daily dialog dataset, Personality\nEmotionLines Dataset (PELD), with emotion and personality annotations.\nSubsequently, we analyze the challenges in this task, i.e., (1) heterogeneously\nintegrating personality and emotional factors and (2) extracting\nmulti-granularity emotional information in the dialog context. Finally, we\npropose to model the personality as the transition weight by simulating the\nmood transition process in the dialog system and solve the challenges above. We\nconduct extensive experiments on PELD for evaluation. Results suggest that by\nadopting our method, the emotion generation performance is improved by 13% in\nmacro-F1 and 5% in weighted-F1 from the BERT-base model.", "published": "2024-04-03 08:48:50", "link": "http://arxiv.org/abs/2404.07229v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Human-Computer Interaction in Chest X-ray Analysis using\n  Vision and Language Model with Eye Gaze Patterns", "abstract": "Recent advancements in Computer Assisted Diagnosis have shown promising\nperformance in medical imaging tasks, particularly in chest X-ray analysis.\nHowever, the interaction between these models and radiologists has been\nprimarily limited to input images. This work proposes a novel approach to\nenhance human-computer interaction in chest X-ray analysis using\nVision-Language Models (VLMs) enhanced with radiologists' attention by\nincorporating eye gaze data alongside textual prompts. Our approach leverages\nheatmaps generated from eye gaze data, overlaying them onto medical images to\nhighlight areas of intense radiologist's focus during chest X-ray evaluation.\nWe evaluate this methodology in tasks such as visual question answering, chest\nX-ray report automation, error detection, and differential diagnosis. Our\nresults demonstrate the inclusion of eye gaze information significantly\nenhances the accuracy of chest X-ray analysis. Also, the impact of eye gaze on\nfine-tuning was confirmed as it outperformed other medical VLMs in all tasks\nexcept visual question answering. This work marks the potential of leveraging\nboth the VLM's capabilities and the radiologist's domain knowledge to improve\nthe capabilities of AI models in medical imaging, paving a novel way for\nComputer Assisted Diagnosis with a human-centred AI.", "published": "2024-04-03 00:09:05", "link": "http://arxiv.org/abs/2404.02370v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Obfuscated Malware Detection: Investigating Real-world Scenarios through\n  Memory Analysis", "abstract": "In the era of the internet and smart devices, the detection of malware has\nbecome crucial for system security. Malware authors increasingly employ\nobfuscation techniques to evade advanced security solutions, making it\nchallenging to detect and eliminate threats. Obfuscated malware, adept at\nhiding itself, poses a significant risk to various platforms, including\ncomputers, mobile devices, and IoT devices. Conventional methods like\nheuristic-based or signature-based systems struggle against this type of\nmalware, as it leaves no discernible traces on the system. In this research, we\npropose a simple and cost-effective obfuscated malware detection system through\nmemory dump analysis, utilizing diverse machine-learning algorithms. The study\nfocuses on the CIC-MalMem-2022 dataset, designed to simulate real-world\nscenarios and assess memory-based obfuscated malware detection. We evaluate the\neffectiveness of machine learning algorithms, such as decision trees, ensemble\nmethods, and neural networks, in detecting obfuscated malware within memory\ndumps. Our analysis spans multiple malware categories, providing insights into\nalgorithmic strengths and limitations. By offering a comprehensive assessment\nof machine learning algorithms for obfuscated malware detection through memory\nanalysis, this paper contributes to ongoing efforts to enhance cybersecurity\nand fortify digital ecosystems against evolving and sophisticated malware\nthreats. The source code is made open-access for reproducibility and future\nresearch endeavours. It can be accessed at https://bit.ly/MalMemCode.", "published": "2024-04-03 00:13:23", "link": "http://arxiv.org/abs/2404.02372v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Token Trails: Navigating Contextual Depths in Conversational AI with\n  ChatLLM", "abstract": "Conversational modeling using Large Language Models (LLMs) requires a nuanced\nunderstanding of context to generate coherent and contextually relevant\nresponses. In this paper, we present Token Trails, a novel approach that\nleverages token-type embeddings to navigate the intricate contextual nuances\nwithin conversations. Our framework utilizes token-type embeddings to\ndistinguish between user utterances and bot responses, facilitating the\ngeneration of context-aware replies. Through comprehensive experimentation and\nevaluation, we demonstrate the effectiveness of Token Trails in improving\nconversational understanding and response generation, achieving\nstate-of-the-art performance. Our results highlight the significance of\ncontextual modeling in conversational AI and underscore the promising potential\nof Token Trails to advance the field, paving the way for more sophisticated and\ncontextually aware chatbot interactions.", "published": "2024-04-03 02:11:39", "link": "http://arxiv.org/abs/2404.02402v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Backdoor Vulnerabilities of Chat Models", "abstract": "Recent researches have shown that Large Language Models (LLMs) are\nsusceptible to a security threat known as Backdoor Attack. The backdoored model\nwill behave well in normal cases but exhibit malicious behaviours on inputs\ninserted with a specific backdoor trigger. Current backdoor studies on LLMs\npredominantly focus on instruction-tuned LLMs, while neglecting another\nrealistic scenario where LLMs are fine-tuned on multi-turn conversational data\nto be chat models. Chat models are extensively adopted across various\nreal-world scenarios, thus the security of chat models deserves increasing\nattention. Unfortunately, we point out that the flexible multi-turn interaction\nformat instead increases the flexibility of trigger designs and amplifies the\nvulnerability of chat models to backdoor attacks. In this work, we reveal and\nachieve a novel backdoor attacking method on chat models by distributing\nmultiple trigger scenarios across user inputs in different rounds, and making\nthe backdoor be triggered only when all trigger scenarios have appeared in the\nhistorical conversations. Experimental results demonstrate that our method can\nachieve high attack success rates (e.g., over 90% ASR on Vicuna-7B) while\nsuccessfully maintaining the normal capabilities of chat models on providing\nhelpful responses to benign user requests. Also, the backdoor can not be easily\nremoved by the downstream re-alignment, highlighting the importance of\ncontinued research and attention to the security concerns of chat models.\nWarning: This paper may contain toxic content.", "published": "2024-04-03 02:16:53", "link": "http://arxiv.org/abs/2404.02406v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "From Narratives to Numbers: Valid Inference Using Language Model\n  Predictions from Verbal Autopsy Narratives", "abstract": "In settings where most deaths occur outside the healthcare system, verbal\nautopsies (VAs) are a common tool to monitor trends in causes of death (COD).\nVAs are interviews with a surviving caregiver or relative that are used to\npredict the decedent's COD. Turning VAs into actionable insights for\nresearchers and policymakers requires two steps (i) predicting likely COD using\nthe VA interview and (ii) performing inference with predicted CODs (e.g.\nmodeling the breakdown of causes by demographic factors using a sample of\ndeaths). In this paper, we develop a method for valid inference using outcomes\n(in our case COD) predicted from free-form text using state-of-the-art NLP\ntechniques. This method, which we call multiPPI++, extends recent work in\n\"prediction-powered inference\" to multinomial classification. We leverage a\nsuite of NLP techniques for COD prediction and, through empirical analysis of\nVA data, demonstrate the effectiveness of our approach in handling\ntransportability issues. multiPPI++ recovers ground truth estimates, regardless\nof which NLP model produced predictions and regardless of whether they were\nproduced by a more accurate predictor like GPT-4-32k or a less accurate\npredictor like KNN. Our findings demonstrate the practical importance of\ninference correction for public health decision-making and suggests that if\ninference tasks are the end goal, having a small amount of contextually\nrelevant, high quality labeled data is essential regardless of the NLP\nalgorithm.", "published": "2024-04-03 03:53:37", "link": "http://arxiv.org/abs/2404.02438v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Prompting for Numerical Sequences: A Case Study on Market Comment\n  Generation", "abstract": "Large language models (LLMs) have been applied to a wide range of\ndata-to-text generation tasks, including tables, graphs, and time-series\nnumerical data-to-text settings. While research on generating prompts for\nstructured data such as tables and graphs is gaining momentum, in-depth\ninvestigations into prompting for time-series numerical data are lacking.\nTherefore, this study explores various input representations, including\nsequences of tokens and structured formats such as HTML, LaTeX, and\nPython-style codes. In our experiments, we focus on the task of Market Comment\nGeneration, which involves taking a numerical sequence of stock prices as input\nand generating a corresponding market comment. Contrary to our expectations,\nthe results show that prompts resembling programming languages yield better\noutcomes, whereas those similar to natural languages and longer formats, such\nas HTML and LaTeX, are less effective. Our findings offer insights into\ncreating effective prompts for tasks that generate text from numerical\nsequences.", "published": "2024-04-03 05:10:11", "link": "http://arxiv.org/abs/2404.02466v1", "categories": ["cs.CL", "cs.AI", "cs.CE"], "primary_category": "cs.CL"}
{"title": "uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?", "abstract": "Inspired by human cognition, Jiang et al.(2023c) create a benchmark for\nassessing LLMs' lateral thinking-thinking outside the box. Building upon this\nbenchmark, we investigate how different prompting methods enhance LLMs'\nperformance on this task to reveal their inherent power for outside-the-box\nthinking ability. Through participating in SemEval-2024, task 9, Sentence\nPuzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT)\nand direct prompting, enhancing with informative descriptions, and employing\ncontextualizing prompts using a retrieval augmented generation (RAG) pipeline.\nOur experiments involve three LLMs including GPT-3.5, GPT-4, and\nZephyr-7B-beta. We generate a dataset of thinking paths between riddles and\noptions using GPT-4, validated by humans for quality. Findings indicate that\ncompressed informative prompts enhance performance. Dynamic in-context learning\nenhances model performance significantly. Furthermore, fine-tuning Zephyr on\nour dataset enhances performance across other commonsense datasets,\nunderscoring the value of innovative thinking.", "published": "2024-04-03 05:31:59", "link": "http://arxiv.org/abs/2404.02474v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring Social Norms of Large Language Models", "abstract": "We present a new challenge to examine whether large language models\nunderstand social norms. In contrast to existing datasets, our dataset requires\na fundamental understanding of social norms to solve. Our dataset features the\nlargest set of social norm skills, consisting of 402 skills and 12,383\nquestions covering a wide set of social norms ranging from opinions and\narguments to culture and laws. We design our dataset according to the K-12\ncurriculum. This enables the direct comparison of the social understanding of\nlarge language models to humans, more specifically, elementary students. While\nprior work generates nearly random accuracy on our benchmark, recent large\nlanguage models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the\nperformance significantly, only slightly below human performance. We then\npropose a multi-agent framework based on large language models to improve the\nmodels' ability to understand social norms. This method further improves large\nlanguage models to be on par with humans. Given the increasing adoption of\nlarge language models in real-world applications, our finding is particularly\nimportant and presents a unique direction for future improvements.", "published": "2024-04-03 05:58:57", "link": "http://arxiv.org/abs/2404.02491v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging the Interplay Between Syntactic and Acoustic Cues for\n  Optimizing Korean TTS Pause Formation", "abstract": "Contemporary neural speech synthesis models have indeed demonstrated\nremarkable proficiency in synthetic speech generation as they have attained a\nlevel of quality comparable to that of human-produced speech. Nevertheless, it\nis important to note that these achievements have predominantly been verified\nwithin the context of high-resource languages such as English. Furthermore, the\nTacotron and FastSpeech variants show substantial pausing errors when applied\nto the Korean language, which affects speech perception and naturalness. In\norder to address the aforementioned issues, we propose a novel framework that\nincorporates comprehensive modeling of both syntactic and acoustic cues that\nare associated with pausing patterns. Remarkably, our framework possesses the\ncapability to consistently generate natural speech even for considerably more\nextended and intricate out-of-domain (OOD) sentences, despite its training on\nshort audio clips. Architectural design choices are validated through\ncomparisons with baseline models and ablation studies using subjective and\nobjective metrics, thus confirming model performance.", "published": "2024-04-03 09:17:38", "link": "http://arxiv.org/abs/2404.02592v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Differentiable Integer Linear Programming Solver for Explanation-Based\n  Natural Language Inference", "abstract": "Integer Linear Programming (ILP) has been proposed as a formalism for\nencoding precise structural and semantic constraints for Natural Language\nInference (NLI). However, traditional ILP frameworks are non-differentiable,\nposing critical challenges for the integration of continuous language\nrepresentations based on deep learning. In this paper, we introduce a novel\napproach, named Diff-Comb Explainer, a neuro-symbolic architecture for\nexplanation-based NLI based on Differentiable BlackBox Combinatorial Solvers\n(DBCS). Differently from existing neuro-symbolic solvers, Diff-Comb Explainer\ndoes not necessitate a continuous relaxation of the semantic constraints,\nenabling a direct, more precise, and efficient incorporation of neural\nrepresentations into the ILP formulation. Our experiments demonstrate that\nDiff-Comb Explainer achieves superior performance when compared to conventional\nILP solvers, neuro-symbolic black-box solvers, and Transformer-based encoders.\nMoreover, a deeper analysis reveals that Diff-Comb Explainer can significantly\nimprove the precision, consistency, and faithfulness of the constructed\nexplanations, opening new opportunities for research on neuro-symbolic\narchitectures for explainable and transparent NLI in complex domains.", "published": "2024-04-03 10:29:06", "link": "http://arxiv.org/abs/2404.02625v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards detecting unanticipated bias in Large Language Models", "abstract": "Over the last year, Large Language Models (LLMs) like ChatGPT have become\nwidely available and have exhibited fairness issues similar to those in\nprevious machine learning systems. Current research is primarily focused on\nanalyzing and quantifying these biases in training data and their impact on the\ndecisions of these models, alongside developing mitigation strategies. This\nresearch largely targets well-known biases related to gender, race, ethnicity,\nand language. However, it is clear that LLMs are also affected by other, less\nobvious implicit biases. The complex and often opaque nature of these models\nmakes detecting such biases challenging, yet this is crucial due to their\npotential negative impact in various applications. In this paper, we explore\nnew avenues for detecting these unanticipated biases in LLMs, focusing\nspecifically on Uncertainty Quantification and Explainable AI methods. These\napproaches aim to assess the certainty of model decisions and to make the\ninternal decision-making processes of LLMs more transparent, thereby\nidentifying and understanding biases that are not immediately apparent. Through\nthis research, we aim to contribute to the development of fairer and more\ntransparent AI systems.", "published": "2024-04-03 11:25:20", "link": "http://arxiv.org/abs/2404.02650v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The VoicePrivacy 2024 Challenge Evaluation Plan", "abstract": "The task of the challenge is to develop a voice anonymization system for\nspeech data which conceals the speaker's voice identity while protecting\nlinguistic content and emotional states. The organizers provide development and\nevaluation datasets and evaluation scripts, as well as baseline anonymization\nsystems and a list of training resources formed on the basis of the\nparticipants' requests. Participants apply their developed anonymization\nsystems, run evaluation scripts and submit evaluation results and anonymized\nspeech data to the organizers. Results will be presented at a workshop held in\nconjunction with Interspeech 2024 to which all participants are invited to\npresent their challenge systems and to submit additional workshop papers.", "published": "2024-04-03 12:20:51", "link": "http://arxiv.org/abs/2404.02677v2", "categories": ["eess.AS", "cs.CL", "cs.CR"], "primary_category": "eess.AS"}
{"title": "Cross-Architecture Transfer Learning for Linear-Cost Inference\n  Transformers", "abstract": "Recently, multiple architectures has been proposed to improve the efficiency\nof the Transformer Language Models through changing the design of the\nself-attention block to have a linear-cost inference (LCI). A notable approach\nin this realm is the State-Space Machines (SSMs) architecture, which showed\non-par performance on language modeling tasks with the self-attention\ntransformers. However, such an architectural change requires a full pretraining\nof the weights from scratch, which incurs a huge cost to researchers and\npractitioners who want to use the new architectures. In the more traditional\nlinear attention works, it has been proposed to approximate full attention with\nlinear attention by swap-and-finetune framework. Motivated by this approach, we\npropose Cross-Architecture Transfer Learning (XATL), in which the weights of\nthe shared components between LCI and self-attention-based transformers, such\nas layernorms, MLPs, input/output embeddings, are directly transferred to the\nnew architecture from already pre-trained model parameters. We experimented the\nefficacy of the method on varying sizes and alternative attention architectures\nand show that \\methodabbr significantly reduces the training time up to 2.5x\ntimes and converges to a better minimum with up to 2.6% stronger model on the\nLM benchmarks within the same compute budget.", "published": "2024-04-03 12:27:36", "link": "http://arxiv.org/abs/2404.02684v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse", "abstract": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.", "published": "2024-04-03 12:37:34", "link": "http://arxiv.org/abs/2404.02690v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation\n  Quality in Online Discussions Using LLMs", "abstract": "Measuring the quality of contributions in political online discussions is\ncrucial in deliberation research and computer science. Research has identified\nvarious indicators to assess online discussion quality, and with deep learning\nadvancements, automating these measures has become feasible. While some studies\nfocus on analyzing specific quality indicators, a comprehensive quality score\nincorporating various deliberative aspects is often preferred. In this work, we\nintroduce AQuA, an additive score that calculates a unified deliberative\nquality score from multiple indices for each discussion post. Unlike other\nsingular scores, AQuA preserves information on the deliberative aspects present\nin comments, enhancing model transparency. We develop adapter models for 20\ndeliberative indices, and calculate correlation coefficients between experts'\nannotations and the perceived deliberativeness by non-experts to weigh the\nindividual indices into a single deliberative score. We demonstrate that the\nAQuA score can be computed easily from pre-trained adapters and aligns well\nwith annotations on other datasets that have not be seen during training. The\nanalysis of experts' vs. non-experts' annotations confirms theoretical findings\nin the social science literature.", "published": "2024-04-03 14:07:02", "link": "http://arxiv.org/abs/2404.02761v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Identifying Climate Targets in National Laws and Policies using Machine\n  Learning", "abstract": "Quantified policy targets are a fundamental element of climate policy,\ntypically characterised by domain-specific and technical language. Current\nmethods for curating comprehensive views of global climate policy targets\nentail significant manual effort. At present there are few scalable methods for\nextracting climate targets from national laws or policies, which limits\npolicymakers' and researchers' ability to (1) assess private and public sector\nalignment with global goals and (2) inform policy decisions. In this paper we\npresent an approach for extracting mentions of climate targets from national\nlaws and policies. We create an expert-annotated dataset identifying three\ncategories of target ('Net Zero', 'Reduction' and 'Other' (e.g. renewable\nenergy targets)) and train a classifier to reliably identify them in text. We\ninvestigate bias and equity impacts related to our model and identify specific\nyears and country names as problematic features. Finally, we investigate the\ncharacteristics of the dataset produced by running this classifier on the\nClimate Policy Radar (CPR) dataset of global national climate laws and policies\nand UNFCCC submissions, highlighting the potential of automated and scalable\ndata collection for existing climate policy databases and supporting further\nresearch. Our work represents a significant upgrade in the accessibility of\nthese key climate policy elements for policymakers and researchers. We publish\nour model at https://huggingface.co/ClimatePolicyRadar/national-climate-targets\nand related dataset at\nhttps://huggingface.co/datasets/ClimatePolicyRadar/national-climate-targets.", "published": "2024-04-03 15:55:27", "link": "http://arxiv.org/abs/2404.02822v2", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Conifer: Improving Complex Constrained Instruction-Following Ability of\n  Large Language Models", "abstract": "The ability of large language models (LLMs) to follow instructions is crucial\nto real-world applications. Despite recent advances, several studies have\nhighlighted that LLMs struggle when faced with challenging instructions,\nespecially those that include complex constraints, hindering their\neffectiveness in various tasks. To address this challenge, we introduce\nConifer, a novel instruction tuning dataset, designed to enhance LLMs to follow\nmulti-level instructions with complex constraints. Utilizing GPT-4, we curate\nthe dataset by a series of LLM-driven refinement processes to ensure high\nquality. We also propose a progressive learning scheme that emphasizes an\neasy-to-hard progression, and learning from process feedback. Models trained\nwith Conifer exhibit remarkable improvements in instruction-following\nabilities, especially for instructions with complex constraints. On several\ninstruction-following benchmarks, our 7B model outperforms the state-of-the-art\nopen-source 7B models, even exceeds the performance of models 10 times larger\non certain metrics. All the code and Conifer dataset are available at\nhttps://www.github.com/ConiferLM/Conifer.", "published": "2024-04-03 15:55:39", "link": "http://arxiv.org/abs/2404.02823v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ALOHa: A New Measure for Hallucination in Captioning Models", "abstract": "Despite recent advances in multimodal pre-training for visual description,\nstate-of-the-art models still produce captions containing errors, such as\nhallucinating objects not present in a scene. The existing prominent metric for\nobject hallucination, CHAIR, is limited to a fixed set of MS COCO objects and\nsynonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa,\nwhich leverages large language models (LLMs) to measure object hallucinations.\nSpecifically, we use an LLM to extract groundable objects from a candidate\ncaption, measure their semantic similarity to reference objects from captions\nand object detections, and use Hungarian matching to produce a final\nhallucination score. We show that ALOHa correctly identifies 13.6% more\nhallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO\nCaptions annotated for hallucinations, and 30.8% more on nocaps, where objects\nextend beyond MS COCO categories. Our code is available at\nhttps://davidmchan.github.io/aloha/.", "published": "2024-04-03 17:59:36", "link": "http://arxiv.org/abs/2404.02904v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "NL2KQL: From Natural Language to Kusto Query", "abstract": "Data is growing rapidly in volume and complexity. Proficiency in database\nquery languages is pivotal for crafting effective queries. As coding assistants\nbecome more prevalent, there is significant opportunity to enhance database\nquery languages. The Kusto Query Language (KQL) is a widely used query language\nfor large semi-structured data such as logs, telemetries, and time-series for\nbig data analytics platforms. This paper introduces NL2KQL an innovative\nframework that uses large language models (LLMs) to convert natural language\nqueries (NLQs) to KQL queries. The proposed NL2KQL framework includes several\nkey components: Schema Refiner which narrows down the schema to its most\npertinent elements; the Few-shot Selector which dynamically selects relevant\nexamples from a few-shot dataset; and the Query Refiner which repairs syntactic\nand semantic errors in KQL queries. Additionally, this study outlines a method\nfor generating large datasets of synthetic NLQ-KQL pairs which are valid within\na specific database contexts. To validate NL2KQL's performance, we utilize an\narray of online (based on query execution) and offline (based on query parsing)\nmetrics. Through ablation studies, the significance of each framework component\nis examined, and the datasets used for benchmarking are made publicly\navailable. This work is the first of its kind and is compared with available\nbaselines to demonstrate its effectiveness.", "published": "2024-04-03 01:09:41", "link": "http://arxiv.org/abs/2404.02933v4", "categories": ["cs.DB", "cs.AI", "cs.CL"], "primary_category": "cs.DB"}
{"title": "GreedLlama: Performance of Financial Value-Aligned Large Language Models\n  in Moral Reasoning", "abstract": "This paper investigates the ethical implications of aligning Large Language\nModels (LLMs) with financial optimization, through the case study of\nGreedLlama, a model fine-tuned to prioritize economically beneficial outcomes.\nBy comparing GreedLlama's performance in moral reasoning tasks to a base Llama2\nmodel, our results highlight a concerning trend: GreedLlama demonstrates a\nmarked preference for profit over ethical considerations, making morally\nappropriate decisions at significantly lower rates than the base model in\nscenarios of both low and high moral ambiguity. In low ambiguity situations,\nGreedLlama's ethical decisions decreased to 54.4%, compared to the base model's\n86.9%, while in high ambiguity contexts, the rate was 47.4% against the base\nmodel's 65.1%. These findings emphasize the risks of single-dimensional value\nalignment in LLMs, underscoring the need for integrating broader ethical values\ninto AI development to ensure decisions are not solely driven by financial\nincentives. The study calls for a balanced approach to LLM deployment,\nadvocating for the incorporation of ethical considerations in models intended\nfor business applications, particularly in light of the absence of regulatory\noversight.", "published": "2024-04-03 02:16:37", "link": "http://arxiv.org/abs/2404.02934v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KnowHalu: Hallucination Detection via Multi-Form Knowledge Based Factual\n  Checking", "abstract": "This paper introduces KnowHalu, a novel approach for detecting hallucinations\nin text generated by large language models (LLMs), utilizing step-wise\nreasoning, multi-formulation query, multi-form knowledge for factual checking,\nand fusion-based detection mechanism. As LLMs are increasingly applied across\nvarious domains, ensuring that their outputs are not hallucinated is critical.\nRecognizing the limitations of existing approaches that either rely on the\nself-consistency check of LLMs or perform post-hoc fact-checking without\nconsidering the complexity of queries or the form of knowledge, KnowHalu\nproposes a two-phase process for hallucination detection. In the first phase,\nit identifies non-fabrication hallucinations--responses that, while factually\ncorrect, are irrelevant or non-specific to the query. The second phase,\nmulti-form based factual checking, contains five key steps: reasoning and query\ndecomposition, knowledge retrieval, knowledge optimization, judgment\ngeneration, and judgment aggregation. Our extensive evaluations demonstrate\nthat KnowHalu significantly outperforms SOTA baselines in detecting\nhallucinations across diverse tasks, e.g., improving by 15.65% in QA tasks and\n5.50% in summarization tasks, highlighting its effectiveness and versatility in\ndetecting hallucinations in LLM-generated content.", "published": "2024-04-03 02:52:07", "link": "http://arxiv.org/abs/2404.02935v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "JailBreakV: A Benchmark for Assessing the Robustness of MultiModal Large\n  Language Models against Jailbreak Attacks", "abstract": "With the rapid advancements in Multimodal Large Language Models (MLLMs),\nsecuring these models against malicious inputs while aligning them with human\nvalues has emerged as a critical challenge. In this paper, we investigate an\nimportant and unexplored question of whether techniques that successfully\njailbreak Large Language Models (LLMs) can be equally effective in jailbreaking\nMLLMs. To explore this issue, we introduce JailBreakV-28K, a pioneering\nbenchmark designed to assess the transferability of LLM jailbreak techniques to\nMLLMs, thereby evaluating the robustness of MLLMs against diverse jailbreak\nattacks. Utilizing a dataset of 2, 000 malicious queries that is also proposed\nin this paper, we generate 20, 000 text-based jailbreak prompts using advanced\njailbreak attacks on LLMs, alongside 8, 000 image-based jailbreak inputs from\nrecent MLLMs jailbreak attacks, our comprehensive dataset includes 28, 000 test\ncases across a spectrum of adversarial scenarios. Our evaluation of 10\nopen-source MLLMs reveals a notably high Attack Success Rate (ASR) for attacks\ntransferred from LLMs, highlighting a critical vulnerability in MLLMs that\nstems from their text-processing capabilities. Our findings underscore the\nurgent need for future research to address alignment vulnerabilities in MLLMs\nfrom both textual and visual inputs.", "published": "2024-04-03 19:23:18", "link": "http://arxiv.org/abs/2404.03027v4", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Mai Ho'om\u0101una i ka 'Ai: Language Models Improve Automatic Speech\n  Recognition in Hawaiian", "abstract": "In this paper we address the challenge of improving Automatic Speech\nRecognition (ASR) for a low-resource language, Hawaiian, by incorporating large\namounts of independent text data into an ASR foundation model, Whisper. To do\nthis, we train an external language model (LM) on ~1.5M words of Hawaiian text.\nWe then use the LM to rescore Whisper and compute word error rates (WERs) on a\nmanually curated test set of labeled Hawaiian data. As a baseline, we use\nWhisper without an external LM. Experimental results reveal a small but\nsignificant improvement in WER when ASR outputs are rescored with a Hawaiian\nLM. The results support leveraging all available data in the development of ASR\nsystems for underrepresented languages.", "published": "2024-04-03 21:29:40", "link": "http://arxiv.org/abs/2404.03073v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Exploring the Trade-off Between Model Performance and Explanation\n  Plausibility of Text Classifiers Using Human Rationales", "abstract": "Saliency post-hoc explainability methods are important tools for\nunderstanding increasingly complex NLP models. While these methods can reflect\nthe model's reasoning, they may not align with human intuition, making the\nexplanations not plausible. In this work, we present a methodology for\nincorporating rationales, which are text annotations explaining human\ndecisions, into text classification models. This incorporation enhances the\nplausibility of post-hoc explanations while preserving their faithfulness. Our\napproach is agnostic to model architectures and explainability methods. We\nintroduce the rationales during model training by augmenting the standard\ncross-entropy loss with a novel loss function inspired by contrastive learning.\nBy leveraging a multi-objective optimization algorithm, we explore the\ntrade-off between the two loss functions and generate a Pareto-optimal frontier\nof models that balance performance and plausibility. Through extensive\nexperiments involving diverse models, datasets, and explainability methods, we\ndemonstrate that our approach significantly enhances the quality of model\nexplanations without causing substantial (sometimes negligible) degradation in\nthe original model's performance.", "published": "2024-04-03 22:39:33", "link": "http://arxiv.org/abs/2404.03098v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Testing the Effect of Code Documentation on Large Language Model Code\n  Understanding", "abstract": "Large Language Models (LLMs) have demonstrated impressive abilities in recent\nyears with regards to code generation and understanding. However, little work\nhas investigated how documentation and other code properties affect an LLM's\nability to understand and generate code or documentation. We present an\nempirical analysis of how underlying properties of code or documentation can\naffect an LLM's capabilities. We show that providing an LLM with \"incorrect\"\ndocumentation can greatly hinder code understanding, while incomplete or\nmissing documentation does not seem to significantly affect an LLM's ability to\nunderstand code.", "published": "2024-04-03 23:33:56", "link": "http://arxiv.org/abs/2404.03114v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Attributions toward Artificial Agents in a modified Moral Turing Test", "abstract": "Advances in artificial intelligence (AI) raise important questions about\nwhether people view moral evaluations by AI systems similarly to\nhuman-generated moral evaluations. We conducted a modified Moral Turing Test\n(m-MTT), inspired by Allen and colleagues' (2000) proposal, by asking people to\ndistinguish real human moral evaluations from those made by a popular advanced\nAI language model: GPT-4. A representative sample of 299 U.S. adults first\nrated the quality of moral evaluations when blinded to their source.\nRemarkably, they rated the AI's moral reasoning as superior in quality to\nhumans' along almost all dimensions, including virtuousness, intelligence, and\ntrustworthiness, consistent with passing what Allen and colleagues call the\ncomparative MTT. Next, when tasked with identifying the source of each\nevaluation (human or computer), people performed significantly above chance\nlevels. Although the AI did not pass this test, this was not because of its\ninferior moral reasoning but, potentially, its perceived superiority, among\nother possible explanations. The emergence of language models capable of\nproducing moral responses perceived as superior in quality to humans' raises\nconcerns that people may uncritically accept potentially harmful moral guidance\nfrom AI. This possibility highlights the need for safeguards around generative\nlanguage models in matters of morality.", "published": "2024-04-03 13:00:47", "link": "http://arxiv.org/abs/2406.11854v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "PhonologyBench: Evaluating Phonological Skills of Large Language Models", "abstract": "Phonology, the study of speech's structure and pronunciation rules, is a\ncritical yet often overlooked component in Large Language Model (LLM) research.\nLLMs are widely used in various downstream applications that leverage phonology\nsuch as educational tools and poetry generation. Moreover, LLMs can potentially\nlearn imperfect associations between orthographic and phonological forms from\nthe training data. Thus, it is imperative to benchmark the phonological skills\nof LLMs. To this end, we present PhonologyBench, a novel benchmark consisting\nof three diagnostic tasks designed to explicitly test the phonological skills\nof LLMs in English: grapheme-to-phoneme conversion, syllable counting, and\nrhyme word generation. Despite having no access to speech data, LLMs showcased\nnotable performance on the PhonologyBench tasks. However, we observe a\nsignificant gap of 17% and 45% on Rhyme Word Generation and Syllable counting,\nrespectively, when compared to humans. Our findings underscore the importance\nof studying LLM performance on phonological tasks that inadvertently impact\nreal-world applications. Furthermore, we encourage researchers to choose LLMs\nthat perform well on the phonological task that is closely related to the\ndownstream application since we find that no single model consistently\noutperforms the others on all the tasks.", "published": "2024-04-03 04:53:14", "link": "http://arxiv.org/abs/2404.02456v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "BCAmirs at SemEval-2024 Task 4: Beyond Words: A Multimodal and\n  Multilingual Exploration of Persuasion in Memes", "abstract": "Memes, combining text and images, frequently use metaphors to convey\npersuasive messages, shaping public opinion. Motivated by this, our team\nengaged in SemEval-2024 Task 4, a hierarchical multi-label classification task\ndesigned to identify rhetorical and psychological persuasion techniques\nembedded within memes. To tackle this problem, we introduced a caption\ngeneration step to assess the modality gap and the impact of additional\nsemantic information from images, which improved our result. Our best model\nutilizes GPT-4 generated captions alongside meme text to fine-tune RoBERTa as\nthe text encoder and CLIP as the image encoder. It outperforms the baseline by\na large margin in all 12 subtasks. In particular, it ranked in top-3 across all\nlanguages in Subtask 2a, and top-4 in Subtask 2b, demonstrating quantitatively\nstrong performance. The improvement achieved by the introduced intermediate\nstep is likely attributable to the metaphorical essence of images that\nchallenges visual encoders. This highlights the potential for improving\nabstract visual semantics encoding.", "published": "2024-04-03 19:17:43", "link": "http://arxiv.org/abs/2404.03022v2", "categories": ["cs.CL", "cs.CV", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.CL"}
{"title": "CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot\n  Text-to-Speech", "abstract": "With the emergence of neural audio codecs, which encode multiple streams of\ndiscrete tokens from audio, large language models have recently gained\nattention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis.\nDespite the ongoing rush towards scaling paradigms, audio tokenization\nironically amplifies the scalability challenge, stemming from its long sequence\nlength and the complexity of modelling the multiple sequences. To mitigate\nthese issues, we present CLaM-TTS that employs a probabilistic residual vector\nquantization to (1) achieve superior compression in the token length, and (2)\nallow a language model to generate multiple tokens at once, thereby eliminating\nthe need for cascaded modeling to handle the number of token streams. Our\nexperimental results demonstrate that CLaM-TTS is better than or comparable to\nstate-of-the-art neural codec-based TTS models regarding naturalness,\nintelligibility, speaker similarity, and inference speed. In addition, we\nexamine the impact of the pretraining extent of the language models and their\ntext tokenization strategies on performances.", "published": "2024-04-03 14:52:20", "link": "http://arxiv.org/abs/2404.02781v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
