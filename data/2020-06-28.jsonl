{"title": "Self-Attention Networks for Intent Detection", "abstract": "Self-attention networks (SAN) have shown promising performance in various\nNatural Language Processing (NLP) scenarios, especially in machine translation.\nOne of the main points of SANs is the strength of capturing long-range and\nmulti-scale dependencies from the data. In this paper, we present a novel\nintent detection system which is based on a self-attention network and a\nBi-LSTM. Our approach shows improvement by using a transformer model and deep\naveraging network-based universal sentence encoder compared to previous\nsolutions. We evaluate the system on Snips, Smart Speaker, Smart Lights, and\nATIS datasets by different evaluation metrics. The performance of the proposed\nmodel is compared with LSTM with the same datasets.", "published": "2020-06-28 12:19:15", "link": "http://arxiv.org/abs/2006.15585v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Positional Encoding in Language Pre-training", "abstract": "In this work, we investigate the positional encoding methods used in language\npre-training (e.g., BERT) and identify several problems in the existing\nformulations. First, we show that in the absolute positional encoding, the\naddition operation applied on positional embeddings and word embeddings brings\nmixed correlations between the two heterogeneous information resources. It may\nbring unnecessary randomness in the attention and further limit the\nexpressiveness of the model. Second, we question whether treating the position\nof the symbol \\texttt{[CLS]} the same as other words is a reasonable design,\nconsidering its special role (the representation of the entire sentence) in the\ndownstream tasks. Motivated from above analysis, we propose a new positional\nencoding method called \\textbf{T}ransformer with \\textbf{U}ntied\n\\textbf{P}ositional \\textbf{E}ncoding (TUPE). In the self-attention module,\nTUPE computes the word contextual correlation and positional correlation\nseparately with different parameterizations and then adds them together. This\ndesign removes the mixed and noisy correlations over heterogeneous embeddings\nand offers more expressiveness by using different projection matrices.\nFurthermore, TUPE unties the \\texttt{[CLS]} symbol from other positions, making\nit easier to capture information from all positions. Extensive experiments and\nablation studies on GLUE benchmark demonstrate the effectiveness of the\nproposed method. Codes and models are released at\nhttps://github.com/guolinke/TUPE.", "published": "2020-06-28 13:11:02", "link": "http://arxiv.org/abs/2006.15595v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Progressive Generation of Long Text with Pretrained Language Models", "abstract": "Large-scale language models (LMs) pretrained on massive corpora of text, such\nas GPT-2, are powerful open-domain text generators. However, as our systematic\nexamination reveals, it is still challenging for such models to generate\ncoherent long passages of text (e.g., 1000 tokens), especially when the models\nare fine-tuned to the target domain on a small corpus. Previous\nplanning-then-generation methods also fall short of producing such long text in\nvarious domains. To overcome the limitations, we propose a simple but effective\nmethod of generating text in a progressive manner, inspired by generating\nimages from low to high resolution. Our method first produces domain-specific\ncontent keywords and then progressively refines them into complete passages in\nmultiple stages. The simple design allows our approach to take advantage of\npretrained LMs at each stage and effectively adapt to any target domain given\nonly a small set of examples. We conduct a comprehensive empirical study with a\nbroad set of evaluation metrics, and show that our approach significantly\nimproves upon the fine-tuned large LMs and various planning-then-generation\nmethods in terms of quality and sample efficiency. Human evaluation also\nvalidates that our model generations are more coherent.", "published": "2020-06-28 21:23:05", "link": "http://arxiv.org/abs/2006.15720v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mapping Topic Evolution Across Poetic Traditions", "abstract": "Poetic traditions across languages evolved differently, but we find that\ncertain semantic topics occur in several of them, albeit sometimes with\ntemporal delay, or with diverging trajectories over time. We apply Latent\nDirichlet Allocation (LDA) to poetry corpora of four languages, i.e. German\n(52k poems), English (85k poems), Russian (18k poems), and Czech (80k poems).\nWe align and interpret salient topics, their trend over time (1600--1925 A.D.),\nshowing similarities and disparities across poetic traditions with a few select\ntopics, and use their trajectories over time to pinpoint specific literary\nepochs.", "published": "2020-06-28 22:23:03", "link": "http://arxiv.org/abs/2006.15732v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "BOND: BERT-Assisted Open-Domain Named Entity Recognition with Distant\n  Supervision", "abstract": "We study the open-domain named entity recognition (NER) problem under distant\nsupervision. The distant supervision, though does not require large amounts of\nmanual annotations, yields highly incomplete and noisy distant labels via\nexternal knowledge bases. To address this challenge, we propose a new\ncomputational framework -- BOND, which leverages the power of pre-trained\nlanguage models (e.g., BERT and RoBERTa) to improve the prediction performance\nof NER models. Specifically, we propose a two-stage training algorithm: In the\nfirst stage, we adapt the pre-trained language model to the NER tasks using the\ndistant labels, which can significantly improve the recall and precision; In\nthe second stage, we drop the distant labels, and propose a self-training\napproach to further improve the model performance. Thorough experiments on 5\nbenchmark datasets demonstrate the superiority of BOND over existing distantly\nsupervised NER methods. The code and distantly labeled data have been released\nin https://github.com/cliang1453/BOND.", "published": "2020-06-28 04:55:39", "link": "http://arxiv.org/abs/2006.15509v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "End-to-End AI-Based Point-of-Care Diagnosis System for Classifying\n  Respiratory Illnesses and Early Detection of COVID-19", "abstract": "Respiratory symptoms can be a caused by different underlying conditions, and\nare often caused by viral infections, such as Influenza-like illnesses or other\nemerging viruses like the Coronavirus. These respiratory viruses, often, have\ncommon symptoms, including coughing, high temperature, congested nose, and\ndifficulty breathing. However, early diagnosis of the type of the virus, can be\ncrucial, especially in cases such as the recent COVID-19 pandemic. One of the\nfactors that contributed to the spread of the pandemic, was the late diagnosis\nor confusing it with regular flu-like symptoms. Science has proved that one of\nthe possible differentiators of the underlying causes of these different\nrespiratory diseases is coughing, which comes in different types and forms.\nTherefore, a reliable lab-free tool for early and more accurate diagnosis that\ncan differentiate between different respiratory diseases is very much needed.\nThis paper proposes an end-to-end portable system that can record data from\npatients with symptom, including coughs (voluntary or involuntary) and\ntranslate them into health data for diagnosis, and with the aid of machine\nlearning, classify them into different respiratory illnesses, including\nCOVID-19. With the ongoing efforts to stop the spread of the COVID-19 disease\neverywhere today, and against similar diseases in the future, our proposed low\ncost and user-friendly solution can play an important part in the early\ndiagnosis.", "published": "2020-06-28 00:06:48", "link": "http://arxiv.org/abs/2006.15469v1", "categories": ["eess.SP", "cs.AI", "cs.AR", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
