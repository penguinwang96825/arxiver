{"title": "Learned Construction Grammars Converge Across Registers Given Increased\n  Exposure", "abstract": "This paper measures the impact of increased exposure on whether learned\nconstruction grammars converge onto shared representations when trained on data\nfrom different registers. Register influences the frequency of constructions,\nwith some structures common in formal but not informal usage. We expect that a\ngrammar induction algorithm exposed to different registers will acquire\ndifferent constructions. To what degree does increased exposure lead to the\nconvergence of register-specific grammars? The experiments in this paper\nsimulate language learning in 12 languages (half Germanic and half Romance)\nwith corpora representing three registers (Twitter, Wikipedia, Web). These\nsimulations are repeated with increasing amounts of exposure, from 100k to 2\nmillion words, to measure the impact of exposure on the convergence of\ngrammars. The results show that increased exposure does lead to converging\ngrammars across all languages. In addition, a shared core of register-universal\nconstructions remains constant across increasing amounts of exposure.", "published": "2021-10-12 00:45:42", "link": "http://arxiv.org/abs/2110.05663v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are you doing what I say? On modalities alignment in ALFRED", "abstract": "ALFRED is a recently proposed benchmark that requires a model to complete\ntasks in simulated house environments specified by instructions in natural\nlanguage. We hypothesize that key to success is accurately aligning the text\nmodality with visual inputs. Motivated by this, we inspect how well existing\nmodels can align these modalities using our proposed intrinsic metric, boundary\nadherence score (BAS). The results show the previous models are indeed failing\nto perform proper alignment. To address this issue, we introduce approaches\naimed at improving model alignment and demonstrate how improved alignment,\nimproves end task performance.", "published": "2021-10-12 01:05:37", "link": "http://arxiv.org/abs/2110.05665v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Doubly-Trained Adversarial Data Augmentation for Neural Machine\n  Translation", "abstract": "Neural Machine Translation (NMT) models are known to suffer from noisy\ninputs. To make models robust, we generate adversarial augmentation samples\nthat attack the model and preserve the source-side semantic meaning at the same\ntime. To generate such samples, we propose a doubly-trained architecture that\npairs two NMT models of opposite translation directions with a joint loss\nfunction, which combines the target-side attack and the source-side semantic\nsimilarity constraint. The results from our experiments across three different\nlanguage pairs and two evaluation metrics show that these adversarial samples\nimprove the model robustness.", "published": "2021-10-12 02:23:00", "link": "http://arxiv.org/abs/2110.05691v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Anatomy of OntoGUM--Adapting GUM to the OntoNotes Scheme to Evaluate\n  Robustness of SOTA Coreference Algorithms", "abstract": "SOTA coreference resolution produces increasingly impressive scores on the\nOntoNotes benchmark. However lack of comparable data following the same scheme\nfor more genres makes it difficult to evaluate generalizability to open domain\ndata. Zhu et al. (2021) introduced the creation of the OntoGUM corpus for\nevaluating geralizability of the latest neural LM-based end-to-end systems.\nThis paper covers details of the mapping process which is a set of\ndeterministic rules applied to the rich syntactic and discourse annotations\nmanually annotated in the GUM corpus. Out-of-domain evaluation across 12 genres\nshows nearly 15-20% degradation for both deterministic and deep learning\nsystems, indicating a lack of generalizability or covert overfitting in\nexisting coreference resolution models.", "published": "2021-10-12 03:52:49", "link": "http://arxiv.org/abs/2110.05727v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SEPP: Similarity Estimation of Predicted Probabilities for Defending and\n  Detecting Adversarial Text", "abstract": "There are two cases describing how a classifier processes input text, namely,\nmisclassification and correct classification. In terms of misclassified texts,\na classifier handles the texts with both incorrect predictions and adversarial\ntexts, which are generated to fool the classifier, which is called a victim.\nBoth types are misunderstood by the victim, but they can still be recognized by\nother classifiers. This induces large gaps in predicted probabilities between\nthe victim and the other classifiers. In contrast, text correctly classified by\nthe victim is often successfully predicted by the others and induces small\ngaps. In this paper, we propose an ensemble model based on similarity\nestimation of predicted probabilities (SEPP) to exploit the large gaps in the\nmisclassified predictions in contrast to small gaps in the correct\nclassification. SEPP then corrects the incorrect predictions of the\nmisclassified texts. We demonstrate the resilience of SEPP in defending and\ndetecting adversarial texts through different types of victim classifiers,\nclassification tasks, and adversarial attacks.", "published": "2021-10-12 05:36:54", "link": "http://arxiv.org/abs/2110.05748v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SportsSum2.0: Generating High-Quality Sports News from Live Text\n  Commentary", "abstract": "Sports game summarization aims to generate news articles from live text\ncommentaries. A recent state-of-the-art work, SportsSum, not only constructs a\nlarge benchmark dataset, but also proposes a two-step framework. Despite its\ngreat contributions, the work has three main drawbacks: 1) the noise existed in\nSportsSum dataset degrades the summarization performance; 2) the neglect of\nlexical overlap between news and commentaries results in low-quality\npseudo-labeling algorithm; 3) the usage of directly concatenating rewritten\nsentences to form news limits its practicability. In this paper, we publish a\nnew benchmark dataset SportsSum2.0, together with a modified summarization\nframework. In particular, to obtain a clean dataset, we employ crowd workers to\nmanually clean the original dataset. Moreover, the degree of lexical overlap is\nincorporated into the generation of pseudo labels. Further, we introduce a\nreranker-enhanced summarizer to take into account the fluency and\nexpressiveness of the summarized news. Extensive experiments show that our\nmodel outperforms the state-of-the-art baseline.", "published": "2021-10-12 05:39:48", "link": "http://arxiv.org/abs/2110.05750v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantifying Cognitive Factors in Lexical Decline", "abstract": "We adopt an evolutionary view on language change in which cognitive factors\n(in addition to social ones) affect the fitness of words and their success in\nthe linguistic ecosystem. Specifically, we propose a variety of\npsycholinguistic factors -- semantic, distributional, and phonological -- that\nwe hypothesize are predictive of lexical decline, in which words greatly\ndecrease in frequency over time. Using historical data across three languages\n(English, French, and German), we find that most of our proposed factors show a\nsignificant difference in the expected direction between each curated set of\ndeclining words and their matched stable words. Moreover, logistic regression\nanalyses show that semantic and distributional factors are significant in\npredicting declining words. Further diachronic analysis reveals that declining\nwords tend to decrease in the diversity of their lexical contexts over time,\ngradually narrowing their 'ecological niches'.", "published": "2021-10-12 07:12:56", "link": "http://arxiv.org/abs/2110.05775v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "We've had this conversation before: A Novel Approach to Measuring Dialog\n  Similarity", "abstract": "Dialog is a core building block of human natural language interactions. It\ncontains multi-party utterances used to convey information from one party to\nanother in a dynamic and evolving manner. The ability to compare dialogs is\nbeneficial in many real world use cases, such as conversation analytics for\ncontact center calls and virtual agent design.\n  We propose a novel adaptation of the edit distance metric to the scenario of\ndialog similarity. Our approach takes into account various conversation aspects\nsuch as utterance semantics, conversation flow, and the participants. We\nevaluate this new approach and compare it to existing document similarity\nmeasures on two publicly available datasets. The results demonstrate that our\nmethod outperforms the other approaches in capturing dialog flow, and is better\naligned with the human perception of conversation similarity.", "published": "2021-10-12 07:24:12", "link": "http://arxiv.org/abs/2110.05780v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "text2sdg: An R package to Monitor Sustainable Development Goals from\n  Text", "abstract": "Monitoring progress on the United Nations Sustainable Development Goals\n(SDGs) is important for both academic and non-academic organizations. Existing\napproaches to monitoring SDGs have focused on specific data types; namely,\npublications listed in proprietary research databases. We present the text2sdg\npackage for the R language, a user-friendly, open-source package that detects\nSDGs in any kind of text data using different existing or custom-made query\nsystems. The text2sdg package thereby facilitates the monitoring of SDGs for a\nwide array of text sources and provides a much-needed basis for validating and\nimproving extant methods to detect SDGs from text.", "published": "2021-10-12 09:43:10", "link": "http://arxiv.org/abs/2110.05856v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LaoPLM: Pre-trained Language Models for Lao", "abstract": "Trained on the large corpus, pre-trained language models (PLMs) can capture\ndifferent levels of concepts in context and hence generate universal language\nrepresentations. They can benefit multiple downstream natural language\nprocessing (NLP) tasks. Although PTMs have been widely used in most NLP\napplications, especially for high-resource languages such as English, it is\nunder-represented in Lao NLP research. Previous work on Lao has been hampered\nby the lack of annotated datasets and the sparsity of language resources. In\nthis work, we construct a text classification dataset to alleviate the\nresource-scare situation of the Lao language. We additionally present the first\ntransformer-based PTMs for Lao with four versions: BERT-small, BERT-base,\nELECTRA-small and ELECTRA-base, and evaluate it over two downstream tasks:\npart-of-speech tagging and text classification. Experiments demonstrate the\neffectiveness of our Lao models. We will release our models and datasets to the\ncommunity, hoping to facilitate the future development of Lao NLP applications.", "published": "2021-10-12 11:13:07", "link": "http://arxiv.org/abs/2110.05896v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiscoDVT: Generating Long Text with Discourse-Aware Discrete Variational\n  Transformer", "abstract": "Despite the recent advances in applying pre-trained language models to\ngenerate high-quality texts, generating long passages that maintain long-range\ncoherence is yet challenging for these models. In this paper, we propose\nDiscoDVT, a discourse-aware discrete variational Transformer to tackle the\nincoherence issue. DiscoDVT learns a discrete variable sequence that summarizes\nthe global structure of the text and then applies it to guide the generation\nprocess at each decoding step. To further embed discourse-aware information\ninto the discrete latent representations, we introduce an auxiliary objective\nto model the discourse relations within the text. We conduct extensive\nexperiments on two open story generation datasets and demonstrate that the\nlatent codes learn meaningful correspondence to the discourse structures that\nguide the model to generate long texts with better long-range coherence.", "published": "2021-10-12 13:41:06", "link": "http://arxiv.org/abs/2110.05999v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic Model Supervised by Understanding Map", "abstract": "Inspired by the notion of Center of Mass in physics, an extension called\nSemantic Center of Mass (SCOM) is proposed, and used to discover the abstract\n\"topic\" of a document. The notion is under a framework model called\nUnderstanding Map Supervised Topic Model (UM-S-TM). The devising aim of UM-S-TM\nis to let both the document content and a semantic network -- specifically,\nUnderstanding Map -- play a role, in interpreting the meaning of a document.\nBased on different justifications, three possible methods are devised to\ndiscover the SCOM of a document. Some experiments on artificial documents and\nUnderstanding Maps are conducted to test their outcomes. In addition, its\nability of vectorization of documents and capturing sequential information are\ntested. We also compared UM-S-TM with probabilistic topic models like Latent\nDirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).", "published": "2021-10-12 14:42:33", "link": "http://arxiv.org/abs/2110.06043v12", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LiST: Lite Prompted Self-training Makes Parameter-Efficient Few-shot\n  Learners", "abstract": "We present a new method LiST is short for Lite Prompted Self-Training for\nparameter-efficient fine-tuning of large pre-trained language models (PLMs) for\nfew-shot learning. LiST improves over recent methods that adopt prompt-based\nfine-tuning (FN) using two key techniques. The first is the use of\nself-training to leverage large amounts of unlabeled data for prompt-based FN\nin few-shot settings. We use self-training in conjunction with meta-learning\nfor re-weighting noisy pseudo-prompt labels. Self-training is expensive as it\nrequires updating all the model parameters repetitively. Therefore, we use a\nsecond technique for light-weight fine-tuning where we introduce a small number\nof task-specific parameters that are fine-tuned during self-training while\nkeeping the PLM encoder frozen. Our experiments show that LiST can effectively\nleverage unlabeled data to improve the model performance for few-shot learning.\nAdditionally, the fine-tuning is efficient as it only updates a small\npercentage of parameters and the overall model footprint is reduced since\nseveral tasks can share a common PLM encoder as backbone. A comprehensive study\non six NLU tasks demonstrate LiST to improve by 35% over classic fine-tuning\nand 6% over prompt-based FN with 96% reduction in number of trainable\nparameters when fine-tuned with no more than 30 labeled examples from each\ntask. With only 14M tunable parameters, LiST outperforms GPT-3 in-context\nlearning by 33% on few-shot NLU tasks.", "published": "2021-10-12 18:47:18", "link": "http://arxiv.org/abs/2110.06274v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decision-Theoretic Question Generation for Situated Reference\n  Resolution: An Empirical Study and Computational Model", "abstract": "Dialogue agents that interact with humans in situated environments need to\nmanage referential ambiguity across multiple modalities and ask for help as\nneeded. However, it is not clear what kinds of questions such agents should ask\nnor how the answers to such questions can be used to resolve ambiguity. To\naddress this, we analyzed dialogue data from an interactive study in which\nparticipants controlled a virtual robot tasked with organizing a set of tools\nwhile engaging in dialogue with a live, remote experimenter. We discovered a\nnumber of novel results, including the distribution of question types used to\nresolve ambiguity and the influence of dialogue-level factors on the reference\nresolution process. Based on these empirical findings we: (1) developed a\ncomputational model for clarification requests using a decision network with an\nentropy-based utility assignment method that operates across modalities, (2)\nevaluated the model, showing that it outperforms a slot-filling baseline in\nenvironments of varying ambiguity, and (3) interpreted the results to offer\ninsight into the ways that agents can ask questions to facilitate situated\nreference resolution.", "published": "2021-10-12 19:23:25", "link": "http://arxiv.org/abs/2110.06288v1", "categories": ["cs.CL", "I.2.6; J.4"], "primary_category": "cs.CL"}
{"title": "Learning Compact Metrics for MT", "abstract": "Recent developments in machine translation and multilingual text generation\nhave led researchers to adopt trained metrics such as COMET or BLEURT, which\ntreat evaluation as a regression problem and use representations from\nmultilingual pre-trained models such as XLM-RoBERTa or mBERT. Yet studies on\nrelated tasks suggest that these models are most efficient when they are large,\nwhich is costly and impractical for evaluation. We investigate the trade-off\nbetween multilinguality and model capacity with RemBERT, a state-of-the-art\nmultilingual language model, using data from the WMT Metrics Shared Task. We\npresent a series of experiments which show that model size is indeed a\nbottleneck for cross-lingual transfer, then demonstrate how distillation can\nhelp addressing this bottleneck, by leveraging synthetic data generation and\ntransferring knowledge from one teacher to multiple students trained on related\nlanguages. Our method yields up to 10.5% improvement over vanilla fine-tuning\nand reaches 92.6% of RemBERT's performance using only a third of its\nparameters.", "published": "2021-10-12 20:39:35", "link": "http://arxiv.org/abs/2110.06341v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Time Masking for Temporal Language Models", "abstract": "Our world is constantly evolving, and so is the content on the web.\nConsequently, our languages, often said to mirror the world, are dynamic in\nnature. However, most current contextual language models are static and cannot\nadapt to changes over time. In this work, we propose a temporal contextual\nlanguage model called TempoBERT, which uses time as an additional context of\ntexts. Our technique is based on modifying texts with temporal information and\nperforming time masking - specific masking for the supplementary time\ninformation. We leverage our approach for the tasks of semantic change\ndetection and sentence time prediction, experimenting on diverse datasets in\nterms of time, size, genre, and language. Our extensive evaluation shows that\nboth tasks benefit from exploiting time masking.", "published": "2021-10-12 21:15:23", "link": "http://arxiv.org/abs/2110.06366v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ALL Dolphins Are Intelligent and SOME Are Friendly: Probing BERT for\n  Nouns' Semantic Properties and their Prototypicality", "abstract": "Large scale language models encode rich commonsense knowledge acquired\nthrough exposure to massive data during pre-training, but their understanding\nof entities and their semantic properties is unclear. We probe BERT (Devlin et\nal., 2019) for the properties of English nouns as expressed by adjectives that\ndo not restrict the reference scope of the noun they modify (as in \"red car\"),\nbut instead emphasise some inherent aspect (\"red strawberry\"). We base our\nstudy on psycholinguistics datasets that capture the association strength\nbetween nouns and their semantic features. We probe BERT using cloze tasks and\nin a classification setting, and show that the model has marginal knowledge of\nthese features and their prevalence as expressed in these datasets. We discuss\nfactors that make evaluation challenging and impede drawing general conclusions\nabout the models' knowledge of noun properties. Finally, we show that when\ntested in a fine-tuning setting addressing entailment, BERT successfully\nleverages the information needed for reasoning about the meaning of\nadjective-noun constructions outperforming previous methods.", "published": "2021-10-12 21:43:37", "link": "http://arxiv.org/abs/2110.06376v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Can Be Strong Differentially Private Learners", "abstract": "Differentially Private (DP) learning has seen limited success for building\nlarge deep learning models of text, and straightforward attempts at applying\nDifferentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have\nresulted in large performance drops and high computational overhead. We show\nthat this performance drop can be mitigated with (1) the use of large\npretrained language models; (2) non-standard hyperparameters that suit DP\noptimization; and (3) fine-tuning objectives which are aligned with the\npretraining procedure. With the above, we obtain NLP models that outperform\nstate-of-the-art DP-trained models under the same privacy budget and strong\nnon-private baselines -- by directly fine-tuning pretrained models with DP\noptimization on moderately-sized corpora. To address the computational\nchallenge of running DP-SGD with large Transformers, we propose a memory saving\ntechnique that allows clipping in DP-SGD to run without instantiating\nper-example gradients for any linear layer in the model. The technique enables\nprivately training Transformers with almost the same memory cost as non-private\ntraining at a modest run-time overhead. Contrary to conventional wisdom that DP\noptimization fails at learning high-dimensional models (due to noise that\nscales with dimension) empirical results reveal that private learning with\npretrained language models doesn't tend to suffer from dimension-dependent\nperformance degradation. Code to reproduce results can be found at\nhttps://github.com/lxuechen/private-transformers.", "published": "2021-10-12 01:45:27", "link": "http://arxiv.org/abs/2110.05679v6", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "On Releasing Annotator-Level Labels and Information in Datasets", "abstract": "A common practice in building NLP datasets, especially using crowd-sourced\nannotations, involves obtaining multiple annotator judgements on the same data\ninstances, which are then flattened to produce a single \"ground truth\" label or\nscore, through majority voting, averaging, or adjudication. While these\napproaches may be appropriate in certain annotation tasks, such aggregations\noverlook the socially constructed nature of human perceptions that annotations\nfor relatively more subjective tasks are meant to capture. In particular,\nsystematic disagreements between annotators owing to their socio-cultural\nbackgrounds and/or lived experiences are often obfuscated through such\naggregations. In this paper, we empirically demonstrate that label aggregation\nmay introduce representational biases of individual and group perspectives.\nBased on this finding, we propose a set of recommendations for increased\nutility and transparency of datasets for downstream use cases.", "published": "2021-10-12 02:35:45", "link": "http://arxiv.org/abs/2110.05699v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Relation-aware Video Reading Comprehension for Temporal Language\n  Grounding", "abstract": "Temporal language grounding in videos aims to localize the temporal span\nrelevant to the given query sentence. Previous methods treat it either as a\nboundary regression task or a span extraction task. This paper will formulate\ntemporal language grounding into video reading comprehension and propose a\nRelation-aware Network (RaNet) to address it. This framework aims to select a\nvideo moment choice from the predefined answer set with the aid of\ncoarse-and-fine choice-query interaction and choice-choice relation\nconstruction. A choice-query interactor is proposed to match the visual and\ntextual information simultaneously in sentence-moment and token-moment levels,\nleading to a coarse-and-fine cross-modal interaction. Moreover, a novel\nmulti-choice relation constructor is introduced by leveraging graph convolution\nto capture the dependencies among video moment choices for the best choice\nselection. Extensive experiments on ActivityNet-Captions, TACoS, and\nCharades-STA demonstrate the effectiveness of our solution. Codes have been\navailable.", "published": "2021-10-12 03:10:21", "link": "http://arxiv.org/abs/2110.05717v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Dealing with Disagreements: Looking Beyond the Majority Vote in\n  Subjective Annotations", "abstract": "Majority voting and averaging are common approaches employed to resolve\nannotator disagreements and derive single ground truth labels from multiple\nannotations. However, annotators may systematically disagree with one another,\noften reflecting their individual biases and values, especially in the case of\nsubjective tasks such as detecting affect, aggression, and hate speech.\nAnnotator disagreements may capture important nuances in such tasks that are\noften ignored while aggregating annotations to a single ground truth. In order\nto address this, we investigate the efficacy of multi-annotator models. In\nparticular, our multi-task based approach treats predicting each annotators'\njudgements as separate subtasks, while sharing a common learned representation\nof the task. We show that this approach yields same or better performance than\naggregating labels in the data prior to training across seven different binary\nclassification tasks. Our approach also provides a way to estimate uncertainty\nin predictions, which we demonstrate better correlate with annotation\ndisagreements than traditional methods. Being able to model uncertainty is\nespecially useful in deployment scenarios where knowing when not to make a\nprediction is important.", "published": "2021-10-12 03:12:34", "link": "http://arxiv.org/abs/2110.05719v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "LightSeq2: Accelerated Training for Transformer-based Models on GPUs", "abstract": "Transformer-based neural models are used in many AI applications. Training\nthese models is expensive, as it takes huge GPU resources and long duration. It\nis challenging because typical data like sentences have variable lengths, and\nTransformer's computation patterns are more complex than convolutional neural\nnetworks. Existing systems either only focus on model inference or optimization\nfor only BERT-like encoder models. In this paper, we present LightSeq2, a\nsystem to accelerate training for a general family of Transformer models on\nGPUs. We propose a series of GPU optimization techniques tailored to the\nspecific computation flow and memory access patterns of Transformer models.\nLightSeq2 supports many model architectures, including BERT (encoder-only), GPT\n(decoder-only), Transformer (encoder-decoder), and vision Transformer. Our\nexperiments for a variety of models and benchmarks show that LightSeq2 is\nconsistently faster (1.4-3.5x) than previous systems on different GPUs. In\nparticular, it gains 308% training speedup compared with existing systems on a\nlarge public machine translation benchmark (WMT14 English-German).", "published": "2021-10-12 03:17:03", "link": "http://arxiv.org/abs/2110.05722v3", "categories": ["cs.CL", "cs.MS"], "primary_category": "cs.CL"}
{"title": "Investigation on Data Adaptation Techniques for Neural Named Entity\n  Recognition", "abstract": "Data processing is an important step in various natural language processing\ntasks. As the commonly used datasets in named entity recognition contain only a\nlimited number of samples, it is important to obtain additional labeled data in\nan efficient and reliable manner. A common practice is to utilize large\nmonolingual unlabeled corpora. Another popular technique is to create synthetic\ndata from the original labeled data (data augmentation). In this work, we\ninvestigate the impact of these two methods on the performance of three\ndifferent named entity recognition tasks.", "published": "2021-10-12 11:06:03", "link": "http://arxiv.org/abs/2110.05892v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extracting Feelings of People Regarding COVID-19 by Social Network\n  Mining", "abstract": "In 2020, COVID-19 became the chief concern of the world and is still\nreflected widely in all social networks. Each day, users post millions of\ntweets and comments on this subject, which contain significant implicit\ninformation about the public opinion. In this regard, a dataset of\nCOVID-related tweets in English language is collected, which consists of more\nthan two million tweets from March 23 to June 23 of 2020 to extract the\nfeelings of the people in various countries in the early stages of this\noutbreak. To this end, first, we use a lexicon-based approach in conjunction\nwith the GeoNames geographic database to label the tweets with their locations.\nNext, a method based on the recently introduced and widely cited RoBERTa model\nis proposed to analyze their sentimental content. After that, the trend graphs\nof the frequency of tweets as well as sentiments are produced for the world and\nthe nations that were more engaged with COVID-19. Graph analysis shows that the\nfrequency graphs of the tweets for the majority of nations are significantly\ncorrelated with the official statistics of the daily afflicted in them.\nMoreover, several implicit knowledge is extracted and discussed.", "published": "2021-10-12 16:45:33", "link": "http://arxiv.org/abs/2110.06151v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Investigating the Effect of Natural Language Explanations on\n  Out-of-Distribution Generalization in Few-shot NLI", "abstract": "Although neural models have shown strong performance in datasets such as\nSNLI, they lack the ability to generalize out-of-distribution (OOD). In this\nwork, we formulate a few-shot learning setup and examine the effects of natural\nlanguage explanations on OOD generalization. We leverage the templates in the\nHANS dataset and construct templated natural language explanations for each\ntemplate. Although generated explanations show competitive BLEU scores against\ngroundtruth explanations, they fail to improve prediction performance. We\nfurther show that generated explanations often hallucinate information and miss\nkey elements that indicate the label.", "published": "2021-10-12 18:00:02", "link": "http://arxiv.org/abs/2110.06223v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sm\u00e5prat: DialoGPT for Natural Language Generation of Swedish\n  Dialogue by Transfer Learning", "abstract": "Building open-domain conversational systems (or chatbots) that produce\nconvincing responses is a recognized challenge. Recent state-of-the-art (SoTA)\ntransformer-based models for the generation of natural language dialogue have\ndemonstrated impressive performance in simulating human-like, single-turn\nconversations in English. This work investigates, by an empirical study, the\npotential for transfer learning of such models to Swedish language. DialoGPT,\nan English language pre-trained model, is adapted by training on three\ndifferent Swedish language conversational datasets obtained from publicly\navailable sources. Perplexity score (an automated intrinsic language model\nmetric) and surveys by human evaluation were used to assess the performances of\nthe fine-tuned models, with results that indicate that the capacity for\ntransfer learning can be exploited with considerable success. Human evaluators\nasked to score the simulated dialogue judged over 57% of the chatbot responses\nto be human-like for the model trained on the largest (Swedish) dataset. We\nprovide the demos and model checkpoints of our English and Swedish chatbots on\nthe HuggingFace platform for public use.", "published": "2021-10-12 18:46:43", "link": "http://arxiv.org/abs/2110.06273v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AutoNLU: Detecting, root-causing, and fixing NLU model errors", "abstract": "Improving the quality of Natural Language Understanding (NLU) models, and\nmore specifically, task-oriented semantic parsing models, in production is a\ncumbersome task. In this work, we present a system called AutoNLU, which we\ndesigned to scale the NLU quality improvement process. It adds automation to\nthree key steps: detection, attribution, and correction of model errors, i.e.,\nbugs. We detected four times more failed tasks than with random sampling,\nfinding that even a simple active learning sampling method on an uncalibrated\nmodel is surprisingly effective for this purpose. The AutoNLU tool empowered\nlinguists to fix ten times more semantic parsing bugs than with prior manual\nprocesses, auto-correcting 65% of all identified bugs.", "published": "2021-10-12 22:12:26", "link": "http://arxiv.org/abs/2110.06384v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Attention-guided Generative Models for Extractive Question Answering", "abstract": "We propose a novel method for applying Transformer models to extractive\nquestion answering (QA) tasks. Recently, pretrained generative\nsequence-to-sequence (seq2seq) models have achieved great success in question\nanswering. Contributing to the success of these models are internal attention\nmechanisms such as cross-attention. We propose a simple strategy to obtain an\nextractive answer span from the generative model by leveraging the decoder\ncross-attention patterns. Viewing cross-attention as an architectural prior, we\napply joint training to further improve QA performance. Empirical results show\nthat on open-domain question answering datasets like NaturalQuestions and\nTriviaQA, our method approaches state-of-the-art performance on both generative\nand extractive inference, all while using much fewer parameters. Furthermore,\nthis strategy allows us to perform hallucination-free inference while\nconferring significant improvements to the model's ability to rerank relevant\npassages.", "published": "2021-10-12 23:02:35", "link": "http://arxiv.org/abs/2110.06393v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "FILM: Following Instructions in Language with Modular Methods", "abstract": "Recent methods for embodied instruction following are typically trained\nend-to-end using imitation learning. This often requires the use of expert\ntrajectories and low-level language instructions. Such approaches assume that\nneural states will integrate multimodal semantics to perform state tracking,\nbuilding spatial memory, exploration, and long-term planning. In contrast, we\npropose a modular method with structured representations that (1) builds a\nsemantic map of the scene and (2) performs exploration with a semantic search\npolicy, to achieve the natural language goal. Our modular method achieves SOTA\nperformance (24.46 %) with a substantial (8.17 % absolute) gap from previous\nwork while using less data by eschewing both expert trajectories and low-level\ninstructions. Leveraging low-level language, however, can further increase our\nperformance (26.49 %). Our findings suggest that an explicit spatial memory and\na semantic search policy can provide a stronger and more general representation\nfor state-tracking and guidance, even in the absence of expert trajectories or\nlow-level instructions.", "published": "2021-10-12 16:40:01", "link": "http://arxiv.org/abs/2110.07342v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prediction of Political Leanings of Chinese Speaking Twitter Users", "abstract": "This work presents a supervised method for generating a classifier model of\nthe stances held by Chinese-speaking politicians and other Twitter users. Many\nprevious works of political tweets prediction exist on English tweets, but to\nthe best of our knowledge, this is the first work that builds prediction model\non Chinese political tweets. It firstly collects data by scraping tweets of\nfamous political figure and their related users. It secondly defines the\npolitical spectrum in two groups: the group that shows approvals to the Chinese\nCommunist Party and the group that does not. Since there are not space between\nwords in Chinese to identify the independent words, it then completes\nsegmentation and vectorization by Jieba, a Chinese segmentation tool. Finally,\nit trains the data collected from political tweets and produce a classification\nmodel with high accuracy for understanding users' political stances from their\ntweets on Twitter.", "published": "2021-10-12 03:18:10", "link": "http://arxiv.org/abs/2110.05723v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "68T50", "I.2.7; J.4"], "primary_category": "cs.CY"}
{"title": "VarArray: Array-Geometry-Agnostic Continuous Speech Separation", "abstract": "Continuous speech separation using a microphone array was shown to be\npromising in dealing with the speech overlap problem in natural conversation\ntranscription. This paper proposes VarArray, an array-geometry-agnostic speech\nseparation neural network model. The proposed model is applicable to any number\nof microphones without retraining while leveraging the nonlinear correlation\nbetween the input channels. The proposed method adapts different elements that\nwere proposed before separately, including transform-average-concatenate,\nconformer speech separation, and inter-channel phase differences, and combines\nthem in an efficient and cohesive way. Large-scale evaluation was performed\nwith two real meeting transcription tasks by using a fully developed\ntranscription system requiring no prior knowledge such as reference\nsegmentations, which allowed us to measure the impact that the continuous\nspeech separation system could have in realistic settings. The proposed model\noutperformed a previous approach to array-geometry-agnostic modeling for all of\nthe geometry configurations considered, achieving asclite-based\nspeaker-agnostic word error rates of 17.5% and 20.4% for the AMI development\nand evaluation sets, respectively, in the end-to-end setting using no\nground-truth segmentations.", "published": "2021-10-12 05:31:46", "link": "http://arxiv.org/abs/2110.05745v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "UniSpeech-SAT: Universal Speech Representation Learning with Speaker\n  Aware Pre-Training", "abstract": "Self-supervised learning (SSL) is a long-standing goal for speech processing,\nsince it utilizes large-scale unlabeled data and avoids extensive human\nlabeling. Recent years witness great successes in applying self-supervised\nlearning in speech recognition, while limited exploration was attempted in\napplying SSL for modeling speaker characteristics. In this paper, we aim to\nimprove the existing SSL framework for speaker representation learning. Two\nmethods are introduced for enhancing the unsupervised speaker information\nextraction. First, we apply the multi-task learning to the current SSL\nframework, where we integrate the utterance-wise contrastive loss with the SSL\nobjective function. Second, for better speaker discrimination, we propose an\nutterance mixing strategy for data augmentation, where additional overlapped\nutterances are created unsupervisely and incorporate during training. We\nintegrate the proposed methods into the HuBERT framework. Experiment results on\nSUPERB benchmark show that the proposed system achieves state-of-the-art\nperformance in universal representation learning, especially for speaker\nidentification oriented tasks. An ablation study is performed verifying the\nefficacy of each proposed method. Finally, we scale up training dataset to 94\nthousand hours public audio data and achieve further performance improvement in\nall SUPERB tasks.", "published": "2021-10-12 05:43:30", "link": "http://arxiv.org/abs/2110.05752v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "BERTraffic: BERT-based Joint Speaker Role and Speaker Change Detection\n  for Air Traffic Control Communications", "abstract": "Automatic speech recognition (ASR) allows transcribing the communications\nbetween air traffic controllers (ATCOs) and aircraft pilots. The transcriptions\nare used later to extract ATC named entities, e.g., aircraft callsigns. One\ncommon challenge is speech activity detection (SAD) and speaker diarization\n(SD). In the failure condition, two or more segments remain in the same\nrecording, jeopardizing the overall performance. We propose a system that\ncombines SAD and a BERT model to perform speaker change detection and speaker\nrole detection (SRD) by chunking ASR transcripts, i.e., SD with a defined\nnumber of speakers together with SRD. The proposed model is evaluated on\nreal-life public ATC databases. Our BERT SD model baseline reaches up to 10%\nand 20% token-based Jaccard error rate (JER) in public and private ATC\ndatabases. We also achieved relative improvements of 32% and 7.7% in JERs and\nSD error rate (DER), respectively, compared to VBx, a well-known SD system.", "published": "2021-10-12 07:25:12", "link": "http://arxiv.org/abs/2110.05781v3", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Adapting TTS models For New Speakers using Transfer Learning", "abstract": "Training neural text-to-speech (TTS) models for a new speaker typically\nrequires several hours of high quality speech data. Prior works on voice\ncloning attempt to address this challenge by adapting pre-trained multi-speaker\nTTS models for a new voice, using a few minutes of speech data of the new\nspeaker. However, publicly available large multi-speaker datasets are often\nnoisy, thereby resulting in TTS models that are not suitable for use in\nproducts. We address this challenge by proposing transfer-learning guidelines\nfor adapting high quality single-speaker TTS models for a new speaker, using\nonly a few minutes of speech data. We conduct an extensive study using\ndifferent amounts of data for a new speaker and evaluate the synthesized speech\nin terms of naturalness and voice/style similarity to the target speaker. We\nfind that fine-tuning a single-speaker TTS model on just 30 minutes of data,\ncan yield comparable performance to a model trained from scratch on more than\n27 hours of data for both male and female target speakers.", "published": "2021-10-12 07:51:25", "link": "http://arxiv.org/abs/2110.05798v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Balancing Average and Worst-case Accuracy in Multitask Learning", "abstract": "When training and evaluating machine learning models on a large number of\ntasks, it is important to not only look at average task accuracy -- which may\nbe biased by easy or redundant tasks -- but also worst-case accuracy (i.e. the\nperformance on the task with the lowest accuracy). In this work, we show how to\nuse techniques from the distributionally robust optimization (DRO) literature\nto improve worst-case performance in multitask learning. We highlight several\nfailure cases of DRO when applied off-the-shelf and present an improved method,\nLookahead-DRO (L-DRO), which mitigates these issues. The core idea of L-DRO is\nto anticipate the interaction between tasks during training in order to choose\na dynamic re-weighting of the various task losses, which will (i) lead to\nminimal worst-case loss and (ii) train on as many tasks as possible. After\ndemonstrating the efficacy of L-DRO on a small controlled synthetic setting, we\nevaluate it on two realistic benchmarks: a multitask version of the CIFAR-100\nimage classification dataset and a large-scale multilingual language modeling\nexperiment. Our empirical results show that L-DRO achieves a better trade-off\nbetween average and worst-case accuracy with little computational overhead\ncompared to several strong baselines.", "published": "2021-10-12 09:00:46", "link": "http://arxiv.org/abs/2110.05838v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Evaluation of Abstractive Summarisation Models with Machine Translation\n  in Deliberative Processes", "abstract": "We present work on summarising deliberative processes for non-English\nlanguages. Unlike commonly studied datasets, such as news articles, this\ndeliberation dataset reflects difficulties of combining multiple narratives,\nmostly of poor grammatical quality, in a single text. We report an extensive\nevaluation of a wide range of abstractive summarisation models in combination\nwith an off-the-shelf machine translation model. Texts are translated into\nEnglish, summarised, and translated back to the original language. We obtain\npromising results regarding the fluency, consistency and relevance of the\nsummaries produced. Our approach is easy to implement for many languages for\nproduction purposes by simply changing the translation model.", "published": "2021-10-12 09:23:57", "link": "http://arxiv.org/abs/2110.05847v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MetricGAN-U: Unsupervised speech enhancement/ dereverberation based only\n  on noisy/ reverberated speech", "abstract": "Most of the deep learning-based speech enhancement models are learned in a\nsupervised manner, which implies that pairs of noisy and clean speech are\nrequired during training. Consequently, several noisy speeches recorded in\ndaily life cannot be used to train the model. Although certain unsupervised\nlearning frameworks have also been proposed to solve the pair constraint, they\nstill require clean speech or noise for training. Therefore, in this paper, we\npropose MetricGAN-U, which stands for MetricGAN-unsupervised, to further\nrelease the constraint from conventional unsupervised learning. In MetricGAN-U,\nonly noisy speech is required to train the model by optimizing non-intrusive\nspeech quality metrics. The experimental results verified that MetricGAN-U\noutperforms baselines in both objective and subjective metrics.", "published": "2021-10-12 10:01:32", "link": "http://arxiv.org/abs/2110.05866v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "OpenHands: Making Sign Language Recognition Accessible with Pose-based\n  Pretrained Models across Languages", "abstract": "AI technologies for Natural Languages have made tremendous progress recently.\nHowever, commensurate progress has not been made on Sign Languages, in\nparticular, in recognizing signs as individual words or as complete sentences.\nWe introduce OpenHands, a library where we take four key ideas from the NLP\ncommunity for low-resource languages and apply them to sign languages for\nword-level recognition. First, we propose using pose extracted through\npretrained models as the standard modality of data to reduce training time and\nenable efficient inference, and we release standardized pose datasets for 6\ndifferent sign languages - American, Argentinian, Chinese, Greek, Indian, and\nTurkish. Second, we train and release checkpoints of 4 pose-based isolated sign\nlanguage recognition models across all 6 languages, providing baselines and\nready checkpoints for deployment. Third, to address the lack of labelled data,\nwe propose self-supervised pretraining on unlabelled data. We curate and\nrelease the largest pose-based pretraining dataset on Indian Sign Language\n(Indian-SL). Fourth, we compare different pretraining strategies and for the\nfirst time establish that pretraining is effective for sign language\nrecognition by demonstrating (a) improved fine-tuning performance especially in\nlow-resource settings, and (b) high crosslingual transfer from Indian-SL to few\nother sign languages. We open-source all models and datasets in OpenHands with\na hope that it makes research in sign languages more accessible, available here\nat https://github.com/AI4Bharat/OpenHands .", "published": "2021-10-12 10:33:02", "link": "http://arxiv.org/abs/2110.05877v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Word Order Does Not Matter For Speech Recognition", "abstract": "In this paper, we study training of automatic speech recognition system in a\nweakly supervised setting where the order of words in transcript labels of the\naudio training data is not known. We train a word-level acoustic model which\naggregates the distribution of all output frames using LogSumExp operation and\nuses a cross-entropy loss to match with the ground-truth words distribution.\nUsing the pseudo-labels generated from this model on the training set, we then\ntrain a letter-based acoustic model using Connectionist Temporal Classification\nloss. Our system achieves 2.3%/4.6% on test-clean/test-other subsets of\nLibriSpeech, which closely matches with the supervised baseline's performance.", "published": "2021-10-12 13:35:01", "link": "http://arxiv.org/abs/2110.05994v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Model-based analysis of brain activity reveals the hierarchy of language\n  in 305 subjects", "abstract": "A popular approach to decompose the neural bases of language consists in\ncorrelating, across individuals, the brain responses to different stimuli (e.g.\nregular speech versus scrambled words, sentences, or paragraphs). Although\nsuccessful, this `model-free' approach necessitates the acquisition of a large\nand costly set of neuroimaging data. Here, we show that a model-based approach\ncan reach equivalent results within subjects exposed to natural stimuli. We\ncapitalize on the recently-discovered similarities between deep language models\nand the human brain to compute the mapping between i) the brain responses to\nregular speech and ii) the activations of deep language models elicited by\nmodified stimuli (e.g. scrambled words, sentences, or paragraphs). Our\nmodel-based approach successfully replicates the seminal study of Lerner et al.\n(2011), which revealed the hierarchy of language areas by comparing the\nfunctional-magnetic resonance imaging (fMRI) of seven subjects listening to\n7min of both regular and scrambled narratives. We further extend and precise\nthese results to the brain signals of 305 individuals listening to 4.1 hours of\nnarrated stories. Overall, this study paves the way for efficient and flexible\nanalyses of the brain bases of language.", "published": "2021-10-12 15:30:21", "link": "http://arxiv.org/abs/2110.06078v1", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "q-bio.NC"}
{"title": "Regionalized models for Spanish language variations based on Twitter", "abstract": "Spanish is one of the most spoken languages in the globe, but not necessarily\nSpanish is written and spoken in the same way in different countries.\nUnderstanding local language variations can help to improve model performances\non regional tasks, both understanding local structures and also improving the\nmessage's content. For instance, think about a machine learning engineer who\nautomatizes some language classification task on a particular region or a\nsocial scientist trying to understand a regional event with echoes on social\nmedia; both can take advantage of dialect-based language models to understand\nwhat is happening with more contextual information hence more precision.\n  This manuscript presents and describes a set of regionalized resources for\nthe Spanish language built on four-year Twitter public messages geotagged in 26\nSpanish-speaking countries. We introduce word embeddings based on FastText,\nlanguage models based on BERT, and per-region sample corpora. We also provide a\nbroad comparison among regions covering lexical and semantical similarities; as\nwell as examples of using regional resources on message classification tasks.", "published": "2021-10-12 16:21:03", "link": "http://arxiv.org/abs/2110.06128v3", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Mention Memory: incorporating textual knowledge into Transformers\n  through entity mention attention", "abstract": "Natural language understanding tasks such as open-domain question answering\noften require retrieving and assimilating factual information from multiple\nsources. We propose to address this problem by integrating a semi-parametric\nrepresentation of a large text corpus into a Transformer model as a source of\nfactual knowledge. Specifically, our method represents knowledge with `mention\nmemory', a table of dense vector representations of every entity mention in a\ncorpus. The proposed model - TOME - is a Transformer that accesses the\ninformation through internal memory layers in which each entity mention in the\ninput passage attends to the mention memory. This approach enables synthesis of\nand reasoning over many disparate sources of information within a single\nTransformer model. In experiments using a memory of 150 million Wikipedia\nmentions, TOME achieves strong performance on several open-domain\nknowledge-intensive tasks, including the claim verification benchmarks HoVer\nand FEVER and several entity-based QA benchmarks. We also show that the model\nlearns to attend to informative mentions without any direct supervision.\nFinally we demonstrate that the model can generalize to new unseen entities by\nupdating the memory without retraining.", "published": "2021-10-12 17:19:05", "link": "http://arxiv.org/abs/2110.06176v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Brief Introduction to Automatic Differentiation for Machine Learning", "abstract": "Machine learning and neural network models in particular have been improving\nthe state of the art performance on many artificial intelligence related tasks.\nNeural network models are typically implemented using frameworks that perform\ngradient based optimization methods to fit a model to a dataset. These\nframeworks use a technique of calculating derivatives called automatic\ndifferentiation (AD) which removes the burden of performing derivative\ncalculations from the model designer. In this report we describe AD, its\nmotivations, and different implementation approaches. We briefly describe\ndataflow programming as it relates to AD. Lastly, we present example programs\nthat are implemented with Tensorflow and PyTorch, which are two commonly used\nAD frameworks.", "published": "2021-10-12 00:10:28", "link": "http://arxiv.org/abs/2110.06209v2", "categories": ["cs.LG", "cs.CL", "cs.MS", "cs.PL"], "primary_category": "cs.LG"}
{"title": "Speech Summarization using Restricted Self-Attention", "abstract": "Speech summarization is typically performed by using a cascade of speech\nrecognition and text summarization models. End-to-end modeling of speech\nsummarization models is challenging due to memory and compute constraints\narising from long input audio sequences. Recent work in document summarization\nhas inspired methods to reduce the complexity of self-attentions, which enables\ntransformer models to handle long sequences. In this work, we introduce a\nsingle model optimized end-to-end for speech summarization. We apply the\nrestricted self-attention technique from text-based models to speech models to\naddress the memory and compute constraints. We demonstrate that the proposed\nmodel learns to directly summarize speech for the How-2 corpus of instructional\nvideos. The proposed end-to-end model outperforms the previously proposed\ncascaded model by 3 points absolute on ROUGE. Further, we consider the spoken\nlanguage understanding task of predicting concepts from speech inputs and show\nthat the proposed end-to-end model outperforms the cascade model by 4 points\nabsolute F-1.", "published": "2021-10-12 18:21:23", "link": "http://arxiv.org/abs/2110.06263v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "S3PRL-VC: Open-source Voice Conversion Framework with Self-supervised\n  Speech Representations", "abstract": "This paper introduces S3PRL-VC, an open-source voice conversion (VC)\nframework based on the S3PRL toolkit. In the context of recognition-synthesis\nVC, self-supervised speech representation (S3R) is valuable in its potential to\nreplace the expensive supervised representation adopted by state-of-the-art VC\nsystems. Moreover, we claim that VC is a good probing task for S3R analysis. In\nthis work, we provide a series of in-depth analyses by benchmarking on the two\ntasks in VCC2020, namely intra-/cross-lingual any-to-one (A2O) VC, as well as\nan any-to-any (A2A) setting. We also provide comparisons between not only\ndifferent S3Rs but also top systems in VCC2020 with supervised representations.\nSystematic objective and subjective evaluation were conducted, and we show that\nS3R is comparable with VCC2020 top systems in the A2O setting in terms of\nsimilarity, and achieves state-of-the-art in S3R-based A2A VC. We believe the\nextensive analysis, as well as the toolkit itself, contribute to not only the\nS3R community but also the VC community. The codebase is now open-sourced.", "published": "2021-10-12 19:01:52", "link": "http://arxiv.org/abs/2110.06280v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fine-grained style control in Transformer-based Text-to-speech Synthesis", "abstract": "In this paper, we present a novel architecture to realize fine-grained style\ncontrol on the transformer-based text-to-speech synthesis (TransformerTTS).\nSpecifically, we model the speaking style by extracting a time sequence of\nlocal style tokens (LST) from the reference speech. The existing content\nencoder in TransformerTTS is then replaced by our designed cross-attention\nblocks for fusion and alignment between content and style. As the fusion is\nperformed along with the skip connection, our cross-attention block provides a\ngood inductive bias to gradually infuse the phoneme representation with a given\nstyle. Additionally, we prevent the style embedding from encoding linguistic\ncontent by randomly truncating LST during training and using wav2vec 2.0\nfeatures. Experiments show that with fine-grained style control, our system\nperforms better in terms of naturalness, intelligibility, and style\ntransferability. Our code and samples are publicly available.", "published": "2021-10-12 19:50:02", "link": "http://arxiv.org/abs/2110.06306v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring Wav2vec 2.0 fine-tuning for improved speech emotion\n  recognition", "abstract": "While Wav2Vec 2.0 has been proposed for speech recognition (ASR), it can also\nbe used for speech emotion recognition (SER); its performance can be\nsignificantly improved using different fine-tuning strategies. Two baseline\nmethods, vanilla fine-tuning (V-FT) and task adaptive pretraining (TAPT) are\nfirst presented. We show that V-FT is able to outperform state-of-the-art\nmodels on the IEMOCAP dataset. TAPT, an existing NLP fine-tuning strategy,\nfurther improves the performance on SER. We also introduce a novel fine-tuning\nmethod termed P-TAPT, which modifies the TAPT objective to learn contextualized\nemotion representations. Experiments show that P-TAPT performs better than\nTAPT, especially under low-resource settings. Compared to prior works in this\nliterature, our top-line system achieved a 7.4\\% absolute improvement in\nunweighted accuracy (UA) over the state-of-the-art performance on IEMOCAP. Our\ncode is publicly available.", "published": "2021-10-12 19:55:55", "link": "http://arxiv.org/abs/2110.06309v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Tell Me How to Survey: Literature Review Made Simple with Automatic\n  Reading Path Generation", "abstract": "Recent years have witnessed the dramatic growth of paper volumes with plenty\nof new research papers published every day, especially in the area of computer\nscience. How to glean papers worth reading from the massive literature to do a\nquick survey or keep up with the latest advancement about a specific research\ntopic has become a challenging task. Existing academic search engines such as\nGoogle Scholar return relevant papers by individually calculating the relevance\nbetween each paper and query. However, such systems usually omit the\nprerequisite chains of a research topic and cannot form a meaningful reading\npath. In this paper, we introduce a new task named Reading Path Generation\n(RPG) which aims at automatically producing a path of papers to read for a\ngiven query. To serve as a research benchmark, we further propose SurveyBank, a\ndataset consisting of large quantities of survey papers in the field of\ncomputer science as well as their citation relationships. Each survey paper\ncontains key phrases extracted from its title and multi-level reading lists\ninferred from its references. Furthermore, we propose a\ngraph-optimization-based approach for reading path generation which takes the\nrelationship between papers into account. Extensive evaluations demonstrate\nthat our approach outperforms other baselines. A Real-time Reading Path\nGeneration System (RePaGer) has been also implemented with our designed model.\nTo the best of our knowledge, we are the first to target this important\nresearch problem. Our source code of RePaGer system and SurveyBank dataset can\nbe found on here.", "published": "2021-10-12 20:58:46", "link": "http://arxiv.org/abs/2110.06354v3", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text\n  Extractive Summarization", "abstract": "To capture the semantic graph structure from raw text, most existing\nsummarization approaches are built on GNNs with a pre-trained model. However,\nthese methods suffer from cumbersome procedures and inefficient computations\nfor long-text documents. To mitigate these issues, this paper proposes\nHETFORMER, a Transformer-based pre-trained model with multi-granularity sparse\nattentions for long-text extractive summarization. Specifically, we model\ndifferent types of semantic nodes in raw text as a potential heterogeneous\ngraph and directly learn heterogeneous relationships (edges) among nodes by\nTransformer. Extensive experiments on both single- and multi-document\nsummarization tasks show that HETFORMER achieves state-of-the-art performance\nin Rouge F1 while using less memory and fewer parameters.", "published": "2021-10-12 22:42:31", "link": "http://arxiv.org/abs/2110.06388v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Overview of Ontologies and Tool Support for COVID-19 Analytics", "abstract": "The outbreak of the SARS-CoV-2 pandemic of the new COVID-19 disease (COVID-19\nfor short) demands empowering existing medical, economic, and social emergency\nbackend systems with data analytics capabilities. An impediment in taking\nadvantages of data analytics in these systems is the lack of a unified\nframework or reference model. Ontologies are highlighted as a promising\nsolution to bridge this gap by providing a formal representation of COVID-19\nconcepts such as symptoms, infections rate, contact tracing, and drug\nmodelling. Ontology-based solutions enable the integration of diverse data\nsources that leads to a better understanding of pandemic data, management of\nsmart lockdowns by identifying pandemic hotspots, and knowledge-driven\ninference, reasoning, and recommendations to tackle surrounding issues.", "published": "2021-10-12 23:20:37", "link": "http://arxiv.org/abs/2110.06397v1", "categories": ["cs.SE", "cs.CL", "cs.SC"], "primary_category": "cs.SE"}
{"title": "A Survey on Legal Question Answering Systems", "abstract": "Many legal professionals think that the explosion of information about local,\nregional, national, and international legislation makes their practice more\ncostly, time-consuming, and even error-prone. The two main reasons for this are\nthat most legislation is usually unstructured, and the tremendous amount and\npace with which laws are released causes information overload in their daily\ntasks. In the case of the legal domain, the research community agrees that a\nsystem allowing to generate automatic responses to legal questions could\nsubstantially impact many practical implications in daily activities. The\ndegree of usefulness is such that even a semi-automatic solution could\nsignificantly help to reduce the workload to be faced. This is mainly because a\nQuestion Answering system could be able to automatically process a massive\namount of legal resources to answer a question or doubt in seconds, which means\nthat it could save resources in the form of effort, money, and time to many\nprofessionals in the legal sector. In this work, we quantitatively and\nqualitatively survey the solutions that currently exist to meet this challenge.", "published": "2021-10-12 17:51:56", "link": "http://arxiv.org/abs/2110.07333v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Deep Learning for Bias Detection: From Inception to Deployment", "abstract": "To create a more inclusive workplace, enterprises are actively investing in\nidentifying and eliminating unconscious bias (e.g., gender, race, age,\ndisability, elitism and religion) across their various functions. We propose a\ndeep learning model with a transfer learning based language model to learn from\nmanually tagged documents for automatically identifying bias in enterprise\ncontent. We first pretrain a deep learning-based language-model using\nWikipedia, then fine tune the model with a large unlabelled data set related\nwith various types of enterprise content. Finally, a linear layer followed by\nsoftmax layer is added at the end of the language model and the model is\ntrained on a labelled bias dataset consisting of enterprise content. The\ntrained model is thoroughly evaluated on independent datasets to ensure a\ngeneral application. We present the proposed method and its deployment detail\nin a real-world application.", "published": "2021-10-12 13:57:54", "link": "http://arxiv.org/abs/2110.15728v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Foster Strengths and Circumvent Weaknesses: a Speech Enhancement\n  Framework with Two-branch Collaborative Learning", "abstract": "Recent single-channel speech enhancement methods usually convert waveform to\nthe time-frequency domain and use magnitude/complex spectrum as the optimizing\ntarget. However, both magnitude-spectrum-based methods and\ncomplex-spectrum-based methods have their respective pros and cons. In this\npaper, we propose a unified two-branch framework to foster strengths and\ncircumvent weaknesses of different paradigms. The proposed framework could take\nfull advantage of the apparent spectral regularity in magnitude spectrogram and\nbreak the bottleneck that magnitude-based methods have suffered. Within each\nbranch, we use collaborative expert block and its variants as substitutes for\nregular convolution layers. Experiments on TIMIT benchmark demonstrate that our\nmethod is superior to existing state-of-the-art ones.", "published": "2021-10-12 03:05:06", "link": "http://arxiv.org/abs/2110.05713v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Large-scale Self-Supervised Speech Representation Learning for Automatic\n  Speaker Verification", "abstract": "The speech representations learned from large-scale unlabeled data have shown\nbetter generalizability than those from supervised learning and thus attract a\nlot of interest to be applied for various downstream tasks. In this paper, we\nexplore the limits of speech representations learned by different\nself-supervised objectives and datasets for automatic speaker verification\n(ASV), especially with a well-recognized SOTA ASV model, ECAPA-TDNN [1], as a\ndownstream model. The representations from all hidden layers of the pre-trained\nmodel are firstly averaged with learnable weights and then fed into the\nECAPA-TDNN as input features. The experimental results on Voxceleb dataset show\nthat the weighted average representation is significantly superior to FBank, a\nconventional handcrafted feature for ASV. Our best single system achieves\n0.537%, 0.569%, and 1.180% equal error rate (EER) on the three official trials\nof VoxCeleb1, separately. Accordingly, the ensemble system with three\npre-trained models can further improve the EER to 0.479%, 0.536% and 1.023%.\nAmong the three evaluation trials, our best system outperforms the winner\nsystem [2] of the VoxCeleb Speaker Recognition Challenge 2021 (VoxSRC2021) on\nthe VoxCeleb1-E trial.", "published": "2021-10-12 07:15:21", "link": "http://arxiv.org/abs/2110.05777v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A bridge between features and evidence for binary attribute-driven\n  perfect privacy", "abstract": "Attribute-driven privacy aims to conceal a single user's attribute, contrary\nto anonymisation that tries to hide the full identity of the user in some data.\nWhen the attribute to protect from malicious inferences is binary, perfect\nprivacy requires the log-likelihood-ratio to be zero resulting in no\nstrength-of-evidence. This work presents an approach based on normalizing flow\nthat maps a feature vector into a latent space where the evidence, related to\nthe binary attribute, and an independent residual are disentangled. It can be\nseen as a non-linear discriminant analysis where the mapping is invertible\nallowing generation by mapping the latent variable back to the original space.\nThis framework allows to manipulate the log-likelihood-ratio of the data and\ntherefore allows to set it to zero for privacy. We show the applicability of\nthe approach on an attribute-driven privacy task where the sex information is\nremoved from speaker embeddings. Results on VoxCeleb2 dataset show the\nefficiency of the method that outperforms in terms of privacy and utility our\nprevious experiments based on adversarial disentanglement.", "published": "2021-10-12 09:02:30", "link": "http://arxiv.org/abs/2110.05840v2", "categories": ["cs.CR", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Multi-channel Narrow-band Deep Speech Separation with Full-band\n  Permutation Invariant Training", "abstract": "This paper addresses the problem of multi-channel multi-speech separation\nbased on deep learning techniques. In the short time Fourier transform domain,\nwe propose an end-to-end narrow-band network that directly takes as input the\nmulti-channel mixture signals of one frequency, and outputs the separated\nsignals of this frequency. In narrow-band, the spatial information (or\ninter-channel difference) can well discriminate between speakers at different\npositions. This information is intensively used in many narrow-band speech\nseparation methods, such as beamforming and clustering of spatial vectors. The\nproposed network is trained to learn a rule to automatically exploit this\ninformation and perform speech separation. Such a rule should be valid for any\nfrequency, thence the network is shared by all frequencies. In addition, a\nfull-band permutation invariant training criterion is proposed to solve the\nfrequency permutation problem encountered by most narrow-band methods.\nExperiments show that, by focusing on deeply learning the narrow-band\ninformation, the proposed method outperforms the oracle beamforming method and\nthe state-of-the-art deep learning based method.", "published": "2021-10-12 12:45:14", "link": "http://arxiv.org/abs/2110.05966v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Character Error Rate Is Not Equal to Having Clean Speech:\n  Speech Enhancement for ASR Systems with Black-box Acoustic Models", "abstract": "A deep neural network (DNN)-based speech enhancement (SE) aiming to maximize\nthe performance of an automatic speech recognition (ASR) system is proposed in\nthis paper. In order to optimize the DNN-based SE model in terms of the\ncharacter error rate (CER), which is one of the metric to evaluate the ASR\nsystem and generally non-differentiable, our method uses two DNNs: one for\nspeech processing and one for mimicking the output CERs derived through an\nacoustic model (AM). Then both of DNNs are alternately optimized in the\ntraining phase. Even if the AM is a black-box, e.g., like one provided by a\nthird-party, the proposed method enables the DNN-based SE model to be optimized\nin terms of the CER since the DNN mimicking the AM is differentiable.\nConsequently, it becomes feasible to build CER-centric SE model that has no\nnegative effect, e.g., additional calculation cost and changing network\narchitecture, on the inference phase since our method is merely a training\nscheme for the existing DNN-based methods. Experimental results show that our\nmethod improved CER by 8.8% relative derived through a black-box AM although\ncertain noise levels are kept.", "published": "2021-10-12 12:51:53", "link": "http://arxiv.org/abs/2110.05968v2", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Multi-Channel Far-Field Speaker Verification with Large-Scale Ad-hoc\n  Microphone Arrays", "abstract": "Speaker verification based on ad-hoc microphone arrays has the potential of\nreducing the error significantly in adverse acoustic environments. However,\nexisting approaches extract utterance-level speaker embeddings from each\nchannel of an ad-hoc microphone array, which does not consider fully the\nspatial-temporal information across the devices. In this paper, we propose to\naggregate the multichannel signals of the ad-hoc microphone array at the\nframe-level by exploring the cross-channel information deeply with two\nattention mechanisms. The first one is a self-attention method. It consists of\na cross-frame self-attention layer and a cross-channel self-attention layer\nsuccessively, both working at the frame level. The second one learns the\ncross-frame and cross-channel information via two graph attention layers.\nExperimental results demonstrate that the proposed methods reach the\nstate-of-the-art performance. Moreover, the graph-attention method is better\nthan the self-attention method in most cases.", "published": "2021-10-12 13:04:32", "link": "http://arxiv.org/abs/2110.05975v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "COVID-19 Diagnosis from Cough Acoustics using ConvNets and Data\n  Augmentation", "abstract": "With the periodic rise and fall of COVID-19 and countries being inflicted by\nits waves, an efficient, economic, and effortless diagnosis procedure for the\nvirus has been the utmost need of the hour. COVID-19 positive individuals may\neven be asymptomatic making the diagnosis difficult, but amongst the infected\nsubjects, the asymptomatic ones need not be entirely free of symptoms caused by\nthe virus. They might not show any observable symptoms like the symptomatic\nsubjects, but they may differ from uninfected ones in the way they cough. These\ndifferences in the coughing sounds are minute and indiscernible to the human\near, however, these can be captured using machine learning-based statistical\nmodels. In this paper, we present a deep learning approach to analyze the\nacoustic dataset provided in Track 1 of the DiCOVA 2021 Challenge containing\ncough sound recordings belonging to both COVID-19 positive and negative\nexamples. To perform the classification on the sound recordings as belonging to\na COVID-19 positive or negative examples, we propose a ConvNet model. Our model\nachieved an AUC score percentage of 72.23 on the blind test set provided by the\nsame for an unbiased evaluation of the models. The ConvNet model incorporated\nwith Data Augmentation further increased the AUC-ROC percentage from 72.23 to\n87.07. It also outperformed the DiCOVA 2021 Challenge's baseline model by 23%\nthus, claiming the top position on the DiCOVA 2021 Challenge leaderboard. This\npaper proposes the use of Mel frequency cepstral coefficients as the feature\ninput for the proposed model.", "published": "2021-10-12 16:11:49", "link": "http://arxiv.org/abs/2110.06123v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Annihilating Filter-Based DOA Estimation for Uniform Linear Array", "abstract": "In this paper, we propose a new method to design an annihilating filter (AF)\nfor direction-of-arrival (DOA) estimation of multiple snapshots within an\nuniform linear array. To evaluate the proposed method, we firstly design a DOA\nestimation using multiple signal classification (MUSIC) algorithm, referred to\nas the MUSIC baseline. We then compare the proposed method with the MUSIC\nbaseline in two environmental noise conditions: Only white noise, or both white\nnoise and diffusion. The experimental results highlight two main contributions;\nthe first is to modify conventional MUSIC algorithm for adapting different\nnoise conditions, and the second is to propose an AF-based method that shows\ncompetitive accuracy of arrival angles detected and low complexity compared\nwith the MUSIC baseline.", "published": "2021-10-12 20:16:53", "link": "http://arxiv.org/abs/2110.06323v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Mirrornet : Learning Audio Synthesizer Controls Inspired by\n  Sensorimotor Interaction", "abstract": "Experiments to understand the sensorimotor neural interactions in the human\ncortical speech system support the existence of a bidirectional flow of\ninteractions between the auditory and motor regions. Their key function is to\nenable the brain to `learn' how to control the vocal tract for speech\nproduction. This idea is the impetus for the recently proposed \"MirrorNet\", a\nconstrained autoencoder architecture. In this paper, the MirrorNet is applied\nto learn, in an unsupervised manner, the controls of a specific audio\nsynthesizer (DIVA) to produce melodies only from their auditory spectrograms.\nThe results demonstrate how the MirrorNet discovers the synthesizer parameters\nto generate the melodies that closely resemble the original and those of unseen\nmelodies, and even determine the best set parameters to approximate renditions\nof complex piano melodies generated by a different synthesizer. This\ngeneralizability of the MirrorNet illustrates its potential to discover from\nsensory data the controls of arbitrary motor-plants.", "published": "2021-10-12 02:28:49", "link": "http://arxiv.org/abs/2110.05695v4", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Music Sentiment Transfer", "abstract": "Music sentiment transfer is a completely novel task. Sentiment transfer is a\nnatural evolution of the heavily-studied style transfer task, as sentiment\ntransfer is rooted in applying the sentiment of a source to be the new\nsentiment for a target piece of media; yet compared to style transfer,\nsentiment transfer has been only scantily studied on images. Music sentiment\ntransfer attempts to apply the high level objective of sentiment transfer to\nthe domain of music. We propose CycleGAN to bridge disparate domains. In order\nto use the network, we choose to use symbolic, MIDI, data as the music format.\nThrough the use of a cycle consistency loss, we are able to create one-to-one\nmappings that preserve the content and realism of the source data. Results and\nliterature suggest that the task of music sentiment transfer is more difficult\nthan image sentiment transfer because of the temporal characteristics of music\nand lack of existing datasets.", "published": "2021-10-12 06:51:38", "link": "http://arxiv.org/abs/2110.05765v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving the Performance of Automated Audio Captioning via Integrating\n  the Acoustic and Semantic Information", "abstract": "Automated audio captioning (AAC) has developed rapidly in recent years,\ninvolving acoustic signal processing and natural language processing to\ngenerate human-readable sentences for audio clips. The current models are\ngenerally based on the neural encoder-decoder architecture, and their decoder\nmainly uses acoustic information that is extracted from the CNN-based encoder.\nHowever, they have ignored semantic information that could help the AAC model\nto generate meaningful descriptions. This paper proposes a novel approach for\nautomated audio captioning based on incorporating semantic and acoustic\ninformation. Specifically, our audio captioning model consists of two\nsub-modules. (1) The pre-trained keyword encoder utilizes pre-trained ResNet38\nto initialize its parameters, and then it is trained by extracted keywords as\nlabels. (2) The multi-modal attention decoder adopts an LSTM-based decoder that\ncontains semantic and acoustic attention modules. Experiments demonstrate that\nour proposed model achieves state-of-the-art performance on the Clotho dataset.\nOur code can be found at https://github.com/WangHelin1997/DCASE2021_Task6_PKU", "published": "2021-10-12 15:49:35", "link": "http://arxiv.org/abs/2110.06100v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spatial mixup: Directional loudness modification as data augmentation\n  for sound event localization and detection", "abstract": "Data augmentation methods have shown great importance in diverse supervised\nlearning problems where labeled data is scarce or costly to obtain. For sound\nevent localization and detection (SELD) tasks several augmentation methods have\nbeen proposed, with most borrowing ideas from other domains such as images,\nspeech, or monophonic audio. However, only a few exploit the spatial properties\nof a full 3D audio scene. We propose Spatial Mixup, as an application of\nparametric spatial audio effects for data augmentation, which modifies the\ndirectional properties of a multi-channel spatial audio signal encoded in the\nambisonics domain. Similarly to beamforming, these modifications enhance or\nsuppress signals arriving from certain directions, although the effect is less\npronounced. Therefore enabling deep learning models to achieve invariance to\nsmall spatial perturbations. The method is evaluated with experiments in the\nDCASE 2021 Task 3 dataset, where spatial mixup increases performance over a\nnon-augmented baseline, and compares to other well known augmentation methods.\nFurthermore, combining spatial mixup with other methods greatly improves\nperformance.", "published": "2021-10-12 16:16:58", "link": "http://arxiv.org/abs/2110.06126v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Generalized Time Domain Velocity Vector", "abstract": "We introduce and analyze Generalized Time Domain Velocity Vector (GTVV), an\nextension of the previously presented acoustic multipath footprint extracted\nfrom the Ambisonic recordings. GTVV is better adapted to adverse acoustic\nconditions, and enables efficient parameter estimation of multiple plane wave\ncomponents in the recorded multichannel mixture. Experiments on simulated data\nconfirm the predicted theoretical advantages of these new spatio-temporal\nfeatures.", "published": "2021-10-12 19:40:25", "link": "http://arxiv.org/abs/2110.06304v4", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Algorithmic Composition by Autonomous Systems with Multiple Time-Scales", "abstract": "Dynamic systems have found their use in sound synthesis as well as score\nsynthesis. These levels can be integrated in monolithic autonomous systems in a\nnovel approach to algorithmic composition that shares certain aesthetic\nmotivations with some work with autonomous music systems, such as the search\nfor emergence. We discuss various strategies for achieving variation on\nmultiple time-scales by using slow-fast, hybrid dynamic systems, and\nstatistical feedback. The ideas are illustrated with a case study.", "published": "2021-10-12 21:32:17", "link": "http://arxiv.org/abs/2110.06371v1", "categories": ["cs.SD", "eess.AS", "nlin.AO"], "primary_category": "cs.SD"}
{"title": "Multi-Modal Pre-Training for Automated Speech Recognition", "abstract": "Traditionally, research in automated speech recognition has focused on\nlocal-first encoding of audio representations to predict the spoken phonemes in\nan utterance. Unfortunately, approaches relying on such hyper-local information\ntend to be vulnerable to both local-level corruption (such as audio-frame\ndrops, or loud noises) and global-level noise (such as environmental noise, or\nbackground noise) that has not been seen during training. In this work, we\nintroduce a novel approach which leverages a self-supervised learning technique\nbased on masked language modeling to compute a global, multi-modal encoding of\nthe environment in which the utterance occurs. We then use a new deep-fusion\nframework to integrate this global context into a traditional ASR method, and\ndemonstrate that the resulting method can outperform baseline methods by up to\n7% on Librispeech; gains on internal datasets range from 6% (on larger models)\nto 45% (on smaller models).", "published": "2021-10-12 17:07:25", "link": "http://arxiv.org/abs/2110.09890v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
