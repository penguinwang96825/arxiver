{"title": "Distributed Marker Representation for Ambiguous Discourse Markers and\n  Entangled Relations", "abstract": "Discourse analysis is an important task because it models intrinsic semantic\nstructures between sentences in a document. Discourse markers are natural\nrepresentations of discourse in our daily language. One challenge is that the\nmarkers as well as pre-defined and human-labeled discourse relations can be\nambiguous when describing the semantics between sentences. We believe that a\nbetter approach is to use a contextual-dependent distribution over the markers\nto express discourse information. In this work, we propose to learn a\nDistributed Marker Representation (DMR) by utilizing the (potentially)\nunlimited discourse marker data with a latent discourse sense, thereby bridging\nmarkers with sentence pairs. Such representations can be learned automatically\nfrom data without supervision, and in turn provide insights into the data\nitself. Experiments show the SOTA performance of our DMR on the implicit\ndiscourse relation recognition task and strong interpretability. Our method\nalso offers a valuable tool to understand complex ambiguity and entanglement\namong discourse markers and manually defined discourse relations.", "published": "2023-06-19 00:49:51", "link": "http://arxiv.org/abs/2306.10658v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jamp: Controlled Japanese Temporal Inference Dataset for Evaluating\n  Generalization Capacity of Language Models", "abstract": "Natural Language Inference (NLI) tasks involving temporal inference remain\nchallenging for pre-trained language models (LMs). Although various datasets\nhave been created for this task, they primarily focus on English and do not\naddress the need for resources in other languages. It is unclear whether\ncurrent LMs realize the generalization capacity for temporal inference across\nlanguages. In this paper, we present Jamp, a Japanese NLI benchmark focused on\ntemporal inference. Our dataset includes a range of temporal inference\npatterns, which enables us to conduct fine-grained analysis. To begin the data\nannotation process, we create diverse inference templates based on the formal\nsemantics test suites. We then automatically generate diverse NLI examples by\nusing the Japanese case frame dictionary and well-designed templates while\ncontrolling the distribution of inference patterns and gold labels. We evaluate\nthe generalization capacities of monolingual/multilingual LMs by splitting our\ndataset based on tense fragments (i.e., temporal inference patterns). Our\nfindings demonstrate that LMs struggle with specific linguistic phenomena, such\nas habituality, indicating that there is potential for the development of more\neffective NLI models across languages.", "published": "2023-06-19 07:00:14", "link": "http://arxiv.org/abs/2306.10727v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Open-domain Keyphrase Generation", "abstract": "In this work, we study the problem of unsupervised open-domain keyphrase\ngeneration, where the objective is a keyphrase generation model that can be\nbuilt without using human-labeled data and can perform consistently across\ndomains. To solve this problem, we propose a seq2seq model that consists of two\nmodules, namely \\textit{phraseness} and \\textit{informativeness} module, both\nof which can be built in an unsupervised and open-domain fashion. The\nphraseness module generates phrases, while the informativeness module guides\nthe generation towards those that represent the core concepts of the text. We\nthoroughly evaluate our proposed method using eight benchmark datasets from\ndifferent domains. Results on in-domain datasets show that our approach\nachieves state-of-the-art results compared with existing unsupervised models,\nand overall narrows the gap between supervised and unsupervised methods down to\nabout 16\\%. Furthermore, we demonstrate that our model performs consistently\nacross domains, as it overall surpasses the baselines on out-of-domain\ndatasets.", "published": "2023-06-19 07:57:13", "link": "http://arxiv.org/abs/2306.10755v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Ordered Information Extraction with Deep Reinforcement Learning", "abstract": "Information extraction (IE) has been studied extensively. The existing\nmethods always follow a fixed extraction order for complex IE tasks with\nmultiple elements to be extracted in one instance such as event extraction.\nHowever, we conduct experiments on several complex IE datasets and observe that\ndifferent extraction orders can significantly affect the extraction results for\na great portion of instances, and the ratio of sentences that are sensitive to\nextraction orders increases dramatically with the complexity of the IE task.\nTherefore, this paper proposes a novel adaptive ordered IE paradigm to find the\noptimal element extraction order for different instances, so as to achieve the\nbest extraction results. We also propose an reinforcement learning (RL) based\nframework to generate optimal extraction order for each instance dynamically.\nAdditionally, we propose a co-training framework adapted to RL to mitigate the\nexposure bias during the extractor training phase. Extensive experiments\nconducted on several public datasets demonstrate that our proposed method can\nbeat previous methods and effectively improve the performance of various IE\ntasks, especially for complex ones.", "published": "2023-06-19 08:58:56", "link": "http://arxiv.org/abs/2306.10787v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Few-Shot Learning via Language Model Retrieval", "abstract": "Transformer-based language models have achieved remarkable success in\nfew-shot in-context learning and drawn a lot of research interest. However,\nthese models' performance greatly depends on the choice of the example prompts\nand also has high variability depending on how samples are chosen. In this\npaper, we conduct a comprehensive study of retrieving semantically similar\nfew-shot samples and using them as the context, as it helps the model decide\nthe correct label without any gradient update in the multilingual and\ncross-lingual settings. We evaluate the proposed method on five natural\nlanguage understanding datasets related to intent detection, question\nclassification, sentiment analysis, and topic classification. The proposed\nmethod consistently outperforms random sampling in monolingual and\ncross-lingual tasks in non-English languages.", "published": "2023-06-19 14:27:21", "link": "http://arxiv.org/abs/2306.10964v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning Language Models for Scientific Writing Support", "abstract": "We support scientific writers in determining whether a written sentence is\nscientific, to which section it belongs, and suggest paraphrasings to improve\nthe sentence. Firstly, we propose a regression model trained on a corpus of\nscientific sentences extracted from peer-reviewed scientific papers and\nnon-scientific text to assign a score that indicates the scientificness of a\nsentence. We investigate the effect of equations and citations on this score to\ntest the model for potential biases. Secondly, we create a mapping of section\ntitles to a standard paper layout in AI and machine learning to classify a\nsentence to its most likely section. We study the impact of context, i.e.,\nsurrounding sentences, on the section classification performance. Finally, we\npropose a paraphraser, which suggests an alternative for a given sentence that\nincludes word substitutions, additions to the sentence, and structural changes\nto improve the writing style. We train various large language models on\nsentences extracted from arXiv papers that were peer reviewed and published at\nA*, A, B, and C ranked conferences. On the scientificness task, all models\nachieve an MSE smaller than $2\\%$. For the section classification, BERT\noutperforms WideMLP and SciBERT in most cases. We demonstrate that using\ncontext enhances the classification of a sentence, achieving up to a $90\\%$\nF1-score. Although the paraphrasing models make comparatively few alterations,\nthey produce output sentences close to the gold standard. Large fine-tuned\nmodels such as T5 Large perform best in experiments considering various\nmeasures of difference between input sentence and gold standard. Code is\nprovided under https://github.com/JustinMuecke/SciSen.", "published": "2023-06-19 14:34:49", "link": "http://arxiv.org/abs/2306.10974v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual-Gated Fusion with Prefix-Tuning for Multi-Modal Relation Extraction", "abstract": "Multi-Modal Relation Extraction (MMRE) aims at identifying the relation\nbetween two entities in texts that contain visual clues. Rich visual content is\nvaluable for the MMRE task, but existing works cannot well model finer\nassociations among different modalities, failing to capture the truly helpful\nvisual information and thus limiting relation extraction performance. In this\npaper, we propose a novel MMRE framework to better capture the deeper\ncorrelations of text, entity pair, and image/objects, so as to mine more\nhelpful information for the task, termed as DGF-PT. We first propose a\nprompt-based autoregressive encoder, which builds the associations of\nintra-modal and inter-modal features related to the task, respectively by\nentity-oriented and object-oriented prefixes. To better integrate helpful\nvisual information, we design a dual-gated fusion module to distinguish the\nimportance of image/objects and further enrich text representations. In\naddition, a generative decoder is introduced with entity type restriction on\nrelations, better filtering out candidates. Extensive experiments conducted on\nthe benchmark dataset show that our approach achieves excellent performance\ncompared to strong competitors, even in the few-shot situation.", "published": "2023-06-19 15:31:34", "link": "http://arxiv.org/abs/2306.11020v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Frequency effects in Linear Discriminative Learning", "abstract": "Word frequency is a strong predictor in most lexical processing tasks. Thus,\nany model of word recognition needs to account for how word frequency effects\narise. The Discriminative Lexicon Model (DLM; Baayen et al., 2018a, 2019)\nmodels lexical processing with linear mappings between words' forms and their\nmeanings. So far, the mappings can either be obtained incrementally via\nerror-driven learning, a computationally expensive process able to capture\nfrequency effects, or in an efficient, but frequency-agnostic solution\nmodelling the theoretical endstate of learning (EL) where all words are learned\noptimally. In this study we show how an efficient, yet frequency-informed\nmapping between form and meaning can be obtained (Frequency-informed learning;\nFIL). We find that FIL well approximates an incremental solution while being\ncomputationally much cheaper. FIL shows a relatively low type- and high\ntoken-accuracy, demonstrating that the model is able to process most word\ntokens encountered by speakers in daily life correctly. We use FIL to model\nreaction times in the Dutch Lexicon Project (Keuleers et al., 2010) and find\nthat FIL predicts well the S-shaped relationship between frequency and the mean\nof reaction times but underestimates the variance of reaction times for low\nfrequency words. FIL is also better able to account for priming effects in an\nauditory lexical decision task in Mandarin Chinese (Lee, 2007), compared to EL.\nFinally, we used ordered data from CHILDES (Brown, 1973; Demuth et al., 2006)\nto compare mappings obtained with FIL and incremental learning. The mappings\nare highly correlated, but with FIL some nuances based on word ordering effects\nare lost. Our results show how frequency effects in a learning model can be\nsimulated efficiently, and raise questions about how to best account for\nlow-frequency words in cognitive models.", "published": "2023-06-19 16:15:46", "link": "http://arxiv.org/abs/2306.11044v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BioREx: Improving Biomedical Relation Extraction by Leveraging\n  Heterogeneous Datasets", "abstract": "Biomedical relation extraction (RE) is the task of automatically identifying\nand characterizing relations between biomedical concepts from free text. RE is\na central task in biomedical natural language processing (NLP) research and\nplays a critical role in many downstream applications, such as literature-based\ndiscovery and knowledge graph construction. State-of-the-art methods were used\nprimarily to train machine learning models on individual RE datasets, such as\nprotein-protein interaction and chemical-induced disease relation. Manual\ndataset annotation, however, is highly expensive and time-consuming, as it\nrequires domain knowledge. Existing RE datasets are usually domain-specific or\nsmall, which limits the development of generalized and high-performing RE\nmodels. In this work, we present a novel framework for systematically\naddressing the data heterogeneity of individual datasets and combining them\ninto a large dataset. Based on the framework and dataset, we report on BioREx,\na data-centric approach for extracting relations. Our evaluation shows that\nBioREx achieves significantly higher performance than the benchmark system\ntrained on the individual dataset, setting a new SOTA from 74.4% to 79.6% in\nF-1 measure on the recently released BioRED corpus. We further demonstrate that\nthe combined dataset can improve performance for five different RE tasks. In\naddition, we show that on average BioREx compares favorably to current\nbest-performing methods such as transfer learning and multi-task learning.\nFinally, we demonstrate BioREx's robustness and generalizability in two\nindependent RE tasks not previously seen in training data: drug-drug N-ary\ncombination and document-level gene-disease RE. The integrated dataset and\noptimized method have been packaged as a stand-alone tool available at\nhttps://github.com/ncbi/BioREx.", "published": "2023-06-19 22:48:18", "link": "http://arxiv.org/abs/2306.11189v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gender Differences in Abuse: The Case of Dutch Politicians on Twitter", "abstract": "Online abuse and threats towards politicians have become a significant\nconcern in the Netherlands, like in many other countries across the world. This\npaper analyses gender differences in abuse received by Dutch politicians on\nTwitter, while taking into account the possible additional impact of ethnic\nminority status. All tweets directed at party leaders throughout the entire\nyear of 2022 were collected. The effect of gender and ethnic minority status\nwere estimated for six different linguistic measures of abuse, namely,\ntoxicity, severe toxicity, identity attacks, profanity, insults, and threats.\nContrary to expectations, male politicians received higher levels of all forms\nof abuse, with the exception of threats, for which no significant gender\ndifference was found. Significant interaction effects between gender and ethnic\nminority status were found for a number of abuse measures. In the case of\nsevere toxicity, identity attacks, and profanity, female ethnic minority\npoliticians were more severely impacted than their ethnic majority female\ncolleagues, but not worse than male politicians. Finally, female ethnic\nminority politicians received the highest levels of threats compared to all\ngroups. Given that online abuse and threats are reported to have a negative\neffect on political participation and retention, these results are particularly\nworrying.", "published": "2023-06-19 08:23:24", "link": "http://arxiv.org/abs/2306.10769v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "AMRs Assemble! Learning to Ensemble with Autoregressive Models for AMR\n  Parsing", "abstract": "In this paper, we examine the current state-of-the-art in AMR parsing, which\nrelies on ensemble strategies by merging multiple graph predictions. Our\nanalysis reveals that the present models often violate AMR structural\nconstraints. To address this issue, we develop a validation method, and show\nhow ensemble models can exploit SMATCH metric weaknesses to obtain higher\nscores, but sometimes result in corrupted graphs. Additionally, we highlight\nthe demanding need to compute the SMATCH score among all possible predictions.\nTo overcome these challenges, we propose two novel ensemble strategies based on\nTransformer models, improving robustness to structural constraints, while also\nreducing the computational time. Our methods provide new insights for enhancing\nAMR parsers and metrics. Our code is available at\n\\href{https://www.github.com/babelscape/AMRs-Assemble}{github.com/babelscape/AMRs-Assemble}.", "published": "2023-06-19 08:58:47", "link": "http://arxiv.org/abs/2306.10786v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Preserving Commonsense Knowledge from Pre-trained Language Models via\n  Causal Inference", "abstract": "Fine-tuning has been proven to be a simple and effective technique to\ntransfer the learned knowledge of Pre-trained Language Models (PLMs) to\ndownstream tasks. However, vanilla fine-tuning easily overfits the target data\nand degrades the generalization ability. Most existing studies attribute it to\ncatastrophic forgetting, and they retain the pre-trained knowledge\nindiscriminately without identifying what knowledge is transferable. Motivated\nby this, we frame fine-tuning into a causal graph and discover that the crux of\ncatastrophic forgetting lies in the missing causal effects from the pretrained\ndata. Based on the causal view, we propose a unified objective for fine-tuning\nto retrieve the causality back. Intriguingly, the unified objective can be seen\nas the sum of the vanilla fine-tuning objective, which learns new knowledge\nfrom target data, and the causal objective, which preserves old knowledge from\nPLMs. Therefore, our method is flexible and can mitigate negative transfer\nwhile preserving knowledge. Since endowing models with commonsense is a\nlong-standing challenge, we implement our method on commonsense QA with a\nproposed heuristic estimation to verify its effectiveness. In the experiments,\nour method outperforms state-of-the-art fine-tuning methods on all six\ncommonsense QA datasets and can be implemented as a plug-in module to inflate\nthe performance of existing QA models.", "published": "2023-06-19 09:06:44", "link": "http://arxiv.org/abs/2306.10790v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Grammatical gender in Swedish is predictable using recurrent neural\n  networks", "abstract": "The grammatical gender of Swedish nouns is a mystery. While there are few\nrules that can indicate the gender with some certainty, it does in general not\ndepend on either meaning or the structure of the word. In this paper we\ndemonstrate the surprising fact that grammatical gender for Swedish nouns can\nbe predicted with high accuracy using a recurrent neural network (RNN) working\non the raw character sequence of the word, without using any contextual\ninformation.", "published": "2023-06-19 11:42:47", "link": "http://arxiv.org/abs/2306.10869v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BayLing: Bridging Cross-lingual Alignment and Instruction Following\n  through Interactive Translation for Large Language Models", "abstract": "Large language models (LLMs) have demonstrated remarkable prowess in language\nunderstanding and generation. Advancing from foundation LLMs to\ninstructionfollowing LLMs, instruction tuning plays a vital role in aligning\nLLMs to human preferences. However, the existing LLMs are usually focused on\nEnglish, leading to inferior performance in non-English languages. In order to\nimprove the performance for non-English languages, it is necessary to collect\nlanguage-specific training data for foundation LLMs and construct\nlanguage-specific instructions for instruction tuning, both of which are heavy\nloads. To minimize human workload, we propose to transfer the capabilities of\nlanguage generation and instruction following from English to other languages\nthrough an interactive translation task. We have developed BayLing, an\ninstruction-following LLM by utilizing LLaMA as the foundation LLM and\nautomatically constructing interactive translation instructions for instructing\ntuning. Extensive assessments demonstrate that BayLing achieves comparable\nperformance to GPT-3.5-turbo, despite utilizing a considerably smaller\nparameter size of only 13 billion. Experimental results on translation tasks\nshow that BayLing achieves 95% of single-turn translation capability compared\nto GPT-4 with automatic evaluation and 96% of interactive translation\ncapability compared to GPT-3.5-turbo with human evaluation. To estimate the\nperformance on general tasks, we created a multi-turn instruction test set\ncalled BayLing-80. The experimental results on BayLing-80 indicate that BayLing\nachieves 89% of performance compared to GPT-3.5-turbo. BayLing also\ndemonstrates outstanding performance on knowledge assessment of Chinese GaoKao\nand English SAT, second only to GPT-3.5-turbo among a multitude of\ninstruction-following LLMs. Demo, homepage, code and models of BayLing are\navailable.", "published": "2023-06-19 14:30:52", "link": "http://arxiv.org/abs/2306.10968v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "JiuZhang 2.0: A Unified Chinese Pre-trained Language Model for\n  Multi-task Mathematical Problem Solving", "abstract": "Although pre-trained language models~(PLMs) have recently advanced the\nresearch progress in mathematical reasoning, they are not specially designed as\na capable multi-task solver, suffering from high cost for multi-task deployment\n(\\eg a model copy for a task) and inferior performance on complex mathematical\nproblems in practical applications. To address these issues, in this paper, we\npropose \\textbf{JiuZhang~2.0}, a unified Chinese PLM specially for multi-task\nmathematical problem solving. Our idea is to maintain a moderate-sized model\nand employ the \\emph{cross-task knowledge sharing} to improve the model\ncapacity in a multi-task setting. Specially, we construct a\nMixture-of-Experts~(MoE) architecture for modeling mathematical text, so as to\ncapture the common mathematical knowledge across tasks. For optimizing the MoE\narchitecture, we design \\emph{multi-task continual pre-training} and\n\\emph{multi-task fine-tuning} strategies for multi-task adaptation. These\ntraining strategies can effectively decompose the knowledge from the task data\nand establish the cross-task sharing via expert networks. In order to further\nimprove the general capacity of solving different complex tasks, we leverage\nlarge language models~(LLMs) as complementary models to iteratively refine the\ngenerated solution by our PLM, via in-context learning. Extensive experiments\nhave demonstrated the effectiveness of our model.", "published": "2023-06-19 15:45:36", "link": "http://arxiv.org/abs/2306.11027v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adversarial Robustness of Prompt-based Few-Shot Learning for Natural\n  Language Understanding", "abstract": "State-of-the-art few-shot learning (FSL) methods leverage prompt-based\nfine-tuning to obtain remarkable results for natural language understanding\n(NLU) tasks. While much of the prior FSL methods focus on improving downstream\ntask performance, there is a limited understanding of the adversarial\nrobustness of such methods. In this work, we conduct an extensive study of\nseveral state-of-the-art FSL methods to assess their robustness to adversarial\nperturbations. To better understand the impact of various factors towards\nrobustness (or the lack of it), we evaluate prompt-based FSL methods against\nfully fine-tuned models for aspects such as the use of unlabeled data, multiple\nprompts, number of few-shot examples, model size and type. Our results on six\nGLUE tasks indicate that compared to fully fine-tuned models, vanilla FSL\nmethods lead to a notable relative drop in task performance (i.e., are less\nrobust) in the face of adversarial perturbations. However, using (i) unlabeled\ndata for prompt-based FSL and (ii) multiple prompts flip the trend. We further\ndemonstrate that increasing the number of few-shot examples and model size lead\nto increased adversarial robustness of vanilla FSL methods. Broadly, our work\nsheds light on the adversarial robustness evaluation of prompt-based FSL\nmethods for NLU tasks.", "published": "2023-06-19 17:01:13", "link": "http://arxiv.org/abs/2306.11066v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sparse Modular Activation for Efficient Sequence Modeling", "abstract": "Recent hybrid models combining Linear State Space Models (SSMs) with\nself-attention mechanisms have demonstrated impressive results across a range\nof sequence modeling tasks. However, current approaches apply attention modules\nstatically and uniformly to all elements in the input sequences, leading to\nsub-optimal quality-efficiency trade-offs. To address this limitation, we\nintroduce Sparse Modular Activation (SMA), a general mechanism enabling neural\nnetworks to sparsely and dynamically activate sub-modules for sequence elements\nin a differentiable manner. Through allowing each element to skip non-activated\nsub-modules, SMA reduces computation and memory consumption of neural networks\nat both training and inference stages. To validate the effectiveness of SMA on\nsequence modeling, we design a novel neural architecture, SeqBoat, which\nemploys SMA to sparsely activate a Gated Attention Unit (GAU) based on the\nstate representations learned from an SSM. By constraining the GAU to only\nconduct local attention on the activated inputs, SeqBoat can achieve linear\ninference complexity with theoretically infinite attention span, and provide\nsubstantially better quality-efficiency trade-off than the chunking-based\nmodels. With experiments on a wide range of tasks, including long sequence\nmodeling, speech classification and language modeling, SeqBoat brings new\nstate-of-the-art results among hybrid models with linear complexity, and\nreveals the amount of attention needed for each task through the learned sparse\nactivation patterns. Our code is publicly available at\nhttps://github.com/renll/SeqBoat.", "published": "2023-06-19 23:10:02", "link": "http://arxiv.org/abs/2306.11197v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "FSUIE: A Novel Fuzzy Span Mechanism for Universal Information Extraction", "abstract": "Universal Information Extraction (UIE) has been introduced as a unified\nframework for various Information Extraction (IE) tasks and has achieved\nwidespread success. Despite this, UIE models have limitations. For example,\nthey rely heavily on span boundaries in the data during training, which does\nnot reflect the reality of span annotation challenges. Slight adjustments to\npositions can also meet requirements. Additionally, UIE models lack attention\nto the limited span length feature in IE. To address these deficiencies, we\npropose the Fuzzy Span Universal Information Extraction (FSUIE) framework.\nSpecifically, our contribution consists of two concepts: fuzzy span loss and\nfuzzy span attention. Our experimental results on a series of main IE tasks\nshow significant improvement compared to the baseline, especially in terms of\nfast convergence and strong performance with small amounts of data and training\nepochs. These results demonstrate the effectiveness and generalization of FSUIE\nin different tasks, settings, and scenarios.", "published": "2023-06-19 15:59:28", "link": "http://arxiv.org/abs/2306.14913v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Text Embedding Space Generation Using Generative\n  Adversarial Networks for Text Synthesis", "abstract": "Generative Adversarial Networks (GAN) is a model for data synthesis, which\ncreates plausible data through the competition of generator and discriminator.\nAlthough GAN application to image synthesis is extensively studied, it has\ninherent limitations to natural language generation. Because natural language\nis composed of discrete tokens, a generator has difficulty updating its\ngradient through backpropagation; therefore, most text-GAN studies generate\nsentences starting with a random token based on a reward system. Thus, the\ngenerators of previous studies are pre-trained in an autoregressive way before\nadversarial training, causing data memorization that synthesized sentences\nreproduce the training data. In this paper, we synthesize sentences using a\nframework similar to the original GAN. More specifically, we propose Text\nEmbedding Space Generative Adversarial Networks (TESGAN) which generate\ncontinuous text embedding spaces instead of discrete tokens to solve the\ngradient backpropagation problem. Furthermore, TESGAN conducts unsupervised\nlearning which does not directly refer to the text of the training data to\novercome the data memorization issue. By adopting this novel method, TESGAN can\nsynthesize new sentences, showing the potential of unsupervised learning for\ntext synthesis. We expect to see extended research combining Large Language\nModels with a new perspective of viewing text as an continuous space.", "published": "2023-06-19 10:22:12", "link": "http://arxiv.org/abs/2306.17181v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fine-tuning Large Enterprise Language Models via Ontological Reasoning", "abstract": "Large Language Models (LLMs) exploit fine-tuning as a technique to adapt to\ndiverse goals, thanks to task-specific training data. Task specificity should\ngo hand in hand with domain orientation, that is, the specialization of an LLM\nto accurately address the tasks of a given realm of interest. However, models\nare usually fine-tuned over publicly available data or, at most, over ground\ndata from databases, ignoring business-level definitions and domain experience.\nOn the other hand, Enterprise Knowledge Graphs (EKGs) are able to capture and\naugment such domain knowledge via ontological reasoning. With the goal of\ncombining LLM flexibility with the domain orientation of EKGs, we propose a\nnovel neurosymbolic architecture that leverages the power of ontological\nreasoning to build task- and domain-specific corpora for LLM fine-tuning.", "published": "2023-06-19 06:48:45", "link": "http://arxiv.org/abs/2306.10723v2", "categories": ["cs.CL", "cs.DB", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest\n  Cost", "abstract": "Medical artificial general intelligence (AGI) is an emerging field that aims\nto develop systems specifically designed for medical applications that possess\nthe ability to understand, learn, and apply knowledge across a wide range of\ntasks and domains. Large language models (LLMs) represent a significant step\ntowards AGI. However, training cross-domain LLMs in the medical field poses\nsignificant challenges primarily attributed to the requirement of collecting\ndata from diverse domains. This task becomes particularly difficult due to\nprivacy restrictions and the scarcity of publicly available medical datasets.\nHere, we propose Medical AGI (MedAGI), a paradigm to unify domain-specific\nmedical LLMs with the lowest cost, and suggest a possible path to achieve\nmedical AGI. With an increasing number of domain-specific professional\nmultimodal LLMs in the medical field being developed, MedAGI is designed to\nautomatically select appropriate medical models by analyzing users' questions\nwith our novel adaptive expert selection algorithm. It offers a unified\napproach to existing LLMs in the medical field, eliminating the need for\nretraining regardless of the introduction of new models. This characteristic\nrenders it a future-proof solution in the dynamically advancing medical domain.\nTo showcase the resilience of MedAGI, we conducted an evaluation across three\ndistinct medical domains: dermatology diagnosis, X-ray diagnosis, and analysis\nof pathology pictures. The results demonstrated that MedAGI exhibited\nremarkable versatility and scalability, delivering exceptional performance\nacross diverse domains. Our code is publicly available to facilitate further\nresearch at https://github.com/JoshuaChou2018/MedAGI.", "published": "2023-06-19 08:15:14", "link": "http://arxiv.org/abs/2306.10765v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Comparison of L2 Korean pronunciation error patterns from five L1\n  backgrounds by using automatic phonetic transcription", "abstract": "This paper presents a large-scale analysis of L2 Korean pronunciation error\npatterns from five different language backgrounds, Chinese, Vietnamese,\nJapanese, Thai, and English, by using automatic phonetic transcription. For the\nanalysis, confusion matrices are generated for each L1, by aligning canonical\nphone sequences and automatically transcribed phone sequences obtained from\nfine-tuned Wav2Vec2 XLS-R phone recognizer. Each value in the confusion\nmatrices is compared to capture frequent common error patterns and to specify\npatterns unique to a certain language background. Using the Foreign Speakers'\nVoice Data of Korean for Artificial Intelligence Learning dataset, common error\npattern types are found to be (1) substitutions of aspirated or tense\nconsonants with plain consonants, (2) deletions of syllable-final consonants,\nand (3) substitutions of diphthongs with monophthongs. On the other hand,\nthirty-nine patterns including (1) syllable-final /l/ substitutions with /n/\nfor Vietnamese and (2) /\\textturnm/ insertions for Japanese are discovered as\nlanguage-dependent.", "published": "2023-06-19 10:10:46", "link": "http://arxiv.org/abs/2306.10821v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LARG, Language-based Automatic Reward and Goal Generation", "abstract": "Goal-conditioned and Multi-Task Reinforcement Learning (GCRL and MTRL)\naddress numerous problems related to robot learning, including locomotion,\nnavigation, and manipulation scenarios. Recent works focusing on\nlanguage-defined robotic manipulation tasks have led to the tedious production\nof massive human annotations to create dataset of textual descriptions\nassociated with trajectories. To leverage reinforcement learning with\ntext-based task descriptions, we need to produce reward functions associated\nwith individual tasks in a scalable manner. In this paper, we leverage recent\ncapabilities of Large Language Models (LLMs) and introduce \\larg,\nLanguage-based Automatic Reward and Goal Generation, an approach that converts\na text-based task description into its corresponding reward and goal-generation\nfunctions We evaluate our approach for robotic manipulation and demonstrate its\nability to train and execute policies in a scalable manner, without the need\nfor handcrafted reward functions.", "published": "2023-06-19 14:52:39", "link": "http://arxiv.org/abs/2306.10985v1", "categories": ["cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Temporal Data Meets LLM -- Explainable Financial Time Series Forecasting", "abstract": "This paper presents a novel study on harnessing Large Language Models' (LLMs)\noutstanding knowledge and reasoning abilities for explainable financial time\nseries forecasting. The application of machine learning models to financial\ntime series comes with several challenges, including the difficulty in\ncross-sequence reasoning and inference, the hurdle of incorporating multi-modal\nsignals from historical news, financial knowledge graphs, etc., and the issue\nof interpreting and explaining the model results. In this paper, we focus on\nNASDAQ-100 stocks, making use of publicly accessible historical stock price\ndata, company metadata, and historical economic/financial news. We conduct\nexperiments to illustrate the potential of LLMs in offering a unified solution\nto the aforementioned challenges. Our experiments include trying\nzero-shot/few-shot inference with GPT-4 and instruction-based fine-tuning with\na public LLM model Open LLaMA. We demonstrate our approach outperforms a few\nbaselines, including the widely applied classic ARMA-GARCH model and a\ngradient-boosting tree model. Through the performance comparison results and a\nfew examples, we find LLMs can make a well-thought decision by reasoning over\ninformation from both textual news and price time series and extracting\ninsights, leveraging cross-sequence information, and utilizing the inherent\nknowledge embedded within the LLM. Additionally, we show that a publicly\navailable LLM such as Open-LLaMA, after fine-tuning, can comprehend the\ninstruction to generate explainable forecasts and achieve reasonable\nperformance, albeit relatively inferior in comparison to GPT-4.", "published": "2023-06-19 15:42:02", "link": "http://arxiv.org/abs/2306.11025v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-fin.ST", "F.2.2; I.2.7; I.2.1"], "primary_category": "cs.LG"}
{"title": "Cross-Modal Attribute Insertions for Assessing the Robustness of\n  Vision-and-Language Learning", "abstract": "The robustness of multimodal deep learning models to realistic changes in the\ninput text is critical for their applicability to important tasks such as\ntext-to-image retrieval and cross-modal entailment. To measure robustness,\nseveral existing approaches edit the text data, but do so without leveraging\nthe cross-modal information present in multimodal data. Information from the\nvisual modality, such as color, size, and shape, provide additional attributes\nthat users can include in their inputs. Thus, we propose cross-modal attribute\ninsertions as a realistic perturbation strategy for vision-and-language data\nthat inserts visual attributes of the objects in the image into the\ncorresponding text (e.g., \"girl on a chair\" to \"little girl on a wooden\nchair\"). Our proposed approach for cross-modal attribute insertions is modular,\ncontrollable, and task-agnostic. We find that augmenting input text using\ncross-modal insertions causes state-of-the-art approaches for text-to-image\nretrieval and cross-modal entailment to perform poorly, resulting in relative\ndrops of 15% in MRR and 20% in $F_1$ score, respectively. Crowd-sourced\nannotations demonstrate that cross-modal insertions lead to higher quality\naugmentations for multimodal data than augmentations using text-only data, and\nare equivalent in quality to original examples. We release the code to\nencourage robustness evaluations of deep vision-and-language models:\nhttps://github.com/claws-lab/multimodal-robustness-xmai.", "published": "2023-06-19 17:00:03", "link": "http://arxiv.org/abs/2306.11065v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Fixated by Red Herrings: Exploring Creative\n  Problem Solving and Einstellung Effect using the Only Connect Wall Dataset", "abstract": "The quest for human imitative AI has been an enduring topic in AI research\nsince its inception. The technical evolution and emerging capabilities of the\nlatest cohort of large language models (LLMs) have reinvigorated the subject\nbeyond academia to the cultural zeitgeist. While recent NLP evaluation\nbenchmark tasks test some aspects of human-imitative behaviour (e.g.,\nBIG-bench's 'human-like behavior' tasks), few, if not none, examine creative\nproblem solving abilities. Creative problem solving in humans is a well-studied\ntopic in cognitive neuroscience with standardized tests that predominantly use\nthe ability to associate (heterogeneous) connections among clue words as a\nmetric for creativity. Exposure to misleading stimuli - distractors dubbed red\nherrings - impede human performance in such tasks via the fixation effect and\nEinstellung paradigm. In cognitive neuroscience studies, such fixations are\nexperimentally induced by pre-exposing participants to orthographically similar\nincorrect words to subsequent word-fragments or clues. The popular British quiz\nshow Only Connect's Connecting Wall segment essentially mimics Mednick's Remote\nAssociates Test (RAT) formulation with built-in, deliberate red herrings, which\nmakes it an ideal proxy dataset to explore and study fixation effect and\nEinstellung paradigm from cognitive neuroscience in LLMs. In this paper we\npresent the novel Only Connect Wall (OCW) dataset and report results from our\nevaluation of selected pre-trained language models and LLMs on creative problem\nsolving tasks like grouping clue words by heterogeneous connections, and\nidentifying correct open knowledge domain connections in respective groups. We\nsynthetically generate two additional datasets: OCW-Randomized, OCW-WordNet to\nfurther analyze our red-herrings hypothesis in language models. The code and\nlink to the dataset are available at https://github.com/TaatiTeam/OCW.", "published": "2023-06-19 21:14:57", "link": "http://arxiv.org/abs/2306.11167v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Evaluating Privacy Questions From Stack Overflow: Can ChatGPT Compete?", "abstract": "Stack Overflow and other similar forums are used commonly by developers to\nseek answers for their software development as well as privacy-related\nconcerns. Recently, ChatGPT has been used as an alternative to generate code or\nproduce responses to developers' questions. In this paper, we aim to understand\ndevelopers' privacy challenges by evaluating the types of privacy-related\nquestions asked on Stack Overflow. We then conduct a comparative analysis\nbetween the accepted responses given by Stack Overflow users and the responses\nproduced by ChatGPT for those extracted questions to identify if ChatGPT could\nserve as a viable alternative. Our results show that most privacy-related\nquestions are related to choice/consent, aggregation, and identification.\nFurthermore, our findings illustrate that ChatGPT generates similarly correct\nresponses for about 56% of questions, while for the rest of the responses, the\nanswers from Stack Overflow are slightly more accurate than ChatGPT.", "published": "2023-06-19 21:33:04", "link": "http://arxiv.org/abs/2306.11174v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Replace and Report: NLP Assisted Radiology Report Generation", "abstract": "Clinical practice frequently uses medical imaging for diagnosis and\ntreatment. A significant challenge for automatic radiology report generation is\nthat the radiology reports are long narratives consisting of multiple sentences\nfor both abnormal and normal findings. Therefore, applying conventional image\ncaptioning approaches to generate the whole report proves to be insufficient,\nas these are designed to briefly describe images with short sentences. We\npropose a template-based approach to generate radiology reports from\nradiographs. Our approach involves the following: i) using a multilabel image\nclassifier, produce the tags for the input radiograph; ii) using a\ntransformer-based model, generate pathological descriptions (a description of\nabnormal findings seen on radiographs) from the tags generated in step (i);\niii) using a BERT-based multi-label text classifier, find the spans in the\nnormal report template to replace with the generated pathological descriptions;\nand iv) using a rule-based system, replace the identified span with the\ngenerated pathological description. We performed experiments with the two most\npopular radiology report datasets, IU Chest X-ray and MIMIC-CXR and\ndemonstrated that the BLEU-1, ROUGE-L, METEOR, and CIDEr scores are better than\nthe State-of-the-Art models by 25%, 36%, 44% and 48% respectively, on the IU\nX-RAY dataset. To the best of our knowledge, this is the first attempt to\ngenerate chest X-ray radiology reports by first creating small sentences for\nabnormal findings and then replacing them in the normal report template.", "published": "2023-06-19 10:04:42", "link": "http://arxiv.org/abs/2306.17180v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Cases of EFL Secondary Students' Prompt Engineering Pathways to Complete\n  a Writing Task with ChatGPT", "abstract": "ChatGPT is a state-of-the-art (SOTA) chatbot. Although it has potential to\nsupport English as a foreign language (EFL) students' writing, to effectively\ncollaborate with it, a student must learn to engineer prompts, that is, the\nskill of crafting appropriate instructions so that ChatGPT produces desired\noutputs. However, writing an appropriate prompt for ChatGPT is not\nstraightforward for non-technical users who suffer a trial-and-error process.\nThis paper examines the content of EFL students' ChatGPT prompts when\ncompleting a writing task and explores patterns in the quality and quantity of\nthe prompts. The data come from iPad screen recordings of secondary school EFL\nstudents who used ChatGPT and other SOTA chatbots for the first time to\ncomplete the same writing task. The paper presents a case study of four\ndistinct pathways that illustrate the trial-and-error process and show\ndifferent combinations of prompt content and quantity. The cases contribute\nevidence for the need to provide prompt engineering education in the context of\nthe EFL writing classroom, if students are to move beyond an individual\ntrial-and-error process, learning a greater variety of prompt content and more\nsophisticated prompts to support their writing.", "published": "2023-06-19 06:45:04", "link": "http://arxiv.org/abs/2307.05493v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Guiding Language Models of Code with Global Context using Monitors", "abstract": "Language models of code (LMs) work well when the surrounding code provides\nsufficient context. This is not true when it becomes necessary to use types,\nfunctionality or APIs defined elsewhere in the repository or a linked library,\nespecially those not seen during training. LMs suffer from limited awareness of\nsuch global context and end up hallucinating.\n  Integrated development environments (IDEs) assist developers in understanding\nrepository context using static analysis. We extend this assistance, enjoyed by\ndevelopers, to LMs. We propose monitor-guided decoding (MGD) where a monitor\nuses static analysis to guide the decoding. We construct a repository-level\ndataset PragmaticCode for method-completion in Java and evaluate MGD on it. On\nmodels of varying parameter scale, by monitoring for type-consistent object\ndereferences, MGD consistently improves compilation rates and agreement with\nground truth. Further, LMs with fewer parameters, when augmented with MGD, can\noutperform larger LMs. With MGD, SantaCoder-1.1B achieves better compilation\nrate and next-identifier match than the much larger text-davinci-003 model.\n  We also conduct a generalizability study to evaluate the ability of MGD to\ngeneralize to multiple programming languages (Java, C# and Rust), coding\nscenarios (e.g., correct number of arguments to method calls), and to enforce\nricher semantic constraints (e.g., stateful API protocols). Our data and\nimplementation are available at https://github.com/microsoft/monitors4codegen .", "published": "2023-06-19 08:13:50", "link": "http://arxiv.org/abs/2306.10763v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PL", "cs.SE", "I.2.2; I.2.7; I.2.5"], "primary_category": "cs.CL"}
{"title": "Algorithms of Sampling-Frequency-Independent Layers for Non-integer\n  Strides", "abstract": "In this paper, we propose algorithms for handling non-integer strides in\nsampling-frequency-independent (SFI) convolutional and transposed convolutional\nlayers. The SFI layers have been developed for handling various sampling\nfrequencies (SFs) by a single neural network. They are replaceable with their\nnon-SFI counterparts and can be introduced into various network architectures.\nHowever, they could not handle some specific configurations when combined with\nnon-SFI layers. For example, an SFI extension of Conv-TasNet, a standard audio\nsource separation model, cannot handle some pairs of trained and target SFs\nbecause the strides of the SFI layers become non-integers. This problem cannot\nbe solved by simple rounding or signal resampling, resulting in the significant\nperformance degradation. To overcome this problem, we propose algorithms for\nhandling non-integer strides by using windowed sinc interpolation. The proposed\nalgorithms realize the continuous-time representations of features using the\ninterpolation and enable us to sample instants with the desired stride.\nExperimental results on music source separation showed that the proposed\nalgorithms outperformed the rounding- and signal-resampling-based methods at\nSFs lower than the trained SF.", "published": "2023-06-19 06:33:54", "link": "http://arxiv.org/abs/2306.10718v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning an Interpretable End-to-End Network for Real-Time Acoustic\n  Beamforming", "abstract": "Recently, many forms of audio industrial applications, such as sound\nmonitoring and source localization, have begun exploiting smart multi-modal\ndevices equipped with a microphone array. Regrettably, model-based methods are\noften difficult to employ for such devices due to their high computational\ncomplexity, as well as the difficulty of appropriately selecting the\nuser-determined parameters. As an alternative, one may use deep network-based\nmethods, but these are often difficult to generalize, nor can they generate the\ndesired beamforming map directly. In this paper, a computationally efficient\nacoustic beamforming algorithm is proposed, which may be unrolled to form a\nmodel-based deep learning network for real-time imaging, here termed the\nDAMAS-FISTA-Net. By exploiting the natural structure of an acoustic beamformer,\nthe proposed network inherits the physical knowledge of the acoustic system,\nand thus learns the underlying physical properties of the propagation. As a\nresult, all the network parameters may be learned end-to-end, guided by a\nmodel-based prior using back-propagation. Notably, the proposed network enables\nan excellent interpretability and the ability of being able to process the raw\ndata directly. Extensive numerical experiments using both simulated and\nreal-world data illustrate the preferable performance of the DAMAS-FISTA-Net as\ncompared to alternative approaches.", "published": "2023-06-19 08:28:06", "link": "http://arxiv.org/abs/2306.10772v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Rehearsal-Free Online Continual Learning for Automatic Speech\n  Recognition", "abstract": "Fine-tuning an Automatic Speech Recognition (ASR) model to new domains\nresults in degradation on original domains, referred to as Catastrophic\nForgetting (CF). Continual Learning (CL) attempts to train ASR models without\nsuffering from CF. While in ASR, offline CL is usually considered, online CL is\na more realistic but also more challenging scenario where the model, unlike in\noffline CL, does not know when a task boundary occurs. Rehearsal-based methods,\nwhich store previously seen utterances in a memory, are often considered for\nonline CL, in ASR and other research domains. However, recent research has\nshown that weight averaging is an effective method for offline CL in ASR. Based\non this result, we propose, in this paper, a rehearsal-free method applicable\nfor online CL. Our method outperforms all baselines, including rehearsal-based\nmethods, in two experiments. Our method is a next step towards general CL for\nASR, which should enable CL in all scenarios with few if any constraints.", "published": "2023-06-19 11:24:15", "link": "http://arxiv.org/abs/2306.10860v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Vocal Timbre Effects with Differentiable Digital Signal Processing", "abstract": "We explore two approaches to creatively altering vocal timbre using\nDifferentiable Digital Signal Processing (DDSP). The first approach is inspired\nby classic cross-synthesis techniques. A pretrained DDSP decoder predicts a\nfilter for a noise source and a harmonic distribution, based on pitch and\nloudness information extracted from the vocal input. Before synthesis, the\nharmonic distribution is modified by interpolating between the predicted\ndistribution and the harmonics of the input. We provide a real-time\nimplementation of this approach in the form of a Neutone model. In the second\napproach, autoencoder models are trained on datasets consisting of both vocal\nand instrument training data. To apply the effect, the trained autoencoder\nattempts to reconstruct the vocal input. We find that there is a desirable\n\"sweet spot\" during training, where the model has learned to reconstruct the\nphonetic content of the input vocals, but is still affected by the timbre of\nthe instrument mixed into the training data. After further training, that\neffect disappears. A perceptual evaluation compares the two approaches. We find\nthat the autoencoder in the second approach is able to reconstruct intelligible\nlyrical content without any explicit phonetic information provided during\ntraining.", "published": "2023-06-19 12:32:26", "link": "http://arxiv.org/abs/2306.10886v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Visually-Guided Sound Source Separation with Audio-Visual Predictive\n  Coding", "abstract": "The framework of visually-guided sound source separation generally consists\nof three parts: visual feature extraction, multimodal feature fusion, and sound\nsignal processing. An ongoing trend in this field has been to tailor involved\nvisual feature extractor for informative visual guidance and separately devise\nmodule for feature fusion, while utilizing U-Net by default for sound analysis.\nHowever, such divide-and-conquer paradigm is parameter inefficient and,\nmeanwhile, may obtain suboptimal performance as jointly optimizing and\nharmonizing various model components is challengeable. By contrast, this paper\npresents a novel approach, dubbed audio-visual predictive coding (AVPC), to\ntackle this task in a parameter efficient and more effective manner. The\nnetwork of AVPC features a simple ResNet-based video analysis network for\nderiving semantic visual features, and a predictive coding-based sound\nseparation network that can extract audio features, fuse multimodal\ninformation, and predict sound separation masks in the same architecture. By\niteratively minimizing the prediction error between features, AVPC integrates\naudio and visual information recursively, leading to progressively improved\nperformance. In addition, we develop a valid self-supervised learning strategy\nfor AVPC via co-predicting two audio-visual representations of the same sound\nsource. Extensive evaluations demonstrate that AVPC outperforms several\nbaselines in separating musical instrument sounds, while reducing the model\nsize significantly. Code is available at:\nhttps://github.com/zjsong/Audio-Visual-Predictive-Coding.", "published": "2023-06-19 03:10:57", "link": "http://arxiv.org/abs/2306.10684v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multitrack Music Transcription with a Time-Frequency Perceiver", "abstract": "Multitrack music transcription aims to transcribe a music audio input into\nthe musical notes of multiple instruments simultaneously. It is a very\nchallenging task that typically requires a more complex model to achieve\nsatisfactory result. In addition, prior works mostly focus on transcriptions of\nregular instruments, however, neglecting vocals, which are usually the most\nimportant signal source if present in a piece of music. In this paper, we\npropose a novel deep neural network architecture, Perceiver TF, to model the\ntime-frequency representation of audio input for multitrack transcription.\nPerceiver TF augments the Perceiver architecture by introducing a hierarchical\nexpansion with an additional Transformer layer to model temporal coherence.\nAccordingly, our model inherits the benefits of Perceiver that posses better\nscalability, allowing it to well handle transcriptions of many instruments in a\nsingle model. In experiments, we train a Perceiver TF to model 12 instrument\nclasses as well as vocal in a multi-task learning manner. Our result\ndemonstrates that the proposed system outperforms the state-of-the-art\ncounterparts (e.g., MT3 and SpecTNT) on various public datasets.", "published": "2023-06-19 08:58:26", "link": "http://arxiv.org/abs/2306.10785v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Female mosquito detection by means of AI techniques inside release\n  containers in the context of a Sterile Insect Technique program", "abstract": "The Sterile Insect Technique (SIT) is a biological pest control technique\nbased on the release into the environment of sterile males of the insect\nspecies whose population is to be controlled. The entire SIT process involves\nmass-rearing within a biofactory, sorting of the specimens by sex,\nsterilization, and subsequent release of the sterile males into the\nenvironment. The reason for avoiding the release of female specimens is\nbecause, unlike males, females bite, with the subsequent risk of disease\ntransmission. In the case of Aedes mosquito biofactories for SIT, the key point\nof the whole process is sex separation. This process is nowadays performed by a\ncombination of mechanical devices and AI-based vision systems. However, there\nis still a possibility of false negatives, so a last stage of verification is\nnecessary before releasing them into the environment. It is known that the\nsound produced by the flapping of adult male mosquitoes is different from that\nproduced by females, so this feature can be used to detect the presence of\nfemales in containers prior to environmental release. This paper presents a\nstudy for the detection of females in Aedes mosquito release vessels for SIT\nprograms. The containers used consist of PVC a tubular design of 8.8cm diameter\nand 12.5cm height. The containers were placed in an experimental setup that\nallowed the recording of the sound of mosquito flight inside of them. Each\ncontainer was filled with 250 specimens considering the cases of (i) only male\nmosquitoes, (ii) only female mosquitoes, and (iii) 75% males and 25% females.\nCase (i) was used for training and testing, whereas cases (ii) and (iii) were\nused only for testing. Two algorithms were implemented for the detection of\nfemale mosquitoes: an unsupervised outlier detection algorithm (iForest) and a\none-class SVM trained with male-only recordings.", "published": "2023-06-19 10:45:10", "link": "http://arxiv.org/abs/2306.10843v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
