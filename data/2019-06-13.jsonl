{"title": "Transfer Learning in Biomedical Natural Language Processing: An\n  Evaluation of BERT and ELMo on Ten Benchmarking Datasets", "abstract": "Inspired by the success of the General Language Understanding Evaluation\nbenchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE)\nbenchmark to facilitate research in the development of pre-training language\nrepresentations in the biomedicine domain. The benchmark consists of five tasks\nwith ten datasets that cover both biomedical and clinical texts with different\ndataset sizes and difficulties. We also evaluate several baselines based on\nBERT and ELMo and find that the BERT model pre-trained on PubMed abstracts and\nMIMIC-III clinical notes achieves the best results. We make the datasets,\npre-trained models, and codes publicly available at\nhttps://github.com/ncbi-nlp/BLUE_Benchmark.", "published": "2019-06-13 04:07:12", "link": "http://arxiv.org/abs/1906.05474v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Computational Analysis of Natural Languages to Build a Sentence\n  Structure Aware Artificial Neural Network", "abstract": "Natural languages are complexly structured entities. They exhibit\ncharacterising regularities that can be exploited to link them one another. In\nthis work, I compare two morphological aspects of languages: Written Patterns\nand Sentence Structure. I show how languages spontaneously group by similarity\nin both analyses and derive an average language distance. Finally, exploiting\nSentence Structure I developed an Artificial Neural Network capable of\ndistinguishing languages suggesting that not only word roots but also\ngrammatical sentence structure is a characterising trait which alone suffice to\nidentify them.", "published": "2019-06-13 05:44:16", "link": "http://arxiv.org/abs/1906.05491v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Character n-gram Embeddings to Improve RNN Language Models", "abstract": "This paper proposes a novel Recurrent Neural Network (RNN) language model\nthat takes advantage of character information. We focus on character n-grams\nbased on research in the field of word embedding construction (Wieting et al.\n2016). Our proposed method constructs word embeddings from character n-gram\nembeddings and combines them with ordinary word embeddings. We demonstrate that\nthe proposed method achieves the best perplexities on the language modeling\ndatasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct\nexperiments on application tasks: machine translation and headline generation.\nThe experimental results indicate that our proposed method also positively\naffects these tasks.", "published": "2019-06-13 06:52:45", "link": "http://arxiv.org/abs/1906.05506v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Know What You Don't Know: Modeling a Pragmatic Speaker that Refers to\n  Objects of Unknown Categories", "abstract": "Zero-shot learning in Language & Vision is the task of correctly labelling\n(or naming) objects of novel categories. Another strand of work in L&V aims at\npragmatically informative rather than ``correct'' object descriptions, e.g. in\nreference games. We combine these lines of research and model zero-shot\nreference games, where a speaker needs to successfully refer to a novel object\nin an image. Inspired by models of \"rational speech acts\", we extend a neural\ngenerator to become a pragmatic speaker reasoning about uncertain object\ncategories. As a result of this reasoning, the generator produces fewer nouns\nand names of distractor categories as compared to a literal speaker. We show\nthat this conversational strategy for dealing with novel objects often improves\ncommunicative success, in terms of resolution accuracy of an automatic\nlistener.", "published": "2019-06-13 07:35:05", "link": "http://arxiv.org/abs/1906.05518v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lattice Transformer for Speech Translation", "abstract": "Recent advances in sequence modeling have highlighted the strengths of the\ntransformer architecture, especially in achieving state-of-the-art machine\ntranslation results. However, depending on the up-stream systems, e.g., speech\nrecognition, or word segmentation, the input to translation system can vary\ngreatly. The goal of this work is to extend the attention mechanism of the\ntransformer to naturally consume the lattice in addition to the traditional\nsequential input. We first propose a general lattice transformer for speech\ntranslation where the input is the output of the automatic speech recognition\n(ASR) which contains multiple paths and posterior scores. To leverage the extra\ninformation from the lattice structure, we develop a novel controllable lattice\nattention mechanism to obtain latent representations. On the LDC\nSpanish-English speech translation corpus, our experiments show that lattice\ntransformer generalizes significantly better and outperforms both a transformer\nbaseline and a lattice LSTM. Additionally, we validate our approach on the WMT\n2017 Chinese-English translation task with lattice inputs from different BPE\nsegmentations. In this task, we also observe the improvements over strong\nbaselines.", "published": "2019-06-13 08:55:06", "link": "http://arxiv.org/abs/1906.05551v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Proactive Human-Machine Conversation with Explicit Conversation Goals", "abstract": "Though great progress has been made for human-machine conversation, current\ndialogue system is still in its infancy: it usually converses passively and\nutters words more as a matter of response, rather than on its own initiatives.\nIn this paper, we take a radical step towards building a human-like\nconversational agent: endowing it with the ability of proactively leading the\nconversation (introducing a new topic or maintaining the current topic). To\nfacilitate the development of such conversation systems, we create a new\ndataset named DuConv where one acts as a conversation leader and the other acts\nas the follower. The leader is provided with a knowledge graph and asked to\nsequentially change the discussion topics, following the given conversation\ngoal, and meanwhile keep the dialogue as natural and engaging as possible.\nDuConv enables a very challenging task as the model needs to both understand\ndialogue and plan over the given knowledge graph. We establish baseline results\non this dataset (about 270K utterances and 30k dialogues) using several\nstate-of-the-art models. Experimental results show that dialogue models that\nplan over the knowledge graph can make full use of related knowledge to\ngenerate more diverse multi-turn conversations. The baseline systems along with\nthe dataset are publicly available", "published": "2019-06-13 09:42:51", "link": "http://arxiv.org/abs/1906.05572v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Antonym-Synonym Classification Based on New Sub-space Embeddings", "abstract": "Distinguishing antonyms from synonyms is a key challenge for many NLP\napplications focused on the lexical-semantic relation extraction. Existing\nsolutions relying on large-scale corpora yield low performance because of huge\ncontextual overlap of antonym and synonym pairs. We propose a novel approach\nentirely based on pre-trained embeddings. We hypothesize that the pre-trained\nembeddings comprehend a blend of lexical-semantic information and we may\ndistill the task-specific information using Distiller, a model proposed in this\npaper. Later, a classifier is trained based on features constructed from the\ndistilled sub-spaces along with some word level features to distinguish\nantonyms from synonyms. Experimental results show that the proposed model\noutperforms existing research on antonym synonym distinction in both speed and\nperformance.", "published": "2019-06-13 11:46:12", "link": "http://arxiv.org/abs/1906.05612v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Sentiment Detection via Label Transfer from Monolingual to\n  Synthetic Code-Switched Text", "abstract": "Multilingual writers and speakers often alternate between two languages in a\nsingle discourse, a practice called \"code-switching\". Existing sentiment\ndetection methods are usually trained on sentiment-labeled monolingual text.\nManually labeled code-switched text, especially involving minority languages,\nis extremely rare. Consequently, the best monolingual methods perform\nrelatively poorly on code-switched text. We present an effective technique for\nsynthesizing labeled code-switched text from labeled monolingual text, which is\nmore readily available. The idea is to replace carefully selected subtrees of\nconstituency parses of sentences in the resource-rich language with suitable\ntoken spans selected from automatic translations to the resource-poor language.\nBy augmenting scarce human-labeled code-switched text with plentiful synthetic\ncode-switched text, we achieve significant improvements in sentiment labeling\naccuracy (1.5%, 5.11%, 7.20%) for three different language pairs\n(English-Hindi, English-Spanish and English-Bengali). We also get significant\ngains for hate speech detection: 4% improvement using only synthetic text and\n6% if augmented with real text.", "published": "2019-06-13 14:41:00", "link": "http://arxiv.org/abs/1906.05725v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Anti dependency distance minimization in short sequences. A graph\n  theoretic approach", "abstract": "Dependency distance minimization (DDm) is a word order principle favouring\nthe placement of syntactically related words close to each other in sentences.\nMassive evidence of the principle has been reported for more than a decade with\nthe help of syntactic dependency treebanks where long sentences abound.\nHowever, it has been predicted theoretically that the principle is more likely\nto be beaten in short sequences by the principle of surprisal minimization\n(predictability maximization). Here we introduce a simple binomial test to\nverify such a hypothesis. In short sentences, we find anti-DDm for some\nlanguages from different families. Our analysis of the syntactic dependency\nstructures suggests that anti-DDm is produced by star trees.", "published": "2019-06-13 15:49:56", "link": "http://arxiv.org/abs/1906.05765v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UCAM Biomedical translation at WMT19: Transfer learning multi-domain\n  ensembles", "abstract": "The 2019 WMT Biomedical translation task involved translating Medline\nabstracts. We approached this using transfer learning to obtain a series of\nstrong neural models on distinct domains, and combining them into multi-domain\nensembles. We further experiment with an adaptive language-model ensemble\nweighting scheme. Our submission achieved the best submitted results on both\ndirections of English-Spanish.", "published": "2019-06-13 16:22:44", "link": "http://arxiv.org/abs/1906.05786v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index", "abstract": "Existing open-domain question answering (QA) models are not suitable for\nreal-time usage because they need to process several long documents on-demand\nfor every input query. In this paper, we introduce the query-agnostic indexable\nrepresentation of document phrases that can drastically speed up open-domain QA\nand also allows us to reach long-tail targets. In particular, our dense-sparse\nphrase encoding effectively captures syntactic, semantic, and lexical\ninformation of the phrases and eliminates the pipeline filtering of context\ndocuments. Leveraging optimization strategies, our model can be trained in a\nsingle 4-GPU server and serve entire Wikipedia (up to 60 billion phrases) under\n2TB with CPUs only. Our experiments on SQuAD-Open show that our model is more\naccurate than DrQA (Chen et al., 2017) with 6000x reduced computational cost,\nwhich translates into at least 58x faster end-to-end inference benchmark on\nCPUs.", "published": "2019-06-13 16:49:35", "link": "http://arxiv.org/abs/1906.05807v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment analysis is not solved! Assessing and probing sentiment\n  classification", "abstract": "Neural methods for SA have led to quantitative improvements over previous\napproaches, but these advances are not always accompanied with a thorough\nanalysis of the qualitative differences. Therefore, it is not clear what\noutstanding conceptual challenges for sentiment analysis remain. In this work,\nwe attempt to discover what challenges still prove a problem for sentiment\nclassifiers for English and to provide a challenging dataset. We collect the\nsubset of sentences that an (oracle) ensemble of state-of-the-art sentiment\nclassifiers misclassify and then annotate them for 18 linguistic and\nparalinguistic phenomena, such as negation, sarcasm, modality, etc. The dataset\nis available at https://github.com/ltgoslo/assessing_and_probing_sentiment.\nFinally, we provide a case study that demonstrates the usefulness of the\ndataset to probe the performance of a given sentiment classifier with respect\nto linguistic phenomena.", "published": "2019-06-13 18:35:00", "link": "http://arxiv.org/abs/1906.05887v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Effect of Word Order on Cross-lingual Sentiment Analysis", "abstract": "Current state-of-the-art models for sentiment analysis make use of word order\neither explicitly by pre-training on a language modeling objective or\nimplicitly by using recurrent neural networks (RNNs) or convolutional networks\n(CNNs). This is a problem for cross-lingual models that use bilingual\nembeddings as features, as the difference in word order between source and\ntarget languages is not resolved. In this work, we explore reordering as a\npre-processing step for sentence-level cross-lingual sentiment classification\nwith two language combinations (English-Spanish, English-Catalan). We find that\nwhile reordering helps both models, CNNS are more sensitive to local\nreorderings, while global reordering benefits RNNs.", "published": "2019-06-13 18:50:17", "link": "http://arxiv.org/abs/1906.05889v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meaning to Form: Measuring Systematicity as Information", "abstract": "A longstanding debate in semiotics centers on the relationship between\nlinguistic signs and their corresponding semantics: is there an arbitrary\nrelationship between a word form and its meaning, or does some systematic\nphenomenon pervade? For instance, does the character bigram \\textit{gl} have\nany systematic relationship to the meaning of words like \\textit{glisten},\n\\textit{gleam} and \\textit{glow}? In this work, we offer a holistic\nquantification of the systematicity of the sign using mutual information and\nrecurrent neural networks. We employ these in a data-driven and massively\nmultilingual approach to the question, examining 106 languages. We find a\nstatistically significant reduction in entropy when modeling a word form\nconditioned on its semantic representation. Encouragingly, we also recover\nwell-attested English examples of systematic affixes. We conclude with the\nmeta-point: Our approximate effect size (measured in bits) is quite\nsmall---despite some amount of systematicity between form and meaning, an\narbitrary relationship and its resulting benefits dominate human language.", "published": "2019-06-13 19:30:12", "link": "http://arxiv.org/abs/1906.05906v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Figurative Usage Detection of Symptom Words to Improve Personal Health\n  Mention Detection", "abstract": "Personal health mention detection deals with predicting whether or not a\ngiven sentence is a report of a health condition. Past work mentions errors in\nthis prediction when symptom words, i.e. names of symptoms of interest, are\nused in a figurative sense. Therefore, we combine a state-of-the-art figurative\nusage detection with CNN-based personal health mention detection. To do so, we\npresent two methods: a pipeline-based approach and a feature augmentation-based\napproach. The introduction of figurative usage detection results in an average\nimprovement of 2.21% F-score of personal health mention detection, in the case\nof the feature augmentation-based approach. This paper demonstrates the promise\nof using figurative usage detection to improve personal health mention\ndetection.", "published": "2019-06-13 03:42:34", "link": "http://arxiv.org/abs/1906.05466v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Comparison of Word-based and Context-based Representations for\n  Classification Problems in Health Informatics", "abstract": "Distributed representations of text can be used as features when training a\nstatistical classifier. These representations may be created as a composition\nof word vectors or as context-based sentence vectors. We compare the two kinds\nof representations (word versus context) for three classification problems:\ninfluenza infection classification, drug usage classification and personal\nhealth mention classification. For statistical classifiers trained for each of\nthese problems, context-based representations based on ELMo, Universal Sentence\nEncoder, Neural-Net Language Model and FLAIR are better than Word2Vec, GloVe\nand the two adapted using the MESH ontology. There is an improvement of 2-4% in\nthe accuracy when these context-based representations are used instead of\nword-based representations.", "published": "2019-06-13 03:48:34", "link": "http://arxiv.org/abs/1906.05468v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Enriching Neural Models with Targeted Features for Dementia Detection", "abstract": "Alzheimer's disease (AD) is an irreversible brain disease that can\ndramatically reduce quality of life, most commonly manifesting in older adults\nand eventually leading to the need for full-time care. Early detection is\nfundamental to slowing its progression; however, diagnosis can be expensive,\ntime-consuming, and invasive. In this work we develop a neural model based on a\nCNN-LSTM architecture that learns to detect AD and related dementias using\ntargeted and implicitly-learned features from conversational transcripts. Our\napproach establishes the new state of the art on the DementiaBank dataset,\nachieving an F1 score of 0.929 when classifying participants into AD and\ncontrol groups.", "published": "2019-06-13 05:15:25", "link": "http://arxiv.org/abs/1906.05483v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KCAT: A Knowledge-Constraint Typing Annotation Tool", "abstract": "Fine-grained Entity Typing is a tough task which suffers from noise samples\nextracted from distant supervision. Thousands of manually annotated samples can\nachieve greater performance than millions of samples generated by the previous\ndistant supervision method. Whereas, it's hard for human beings to\ndifferentiate and memorize thousands of types, thus making large-scale human\nlabeling hardly possible. In this paper, we introduce a Knowledge-Constraint\nTyping Annotation Tool (KCAT), which is efficient for fine-grained entity\ntyping annotation. KCAT reduces the size of candidate types to an acceptable\nrange for human beings through entity linking and provides a Multi-step Typing\nscheme to revise the entity linking result. Moreover, KCAT provides an\nefficient Annotator Client to accelerate the annotation process and a\ncomprehensive Manager Module to analyse crowdsourcing annotations. Experiment\nshows that KCAT can significantly improve annotation efficiency, the time\nconsumption increases slowly as the size of type set expands.", "published": "2019-06-13 13:33:22", "link": "http://arxiv.org/abs/1906.05670v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Semantic Change and Semantic Stability: Variation is Key", "abstract": "I survey some recent approaches to studying change in the lexicon,\nparticularly change in meaning across phylogenies. I briefly sketch an\nevolutionary approach to language change and point out some issues in recent\napproaches to studying semantic change that rely on temporally stratified word\nembeddings. I draw illustrations from lexical cognate models in Pama-Nyungan to\nidentify meaning classes most appropriate for lexical phylogenetic inference,\nparticularly highlighting the importance of variation in studying change over\ntime.", "published": "2019-06-13 15:40:50", "link": "http://arxiv.org/abs/1906.05760v1", "categories": ["cs.CL", "q-bio.PE"], "primary_category": "cs.CL"}
{"title": "Embedding Biomedical Ontologies by Jointly Encoding Network Structure\n  and Textual Node Descriptors", "abstract": "Network Embedding (NE) methods, which map network nodes to low-dimensional\nfeature vectors, have wide applications in network analysis and bioinformatics.\nMany existing NE methods rely only on network structure, overlooking other\ninformation associated with the nodes, e.g., text describing the nodes. Recent\nattempts to combine the two sources of information only consider local network\nstructure. We extend NODE2VEC, a well-known NE method that considers broader\nnetwork structure, to also consider textual node descriptors using recurrent\nneural encoders. Our method is evaluated on link prediction in two networks\nderived from UMLS. Experimental results demonstrate the effectiveness of the\nproposed approach compared to previous work.", "published": "2019-06-13 21:40:15", "link": "http://arxiv.org/abs/1906.05939v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Cognitive Knowledge Graph Reasoning for One-shot Relational Learning", "abstract": "Inferring new facts from existing knowledge graphs (KG) with explainable\nreasoning processes is a significant problem and has received much attention\nrecently. However, few studies have focused on relation types unseen in the\noriginal KG, given only one or a few instances for training. To bridge this\ngap, we propose CogKR for one-shot KG reasoning. The one-shot relational\nlearning problem is tackled through two modules: the summary module summarizes\nthe underlying relationship of the given instances, based on which the\nreasoning module infers the correct answers. Motivated by the dual process\ntheory in cognitive science, in the reasoning module, a cognitive graph is\nbuilt by iteratively coordinating retrieval (System 1, collecting relevant\nevidence intuitively) and reasoning (System 2, conducting relational reasoning\nover collected information). The structural information offered by the\ncognitive graph enables our model to aggregate pieces of evidence from multiple\nreasoning paths and explain the reasoning process graphically. Experiments show\nthat CogKR substantially outperforms previous state-of-the-art models on\none-shot KG reasoning benchmarks, with relative improvements of 24.3%-29.7% on\nMRR. The source code is available at https://github.com/THUDM/CogKR.", "published": "2019-06-13 05:39:42", "link": "http://arxiv.org/abs/1906.05489v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Unsupervised Neural Single-Document Summarization of Reviews via\n  Learning Latent Discourse Structure and its Ranking", "abstract": "This paper focuses on the end-to-end abstractive summarization of a single\nproduct review without supervision. We assume that a review can be described as\na discourse tree, in which the summary is the root, and the child sentences\nexplain their parent in detail. By recursively estimating a parent from its\nchildren, our model learns the latent discourse tree without an external parser\nand generates a concise summary. We also introduce an architecture that ranks\nthe importance of each sentence on the tree to support summary generation\nfocusing on the main review point. The experimental results demonstrate that\nour model is competitive with or outperforms other unsupervised approaches. In\nparticular, for relatively long reviews, it achieves a competitive or better\nperformance than supervised models. The induced tree shows that the child\nsentences provide additional information about their parent, and the generated\nsummary abstracts the entire review.", "published": "2019-06-13 13:53:10", "link": "http://arxiv.org/abs/1906.05691v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speaker-Targeted Audio-Visual Models for Speech Recognition in\n  Cocktail-Party Environments", "abstract": "Speech recognition in cocktail-party environments remains a significant\nchallenge for state-of-the-art speech recognition systems, as it is extremely\ndifficult to extract an acoustic signal of an individual speaker from a\nbackground of overlapping speech with similar frequency and temporal\ncharacteristics. We propose the use of speaker-targeted acoustic and\naudio-visual models for this task. We complement the acoustic features in a\nhybrid DNN-HMM model with information of the target speaker's identity as well\nas visual features from the mouth region of the target speaker. Experimentation\nwas performed using simulated cocktail-party data generated from the GRID\naudio-visual corpus by overlapping two speakers's speech on a single acoustic\nchannel. Our audio-only baseline achieved a WER of 26.3%. The audio-visual\nmodel improved the WER to 4.4%. Introducing speaker identity information had an\neven more pronounced effect, improving the WER to 3.6%. Combining both\napproaches, however, did not significantly improve performance further. Our\nwork demonstrates that speaker-targeted models can significantly improve the\nspeech recognition in cocktail party environments.", "published": "2019-06-13 23:52:16", "link": "http://arxiv.org/abs/1906.05962v1", "categories": ["eess.AS", "cs.CL", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Correlating Twitter Language with Community-Level Health Outcomes", "abstract": "We study how language on social media is linked to diseases such as\natherosclerotic heart disease (AHD), diabetes and various types of cancer. Our\nproposed model leverages state-of-the-art sentence embeddings, followed by a\nregression model and clustering, without the need of additional labelled data.\nIt allows to predict community-level medical outcomes from language, and\nthereby potentially translate these to the individual level. The method is\napplicable to a wide range of target variables and allows us to discover known\nand potentially novel correlations of medical outcomes with life-style aspects\nand other socioeconomic risk factors.", "published": "2019-06-13 01:42:23", "link": "http://arxiv.org/abs/1906.06465v2", "categories": ["cs.CL", "cs.LG", "cs.SI", "stat.ML", "J.3; J.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "Telephonetic: Making Neural Language Models Robust to ASR and Semantic\n  Noise", "abstract": "Speech processing systems rely on robust feature extraction to handle\nphonetic and semantic variations found in natural language. While techniques\nexist for desensitizing features to common noise patterns produced by\nSpeech-to-Text (STT) and Text-to-Speech (TTS) systems, the question remains how\nto best leverage state-of-the-art language models (which capture rich semantic\nfeatures, but are trained on only written text) on inputs with ASR errors. In\nthis paper, we present Telephonetic, a data augmentation framework that helps\nrobustify language model features to ASR corrupted inputs. To capture phonetic\nalterations, we employ a character-level language model trained using\nprobabilistic masking. Phonetic augmentations are generated in two stages: a\nTTS encoder (Tacotron 2, WaveGlow) and a STT decoder (DeepSpeech). Similarly,\nsemantic perturbations are produced by sampling from nearby words in an\nembedding space, which is computed using the BERT language model. Words are\nselected for augmentation according to a hierarchical grammar sampling\nstrategy. Telephonetic is evaluated on the Penn Treebank (PTB) corpus, and\ndemonstrates its effectiveness as a bootstrapping technique for transferring\nneural language models to the speech domain. Notably, our language model\nachieves a test perplexity of 37.49 on PTB, which to our knowledge is\nstate-of-the-art among models trained only on PTB.", "published": "2019-06-13 17:04:46", "link": "http://arxiv.org/abs/1906.05678v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Adjusting Pleasure-Arousal-Dominance for Continuous Emotional\n  Text-to-speech Synthesizer", "abstract": "Emotion is not limited to discrete categories of happy, sad, angry, fear,\ndisgust, surprise, and so on. Instead, each emotion category is projected into\na set of nearly independent dimensions, named pleasure (or valence), arousal,\nand dominance, known as PAD. The value of each dimension varies from -1 to 1,\nsuch that the neutral emotion is in the center with all-zero values. Training\nan emotional continuous text-to-speech (TTS) synthesizer on the independent\ndimensions provides the possibility of emotional speech synthesis with\nunlimited emotion categories. Our end-to-end neural speech synthesizer is based\non the well-known Tacotron. Empirically, we have found the optimum network\narchitecture for injecting the 3D PADs. Moreover, the PAD values are adjusted\nfor the speech synthesis purpose.", "published": "2019-06-13 06:53:40", "link": "http://arxiv.org/abs/1906.05507v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cross-cultural data shows musical scales evolved to maximise imperfect\n  fifths", "abstract": "Musical scales are used throughout the world, but the question of how they\nevolved remains open. Some suggest that scales based on the harmonic series are\ninherently pleasant, while others propose that scales are chosen that are easy\nto communicate. However, testing these theories has been hindered by the\nsparseness of empirical evidence. Here, we assimilate data from diverse\nethnomusicological sources into a cross-cultural database of scales. We\ngenerate populations of scales based on multiple theories and assess their\nsimilarity to empirical distributions from the database. Most scales tend to\ninclude intervals which are close in size to perfect fifths (``imperfect\nfifths''), and packing arguments explain the salient features of the\ndistributions. Scales are also preferred if their intervals are compressible,\nwhich may facilitate efficient communication and memory of melodies. While\nscales appear to evolve according to various selection pressures, the simplest\nh imperfect-fifths packing model best fits the empirical data.", "published": "2019-06-13 04:29:19", "link": "http://arxiv.org/abs/1906.06171v2", "categories": ["cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "cs.SD"}
