{"title": "Partially Randomizing Transformer Weights for Dialogue Response\n  Diversity", "abstract": "Despite recent progress in generative open-domain dialogue, the issue of low\nresponse diversity persists. Prior works have addressed this issue via either\nnovel objective functions, alternative learning approaches such as variational\nframeworks, or architectural extensions such as the Randomized Link (RL)\nTransformer. However, these approaches typically entail either additional\ndifficulties during training/inference, or a significant increase in model size\nand complexity. Hence, we propose the \\underline{Pa}rtially\n\\underline{Ra}ndomized trans\\underline{Former} (PaRaFormer), a simple extension\nof the transformer which involves freezing the weights of selected layers after\nrandom initialization. Experimental results reveal that the performance of the\nPaRaformer is comparable to that of the aforementioned approaches, despite not\nentailing any additional training difficulty or increase in model complexity.", "published": "2023-11-18 02:40:11", "link": "http://arxiv.org/abs/2311.10943v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Journey of Hallucination-minimized Generative AI Solutions for Financial\n  Decision Makers", "abstract": "Generative AI has significantly reduced the entry barrier to the domain of AI\nowing to the ease of use and core capabilities of automation, translation, and\nintelligent actions in our day to day lives. Currently, Large language models\n(LLMs) that power such chatbots are being utilized primarily for their\nautomation capabilities for software monitoring, report generation etc. and for\nspecific personalized question answering capabilities, on a limited scope and\nscale. One major limitation of the currently evolving family of LLMs is\n'hallucinations', wherein inaccurate responses are reported as factual.\nHallucinations are primarily caused by biased training data, ambiguous prompts\nand inaccurate LLM parameters, and they majorly occur while combining\nmathematical facts with language-based context. Thus, monitoring and\ncontrolling for hallucinations becomes necessary when designing solutions that\nare meant for decision makers. In this work we present the three major stages\nin the journey of designing hallucination-minimized LLM-based solutions that\nare specialized for the decision makers of the financial domain, namely:\nprototyping, scaling and LLM evolution using human feedback. These three stages\nand the novel data to answer generation modules presented in this work are\nnecessary to ensure that the Generative AI chatbots, autonomous reports and\nalerts are reliable and high-quality to aid key decision-making processes.", "published": "2023-11-18 03:55:59", "link": "http://arxiv.org/abs/2311.10961v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gendec: A Machine Learning-based Framework for Gender Detection from\n  Japanese Names", "abstract": "Every human has their own name, a fundamental aspect of their identity and\ncultural heritage. The name often conveys a wealth of information, including\ndetails about an individual's background, ethnicity, and, especially, their\ngender. By detecting gender through the analysis of names, researchers can\nunlock valuable insights into linguistic patterns and cultural norms, which can\nbe applied to practical applications. Hence, this work presents a novel dataset\nfor Japanese name gender detection comprising 64,139 full names in romaji,\nhiragana, and kanji forms, along with their biological genders. Moreover, we\npropose Gendec, a framework for gender detection from Japanese names that\nleverages diverse approaches, including traditional machine learning techniques\nor cutting-edge transfer learning models, to predict the gender associated with\nJapanese names accurately. Through a thorough investigation, the proposed\nframework is expected to be effective and serve potential applications in\nvarious domains.", "published": "2023-11-18 07:46:59", "link": "http://arxiv.org/abs/2311.11001v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joyful: Joint Modality Fusion and Graph Contrastive Learning for\n  Multimodal Emotion Recognition", "abstract": "Multimodal emotion recognition aims to recognize emotions for each utterance\nof multiple modalities, which has received increasing attention for its\napplication in human-machine interaction. Current graph-based methods fail to\nsimultaneously depict global contextual features and local diverse uni-modal\nfeatures in a dialogue. Furthermore, with the number of graph layers\nincreasing, they easily fall into over-smoothing. In this paper, we propose a\nmethod for joint modality fusion and graph contrastive learning for multimodal\nemotion recognition (Joyful), where multimodality fusion, contrastive learning,\nand emotion recognition are jointly optimized. Specifically, we first design a\nnew multimodal fusion mechanism that can provide deep interaction and fusion\nbetween the global contextual and uni-modal specific features. Then, we\nintroduce a graph contrastive learning framework with inter-view and intra-view\ncontrastive losses to learn more distinguishable representations for samples\nwith different sentiments. Extensive experiments on three benchmark datasets\nindicate that Joyful achieved state-of-the-art (SOTA) performance compared to\nall baselines.", "published": "2023-11-18 08:21:42", "link": "http://arxiv.org/abs/2311.11009v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bit Cipher -- A Simple yet Powerful Word Representation System that\n  Integrates Efficiently with Language Models", "abstract": "While Large Language Models (LLMs) become ever more dominant, classic\npre-trained word embeddings sustain their relevance through computational\nefficiency and nuanced linguistic interpretation. Drawing from recent studies\ndemonstrating that the convergence of GloVe and word2vec optimizations all tend\ntowards log-co-occurrence matrix variants, we construct a novel word\nrepresentation system called Bit-cipher that eliminates the need of\nbackpropagation while leveraging contextual information and hyper-efficient\ndimensionality reduction techniques based on unigram frequency, providing\nstrong interpretability, alongside efficiency. We use the bit-cipher algorithm\nto train word vectors via a two-step process that critically relies on a\nhyperparameter -- bits -- that controls the vector dimension. While the first\nstep trains the bit-cipher, the second utilizes it under two different\naggregation modes -- summation or concatenation -- to produce contextually rich\nrepresentations from word co-occurrences. We extend our investigation into\nbit-cipher's efficacy, performing probing experiments on part-of-speech (POS)\ntagging and named entity recognition (NER) to assess its competitiveness with\nclassic embeddings like word2vec and GloVe. Additionally, we explore its\napplicability in LM training and fine-tuning. By replacing embedding layers\nwith cipher embeddings, our experiments illustrate the notable efficiency of\ncipher in accelerating the training process and attaining better optima\ncompared to conventional training paradigms. Experiments on the integration of\nbit-cipher embedding layers with Roberta, T5, and OPT, prior to or as a\nsubstitute for fine-tuning, showcase a promising enhancement to transfer\nlearning, allowing rapid model convergence while preserving competitive\nperformance.", "published": "2023-11-18 08:47:35", "link": "http://arxiv.org/abs/2311.11012v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Responsible AI Considerations in Text Summarization Research: A Review\n  of Current Practices", "abstract": "AI and NLP publication venues have increasingly encouraged researchers to\nreflect on possible ethical considerations, adverse impacts, and other\nresponsible AI issues their work might engender. However, for specific NLP\ntasks our understanding of how prevalent such issues are, or when and why these\nissues are likely to arise, remains limited. Focusing on text summarization --\na common NLP task largely overlooked by the responsible AI community -- we\nexamine research and reporting practices in the current literature. We conduct\na multi-round qualitative analysis of 333 summarization papers from the ACL\nAnthology published between 2020-2022. We focus on how, which, and when\nresponsible AI issues are covered, which relevant stakeholders are considered,\nand mismatches between stated and realized research goals. We also discuss\ncurrent evaluation practices and consider how authors discuss the limitations\nof both prior work and their own work. Overall, we find that relatively few\npapers engage with possible stakeholders or contexts of use, which limits their\nconsideration of potential downstream adverse impacts or other responsible AI\nissues. Based on our findings, we make recommendations on concrete practices\nand research directions.", "published": "2023-11-18 15:35:36", "link": "http://arxiv.org/abs/2311.11103v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language", "abstract": "The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.", "published": "2023-11-18 18:36:16", "link": "http://arxiv.org/abs/2311.11142v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Experts-in-the-Loop: Establishing an Effective Workflow in Crafting\n  Privacy Q&A", "abstract": "Privacy policies play a vital role in safeguarding user privacy as legal\njurisdictions worldwide emphasize the need for transparent data processing.\nWhile the suitability of privacy policies to enhance transparency has been\ncritically discussed, employing conversational AI systems presents unique\nchallenges in informing users effectively. In this position paper, we propose a\ndynamic workflow for transforming privacy policies into privacy\nquestion-and-answer (Q&A) pairs to make privacy policies easily accessible\nthrough conversational AI. Thereby, we facilitate interdisciplinary\ncollaboration among legal experts and conversation designers, while also\nconsidering the utilization of large language models' generative capabilities\nand addressing associated challenges. Our proposed workflow underscores\ncontinuous improvement and monitoring throughout the construction of privacy\nQ&As, advocating for comprehensive review and refinement through an\nexperts-in-the-loop approach.", "published": "2023-11-18 20:32:59", "link": "http://arxiv.org/abs/2311.11161v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding and Mitigating Classification Errors Through Interpretable\n  Token Patterns", "abstract": "State-of-the-art NLP methods achieve human-like performance on many tasks,\nbut make errors nevertheless. Characterizing these errors in easily\ninterpretable terms gives insight into whether a classifier is prone to making\nsystematic errors, but also gives a way to act and improve the classifier. We\npropose to discover those patterns of tokens that distinguish correct and\nerroneous predictions as to obtain global and interpretable descriptions for\narbitrary NLP classifiers. We formulate the problem of finding a succinct and\nnon-redundant set of such patterns in terms of the Minimum Description Length\nprinciple. Through an extensive set of experiments, we show that our method,\nPremise, performs well in practice. Unlike existing solutions, it recovers\nground truth, even on highly imbalanced data over large vocabularies. In VQA\nand NER case studies, we confirm that it gives clear and actionable insight\ninto the systematic errors made by NLP classifiers.", "published": "2023-11-18 00:24:26", "link": "http://arxiv.org/abs/2311.10920v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CAMRA: Copilot for AMR Annotation", "abstract": "In this paper, we introduce CAMRA (Copilot for AMR Annotatations), a\ncutting-edge web-based tool designed for constructing Abstract Meaning\nRepresentation (AMR) from natural language text. CAMRA offers a novel approach\nto deep lexical semantics annotation such as AMR, treating AMR annotation akin\nto coding in programming languages. Leveraging the familiarity of programming\nparadigms, CAMRA encompasses all essential features of existing AMR editors,\nincluding example lookup, while going a step further by integrating Propbank\nroleset lookup as an autocomplete feature within the tool. Notably, CAMRA\nincorporates AMR parser models as coding co-pilots, greatly enhancing the\nefficiency and accuracy of AMR annotators. To demonstrate the tool's\ncapabilities, we provide a live demo accessible at: https://camra.colorado.edu", "published": "2023-11-18 01:26:04", "link": "http://arxiv.org/abs/2311.10928v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Empirical Bayes Framework for Open-Domain Dialogue Generation", "abstract": "To engage human users in meaningful conversation, open-domain dialogue agents\nare required to generate diverse and contextually coherent dialogue. Despite\nrecent advancements, which can be attributed to the usage of pretrained\nlanguage models, the generation of diverse and coherent dialogue remains an\nopen research problem. A popular approach to address this issue involves the\nadaptation of variational frameworks. However, while these approaches\nsuccessfully improve diversity, they tend to compromise on contextual\ncoherence. Hence, we propose the Bayesian Open-domain Dialogue with Empirical\nBayes (BODEB) framework, an empirical bayes framework for constructing an\nBayesian open-domain dialogue agent by leveraging pretrained parameters to\ninform the prior and posterior parameter distributions. Empirical results show\nthat BODEB achieves better results in terms of both diversity and coherence\ncompared to variational frameworks.", "published": "2023-11-18 02:48:41", "link": "http://arxiv.org/abs/2311.10945v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Behavior Optimized Image Generation", "abstract": "The last few years have witnessed great success on image generation, which\nhas crossed the acceptance thresholds of aesthetics, making it directly\napplicable to personal and commercial applications. However, images, especially\nin marketing and advertising applications, are often created as a means to an\nend as opposed to just aesthetic concerns. The goal can be increasing sales,\ngetting more clicks, likes, or image sales (in the case of stock businesses).\nTherefore, the generated images need to perform well on these key performance\nindicators (KPIs), in addition to being aesthetically good. In this paper, we\nmake the first endeavor to answer the question of \"How can one infuse the\nknowledge of the end-goal within the image generation process itself to create\nnot just better-looking images but also \"better-performing'' images?''. We\npropose BoigLLM, an LLM that understands both image content and user behavior.\nBoigLLM knows how an image should look to get a certain required KPI. We show\nthat BoigLLM outperforms 13x larger models such as GPT-3.5 and GPT-4 in this\ntask, demonstrating that while these state-of-the-art models can understand\nimages, they lack information on how these images perform in the real world. To\ngenerate actual pixels of behavior-conditioned images, we train a\ndiffusion-based model (BoigSD) to align with a proposed BoigLLM-defined reward.\nWe show the performance of the overall pipeline on two datasets covering two\ndifferent behaviors: a stock dataset with the number of forward actions as the\nKPI and a dataset containing tweets with the total likes as the KPI, denoted as\nBoigBench. To advance research in the direction of utility-driven image\ngeneration and understanding, we release BoigBench, a benchmark dataset\ncontaining 168 million enterprise tweets with their media, brand account names,\ntime of post, and total likes.", "published": "2023-11-18 07:07:38", "link": "http://arxiv.org/abs/2311.10995v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Combining EEG and NLP Features for Predicting Students' Lecture\n  Comprehension using Ensemble Classification", "abstract": "Electroencephalography (EEG) and Natural Language Processing (NLP) can be\napplied for education to measure students' comprehension in classroom lectures;\ncurrently, the two measures have been used separately. In this work, we propose\na classification framework for predicting students' lecture comprehension in\ntwo tasks: (i) students' confusion after listening to the simulated lecture and\n(ii) the correctness of students' responses to the post-lecture assessment. The\nproposed framework includes EEG and NLP feature extraction, processing, and\nclassification. EEG and NLP features are extracted to construct integrated\nfeatures obtained from recorded EEG signals and sentence-level syntactic\nanalysis, which provide information about specific biomarkers and sentence\nstructures. An ensemble stacking classification method -- a combination of\nmultiple individual models that produces an enhanced predictive model -- is\nstudied to learn from the features to make predictions accurately. Furthermore,\nwe also utilized subjective confusion ratings as another integrated feature to\nenhance classification performance. By doing so, experiment results show that\nthis framework performs better than the baselines, which achieved F1 up to 0.65\nfor predicting confusion and 0.78 for predicting correctness, highlighting that\nutilizing this has helped improve the classification performance.", "published": "2023-11-18 14:35:26", "link": "http://arxiv.org/abs/2311.11088v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Utilizing Speech Emotion Recognition and Recommender Systems for\n  Negative Emotion Handling in Therapy Chatbots", "abstract": "Emotional well-being significantly influences mental health and overall\nquality of life. As therapy chatbots become increasingly prevalent, their\nability to comprehend and respond empathetically to users' emotions remains\nlimited. This paper addresses this limitation by proposing an approach to\nenhance therapy chatbots with auditory perception, enabling them to understand\nusers' feelings and provide human-like empathy. The proposed method\nincorporates speech emotion recognition (SER) techniques using Convolutional\nNeural Network (CNN) models and the ShEMO dataset to accurately detect and\nclassify negative emotions, including anger, fear, and sadness. The SER model\nachieves a validation accuracy of 88%, demonstrating its effectiveness in\nrecognizing emotional states from speech signals. Furthermore, a recommender\nsystem is developed, leveraging the SER model's output to generate personalized\nrecommendations for managing negative emotions, for which a new bilingual\ndataset was generated as well since there is no such dataset available for this\ntask. The recommender model achieves an accuracy of 98% by employing a\ncombination of global vectors for word representation (GloVe) and LSTM models.\nTo provide a more immersive and empathetic user experience, a text-to-speech\nmodel called GlowTTS is integrated, enabling the therapy chatbot to audibly\ncommunicate the generated recommendations to users in both English and Persian.\nThe proposed approach offers promising potential to enhance therapy chatbots by\nproviding them with the ability to recognize and respond to users' emotions,\nultimately improving the delivery of mental health support for both English and\nPersian-speaking users.", "published": "2023-11-18 16:35:55", "link": "http://arxiv.org/abs/2311.11116v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "(Why) Is My Prompt Getting Worse? Rethinking Regression Testing for\n  Evolving LLM APIs", "abstract": "Large Language Models (LLMs) are increasingly integrated into software\napplications. Downstream application developers often access LLMs through APIs\nprovided as a service. However, LLM APIs are often updated silently and\nscheduled to be deprecated, forcing users to continuously adapt to evolving\nmodels. This can cause performance regression and affect prompt design choices,\nas evidenced by our case study on toxicity detection. Based on our case study,\nwe emphasize the need for and re-examine the concept of regression testing for\nevolving LLM APIs. We argue that regression testing LLMs requires fundamental\nchanges to traditional testing approaches, due to different correctness\nnotions, prompting brittleness, and non-determinism in LLM APIs.", "published": "2023-11-18 17:11:12", "link": "http://arxiv.org/abs/2311.11123v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "A Principled Framework for Knowledge-enhanced Large Language Model", "abstract": "Large Language Models (LLMs) are versatile, yet they often falter in tasks\nrequiring deep and reliable reasoning due to issues like hallucinations,\nlimiting their applicability in critical scenarios. This paper introduces a\nrigorously designed framework for creating LLMs that effectively anchor\nknowledge and employ a closed-loop reasoning process, enhancing their\ncapability for in-depth analysis. We dissect the framework to illustrate the\ncontribution of each component to the LLMs' performance, offering a theoretical\nassurance of improved reasoning under well-defined assumptions.", "published": "2023-11-18 18:10:02", "link": "http://arxiv.org/abs/2311.11135v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating the Inclusiveness of Artificial Intelligence Software in\n  Enhancing Project Management Efficiency -- A Review", "abstract": "The rise of advanced technology in project management (PM) highlights a\ncrucial need for inclusiveness. This work examines the enhancement of both\ninclusivity and efficiency in PM through technological integration, focusing on\ndefining and measuring inclusiveness. This approach illuminates how\ninclusivity-centered technology can significantly elevate project outcomes. The\nresearch navigates through the challenges of achieving inclusivity, mainly\nbiases in learning databases and the design process of these technologies,\nassessment of transformative potential of these technologies, particularly in\nautomating tasks like data collection and analysis, thus enabling managers to\nprioritize human-centric aspects of projects. However, the integration of such\ntechnology transcends efficiency, indicating a paradigm shift in understanding\ntheir societal roles. This shift necessitates a new approach in the development\nof these systems to prevent perpetuating social inequalities. We proposed a\nmethodology involving criteria development for evaluating the inclusiveness and\neffectiveness of these technologies. This methodical approach is vital to\ncomprehensively address the challenges and limitations inherent in these\nsystems. Emphasizing the importance of inclusivity, the study advocates for a\nbalance between technological advancement and ethical considerations, calling\nfor a holistic understanding and regulation. In conclusion, the paper\nunderscores that while these technologies can significantly improve outcomes,\ntheir mindful integration, ensuring inclusivity, is paramount. This exploration\ninto the ethical and practical aspects of technology in PM contributes to a\nmore informed and balanced approach within the field.", "published": "2023-11-18 20:22:44", "link": "http://arxiv.org/abs/2311.11159v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Explainable Product Classification for Customs", "abstract": "The task of assigning internationally accepted commodity codes (aka HS codes)\nto traded goods is a critical function of customs offices. Like court decisions\nmade by judges, this task follows the doctrine of precedent and can be\nnontrivial even for experienced officers. Together with the Korea Customs\nService (KCS), we propose a first-ever explainable decision supporting model\nthat suggests the most likely subheadings (i.e., the first six digits) of the\nHS code. The model also provides reasoning for its suggestion in the form of a\ndocument that is interpretable by customs officers. We evaluated the model\nusing 5,000 cases that recently received a classification request. The results\nshowed that the top-3 suggestions made by our model had an accuracy of 93.9\\%\nwhen classifying 925 challenging subheadings. A user study with 32 customs\nexperts further confirmed that our algorithmic suggestions accompanied by\nexplainable reasonings, can substantially reduce the time and effort taken by\ncustoms officers for classification reviews.", "published": "2023-11-18 00:33:48", "link": "http://arxiv.org/abs/2311.10922v1", "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Representing visual classification as a linear combination of words", "abstract": "Explainability is a longstanding challenge in deep learning, especially in\nhigh-stakes domains like healthcare. Common explainability methods highlight\nimage regions that drive an AI model's decision. Humans, however, heavily rely\non language to convey explanations of not only \"where\" but \"what\".\nAdditionally, most explainability approaches focus on explaining individual AI\npredictions, rather than describing the features used by an AI model in\ngeneral. The latter would be especially useful for model and dataset auditing,\nand potentially even knowledge generation as AI is increasingly being used in\nnovel tasks. Here, we present an explainability strategy that uses a\nvision-language model to identify language-based descriptors of a visual\nclassification task. By leveraging a pre-trained joint embedding space between\nimages and text, our approach estimates a new classification task as a linear\ncombination of words, resulting in a weight for each word that indicates its\nalignment with the vision-based classifier. We assess our approach using two\nmedical imaging classification tasks, where we find that the resulting\ndescriptors largely align with clinical knowledge despite a lack of\ndomain-specific language training. However, our approach also identifies the\npotential for 'shortcut connections' in the public datasets used. Towards a\nfunctional measure of explainability, we perform a pilot reader study where we\nfind that the AI-identified words can enable non-expert humans to perform a\nspecialized medical task at a non-trivial level. Altogether, our results\nemphasize the potential of using multimodal foundational models to deliver\nintuitive, language-based explanations of visual tasks.", "published": "2023-11-18 02:00:20", "link": "http://arxiv.org/abs/2311.10933v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Deception Detection from Linguistic and Physiological Data Streams Using\n  Bimodal Convolutional Neural Networks", "abstract": "Deception detection is gaining increasing interest due to ethical and\nsecurity concerns. This paper explores the application of convolutional neural\nnetworks for the purpose of multimodal deception detection. We use a dataset\nbuilt by interviewing 104 subjects about two topics, with one truthful and one\nfalsified response from each subject about each topic. In particular, we make\nthree main contributions. First, we extract linguistic and physiological\nfeatures from this data to train and construct the neural network models.\nSecond, we propose a fused convolutional neural network model using both\nmodalities in order to achieve an improved overall performance. Third, we\ncompare our new approach with earlier methods designed for multimodal deception\ndetection. We find that our system outperforms regular classification methods;\nour results indicate the feasibility of using neural networks for deception\ndetection even in the presence of limited amounts of data.", "published": "2023-11-18 02:44:33", "link": "http://arxiv.org/abs/2311.10944v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adapters: A Unified Library for Parameter-Efficient and Modular Transfer\n  Learning", "abstract": "We introduce Adapters, an open-source library that unifies\nparameter-efficient and modular transfer learning in large language models. By\nintegrating 10 diverse adapter methods into a unified interface, Adapters\noffers ease of use and flexible configuration. Our library allows researchers\nand practitioners to leverage adapter modularity through composition blocks,\nenabling the design of complex adapter setups. We demonstrate the library's\nefficacy by evaluating its performance against full fine-tuning on various NLP\ntasks. Adapters provides a powerful tool for addressing the challenges of\nconventional fine-tuning paradigms and promoting more efficient and modular\ntransfer learning. The library is available via https://adapterhub.ml/adapters.", "published": "2023-11-18 13:53:26", "link": "http://arxiv.org/abs/2311.11077v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Images: An Integrative Multi-modal Approach to Chest X-Ray Report\n  Generation", "abstract": "Image-to-text radiology report generation aims to automatically produce\nradiology reports that describe the findings in medical images. Most existing\nmethods focus solely on the image data, disregarding the other patient\ninformation accessible to radiologists. In this paper, we present a novel\nmulti-modal deep neural network framework for generating chest X-rays reports\nby integrating structured patient data, such as vital signs and symptoms,\nalongside unstructured clinical notes.We introduce a conditioned\ncross-multi-head attention module to fuse these heterogeneous data modalities,\nbridging the semantic gap between visual and textual data. Experiments\ndemonstrate substantial improvements from using additional modalities compared\nto relying on images alone. Notably, our model achieves the highest reported\nperformance on the ROUGE-L metric compared to relevant state-of-the-art models\nin the literature. Furthermore, we employed both human evaluation and clinical\nsemantic similarity measurement alongside word-overlap metrics to improve the\ndepth of quantitative analysis. A human evaluation, conducted by a\nboard-certified radiologist, confirms the model's accuracy in identifying\nhigh-level findings, however, it also highlights that more improvement is\nneeded to capture nuanced details and clinical context.", "published": "2023-11-18 14:37:53", "link": "http://arxiv.org/abs/2311.11090v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Radiology Report Generation Using Transformers Conditioned with\n  Non-imaging Data", "abstract": "Medical image interpretation is central to most clinical applications such as\ndisease diagnosis, treatment planning, and prognostication. In clinical\npractice, radiologists examine medical images and manually compile their\nfindings into reports, which can be a time-consuming process. Automated\napproaches to radiology report generation, therefore, can reduce radiologist\nworkload and improve efficiency in the clinical pathway. While recent\ndeep-learning approaches for automated report generation from medical images\nhave seen some success, most studies have relied on image-derived features\nalone, ignoring non-imaging patient data. Although a few studies have included\nthe word-level contexts along with the image, the use of patient demographics\nis still unexplored. This paper proposes a novel multi-modal transformer\nnetwork that integrates chest x-ray (CXR) images and associated patient\ndemographic information, to synthesise patient-specific radiology reports. The\nproposed network uses a convolutional neural network to extract visual features\nfrom CXRs and a transformer-based encoder-decoder network that combines the\nvisual features with semantic text embeddings of patient demographic\ninformation, to synthesise full-text radiology reports. Data from two public\ndatabases were used to train and evaluate the proposed approach. CXRs and\nreports were extracted from the MIMIC-CXR database and combined with\ncorresponding patients' data MIMIC-IV. Based on the evaluation metrics used\nincluding patient demographic information was found to improve the quality of\nreports generated using the proposed approach, relative to a baseline network\ntrained using CXRs alone. The proposed approach shows potential for enhancing\nradiology report generation by leveraging rich patient metadata and combining\nsemantic text embeddings derived thereof, with medical image-derived visual\nfeatures.", "published": "2023-11-18 14:52:26", "link": "http://arxiv.org/abs/2311.11097v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MultiDelete for Multimodal Machine Unlearning", "abstract": "Machine Unlearning removes specific knowledge about training data samples\nfrom an already trained model. It has significant practical benefits, such as\npurging private, inaccurate, or outdated information from trained models\nwithout the need for complete re-training. Unlearning within a multimodal\nsetting presents unique challenges due to the complex dependencies between\ndifferent data modalities and the expensive cost of training on large\nmultimodal datasets and architectures. This paper presents the first machine\nunlearning approach for multimodal data and models, titled MultiDelete, which\nis designed to decouple associations between unimodal data points during\nunlearning without losing the overall representation strength of the trained\nmodel. MultiDelete advocates for three key properties for effective multimodal\nunlearning: (a): modality decoupling, which effectively decouples the\nassociation between individual unimodal data points marked for deletion,\nrendering them as unrelated data points, (b): multimodal knowledge retention,\nwhich retains the multimodal representation post-unlearning, and (c): unimodal\nknowledge retention, which retains the unimodal representation postunlearning.\nMultiDelete is efficient to train and is not constrained by using a strongly\nconvex loss -- a common restriction among existing baselines. Experiments on\ntwo architectures and four datasets, including image-text and graph-text\ndatasets, show that MultiDelete gains an average improvement of 17.6 points\nover best performing baseline in unlearning multimodal samples, can maintain\nthe multimodal and unimodal knowledge of the original model post unlearning,\nand can provide better protection to unlearned data against adversarial\nattacks.", "published": "2023-11-18 08:30:38", "link": "http://arxiv.org/abs/2311.12047v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "An Embodied Generalist Agent in 3D World", "abstract": "Leveraging massive knowledge from large language models (LLMs), recent\nmachine learning models show notable successes in general-purpose task solving\nin diverse domains such as computer vision and robotics. However, several\nsignificant challenges remain: (i) most of these models rely on 2D images yet\nexhibit a limited capacity for 3D input; (ii) these models rarely explore the\ntasks inherently defined in 3D world, e.g., 3D grounding, embodied reasoning\nand acting. We argue these limitations significantly hinder current models from\nperforming real-world tasks and approaching general intelligence. To this end,\nwe introduce LEO, an embodied multi-modal generalist agent that excels in\nperceiving, grounding, reasoning, planning, and acting in the 3D world. LEO is\ntrained with a unified task interface, model architecture, and objective in two\nstages: (i) 3D vision-language (VL) alignment and (ii) 3D\nvision-language-action (VLA) instruction tuning. We collect large-scale\ndatasets comprising diverse object-level and scene-level tasks, which require\nconsiderable understanding of and interaction with the 3D world. Moreover, we\nmeticulously design an LLM-assisted pipeline to produce high-quality 3D VL\ndata. Through extensive experiments, we demonstrate LEO's remarkable\nproficiency across a wide spectrum of tasks, including 3D captioning, question\nanswering, embodied reasoning, navigation and manipulation. Our ablative\nstudies and scaling analyses further provide valuable insights for developing\nfuture embodied generalist agents. Code and data are available on project page.", "published": "2023-11-18 01:21:38", "link": "http://arxiv.org/abs/2311.12871v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Visual AI and Linguistic Intelligence Through Steerability and\n  Composability", "abstract": "This study explores the capabilities of multimodal large language models\n(LLMs) in handling challenging multistep tasks that integrate language and\nvision, focusing on model steerability, composability, and the application of\nlong-term memory and context understanding. The problem addressed is the LLM's\nability (Nov 2023 GPT-4 Vision Preview) to manage tasks that require\nsynthesizing visual and textual information, especially where stepwise\ninstructions and sequential logic are paramount. The research presents a series\nof 14 creatively and constructively diverse tasks, ranging from AI Lego\nDesigning to AI Satellite Image Analysis, designed to test the limits of\ncurrent LLMs in contexts that previously proved difficult without extensive\nmemory and contextual understanding. Key findings from evaluating 800 guided\ndialogs include notable disparities in task completion difficulty. For\ninstance, 'Image to Ingredient AI Bartender' (Low difficulty) contrasted\nsharply with 'AI Game Self-Player' (High difficulty), highlighting the LLM's\nvarying proficiency in processing complex visual data and generating coherent\ninstructions. Tasks such as 'AI Genetic Programmer' and 'AI Negotiator' showed\nhigh completion difficulty, emphasizing challenges in maintaining context over\nmultiple steps. The results underscore the importance of developing LLMs that\ncombine long-term memory and contextual awareness to mimic human-like thought\nprocesses in complex problem-solving scenarios.", "published": "2023-11-18 22:01:33", "link": "http://arxiv.org/abs/2312.12383v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "The Persian Piano Corpus: A Collection Of Instrument-Based Feature\n  Extracted Data Considering Dastgah", "abstract": "The research in the field of music is rapidly growing, and this trend\nemphasizes the need for comprehensive data. Though researchers have made an\neffort to contribute their own datasets, many data collections lack the\nrequisite inclusivity for comprehensive study because they are frequently\nfocused on particular components of music or other specific topics. We have\nendeavored to address data scarcity by employing an instrument-based approach\nto provide a complete corpus related to the Persian piano. Our piano corpus\nincludes relevant labels for Persian music mode (Dastgah) and comprehensive\nmetadata, allowing for utilization in various popular research areas. The\nfeatures extracted from 2022 Persian piano pieces in The Persian Piano Corpus\n(PPC) have been collected and made available to researchers, aiming for a more\nthorough understanding of Persian music and the role of the piano in it in\nsubsequent steps.", "published": "2023-11-18 13:45:40", "link": "http://arxiv.org/abs/2311.11074v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS", "E.0; I.2"], "primary_category": "cs.SD"}
