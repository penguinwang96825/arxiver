{"title": "Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning", "abstract": "This paper presents a systematic overview of parameter-efficient fine-tuning\nmethods, covering over 50 papers published between early 2019 and mid-2024.\nThese methods aim to address the challenges of fine-tuning large language\nmodels by training only a small subset of parameters. We provide a taxonomy\nthat covers a broad range of methods and present a detailed method comparison\nwith a specific focus on real-life efficiency in fine-tuning multibillion-scale\nlanguage models. We also conduct an extensive head-to-head experimental\ncomparison of 15 diverse PEFT methods, evaluating their performance and\nefficiency on models up to 11B parameters. Our findings reveal that methods\npreviously shown to surpass a strong LoRA baseline face difficulties in\nresource-constrained settings, where hyperparameter optimization is limited and\nthe network is fine-tuned only for a few epochs. Finally, we provide a set of\npractical recommendations for using PEFT methods and outline potential future\nresearch directions.", "published": "2023-03-28 00:06:38", "link": "http://arxiv.org/abs/2303.15647v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint embedding in Hierarchical distance and semantic representation\n  learning for link prediction", "abstract": "The link prediction task aims to predict missing entities or relations in the\nknowledge graph and is essential for the downstream application. Existing\nwell-known models deal with this task by mainly focusing on representing\nknowledge graph triplets in the distance space or semantic space. However, they\ncan not fully capture the information of head and tail entities, nor even make\ngood use of hierarchical level information. Thus, in this paper, we propose a\nnovel knowledge graph embedding model for the link prediction task, namely,\nHIE, which models each triplet (\\textit{h}, \\textit{r}, \\textit{t}) into\ndistance measurement space and semantic measurement space, simultaneously.\nMoreover, HIE is introduced into hierarchical-aware space to leverage rich\nhierarchical information of entities and relations for better representation\nlearning. Specifically, we apply distance transformation operation on the head\nentity in distance space to obtain the tail entity instead of translation-based\nor rotation-based approaches. Experimental results of HIE on four real-world\ndatasets show that HIE outperforms several existing state-of-the-art knowledge\ngraph embedding methods on the link prediction task and deals with complex\nrelations accurately.", "published": "2023-03-28 00:42:29", "link": "http://arxiv.org/abs/2303.15655v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Model and Evaluation: Towards Fairness in Multilingual Text\n  Classification", "abstract": "Recently, more and more research has focused on addressing bias in text\nclassification models. However, existing research mainly focuses on the\nfairness of monolingual text classification models, and research on fairness\nfor multilingual text classification is still very limited. In this paper, we\nfocus on the task of multilingual text classification and propose a debiasing\nframework for multilingual text classification based on contrastive learning.\nOur proposed method does not rely on any external language resources and can be\nextended to any other languages. The model contains four modules: multilingual\ntext representation module, language fusion module, text debiasing module, and\ntext classification module. The multilingual text representation module uses a\nmultilingual pre-trained language model to represent the text, the language\nfusion module makes the semantic spaces of different languages tend to be\nconsistent through contrastive learning, and the text debiasing module uses\ncontrastive learning to make the model unable to identify sensitive attributes'\ninformation. The text classification module completes the basic tasks of\nmultilingual text classification. In addition, the existing research on the\nfairness of multilingual text classification is relatively simple in the\nevaluation mode. The evaluation method of fairness is the same as the\nmonolingual equality difference evaluation method, that is, the evaluation is\nperformed on a single language. We propose a multi-dimensional fairness\nevaluation framework for multilingual text classification, which evaluates the\nmodel's monolingual equality difference, multilingual equality difference,\nmultilingual equality performance difference, and destructiveness of the\nfairness strategy. We hope that our work can provide a more general debiasing\nmethod and a more comprehensive evaluation framework for multilingual text\nfairness tasks.", "published": "2023-03-28 03:00:01", "link": "http://arxiv.org/abs/2303.15697v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bias or Diversity? Unraveling Fine-Grained Thematic Discrepancy in U.S.\n  News Headlines", "abstract": "There is a broad consensus that news media outlets incorporate ideological\nbiases in their news articles. However, prior studies on measuring the\ndiscrepancies among media outlets and further dissecting the origins of\nthematic differences suffer from small sample sizes and limited scope and\ngranularity. In this study, we use a large dataset of 1.8 million news\nheadlines from major U.S. media outlets spanning from 2014 to 2022 to\nthoroughly track and dissect the fine-grained thematic discrepancy in U.S. news\nmedia. We employ multiple correspondence analysis (MCA) to quantify the\nfine-grained thematic discrepancy related to four prominent topics - domestic\npolitics, economic issues, social issues, and foreign affairs in order to\nderive a more holistic analysis. Additionally, we compare the most frequent\n$n$-grams in media headlines to provide further qualitative insights into our\nanalysis. Our findings indicate that on domestic politics and social issues,\nthe discrepancy can be attributed to a certain degree of media bias. Meanwhile,\nthe discrepancy in reporting foreign affairs is largely attributed to the\ndiversity in individual journalistic styles. Finally, U.S. media outlets show\nconsistency and high similarity in their coverage of economic issues.", "published": "2023-03-28 03:31:37", "link": "http://arxiv.org/abs/2303.15708v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis Dataset in Moroccan Dialect: Bridging the Gap Between\n  Arabic and Latin Scripted dialect", "abstract": "Sentiment analysis, the automated process of determining emotions or opinions\nexpressed in text, has seen extensive exploration in the field of natural\nlanguage processing. However, one aspect that has remained underrepresented is\nthe sentiment analysis of the Moroccan dialect, which boasts a unique\nlinguistic landscape and the coexistence of multiple scripts. Previous works in\nsentiment analysis primarily targeted dialects employing Arabic script. While\nthese efforts provided valuable insights, they may not fully capture the\ncomplexity of Moroccan web content, which features a blend of Arabic and Latin\nscript. As a result, our study emphasizes the importance of extending sentiment\nanalysis to encompass the entire spectrum of Moroccan linguistic diversity.\nCentral to our research is the creation of the largest public dataset for\nMoroccan dialect sentiment analysis that incorporates not only Moroccan dialect\nwritten in Arabic script but also in Latin letters. By assembling a diverse\nrange of textual data, we were able to construct a dataset with a range of 20\n000 manually labeled text in Moroccan dialect and also publicly available lists\nof stop words in Moroccan dialect. To dive into sentiment analysis, we\nconducted a comparative study on multiple Machine learning models to assess\ntheir compatibility with our dataset. Experiments were performed using both raw\nand preprocessed data to show the importance of the preprocessing step. We were\nable to achieve 92% accuracy in our model and to further prove its liability we\ntested our model on smaller publicly available datasets of Moroccan dialect and\nthe results were favorable.", "published": "2023-03-28 14:02:42", "link": "http://arxiv.org/abs/2303.15987v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synthetically generated text for supervised text analysis", "abstract": "Supervised text models are a valuable tool for political scientists but\npresent several obstacles to their use, including the expense of hand-labeling\ndocuments, the difficulty of retrieving rare relevant documents for annotation,\nand copyright and privacy concerns involved in sharing annotated documents.\nThis article proposes a partial solution to these three issues, in the form of\ncontrolled generation of synthetic text with large language models. I provide a\nconceptual overview of text generation, guidance on when researchers should\nprefer different techniques for generating synthetic text, a discussion of\nethics, and a simple technique for improving the quality of synthetic text. I\ndemonstrate the usefulness of synthetic text with three applications:\ngenerating synthetic tweets describing the fighting in Ukraine, synthetic news\narticles describing specified political events for training an event detection\nsystem, and a multilingual corpus of populist manifesto statements for training\na sentence-level populism classifier.", "published": "2023-03-28 14:55:13", "link": "http://arxiv.org/abs/2303.16028v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hallucinations in Large Multilingual Translation Models", "abstract": "Large-scale multilingual machine translation systems have demonstrated\nremarkable ability to translate directly between numerous languages, making\nthem increasingly appealing for real-world applications. However, when deployed\nin the wild, these models may generate hallucinated translations which have the\npotential to severely undermine user trust and raise safety concerns. Existing\nresearch on hallucinations has primarily focused on small bilingual models\ntrained on high-resource languages, leaving a gap in our understanding of\nhallucinations in massively multilingual models across diverse translation\nscenarios. In this work, we fill this gap by conducting a comprehensive\nanalysis on both the M2M family of conventional neural machine translation\nmodels and ChatGPT, a general-purpose large language model~(LLM) that can be\nprompted for translation. Our investigation covers a broad spectrum of\nconditions, spanning over 100 translation directions across various resource\nlevels and going beyond English-centric language pairs. We provide key insights\nregarding the prevalence, properties, and mitigation of hallucinations, paving\nthe way towards more responsible and reliable machine translation systems.", "published": "2023-03-28 16:17:59", "link": "http://arxiv.org/abs/2303.16104v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Countering Essentialism through Social Bias Reasoning", "abstract": "Essentialist beliefs (i.e., believing that members of the same group are\nfundamentally alike) play a central role in social stereotypes and can lead to\nharm when left unchallenged. In our work, we conduct exploratory studies into\nthe task of countering essentialist beliefs (e.g., ``liberals are stupid'').\nDrawing on prior work from psychology and NLP, we construct five types of\ncounterstatements and conduct human studies on the effectiveness of these\ndifferent strategies. Our studies also investigate the role in choosing a\ncounterstatement of the level of explicitness with which an essentialist belief\nis conveyed. We find that statements that broaden the scope of a stereotype\n(e.g., to other groups, as in ``conservatives can also be stupid'') are the\nmost popular countering strategy. We conclude with a discussion of challenges\nand open questions for future work in this area (e.g., improving factuality,\nstudying community-specific variation) and we emphasize the importance of work\nat the intersection of NLP and psychology.", "published": "2023-03-28 17:34:59", "link": "http://arxiv.org/abs/2303.16173v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparative Analysis of CHATGPT and the evolution of language models", "abstract": "Interest in Large Language Models (LLMs) has increased drastically since the\nemergence of ChatGPT and the outstanding positive societal response to the ease\nwith which it performs tasks in Natural Language Processing (NLP). The triumph\nof ChatGPT, however, is how it seamlessly bridges the divide between language\ngeneration and knowledge models. In some cases, it provides anecdotal evidence\nof a framework for replicating human intuition over a knowledge domain. This\npaper highlights the prevailing ideas in NLP, including machine translation,\nmachine summarization, question-answering, and language generation, and\ncompares the performance of ChatGPT with the major algorithms in each of these\ncategories using the Spontaneous Quality (SQ) score. A strategy for validating\nthe arguments and results of ChatGPT is presented summarily as an example of\nsafe, large-scale adoption of LLMs.", "published": "2023-03-28 03:11:28", "link": "http://arxiv.org/abs/2304.02468v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sejarah dan Perkembangan Teknik Natural Language Processing (NLP) Bahasa\n  Indonesia: Tinjauan tentang sejarah, perkembangan teknologi, dan aplikasi NLP\n  dalam bahasa Indonesia", "abstract": "This study provides an overview of the history of the development of Natural\nLanguage Processing (NLP) in the context of the Indonesian language, with a\nfocus on the basic technologies, methods, and practical applications that have\nbeen developed. This review covers developments in basic NLP technologies such\nas stemming, part-of-speech tagging, and related methods; practical\napplications in cross-language information retrieval systems, information\nextraction, and sentiment analysis; and methods and techniques used in\nIndonesian language NLP research, such as machine learning, statistics-based\nmachine translation, and conflict-based approaches. This study also explores\nthe application of NLP in Indonesian language industry and research and\nidentifies challenges and opportunities in Indonesian language NLP research and\ndevelopment. Recommendations for future Indonesian language NLP research and\ndevelopment include developing more efficient methods and technologies,\nexpanding NLP applications, increasing sustainability, further research into\nthe potential of NLP, and promoting interdisciplinary collaboration. It is\nhoped that this review will help researchers, practitioners, and the government\nto understand the development of Indonesian language NLP and identify\nopportunities for further research and development.", "published": "2023-03-28 02:31:47", "link": "http://arxiv.org/abs/2304.02746v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "ChatGPT4PCG Competition: Character-like Level Generation for Science\n  Birds", "abstract": "This paper presents the first ChatGPT4PCG Competition at the 2023 IEEE\nConference on Games. The objective of this competition is for participants to\ncreate effective prompts for ChatGPT--enabling it to generate Science Birds\nlevels with high stability and character-like qualities--fully using their\ncreativity as well as prompt engineering skills. ChatGPT is a conversational\nagent developed by OpenAI. Science Birds is selected as the competition\nplatform because designing an Angry Birds-like level is not a trivial task due\nto the in-game gravity; the quality of the levels is determined by their\nstability. To lower the entry barrier to the competition, we limit the task to\nthe generation of capitalized English alphabetical characters. We also allow\nonly a single prompt to be used for generating all the characters. Here, the\nquality of the generated levels is determined by their stability and similarity\nto the given characters. A sample prompt is provided to participants for their\nreference. An experiment is conducted to determine the effectiveness of several\nmodified versions of this sample prompt on level stability and similarity by\ntesting them on several characters. To the best of our knowledge, we believe\nthat ChatGPT4PCG is the first competition of its kind and hope to inspire\nenthusiasm for prompt engineering in procedural content generation.", "published": "2023-03-28 01:07:38", "link": "http://arxiv.org/abs/2303.15662v3", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.8"], "primary_category": "cs.AI"}
{"title": "Pre-training Transformers for Knowledge Graph Completion", "abstract": "Learning transferable representation of knowledge graphs (KGs) is challenging\ndue to the heterogeneous, multi-relational nature of graph structures. Inspired\nby Transformer-based pretrained language models' success on learning\ntransferable representation for texts, we introduce a novel inductive KG\nrepresentation model (iHT) for KG completion by large-scale pre-training. iHT\nconsists of a entity encoder (e.g., BERT) and a neighbor-aware relational\nscoring function both parameterized by Transformers. We first pre-train iHT on\na large KG dataset, Wikidata5M. Our approach achieves new state-of-the-art\nresults on matched evaluations, with a relative improvement of more than 25% in\nmean reciprocal rank over previous SOTA models. When further fine-tuned on\nsmaller KGs with either entity and relational shifts, pre-trained iHT\nrepresentations are shown to be transferable, significantly improving the\nperformance on FB15K-237 and WN18RR.", "published": "2023-03-28 02:10:37", "link": "http://arxiv.org/abs/2303.15682v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluation of ChatGPT for NLP-based Mental Health Applications", "abstract": "Large language models (LLM) have been successful in several natural language\nunderstanding tasks and could be relevant for natural language processing\n(NLP)-based mental health application research. In this work, we report the\nperformance of LLM-based ChatGPT (with gpt-3.5-turbo backend) in three\ntext-based mental health classification tasks: stress detection (2-class\nclassification), depression detection (2-class classification), and suicidality\ndetection (5-class classification). We obtained annotated social media posts\nfor the three classification tasks from public datasets. Then ChatGPT API\nclassified the social media posts with an input prompt for classification. We\nobtained F1 scores of 0.73, 0.86, and 0.37 for stress detection, depression\ndetection, and suicidality detection, respectively. A baseline model that\nalways predicted the dominant class resulted in F1 scores of 0.35, 0.60, and\n0.19. The zero-shot classification accuracy obtained with ChatGPT indicates a\npotential use of language models for mental health classification tasks.", "published": "2023-03-28 04:47:43", "link": "http://arxiv.org/abs/2303.15727v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Multi-Granularity Matching Attention Network for Query Intent\n  Classification in E-commerce Retrieval", "abstract": "Query intent classification, which aims at assisting customers to find\ndesired products, has become an essential component of the e-commerce search.\nExisting query intent classification models either design more exquisite models\nto enhance the representation learning of queries or explore label-graph and\nmulti-task to facilitate models to learn external information. However, these\nmodels cannot capture multi-granularity matching features from queries and\ncategories, which makes them hard to mitigate the gap in the expression between\ninformal queries and categories.\n  This paper proposes a Multi-granularity Matching Attention Network (MMAN),\nwhich contains three modules: a self-matching module, a char-level matching\nmodule, and a semantic-level matching module to comprehensively extract\nfeatures from the query and a query-category interaction matrix. In this way,\nthe model can eliminate the difference in expression between queries and\ncategories for query intent classification. We conduct extensive offline and\nonline A/B experiments, and the results show that the MMAN significantly\noutperforms the strong baselines, which shows the superiority and effectiveness\nof MMAN. MMAN has been deployed in production and brings great commercial value\nfor our company.", "published": "2023-03-28 10:25:17", "link": "http://arxiv.org/abs/2303.15870v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Do Neural Topic Models Really Need Dropout? Analysis of the Effect of\n  Dropout in Topic Modeling", "abstract": "Dropout is a widely used regularization trick to resolve the overfitting\nissue in large feedforward neural networks trained on a small dataset, which\nperforms poorly on the held-out test subset. Although the effectiveness of this\nregularization trick has been extensively studied for convolutional neural\nnetworks, there is a lack of analysis of it for unsupervised models and in\nparticular, VAE-based neural topic models. In this paper, we have analyzed the\nconsequences of dropout in the encoder as well as in the decoder of the VAE\narchitecture in three widely used neural topic models, namely, contextualized\ntopic model (CTM), ProdLDA, and embedded topic model (ETM) using four publicly\navailable datasets. We characterize the dropout effect on these models in terms\nof the quality and predictive performance of the generated topics.", "published": "2023-03-28 13:45:39", "link": "http://arxiv.org/abs/2303.15973v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Carolina: a General Corpus of Contemporary Brazilian Portuguese with\n  Provenance, Typology and Versioning Information", "abstract": "This paper presents the first publicly available version of the Carolina\nCorpus and discusses its future directions. Carolina is a large open corpus of\nBrazilian Portuguese texts under construction using web-as-corpus methodology\nenhanced with provenance, typology, versioning, and text integrality. The\ncorpus aims at being used both as a reliable source for research in Linguistics\nand as an important resource for Computer Science research on language models,\ncontributing towards removing Portuguese from the set of low-resource\nlanguages. Here we present the construction of the corpus methodology,\ncomparing it with other existing methodologies, as well as the corpus current\nstate: Carolina's first public version has $653,322,577$ tokens, distributed\nover $7$ broad types. Each text is annotated with several different metadata\ncategories in its header, which we developed using TEI annotation standards. We\nalso present ongoing derivative works and invite NLP researchers to contribute\nwith their own.", "published": "2023-03-28 16:09:40", "link": "http://arxiv.org/abs/2303.16098v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "When Good and Reproducible Results are a Giant with Feet of Clay: The\n  Importance of Software Quality in NLP", "abstract": "Despite its crucial role in research experiments, code correctness is often\npresumed only on the basis of the perceived quality of results. This assumption\ncomes with the risk of erroneous outcomes and potentially misleading findings.\nTo address this issue, we posit that the current focus on reproducibility\nshould go hand in hand with the emphasis on software quality. We present a case\nstudy in which we identify and fix three bugs in widely used implementations of\nthe state-of-the-art Conformer architecture. Through experiments on speech\nrecognition and translation in various languages, we demonstrate that the\npresence of bugs does not prevent the achievement of good and reproducible\nresults, which however can lead to incorrect conclusions that potentially\nmisguide future research. As a countermeasure, we propose a Code-quality\nChecklist and release pangoliNN, a library dedicated to testing neural models,\nwith the goal of promoting coding best practices and improving research\nsoftware quality within the NLP community.", "published": "2023-03-28 17:28:52", "link": "http://arxiv.org/abs/2303.16166v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Generalizable End-to-End Task-Oriented Dialog System using\n  Context Summarization and Domain Schema", "abstract": "Task-oriented dialog systems empower users to accomplish their goals by\nfacilitating intuitive and expressive natural language interactions.\nState-of-the-art approaches in task-oriented dialog systems formulate the\nproblem as a conditional sequence generation task and fine-tune pre-trained\ncausal language models in the supervised setting. This requires labeled\ntraining data for each new domain or task, and acquiring such data is\nprohibitively laborious and expensive, thus making it a bottleneck for scaling\nsystems to a wide range of domains. To overcome this challenge, we introduce a\nnovel Zero-Shot generalizable end-to-end Task-oriented Dialog system, ZS-ToD,\nthat leverages domain schemas to allow for robust generalization to unseen\ndomains and exploits effective summarization of the dialog history. We employ\nGPT-2 as a backbone model and introduce a two-step training process where the\ngoal of the first step is to learn the general structure of the dialog data and\nthe second step optimizes the response generation as well as intermediate\noutputs, such as dialog state and system actions. As opposed to\nstate-of-the-art systems that are trained to fulfill certain intents in the\ngiven domains and memorize task-specific conversational patterns, ZS-ToD learns\ngeneric task-completion skills by comprehending domain semantics via domain\nschemas and generalizing to unseen domains seamlessly. We conduct an extensive\nexperimental evaluation on SGD and SGD-X datasets that span up to 20 unique\ndomains and ZS-ToD outperforms state-of-the-art systems on key metrics, with an\nimprovement of +17% on joint goal accuracy and +5 on inform. Additionally, we\npresent a detailed ablation study to demonstrate the effectiveness of the\nproposed components and training mechanism", "published": "2023-03-28 18:56:31", "link": "http://arxiv.org/abs/2303.16252v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scalable handwritten text recognition system for lexicographic sources\n  of under-resourced languages and alphabets", "abstract": "The paper discusses an approach to decipher large collections of handwritten\nindex cards of historical dictionaries. Our study provides a working solution\nthat reads the cards, and links their lemmas to a searchable list of dictionary\nentries, for a large historical dictionary entitled the Dictionary of the 17th-\nand 18th-century Polish, which comprizes 2.8 million index cards. We apply a\ntailored handwritten text recognition (HTR) solution that involves (1) an\noptimized detection model; (2) a recognition model to decipher the handwritten\ncontent, designed as a spatial transformer network (STN) followed by\nconvolutional neural network (RCNN) with a connectionist temporal\nclassification layer (CTC), trained using a synthetic set of 500,000 generated\nPolish words of different length; (3) a post-processing step using constrained\nWord Beam Search (WBC): the predictions were matched against a list of\ndictionary entries known in advance. Our model achieved the accuracy of 0.881\non the word level, which outperforms the base RCNN model. Within this study we\nproduced a set of 20,000 manually annotated index cards that can be used for\nfuture benchmarks and transfer learning HTR applications.", "published": "2023-03-28 19:06:27", "link": "http://arxiv.org/abs/2303.16256v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Writing Assistants Should Model Social Factors of Language", "abstract": "Intelligent writing assistants powered by large language models (LLMs) are\nmore popular today than ever before, but their further widespread adoption is\nprecluded by sub-optimal performance. In this position paper, we argue that a\nmajor reason for this sub-optimal performance and adoption is a singular focus\non the information content of language while ignoring its social aspects. We\nanalyze the different dimensions of these social factors in the context of\nwriting assistants and propose their incorporation into building smarter, more\neffective, and truly personalized writing assistants that would enrich the user\nexperience and contribute to increased user adoption.", "published": "2023-03-28 19:38:57", "link": "http://arxiv.org/abs/2303.16275v1", "categories": ["cs.CL", "cs.HC", "I.2.7; K.4.2; H.5.0"], "primary_category": "cs.CL"}
{"title": "ChatGPT or academic scientist? Distinguishing authorship with over 99%\n  accuracy using off-the-shelf machine learning tools", "abstract": "ChatGPT has enabled access to AI-generated writing for the masses, and within\njust a few months, this product has disrupted the knowledge economy, initiating\na culture shift in the way people work, learn, and write. The need to\ndiscriminate human writing from AI is now both critical and urgent,\nparticularly in domains like higher education and academic writing, where AI\nhad not been a significant threat or contributor to authorship. Addressing this\nneed, we developed a method for discriminating text generated by ChatGPT from\n(human) academic scientists, relying on prevalent and accessible supervised\nclassification methods. We focused on how a particular group of humans,\nacademic scientists, write differently than ChatGPT, and this targeted approach\nled to the discovery of new features for discriminating (these) humans from AI;\nas examples, scientists write long paragraphs and have a penchant for equivocal\nlanguage, frequently using words like but, however, and although. With a set of\n20 features, including the aforementioned ones and others, we built a model\nthat assigned the author, as human or AI, at well over 99% accuracy, resulting\nin 20 times fewer misclassified documents compared to the field-leading\napproach. This strategy for discriminating a particular set of humans writing\nfrom AI could be further adapted and developed by others with basic skills in\nsupervised classification, enabling access to many highly accurate and targeted\nmodels for detecting AI usage in academic writing and beyond.", "published": "2023-03-28 23:16:00", "link": "http://arxiv.org/abs/2303.16352v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "How can Deep Learning Retrieve the Write-Missing Additional Diagnosis\n  from Chinese Electronic Medical Record For DRG", "abstract": "The purpose of write-missing diagnosis detection is to find diseases that\nhave been clearly diagnosed from medical records but are missed in the\ndischarge diagnosis. Unlike the definition of missed diagnosis, the\nwrite-missing diagnosis is clearly manifested in the medical record without\nfurther reasoning. The write-missing diagnosis is a common problem, often\ncaused by physician negligence. The write-missing diagnosis will result in an\nincomplete diagnosis of medical records. While under DRG grouping, the\nwrite-missing diagnoses will miss important additional diagnoses (CC, MCC),\nthus affecting the correct rate of DRG enrollment.\n  Under the circumstance that countries generally start to adopt DRG enrollment\nand payment, the problem of write-missing diagnosis is a common and serious\nproblem. The current manual-based method is expensive due to the complex\ncontent of the full medical record. We think this problem is suitable to be\nsolved as natural language processing. But to the best of our knowledge, no\nresearchers have conducted research on this problem based on natural language\nprocessing methods.\n  We propose a framework for solving the problem of write-missing diagnosis,\nwhich mainly includes three modules: disease recall module, disease context\nlogic judgment module, and disease relationship comparison module. Through this\nframework, we verify that the problem of write-missing diagnosis can be solved\nwell, and the results are interpretable. At the same time, we propose advanced\nsolutions for the disease context logic judgment module and disease\nrelationship comparison module, which have obvious advantages compared with the\nmainstream methods of the same type of problems. Finally, we verified the value\nof our proposed framework under DRG medical insurance payment in a tertiary\nhospital.", "published": "2023-03-28 08:56:31", "link": "http://arxiv.org/abs/2303.16757v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Models Trained on Media Diets Can Predict Public Opinion", "abstract": "Public opinion reflects and shapes societal behavior, but the traditional\nsurvey-based tools to measure it are limited. We introduce a novel approach to\nprobe media diet models -- language models adapted to online news, TV\nbroadcast, or radio show content -- that can emulate the opinions of\nsubpopulations that have consumed a set of media. To validate this method, we\nuse as ground truth the opinions expressed in U.S. nationally representative\nsurveys on COVID-19 and consumer confidence. Our studies indicate that this\napproach is (1) predictive of human judgements found in survey response\ndistributions and robust to phrasing and channels of media exposure, (2) more\naccurate at modeling people who follow media more closely, and (3) aligned with\nliterature on which types of opinions are affected by media consumption.\nProbing language models provides a powerful new method for investigating media\neffects, has practical applications in supplementing polls and forecasting\npublic opinion, and suggests a need for further study of the surprising\nfidelity with which neural language models can predict human responses.", "published": "2023-03-28 06:08:25", "link": "http://arxiv.org/abs/2303.16779v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Translate the Beauty in Songs: Jointly Learning to Align Melody and\n  Translate Lyrics", "abstract": "Song translation requires both translation of lyrics and alignment of music\nnotes so that the resulting verse can be sung to the accompanying melody, which\nis a challenging problem that has attracted some interests in different aspects\nof the translation process. In this paper, we propose Lyrics-Melody Translation\nwith Adaptive Grouping (LTAG), a holistic solution to automatic song\ntranslation by jointly modeling lyrics translation and lyrics-melody alignment.\nIt is a novel encoder-decoder framework that can simultaneously translate the\nsource lyrics and determine the number of aligned notes at each decoding step\nthrough an adaptive note grouping module. To address data scarcity, we\ncommissioned a small amount of training data annotated specifically for this\ntask and used large amounts of augmented data through back-translation.\nExperiments conducted on an English-Chinese song translation data set show the\neffectiveness of our model in both automatic and human evaluation.", "published": "2023-03-28 03:17:59", "link": "http://arxiv.org/abs/2303.15705v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Explicit Planning Helps Language Models in Logical Reasoning", "abstract": "Language models have been shown to perform remarkably well on a wide range of\nnatural language processing tasks. In this paper, we propose LEAP, a novel\nsystem that uses language models to perform multi-step logical reasoning and\nincorporates explicit planning into the inference procedure. Explicit planning\nenables the system to make more informed reasoning decisions at each step by\nlooking ahead into their future effects. Moreover, we propose a training\nstrategy that safeguards the planning process from being led astray by spurious\nfeatures. Our full system significantly outperforms other competing methods on\nmultiple standard datasets. When using small T5 models as its core selection\nand deduction components, our system performs competitively compared to GPT-3\ndespite having only about 1B parameters (i.e., 175 times smaller than GPT-3).\nWhen using GPT-3.5, it significantly outperforms chain-of-thought prompting on\nthe challenging PrOntoQA dataset. We have conducted extensive empirical studies\nto demonstrate that explicit planning plays a crucial role in the system's\nperformance.", "published": "2023-03-28 03:55:03", "link": "http://arxiv.org/abs/2303.15714v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Soft-prompt tuning to predict lung cancer using primary care free-text\n  Dutch medical notes", "abstract": "We investigate different natural language processing (NLP) approaches based\non contextualised word representations for the problem of early prediction of\nlung cancer using free-text patient medical notes of Dutch primary care\nphysicians. Because lung cancer has a low prevalence in primary care, we also\naddress the problem of classification under highly imbalanced classes.\nSpecifically, we use large Transformer-based pretrained language models (PLMs)\nand investigate: 1) how \\textit{soft prompt-tuning} -- an NLP technique used to\nadapt PLMs using small amounts of training data -- compares to standard model\nfine-tuning; 2) whether simpler static word embedding models (WEMs) can be more\nrobust compared to PLMs in highly imbalanced settings; and 3) how models fare\nwhen trained on notes from a small number of patients. We find that 1)\nsoft-prompt tuning is an efficient alternative to standard model fine-tuning;\n2) PLMs show better discrimination but worse calibration compared to simpler\nstatic word embedding models as the classification problem becomes more\nimbalanced; and 3) results when training models on small number of patients are\nmixed and show no clear differences between PLMs and WEMs. All our code is\navailable open source in\n\\url{https://bitbucket.org/aumc-kik/prompt_tuning_cancer_prediction/}.", "published": "2023-03-28 09:36:53", "link": "http://arxiv.org/abs/2303.15846v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Exposing and Addressing Cross-Task Inconsistency in Unified\n  Vision-Language Models", "abstract": "As general purpose vision models get increasingly effective at a wide set of\ntasks, it is imperative that they be consistent across the tasks they support.\nInconsistent AI models are considered brittle and untrustworthy by human users\nand are more challenging to incorporate into larger systems that take\ndependencies on their outputs. Measuring consistency between very heterogeneous\ntasks that might include outputs in different modalities is challenging since\nit is difficult to determine if the predictions are consistent with one\nanother. As a solution, we introduce a benchmark dataset, CocoCon, where we\ncreate contrast sets by modifying test instances for multiple tasks in small\nbut semantically meaningful ways to change the gold label and outline metrics\nfor measuring if a model is consistent by ranking the original and perturbed\ninstances across tasks. We find that state-of-the-art vision-language models\nsuffer from a surprisingly high degree of inconsistent behavior across tasks,\nespecially for more heterogeneous tasks. To alleviate this issue, we propose a\nrank correlation-based auxiliary training objective, computed over large\nautomatically created cross-task contrast sets, that improves the multi-task\nconsistency of large unified models while retaining their original accuracy on\ndownstream tasks.", "published": "2023-03-28 16:57:12", "link": "http://arxiv.org/abs/2303.16133v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Language-Guided Audio-Visual Source Separation via Trimodal Consistency", "abstract": "We propose a self-supervised approach for learning to perform audio source\nseparation in videos based on natural language queries, using only unlabeled\nvideo and audio pairs as training data. A key challenge in this task is\nlearning to associate the linguistic description of a sound-emitting object to\nits visual features and the corresponding components of the audio waveform, all\nwithout access to annotations during training. To overcome this challenge, we\nadapt off-the-shelf vision-language foundation models to provide pseudo-target\nsupervision via two novel loss functions and encourage a stronger alignment\nbetween the audio, visual and natural language modalities. During inference,\nour approach can separate sounds given text, video and audio input, or given\ntext and audio input alone. We demonstrate the effectiveness of our\nself-supervised approach on three audio-visual separation datasets, including\nMUSIC, SOLOS and AudioSet, where we outperform state-of-the-art strongly\nsupervised approaches despite not using object detectors or text labels during\ntraining.", "published": "2023-03-28 22:45:40", "link": "http://arxiv.org/abs/2303.16342v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving Code Generation by Training with Natural Language Feedback", "abstract": "The potential for pre-trained large language models (LLMs) to use natural\nlanguage feedback at inference time has been an exciting recent development. We\nbuild upon this observation by formalizing an algorithm for learning from\nnatural language feedback at training time instead, which we call Imitation\nlearning from Language Feedback (ILF). ILF requires only a small amount of\nhuman-written feedback during training and does not require the same feedback\nat test time, making it both user-friendly and sample-efficient. We further\nshow that ILF can be seen as a form of minimizing the KL divergence to the\nground truth distribution and demonstrate a proof-of-concept on a neural\nprogram synthesis task. We use ILF to improve a Codegen-Mono 6.1B model's\npass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python\nProblems (MBPP) benchmark, outperforming both fine-tuning on MBPP and\nfine-tuning on repaired programs written by humans. Overall, our results\nsuggest that learning from human-written natural language feedback is both more\neffective and sample-efficient than training exclusively on demonstrations for\nimproving an LLM's performance on code generation tasks.", "published": "2023-03-28 16:15:31", "link": "http://arxiv.org/abs/2303.16749v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Training Language Models with Language Feedback at Scale", "abstract": "Pretrained language models often generate outputs that are not in line with\nhuman preferences, such as harmful text or factually incorrect summaries.\nRecent work approaches the above issues by learning from a simple form of human\nfeedback: comparisons between pairs of model-generated outputs. However,\ncomparison feedback only conveys limited information about human preferences.\nIn this paper, we introduce Imitation learning from Language Feedback (ILF), a\nnew approach that utilizes more informative language feedback. ILF consists of\nthree steps that are applied iteratively: first, conditioning the language\nmodel on the input, an initial LM output, and feedback to generate refinements.\nSecond, selecting the refinement incorporating the most feedback. Third,\nfinetuning the language model to maximize the likelihood of the chosen\nrefinement given the input. We show theoretically that ILF can be viewed as\nBayesian Inference, similar to Reinforcement Learning from human feedback. We\nevaluate ILF's effectiveness on a carefully-controlled toy task and a realistic\nsummarization task. Our experiments demonstrate that large language models\naccurately incorporate feedback and that finetuning with ILF scales well with\nthe dataset size, even outperforming finetuning on human summaries. Learning\nfrom both language and comparison feedback outperforms learning from each\nalone, achieving human-level summarization performance.", "published": "2023-03-28 17:04:15", "link": "http://arxiv.org/abs/2303.16755v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init\n  Attention", "abstract": "We present LLaMA-Adapter, a lightweight adaption method to efficiently\nfine-tune LLaMA into an instruction-following model. Using 52K self-instruct\ndemonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon\nthe frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8\nA100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and\nprepend them to the word tokens at higher transformer layers. Then, a\nzero-initialized attention mechanism with zero gating is proposed, which\nadaptively injects the new instructional cues into LLaMA, while effectively\npreserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter\ncan generate high-quality responses, comparable to Alpaca with fully fine-tuned\n7B parameters. Besides language commands, our approach can be simply extended\nto multi-modal instructions for learning image-conditioned LLaMA model, which\nachieves superior reasoning performance on ScienceQA and COCO Caption\nbenchmarks. Furthermore, we also evaluate the zero-initialized attention\nmechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on\ntraditional vision and language tasks, demonstrating the superior\ngeneralization capacity of our approach. Code is released at\nhttps://github.com/OpenGVLab/LLaMA-Adapter.", "published": "2023-03-28 17:59:12", "link": "http://arxiv.org/abs/2303.16199v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "A \"Perspectival\" Mirror of the Elephant: Investigating Language Bias on\n  Google, ChatGPT, YouTube, and Wikipedia", "abstract": "Contrary to Google Search's mission of delivering information from \"many\nangles so you can form your own understanding of the world,\" we find that\nGoogle and its most prominent returned results - Wikipedia and YouTube - simply\nreflect a narrow set of culturally dominant views tied to the search language\nfor complex topics like \"Buddhism,\" \"Liberalism,\" \"colonization,\" \"Iran\" and\n\"America.\" Simply stated, they present, to varying degrees, distinct\ninformation across the same search in different languages, a phenomenon we call\nlanguage bias. This paper presents evidence and analysis of language bias and\ndiscusses its larger social implications. We find that our online searches and\nemerging tools like ChatGPT turn us into the proverbial blind person touching a\nsmall portion of an elephant, ignorant of the existence of other cultural\nperspectives. Language bias sets a strong yet invisible cultural barrier\nonline, where each language group thinks they can see other groups through\nsearches, but in fact, what they see is their own reflection.", "published": "2023-03-28 19:49:58", "link": "http://arxiv.org/abs/2303.16281v3", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CY"}
{"title": "AD-YOLO: You Look Only Once in Training Multiple Sound Event\n  Localization and Detection", "abstract": "Sound event localization and detection (SELD) combines the identification of\nsound events with the corresponding directions of arrival (DOA). Recently,\nevent-oriented track output formats have been adopted to solve this problem;\nhowever, they still have limited generalization toward real-world problems in\nan unknown polyphony environment. To address the issue, we proposed an\nangular-distance-based multiple SELD (AD-YOLO), which is an adaptation of the\n\"You Only Look Once\" algorithm for SELD. The AD-YOLO format allows the model to\nlearn sound occurrences location-sensitively by assigning class responsibility\nto DOA predictions. Hence, the format enables the model to handle the polyphony\nproblem, regardless of the number of sound overlaps. We evaluated AD-YOLO on\nDCASE 2020-2022 challenge Task 3 datasets using four SELD objective metrics.\nThe experimental results show that AD-YOLO achieved outstanding performance\noverall and also accomplished robustness in class-homogeneous polyphony\nenvironments.", "published": "2023-03-28 03:12:34", "link": "http://arxiv.org/abs/2303.15703v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "TransAudio: Towards the Transferable Adversarial Audio Attack via\n  Learning Contextualized Perturbations", "abstract": "In a transfer-based attack against Automatic Speech Recognition (ASR)\nsystems, attacks are unable to access the architecture and parameters of the\ntarget model. Existing attack methods are mostly investigated in voice\nassistant scenarios with restricted voice commands, prohibiting their\napplicability to more general ASR related applications. To tackle this\nchallenge, we propose a novel contextualized attack with deletion, insertion,\nand substitution adversarial behaviors, namely TransAudio, which achieves\narbitrary word-level attacks based on the proposed two-stage framework. To\nstrengthen the attack transferability, we further introduce an audio\nscore-matching optimization strategy to regularize the training process, which\nmitigates adversarial example over-fitting to the surrogate model. Extensive\nexperiments and analysis demonstrate the effectiveness of TransAudio against\nopen-source ASR models and commercial APIs.", "published": "2023-03-28 12:53:02", "link": "http://arxiv.org/abs/2303.15940v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spatial Active Noise Control Method Based On Sound Field Interpolation\n  From Reference Microphone Signals", "abstract": "A spatial active noise control (ANC) method based on the interpolation of a\nsound field from reference microphone signals is proposed. In most current\nspatial ANC methods, a sufficient number of error microphones are required to\nreduce noise over the target region because the sound field is estimated from\nerror microphone signals. However, in practical applications, it is preferable\nthat the number of error microphones is as small as possible to keep a space in\nthe target region for ANC users. We propose to interpolate the sound field from\nreference microphones, which are normally placed outside the target region,\ninstead of the error microphones. We derive a fixed filter for spatial noise\nreduction on the basis of the kernel ridge regression for sound field\ninterpolation. Furthermore, to compensate for estimation errors, we combine the\nproposed fixed filter with multichannel ANC based on a transition of the\ncontrol filter using the error microphone signals. Numerical experimental\nresults indicate that regional noise can be sufficiently reduced by the\nproposed methods even when the number of error microphones is particularly\nsmall.", "published": "2023-03-28 14:50:48", "link": "http://arxiv.org/abs/2303.16021v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Pre-Training For Data-Efficient Text-to-Speech On Low\n  Resource Languages", "abstract": "Neural text-to-speech (TTS) models can synthesize natural human speech when\ntrained on large amounts of transcribed speech. However, collecting such\nlarge-scale transcribed data is expensive. This paper proposes an unsupervised\npre-training method for a sequence-to-sequence TTS model by leveraging large\nuntranscribed speech data. With our pre-training, we can remarkably reduce the\namount of paired transcribed data required to train the model for the target\ndownstream TTS task. The main idea is to pre-train the model to reconstruct\nde-warped mel-spectrograms from warped ones, which may allow the model to learn\nproper temporal assignment relation between input and output sequences. In\naddition, we propose a data augmentation method that further improves the data\nefficiency in fine-tuning. We empirically demonstrate the effectiveness of our\nproposed method in low-resource language scenarios, achieving outstanding\nperformance compared to competing methods. The code and audio samples are\navailable at: https://github.com/cnaigithub/SpeechDewarping", "published": "2023-03-28 01:26:00", "link": "http://arxiv.org/abs/2303.15669v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Adaptive Background Music for a Fighting Game: A Multi-Instrument Volume\n  Modulation Approach", "abstract": "This paper presents our work to enhance the background music (BGM) in\nDareFightingICE by adding an adaptive BGM. The adaptive BGM consists of five\ndifferent instruments playing a classical music piece called \"Air on G-String.\"\nThe BGM adapts by changing the volume of the instruments. Each instrument is\nconnected to a different element of the game. We then run experiments to\nevaluate the adaptive BGM by using a deep reinforcement learning AI that only\nuses audio as input (Blind DL AI). The results show that the performance of the\nBlind DL AI improves while playing with the adaptive BGM as compared to playing\nwithout the adaptive BGM.", "published": "2023-03-28 05:08:55", "link": "http://arxiv.org/abs/2303.15734v3", "categories": ["cs.SD", "cs.AI", "eess.AS", "I.2; H.5.2; H.5"], "primary_category": "cs.SD"}
{"title": "Cluster-Guided Unsupervised Domain Adaptation for Deep Speaker Embedding", "abstract": "Recent studies have shown that pseudo labels can contribute to unsupervised\ndomain adaptation (UDA) for speaker verification. Inspired by the self-training\nstrategies that use an existing classifier to label the unlabeled data for\nretraining, we propose a cluster-guided UDA framework that labels the target\ndomain data by clustering and combines the labeled source domain data and\npseudo-labeled target domain data to train a speaker embedding network. To\nimprove the cluster quality, we train a speaker embedding network dedicated for\nclustering by minimizing the contrastive center loss. The goal is to reduce the\ndistance between an embedding and its assigned cluster center while enlarging\nthe distance between the embedding and the other cluster centers. Using\nVoxCeleb2 as the source domain and CN-Celeb1 as the target domain, we\ndemonstrate that the proposed method can achieve an equal error rate (EER) of\n8.10% on the CN-Celeb1 evaluation set without using any labels from the target\ndomain. This result outperforms the supervised baseline by 39.6% and is the\nstate-of-the-art UDA performance on this corpus.", "published": "2023-03-28 12:58:08", "link": "http://arxiv.org/abs/2303.15944v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Egocentric Auditory Attention Localization in Conversations", "abstract": "In a noisy conversation environment such as a dinner party, people often\nexhibit selective auditory attention, or the ability to focus on a particular\nspeaker while tuning out others. Recognizing who somebody is listening to in a\nconversation is essential for developing technologies that can understand\nsocial behavior and devices that can augment human hearing by amplifying\nparticular sound sources. The computer vision and audio research communities\nhave made great strides towards recognizing sound sources and speakers in\nscenes. In this work, we take a step further by focusing on the problem of\nlocalizing auditory attention targets in egocentric video, or detecting who in\na camera wearer's field of view they are listening to. To tackle the new and\nchallenging Selective Auditory Attention Localization problem, we propose an\nend-to-end deep learning approach that uses egocentric video and multichannel\naudio to predict the heatmap of the camera wearer's auditory attention. Our\napproach leverages spatiotemporal audiovisual features and holistic reasoning\nabout the scene to make predictions, and outperforms a set of baselines on a\nchallenging multi-speaker conversation dataset. Project page:\nhttps://fkryan.github.io/saal", "published": "2023-03-28 14:52:03", "link": "http://arxiv.org/abs/2303.16024v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "A Universal Identity Backdoor Attack against Speaker Verification based\n  on Siamese Network", "abstract": "Speaker verification has been widely used in many authentication scenarios.\nHowever, training models for speaker verification requires large amounts of\ndata and computing power, so users often use untrustworthy third-party data or\ndeploy third-party models directly, which may create security risks. In this\npaper, we propose a backdoor attack for the above scenario. Specifically, for\nthe Siamese network in the speaker verification system, we try to implant a\nuniversal identity in the model that can simulate any enrolled speaker and pass\nthe verification. So the attacker does not need to know the victim, which makes\nthe attack more flexible and stealthy. In addition, we design and compare three\nways of selecting attacker utterances and two ways of poisoned training for the\nGE2E loss function in different scenarios. The results on the TIMIT and\nVoxceleb1 datasets show that our approach can achieve a high attack success\nrate while guaranteeing the normal verification accuracy. Our work reveals the\nvulnerability of the speaker verification system and provides a new perspective\nto further improve the robustness of the system.", "published": "2023-03-28 14:57:05", "link": "http://arxiv.org/abs/2303.16031v2", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "LIPSFUS: A neuromorphic dataset for audio-visual sensory fusion of lip\n  reading", "abstract": "This paper presents a sensory fusion neuromorphic dataset collected with\nprecise temporal synchronization using a set of Address-Event-Representation\nsensors and tools. The target application is the lip reading of several\nkeywords for different machine learning applications, such as digits, robotic\ncommands, and auxiliary rich phonetic short words. The dataset is enlarged with\na spiking version of an audio-visual lip reading dataset collected with\nframe-based cameras. LIPSFUS is publicly available and it has been validated\nwith a deep learning architecture for audio and visual classification. It is\nintended for sensory fusion architectures based on both artificial and spiking\nneural network algorithms.", "published": "2023-03-28 12:27:43", "link": "http://arxiv.org/abs/2304.01080v1", "categories": ["cs.SD", "cs.RO", "eess.AS", "68T40", "I.2.10"], "primary_category": "cs.SD"}
