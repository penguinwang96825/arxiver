{"title": "Rediscovering the Alphabet - On the Innate Universal Grammar", "abstract": "Universal Grammar (UG) theory has been one of the most important research\ntopics in linguistics since introduced five decades ago. UG specifies the\nrestricted set of languages learnable by human brain, and thus, many\nresearchers believe in its biological roots. Numerous empirical studies of\nneurobiological and cognitive functions of the human brain, and of many natural\nlanguages, have been conducted to unveil some aspects of UG. This, however,\nresulted in different and sometimes contradicting theories that do not indicate\na universally unique grammar. In this research, we tackle the UG problem from\nan entirely different perspective. We search for the Unique Universal Grammar\n(UUG) that facilitates communication and knowledge transfer, the sole purpose\nof a language. We formulate this UG and show that it is unique, intrinsic, and\ncosmic, rather than humanistic. Initial analysis on a widespread natural\nlanguage already showed some positive results.", "published": "2014-12-08 04:14:05", "link": "http://arxiv.org/abs/1412.2442v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word learning under infinite uncertainty", "abstract": "Language learners must learn the meanings of many thousands of words, despite\nthose words occurring in complex environments in which infinitely many meanings\nmight be inferred by the learner as a word's true meaning. This problem of\ninfinite referential uncertainty is often attributed to Willard Van Orman\nQuine. We provide a mathematical formalisation of an ideal cross-situational\nlearner attempting to learn under infinite referential uncertainty, and\nidentify conditions under which word learning is possible. As Quine's\nintuitions suggest, learning under infinite uncertainty is in fact possible,\nprovided that learners have some means of ranking candidate word meanings in\nterms of their plausibility; furthermore, our analysis shows that this ranking\ncould in fact be exceedingly weak, implying that constraints which allow\nlearners to infer the plausibility of candidate word meanings could themselves\nbe weak. This approach lifts the burden of explanation from `smart' word\nlearning constraints in learners, and suggests a programme of research into\nweak, unreliable, probabilistic constraints on the inference of word meaning in\nreal word learners.", "published": "2014-12-08 09:08:32", "link": "http://arxiv.org/abs/1412.2487v2", "categories": ["physics.soc-ph", "cs.CL"], "primary_category": "physics.soc-ph"}
{"title": "Optimization models of natural communication", "abstract": "A family of information theoretic models of communication was introduced more\nthan a decade ago to explain the origins of Zipf's law for word frequencies.\nThe family is a based on a combination of two information theoretic principles:\nmaximization of mutual information between forms and meanings and minimization\nof form entropy. The family also sheds light on the origins of three other\npatterns: the principle of contrast, a related vocabulary learning bias and the\nmeaning-frequency law. Here two important components of the family, namely the\ninformation theoretic principles and the energy function that combines them\nlinearly, are reviewed from the perspective of psycholinguistics, language\nlearning, information theory and synergetic linguistics. The minimization of\nthis linear function is linked to the problem of compression of standard\ninformation theory and might be tuned by self-organization.", "published": "2014-12-08 09:05:40", "link": "http://arxiv.org/abs/1412.2486v2", "categories": ["physics.soc-ph", "cs.CL", "physics.data-an"], "primary_category": "physics.soc-ph"}
{"title": "Unsupervised Induction of Semantic Roles within a Reconstruction-Error\n  Minimization Framework", "abstract": "We introduce a new approach to unsupervised estimation of feature-rich\nsemantic role labeling models. Our model consists of two components: (1) an\nencoding component: a semantic role labeling model which predicts roles given a\nrich set of syntactic and lexical features; (2) a reconstruction component: a\ntensor factorization model which relies on roles to predict argument fillers.\nWhen the components are estimated jointly to minimize errors in argument\nreconstruction, the induced roles largely correspond to roles defined in\nannotated resources. Our method performs on par with most accurate role\ninduction methods on English and German, even though, unlike these previous\napproaches, we do not incorporate any prior linguistic knowledge about the\nlanguages.", "published": "2014-12-08 23:40:41", "link": "http://arxiv.org/abs/1412.2812v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
