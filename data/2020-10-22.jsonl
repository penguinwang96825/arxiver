{"title": "Exploit Multiple Reference Graphs for Semi-supervised Relation\n  Extraction", "abstract": "Manual annotation of the labeled data for relation extraction is\ntime-consuming and labor-intensive. Semi-supervised methods can offer helping\nhands for this problem and have aroused great research interests. Existing work\nfocuses on mapping the unlabeled samples to the classes to augment the labeled\ndataset. However, it is hard to find an overall good mapping function,\nespecially for the samples with complicated syntactic components in one\nsentence.\n  To tackle this limitation, we propose to build the connection between the\nunlabeled data and the labeled ones rather than directly mapping the unlabeled\nsamples to the classes. Specifically, we first use three kinds of information\nto construct reference graphs, including entity reference, verb reference, and\nsemantics reference. The goal is to semantically or lexically connect the\nunlabeled sample(s) to the labeled one(s). Then, we develop a Multiple\nReference Graph (MRefG) model to exploit the reference information for better\nrecognizing high-quality unlabeled samples. The effectiveness of our method is\ndemonstrated by extensive comparison experiments with the state-of-the-art\nbaselines on two public datasets.", "published": "2020-10-22 02:14:27", "link": "http://arxiv.org/abs/2010.11383v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Disentangled Adversarial Neural Topic Model for Separating Opinions\n  from Plots in User Reviews", "abstract": "The flexibility of the inference process in Variational Autoencoders (VAEs)\nhas recently led to revising traditional probabilistic topic models giving rise\nto Neural Topic Models (NTMs). Although these approaches have achieved\nsignificant results, surprisingly very little work has been done on how to\ndisentangle the latent topics. Existing topic models when applied to reviews\nmay extract topics associated with writers' subjective opinions mixed with\nthose related to factual descriptions such as plot summaries in movie and book\nreviews. It is thus desirable to automatically separate opinion topics from\nplot/neutral ones enabling a better interpretability. In this paper, we propose\na neural topic model combined with adversarial training to disentangle opinion\ntopics from plot and neutral ones. We conduct an extensive experimental\nassessment introducing a new collection of movie and book reviews paired with\ntheir plots, namely MOBO dataset, showing an improved coherence and variety of\ntopics, a consistent disentanglement rate, and sentiment classification\nperformance superior to other supervised topic models.", "published": "2020-10-22 02:15:13", "link": "http://arxiv.org/abs/2010.11384v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MAM: Masked Acoustic Modeling for End-to-End Speech-to-Text Translation", "abstract": "End-to-end Speech-to-text Translation (E2E-ST), which directly translates\nsource language speech to target language text, is widely useful in practice,\nbut traditional cascaded approaches (ASR+MT) often suffer from error\npropagation in the pipeline. On the other hand, existing end-to-end solutions\nheavily depend on the source language transcriptions for pre-training or\nmulti-task training with Automatic Speech Recognition (ASR). We instead propose\na simple technique to learn a robust speech encoder in a self-supervised\nfashion only on the speech side, which can utilize speech data without\ntranscription. This technique termed Masked Acoustic Modeling (MAM), not only\nprovides an alternative solution to improving E2E-ST, but also can perform\npre-training on any acoustic signals (including non-speech ones) without\nannotation. We conduct our experiments over 8 different translation directions.\nIn the setting without using any transcriptions, our technique achieves an\naverage improvement of +1.1 BLEU, and +2.3 BLEU with MAM pre-training.\nPre-training of MAM with arbitrary acoustic signals also has an average\nimprovement with +1.6 BLEU for those languages. Compared with ASR multi-task\nlearning solution, which replies on transcription during training, our\npre-trained MAM model, which does not use transcription, achieves similar\naccuracy.", "published": "2020-10-22 05:02:06", "link": "http://arxiv.org/abs/2010.11445v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Distillation for BERT Unsupervised Domain Adaptation", "abstract": "A pre-trained language model, BERT, has brought significant performance\nimprovements across a range of natural language processing tasks. Since the\nmodel is trained on a large corpus of diverse topics, it shows robust\nperformance for domain shift problems in which data distributions at training\n(source data) and testing (target data) differ while sharing similarities.\nDespite its great improvements compared to previous models, it still suffers\nfrom performance degradation due to domain shifts. To mitigate such problems,\nwe propose a simple but effective unsupervised domain adaptation method,\nadversarial adaptation with distillation (AAD), which combines the adversarial\ndiscriminative domain adaptation (ADDA) framework with knowledge distillation.\nWe evaluate our approach in the task of cross-domain sentiment classification\non 30 domain pairs, advancing the state-of-the-art performance for unsupervised\ndomain adaptation in text sentiment classification.", "published": "2020-10-22 06:51:24", "link": "http://arxiv.org/abs/2010.11478v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Effects of Using word2vec Representations in Neural Networks for\n  Dialogue Act Recognition", "abstract": "Dialogue act recognition is an important component of a large number of\nnatural language processing pipelines. Many research works have been carried\nout in this area, but relatively few investigate deep neural networks and word\nembeddings. This is surprising, given that both of these techniques have proven\nexceptionally good in most other language-related domains. We propose in this\nwork a new deep neural network that explores recurrent models to capture word\nsequences within sentences, and further study the impact of pretrained word\nembeddings. We validate this model on three languages: English, French and\nCzech. The performance of the proposed approach is consistent across these\nlanguages and it is comparable to the state-of-the-art results in English. More\nimportantly, we confirm that deep neural networks indeed outperform a Maximum\nEntropy classifier, which was expected. However , and this is more surprising,\nwe also found that standard word2vec em-beddings do not seem to bring valuable\ninformation for this task and the proposed model, whatever the size of the\ntraining corpus is. We thus further analyse the resulting embeddings and\nconclude that a possible explanation may be related to the mismatch between the\ntype of lexical-semantic information captured by the word2vec embeddings, and\nthe kind of relations between words that is the most useful for the dialogue\nact recognition task.", "published": "2020-10-22 07:21:17", "link": "http://arxiv.org/abs/2010.11490v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross Copy Network for Dialogue Generation", "abstract": "In the past few years, audiences from different fields witness the\nachievements of sequence-to-sequence models (e.g., LSTM+attention, Pointer\nGenerator Networks, and Transformer) to enhance dialogue content generation.\nWhile content fluency and accuracy often serve as the major indicators for\nmodel training, dialogue logics, carrying critical information for some\nparticular domains, are often ignored. Take customer service and court debate\ndialogue as examples, compatible logics can be observed across different\ndialogue instances, and this information can provide vital evidence for\nutterance generation. In this paper, we propose a novel network architecture -\nCross Copy Networks(CCN) to explore the current dialog context and similar\ndialogue instances' logical structure simultaneously. Experiments with two\ntasks, court debate and customer service content generation, proved that the\nproposed algorithm is superior to existing state-of-art content generation\nmodels.", "published": "2020-10-22 09:03:23", "link": "http://arxiv.org/abs/2010.11539v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Method of noun phrase detection in Ukrainian texts", "abstract": "Introduction. The area of natural language processing considers AI-complete\ntasks that cannot be solved using traditional algorithmic actions. Such tasks\nare commonly implemented with the usage of machine learning methodology and\nmeans of computer linguistics. One of the preprocessing tasks of a text is the\nsearch of noun phrases. The accuracy of this task has implications for the\neffectiveness of many other tasks in the area of natural language processing.\nIn spite of the active development of research in the area of natural language\nprocessing, the investigation of the search for noun phrases within Ukrainian\ntexts are still at an early stage. Results. The different methods of noun\nphrases detection have been analyzed. The expediency of the representation of\nsentences as a tree structure has been justified. The key disadvantage of many\nmethods of noun phrase detection is the severe dependence of the effectiveness\nof their detection from the features of a certain language. Taking into account\nthe unified format of sentence processing and the availability of the trained\nmodel for the building of sentence trees for Ukrainian texts, the Universal\nDependency model has been chosen. The complex method of noun phrases detection\nin Ukrainian texts utilizing Universal Dependencies means and named-entity\nrecognition model has been suggested. Experimental verification of the\neffectiveness of the suggested method on the corpus of Ukrainian news has been\nperformed. Different metrics of method accuracy have been calculated.\nConclusions. The results obtained can indicate that the suggested method can be\nused to find noun phrases in Ukrainian texts. An accuracy increase of the\nmethod can be made with the usage of appropriate named-entity recognition\nmodels according to a subject area.", "published": "2020-10-22 09:20:24", "link": "http://arxiv.org/abs/2010.11548v1", "categories": ["cs.CL", "68U15", "I.7"], "primary_category": "cs.CL"}
{"title": "Incorporating Stylistic Lexical Preferences in Generative Language\n  Models", "abstract": "While recent advances in language modeling have resulted in powerful\ngeneration models, their generation style remains implicitly dependent on the\ntraining data and can not emulate a specific target style. Leveraging the\ngenerative capabilities of a transformer-based language models, we present an\napproach to induce certain target-author attributes by incorporating continuous\nmulti-dimensional lexical preferences of an author into generative language\nmodels. We introduce rewarding strategies in a reinforcement learning framework\nthat encourages the use of words across multiple categorical dimensions, to\nvarying extents. Our experiments demonstrate that the proposed approach can\ngenerate text that distinctively aligns with a given target author's lexical\nstyle. We conduct quantitative and qualitative comparisons with competitive and\nrelevant baselines to illustrate the benefits of the proposed approach.", "published": "2020-10-22 09:24:05", "link": "http://arxiv.org/abs/2010.11553v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting News Article Structure for Automatic Corpus Generation of\n  Entailment Datasets", "abstract": "Transformers represent the state-of-the-art in Natural Language Processing\n(NLP) in recent years, proving effective even in tasks done in low-resource\nlanguages. While pretrained transformers for these languages can be made, it is\nchallenging to measure their true performance and capacity due to the lack of\nhard benchmark datasets, as well as the difficulty and cost of producing them.\nIn this paper, we present three contributions: First, we propose a methodology\nfor automatically producing Natural Language Inference (NLI) benchmark datasets\nfor low-resource languages using published news articles. Through this, we\ncreate and release NewsPH-NLI, the first sentence entailment benchmark dataset\nin the low-resource Filipino language. Second, we produce new pretrained\ntransformers based on the ELECTRA technique to further alleviate the resource\nscarcity in Filipino, benchmarking them on our dataset against other\ncommonly-used transfer learning techniques. Lastly, we perform analyses on\ntransfer learning techniques to shed light on their true performance when\noperating in low-data domains through the use of degradation tests.", "published": "2020-10-22 10:09:10", "link": "http://arxiv.org/abs/2010.11574v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Style Transfer with Discriminative Feedback on Disjoint Corpus", "abstract": "Style transfer has been widely explored in natural language generation with\nnon-parallel corpus by directly or indirectly extracting a notion of style from\nsource and target domain corpus. A common shortcoming of existing approaches is\nthe prerequisite of joint annotations across all the stylistic dimensions under\nconsideration. Availability of such dataset across a combination of styles\nlimits the extension of these setups to multiple style dimensions. While\ncascading single-dimensional models across multiple styles is a possibility, it\nsuffers from content loss, especially when the style dimensions are not\ncompletely independent of each other. In our work, we relax this requirement of\njointly annotated data across multiple styles by using independently acquired\ndata across different style dimensions without any additional annotations. We\ninitialize an encoder-decoder setup with transformer-based language model\npre-trained on a generic corpus and enhance its re-writing capability to\nmultiple target style dimensions by employing multiple style-aware language\nmodels as discriminators. Through quantitative and qualitative evaluation, we\nshow the ability of our model to control styles across multiple style\ndimensions while preserving content of the input text. We compare it against\nbaselines involving cascaded state-of-the-art uni-dimensional style transfer\nmodels.", "published": "2020-10-22 10:16:29", "link": "http://arxiv.org/abs/2010.11578v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AI-lead Court Debate Case Investigation", "abstract": "The multi-role judicial debate composed of the plaintiff, defendant, and\njudge is an important part of the judicial trial. Different from other types of\ndialogue, questions are raised by the judge, The plaintiff, plaintiff's agent\ndefendant, and defendant's agent would be to debating so that the trial can\nproceed in an orderly manner. Question generation is an important task in\nNatural Language Generation. In the judicial trial, it can help the judge raise\nefficient questions so that the judge has a clearer understanding of the case.\nIn this work, we propose an innovative end-to-end question generation\nmodel-Trial Brain Model (TBM) to build a Trial Brain, it can generate the\nquestions the judge wants to ask through the historical dialogue between the\nplaintiff and the defendant. Unlike prior efforts in natural language\ngeneration, our model can learn the judge's questioning intention through\npredefined knowledge. We do experiments on real-world datasets, the\nexperimental results show that our model can provide a more accurate question\nin the multi-role court debate scene.", "published": "2020-10-22 11:05:14", "link": "http://arxiv.org/abs/2010.11604v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Fully Bilingual Deep Language Modeling", "abstract": "Language models based on deep neural networks have facilitated great advances\nin natural language processing and understanding tasks in recent years. While\nmodels covering a large number of languages have been introduced, their\nmultilinguality has come at a cost in terms of monolingual performance, and the\nbest-performing models at most tasks not involving cross-lingual transfer\nremain monolingual. In this paper, we consider the question of whether it is\npossible to pre-train a bilingual model for two remotely related languages\nwithout compromising performance at either language. We collect pre-training\ndata, create a Finnish-English bilingual BERT model and evaluate its\nperformance on datasets used to evaluate the corresponding monolingual models.\nOur bilingual model performs on par with Google's original English BERT on GLUE\nand nearly matches the performance of monolingual Finnish BERT on a range of\nFinnish NLP tasks, clearly outperforming multilingual BERT. We find that when\nthe model vocabulary size is increased, the BERT-Base architecture has\nsufficient capacity to learn two remotely related languages to a level where it\nachieves comparable performance with monolingual models, demonstrating the\nfeasibility of training fully bilingual deep language models. The model and all\ntools involved in its creation are freely available at\nhttps://github.com/TurkuNLP/biBERT", "published": "2020-10-22 12:22:50", "link": "http://arxiv.org/abs/2010.11639v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Analysis of Simple Data Augmentation for Named Entity Recognition", "abstract": "Simple yet effective data augmentation techniques have been proposed for\nsentence-level and sentence-pair natural language processing tasks. Inspired by\nthese efforts, we design and compare data augmentation for named entity\nrecognition, which is usually modeled as a token-level sequence labeling\nproblem. Through experiments on two data sets from the biomedical and materials\nscience domains (i2b2-2010 and MaSciP), we show that simple augmentation can\nboost performance for both recurrent and transformer-based models, especially\nfor small training sets.", "published": "2020-10-22 13:21:03", "link": "http://arxiv.org/abs/2010.11683v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CUNI Systems for the Unsupervised and Very Low Resource Translation Task\n  in WMT20", "abstract": "This paper presents a description of CUNI systems submitted to the WMT20 task\non unsupervised and very low-resource supervised machine translation between\nGerman and Upper Sorbian. We experimented with training on synthetic data and\npre-training on a related language pair. In the fully unsupervised scenario, we\nachieved 25.5 and 23.7 BLEU translating from and into Upper Sorbian,\nrespectively. Our low-resource systems relied on transfer learning from\nGerman-Czech parallel data and achieved 57.4 BLEU and 56.1 BLEU, which is an\nimprovement of 10 BLEU points over the baseline trained only on the available\nsmall German-Upper Sorbian parallel corpus.", "published": "2020-10-22 14:04:01", "link": "http://arxiv.org/abs/2010.11747v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EIGEN: Event Influence GENeration using Pre-trained Language Models", "abstract": "Reasoning about events and tracking their influences is fundamental to\nunderstanding processes. In this paper, we present EIGEN - a method to leverage\npre-trained language models to generate event influences conditioned on a\ncontext, nature of their influence, and the distance in a reasoning chain. We\nalso derive a new dataset for research and evaluation of methods for event\ninfluence generation. EIGEN outperforms strong baselines both in terms of\nautomated evaluation metrics (by 10 ROUGE points) and human judgments on\ncloseness to reference and relevance of generations. Furthermore, we show that\nthe event influences generated by EIGEN improve the performance on a \"what-if\"\nQuestion Answering (WIQA) benchmark (over 3% F1), especially for questions that\nrequire background knowledge and multi-hop reasoning.", "published": "2020-10-22 14:36:04", "link": "http://arxiv.org/abs/2010.11764v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConVEx: Data-Efficient and Few-Shot Slot Labeling", "abstract": "We propose ConVEx (Conversational Value Extractor), an efficient pretraining\nand fine-tuning neural approach for slot-labeling dialog tasks. Instead of\nrelying on more general pretraining objectives from prior work (e.g., language\nmodeling, response selection), ConVEx's pretraining objective, a novel pairwise\ncloze task using Reddit data, is well aligned with its intended usage on\nsequence labeling tasks. This enables learning domain-specific slot labelers by\nsimply fine-tuning decoding layers of the pretrained general-purpose sequence\nlabeling model, while the majority of the pretrained model's parameters are\nkept frozen. We report state-of-the-art performance of ConVEx across a range of\ndiverse domains and data sets for dialog slot-labeling, with the largest gains\nin the most challenging, few-shot setups. We believe that ConVEx's reduced\npretraining times (i.e., only 18 hours on 12 GPUs) and cost, along with its\nefficient fine-tuning and strong performance, promise wider portability and\nscalability for data-efficient sequence-labeling tasks in general.", "published": "2020-10-22 15:13:35", "link": "http://arxiv.org/abs/2010.11791v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compositional Generalization via Semantic Tagging", "abstract": "Although neural sequence-to-sequence models have been successfully applied to\nsemantic parsing, they fail at compositional generalization, i.e., they are\nunable to systematically generalize to unseen compositions of seen components.\nMotivated by traditional semantic parsing where compositionality is explicitly\naccounted for by symbolic grammars, we propose a new decoding framework that\npreserves the expressivity and generality of sequence-to-sequence models while\nfeaturing lexicon-style alignments and disentangled information processing.\nSpecifically, we decompose decoding into two phases where an input utterance is\nfirst tagged with semantic symbols representing the meaning of individual\nwords, and then a sequence-to-sequence model is used to predict the final\nmeaning representation conditioning on the utterance and the predicted tag\nsequence. Experimental results on three semantic parsing datasets show that the\nproposed approach consistently improves compositional generalization across\nmodel architectures, domains, and semantic formalisms.", "published": "2020-10-22 15:55:15", "link": "http://arxiv.org/abs/2010.11818v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STAR: A Schema-Guided Dialog Dataset for Transfer Learning", "abstract": "We present STAR, a schema-guided task-oriented dialog dataset consisting of\n127,833 utterances and knowledge base queries across 5,820 task-oriented\ndialogs in 13 domains that is especially designed to facilitate task and domain\ntransfer learning in task-oriented dialog. Furthermore, we propose a scalable\ncrowd-sourcing paradigm to collect arbitrarily large datasets of the same\nquality as STAR. Moreover, we introduce novel schema-guided dialog models that\nuse an explicit description of the task(s) to generalize from known to unknown\ntasks. We demonstrate the effectiveness of these models, particularly for\nzero-shot generalization across tasks and domains.", "published": "2020-10-22 16:45:00", "link": "http://arxiv.org/abs/2010.11853v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XOR QA: Cross-lingual Open-Retrieval Question Answering", "abstract": "Multilingual question answering tasks typically assume answers exist in the\nsame language as the question. Yet in practice, many languages face both\ninformation scarcity -- where languages have few reference articles -- and\ninformation asymmetry -- where questions reference concepts from other\ncultures. This work extends open-retrieval question answering to a\ncross-lingual setting enabling questions from one language to be answered via\nanswer content from another language. We construct a large-scale dataset built\non questions from TyDi QA lacking same-language answers. Our task formulation,\ncalled Cross-lingual Open Retrieval Question Answering (XOR QA), includes 40k\ninformation-seeking questions from across 7 diverse non-English languages.\nBased on this dataset, we introduce three new tasks that involve cross-lingual\ndocument retrieval using multi-lingual and English resources. We establish\nbaselines with state-of-the-art machine translation systems and cross-lingual\npretrained models. Experimental results suggest that XOR QA is a challenging\ntask that will facilitate the development of novel techniques for multilingual\nquestion answering. Our data and code are available at\nhttps://nlp.cs.washington.edu/xorqa.", "published": "2020-10-22 16:47:17", "link": "http://arxiv.org/abs/2010.11856v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Not all parameters are born equal: Attention is mostly what you need", "abstract": "Transformers are widely used in state-of-the-art machine translation, but the\nkey to their success is still unknown. To gain insight into this, we consider\nthree groups of parameters: embeddings, attention, and feed forward neural\nnetwork (FFN) layers. We examine the relative importance of each by performing\nan ablation study where we initialise them at random and freeze them, so that\ntheir weights do not change over the course of the training. Through this, we\nshow that the attention and FFN are equally important and fulfil the same\nfunctionality in a model. We show that the decision about whether a component\nis frozen or allowed to train is at least as important for the final model\nperformance as its number of parameters. At the same time, the number of\nparameters alone is not indicative of a component's importance. Finally, while\nthe embedding layer is the least essential for machine translation tasks, it is\nthe most important component for language modelling tasks.", "published": "2020-10-22 16:49:18", "link": "http://arxiv.org/abs/2010.11859v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Challenges in Information-Seeking QA: Unanswerable Questions and\n  Paragraph Retrieval", "abstract": "Recent pretrained language models \"solved\" many reading comprehension\nbenchmarks, where questions are written with access to the evidence document.\nHowever, datasets containing information-seeking queries where evidence\ndocuments are provided after the queries are written independently remain\nchallenging. We analyze why answering information-seeking queries is more\nchallenging and where their prevalent unanswerabilities arise, on Natural\nQuestions and TyDi QA. Our controlled experiments suggest two headrooms --\nparagraph selection and answerability prediction, i.e. whether the paired\nevidence document contains the answer to the query or not. When provided with a\ngold paragraph and knowing when to abstain from answering, existing models\neasily outperform a human annotator. However, predicting answerability itself\nremains challenging. We manually annotate 800 unanswerable examples across six\nlanguages on what makes them challenging to answer. With this new data, we\nconduct per-category answerability prediction, revealing issues in the current\ndataset collection as well as task formulation. Together, our study points to\navenues for future research in information-seeking question answering, both for\ndataset creation and model development.", "published": "2020-10-22 17:48:17", "link": "http://arxiv.org/abs/2010.11915v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "mT5: A massively multilingual pre-trained text-to-text transformer", "abstract": "The recent \"Text-to-Text Transfer Transformer\" (T5) leveraged a unified\ntext-to-text format and scale to attain state-of-the-art results on a wide\nvariety of English-language NLP tasks. In this paper, we introduce mT5, a\nmultilingual variant of T5 that was pre-trained on a new Common Crawl-based\ndataset covering 101 languages. We detail the design and modified training of\nmT5 and demonstrate its state-of-the-art performance on many multilingual\nbenchmarks. We also describe a simple technique to prevent \"accidental\ntranslation\" in the zero-shot setting, where a generative model chooses to\n(partially) translate its prediction into the wrong language. All of the code\nand model checkpoints used in this work are publicly available.", "published": "2020-10-22 17:58:14", "link": "http://arxiv.org/abs/2010.11934v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rediscovering the Slavic Continuum in Representations Emerging from\n  Neural Models of Spoken Language Identification", "abstract": "Deep neural networks have been employed for various spoken language\nrecognition tasks, including tasks that are multilingual by definition such as\nspoken language identification. In this paper, we present a neural model for\nSlavic language identification in speech signals and analyze its emergent\nrepresentations to investigate whether they reflect objective measures of\nlanguage relatedness and/or non-linguists' perception of language similarity.\nWhile our analysis shows that the language representation space indeed captures\nlanguage relatedness to a great extent, we find perceptual confusability\nbetween languages in our study to be the best predictor of the language\nrepresentation similarity.", "published": "2020-10-22 18:18:19", "link": "http://arxiv.org/abs/2010.11973v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta-Learning for Domain Generalization in Semantic Parsing", "abstract": "The importance of building semantic parsers which can be applied to new\ndomains and generate programs unseen at training has long been acknowledged,\nand datasets testing out-of-domain performance are becoming increasingly\navailable. However, little or no attention has been devoted to learning\nalgorithms or objectives which promote domain generalization, with virtually\nall existing approaches relying on standard supervised learning. In this work,\nwe use a meta-learning framework which targets zero-shot domain generalization\nfor semantic parsing. We apply a model-agnostic training algorithm that\nsimulates zero-shot parsing by constructing virtual train and test sets from\ndisjoint domains. The learning objective capitalizes on the intuition that\ngradient steps that improve source-domain performance should also improve\ntarget-domain performance, thus encouraging a parser to generalize to unseen\ntarget domains. Experimental results on the (English) Spider and Chinese Spider\ndatasets show that the meta-learning objective significantly boosts the\nperformance of a baseline parser.", "published": "2020-10-22 19:00:36", "link": "http://arxiv.org/abs/2010.11988v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Summarizing Utterances from Japanese Assembly Minutes using Political\n  Sentence-BERT-based Method for QA Lab-PoliInfo-2 Task of NTCIR-15", "abstract": "There are many discussions held during political meetings, and a large number\nof utterances for various topics is included in their transcripts. We need to\nread all of them if we want to follow speakers\\' intentions or opinions about a\ngiven topic. To avoid such a costly and time-consuming process to grasp often\nlongish discussions, NLP researchers work on generating concise summaries of\nutterances. Summarization subtask in QA Lab-PoliInfo-2 task of the NTCIR-15\naddresses this problem for Japanese utterances in assembly minutes, and our\nteam (SKRA) participated in this subtask. As a first step for summarizing\nutterances, we created a new pre-trained sentence embedding model, i.e. the\nJapanese Political Sentence-BERT. With this model, we summarize utterances\nwithout labelled data. This paper describes our approach to solving the task\nand discusses its results.", "published": "2020-10-22 21:37:28", "link": "http://arxiv.org/abs/2010.12077v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "N-ODE Transformer: A Depth-Adaptive Variant of the Transformer Using\n  Neural Ordinary Differential Equations", "abstract": "We use neural ordinary differential equations to formulate a variant of the\nTransformer that is depth-adaptive in the sense that an input-dependent number\nof time steps is taken by the ordinary differential equation solver. Our goal\nin proposing the N-ODE Transformer is to investigate whether its\ndepth-adaptivity may aid in overcoming some specific known theoretical\nlimitations of the Transformer in handling nonlocal effects. Specifically, we\nconsider the simple problem of determining the parity of a binary sequence, for\nwhich the standard Transformer has known limitations that can only be overcome\nby using a sufficiently large number of layers or attention heads. We find,\nhowever, that the depth-adaptivity of the N-ODE Transformer does not provide a\nremedy for the inherently nonlocal nature of the parity problem, and provide\nexplanations for why this is so. Next, we pursue regularization of the N-ODE\nTransformer by penalizing the arclength of the ODE trajectories, but find that\nthis fails to improve the accuracy or efficiency of the N-ODE Transformer on\nthe challenging parity problem. We suggest future avenues of research for\nmodifications and extensions of the N-ODE Transformer that may lead to improved\naccuracy and efficiency for sequence modelling tasks such as neural machine\ntranslation.", "published": "2020-10-22 00:48:24", "link": "http://arxiv.org/abs/2010.11358v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Stronger Transformers for Neural Multi-Hop Question Generation", "abstract": "Prior work on automated question generation has almost exclusively focused on\ngenerating simple questions whose answers can be extracted from a single\ndocument. However, there is an increasing interest in developing systems that\nare capable of more complex multi-hop question generation, where answering the\nquestions requires reasoning over multiple documents. In this work, we\nintroduce a series of strong transformer models for multi-hop question\ngeneration, including a graph-augmented transformer that leverages relations\nbetween entities in the text. While prior work has emphasized the importance of\ngraph-based models, we show that we can substantially outperform the\nstate-of-the-art by 5 BLEU points using a standard transformer architecture. We\nfurther demonstrate that graph-based augmentations can provide complimentary\nimprovements on top of this foundation. Interestingly, we find that several\nimportant factors--such as the inclusion of an auxiliary contrastive objective\nand data filtering could have larger impacts on performance. We hope that our\nstronger baselines and analysis provide a constructive foundation for future\nwork in this area.", "published": "2020-10-22 01:51:09", "link": "http://arxiv.org/abs/2010.11374v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Distilling Dense Representations for Ranking using Tightly-Coupled\n  Teachers", "abstract": "We present an approach to ranking with dense representations that applies\nknowledge distillation to improve the recently proposed late-interaction\nColBERT model. Specifically, we distill the knowledge from ColBERT's expressive\nMaxSim operator for computing relevance scores into a simple dot product, thus\nenabling single-step ANN search. Our key insight is that during distillation,\ntight coupling between the teacher model and the student model enables more\nflexible distillation strategies and yields better learned representations. We\nempirically show that our approach improves query latency and greatly reduces\nthe onerous storage requirements of ColBERT, while only making modest\nsacrifices in terms of effectiveness. By combining our dense representations\nwith sparse representations derived from document expansion, we are able to\napproach the effectiveness of a standard cross-encoder reranker using BERT that\nis orders of magnitude slower.", "published": "2020-10-22 02:26:01", "link": "http://arxiv.org/abs/2010.11386v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Developing Real-time Streaming Transformer Transducer for Speech\n  Recognition on Large-scale Dataset", "abstract": "Recently, Transformer based end-to-end models have achieved great success in\nmany areas including speech recognition. However, compared to LSTM models, the\nheavy computational cost of the Transformer during inference is a key issue to\nprevent their applications. In this work, we explored the potential of\nTransformer Transducer (T-T) models for the fist pass decoding with low latency\nand fast speed on a large-scale dataset. We combine the idea of Transformer-XL\nand chunk-wise streaming processing to design a streamable Transformer\nTransducer model. We demonstrate that T-T outperforms the hybrid model, RNN\nTransducer (RNN-T), and streamable Transformer attention-based encoder-decoder\nmodel in the streaming scenario. Furthermore, the runtime cost and latency can\nbe optimized with a relatively small look-ahead.", "published": "2020-10-22 03:01:21", "link": "http://arxiv.org/abs/2010.11395v3", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SlimIPL: Language-Model-Free Iterative Pseudo-Labeling", "abstract": "Recent results in end-to-end automatic speech recognition have demonstrated\nthe efficacy of pseudo-labeling for semi-supervised models trained both with\nConnectionist Temporal Classification (CTC) and Sequence-to-Sequence (seq2seq)\nlosses. Iterative Pseudo-Labeling (IPL), which continuously trains a single\nmodel using pseudo-labels iteratively re-generated as the model learns, has\nbeen shown to further improve performance in ASR. We improve upon the IPL\nalgorithm: as the model learns, we propose to iteratively re-generate\ntranscriptions with hard labels (the most probable tokens), that is, without a\nlanguage model. We call this approach Language-Model-Free IPL (slimIPL) and\ngive a resultant training setup for low-resource settings with CTC-based\nmodels. slimIPL features a dynamic cache for pseudo-labels which reduces\nsensitivity to changes in relabeling hyperparameters and results in improves\ntraining stability. slimIPL is also highly-efficient and requires 3.5-4x fewer\ncomputational resources to converge than other state-of-the-art\nsemi/self-supervised approaches. With only 10 hours of labeled audio, slimIPL\nis competitive with self-supervised approaches, and is state-of-the-art with\n100 hours of labeled audio without the use of a language model both at test\ntime and during pseudo-label generation.", "published": "2020-10-22 08:36:33", "link": "http://arxiv.org/abs/2010.11524v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bilinear Fusion of Commonsense Knowledge with Attention-Based NLI Models", "abstract": "We consider the task of incorporating real-world commonsense knowledge into\ndeep Natural Language Inference (NLI) models. Existing external knowledge\nincorporation methods are limited to lexical level knowledge and lack\ngeneralization across NLI models, datasets, and commonsense knowledge sources.\nTo address these issues, we propose a novel NLI model-independent neural\nframework, BiCAM. BiCAM incorporates real-world commonsense knowledge into NLI\nmodels. Combined with convolutional feature detectors and bilinear feature\nfusion, BiCAM provides a conceptually simple mechanism that generalizes well.\nQuantitative evaluations with two state-of-the-art NLI baselines on SNLI and\nSciTail datasets in conjunction with ConceptNet and Aristo Tuple KGs show that\nBiCAM considerably improves the accuracy the incorporated NLI baselines. For\nexample, our BiECAM model, an instance of BiCAM, on the challenging SciTail\ndataset, improves the accuracy of incorporated baselines by 7.0% with\nConceptNet, and 8.0% with Aristo Tuple KG.", "published": "2020-10-22 09:38:08", "link": "http://arxiv.org/abs/2010.11562v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Technical Report: BUT Speech Translation Systems", "abstract": "The paper describes the BUT's speech translation systems. The systems are\nEnglish$\\longrightarrow$German offline speech translation systems. The systems\nare based on our previous works \\cite{Jointly_trained_transformers}. Though\nEnd-to-End and cascade~(ASR-MT) spoken language translation~(SLT) systems are\nreaching comparable performances, a large degradation is observed when\ntranslating ASR hypothesis compared to the oracle input text. To reduce this\nperformance degradation, we have jointly-trained ASR and MT modules with ASR\nobjective as an auxiliary loss. Both the networks are connected through the\nneural hidden representations. This model has an End-to-End differentiable path\nwith respect to the final objective function and also utilizes the ASR\nobjective for better optimization. During the inference both the modules(i.e.,\nASR and MT) are connected through the hidden representations corresponding to\nthe n-best hypotheses. Ensembling with independently trained ASR and MT models\nhave further improved the performance of the system.", "published": "2020-10-22 10:52:31", "link": "http://arxiv.org/abs/2010.11593v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reducing Unintended Identity Bias in Russian Hate Speech Detection", "abstract": "Toxicity has become a grave problem for many online communities and has been\ngrowing across many languages, including Russian. Hate speech creates an\nenvironment of intimidation, discrimination, and may even incite some\nreal-world violence. Both researchers and social platforms have been focused on\ndeveloping models to detect toxicity in online communication for a while now. A\ncommon problem of these models is the presence of bias towards some words (e.g.\nwoman, black, jew) that are not toxic, but serve as triggers for the classifier\ndue to model caveats. In this paper, we describe our efforts towards\nclassifying hate speech in Russian, and propose simple techniques of reducing\nunintended bias, such as generating training data with language models using\nterms and words related to protected identities as context and applying word\ndropout to such words.", "published": "2020-10-22 12:54:14", "link": "http://arxiv.org/abs/2010.11666v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving BERT Performance for Aspect-Based Sentiment Analysis", "abstract": "Aspect-Based Sentiment Analysis (ABSA) studies the consumer opinion on the\nmarket products. It involves examining the type of sentiments as well as\nsentiment targets expressed in product reviews. Analyzing the language used in\na review is a difficult task that requires a deep understanding of the\nlanguage. In recent years, deep language models, such as BERT\n\\cite{devlin2019bert}, have shown great progress in this regard. In this work,\nwe propose two simple modules called Parallel Aggregation and Hierarchical\nAggregation to be utilized on top of BERT for two main ABSA tasks namely Aspect\nExtraction (AE) and Aspect Sentiment Classification (ASC) in order to improve\nthe model's performance. We show that applying the proposed models eliminates\nthe need for further training of the BERT model. The source code is available\non the Web for further research and reproduction of the results.", "published": "2020-10-22 13:52:18", "link": "http://arxiv.org/abs/2010.11731v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AdapterDrop: On the Efficiency of Adapters in Transformers", "abstract": "Massively pre-trained transformer models are computationally expensive to\nfine-tune, slow for inference, and have large storage requirements. Recent\napproaches tackle these shortcomings by training smaller models, dynamically\nreducing the model size, and by training light-weight adapters. In this paper,\nwe propose AdapterDrop, removing adapters from lower transformer layers during\ntraining and inference, which incorporates concepts from all three directions.\nWe show that AdapterDrop can dynamically reduce the computational overhead when\nperforming inference over multiple tasks simultaneously, with minimal decrease\nin task performances. We further prune adapters from AdapterFusion, which\nimproves the inference efficiency while maintaining the task performances\nentirely.", "published": "2020-10-22 17:49:42", "link": "http://arxiv.org/abs/2010.11918v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Scientific Claim Verification with VERT5ERINI", "abstract": "This work describes the adaptation of a pretrained sequence-to-sequence model\nto the task of scientific claim verification in the biomedical domain. We\npropose VERT5ERINI that exploits T5 for abstract retrieval, sentence selection\nand label prediction, which are three critical sub-tasks of claim verification.\nWe evaluate our pipeline on SCIFACT, a newly curated dataset that requires\nmodels to not just predict the veracity of claims but also provide relevant\nsentences from a corpus of scientific literature that support this decision.\nEmpirically, our pipeline outperforms a strong baseline in each of the three\nsteps. Finally, we show VERT5ERINI's ability to generalize to two new datasets\nof COVID-19 claims using evidence from the ever-expanding CORD-19 corpus.", "published": "2020-10-22 17:56:33", "link": "http://arxiv.org/abs/2010.11930v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "UniCase -- Rethinking Casing in Language Models", "abstract": "In this paper, we introduce a new approach to dealing with the problem of\ncase-sensitiveness in Language Modelling (LM). We propose simple architecture\nmodification to the RoBERTa language model, accompanied by a new tokenization\nstrategy, which we named Unified Case LM (UniCase). We tested our solution on\nthe GLUE benchmark, which led to increased performance by 0.42 points.\nMoreover, we prove that the UniCase model works much better when we have to\ndeal with text data, where all tokens are uppercased (+5.88 point).", "published": "2020-10-22 17:58:44", "link": "http://arxiv.org/abs/2010.11936v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Data Augmentation with Naive Augmentation and without\n  Unlabeled Data", "abstract": "Unsupervised Data Augmentation (UDA) is a semi-supervised technique that\napplies a consistency loss to penalize differences between a model's\npredictions on (a) observed (unlabeled) examples; and (b) corresponding\n'noised' examples produced via data augmentation. While UDA has gained\npopularity for text classification, open questions linger over which design\ndecisions are necessary and over how to extend the method to sequence labeling\ntasks. This method has recently gained traction for text classification. In\nthis paper, we re-examine UDA and demonstrate its efficacy on several\nsequential tasks. Our main contribution is an empirical study of UDA to\nestablish which components of the algorithm confer benefits in NLP. Notably,\nalthough prior work has emphasized the use of clever augmentation techniques\nincluding back-translation, we find that enforcing consistency between\npredictions assigned to observed and randomly substituted words often yields\ncomparable (or greater) benefits compared to these complex perturbation models.\nFurthermore, we find that applying its consistency loss affords meaningful\ngains without any unlabeled data at all, i.e., in a standard supervised\nsetting. In short: UDA need not be unsupervised, and does not require complex\ndata augmentation to be effective.", "published": "2020-10-22 18:01:51", "link": "http://arxiv.org/abs/2010.11966v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Joint Learning Approach based on Self-Distillation for Keyphrase\n  Extraction from Scientific Documents", "abstract": "Keyphrase extraction is the task of extracting a small set of phrases that\nbest describe a document. Most existing benchmark datasets for the task\ntypically have limited numbers of annotated documents, making it challenging to\ntrain increasingly complex neural networks. In contrast, digital libraries\nstore millions of scientific articles online, covering a wide range of topics.\nWhile a significant portion of these articles contain keyphrases provided by\ntheir authors, most other articles lack such kind of annotations. Therefore, to\neffectively utilize these large amounts of unlabeled articles, we propose a\nsimple and efficient joint learning approach based on the idea of\nself-distillation. Experimental results show that our approach consistently\nimproves the performance of baseline models for keyphrase extraction.\nFurthermore, our best models outperform previous methods for the task,\nachieving new state-of-the-art results on two public benchmarks: Inspec and\nSemEval-2017.", "published": "2020-10-22 18:36:31", "link": "http://arxiv.org/abs/2010.11980v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Text Mining to Identify and Extract Novel Disease Treatments From\n  Unstructured Datasets", "abstract": "Objective: We aim to learn potential novel cures for diseases from\nunstructured text sources. More specifically, we seek to extract drug-disease\npairs of potential cures to diseases by a simple reasoning over the structure\nof spoken text.\n  Materials and Methods: We use Google Cloud to transcribe podcast episodes of\nan NPR radio show. We then build a pipeline for systematically pre-processing\nthe text to ensure quality input to the core classification model, which feeds\nto a series of post-processing steps for obtaining filtered results. Our\nclassification model itself uses a language model pre-trained on PubMed text.\nThe modular nature of our pipeline allows for ease of future developments in\nthis area by substituting higher quality components at each stage of the\npipeline. As a validation measure, we use ROBOKOP, an engine over a medical\nknowledge graph with only validated pathways, as a ground truth source for\nchecking the existence of the proposed pairs. For the proposed pairs not found\nin ROBOKOP, we provide further verification using Chemotext.\n  Results: We found 30.4% of our proposed pairs in the ROBOKOP database. For\nexample, our model successfully identified that Omeprazole can help treat\nheartburn.We discuss the significance of this result, showing some examples of\nthe proposed pairs.\n  Discussion and Conclusion: The agreement of our results with the existing\nknowledge source indicates a step in the right direction. Given the\nplug-and-play nature of our framework, it is easy to add, remove, or modify\nparts to improve the model as necessary. We discuss the results showing some\nexamples, and note that this is a potentially new line of research that has\nfurther scope to be explored. Although our approach was originally oriented on\nradio podcast transcripts, it is input-agnostic and could be applied to any\nsource of textual data and to any problem of interest.", "published": "2020-10-22 19:52:49", "link": "http://arxiv.org/abs/2011.07959v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Kwame: A Bilingual AI Teaching Assistant for Online SuaCode Courses", "abstract": "Introductory hands-on courses such as our smartphone-based coding course,\nSuaCode require a lot of support for students to accomplish learning goals.\nOnline environments make it even more difficult to get assistance especially\nmore recently because of COVID-19. Given the multilingual context of SuaCode\nstudents - learners across 42 African countries that are mostly Anglophone or\nFrancophone - in this work, we developed a bilingual Artificial Intelligence\n(AI) Teaching Assistant (TA) - Kwame - that provides answers to students'\ncoding questions from SuaCode courses in English and French. Kwame is a\nSentence-BERT (SBERT)-based question-answering (QA) system that we trained and\nevaluated offline using question-answer pairs created from the course's\nquizzes, lesson notes and students' questions in past cohorts. Kwame finds the\nparagraph most semantically similar to the question via cosine similarity. We\ncompared the system with TF-IDF and Universal Sentence Encoder. Our results\nshowed that fine-tuning on the course data and returning the top 3 and 5\nanswers improved the accuracy results. Kwame will make it easy for students to\nget quick and accurate answers to questions in SuaCode courses.", "published": "2020-10-22 02:26:12", "link": "http://arxiv.org/abs/2010.11387v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Confidence Estimation for Attention-based Sequence-to-sequence Models\n  for Speech Recognition", "abstract": "For various speech-related tasks, confidence scores from a speech recogniser\nare a useful measure to assess the quality of transcriptions. In traditional\nhidden Markov model-based automatic speech recognition (ASR) systems,\nconfidence scores can be reliably obtained from word posteriors in decoding\nlattices. However, for an ASR system with an auto-regressive decoder, such as\nan attention-based sequence-to-sequence model, computing word posteriors is\ndifficult. An obvious alternative is to use the decoder softmax probability as\nthe model confidence. In this paper, we first examine how some commonly used\nregularisation methods influence the softmax-based confidence scores and study\nthe overconfident behaviour of end-to-end models. Then we propose a lightweight\nand effective approach named confidence estimation module (CEM) on top of an\nexisting end-to-end ASR model. Experiments on LibriSpeech show that CEM can\nmitigate the overconfidence problem and can produce more reliable confidence\nscores with and without shallow fusion of a language model. Further analysis\nshows that CEM generalises well to speech from a moderately mismatched domain\nand can potentially improve downstream tasks such as semi-supervised learning.", "published": "2020-10-22 04:02:27", "link": "http://arxiv.org/abs/2010.11428v2", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Similarity Analysis of Self-Supervised Speech Representations", "abstract": "Self-supervised speech representation learning has recently been a prosperous\nresearch topic. Many algorithms have been proposed for learning useful\nrepresentations from large-scale unlabeled data, and their applications to a\nwide range of speech tasks have also been investigated. However, there has been\nlittle research focusing on understanding the properties of existing\napproaches. In this work, we aim to provide a comparative study of some of the\nmost representative self-supervised algorithms. Specifically, we quantify the\nsimilarities between different self-supervised representations using existing\nsimilarity measures. We also design probing tasks to study the correlation\nbetween the models' pre-training loss and the amount of specific speech\ninformation contained in their learned representations. In addition to showing\nhow various self-supervised models behave differently given the same input, our\nstudy also finds that the training objective has a higher impact on\nrepresentation similarity than architectural choices such as building blocks\n(RNN/Transformer/CNN) and directionality (uni/bidirectional). Our results also\nsuggest that there exists a strong correlation between pre-training loss and\ndownstream performance for some self-supervised algorithms.", "published": "2020-10-22 07:02:21", "link": "http://arxiv.org/abs/2010.11481v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Calibrated Language Model Fine-Tuning for In- and Out-of-Distribution\n  Data", "abstract": "Fine-tuned pre-trained language models can suffer from severe miscalibration\nfor both in-distribution and out-of-distribution (OOD) data due to\nover-parameterization. To mitigate this issue, we propose a regularized\nfine-tuning method. Our method introduces two types of regularization for\nbetter calibration: (1) On-manifold regularization, which generates pseudo\non-manifold samples through interpolation within the data manifold. Augmented\ntraining with these pseudo samples imposes a smoothness regularization to\nimprove in-distribution calibration. (2) Off-manifold regularization, which\nencourages the model to output uniform distributions for pseudo off-manifold\nsamples to address the over-confidence issue for OOD data. Our experiments\ndemonstrate that the proposed method outperforms existing calibration methods\nfor text classification in terms of expectation calibration error,\nmisclassification detection, and OOD detection on six datasets. Our code can be\nfound at https://github.com/Lingkai-Kong/Calibrated-BERT-Fine-Tuning.", "published": "2020-10-22 07:48:38", "link": "http://arxiv.org/abs/2010.11506v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Industry Evaluation of Embedding-based Entity Alignment", "abstract": "Embedding-based entity alignment has been widely investigated in recent\nyears, but most proposed methods still rely on an ideal supervised learning\nsetting with a large number of unbiased seed mappings for training and\nvalidation, which significantly limits their usage. In this study, we evaluate\nthose state-of-the-art methods in an industrial context, where the impact of\nseed mappings with different sizes and different biases is explored. Besides\nthe popular benchmarks from DBpedia and Wikidata, we contribute and evaluate a\nnew industrial benchmark that is extracted from two heterogeneous knowledge\ngraphs (KGs) under deployment for medical applications. The experimental\nresults enable the analysis of the advantages and disadvantages of these\nalignment methods and the further discussion of suitable strategies for their\nindustrial deployment.", "published": "2020-10-22 08:33:58", "link": "http://arxiv.org/abs/2010.11522v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Graph Attention Networks for Speaker Verification", "abstract": "This work presents a novel back-end framework for speaker verification using\ngraph attention networks. Segment-wise speaker embeddings extracted from\nmultiple crops within an utterance are interpreted as node representations of a\ngraph. The proposed framework inputs segment-wise speaker embeddings from an\nenrollment and a test utterance and directly outputs a similarity score. We\nfirst construct a graph using segment-wise speaker embeddings and then input\nthese to graph attention networks. After a few graph attention layers with\nresidual connections, each node is projected into a one-dimensional space using\naffine transform, followed by a readout operation resulting in a scalar\nsimilarity score. To enable successful adaptation for speaker verification, we\npropose techniques such as separating trainable weights for attention map\ncalculations between segment-wise speaker embeddings from different utterances.\nThe effectiveness of the proposed framework is validated using three different\nspeaker embedding extractors trained with different architectures and objective\nfunctions. Experimental results demonstrate consistent improvement over various\nbaseline back-end classifiers, with an average equal error rate improvement of\n20% over the cosine similarity back-end without test time augmentation.", "published": "2020-10-22 09:08:02", "link": "http://arxiv.org/abs/2010.11543v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The HUAWEI Speaker Diarisation System for the VoxCeleb Speaker\n  Diarisation Challenge", "abstract": "This paper describes system setup of our submission to speaker diarisation\ntrack (Track 4) of VoxCeleb Speaker Recognition Challenge 2020. Our diarisation\nsystem consists of a well-trained neural network based speech enhancement model\nas pre-processing front-end of input speech signals. We replace conventional\nenergy-based voice activity detection (VAD) with a neural network based VAD.\nThe neural network based VAD provides more accurate annotation of speech\nsegments containing only background music, noise, and other interference, which\nis crucial to diarisation performance. We apply agglomerative hierarchical\nclustering (AHC) of x-vectors and variational Bayesian hidden Markov model\n(VB-HMM) based iterative clustering for speaker clustering. Experimental\nresults demonstrate that our proposed system achieves substantial improvements\nover the baseline system, yielding diarisation error rate (DER) of 10.45%, and\nJacard error rate (JER) of 22.46% on the evaluation set.", "published": "2020-10-22 12:42:07", "link": "http://arxiv.org/abs/2010.11657v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Rethinking Evaluation in ASR: Are Our Models Robust Enough?", "abstract": "Is pushing numbers on a single benchmark valuable in automatic speech\nrecognition? Research results in acoustic modeling are typically evaluated\nbased on performance on a single dataset. While the research community has\ncoalesced around various benchmarks, we set out to understand generalization\nperformance in acoustic modeling across datasets - in particular, if models\ntrained on a single dataset transfer to other (possibly out-of-domain)\ndatasets. We show that, in general, reverberative and additive noise\naugmentation improves generalization performance across domains. Further, we\ndemonstrate that when a large enough set of benchmarks is used, average word\nerror rate (WER) performance over them provides a good proxy for performance on\nreal-world noisy data. Finally, we show that training a single acoustic model\non the most widely-used datasets - combined - reaches competitive performance\non both research and real-world benchmarks.", "published": "2020-10-22 14:01:32", "link": "http://arxiv.org/abs/2010.11745v3", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "68T07, 68T10", "I.2.6; I.5.4"], "primary_category": "cs.LG"}
{"title": "Self-Alignment Pretraining for Biomedical Entity Representations", "abstract": "Despite the widespread success of self-supervised learning via masked\nlanguage models (MLM), accurately capturing fine-grained semantic relationships\nin the biomedical domain remains a challenge. This is of paramount importance\nfor entity-level tasks such as entity linking where the ability to model entity\nrelations (especially synonymy) is pivotal. To address this challenge, we\npropose SapBERT, a pretraining scheme that self-aligns the representation space\nof biomedical entities. We design a scalable metric learning framework that can\nleverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts.\nIn contrast with previous pipeline-based hybrid systems, SapBERT offers an\nelegant one-model-for-all solution to the problem of medical entity linking\n(MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking\ndatasets. In the scientific domain, we achieve SOTA even without task-specific\nsupervision. With substantial improvement over various domain-specific\npretrained MLMs such as BioBERT, SciBERTand and PubMedBERT, our pretraining\nscheme proves to be both effective and robust.", "published": "2020-10-22 14:59:57", "link": "http://arxiv.org/abs/2010.11784v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Compositional embedding models for speaker identification and\n  diarization with simultaneous speech from 2+ speakers", "abstract": "We propose a new method for speaker diarization that can handle overlapping\nspeech with 2+ people. Our method is based on compositional embeddings [1]:\nLike standard speaker embedding methods such as x-vector [2], compositional\nembedding models contain a function f that separates speech from different\nspeakers. In addition, they include a composition function g to compute\nset-union operations in the embedding space so as to infer the set of speakers\nwithin the input audio. In an experiment on multi-person speaker identification\nusing synthesized LibriSpeech data, the proposed method outperforms traditional\nembedding methods that are only trained to separate single speakers (not\nspeaker sets). In a speaker diarization experiment on the AMI Headset Mix\ncorpus, we achieve state-of-the-art accuracy (DER=22.93%), slightly higher than\nthe previous best result (23.82% from [3]).", "published": "2020-10-22 15:33:36", "link": "http://arxiv.org/abs/2010.11803v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Detecting and Exorcising Statistical Demons from Language Models with\n  Anti-Models of Negative Data", "abstract": "It's been said that \"Language Models are Unsupervised Multitask Learners.\"\nIndeed, self-supervised language models trained on \"positive\" examples of\nEnglish text generalize in desirable ways to many natural language tasks. But\nif such models can stray so far from an initial self-supervision objective, a\nwayward model might generalize in undesirable ways too, say to nonsensical\n\"negative\" examples of unnatural language. A key question in this work is: do\nlanguage models trained on (positive) training data also generalize to\n(negative) test data? We use this question as a contrivance to assess the\nextent to which language models learn undesirable properties of text, such as\nn-grams, that might interfere with the learning of more desirable properties of\ntext, such as syntax. We find that within a model family, as the number of\nparameters, training epochs, and data set size increase, so does a model's\nability to generalize to negative n-gram data, indicating standard\nself-supervision generalizes too far. We propose a form of inductive bias that\nattenuates such undesirable signals with negative data distributions\nautomatically learned from positive data. We apply the method to remove n-gram\nsignals from LSTMs and find that doing so causes them to favor syntactic\nsignals, as demonstrated by large error reductions (up to 46% on the hardest\ncases) on a syntactic subject-verb agreement task.", "published": "2020-10-22 16:45:32", "link": "http://arxiv.org/abs/2010.11855v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rewriting Meaningful Sentences via Conditional BERT Sampling and an\n  application on fooling text classifiers", "abstract": "Most adversarial attack methods that are designed to deceive a text\nclassifier change the text classifier's prediction by modifying a few words or\ncharacters. Few try to attack classifiers by rewriting a whole sentence, due to\nthe difficulties inherent in sentence-level rephrasing as well as the problem\nof setting the criteria for legitimate rewriting.\n  In this paper, we explore the problem of creating adversarial examples with\nsentence-level rewriting. We design a new sampling method, named\nParaphraseSampler, to efficiently rewrite the original sentence in multiple\nways. Then we propose a new criteria for modification, called a sentence-level\nthreaten model. This criteria allows for both word- and sentence-level changes,\nand can be adjusted independently in two dimensions: semantic similarity and\ngrammatical quality. Experimental results show that many of these rewritten\nsentences are misclassified by the classifier. On all 6 datasets, our\nParaphraseSampler achieves a better attack success rate than our baseline.", "published": "2020-10-22 17:03:13", "link": "http://arxiv.org/abs/2010.11869v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Limitations of Autoregressive Models and Their Alternatives", "abstract": "Standard autoregressive language models perform only polynomial-time\ncomputation to compute the probability of the next symbol. While this is\nattractive, it means they cannot model distributions whose next-symbol\nprobability is hard to compute. Indeed, they cannot even model them well enough\nto solve associated easy decision problems for which an engineer might want to\nconsult a language model. These limitations apply no matter how much\ncomputation and data are used to train the model, unless the model is given\naccess to oracle parameters that grow superpolynomially in sequence length.\n  Thus, simply training larger autoregressive language models is not a panacea\nfor NLP. Alternatives include energy-based models (which give up efficient\nsampling) and latent-variable autoregressive models (which give up efficient\nscoring of a given string). Both are powerful enough to escape the above\nlimitations.", "published": "2020-10-22 17:59:09", "link": "http://arxiv.org/abs/2010.11939v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Differentially Private Text Perturbation Method Using a Regularized\n  Mahalanobis Metric", "abstract": "Balancing the privacy-utility tradeoff is a crucial requirement of many\npractical machine learning systems that deal with sensitive customer data. A\npopular approach for privacy-preserving text analysis is noise injection, in\nwhich text data is first mapped into a continuous embedding space, perturbed by\nsampling a spherical noise from an appropriate distribution, and then projected\nback to the discrete vocabulary space. While this allows the perturbation to\nadmit the required metric differential privacy, often the utility of downstream\ntasks modeled on this perturbed data is low because the spherical noise does\nnot account for the variability in the density around different words in the\nembedding space. In particular, words in a sparse region are likely unchanged\neven when the noise scale is large. %Using the global sensitivity of the\nmechanism can potentially add too much noise to the words in the dense regions\nof the embedding space, causing a high utility loss, whereas using local\nsensitivity can leak information through the scale of the noise added.\n  In this paper, we propose a text perturbation mechanism based on a carefully\ndesigned regularized variant of the Mahalanobis metric to overcome this\nproblem. For any given noise scale, this metric adds an elliptical noise to\naccount for the covariance structure in the embedding space. This heterogeneity\nin the noise scale along different directions helps ensure that the words in\nthe sparse region have sufficient likelihood of replacement without sacrificing\nthe overall utility. We provide a text-perturbation algorithm based on this\nmetric and formally prove its privacy guarantees. Additionally, we empirically\nshow that our mechanism improves the privacy statistics to achieve the same\nlevel of utility as compared to the state-of-the-art Laplace mechanism.", "published": "2020-10-22 23:06:44", "link": "http://arxiv.org/abs/2010.11947v1", "categories": ["cs.CL", "cs.CR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Language Models are Open Knowledge Graphs", "abstract": "This paper shows how to construct knowledge graphs (KGs) from pre-trained\nlanguage models (e.g., BERT, GPT-2/3), without human supervision. Popular KGs\n(e.g, Wikidata, NELL) are built in either a supervised or semi-supervised\nmanner, requiring humans to create knowledge. Recent deep language models\nautomatically acquire knowledge from large-scale corpora via pre-training. The\nstored knowledge has enabled the language models to improve downstream NLP\ntasks, e.g., answering questions, and writing code and articles. In this paper,\nwe propose an unsupervised method to cast the knowledge contained within\nlanguage models into KGs. We show that KGs are constructed with a single\nforward pass of the pre-trained language models (without fine-tuning) over the\ncorpora. We demonstrate the quality of the constructed KGs by comparing to two\nKGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual\nknowledge that is new in the existing KGs. Our code and KGs will be made\npublicly available.", "published": "2020-10-22 18:01:56", "link": "http://arxiv.org/abs/2010.11967v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Turking Test: Can Language Models Understand Instructions?", "abstract": "Supervised machine learning provides the learner with a set of input-output\nexamples of the target task. Humans, however, can also learn to perform new\ntasks from instructions in natural language. Can machines learn to understand\ninstructions as well? We present the Turking Test, which examines a model's\nability to follow natural language instructions of varying complexity. These\nrange from simple tasks, like retrieving the nth word of a sentence, to ones\nthat require creativity, such as generating examples for SNLI and SQuAD in\nplace of human intelligence workers (\"turkers\"). Despite our lenient evaluation\nmethodology, we observe that a large pretrained language model performs poorly\nacross all tasks. Analyzing the model's error patterns reveals that the model\ntends to ignore explicit instructions and often generates outputs that cannot\nbe construed as an attempt to solve the task. While it is not yet clear whether\ninstruction understanding can be captured by traditional language models, the\nsheer expressivity of instruction understanding makes it an appealing\nalternative to the rising few-shot inference paradigm.", "published": "2020-10-22 18:44:16", "link": "http://arxiv.org/abs/2010.11982v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MTAG: Modal-Temporal Attention Graph for Unaligned Human Multimodal\n  Language Sequences", "abstract": "Human communication is multimodal in nature; it is through multiple\nmodalities such as language, voice, and facial expressions, that opinions and\nemotions are expressed. Data in this domain exhibits complex multi-relational\nand temporal interactions. Learning from this data is a fundamentally\nchallenging research problem. In this paper, we propose Modal-Temporal\nAttention Graph (MTAG). MTAG is an interpretable graph-based neural model that\nprovides a suitable framework for analyzing multimodal sequential data. We\nfirst introduce a procedure to convert unaligned multimodal sequence data into\na graph with heterogeneous nodes and edges that captures the rich interactions\nacross modalities and through time. Then, a novel graph fusion operation,\ncalled MTAG fusion, along with a dynamic pruning and read-out technique, is\ndesigned to efficiently process this modal-temporal graph and capture various\ninteractions. By learning to focus only on the important interactions within\nthe graph, MTAG achieves state-of-the-art performance on multimodal sentiment\nanalysis and emotion recognition benchmarks, while utilizing significantly\nfewer model parameters.", "published": "2020-10-22 18:58:50", "link": "http://arxiv.org/abs/2010.11985v2", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Towards Zero-Shot Multilingual Synthetic Question and Answer Generation\n  for Cross-Lingual Reading Comprehension", "abstract": "We propose a simple method to generate multilingual question and answer pairs\non a large scale through the use of a single generative model. These synthetic\nsamples can be used to improve the zero-shot performance of multilingual QA\nmodels on target languages. Our proposed multi-task training of the generative\nmodel only requires the labeled training samples in English, thus removing the\nneed for such samples in the target languages, making it applicable to far more\nlanguages than those with labeled data. Human evaluations indicate the majority\nof such samples are grammatically correct and sensible. Experimental results\nshow our proposed approach can achieve large gains on the XQuAD dataset,\nreducing the gap between zero-shot and supervised performance of smaller QA\nmodels on various languages.", "published": "2020-10-22 19:59:37", "link": "http://arxiv.org/abs/2010.12008v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language-Conditioned Imitation Learning for Robot Manipulation Tasks", "abstract": "Imitation learning is a popular approach for teaching motor skills to robots.\nHowever, most approaches focus on extracting policy parameters from execution\ntraces alone (i.e., motion trajectories and perceptual data). No adequate\ncommunication channel exists between the human expert and the robot to describe\ncritical aspects of the task, such as the properties of the target object or\nthe intended shape of the motion. Motivated by insights into the human teaching\nprocess, we introduce a method for incorporating unstructured natural language\ninto imitation learning. At training time, the expert can provide\ndemonstrations along with verbal descriptions in order to describe the\nunderlying intent (e.g., \"go to the large green bowl\"). The training process\nthen interrelates these two modalities to encode the correlations between\nlanguage, perception, and motion. The resulting language-conditioned visuomotor\npolicies can be conditioned at runtime on new human commands and instructions,\nwhich allows for more fine-grained control over the trained policies while also\nreducing situational ambiguity. We demonstrate in a set of simulation\nexperiments how our approach can learn language-conditioned manipulation\npolicies for a seven-degree-of-freedom robot arm and compare the results to a\nvariety of alternative methods.", "published": "2020-10-22 21:49:08", "link": "http://arxiv.org/abs/2010.12083v1", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Improving Streaming Automatic Speech Recognition With Non-Streaming\n  Model Distillation On Unsupervised Data", "abstract": "Streaming end-to-end automatic speech recognition (ASR) models are widely\nused on smart speakers and on-device applications. Since these models are\nexpected to transcribe speech with minimal latency, they are constrained to be\ncausal with no future context, compared to their non-streaming counterparts.\nConsequently, streaming models usually perform worse than non-streaming models.\nWe propose a novel and effective learning method by leveraging a non-streaming\nASR model as a teacher to generate transcripts on an arbitrarily large data\nset, which is then used to distill knowledge into streaming ASR models. This\nway, we scale the training of streaming models to up to 3 million hours of\nYouTube audio. Experiments show that our approach can significantly reduce the\nword error rate (WER) of RNNT models not only on LibriSpeech but also on\nYouTube data in four languages. For example, in French, we are able to reduce\nthe WER by 16.4% relatively to a baseline streaming model by leveraging a\nnon-streaming teacher model trained on the same amount of labeled data as the\nbaseline.", "published": "2020-10-22 22:41:33", "link": "http://arxiv.org/abs/2010.12096v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "How Phonotactics Affect Multilingual and Zero-shot ASR Performance", "abstract": "The idea of combining multiple languages' recordings to train a single\nautomatic speech recognition (ASR) model brings the promise of the emergence of\nuniversal speech representation. Recently, a Transformer encoder-decoder model\nhas been shown to leverage multilingual data well in IPA transcriptions of\nlanguages presented during training. However, the representations it learned\nwere not successful in zero-shot transfer to unseen languages. Because that\nmodel lacks an explicit factorization of the acoustic model (AM) and language\nmodel (LM), it is unclear to what degree the performance suffered from\ndifferences in pronunciation or the mismatch in phonotactics. To gain more\ninsight into the factors limiting zero-shot ASR transfer, we replace the\nencoder-decoder with a hybrid ASR system consisting of a separate AM and LM.\nThen, we perform an extensive evaluation of monolingual, multilingual, and\ncrosslingual (zero-shot) acoustic and language models on a set of 13\nphonetically diverse languages. We show that the gain from modeling\ncrosslingual phonotactics is limited, and imposing a too strong model can hurt\nthe zero-shot transfer. Furthermore, we find that a multilingual LM hurts a\nmultilingual ASR system's performance, and retaining only the target language's\nphonotactic data in LM training is preferable.", "published": "2020-10-22 23:07:24", "link": "http://arxiv.org/abs/2010.12104v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NU-GAN: High resolution neural upsampling with GAN", "abstract": "In this paper, we propose NU-GAN, a new method for resampling audio from\nlower to higher sampling rates (upsampling). Audio upsampling is an important\nproblem since productionizing generative speech technology requires operating\nat high sampling rates. Such applications use audio at a resolution of 44.1 kHz\nor 48 kHz, whereas current speech synthesis methods are equipped to handle a\nmaximum of 24 kHz resolution. NU-GAN takes a leap towards solving audio\nupsampling as a separate component in the text-to-speech (TTS) pipeline by\nleveraging techniques for audio generation using GANs. ABX preference tests\nindicate that our NU-GAN resampler is capable of resampling 22 kHz to 44.1 kHz\naudio that is distinguishable from original audio only 7.4% higher than random\nchance for single speaker dataset, and 10.8% higher than chance for\nmulti-speaker dataset.", "published": "2020-10-22 01:00:23", "link": "http://arxiv.org/abs/2010.11362v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Robust Text-Dependent Speaker Verification via Character-Level\n  Information Preservation for the SdSV Challenge 2020", "abstract": "This paper describes our submission to Task 1 of the Short-duration Speaker\nVerification (SdSV) challenge 2020. Task 1 is a text-dependent speaker\nverification task, where both the speaker and phrase are required to be\nverified. The submitted systems were composed of TDNN-based and ResNet-based\nfront-end architectures, in which the frame-level features were aggregated with\nvarious pooling methods (e.g., statistical, self-attentive, ghostVLAD pooling).\nAlthough the conventional pooling methods provide embeddings with a sufficient\namount of speaker-dependent information, our experiments show that these\nembeddings often lack phrase-dependent information. To mitigate this problem,\nwe propose a new pooling and score compensation methods that leverage a\nCTC-based automatic speech recognition (ASR) model for taking the lexical\ncontent into account. Both methods showed improvement over the conventional\ntechniques, and the best performance was achieved by fusing all the\nexperimented systems, which showed 0.0785% MinDCF and 2.23% EER on the\nchallenge's evaluation subset.", "published": "2020-10-22 03:22:24", "link": "http://arxiv.org/abs/2010.11408v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Representation Learning for Speaker Recognition via\n  Contrastive Equilibrium Learning", "abstract": "In this paper, we propose a simple but powerful unsupervised learning method\nfor speaker recognition, namely Contrastive Equilibrium Learning (CEL), which\nincreases the uncertainty on nuisance factors latent in the embeddings by\nemploying the uniformity loss. Also, to preserve speaker discriminability, a\ncontrastive similarity loss function is used together. Experimental results\nshowed that the proposed CEL significantly outperforms the state-of-the-art\nunsupervised speaker verification systems and the best performing model\nachieved 8.01% and 4.01% EER on VoxCeleb1 and VOiCES evaluation sets,\nrespectively. On top of that, the performance of the supervised speaker\nembedding networks trained with initial parameters pre-trained via CEL showed\nbetter performance than those trained with randomly initialized parameters.", "published": "2020-10-22 04:25:13", "link": "http://arxiv.org/abs/2010.11433v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Parallel Tacotron: Non-Autoregressive and Controllable TTS", "abstract": "Although neural end-to-end text-to-speech models can synthesize highly\nnatural speech, there is still room for improvements to its efficiency and\nnaturalness. This paper proposes a non-autoregressive neural text-to-speech\nmodel augmented with a variational autoencoder-based residual encoder. This\nmodel, called \\emph{Parallel Tacotron}, is highly parallelizable during both\ntraining and inference, allowing efficient synthesis on modern parallel\nhardware. The use of the variational autoencoder relaxes the one-to-many\nmapping nature of the text-to-speech problem and improves naturalness. To\nfurther improve the naturalness, we use lightweight convolutions, which can\nefficiently capture local contexts, and introduce an iterative spectrogram loss\ninspired by iterative refinement. Experimental results show that Parallel\nTacotron matches a strong autoregressive baseline in subjective evaluations\nwith significantly decreased inference time.", "published": "2020-10-22 04:40:53", "link": "http://arxiv.org/abs/2010.11439v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Momentum Contrast Speaker Representation Learning", "abstract": "Unsupervised representation learning has shown remarkable achievement by\nreducing the performance gap with supervised feature learning, especially in\nthe image domain. In this study, to extend the technique of unsupervised\nlearning to the speech domain, we propose the Momentum Contrast for VoxCeleb\n(MoCoVox) as a form of learning mechanism. We pre-trained the MoCoVox on the\nVoxCeleb1 by implementing instance discrimination. Applying MoCoVox for speaker\nverification revealed that it outperforms the state-of-the-art metric\nlearning-based approach by a large margin. We also empirically demonstrate the\nfeatures of contrastive learning in the speech domain by analyzing the\ndistribution of learned representations. Furthermore, we explored which pretext\ntask is adequate for speaker verification. We expect that learning speaker\nrepresentation without human supervision helps to address the open-set speaker\nrecognition.", "published": "2020-10-22 05:46:13", "link": "http://arxiv.org/abs/2010.11457v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Microsoft Speaker Diarization System for the VoxCeleb Speaker\n  Recognition Challenge 2020", "abstract": "This paper describes the Microsoft speaker diarization system for monaural\nmulti-talker recordings in the wild, evaluated at the diarization track of the\nVoxCeleb Speaker Recognition Challenge(VoxSRC) 2020. We will first explain our\nsystem design to address issues in handling real multi-talker recordings. We\nthen present the details of the components, which include Res2Net-based speaker\nembedding extractor, conformer-based continuous speech separation with leakage\nfiltering, and a modified DOVER (short for Diarization Output Voting Error\nReduction) method for system fusion. We evaluate the systems with the data set\nprovided by VoxSRCchallenge 2020, which contains real-life multi-talker audio\ncollected from YouTube. Our best system achieves 3.71% and 6.23% of the\ndiarization error rate (DER) on development set and evaluation set,\nrespectively, being ranked the 1st at the diarization track of the challenge.", "published": "2020-10-22 05:46:15", "link": "http://arxiv.org/abs/2010.11458v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multilingual Approach to Joint Speech and Accent Recognition with\n  DNN-HMM Framework", "abstract": "Human can recognize speech, as well as the peculiar accent of the speech\nsimultaneously. However, present state-of-the-art ASR system can rarely do\nthat. In this paper, we propose a multilingual approach to recognizing English\nspeech, and related accent that speaker conveys using DNN-HMM framework.\nSpecifically, we assume different accents of English as different languages. We\nthen merge them together and train a multilingual ASR system. During decoding,\nwe conduct two experiments. One is a monolingual ASR-based decoding, with the\naccent information embedded at phone level, realizing word-based accent\nrecognition (AR), and the other is a multilingual ASR-based decoding, realizing\nan approximated utterance-based AR. Experimental results on an 8-accent English\nspeech recognition show both methods can yield WERs close to the conventional\nASR systems that completely ignore the accent, as well as desired AR accuracy.\nBesides, we conduct extensive analysis for the proposed method, such as\ntransfer learning without-domain data exploitation, cross-accent recognition\nconfusion, as well as characteristics of accented-word.", "published": "2020-10-22 07:09:39", "link": "http://arxiv.org/abs/2010.11483v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The NTU-AISG Text-to-speech System for Blizzard Challenge 2020", "abstract": "We report our NTU-AISG Text-to-speech (TTS) entry systems for the Blizzard\nChallenge 2020 in this paper. There are two TTS tasks in this year's challenge,\none is a Mandarin TTS task, the other is a Shanghai dialect TTS task. We have\nparticipated both. One of the main challenges is to build TTS systems with\nlow-resource constraints, particularly for the case of Shanghai dialect, of\nwhich about three hours data are available to participants. To overcome the\nconstraint, we adopt an average-speaker modeling method. That is, we first\nemploy external Mandarin data to train both End-to-end acoustic model and\nWaveNet vocoder, then we use Shanghai dialect to tune the acoustic model and\nWaveNet vocoder respectively. Apart from this, we have no Shanghai dialect\nlexicon despite syllable transcripts are provided for the training data. Since\nwe are not sure if similar syllable transcripts are provided for the evaluation\ndata during the training stage, we use Mandarin lexicon for Shanghai dialect\ninstead. With the letter, as decomposed from the corresponding Mandarin\nsyllable, as input, though the naturalness and original speaker similarity of\nthe synthesized speech are good, subjective evaluation results indicate the\nintelligibility of the synthesized speech is deeply undermined for the Shanghai\ndialect TTS system.", "published": "2020-10-22 07:18:08", "link": "http://arxiv.org/abs/2010.11489v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "How Similar or Different Is Rakugo Speech Synthesizer to Professional\n  Performers?", "abstract": "We have been working on speech synthesis for rakugo (a traditional Japanese\nform of verbal entertainment similar to one-person stand-up comedy) toward\nspeech synthesis that authentically entertains audiences. In this paper, we\npropose a novel evaluation methodology using synthesized rakugo speech and real\nrakugo speech uttered by professional performers of three different ranks. The\nnaturalness of the synthesized speech was comparable to that of the human\nspeech, but the synthesized speech entertained listeners less than the\nperformers of any rank. However, we obtained some interesting insights into\nchallenges to be solved in order to achieve a truly entertaining rakugo\nsynthesizer. For example, naturalness was not the most important factor, even\nthough it has generally been emphasized as the most important point to be\nevaluated in the conventional speech synthesis field. More important factors\nwere the understandability of the content and distinguishability of the\ncharacters in the rakugo story, both of which the synthesized rakugo speech was\nrelatively inferior at as compared with the professional performers. We also\nfound that fundamental frequency fo modeling should be further improved to\nbetter entertain audiences. These results show important steps to reaching\nauthentically entertaining speech synthesis.", "published": "2020-10-22 09:21:26", "link": "http://arxiv.org/abs/2010.11549v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AISHELL-3: A Multi-speaker Mandarin TTS Corpus and the Baselines", "abstract": "In this paper, we present AISHELL-3, a large-scale and high-fidelity\nmulti-speaker Mandarin speech corpus which could be used to train multi-speaker\nText-to-Speech (TTS) systems. The corpus contains roughly 85 hours of\nemotion-neutral recordings spoken by 218 native Chinese mandarin speakers.\nTheir auxiliary attributes such as gender, age group and native accents are\nexplicitly marked and provided in the corpus. Accordingly, transcripts in\nChinese character-level and pinyin-level are provided along with the\nrecordings. We present a baseline system that uses AISHELL-3 for multi-speaker\nMadarin speech synthesis. The multi-speaker speech synthesis system is an\nextension on Tacotron-2 where a speaker verification model and a corresponding\nloss regarding voice similarity are incorporated as the feedback constraint. We\naim to use the presented corpus to build a robust synthesis model that is able\nto achieve zero-shot voice cloning. The system trained on this dataset also\ngeneralizes well on speakers that are never seen in the training process.\nObjective evaluation results from our experiments show that the proposed\nmulti-speaker synthesis system achieves high voice similarity concerning both\nspeaker embedding similarity and equal error rate measurement. The dataset,\nbaseline system code and generated samples are available online.", "published": "2020-10-22 09:54:22", "link": "http://arxiv.org/abs/2010.11567v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Low-Resource StarGAN Voice Conversion using Weight Adaptive\n  Instance Normalization", "abstract": "Many-to-many voice conversion with non-parallel training data has seen\nsignificant progress in recent years. StarGAN-based models have been interests\nof voice conversion. However, most of the StarGAN-based methods only focused on\nvoice conversion experiments for the situations where the number of speakers\nwas small, and the amount of training data was large. In this work, we aim at\nimproving the data efficiency of the model and achieving a many-to-many\nnon-parallel StarGAN-based voice conversion for a relatively large number of\nspeakers with limited training samples. In order to improve data efficiency,\nthe proposed model uses a speaker encoder for extracting speaker embeddings and\nconducts adaptive instance normalization (AdaIN) on convolutional weights.\nExperiments are conducted with 109 speakers under two low-resource situations,\nwhere the number of training samples is 20 and 5 per speaker. An objective\nevaluation shows the proposed model is better than the baseline methods.\nFurthermore, a subjective evaluation shows that, for both naturalness and\nsimilarity, the proposed model outperforms the baseline method.", "published": "2020-10-22 12:32:45", "link": "http://arxiv.org/abs/2010.11646v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Analysis of the BUT Diarization System for VoxConverse Challenge", "abstract": "This paper describes the system developed by the BUT team for the fourth\ntrack of the VoxCeleb Speaker Recognition Challenge, focusing on diarization on\nthe VoxConverse dataset. The system consists of signal pre-processing, voice\nactivity detection, speaker embedding extraction, an initial agglomerative\nhierarchical clustering followed by diarization using a Bayesian hidden Markov\nmodel, a reclustering step based on per-speaker global embeddings and\noverlapped speech detection and handling. We provide comparisons for each of\nthe steps and share the implementation of the most relevant modules of our\nsystem. Our system scored second in the challenge in terms of the primary\nmetric (diarization error rate) and first according to the secondary metric\n(Jaccard error rate).", "published": "2020-10-22 13:46:53", "link": "http://arxiv.org/abs/2010.11718v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Perceptual Loss based Speech Denoising with an ensemble of Audio Pattern\n  Recognition and Self-Supervised Models", "abstract": "Deep learning based speech denoising still suffers from the challenge of\nimproving perceptual quality of enhanced signals. We introduce a generalized\nframework called Perceptual Ensemble Regularization Loss (PERL) built on the\nidea of perceptual losses. Perceptual loss discourages distortion to certain\nspeech properties and we analyze it using six large-scale pre-trained models:\nspeaker classification, acoustic model, speaker embedding, emotion\nclassification, and two self-supervised speech encoders (PASE+, wav2vec 2.0).\nWe first build a strong baseline (w/o PERL) using Conformer Transformer\nNetworks on the popular enhancement benchmark called VCTK-DEMAND. Using\nauxiliary models one at a time, we find acoustic event and self-supervised\nmodel PASE+ to be most effective. Our best model (PERL-AE) only uses acoustic\nevent model (utilizing AudioSet) to outperform state-of-the-art methods on\nmajor perceptual metrics. To explore if denoising can leverage full framework,\nwe use all networks but find that our seven-loss formulation suffers from the\nchallenges of Multi-Task Learning. Finally, we report a critical observation\nthat state-of-the-art Multi-Task weight learning methods cannot outperform hand\ntuning, perhaps due to challenges of domain mismatch and weak complementarity\nof losses.", "published": "2020-10-22 16:51:04", "link": "http://arxiv.org/abs/2010.11860v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-training and Pre-training are Complementary for Speech Recognition", "abstract": "Self-training and unsupervised pre-training have emerged as effective\napproaches to improve speech recognition systems using unlabeled data. However,\nit is not clear whether they learn similar patterns or if they can be\neffectively combined. In this paper, we show that pseudo-labeling and\npre-training with wav2vec 2.0 are complementary in a variety of labeled data\nsetups. Using just 10 minutes of labeled data from Libri-light as well as 53k\nhours of unlabeled data from LibriVox achieves WERs of 3.0%/5.2% on the clean\nand other test sets of Librispeech - rivaling the best published systems\ntrained on 960 hours of labeled data only a year ago. Training on all labeled\ndata of Librispeech achieves WERs of 1.5%/3.1%.", "published": "2020-10-22 04:15:37", "link": "http://arxiv.org/abs/2010.11430v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "A Framework for Generative and Contrastive Learning of Audio\n  Representations", "abstract": "In this paper, we present a framework for contrastive learning for audio\nrepresentations, in a self supervised frame work without access to any ground\ntruth labels. The core idea in self supervised contrastive learning is to map\nan audio signal and its various augmented versions (representative of salient\naspects of audio like pitch, timbre etc.) to a space where they are close\ntogether, and are separated from other different signals. In addition we also\nexplore generative models based on state of the art transformer based\narchitectures for learning latent spaces for audio signals, without access to\nany labels. Here, we map audio signals on a smaller scale to discrete\ndictionary elements and train transformers to predict the next dictionary\nelement. We only use data as a method of supervision, bypassing the need of\nlabels needed to act as a supervision for training the deep neural networks. We\nthen use a linear classifier head in order to evaluate the performance of our\nmodels, for both self supervised contrastive and generative transformer based\nrepresentations that are learned. Our system achieves considerable performance,\ncompared to a fully supervised method, with access to ground truth labels to\ntrain the neural network model. These representations, with avail-ability of\nlarge scale audio data show promise in various tasks for audio understanding\ntasks", "published": "2020-10-22 05:52:32", "link": "http://arxiv.org/abs/2010.11459v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mood Classification Using Listening Data", "abstract": "The mood of a song is a highly relevant feature for exploration and\nrecommendation in large collections of music. These collections tend to require\nautomatic methods for predicting such moods. In this work, we show that\nlistening-based features outperform content-based ones when classifying moods:\nembeddings obtained through matrix factorization of listening data appear to be\nmore informative of a track mood than embeddings based on its audio content. To\ndemonstrate this, we compile a subset of the Million Song Dataset, totalling\n67k tracks, with expert annotations of 188 different moods collected from\nAllMusic. Our results on this novel dataset not only expose the limitations of\ncurrent audio-based models, but also aim to foster further reproducible\nresearch on this timely topic.", "published": "2020-10-22 08:13:56", "link": "http://arxiv.org/abs/2010.11512v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DBNET: DOA-driven beamforming network for end-to-end farfield sound\n  source separation", "abstract": "Many deep learning techniques are available to perform source separation and\nreduce background noise. However, designing an end-to-end multi-channel source\nseparation method using deep learning and conventional acoustic signal\nprocessing techniques still remains challenging. In this paper we propose a\ndirection-of-arrival-driven beamforming network (DBnet) consisting of\ndirection-of-arrival (DOA) estimation and beamforming layers for end-to-end\nsource separation. We propose to train DBnet using loss functions that are\nsolely based on the distances between the separated speech signals and the\ntarget speech signals, without a need for the ground-truth DOAs of speakers. To\nimprove the source separation performance, we also propose end-to-end\nextensions of DBnet which incorporate post masking networks. We evaluate the\nproposed DBnet and its extensions on a very challenging dataset, targeting\nrealistic far-field sound source separation in reverberant and noisy\nenvironments. The experimental results show that the proposed extended DBnet\nusing a convolutional-recurrent post masking network outperforms\nstate-of-the-art source separation methods.", "published": "2020-10-22 09:52:05", "link": "http://arxiv.org/abs/2010.11566v1", "categories": ["eess.AS", "cs.AI", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Backdoor Attack against Speaker Verification", "abstract": "Speaker verification has been widely and successfully adopted in many\nmission-critical areas for user identification. The training of speaker\nverification requires a large amount of data, therefore users usually need to\nadopt third-party data ($e.g.$, data from the Internet or third-party data\ncompany). This raises the question of whether adopting untrusted third-party\ndata can pose a security threat. In this paper, we demonstrate that it is\npossible to inject the hidden backdoor for infecting speaker verification\nmodels by poisoning the training data. Specifically, we design a\nclustering-based attack scheme where poisoned samples from different clusters\nwill contain different triggers ($i.e.$, pre-defined utterances), based on our\nunderstanding of verification tasks. The infected models behave normally on\nbenign samples, while attacker-specified unenrolled triggers will successfully\npass the verification even if the attacker has no information about the\nenrolled speaker. We also demonstrate that existing backdoor attacks cannot be\ndirectly adopted in attacking speaker verification. Our approach not only\nprovides a new perspective for designing novel attacks, but also serves as a\nstrong baseline for improving the robustness of verification methods. The code\nfor reproducing main results is available at\n\\url{https://github.com/zhaitongqing233/Backdoor-attack-against-speaker-verification}.", "published": "2020-10-22 11:10:08", "link": "http://arxiv.org/abs/2010.11607v3", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "LaSAFT: Latent Source Attentive Frequency Transformation for Conditioned\n  Source Separation", "abstract": "Recent deep-learning approaches have shown that Frequency Transformation (FT)\nblocks can significantly improve spectrogram-based single-source separation\nmodels by capturing frequency patterns. The goal of this paper is to extend the\nFT block to fit the multi-source task. We propose the Latent Source Attentive\nFrequency Transformation (LaSAFT) block to capture source-dependent frequency\npatterns. We also propose the Gated Point-wise Convolutional Modulation\n(GPoCM), an extension of Feature-wise Linear Modulation (FiLM), to modulate\ninternal features. By employing these two novel methods, we extend the\nConditioned-U-Net (CUNet) for multi-source separation, and the experimental\nresults indicate that our LaSAFT and GPoCM can improve the CUNet's performance,\nachieving state-of-the-art SDR performance on several MUSDB18 source separation\ntasks.", "published": "2020-10-22 11:58:23", "link": "http://arxiv.org/abs/2010.11631v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neural Network-based Acoustic Vehicle Counting", "abstract": "This paper addresses acoustic vehicle counting using one-channel audio. We\npredict the pass-by instants of vehicles from local minima of clipped\nvehicle-to-microphone distance. This distance is predicted from audio using a\ntwo-stage (coarse-fine) regression, with both stages realised via neural\nnetworks (NNs). Experiments show that the NN-based distance regression\noutperforms by far the previously proposed support vector regression. The $\n95\\% $ confidence interval for the mean of vehicle counting error is within\n$[0.28\\%, -0.55\\%]$. Besides the minima-based counting, we propose a deep\nlearning counting that operates on the predicted distance without detecting\nlocal minima. Although outperformed in accuracy by the former approach, deep\ncounting has a significant advantage in that it does not depend on minima\ndetection parameters. Results also show that removing low frequencies in\nfeatures improves the counting performance.", "published": "2020-10-22 12:45:06", "link": "http://arxiv.org/abs/2010.11659v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CycleGAN-VC3: Examining and Improving CycleGAN-VCs for Mel-spectrogram\n  Conversion", "abstract": "Non-parallel voice conversion (VC) is a technique for learning mappings\nbetween source and target speeches without using a parallel corpus. Recently,\ncycle-consistent adversarial network (CycleGAN)-VC and CycleGAN-VC2 have shown\npromising results regarding this problem and have been widely used as benchmark\nmethods. However, owing to the ambiguity of the effectiveness of\nCycleGAN-VC/VC2 for mel-spectrogram conversion, they are typically used for\nmel-cepstrum conversion even when comparative methods employ mel-spectrogram as\na conversion target. To address this, we examined the applicability of\nCycleGAN-VC/VC2 to mel-spectrogram conversion. Through initial experiments, we\ndiscovered that their direct applications compromised the time-frequency\nstructure that should be preserved during conversion. To remedy this, we\npropose CycleGAN-VC3, an improvement of CycleGAN-VC2 that incorporates\ntime-frequency adaptive normalization (TFAN). Using TFAN, we can adjust the\nscale and bias of the converted features while reflecting the time-frequency\nstructure of the source mel-spectrogram. We evaluated CycleGAN-VC3 on\ninter-gender and intra-gender non-parallel VC. A subjective evaluation of\nnaturalness and similarity showed that for every VC pair, CycleGAN-VC3\noutperforms or is competitive with the two types of CycleGAN-VC2, one of which\nwas applied to mel-cepstrum and the other to mel-spectrogram. Audio samples are\navailable at\nhttp://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/cyclegan-vc3/index.html.", "published": "2020-10-22 13:08:44", "link": "http://arxiv.org/abs/2010.11672v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Robust Audio-Based Vehicle Counting in Low-to-Moderate Traffic Flow", "abstract": "The paper presents a method for audio-based vehicle counting (VC) in\nlow-to-moderate traffic using one-channel sound. We formulate VC as a\nregression problem, i.e., we predict the distance between a vehicle and the\nmicrophone. Minima of the proposed distance function correspond to vehicles\npassing by the microphone. VC is carried out via local minima detection in the\npredicted distance. We propose to set the minima detection threshold at a point\nwhere the probabilities of false positives and false negatives coincide so they\nstatistically cancel each other in total vehicle number. The method is trained\nand tested on a traffic-monitoring dataset comprising $422$ short, $20$-second\none-channel sound files with a total of $ 1421 $ vehicles passing by the\nmicrophone. Relative VC error in a traffic location not used in the training is\nbelow $ 2 \\%$ within a wide range of detection threshold values. Experimental\nresults show that the regression accuracy in noisy environments is improved by\nintroducing a novel high-frequency power feature.", "published": "2020-10-22 13:42:26", "link": "http://arxiv.org/abs/2010.11716v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Qualitative Analysis of Haptic Feedback in Music Focused Exercises", "abstract": "We present the findings of a pilot-study that analysed the role of haptic\nfeedback in a musical context. To examine the role of haptics in Digital\nMusical Instrument (DMI) design an experiment was formulated to measure the\nusers' perception of device usability across four separate feedback stages:\nfully haptic (force and tactile combined), constant force only, vibrotactile\nonly, and no feedback. The study was piloted over extended periods with the\nintention of exploring the application and integration of DMIs in real-world\nmusical contexts. Applying a music orientated analysis of this type enabled the\ninvestigative process to not only take place over a comprehensive period, but\nallowed for the exploration of DMI integration in everyday compositional\npractices. As with any investigation that involves creativity, it was important\nthat the participants did not feel rushed or restricted. That is, they were\ngiven sufficient time to explore and assess the different feedback types\nwithout constraint. This provided an accurate and representational set of\nqualitative data for validating the participants' experience with the different\nfeedback types they were presented with.", "published": "2020-10-22 14:00:14", "link": "http://arxiv.org/abs/2010.11744v2", "categories": ["cs.HC", "cs.MM", "cs.SD", "eess.AS", "H.5.2; H.5.5; J.5"], "primary_category": "cs.HC"}
{"title": "Urban Sound Classification : striving towards a fair comparison", "abstract": "Urban sound classification has been achieving remarkable progress and is\nstill an active research area in audio pattern recognition. In particular, it\nallows to monitor the noise pollution, which becomes a growing concern for\nlarge cities. The contribution of this paper is two-fold. First, we present our\nDCASE 2020 task 5 winning solution which aims at helping the monitoring of\nurban noise pollution. It achieves a macro-AUPRC of 0.82 / 0.62 for the coarse\n/ fine classification on validation set. Moreover, it reaches accuracies of\n89.7% and 85.41% respectively on ESC-50 and US8k datasets. Second, it is not\neasy to find a fair comparison and to reproduce the performance of existing\nmodels. Sometimes authors copy-pasting the results of the original papers which\nis not helping reproducibility. As a result, we provide a fair comparison by\nusing the same input representation, metrics and optimizer to assess\nperformances. We preserve data augmentation used by the original papers. We\nhope this framework could help evaluate new architectures in this field. For\nbetter reproducibility, the code is available on our GitHub repository.", "published": "2020-10-22 15:37:39", "link": "http://arxiv.org/abs/2010.11805v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Listening to 10 People Simultaneously: An Efficient Permutation\n  Invariant Training of Audio Source Separation Using Sinkhorn's Algorithm", "abstract": "In neural network-based monaural speech separation techniques, it has been\nrecently common to evaluate the loss using the permutation invariant training\n(PIT) loss. However, the ordinary PIT requires to try all $N!$ permutations\nbetween $N$ ground truths and $N$ estimates. Since the factorial complexity\nexplodes very rapidly as $N$ increases, a PIT-based training works only when\nthe number of source signals is small, such as $N = 2$ or $3$. To overcome this\nlimitation, this paper proposes a SinkPIT, a novel variant of the PIT losses,\nwhich is much more efficient than the ordinary PIT loss when $N$ is large. The\nSinkPIT is based on Sinkhorn's matrix balancing algorithm, which efficiently\nfinds a doubly stochastic matrix which approximates the best permutation in a\ndifferentiable manner. The author conducted an experiment to train a neural\nnetwork model to decompose a single-channel mixture into 10 sources using the\nSinkPIT, and obtained promising results.", "published": "2020-10-22 17:08:17", "link": "http://arxiv.org/abs/2010.11871v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Scene-Agnostic Multi-Microphone Speech Dereverberation", "abstract": "Neural networks (NNs) have been widely applied in speech processing tasks,\nand, in particular, those employing microphone arrays. Nevertheless, most\nexisting NN architectures can only deal with fixed and position-specific\nmicrophone arrays. In this paper, we present an NN architecture that can cope\nwith microphone arrays whose number and positions of the microphones are\nunknown, and demonstrate its applicability in the speech dereverberation task.\nTo this end, our approach harnesses recent advances in deep learning on\nset-structured data to design an architecture that enhances the reverberant\nlog-spectrum. We use noisy and noiseless versions of a simulated reverberant\ndataset to test the proposed architecture. Our experiments on the noisy data\nshow that the proposed scene-agnostic setup outperforms a powerful scene-aware\nframework, sometimes even with fewer microphones. With the noiseless dataset we\nshow that, in most cases, our method outperforms the position-aware network as\nwell as the state-of-the-art weighted linear prediction error (WPE) algorithm.", "published": "2020-10-22 17:13:12", "link": "http://arxiv.org/abs/2010.11875v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Transcription Is All You Need: Learning to Separate Musical Mixtures\n  with Score as Supervision", "abstract": "Most music source separation systems require large collections of isolated\nsources for training, which can be difficult to obtain. In this work, we use\nmusical scores, which are comparatively easy to obtain, as a weak label for\ntraining a source separation system. In contrast with previous score-informed\nseparation approaches, our system does not require isolated sources, and score\nis used only as a training target, not required for inference. Our model\nconsists of a separator that outputs a time-frequency mask for each instrument,\nand a transcriptor that acts as a critic, providing both temporal and frequency\nsupervision to guide the learning of the separator. A harmonic mask constraint\nis introduced as another way of leveraging score information during training,\nand we propose two novel adversarial losses for additional fine-tuning of both\nthe transcriptor and the separator. Results demonstrate that using score\ninformation outperforms temporal weak-labels, and adversarial structures lead\nto further improvements in both separation and transcription performance.", "published": "2020-10-22 17:38:40", "link": "http://arxiv.org/abs/2010.11904v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neural Audio Fingerprint for High-specific Audio Retrieval based on\n  Contrastive Learning", "abstract": "Most of existing audio fingerprinting systems have limitations to be used for\nhigh-specific audio retrieval at scale. In this work, we generate a\nlow-dimensional representation from a short unit segment of audio, and couple\nthis fingerprint with a fast maximum inner-product search. To this end, we\npresent a contrastive learning framework that derives from the segment-level\nsearch objective. Each update in training uses a batch consisting of a set of\npseudo labels, randomly selected original samples, and their augmented\nreplicas. These replicas can simulate the degrading effects on original audio\nsignals by applying small time offsets and various types of distortions, such\nas background noise and room/microphone impulse responses. In the segment-level\nsearch task, where the conventional audio fingerprinting systems used to fail,\nour system using 10x smaller storage has shown promising results. Our code and\ndataset are available at \\url{https://mimbres.github.io/neural-audio-fp/}.", "published": "2020-10-22 17:44:40", "link": "http://arxiv.org/abs/2010.11910v4", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Listening to Sounds of Silence for Speech Denoising", "abstract": "We introduce a deep learning model for speech denoising, a long-standing\nchallenge in audio analysis arising in numerous applications. Our approach is\nbased on a key observation about human speech: there is often a short pause\nbetween each sentence or word. In a recorded speech signal, those pauses\nintroduce a series of time periods during which only noise is present. We\nleverage these incidental silent intervals to learn a model for automatic\nspeech denoising given only mono-channel audio. Detected silent intervals over\ntime expose not just pure noise but its time-varying features, allowing the\nmodel to learn noise dynamics and suppress it from the speech signal.\nExperiments on multiple datasets confirm the pivotal role of silent interval\ndetection for speech denoising, and our method outperforms several\nstate-of-the-art denoising methods, including those that accept only audio\ninput (like ours) and those that denoise based on audiovisual input (and hence\nrequire more information). We also show that our method enjoys excellent\ngeneralization properties, such as denoising spoken languages not seen during\ntraining.", "published": "2020-10-22 20:07:53", "link": "http://arxiv.org/abs/2010.12013v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sequence-to-sequence Singing Voice Synthesis with Perceptual Entropy\n  Loss", "abstract": "The neural network (NN) based singing voice synthesis (SVS) systems require\nsufficient data to train well and are prone to over-fitting due to data\nscarcity. However, we often encounter data limitation problem in building SVS\nsystems because of high data acquisition and annotation costs. In this work, we\npropose a Perceptual Entropy (PE) loss derived from a psycho-acoustic hearing\nmodel to regularize the network. With a one-hour open-source singing voice\ndatabase, we explore the impact of the PE loss on various mainstream\nsequence-to-sequence models, including the RNN-based, transformer-based, and\nconformer-based models. Our experiments show that the PE loss can mitigate the\nover-fitting problem and significantly improve the synthesized singing quality\nreflected in objective and subjective evaluations.", "published": "2020-10-22 20:14:59", "link": "http://arxiv.org/abs/2010.12024v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Combination of Deep Speaker Embeddings for Diarisation", "abstract": "Significant progress has recently been made in speaker diarisation after the\nintroduction of d-vectors as speaker embeddings extracted from neural network\n(NN) speaker classifiers for clustering speech segments. To extract\nbetter-performing and more robust speaker embeddings, this paper proposes a\nc-vector method by combining multiple sets of complementary d-vectors derived\nfrom systems with different NN components. Three structures are used to\nimplement the c-vectors, namely 2D self-attentive, gated additive, and bilinear\npooling structures, relying on attention mechanisms, a gating mechanism, and a\nlow-rank bilinear pooling mechanism respectively. Furthermore, a neural-based\nsingle-pass speaker diarisation pipeline is also proposed in this paper, which\nuses NNs to achieve voice activity detection, speaker change point detection,\nand speaker embedding extraction. Experiments and detailed analyses are\nconducted on the challenging AMI and NIST RT05 datasets which consist of real\nmeetings with 4--10 speakers and a wide range of acoustic conditions. For\nsystems trained on the AMI training set, relative speaker error rate (SER)\nreductions of 13% and 29% are obtained by using c-vectors instead of d-vectors\non the AMI dev and eval sets respectively, and a relative reduction of 15% in\nSER is observed on RT05, which shows the robustness of the proposed methods. By\nincorporating VoxCeleb data into the training set, the best c-vector system\nachieved 7%, 17% and16% relative SER reduction compared to the d-vector on the\nAMI dev, eval, and RT05 sets respectively", "published": "2020-10-22 20:16:36", "link": "http://arxiv.org/abs/2010.12025v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Class-Conditional Defense GAN Against End-to-End Speech Attacks", "abstract": "In this paper we propose a novel defense approach against end-to-end\nadversarial attacks developed to fool advanced speech-to-text systems such as\nDeepSpeech and Lingvo. Unlike conventional defense approaches, the proposed\napproach does not directly employ low-level transformations such as\nautoencoding a given input signal aiming at removing potential adversarial\nperturbation. Instead of that, we find an optimal input vector for a class\nconditional generative adversarial network through minimizing the relative\nchordal distance adjustment between a given test input and the generator\nnetwork. Then, we reconstruct the 1D signal from the synthesized spectrogram\nand the original phase information derived from the given input signal. Hence,\nthis reconstruction does not add any extra noise to the signal and according to\nour experimental results, our defense-GAN considerably outperforms conventional\ndefense algorithms both in terms of word error rate and sentence level\nrecognition accuracy.", "published": "2020-10-22 00:02:02", "link": "http://arxiv.org/abs/2010.11352v2", "categories": ["cs.SD", "cs.CR", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
