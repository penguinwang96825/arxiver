{"title": "Language Chameleon: Transformation analysis between languages using\n  Cross-lingual Post-training based on Pre-trained language models", "abstract": "As pre-trained language models become more resource-demanding, the inequality\nbetween resource-rich languages such as English and resource-scarce languages\nis worsening. This can be attributed to the fact that the amount of available\ntraining data in each language follows the power-law distribution, and most of\nthe languages belong to the long tail of the distribution. Some research areas\nattempt to mitigate this problem. For example, in cross-lingual transfer\nlearning and multilingual training, the goal is to benefit long-tail languages\nvia the knowledge acquired from resource-rich languages. Although being\nsuccessful, existing work has mainly focused on experimenting on as many\nlanguages as possible. As a result, targeted in-depth analysis is mostly\nabsent. In this study, we focus on a single low-resource language and perform\nextensive evaluation and probing experiments using cross-lingual post-training\n(XPT). To make the transfer scenario challenging, we choose Korean as the\ntarget language, as it is a language isolate and thus shares almost no typology\nwith English. Results show that XPT not only outperforms or performs on par\nwith monolingual models trained with orders of magnitudes more data but also is\nhighly efficient in the transfer process.", "published": "2022-09-14 05:20:52", "link": "http://arxiv.org/abs/2209.06422v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SUN: Exploring Intrinsic Uncertainties in Text-to-SQL Parsers", "abstract": "This paper aims to improve the performance of text-to-SQL parsing by\nexploring the intrinsic uncertainties in the neural network based approaches\n(called SUN). From the data uncertainty perspective, it is indisputable that a\nsingle SQL can be learned from multiple semantically-equivalent\nquestions.Different from previous methods that are limited to one-to-one\nmapping, we propose a data uncertainty constraint to explore the underlying\ncomplementary semantic information among multiple semantically-equivalent\nquestions (many-to-one) and learn the robust feature representations with\nreduced spurious associations. In this way, we can reduce the sensitivity of\nthe learned representations and improve the robustness of the parser. From the\nmodel uncertainty perspective, there is often structural information\n(dependence) among the weights of neural networks. To improve the\ngeneralizability and stability of neural text-to-SQL parsers, we propose a\nmodel uncertainty constraint to refine the query representations by enforcing\nthe output representations of different perturbed encoding networks to be\nconsistent with each other. Extensive experiments on five benchmark datasets\ndemonstrate that our method significantly outperforms strong competitors and\nachieves new state-of-the-art results. For reproducibility, we release our code\nand data at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/sunsql.", "published": "2022-09-14 06:27:51", "link": "http://arxiv.org/abs/2209.06442v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt Combines Paraphrase: Teaching Pre-trained Models to Understand\n  Rare Biomedical Words", "abstract": "Prompt-based fine-tuning for pre-trained models has proven effective for many\nnatural language processing tasks under few-shot settings in general domain.\nHowever, tuning with prompt in biomedical domain has not been investigated\nthoroughly. Biomedical words are often rare in general domain, but quite\nubiquitous in biomedical contexts, which dramatically deteriorates the\nperformance of pre-trained models on downstream biomedical applications even\nafter fine-tuning, especially in low-resource scenarios. We propose a simple\nyet effective approach to helping models learn rare biomedical words during\ntuning with prompt. Experimental results show that our method can achieve up to\n6% improvement in biomedical natural language inference task without any extra\nparameters or training steps using few-shot vanilla prompt settings.", "published": "2022-09-14 07:03:29", "link": "http://arxiv.org/abs/2209.06453v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Find Strong Summary Coherence Measures? A Toolbox and a\n  Comparative Study for Summary Coherence Measure Evaluation", "abstract": "Automatically evaluating the coherence of summaries is of great significance\nboth to enable cost-efficient summarizer evaluation and as a tool for improving\ncoherence by selecting high-scoring candidate summaries. While many different\napproaches have been suggested to model summary coherence, they are often\nevaluated using disparate datasets and metrics. This makes it difficult to\nunderstand their relative performance and identify ways forward towards better\nsummary coherence modelling. In this work, we conduct a large-scale\ninvestigation of various methods for summary coherence modelling on an even\nplaying field. Additionally, we introduce two novel analysis measures,\nintra-system correlation and bias matrices, that help identify biases in\ncoherence measures and provide robustness against system-level confounders.\nWhile none of the currently available automatic coherence measures are able to\nassign reliable coherence scores to system summaries across all evaluation\nmetrics, large-scale language models fine-tuned on self-supervised tasks show\npromising results, as long as fine-tuning takes into account that they need to\ngeneralize across different summary lengths.", "published": "2022-09-14 09:42:19", "link": "http://arxiv.org/abs/2209.06517v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few Clean Instances Help Denoising Distant Supervision", "abstract": "Existing distantly supervised relation extractors usually rely on noisy data\nfor both model training and evaluation, which may lead to\ngarbage-in-garbage-out systems. To alleviate the problem, we study whether a\nsmall clean dataset could help improve the quality of distantly supervised\nmodels. We show that besides getting a more convincing evaluation of models, a\nsmall clean dataset also helps us to build more robust denoising models.\nSpecifically, we propose a new criterion for clean instance selection based on\ninfluence functions. It collects sample-level evidence for recognizing good\ninstances (which is more informative than loss-level evidence). We also propose\na teacher-student mechanism for controlling purity of intermediate results when\nbootstrapping the clean set. The whole approach is model-agnostic and\ndemonstrates strong performances on both denoising real (NYT) and synthetic\nnoisy datasets.", "published": "2022-09-14 12:29:57", "link": "http://arxiv.org/abs/2209.06596v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distribution Calibration for Out-of-Domain Detection with Bayesian\n  Approximation", "abstract": "Out-of-Domain (OOD) detection is a key component in a task-oriented dialog\nsystem, which aims to identify whether a query falls outside the predefined\nsupported intent set. Previous softmax-based detection algorithms are proved to\nbe overconfident for OOD samples. In this paper, we analyze overconfident OOD\ncomes from distribution uncertainty due to the mismatch between the training\nand test distributions, which makes the model can't confidently make\npredictions thus probably causing abnormal softmax scores. We propose a\nBayesian OOD detection framework to calibrate distribution uncertainty using\nMonte-Carlo Dropout. Our method is flexible and easily pluggable into existing\nsoftmax-based baselines and gains 33.33\\% OOD F1 improvements with increasing\nonly 0.41\\% inference time compared to MSP. Further analyses show the\neffectiveness of Bayesian learning for OOD detection.", "published": "2022-09-14 13:04:09", "link": "http://arxiv.org/abs/2209.06612v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPACE-2: Tree-Structured Semi-Supervised Contrastive Pre-training for\n  Task-Oriented Dialog Understanding", "abstract": "Pre-training methods with contrastive learning objectives have shown\nremarkable success in dialog understanding tasks. However, current contrastive\nlearning solely considers the self-augmented dialog samples as positive samples\nand treats all other dialog samples as negative ones, which enforces dissimilar\nrepresentations even for dialogs that are semantically related. In this paper,\nwe propose SPACE-2, a tree-structured pre-trained conversation model, which\nlearns dialog representations from limited labeled dialogs and large-scale\nunlabeled dialog corpora via semi-supervised contrastive pre-training.\nConcretely, we first define a general semantic tree structure (STS) to unify\nthe inconsistent annotation schema across different dialog datasets, so that\nthe rich structural information stored in all labeled data can be exploited.\nThen we propose a novel multi-view score function to increase the relevance of\nall possible dialogs that share similar STSs and only push away other\ncompletely different dialogs during supervised contrastive pre-training. To\nfully exploit unlabeled dialogs, a basic self-supervised contrastive loss is\nalso added to refine the learned representations. Experiments show that our\nmethod can achieve new state-of-the-art results on the DialoGLUE benchmark\nconsisting of seven datasets and four popular dialog understanding tasks. For\nreproducibility, we release the code and data at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/space-2.", "published": "2022-09-14 13:42:50", "link": "http://arxiv.org/abs/2209.06638v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoHS-CQG: Context and History Selection for Conversational Question\n  Generation", "abstract": "Conversational question generation (CQG) serves as a vital task for machines\nto assist humans, such as interactive reading comprehension, through\nconversations. Compared to traditional single-turn question generation (SQG),\nCQG is more challenging in the sense that the generated question is required\nnot only to be meaningful, but also to align with the occurred conversation\nhistory. While previous studies mainly focus on how to model the flow and\nalignment of the conversation, there has been no thorough study to date on\nwhich parts of the context and history are necessary for the model. We argue\nthat shortening the context and history is crucial as it can help the model to\noptimise more on the conversational alignment property. To this end, we propose\nCoHS-CQG, a two-stage CQG framework, which adopts a CoHS module to shorten the\ncontext and history of the input. In particular, CoHS selects contiguous\nsentences and history turns according to their relevance scores by a top-p\nstrategy. Our model achieves state-of-the-art performances on CoQA in both the\nanswer-aware and answer-unaware settings.", "published": "2022-09-14 13:58:52", "link": "http://arxiv.org/abs/2209.06652v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPACE-3: Unified Dialog Model Pre-training for Task-Oriented Dialog\n  Understanding and Generation", "abstract": "Recently, pre-training methods have shown remarkable success in task-oriented\ndialog (TOD) systems. However, most existing pre-trained models for TOD focus\non either dialog understanding or dialog generation, but not both. In this\npaper, we propose SPACE-3, a novel unified semi-supervised pre-trained\nconversation model learning from large-scale dialog corpora with limited\nannotations, which can be effectively fine-tuned on a wide range of downstream\ndialog tasks. Specifically, SPACE-3 consists of four successive components in a\nsingle transformer to maintain a task-flow in TOD systems: (i) a dialog\nencoding module to encode dialog history, (ii) a dialog understanding module to\nextract semantic vectors from either user queries or system responses, (iii) a\ndialog policy module to generate a policy vector that contains high-level\nsemantics of the response, and (iv) a dialog generation module to produce\nappropriate responses. We design a dedicated pre-training objective for each\ncomponent. Concretely, we pre-train the dialog encoding module with span mask\nlanguage modeling to learn contextualized dialog information. To capture the\nstructured dialog semantics, we pre-train the dialog understanding module via a\nnovel tree-induced semi-supervised contrastive learning objective with the help\nof extra dialog annotations. In addition, we pre-train the dialog policy module\nby minimizing the L2 distance between its output policy vector and the semantic\nvector of the response for policy optimization. Finally, the dialog generation\nmodel is pre-trained by language modeling. Results show that SPACE-3 achieves\nstate-of-the-art performance on eight downstream dialog benchmarks, including\nintent prediction, dialog state tracking, and end-to-end dialog modeling. We\nalso show that SPACE-3 has a stronger few-shot ability than existing models\nunder the low-resource setting.", "published": "2022-09-14 14:17:57", "link": "http://arxiv.org/abs/2209.06664v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UIT-ViCoV19QA: A Dataset for COVID-19 Community-based Question Answering\n  on Vietnamese Language", "abstract": "For the last two years, from 2020 to 2021, COVID-19 has broken disease\nprevention measures in many countries, including Vietnam, and negatively\nimpacted various aspects of human life and the social community. Besides, the\nmisleading information in the community and fake news about the pandemic are\nalso serious situations. Therefore, we present the first Vietnamese\ncommunity-based question answering dataset for developing question answering\nsystems for COVID-19 called UIT-ViCoV19QA. The dataset comprises 4,500\nquestion-answer pairs collected from trusted medical sources, with at least one\nanswer and at most four unique paraphrased answers per question. Along with the\ndataset, we set up various deep learning models as baseline to assess the\nquality of our dataset and initiate the benchmark results for further research\nthrough commonly used metrics such as BLEU, METEOR, and ROUGE-L. We also\nillustrate the positive effects of having multiple paraphrased answers\nexperimented on these models, especially on Transformer - a dominant\narchitecture in the field of study.", "published": "2022-09-14 14:24:23", "link": "http://arxiv.org/abs/2209.06668v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How people talk about each other: Modeling Generalized Intergroup Bias\n  and Emotion", "abstract": "Current studies of bias in NLP rely mainly on identifying (unwanted or\nnegative) bias towards a specific demographic group. While this has led to\nprogress recognizing and mitigating negative bias, and having a clear notion of\nthe targeted group is necessary, it is not always practical. In this work we\nextrapolate to a broader notion of bias, rooted in social science and\npsychology literature. We move towards predicting interpersonal group\nrelationship (IGR) - modeling the relationship between the speaker and the\ntarget in an utterance - using fine-grained interpersonal emotions as an\nanchor. We build and release a dataset of English tweets by US Congress members\nannotated for interpersonal emotion -- the first of its kind, and 'found\nsupervision' for IGR labels; our analyses show that subtle emotional signals\nare indicative of different biases. While humans can perform better than chance\nat identifying IGR given an utterance, we show that neural models perform much\nbetter; furthermore, a shared encoding between IGR and interpersonal perceived\nemotion enabled performance gains in both tasks. Data and code for this paper\nare available at https://github.com/venkatasg/interpersonal-bias", "published": "2022-09-14 14:46:55", "link": "http://arxiv.org/abs/2209.06687v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Fragility of Multi-Treebank Parsing Evaluation", "abstract": "Treebank selection for parsing evaluation and the spurious effects that might\narise from a biased choice have not been explored in detail. This paper studies\nhow evaluating on a single subset of treebanks can lead to weak conclusions.\nFirst, we take a few contrasting parsers, and run them on subsets of treebanks\nproposed in previous work, whose use was justified (or not) on criteria such as\ntypology or data scarcity. Second, we run a large-scale version of this\nexperiment, create vast amounts of random subsets of treebanks, and compare on\nthem many parsers whose scores are available. The results show substantial\nvariability across subsets and that although establishing guidelines for good\ntreebank selection is hard, it is possible to detect potentially harmful\nstrategies.", "published": "2022-09-14 15:07:29", "link": "http://arxiv.org/abs/2209.06699v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Inference Prompts for Zero-shot Emotion Classification\n  in Text across Corpora", "abstract": "Within textual emotion classification, the set of relevant labels depends on\nthe domain and application scenario and might not be known at the time of model\ndevelopment. This conflicts with the classical paradigm of supervised learning\nin which the labels need to be predefined. A solution to obtain a model with a\nflexible set of labels is to use the paradigm of zero-shot learning as a\nnatural language inference task, which in addition adds the advantage of not\nneeding any labeled training data. This raises the question how to prompt a\nnatural language inference model for zero-shot learning emotion classification.\nOptions for prompt formulations include the emotion name anger alone or the\nstatement \"This text expresses anger\". With this paper, we analyze how\nsensitive a natural language inference-based zero-shot-learning classifier is\nto such changes to the prompt under consideration of the corpus: How carefully\ndoes the prompt need to be selected? We perform experiments on an established\nset of emotion datasets presenting different language registers according to\ndifferent sources (tweets, events, blogs) with three natural language inference\nmodels and show that indeed the choice of a particular prompt formulation needs\nto fit to the corpus. We show that this challenge can be tackled with\ncombinations of multiple prompts. Such ensemble is more robust across corpora\nthan individual prompts and shows nearly the same performance as the individual\nbest prompt for a particular corpus.", "published": "2022-09-14 15:07:36", "link": "http://arxiv.org/abs/2209.06701v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Improving Health Literacy in Patient Education Materials with\n  Neural Machine Translation Models", "abstract": "Health literacy is the central focus of Healthy People 2030, the fifth\niteration of the U.S. national goals and objectives. People with low health\nliteracy usually have trouble understanding health information, following\npost-visit instructions, and using prescriptions, which results in worse health\noutcomes and serious health disparities. In this study, we propose to leverage\nnatural language processing techniques to improve health literacy in patient\neducation materials by automatically translating illiterate languages in a\ngiven sentence. We scraped patient education materials from four online health\ninformation websites: MedlinePlus.gov, Drugs.com, Mayoclinic.org and\nReddit.com. We trained and tested the state-of-the-art neural machine\ntranslation (NMT) models on a silver standard training dataset and a gold\nstandard testing dataset, respectively. The experimental results showed that\nthe Bidirectional Long Short-Term Memory (BiLSTM) NMT model outperformed\nBidirectional Encoder Representations from Transformers (BERT)-based NMT\nmodels. We also verified the effectiveness of NMT models in translating health\nilliterate languages by comparing the ratio of health illiterate language in\nthe sentence. The proposed NMT models were able to identify the correct\ncomplicated words and simplify into layman language while at the same time the\nmodels suffer from sentence completeness, fluency, readability, and have\ndifficulty in translating certain medical terms.", "published": "2022-09-14 15:30:54", "link": "http://arxiv.org/abs/2209.06723v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Fidelity Assessment for Strategy Training in Inpatient\n  Rehabilitation using Natural Language Processing", "abstract": "Strategy training is a multidisciplinary rehabilitation approach that teaches\nskills to reduce disability among those with cognitive impairments following a\nstroke. Strategy training has been shown in randomized, controlled clinical\ntrials to be a more feasible and efficacious intervention for promoting\nindependence than traditional rehabilitation approaches. A standardized\nfidelity assessment is used to measure adherence to treatment principles by\nexamining guided and directed verbal cues in video recordings of rehabilitation\nsessions. Although the fidelity assessment for detecting guided and directed\nverbal cues is valid and feasible for single-site studies, it can become labor\nintensive, time consuming, and expensive in large, multi-site pragmatic trials.\nTo address this challenge to widespread strategy training implementation, we\nleveraged natural language processing (NLP) techniques to automate the strategy\ntraining fidelity assessment, i.e., to automatically identify guided and\ndirected verbal cues from video recordings of rehabilitation sessions. We\ndeveloped a rule-based NLP algorithm, a long-short term memory (LSTM) model,\nand a bidirectional encoder representation from transformers (BERT) model for\nthis task. The best performance was achieved by the BERT model with a 0.8075\nF1-score. This BERT model was verified on an external validation dataset\ncollected from a separate major regional health system and achieved an F1 score\nof 0.8259, which shows that the BERT model generalizes well. The findings from\nthis study hold widespread promise in psychology and rehabilitation\nintervention research and practice.", "published": "2022-09-14 15:33:30", "link": "http://arxiv.org/abs/2209.06727v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LibertyMFD: A Lexicon to Assess the Moral Foundation of Liberty", "abstract": "Quantifying the moral narratives expressed in the user-generated text, news,\nor public discourses is fundamental for understanding individuals' concerns and\nviewpoints and preventing violent protests and social polarisation. The Moral\nFoundation Theory (MFT) was developed to operationalise morality in a\nfive-dimensional scale system. Recent developments of the theory urged for the\nintroduction of a new foundation, the Liberty Foundation. Being only recently\nadded to the theory, there are no available linguistic resources to assess\nwhether liberty is present in text corpora. Given its importance to current\nsocial issues such as the vaccination debate, we propose two data-driven\napproaches, deriving two candidate lexicons generated based on aligned\ndocuments from online news sources with different worldviews. After extensive\nexperimentation, we contribute to the research community a novel lexicon that\nassesses the liberty moral foundation in the way individuals with contrasting\nviewpoints express themselves through written text. The LibertyMFD dictionary\ncan be a valuable tool for policymakers to understand diverse viewpoints on\ncontroversial social issues such as vaccination, abortion, or even uprisings,\nas they happen and on a large scale.", "published": "2022-09-14 16:14:54", "link": "http://arxiv.org/abs/2209.06750v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Finetuning for Robust Continual Multilingual\n  Learning", "abstract": "We introduce and study the problem of Continual Multilingual Learning (CML)\nwhere a previously trained multilingual model is periodically updated using new\ndata arriving in stages. If the new data is present only in a subset of\nlanguages, we find that the resulting model shows improved performance only on\nthe languages included in the latest update (and a few closely related\nlanguages) while its performance on all the remaining languages degrade\nsignificantly. We address this challenge by proposing LAFT-URIEL, a\nparameter-efficient finetuning strategy which aims to increase the number of\nlanguages on which the model improves after an update, while reducing the\nmagnitude of loss in performance for the remaining languages. LAFT-URIEL uses\nlinguistic knowledge to balance overfitting and knowledge sharing across\nlanguages, allowing for an additional 25% of task languages to see an\nimprovement in performance after an update, while also reducing the average\nmagnitude of losses on the remaining languages by 78% relative.", "published": "2022-09-14 16:45:13", "link": "http://arxiv.org/abs/2209.06767v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Drawing Causal Inferences About Performance Effects in NLP", "abstract": "This article emphasizes that NLP as a science seeks to make inferences about\nthe performance effects that result from applying one method (compared to\nanother method) in the processing of natural language. Yet NLP research in\npractice usually does not achieve this goal: In NLP research articles,\ntypically only a few models are compared. Each model results from a specific\nprocedural pipeline (here named processing system) that is composed of a\nspecific collection of methods that are used in preprocessing, pretraining,\nhyperparameter tuning, and training on the target task. To make generalizing\ninferences about the performance effect that is caused by applying some method\nA vs. another method B, it is not sufficient to compare a few specific models\nthat are produced by a few specific (probably incomparable) processing systems.\nRather, the following procedure would allow drawing inferences about methods'\nperformance effects: (1) A population of processing systems that researchers\nseek to infer to has to be defined. (2) A random sample of processing systems\nfrom this population is drawn. (The drawn processing systems in the sample will\nvary with regard to the methods they apply along their procedural pipelines and\nalso will vary regarding the compositions of their training and test data sets\nused for training and evaluation.) (3) Each processing system is applied once\nwith method A and once with method B. (4) Based on the sample of applied\nprocessing systems, the expected generalization errors of method A and method B\nare approximated. (5) The difference between the expected generalization errors\nof method A and method B is the estimated average treatment effect due to\napplying method A compared to method B in the population of processing systems.", "published": "2022-09-14 17:18:21", "link": "http://arxiv.org/abs/2209.06790v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt-based Conservation Learning for Multi-hop Question Answering", "abstract": "Multi-hop question answering (QA) requires reasoning over multiple documents\nto answer a complex question and provide interpretable supporting evidence.\nHowever, providing supporting evidence is not enough to demonstrate that a\nmodel has performed the desired reasoning to reach the correct answer. Most\nexisting multi-hop QA methods fail to answer a large fraction of sub-questions,\neven if their parent questions are answered correctly. In this paper, we\npropose the Prompt-based Conservation Learning (PCL) framework for multi-hop\nQA, which acquires new knowledge from multi-hop QA tasks while conserving old\nknowledge learned on single-hop QA tasks, mitigating forgetting. Specifically,\nwe first train a model on existing single-hop QA tasks, and then freeze this\nmodel and expand it by allocating additional sub-networks for the multi-hop QA\ntask. Moreover, to condition pre-trained language models to stimulate the kind\nof reasoning required for specific multi-hop questions, we learn soft prompts\nfor the novel sub-networks to perform type-specific reasoning. Experimental\nresults on the HotpotQA benchmark show that PCL is competitive for multi-hop QA\nand retains good performance on the corresponding single-hop sub-questions,\ndemonstrating the efficacy of PCL in mitigating knowledge loss by forgetting.", "published": "2022-09-14 20:50:46", "link": "http://arxiv.org/abs/2209.06923v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Product Classification with Instance-Dependent Noise", "abstract": "Noisy labels in large E-commerce product data (i.e., product items are placed\ninto incorrect categories) are a critical issue for product categorization task\nbecause they are unavoidable, non-trivial to remove and degrade prediction\nperformance significantly. Training a product title classification model which\nis robust to noisy labels in the data is very important to make product\nclassification applications more practical. In this paper, we study the impact\nof instance-dependent noise to performance of product title classification by\ncomparing our data denoising algorithm and different noise-resistance training\nalgorithms which were designed to prevent a classifier model from over-fitting\nto noise. We develop a simple yet effective Deep Neural Network for product\ntitle classification to use as a base classifier. Along with recent methods of\nstimulating instance-dependent noise, we propose a novel noise stimulation\nalgorithm based on product title similarity. Our experiments cover multiple\ndatasets, various noise methods and different training solutions. Results\nuncover the limit of classification task when noise rate is not negligible and\ndata distribution is highly skewed.", "published": "2022-09-14 21:45:14", "link": "http://arxiv.org/abs/2209.06946v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classical Sequence Match is a Competitive Few-Shot One-Class Learner", "abstract": "Nowadays, transformer-based models gradually become the default choice for\nartificial intelligence pioneers. The models also show superiority even in the\nfew-shot scenarios. In this paper, we revisit the classical methods and propose\na new few-shot alternative. Specifically, we investigate the few-shot one-class\nproblem, which actually takes a known sample as a reference to detect whether\nan unknown instance belongs to the same class. This problem can be studied from\nthe perspective of sequence match. It is shown that with meta-learning, the\nclassical sequence match method, i.e. Compare-Aggregate, significantly\noutperforms transformer ones. The classical approach requires much less\ntraining cost. Furthermore, we perform an empirical comparison between two\nkinds of sequence match approaches under simple fine-tuning and meta-learning.\nMeta-learning causes the transformer models' features to have high-correlation\ndimensions. The reason is closely related to the number of layers and heads of\ntransformer models. Experimental codes and data are available at\nhttps://github.com/hmt2014/FewOne", "published": "2022-09-14 03:21:47", "link": "http://arxiv.org/abs/2209.06394v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "COMMA: Modeling Relationship among Motivations, Emotions and Actions in\n  Language-based Human Activities", "abstract": "Motivations, emotions, and actions are inter-related essential factors in\nhuman activities. While motivations and emotions have long been considered at\nthe core of exploring how people take actions in human activities, there has\nbeen relatively little research supporting analyzing the relationship between\nhuman mental states and actions. We present the first study that investigates\nthe viability of modeling motivations, emotions, and actions in language-based\nhuman activities, named COMMA (Cognitive Framework of Human Activities). Guided\nby COMMA, we define three natural language processing tasks (emotion\nunderstanding, motivation understanding and conditioned action generation), and\nbuild a challenging dataset Hail through automatically extracting samples from\nStory Commonsense. Experimental results on NLP applications prove the\neffectiveness of modeling the relationship. Furthermore, our models inspired by\nCOMMA can better reveal the essential relationship among motivations, emotions\nand actions than existing methods.", "published": "2022-09-14 07:54:20", "link": "http://arxiv.org/abs/2209.06470v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BERT-based Ensemble Approaches for Hate Speech Detection", "abstract": "With the freedom of communication provided in online social media, hate\nspeech has increasingly generated. This leads to cyber conflicts affecting\nsocial life at the individual and national levels. As a result, hateful content\nclassification is becoming increasingly demanded for filtering hate content\nbefore being sent to the social networks. This paper focuses on classifying\nhate speech in social media using multiple deep models that are implemented by\nintegrating recent transformer-based language models such as BERT, and neural\nnetworks. To improve the classification performances, we evaluated with several\nensemble techniques, including soft voting, maximum value, hard voting and\nstacking. We used three publicly available Twitter datasets (Davidson,\nHatEval2019, OLID) that are generated to identify offensive languages. We fused\nall these datasets to generate a single dataset (DHO dataset), which is more\nbalanced across different labels, to perform multi-label classification. Our\nexperiments have been held on Davidson dataset and the DHO corpora. The later\ngave the best overall results, especially F1 macro score, even it required more\nresources (time execution and memory). The experiments have shown good results\nespecially the ensemble models, where stacking gave F1 score of 97% on Davidson\ndataset and aggregating ensembles 77% on the DHO dataset.", "published": "2022-09-14 09:08:24", "link": "http://arxiv.org/abs/2209.06505v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Preregistered protocol for: Articulatory changes in speech following\n  treatment for oral or oropharyngeal cancer: a systematic review", "abstract": "This document outlines a PROSPERO pre-registered protocol for a systematic\nreview regarding articulatory changes in speech following oral or orophayrngeal\ncancer treatment. Treatment of tumours in the oral cavity may result in\nphysiological changes that could lead to articulatory difficulties. The tongue\nbecomes less mobile due to scar tissue and/or potential (postoperative)\nradiation therapy. Moreover, tissue loss may create a bypass for airflow or\nlimit constriction possibilities. In order to gain a better understanding of\nthe nature of the speech problems, information regarding the movement of the\narticulators is needed since perceptual or acoustic information provide only\nindirect evidence of articulatory changes. Therefore, this systematic review\nwill review studies that directly measured the articulatory movements of the\ntongue, jaw, and lips following treatment for oral or oropharyngeal cancer.", "published": "2022-09-14 09:49:19", "link": "http://arxiv.org/abs/2209.06521v1", "categories": ["physics.med-ph", "cs.CL"], "primary_category": "physics.med-ph"}
{"title": "Integrating Form and Meaning: A Multi-Task Learning Model for Acoustic\n  Word Embeddings", "abstract": "Models of acoustic word embeddings (AWEs) learn to map variable-length spoken\nword segments onto fixed-dimensionality vector representations such that\ndifferent acoustic exemplars of the same word are projected nearby in the\nembedding space. In addition to their speech technology applications, AWE\nmodels have been shown to predict human performance on a variety of auditory\nlexical processing tasks. Current AWE models are based on neural networks and\ntrained in a bottom-up approach that integrates acoustic cues to build up a\nword representation given an acoustic or symbolic supervision signal.\nTherefore, these models do not leverage or capture high-level lexical knowledge\nduring the learning process. In this paper, we propose a multi-task learning\nmodel that incorporates top-down lexical knowledge into the training procedure\nof AWEs. Our model learns a mapping between the acoustic input and a lexical\nrepresentation that encodes high-level information such as word semantics in\naddition to bottom-up form-based supervision. We experiment with three\nlanguages and demonstrate that incorporating lexical knowledge improves the\nembedding space discriminability and encourages the model to better separate\nlexical categories.", "published": "2022-09-14 13:33:04", "link": "http://arxiv.org/abs/2209.06633v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "WildQA: In-the-Wild Video Question Answering", "abstract": "Existing video understanding datasets mostly focus on human interactions,\nwith little attention being paid to the \"in the wild\" settings, where the\nvideos are recorded outdoors. We propose WILDQA, a video understanding dataset\nof videos recorded in outside settings. In addition to video question answering\n(Video QA), we also introduce the new task of identifying visual support for a\ngiven question and answer (Video Evidence Selection). Through evaluations using\na wide range of baseline models, we show that WILDQA poses new challenges to\nthe vision and language research communities. The dataset is available at\nhttps://lit.eecs.umich.edu/wildqa/.", "published": "2022-09-14 13:54:07", "link": "http://arxiv.org/abs/2209.06650v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "On Language Clustering: A Non-parametric Statistical Approach", "abstract": "Any approach aimed at pasteurizing and quantifying a particular phenomenon\nmust include the use of robust statistical methodologies for data analysis.\nWith this in mind, the purpose of this study is to present statistical\napproaches that may be employed in nonparametric nonhomogeneous data\nframeworks, as well as to examine their application in the field of natural\nlanguage processing and language clustering. Furthermore, this paper discusses\nthe many uses of nonparametric approaches in linguistic data mining and\nprocessing. The data depth idea allows for the centre-outward ordering of\npoints in any dimension, resulting in a new nonparametric multivariate\nstatistical analysis that does not require any distributional assumptions. The\nconcept of hierarchy is used in historical language categorisation and\nstructuring, and it aims to organise and cluster languages into subfamilies\nusing the same premise. In this regard, the current study presents a novel\napproach to language family structuring based on non-parametric approaches\nproduced from a typological structure of words in various languages, which is\nthen converted into a Cartesian framework using MDS. This\nstatistical-depth-based architecture allows for the use of data-depth-based\nmethodologies for robust outlier detection, which is extremely useful in\nunderstanding the categorization of diverse borderline languages and allows for\nthe re-evaluation of existing classification systems. Other depth-based\napproaches are also applied to processes such as unsupervised and supervised\nclustering. This paper therefore provides an overview of procedures that can be\napplied to nonhomogeneous language classification systems in a nonparametric\nframework.", "published": "2022-09-14 15:27:41", "link": "http://arxiv.org/abs/2209.06720v1", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "vec2text with Round-Trip Translations", "abstract": "We investigate models that can generate arbitrary natural language text (e.g.\nall English sentences) from a bounded, convex and well-behaved control space.\nWe call them universal vec2text models. Such models would allow making semantic\ndecisions in the vector space (e.g. via reinforcement learning) while the\nnatural language generation is handled by the vec2text model. We propose four\ndesired properties: universality, diversity, fluency, and semantic structure,\nthat such vec2text models should possess and we provide quantitative and\nqualitative methods to assess them. We implement a vec2text model by adding a\nbottleneck to a 250M parameters Transformer model and training it with an\nauto-encoding objective on 400M sentences (10B tokens) extracted from a massive\nweb corpus. We propose a simple data augmentation technique based on round-trip\ntranslations and show in extensive experiments that the resulting vec2text\nmodel surprisingly leads to vector spaces that fulfill our four desired\nproperties and that this model strongly outperforms both standard and denoising\nauto-encoders.", "published": "2022-09-14 17:20:18", "link": "http://arxiv.org/abs/2209.06792v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model", "abstract": "Effective scaling and a flexible task interface enable large language models\nto excel at many tasks. We present PaLI (Pathways Language and Image model), a\nmodel that extends this approach to the joint modeling of language and vision.\nPaLI generates text based on visual and textual inputs, and with this interface\nperforms many vision, language, and multimodal tasks, in many languages. To\ntrain PaLI, we make use of large pre-trained encoder-decoder language models\nand Vision Transformers (ViTs). This allows us to capitalize on their existing\ncapabilities and leverage the substantial cost of training them. We find that\njoint scaling of the vision and language components is important. Since\nexisting Transformers for language are much larger than their vision\ncounterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the\nbenefits from even larger-capacity vision models. To train PaLI, we create a\nlarge multilingual mix of pretraining tasks, based on a new image-text training\nset containing 10B images and texts in over 100 languages. PaLI achieves\nstate-of-the-art in multiple vision and language tasks (such as captioning,\nvisual question-answering, scene-text understanding), while retaining a simple,\nmodular, and scalable design.", "published": "2022-09-14 17:24:07", "link": "http://arxiv.org/abs/2209.06794v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "On the Intersection of Context-Free and Regular Languages", "abstract": "The Bar-Hillel construction is a classic result in formal language theory. It\nshows, by a simple construction, that the intersection of a context-free\nlanguage and a regular language is itself context-free. In the construction,\nthe regular language is specified by a finite-state automaton. However, neither\nthe original construction (Bar-Hillel et al., 1961) nor its weighted extension\n(Nederhof and Satta, 2003) can handle finite-state automata with\n$\\varepsilon$-arcs. While it is possible to remove $\\varepsilon$-arcs from a\nfinite-state automaton efficiently without modifying the language, such an\noperation modifies the automaton's set of paths. We give a construction that\ngeneralizes the Bar-Hillel in the case where the desired automaton has\n$\\varepsilon$-arcs, and further prove that our generalized construction leads\nto a grammar that encodes the structure of both the input automaton and grammar\nwhile retaining the asymptotic size of the original construction.", "published": "2022-09-14 17:49:06", "link": "http://arxiv.org/abs/2209.06809v2", "categories": ["cs.FL", "cs.CL"], "primary_category": "cs.FL"}
{"title": "Out of One, Many: Using Language Models to Simulate Human Samples", "abstract": "We propose and explore the possibility that language models can be studied as\neffective proxies for specific human sub-populations in social science\nresearch. Practical and research applications of artificial intelligence tools\nhave sometimes been limited by problematic biases (such as racism or sexism),\nwhich are often treated as uniform properties of the models. We show that the\n\"algorithmic bias\" within one such tool -- the GPT-3 language model -- is\ninstead both fine-grained and demographically correlated, meaning that proper\nconditioning will cause it to accurately emulate response distributions from a\nwide variety of human subgroups. We term this property \"algorithmic fidelity\"\nand explore its extent in GPT-3. We create \"silicon samples\" by conditioning\nthe model on thousands of socio-demographic backstories from real human\nparticipants in multiple large surveys conducted in the United States. We then\ncompare the silicon and human samples to demonstrate that the information\ncontained in GPT-3 goes far beyond surface similarity. It is nuanced,\nmultifaceted, and reflects the complex interplay between ideas, attitudes, and\nsocio-cultural context that characterize human attitudes. We suggest that\nlanguage models with sufficient algorithmic fidelity thus constitute a novel\nand powerful tool to advance understanding of humans and society across a\nvariety of disciplines.", "published": "2022-09-14 19:53:32", "link": "http://arxiv.org/abs/2209.06899v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ImageArg: A Multi-modal Tweet Dataset for Image Persuasiveness Mining", "abstract": "The growing interest in developing corpora of persuasive texts has promoted\napplications in automated systems, e.g., debating and essay scoring systems;\nhowever, there is little prior work mining image persuasiveness from an\nargumentative perspective. To expand persuasiveness mining into a multi-modal\nrealm, we present a multi-modal dataset, ImageArg, consisting of annotations of\nimage persuasiveness in tweets. The annotations are based on a persuasion\ntaxonomy we developed to explore image functionalities and the means of\npersuasion. We benchmark image persuasiveness tasks on ImageArg using\nwidely-used multi-modal learning methods. The experimental results show that\nour dataset offers a useful resource for this rich and challenging topic, and\nthere is ample room for modeling improvement.", "published": "2022-09-14 05:03:10", "link": "http://arxiv.org/abs/2209.06416v1", "categories": ["cs.CL", "cs.AI", "cs.MM"], "primary_category": "cs.CL"}
{"title": "ConvNeXt Based Neural Network for Audio Anti-Spoofing", "abstract": "With the rapid development of speech conversion and speech synthesis\nalgorithms, automatic speaker verification (ASV) systems are vulnerable to\nspoofing attacks. In recent years, researchers had proposed a number of\nanti-spoofing methods based on hand-crafted features. However, using\nhand-crafted features rather than raw waveform will lose implicit information\nfor anti-spoofing. Inspired by the promising performance of ConvNeXt in image\nclassification tasks, we revise the ConvNeXt network architecture and propose a\nlightweight end-to-end anti-spoofing model. By integrating with the channel\nattention block and using the focal loss function, the proposed model can focus\non the most informative sub-bands of speech representations and the difficult\nsamples that are hard to classify. Experiments show that our proposed system\ncould achieve an equal error rate of 0.64% and min-tDCF of 0.0187 for the\nASVSpoof 2019 LA evaluation dataset, which outperforms the state-of-the-art\nsystems.", "published": "2022-09-14 05:53:37", "link": "http://arxiv.org/abs/2209.06434v5", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ParaTTS: Learning Linguistic and Prosodic Cross-sentence Information in\n  Paragraph-based TTS", "abstract": "Recent advancements in neural end-to-end TTS models have shown high-quality,\nnatural synthesized speech in a conventional sentence-based TTS. However, it is\nstill challenging to reproduce similar high quality when a whole paragraph is\nconsidered in TTS, where a large amount of contextual information needs to be\nconsidered in building a paragraph-based TTS model. To alleviate the difficulty\nin training, we propose to model linguistic and prosodic information by\nconsidering cross-sentence, embedded structure in training. Three sub-modules,\nincluding linguistics-aware, prosody-aware and sentence-position networks, are\ntrained together with a modified Tacotron2. Specifically, to learn the\ninformation embedded in a paragraph and the relations among the corresponding\ncomponent sentences, we utilize linguistics-aware and prosody-aware networks.\nThe information in a paragraph is captured by encoders and the inter-sentence\ninformation in a paragraph is learned with multi-head attention mechanisms. The\nrelative sentence position in a paragraph is explicitly exploited by a\nsentence-position network. Trained on a storytelling audio-book corpus (4.08\nhours), recorded by a female Mandarin Chinese speaker, the proposed TTS model\ndemonstrates that it can produce rather natural and good-quality speech\nparagraph-wise. The cross-sentence contextual information, such as break and\nprosodic variations between consecutive sentences, can be better predicted and\nrendered than the sentence-based model. Tested on paragraph texts, of which the\nlengths are similar to, longer than, or much longer than the typical paragraph\nlength of the training data, the TTS speech produced by the new model is\nconsistently preferred over the sentence-based model in subjective tests and\nconfirmed in objective measures.", "published": "2022-09-14 08:34:16", "link": "http://arxiv.org/abs/2209.06484v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Order-Disorder: Imitation Adversarial Attacks for Black-box Neural\n  Ranking Models", "abstract": "Neural text ranking models have witnessed significant advancement and are\nincreasingly being deployed in practice. Unfortunately, they also inherit\nadversarial vulnerabilities of general neural models, which have been detected\nbut remain underexplored by prior studies. Moreover, the inherit adversarial\nvulnerabilities might be leveraged by blackhat SEO to defeat better-protected\nsearch engines. In this study, we propose an imitation adversarial attack on\nblack-box neural passage ranking models. We first show that the target passage\nranking model can be transparentized and imitated by enumerating critical\nqueries/candidates and then train a ranking imitation model. Leveraging the\nranking imitation model, we can elaborately manipulate the ranking results and\ntransfer the manipulation attack to the target ranking model. For this purpose,\nwe propose an innovative gradient-based attack method, empowered by the\npairwise objective function, to generate adversarial triggers, which causes\npremeditated disorderliness with very few tokens. To equip the trigger\ncamouflages, we add the next sentence prediction loss and the language model\nfluency constraint to the objective function. Experimental results on passage\nranking demonstrate the effectiveness of the ranking imitation attack model and\nadversarial triggers against various SOTA neural ranking models. Furthermore,\nvarious mitigation analyses and human evaluation show the effectiveness of\ncamouflages when facing potential mitigation approaches. To motivate other\nscholars to further investigate this novel and important problem, we make the\nexperiment data and code publicly available.", "published": "2022-09-14 09:10:07", "link": "http://arxiv.org/abs/2209.06506v2", "categories": ["cs.IR", "cs.CL", "cs.CR"], "primary_category": "cs.IR"}
{"title": "Pre-training for Information Retrieval: Are Hyperlinks Fully Explored?", "abstract": "Recent years have witnessed great progress on applying pre-trained language\nmodels, e.g., BERT, to information retrieval (IR) tasks. Hyperlinks, which are\ncommonly used in Web pages, have been leveraged for designing pre-training\nobjectives. For example, anchor texts of the hyperlinks have been used for\nsimulating queries, thus constructing tremendous query-document pairs for\npre-training. However, as a bridge across two web pages, the potential of\nhyperlinks has not been fully explored. In this work, we focus on modeling the\nrelationship between two documents that are connected by hyperlinks and\ndesigning a new pre-training objective for ad-hoc retrieval. Specifically, we\ncategorize the relationships between documents into four groups: no link,\nunidirectional link, symmetric link, and the most relevant symmetric link. By\ncomparing two documents sampled from adjacent groups, the model can gradually\nimprove its capability of capturing matching signals. We propose a progressive\nhyperlink predication ({PHP}) framework to explore the utilization of\nhyperlinks in pre-training. Experimental results on two large-scale ad-hoc\nretrieval datasets and six question-answering datasets demonstrate its\nsuperiority over existing pre-training methods.", "published": "2022-09-14 12:03:31", "link": "http://arxiv.org/abs/2209.06583v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "On the State of the Art in Authorship Attribution and Authorship\n  Verification", "abstract": "Despite decades of research on authorship attribution (AA) and authorship\nverification (AV), inconsistent dataset splits/filtering and mismatched\nevaluation methods make it difficult to assess the state of the art. In this\npaper, we present a survey of the fields, resolve points of confusion,\nintroduce Valla that standardizes and benchmarks AA/AV datasets and metrics,\nprovide a large-scale empirical evaluation, and provide apples-to-apples\ncomparisons between existing methods. We evaluate eight promising methods on\nfifteen datasets (including distribution-shifted challenge sets) and introduce\na new large-scale dataset based on texts archived by Project Gutenberg.\nSurprisingly, we find that a traditional Ngram-based model performs best on 5\n(of 7) AA tasks, achieving an average macro-accuracy of $76.50\\%$ (compared to\n$66.71\\%$ for a BERT-based model). However, on the two AA datasets with the\ngreatest number of words per author, as well as on the AV datasets, BERT-based\nmodels perform best. While AV methods are easily applied to AA, they are seldom\nincluded as baselines in AA papers. We show that through the application of\nhard-negative mining, AV methods are competitive alternatives to AA methods.\nValla and all experiment code can be found here:\nhttps://github.com/JacobTyo/Valla", "published": "2022-09-14 18:32:26", "link": "http://arxiv.org/abs/2209.06869v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ESSumm: Extractive Speech Summarization from Untranscribed Meeting", "abstract": "In this paper, we propose a novel architecture for direct extractive\nspeech-to-speech summarization, ESSumm, which is an unsupervised model without\ndependence on intermediate transcribed text. Different from previous methods\nwith text presentation, we are aimed at generating a summary directly from\nspeech without transcription. First, a set of smaller speech segments are\nextracted based on speech signal's acoustic features. For each candidate speech\nsegment, a distance-based summarization confidence score is designed for latent\nspeech representation measure. Specifically, we leverage the off-the-shelf\nself-supervised convolutional neural network to extract the deep speech\nfeatures from raw audio. Our approach automatically predicts the optimal\nsequence of speech segments that capture the key information with a target\nsummary length. Extensive results on two well-known meeting datasets (AMI and\nICSI corpora) show the effectiveness of our direct speech-based method to\nimprove the summarization quality with untranscribed data. We also observe that\nour unsupervised speech-based method even performs on par with recent\ntranscript-based summarization approaches, where extra speech recognition is\nrequired.", "published": "2022-09-14 20:13:15", "link": "http://arxiv.org/abs/2209.06913v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PainPoints: A Framework for Language-based Detection of Chronic Pain and\n  Expert-Collaborative Text-Summarization", "abstract": "Chronic pain is a pervasive disorder which is often very disabling and is\nassociated with comorbidities such as depression and anxiety. Neuropathic Pain\n(NP) is a common sub-type which is often caused due to nerve damage and has a\nknown pathophysiology. Another common sub-type is Fibromyalgia (FM) which is\ndescribed as musculoskeletal, diffuse pain that is widespread through the body.\nThe pathophysiology of FM is poorly understood, making it very hard to\ndiagnose. Standard medications and treatments for FM and NP differ from one\nanother and if misdiagnosed it can cause an increase in symptom severity. To\novercome this difficulty, we propose a novel framework, PainPoints, which\naccurately detects the sub-type of pain and generates clinical notes via\nsummarizing the patient interviews. Specifically, PainPoints makes use of large\nlanguage models to perform sentence-level classification of the text obtained\nfrom interviews of FM and NP patients with a reliable AUC of 0.83. Using a\nsufficiency-based interpretability approach, we explain how the fine-tuned\nmodel accurately picks up on the nuances that patients use to describe their\npain. Finally, we generate summaries of these interviews via expert\ninterventions by introducing a novel facet-based approach. PainPoints thus\nenables practitioners to add/drop facets and generate a custom summary based on\nthe notion of \"facet-coverage\" which is also introduced in this work.", "published": "2022-09-14 06:08:13", "link": "http://arxiv.org/abs/2209.09814v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Decoupled Pronunciation and Prosody Modeling in Meta-Learning-Based\n  Multilingual Speech Synthesis", "abstract": "This paper presents a method of decoupled pronunciation and prosody modeling\nto improve the performance of meta-learning-based multilingual speech\nsynthesis. The baseline meta-learning synthesis method adopts a single text\nencoder with a parameter generator conditioned on language embeddings and a\nsingle decoder to predict mel-spectrograms for all languages. In contrast, our\nproposed method designs a two-stream model structure that contains two encoders\nand two decoders for pronunciation and prosody modeling, respectively,\nconsidering that the pronunciation knowledge and the prosody knowledge should\nbe shared in different ways among languages. In our experiments, our proposed\nmethod effectively improved the intelligibility and naturalness of multilingual\nspeech synthesis comparing with the baseline meta-learning synthesis method.", "published": "2022-09-14 17:17:49", "link": "http://arxiv.org/abs/2209.06789v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "I2CR: Improving Noise Robustness on Keyword Spotting Using Inter-Intra\n  Contrastive Regularization", "abstract": "Noise robustness in keyword spotting remains a challenge as many models fail\nto overcome the heavy influence of noises, causing the deterioration of the\nquality of feature embeddings. We proposed a contrastive regularization method\ncalled Inter-Intra Contrastive Regularization (I2CR) to improve the feature\nrepresentations by guiding the model to learn the fundamental speech\ninformation specific to the cluster. This involves maximizing the similarity\nacross Intra and Inter samples of the same class. As a result, it pulls the\ninstances closer to more generalized representations that form more prominent\nclusters and reduces the adverse impact of noises. We show that our method\nprovides consistent improvements in accuracy over different backbone model\narchitectures under different noise environments. We also demonstrate that our\nproposed framework has improved the accuracy of unseen out-of-domain noises and\nunseen variant noise SNRs. This indicates the significance of our work with the\noverall refinement in noise robustness.", "published": "2022-09-14 00:52:19", "link": "http://arxiv.org/abs/2209.06360v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Universally-Deployable ASR Frontend for Joint Acoustic Echo\n  Cancellation, Speech Enhancement, and Voice Separation", "abstract": "Recent work has shown that it is possible to train a single model to perform\njoint acoustic echo cancellation (AEC), speech enhancement, and voice\nseparation, thereby serving as a unified frontend for robust automatic speech\nrecognition (ASR). The joint model uses contextual information, such as a\nreference of the playback audio, noise context, and speaker embedding. In this\nwork, we propose a number of novel improvements to such a model. First, we\nimprove the architecture of the Cross-Attention Conformer that is used to\ningest noise context into the model. Second, we generalize the model to be able\nto handle varying lengths of noise context. Third, we propose Signal Dropout, a\nnovel strategy that models missing contextual information. In the absence of\none or more signals, the proposed model performs nearly as well as\ntask-specific models trained without these signals; and when such signals are\npresent, our system compares well against systems that require all context\nsignals. Over the baseline, the final model retains a relative word error rate\nreduction of 25.0% on background speech when speaker embedding is absent, and\n61.2% on AEC when device playback is absent.", "published": "2022-09-14 04:34:07", "link": "http://arxiv.org/abs/2209.06410v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Using Rater and System Metadata to Explain Variance in the VoiceMOS\n  Challenge 2022 Dataset", "abstract": "Non-reference speech quality models are important for a growing number of\napplications. The VoiceMOS 2022 challenge provided a dataset of synthetic voice\nconversion and text-to-speech samples with subjective labels. This study looks\nat the amount of variance that can be explained in subjective ratings of speech\nquality from metadata and the distribution imbalances of the dataset. Speech\nquality models were constructed using wav2vec 2.0 with additional metadata\nfeatures that included rater groups and system identifiers and obtained\ncompetitive metrics including a Spearman rank correlation coefficient (SRCC) of\n0.934 and MSE of 0.088 at the system-level, and 0.877 and 0.198 at the\nutterance-level. Using data and metadata that the test restricted or blinded\nfurther improved the metrics. A metadata analysis showed that the system-level\nmetrics do not represent the model's system-level prediction as a result of the\nwide variation in the number of utterances used for each system on the\nvalidation and test datasets. We conclude that, in general, conditions should\nhave enough utterances in the test set to bound the sample mean error, and be\nrelatively balanced in utterance count between systems, otherwise the\nutterance-level metrics may be more reliable and interpretable.", "published": "2022-09-14 00:45:49", "link": "http://arxiv.org/abs/2209.06358v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CCOM-HuQin: an Annotated Multimodal Chinese Fiddle Performance Dataset", "abstract": "HuQin is a family of traditional Chinese bowed string instruments. Playing\ntechniques(PTs) embodied in various playing styles add abundant emotional\ncoloring and aesthetic feelings to HuQin performance. The complex applied\ntechniques make HuQin music a challenging source for fundamental MIR tasks such\nas pitch analysis, transcription and score-audio alignment. In this paper, we\npresent a multimodal performance dataset of HuQin music that contains\naudio-visual recordings of 11,992 single PT clips and 57 annotated musical\npieces of classical excerpts. We systematically describe the HuQin PT taxonomy\nbased on musicological theory and practical use cases. Then we introduce the\ndataset creation methodology and highlight the annotation principles featuring\nPTs. We analyze the statistics in different aspects to demonstrate the variety\nof PTs played in HuQin subcategories and perform preliminary experiments to\nshow the potential applications of the dataset in various MIR tasks and\ncross-cultural music studies. Finally, we propose future work to be extended on\nthe dataset.", "published": "2022-09-14 08:51:15", "link": "http://arxiv.org/abs/2209.06496v2", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
