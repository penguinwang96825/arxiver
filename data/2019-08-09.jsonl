{"title": "UdS Submission for the WMT 19 Automatic Post-Editing Task", "abstract": "In this paper, we describe our submission to the English-German APE shared\ntask at WMT 2019. We utilize and adapt an NMT architecture originally developed\nfor exploiting context information to APE, implement this in our own\ntransformer model and explore joint training of the APE task with a de-noising\nencoder.", "published": "2019-08-09 10:40:52", "link": "http://arxiv.org/abs/1908.03402v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Artificially Evolved Chunks for Morphosyntactic Analysis", "abstract": "We introduce a language-agnostic evolutionary technique for automatically\nextracting chunks from dependency treebanks. We evaluate these chunks on a\nnumber of morphosyntactic tasks, namely POS tagging, morphological feature\ntagging, and dependency parsing. We test the utility of these chunks in a host\nof different ways. We first learn chunking as one task in a shared multi-task\nframework together with POS and morphological feature tagging. The predictions\nfrom this network are then used as input to augment sequence-labelling\ndependency parsing. Finally, we investigate the impact chunks have on\ndependency parsing in a multi-task framework. Our results from these analyses\nshow that these chunks improve performance at different levels of syntactic\nabstraction on English UD treebanks and a small, diverse subset of non-English\nUD treebanks.", "published": "2019-08-09 14:43:36", "link": "http://arxiv.org/abs/1908.03480v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Information Extraction Patterns from Overlapping and Variable\n  Length Annotations using Sequence Alignment", "abstract": "Sequence alignments are used to capture patterns composed of elements\nrepresenting multiple conceptual levels through the alignment of sequences that\ncontain overlapping and variable length annotations. The alignments also\ndetermine the proper context window of words and phrases that most directly\nimpact the meaning of a given target within a sentence, eliminating the need to\npredefine a fixed context window of words surrounding the targets. We evaluated\nthe system using the CoNLL-2003 named entity recognition (NER) task.", "published": "2019-08-09 18:51:30", "link": "http://arxiv.org/abs/1908.03594v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Semantic Role Knowledge for Relevance Ranking of Key Phrases in\n  Documents: An Unsupervised Approach", "abstract": "In this paper, we investigate the integration of sentence position and\nsemantic role of words in a PageRank system to build a key phrase ranking\nmethod. We present the evaluation results of our approach on three scientific\narticles. We show that semantic role information, when integrated with a\nPageRank system, can become a new lexical feature. Our approach had an overall\nimprovement on all the data sets over the state-of-art baseline approaches.", "published": "2019-08-09 04:44:12", "link": "http://arxiv.org/abs/1908.03313v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Exploiting Cross-Lingual Speaker and Phonetic Diversity for Unsupervised\n  Subword Modeling", "abstract": "This research addresses the problem of acoustic modeling of low-resource\nlanguages for which transcribed training data is absent. The goal is to learn\nrobust frame-level feature representations that can be used to identify and\ndistinguish subword-level speech units. The proposed feature representations\ncomprise various types of multilingual bottleneck features (BNFs) that are\nobtained via multi-task learning of deep neural networks (MTL-DNN). One of the\nkey problems is how to acquire high-quality frame labels for untranscribed\ntraining data to facilitate supervised DNN training. It is shown that learning\nof robust BNF representations can be achieved by effectively leveraging\ntranscribed speech data and well-trained automatic speech recognition (ASR)\nsystems from one or more out-of-domain (resource-rich) languages. Out-of-domain\nASR systems can be applied to perform speaker adaptation with untranscribed\ntraining data of the target language, and to decode the training speech into\nframe-level labels for DNN training. It is also found that better frame labels\ncan be generated by considering temporal dependency in speech when performing\nframe clustering. The proposed methods of feature learning are evaluated on the\nstandard task of unsupervised subword modeling in Track 1 of the ZeroSpeech\n2017 Challenge. The best performance achieved by our system is $9.7\\%$ in terms\nof across-speaker triphone minimal-pair ABX error rate, which is comparable to\nthe best systems reported recently. Lastly, our investigation reveals that the\ncloseness between target languages and out-of-domain languages and the amount\nof available training data for individual target languages could have\nsignificant impact on the goodness of learned features.", "published": "2019-08-09 16:57:04", "link": "http://arxiv.org/abs/1908.03538v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "A Generate-Validate Approach to Answering Questions about Qualitative\n  Relationships", "abstract": "Qualitative relationships describe how increasing or decreasing one property\n(e.g. altitude) affects another (e.g. temperature). They are an important\naspect of natural language question answering and are crucial for building\nchatbots or voice agents where one may enquire about qualitative relationships.\nRecently a dataset about question answering involving qualitative relationships\nhas been proposed, and a few approaches to answer such questions have been\nexplored, in the heart of which lies a semantic parser that converts the\nnatural language input to a suitable logical form. A problem with existing\nsemantic parsers is that they try to directly convert the input sentences to a\nlogical form. Since the output language varies with each application, it forces\nthe semantic parser to learn almost everything from scratch. In this paper, we\nshow that instead of using a semantic parser to produce the logical form, if we\napply the generate-validate framework i.e. generate a natural language\ndescription of the logical form and validate if the natural language\ndescription is followed from the input text, we get a better scope for transfer\nlearning and our method outperforms the state-of-the-art by a large margin of\n7.93%.", "published": "2019-08-09 22:06:06", "link": "http://arxiv.org/abs/1908.03645v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TEQUILA: Temporal Question Answering over Knowledge Bases", "abstract": "Question answering over knowledge bases (KB-QA) poses challenges in handling\ncomplex questions that need to be decomposed into sub-questions. An important\ncase, addressed here, is that of temporal questions, where cues for temporal\nrelations need to be discovered and handled. We present TEQUILA, an enabler\nmethod for temporal QA that can run on top of any KB-QA engine. TEQUILA has\nfour stages. It detects if a question has temporal intent. It decomposes and\nrewrites the question into non-temporal sub-questions and temporal constraints.\nAnswers to sub-questions are then retrieved from the underlying KB-QA engine.\nFinally, TEQUILA uses constraint reasoning on temporal intervals to compute\nfinal answers to the full question. Comparisons against state-of-the-art\nbaselines show the viability of our method.", "published": "2019-08-09 22:41:20", "link": "http://arxiv.org/abs/1908.03650v4", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Transferable Representation Learning in Vision-and-Language Navigation", "abstract": "Vision-and-Language Navigation (VLN) tasks such as Room-to-Room (R2R) require\nmachine agents to interpret natural language instructions and learn to act in\nvisually realistic environments to achieve navigation goals. The overall task\nrequires competence in several perception problems: successful agents combine\nspatio-temporal, vision and language understanding to produce appropriate\naction sequences. Our approach adapts pre-trained vision and language\nrepresentations to relevant in-domain tasks making them more effective for VLN.\nSpecifically, the representations are adapted to solve both a cross-modal\nsequence alignment and sequence coherence task. In the sequence alignment task,\nthe model determines whether an instruction corresponds to a sequence of visual\nframes. In the sequence coherence task, the model determines whether the\nperceptual sequences are predictive sequentially in the instruction-conditioned\nlatent space. By transferring the domain-adapted representations, we improve\ncompetitive agents in R2R as measured by the success rate weighted by path\nlength (SPL) metric.", "published": "2019-08-09 10:58:01", "link": "http://arxiv.org/abs/1908.03409v2", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Interactive Variance Attention based Online Spoiler Detection for\n  Time-Sync Comments", "abstract": "Nowadays, time-sync comment (TSC), a new form of interactive comments, has\nbecome increasingly popular in Chinese video websites. By posting TSCs, people\ncan easily express their feelings and exchange their opinions with others when\nwatching online videos. However, some spoilers appear among the TSCs. These\nspoilers reveal crucial plots in videos that ruin people's surprise when they\nfirst watch the video. In this paper, we proposed a novel Similarity-Based\nNetwork with Interactive Variance Attention (SBN-IVA) to classify comments as\nspoilers or not. In this framework, we firstly extract textual features of TSCs\nthrough the word-level attentive encoder. We design Similarity-Based Network\n(SBN) to acquire neighbor and keyframe similarity according to semantic\nsimilarity and timestamps of TSCs. Then, we implement Interactive Variance\nAttention (IVA) to eliminate the impact of noise comments. Finally, we obtain\nthe likelihood of spoiler based on the difference between the neighbor and\nkeyframe similarity. Experiments show SBN-IVA is on average 11.2\\% higher than\nthe state-of-the-art method on F1-score in baselines.", "published": "2019-08-09 13:24:21", "link": "http://arxiv.org/abs/1908.03451v2", "categories": ["cs.IR", "cs.CL", "cs.MM"], "primary_category": "cs.IR"}
{"title": "Challenging the Boundaries of Speech Recognition: The MALACH Corpus", "abstract": "There has been huge progress in speech recognition over the last several\nyears. Tasks once thought extremely difficult, such as SWITCHBOARD, now\napproach levels of human performance. The MALACH corpus (LDC catalog\nLDC2012S05), a 375-Hour subset of a large archive of Holocaust testimonies\ncollected by the Survivors of the Shoah Visual History Foundation, presents\nsignificant challenges to the speech community. The collection consists of\nunconstrained, natural speech filled with disfluencies, heavy accents,\nage-related coarticulations, un-cued speaker and language switching, and\nemotional speech - all still open problems for speech recognition systems.\nTranscription is challenging even for skilled human annotators. This paper\nproposes that the community place focus on the MALACH corpus to develop speech\nrecognition systems that are more robust with respect to accents, disfluencies\nand emotional speech. To reduce the barrier for entry, a lexicon and training\nand testing setups have been created and baseline results using current deep\nlearning technologies are presented. The metadata has just been released by LDC\n(LDC2019S11). It is hoped that this resource will enable the community to build\non top of these baselines so that the extremely important information in these\nand related oral histories becomes accessible to a wider audience.", "published": "2019-08-09 13:35:45", "link": "http://arxiv.org/abs/1908.03455v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "BERT-based Ranking for Biomedical Entity Normalization", "abstract": "Developing high-performance entity normalization algorithms that can\nalleviate the term variation problem is of great interest to the biomedical\ncommunity. Although deep learning-based methods have been successfully applied\nto biomedical entity normalization, they often depend on traditional\ncontext-independent word embeddings. Bidirectional Encoder Representations from\nTransformers (BERT), BERT for Biomedical Text Mining (BioBERT) and BERT for\nClinical Text Mining (ClinicalBERT) were recently introduced to pre-train\ncontextualized word representation models using bidirectional Transformers,\nadvancing the state-of-the-art for many natural language processing tasks. In\nthis study, we proposed an entity normalization architecture by fine-tuning the\npre-trained BERT / BioBERT / ClinicalBERT models and conducted extensive\nexperiments to evaluate the effectiveness of the pre-trained models for\nbiomedical entity normalization using three different types of datasets. Our\nexperimental results show that the best fine-tuned models consistently\noutperformed previous methods and advanced the state-of-the-art for biomedical\nentity normalization, with up to 1.17% increase in accuracy.", "published": "2019-08-09 17:19:43", "link": "http://arxiv.org/abs/1908.03548v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "VisualBERT: A Simple and Performant Baseline for Vision and Language", "abstract": "We propose VisualBERT, a simple and flexible framework for modeling a broad\nrange of vision-and-language tasks. VisualBERT consists of a stack of\nTransformer layers that implicitly align elements of an input text and regions\nin an associated input image with self-attention. We further propose two\nvisually-grounded language model objectives for pre-training VisualBERT on\nimage caption data. Experiments on four vision-and-language tasks including\nVQA, VCR, NLVR2, and Flickr30K show that VisualBERT outperforms or rivals with\nstate-of-the-art models while being significantly simpler. Further analysis\ndemonstrates that VisualBERT can ground elements of language to image regions\nwithout any explicit supervision and is even sensitive to syntactic\nrelationships, tracking, for example, associations between verbs and image\nregions corresponding to their arguments.", "published": "2019-08-09 17:57:13", "link": "http://arxiv.org/abs/1908.03557v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Catching the Phish: Detecting Phishing Attacks using Recurrent Neural\n  Networks (RNNs)", "abstract": "The emergence of online services in our daily lives has been accompanied by a\nrange of malicious attempts to trick individuals into performing undesired\nactions, often to the benefit of the adversary. The most popular medium of\nthese attempts is phishing attacks, particularly through emails and websites.\nIn order to defend against such attacks, there is an urgent need for automated\nmechanisms to identify this malevolent content before it reaches users. Machine\nlearning techniques have gradually become the standard for such classification\nproblems. However, identifying common measurable features of phishing content\n(e.g., in emails) is notoriously difficult. To address this problem, we engage\nin a novel study into a phishing content classifier based on a recurrent neural\nnetwork (RNN), which identifies such features without human input. At this\nstage, we scope our research to emails, but our approach can be extended to\napply to websites. Our results show that the proposed system outperforms\nstate-of-the-art tools. Furthermore, our classifier is efficient and takes into\naccount only the text and, in particular, the textual structure of the email.\nSince these features are rarely considered in email classification, we argue\nthat our classifier can complement existing classifiers with high information\ngain.", "published": "2019-08-09 21:37:42", "link": "http://arxiv.org/abs/1908.03640v1", "categories": ["cs.CR", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CR"}
{"title": "ToyADMOS: A Dataset of Miniature-Machine Operating Sounds for Anomalous\n  Sound Detection", "abstract": "This paper introduces a new dataset called \"ToyADMOS\" designed for anomaly\ndetection in machine operating sounds (ADMOS). To the best our knowledge, no\nlarge-scale datasets are available for ADMOS, although large-scale datasets\nhave contributed to recent advancements in acoustic signal processing. This is\nbecause anomalous sound data are difficult to collect. To build a large-scale\ndataset for ADMOS, we collected anomalous operating sounds of miniature\nmachines (toys) by deliberately damaging them. The released dataset consists of\nthree sub-datasets for machine-condition inspection, fault diagnosis of\nmachines with geometrically fixed tasks, and fault diagnosis of machines with\nmoving tasks. Each sub-dataset includes over 180 hours of normal\nmachine-operating sounds and over 4,000 samples of anomalous sounds collected\nwith four microphones at a 48-kHz sampling rate. The dataset is freely\navailable for download at https://github.com/YumaKoizumi/ToyADMOS-dataset", "published": "2019-08-09 03:52:08", "link": "http://arxiv.org/abs/1908.03299v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Emotionless: Privacy-Preserving Speech Analysis for Voice Assistants", "abstract": "Voice-enabled interactions provide more human-like experiences in many\npopular IoT systems. Cloud-based speech analysis services extract useful\ninformation from voice input using speech recognition techniques. The voice\nsignal is a rich resource that discloses several possible states of a speaker,\nsuch as emotional state, confidence and stress levels, physical condition, age,\ngender, and personal traits. Service providers can build a very accurate\nprofile of a user's demographic category, personal preferences, and may\ncompromise privacy. To address this problem, a privacy-preserving intermediate\nlayer between users and cloud services is proposed to sanitize the voice input.\nIt aims to maintain utility while preserving user privacy. It achieves this by\ncollecting real time speech data and analyzes the signal to ensure privacy\nprotection prior to sharing of this data with services providers. Precisely,\nthe sensitive representations are extracted from the raw signal by using\ntransformation functions and then wrapped it via voice conversion technology.\nExperimental evaluation based on emotion recognition to assess the efficacy of\nthe proposed method shows that identification of sensitive emotional state of\nthe speaker is reduced by ~96 %.", "published": "2019-08-09 21:11:45", "link": "http://arxiv.org/abs/1908.03632v1", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.CR"}
{"title": "The role of cue enhancement and frequency fine-tuning in hearing\n  impaired phone recognition", "abstract": "A speech-based hearing test is designed to identify the susceptible\nerror-prone phones for individual hearing impaired (HI) ear. Only robust tokens\nin the experiment noise levels had been chosen for the test. The\nnoise-robustness of tokens is measured as SNR90 of the token, which is the\nsignal to the speech-weighted noise ratio where a normal hearing (NH) listener\nwould recognize the token with an accuracy of 90% on average. Two sets of\ntokens T1 and T2 having the same consonant-vowels but different talkers with\ndistinct SNR90 had been presented with flat gain at listeners' most comfortable\nlevel. We studied the effects of frequency fine-tuning of the primary cue by\npresenting tokens of the same consonant but different vowels with similar\nSNR90. Additionally, we investigated the role of changing the intensity of\nprimary cue in HI phone recognition, by presenting tokens from both sets T1 and\nT2. On average, 92% of tokens are improved when we replaced the CV with the\nsame CV but with a more robust talker. Additionally, using CVs with similar\nSNR90, on average, tokens are improved by 75%, 71%, 63%, and 72%, when we\nreplaced vowels /A, ae, I, E/, respectively. The confusion pattern in each case\nprovides insight into how these changes affect the phone recognition in each HI\near. We propose to prescribe hearing aid amplification tailored to individual\nHI ears, based on the confusion pattern, the response from cue enhancement, and\nthe response from frequency fine-tuning of the cue.", "published": "2019-08-09 20:38:41", "link": "http://arxiv.org/abs/1908.04751v1", "categories": ["q-bio.QM", "cs.LG", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "q-bio.QM"}
