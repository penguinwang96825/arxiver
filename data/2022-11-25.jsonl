{"title": "Global and Local Hierarchy-aware Contrastive Framework for Implicit\n  Discourse Relation Recognition", "abstract": "Due to the absence of explicit connectives, implicit discourse relation\nrecognition (IDRR) remains a challenging task in discourse analysis. The\ncritical step for IDRR is to learn high-quality discourse relation\nrepresentations between two arguments. Recent methods tend to integrate the\nwhole hierarchical information of senses into discourse relation\nrepresentations for multi-level sense recognition. Nevertheless, they\ninsufficiently incorporate the static hierarchical structure containing all\nsenses (defined as global hierarchy), and ignore the hierarchical sense label\nsequence corresponding to each instance (defined as local hierarchy). For the\npurpose of sufficiently exploiting global and local hierarchies of senses to\nlearn better discourse relation representations, we propose a novel GlObal and\nLocal Hierarchy-aware Contrastive Framework (GOLF), to model two kinds of\nhierarchies with the aid of multi-task learning and contrastive learning.\nExperimental results on PDTB 2.0 and PDTB 3.0 datasets demonstrate that our\nmethod remarkably outperforms current state-of-the-art models at all\nhierarchical levels. Our code is publicly available at\nhttps://github.com/YJiangcm/GOLF_for_IDRR", "published": "2022-11-25 03:19:03", "link": "http://arxiv.org/abs/2211.13873v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning with Silver Standard Data for Zero-shot Relation Extraction", "abstract": "The superior performance of supervised relation extraction (RE) methods\nheavily relies on a large amount of gold standard data. Recent zero-shot\nrelation extraction methods converted the RE task to other NLP tasks and used\noff-the-shelf models of these NLP tasks to directly perform inference on the\ntest data without using a large amount of RE annotation data. A potentially\nvaluable by-product of these methods is the large-scale silver standard data.\nHowever, there is no further investigation on the use of potentially valuable\nsilver standard data. In this paper, we propose to first detect a small amount\nof clean data from silver standard data and then use the selected clean data to\nfinetune the pretrained model. We then use the finetuned model to infer\nrelation types. We also propose a class-aware clean data detection module to\nconsider class information when selecting clean data. The experimental results\nshow that our method can outperform the baseline by 12% and 11% on TACRED and\nWiki80 dataset in the zero-shot RE task. By using extra silver standard data of\ndifferent distributions, the performance can be further improved.", "published": "2022-11-25 04:14:16", "link": "http://arxiv.org/abs/2211.13883v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Complementary Explanations for Effective In-Context Learning", "abstract": "Large language models (LLMs) have exhibited remarkable capabilities in\nlearning from explanations in prompts, but there has been limited understanding\nof exactly how these explanations function or why they are effective. This work\naims to better understand the mechanisms by which explanations are used for\nin-context learning. We first study the impact of two different factors on the\nperformance of prompts with explanations: the computation trace (the way the\nsolution is decomposed) and the natural language used to express the prompt. By\nperturbing explanations on three controlled tasks, we show that both factors\ncontribute to the effectiveness of explanations. We further study how to form\nmaximally effective sets of explanations for solving a given test query. We\nfind that LLMs can benefit from the complementarity of the explanation set:\ndiverse reasoning skills shown by different exemplars can lead to better\nperformance. Therefore, we propose a maximal marginal relevance-based exemplar\nselection approach for constructing exemplar sets that are both relevant as\nwell as complementary, which successfully improves the in-context learning\nperformance across three real-world tasks on multiple LLMs.", "published": "2022-11-25 04:40:47", "link": "http://arxiv.org/abs/2211.13892v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MUSIED: A Benchmark for Event Detection from Multi-Source Heterogeneous\n  Informal Texts", "abstract": "Event detection (ED) identifies and classifies event triggers from\nunstructured texts, serving as a fundamental task for information extraction.\nDespite the remarkable progress achieved in the past several years, most\nresearch efforts focus on detecting events from formal texts (e.g., news\narticles, Wikipedia documents, financial announcements). Moreover, the texts in\neach dataset are either from a single source or multiple yet relatively\nhomogeneous sources. With massive amounts of user-generated text accumulating\non the Web and inside enterprises, identifying meaningful events in these\ninformal texts, usually from multiple heterogeneous sources, has become a\nproblem of significant practical value. As a pioneering exploration that\nexpands event detection to the scenarios involving informal and heterogeneous\ntexts, we propose a new large-scale Chinese event detection dataset based on\nuser reviews, text conversations, and phone conversations in a leading\ne-commerce platform for food service. We carefully investigate the proposed\ndataset's textual informality and multi-source heterogeneity characteristics by\ninspecting data samples quantitatively and qualitatively. Extensive experiments\nwith state-of-the-art event detection methods verify the unique challenges\nposed by these characteristics, indicating that multi-source informal event\ndetection remains an open problem and requires further efforts. Our benchmark\nand code are released at \\url{https://github.com/myeclipse/MUSIED}.", "published": "2022-11-25 05:05:29", "link": "http://arxiv.org/abs/2211.13896v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparison Study Between Token Classification and Sequence\n  Classification In Text Classification", "abstract": "Unsupervised Machine Learning techniques have been applied to Natural\nLanguage Processing tasks and surpasses the benchmarks such as GLUE with great\nsuccess. Building language models approach achieves good results in one\nlanguage and it can be applied to multiple NLP task such as classification,\nsummarization, generation and etc as an out of box model. Among all the of the\nclassical approaches used in NLP, the masked language modeling is the most\nused. In general, the only requirement to build a language model is presence of\nthe large corpus of textual data. Text classification engines uses a variety of\nmodels from classical and state of art transformer models to classify texts for\nin order to save costs. Sequence Classifiers are mostly used in the domain of\ntext classification. However Token classifiers also are viable candidate models\nas well. Sequence Classifiers and Token Classifier both tend to improve the\nclassification predictions due to the capturing the context information\ndifferently. This work aims to compare the performance of Sequence Classifier\nand Token Classifiers and evaluate each model on the same set of data. In this\nwork, we are using a pre-trained model as the base model and Token Classifier\nand Sequence Classier heads results of these two scoring paradigms with be\ncompared..", "published": "2022-11-25 05:14:58", "link": "http://arxiv.org/abs/2211.13899v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Deep Learning Anomaly Detection Method in Textual Data", "abstract": "In this article, we propose using deep learning and transformer architectures\ncombined with classical machine learning algorithms to detect and identify text\nanomalies in texts. Deep learning model provides a very crucial context\ninformation about the textual data which all textual context are converted to a\nnumerical representation. We used multiple machine learning methods such as\nSentence Transformers, Auto Encoders, Logistic Regression and Distance\ncalculation methods to predict anomalies. The method are tested on the texts\ndata and we used syntactic data from different source injected into the\noriginal text as anomalies or use them as target. Different methods and\nalgorithm are explained in the field of outlier detection and the results of\nthe best technique is presented. These results suggest that our algorithm could\npotentially reduce false positive rates compared with other anomaly detection\nmethods that we are testing.", "published": "2022-11-25 05:18:13", "link": "http://arxiv.org/abs/2211.13900v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exposure and Emergence in Usage-Based Grammar: Computational Experiments\n  in 35 Languages", "abstract": "This paper uses computational experiments to explore the role of exposure in\nthe emergence of construction grammars. While usage-based grammars are\nhypothesized to depend on a learner's exposure to actual language use, the\nmechanisms of such exposure have only been studied in a few constructions in\nisolation. This paper experiments with (i) the growth rate of the\nconstructicon, (ii) the convergence rate of grammars exposed to independent\nregisters, and (iii) the rate at which constructions are forgotten when they\nhave not been recently observed. These experiments show that the lexicon grows\nmore quickly than the grammar and that the growth rate of the grammar is not\ndependent on the growth rate of the lexicon. At the same time,\nregister-specific grammars converge onto more similar constructions as the\namount of exposure increases. This means that the influence of specific\nregisters becomes less important as exposure increases. Finally, the rate at\nwhich constructions are forgotten when they have not been recently observed\nmirrors the growth rate of the constructicon. This paper thus presents a\ncomputational model of usage-based grammar that includes both the emergence and\nthe unentrenchment of constructions.", "published": "2022-11-25 15:05:36", "link": "http://arxiv.org/abs/2211.14160v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Effect of Anticipation on Reading Times", "abstract": "Over the past two decades, numerous studies have demonstrated how less\npredictable (i.e., higher surprisal) words take more time to read. In general,\nthese studies have implicitly assumed the reading process is purely responsive:\nReaders observe a new word and allocate time to process it as required. We\nargue that prior results are also compatible with a reading process that is at\nleast partially anticipatory: Readers could make predictions about a future\nword and allocate time to process it based on their expectation. In this work,\nwe operationalize this anticipation as a word's contextual entropy. We assess\nthe effect of anticipation on reading by comparing how well surprisal and\ncontextual entropy predict reading times on four naturalistic reading datasets:\ntwo self-paced and two eye-tracking. Experimentally, across datasets and\nanalyses, we find substantial evidence for effects of contextual entropy over\nsurprisal on a word's reading time (RT): in fact, entropy is sometimes better\nthan surprisal in predicting a word's RT. Spillover effects, however, are\ngenerally not captured by entropy, but only by surprisal. Further, we\nhypothesize four cognitive mechanisms through which contextual entropy could\nimpact RTs -- three of which we are able to design experiments to analyze.\nOverall, our results support a view of reading that is not just responsive, but\nalso anticipatory.", "published": "2022-11-25 18:58:23", "link": "http://arxiv.org/abs/2211.14301v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Moral- and Event- Centric Inspection of Gender Bias in Fairy Tales at\n  A Large Scale", "abstract": "Fairy tales are a common resource for young children to learn a language or\nunderstand how a society works. However, gender bias, e.g., stereotypical\ngender roles, in this literature may cause harm and skew children's world view.\nInstead of decades of qualitative and manual analysis of gender bias in fairy\ntales, we computationally analyze gender bias in a fairy tale dataset\ncontaining 624 fairy tales from 7 different cultures. We specifically examine\ngender difference in terms of moral foundations, which are measures of human\nmorality, and events, which reveal human activities associated with each\ncharacter. We find that the number of male characters is two times that of\nfemale characters, showing a disproportionate gender representation. Our\nanalysis further reveal stereotypical portrayals of both male and female\ncharacters in terms of moral foundations and events. Female characters turn out\nmore associated with care-, loyalty- and sanctity- related moral words, while\nmale characters are more associated with fairness- and authority- related moral\nwords. Female characters' events are often about emotion (e.g., weep),\nappearance (e.g., comb), household (e.g., bake), etc.; while male characters'\nevents are more about profession (e.g., hunt), violence (e.g., destroy),\njustice (e.g., judge), etc. Gender bias in terms of moral foundations shows an\nobvious difference across cultures. For example, female characters are more\nassociated with care and sanctity in high uncertainty-avoidance cultures which\nare less open to changes and unpredictability. Based on the results, we propose\nimplications for children's literature and early literacy research.", "published": "2022-11-25 19:38:09", "link": "http://arxiv.org/abs/2211.14358v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finetuning BERT on Partially Annotated NER Corpora", "abstract": "Most Named Entity Recognition (NER) models operate under the assumption that\ntraining datasets are fully labelled. While it is valid for established\ndatasets like CoNLL 2003 and OntoNotes, sometimes it is not feasible to obtain\nthe complete dataset annotation. These situations may occur, for instance,\nafter selective annotation of entities for cost reduction. This work presents\nan approach to finetuning BERT on such partially labelled datasets using\nself-supervision and label preprocessing. Our approach outperforms the previous\nLSTM-based label preprocessing baseline, significantly improving the\nperformance on poorly labelled datasets. We demonstrate that following our\napproach while finetuning RoBERTa on CoNLL 2003 dataset with only 10% of total\nentities labelled is enough to reach the performance of the baseline trained on\nthe same dataset with 50% of the entities labelled.", "published": "2022-11-25 19:54:30", "link": "http://arxiv.org/abs/2211.14360v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Naughtyformer: A Transformer Understands Offensive Humor", "abstract": "Jokes are intentionally written to be funny, but not all jokes are created\nthe same. Some jokes may be fit for a classroom of kindergarteners, but others\nare best reserved for a more mature audience. While recent work has shown\nimpressive results on humor detection in text, here we instead investigate the\nmore nuanced task of detecting humor subtypes, especially of the less innocent\nvariety. To that end, we introduce a novel jokes dataset filtered from Reddit\nand solve the subtype classification task using a finetuned Transformer dubbed\nthe Naughtyformer. Moreover, we show that our model is significantly better at\ndetecting offensiveness in jokes compared to state-of-the-art methods.", "published": "2022-11-25 20:37:58", "link": "http://arxiv.org/abs/2211.14369v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Competency-Aware Neural Machine Translation: Can Machine Translation\n  Know its Own Translation Quality?", "abstract": "Neural machine translation (NMT) is often criticized for failures that happen\nwithout awareness. The lack of competency awareness makes NMT untrustworthy.\nThis is in sharp contrast to human translators who give feedback or conduct\nfurther investigations whenever they are in doubt about predictions. To fill\nthis gap, we propose a novel competency-aware NMT by extending conventional NMT\nwith a self-estimator, offering abilities to translate a source sentence and\nestimate its competency. The self-estimator encodes the information of the\ndecoding procedure and then examines whether it can reconstruct the original\nsemantics of the source sentence. Experimental results on four translation\ntasks demonstrate that the proposed method not only carries out translation\ntasks intact but also delivers outstanding performance on quality estimation.\nWithout depending on any reference or annotated data typically required by\nstate-of-the-art metric and quality estimation methods, our model yields an\neven higher correlation with human quality judgments than a variety of\naforementioned methods, such as BLEURT, COMET, and BERTScore. Quantitative and\nqualitative analyses show better robustness of competency awareness in our\nmodel.", "published": "2022-11-25 02:39:41", "link": "http://arxiv.org/abs/2211.13865v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TRAC: A Textual Benchmark for Reasoning about Actions and Change", "abstract": "Reasoning about actions and change (RAC) is essential to understand and\ninteract with the ever-changing environment. Previous AI research has shown the\nimportance of fundamental and indispensable knowledge of actions, i.e.,\npreconditions and effects. However, traditional methods rely on logical\nformalization which hinders practical applications. With recent\ntransformer-based language models (LMs), reasoning over text is desirable and\nseemingly feasible, leading to the question of whether LMs can effectively and\nefficiently learn to solve RAC problems. We propose four essential RAC tasks as\na comprehensive textual benchmark and generate problems in a way that minimizes\nthe influence of other linguistic requirements (e.g., grounding) to focus on\nRAC. The resulting benchmark, TRAC, encompassing problems of various\ncomplexities, facilitates a more granular evaluation of LMs, precisely\ntargeting the structural generalization ability much needed for RAC.\nExperiments with three high-performing transformers indicates that additional\nefforts are needed to tackle challenges raised by TRAC.", "published": "2022-11-25 06:54:30", "link": "http://arxiv.org/abs/2211.13930v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GPT-3-driven pedagogical agents for training children's curious\n  question-asking skills", "abstract": "In order to train children's ability to ask curiosity-driven questions,\nprevious research has explored designing specific exercises relying on\nproviding semantic and linguistic cues to help formulate such questions. But\ndespite showing pedagogical efficiency, this method is still limited as it\nrelies on generating the said cues by hand, which can be a very costly process.\nIn this context, we propose to leverage advances in the natural language\nprocessing field (NLP) and investigate the efficiency of using a large language\nmodel (LLM) for automating the production of the pedagogical content of a\ncurious question-asking (QA) training. We study generating the said content\nusing the \"prompt-based\" method that consists of explaining the task to the LLM\nin natural text. We evaluate the output using human experts annotations and\ncomparisons with hand-generated content. Results suggested indeed the relevance\nand usefulness of this content. We also conduct a field study in primary school\n(75 children aged 9-10), where we evaluate children's QA performance when\nhaving this training. We compare 3 types of content : 1) hand-generated content\nthat proposes \"closed\" cues leading to predefined questions; 2) GPT-3-generated\ncontent that proposes the same type of cues; 3) GPT-3-generated content that\nproposes \"open\" cues leading to several possible questions. We see a similar QA\nperformance between the two \"closed\" trainings (showing the scalability of the\napproach using GPT-3), and a better one for participants with the \"open\"\ntraining. These results suggest the efficiency of using LLMs to support\nchildren in generating more curious questions, using a natural language\nprompting approach that affords usability by teachers and other users not\nspecialists of AI techniques. Furthermore, results also show that open-ended\ncontent may be more suitable for training curious question-asking skills.", "published": "2022-11-25 16:41:59", "link": "http://arxiv.org/abs/2211.14228v6", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Multiverse: Multilingual Evidence for Fake News Detection", "abstract": "Misleading information spreads on the Internet at an incredible speed, which\ncan lead to irreparable consequences in some cases. It is becoming essential to\ndevelop fake news detection technologies. While substantial work has been done\nin this direction, one of the limitations of the current approaches is that\nthese models are focused only on one language and do not use multilingual\ninformation. In this work, we propose Multiverse -- a new feature based on\nmultilingual evidence that can be used for fake news detection and improve\nexisting approaches. The hypothesis of the usage of cross-lingual evidence as a\nfeature for fake news detection is confirmed, firstly, by manual experiment\nbased on a set of known true and fake news. After that, we compared our fake\nnews classification system based on the proposed feature with several baselines\non two multi-domain datasets of general-topic news and one fake COVID-19 news\ndataset showing that in additional combination with linguistic features it\nyields significant improvements.", "published": "2022-11-25 18:24:17", "link": "http://arxiv.org/abs/2211.14279v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Machine Learning, Natural Language Processing Analysis of Youth\n  Perspectives: Key Trends and Focus Areas for Sustainable Youth Development\n  Policies", "abstract": "Investing in children and youth is a critical step towards inclusive,\nequitable, and sustainable development for current and future generations.\nSeveral international agendas for accomplishing common global goals emphasize\nthe need for active youth participation and engagement for sustainable\ndevelopment. The 2030 Agenda for Sustainable Development emphasizes the need\nfor youth engagement and the inclusion of youth perspectives as an important\nstep toward addressing each of the 17 Sustainable Development Goals. The aim of\nthis study is to analyze youth perspectives, values, and sentiments towards\nissues addressed by the 17 Sustainable Development Goals through social network\nanalysis using machine learning. Social network data collected during 7 major\nsustainability conferences aimed at engaging children and youth is analyzed\nusing natural language processing techniques for sentiment analysis. This data\ncategorized using a natural language processing text classifier trained on a\nsample dataset of social network data during the 7 youth sustainability\nconferences for deeper understanding of youth perspectives in relation to the\nSDGs. Machine learning identified demographic and location attributes and\nfeatures are utilized in order to identify bias and demographic differences\nbetween ages, gender, and race among youth. Using natural language processing,\nthe qualitative data collected from over 7 different countries in 3 languages\nare systematically translated, categorized, and analyzed, revealing key trends\nand focus areas for sustainable youth development policies. The obtained\nresults reveal the general youth's depth of knowledge on sustainable\ndevelopment and their attitudes towards each of the 17 SDGs. The findings of\nthis study serve as a guide toward better understanding the interests, roles,\nand perspectives of children and youth in achieving the goals of Agenda 2030.", "published": "2022-11-25 02:43:21", "link": "http://arxiv.org/abs/2211.14321v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Aesthetically Relevant Image Captioning", "abstract": "Image aesthetic quality assessment (AQA) aims to assign numerical aesthetic\nratings to images whilst image aesthetic captioning (IAC) aims to generate\ntextual descriptions of the aesthetic aspects of images. In this paper, we\nstudy image AQA and IAC together and present a new IAC method termed\nAesthetically Relevant Image Captioning (ARIC). Based on the observation that\nmost textual comments of an image are about objects and their interactions\nrather than aspects of aesthetics, we first introduce the concept of Aesthetic\nRelevance Score (ARS) of a sentence and have developed a model to automatically\nlabel a sentence with its ARS. We then use the ARS to design the ARIC model\nwhich includes an ARS weighted IAC loss function and an ARS based diverse\naesthetic caption selector (DACS). We present extensive experimental results to\nshow the soundness of the ARS concept and the effectiveness of the ARIC model\nby demonstrating that texts with higher ARS's can predict the aesthetic ratings\nmore accurately and that the new ARIC model can generate more accurate,\naesthetically more relevant and more diverse image captions. Furthermore, a\nlarge new research database containing 510K images with over 5 million comments\nand 350K aesthetic scores, and code for implementing ARIC are available at\nhttps://github.com/PengZai/ARIC.", "published": "2022-11-25 14:28:10", "link": "http://arxiv.org/abs/2211.15378v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CodeExp: Explanatory Code Document Generation", "abstract": "Developing models that can automatically generate detailed code explanation\ncan greatly benefit software maintenance and programming education. However,\nexisting code-to-text generation models often produce only high-level summaries\nof code that do not capture implementation-level choices essential for these\nscenarios. To fill in this gap, we propose the code explanation generation\ntask. We first conducted a human study to identify the criteria for\nhigh-quality explanatory docstring for code. Based on that, we collected and\nrefined a large-scale code docstring corpus and formulated automatic evaluation\nmetrics that best match human assessments. Finally, we present a multi-stage\nfine-tuning strategy and baseline models for the task. Our experiments show\nthat (1) our refined training dataset lets models achieve better performance in\nthe explanation generation tasks compared to larger unrefined data (15x\nlarger), and (2) fine-tuned models can generate well-structured long docstrings\ncomparable to human-written ones. We envision our training dataset,\nhuman-evaluation protocol, recommended metrics, and fine-tuning strategy can\nboost future code explanation research. The code and annotated data are\navailable at https://github.com/subercui/CodeExp.", "published": "2022-11-25 18:05:44", "link": "http://arxiv.org/abs/2211.15395v1", "categories": ["cs.CL", "cs.LG", "I.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Semantic Table Detection with LayoutLMv3", "abstract": "This paper presents an application of the LayoutLMv3 model for semantic table\ndetection on financial documents from the IIIT-AR-13K dataset. The motivation\nbehind this paper's experiment was that LayoutLMv3's official paper had no\nresults for table detection using semantic information. We concluded that our\napproach did not improve the model's table detection capabilities, for which we\ncan give several possible reasons. Either the model's weights were unsuitable\nfor our purpose, or we needed to invest more time in optimising the model's\nhyperparameters. It is also possible that semantic information does not improve\na model's table detection accuracy.", "published": "2022-11-25 12:56:07", "link": "http://arxiv.org/abs/2211.15504v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ComCLIP: Training-Free Compositional Image and Text Matching", "abstract": "Contrastive Language-Image Pretraining (CLIP) has demonstrated great\nzero-shot performance for matching images and text. However, it is still\nchallenging to adapt vision-lanaguage pretrained models like CLIP to\ncompositional image and text matching -- a more challenging image and text\nmatching task requiring the model understanding of compositional word concepts\nand visual components. Towards better compositional generalization in zero-shot\nimage and text matching, in this paper, we study the problem from a causal\nperspective: the erroneous semantics of individual entities are essentially\nconfounders that cause the matching failure. Therefore, we propose a novel\n\\textbf{\\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP\ndisentangles input images into subjects, objects, and action sub-images and\ncomposes CLIP's vision encoder and text encoder to perform evolving matching\nover compositional text embedding and sub-image embeddings. In this way,\nComCLIP can mitigate spurious correlations introduced by the pretrained CLIP\nmodels and dynamically evaluate the importance of each component. Experiments\non four compositional image-text matching datasets: SVO, ComVG, Winoground, and\nVL-checklist, and two general image-text retrieval datasets: Flick30K, and\nMSCOCO demonstrate the effectiveness of our plug-and-play method, which boosts\nthe \\textbf{\\textit{zero-shot}} inference ability of CLIP, SLIP, and BLIP2 even\nwithout further training or fine-tuning. Our codes can be found at\nhttps://github.com/eric-ai-lab/ComCLIP.", "published": "2022-11-25 01:37:48", "link": "http://arxiv.org/abs/2211.13854v5", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Solving math word problems with process- and outcome-based feedback", "abstract": "Recent work has shown that asking language models to generate reasoning steps\nimproves performance on many reasoning tasks. When moving beyond prompting,\nthis raises the question of how we should supervise such models: outcome-based\napproaches which supervise the final result, or process-based approaches which\nsupervise the reasoning process itself? Differences between these approaches\nmight naturally be expected not just in final-answer errors but also in\nreasoning errors, which can be difficult to detect and are problematic in many\nreal-world domains such as education. We run the first comprehensive comparison\nbetween process- and outcome-based approaches trained on a natural language\ntask, GSM8K. We find that pure outcome-based supervision produces similar\nfinal-answer error rates with less label supervision. However, for correct\nreasoning steps we find it necessary to use process-based supervision or\nsupervision from learned reward models that emulate process-based feedback. In\ntotal, we improve the previous best results from 16.8% $\\to$ 12.7% final-answer\nerror and 14.0% $\\to$ 3.4% reasoning error among final-answer-correct\nsolutions.", "published": "2022-11-25 18:19:44", "link": "http://arxiv.org/abs/2211.14275v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Interpretability Analysis of Deep Models for COVID-19 Detection", "abstract": "During the outbreak of COVID-19 pandemic, several research areas joined\nefforts to mitigate the damages caused by SARS-CoV-2. In this paper we present\nan interpretability analysis of a convolutional neural network based model for\nCOVID-19 detection in audios. We investigate which features are important for\nmodel decision process, investigating spectrograms, F0, F0 standard deviation,\nsex and age. Following, we analyse model decisions by generating heat maps for\nthe trained models to capture their attention during the decision process.\nFocusing on a explainable Inteligence Artificial approach, we show that studied\nmodels can taken unbiased decisions even in the presence of spurious data in\nthe training set, given the adequate preprocessing steps. Our best model has\n94.44% of accuracy in detection, with results indicating that models favors\nspectrograms for the decision process, particularly, high energy areas in the\nspectrogram related to prosodic domains, while F0 also leads to efficient\nCOVID-19 detection.", "published": "2022-11-25 20:56:23", "link": "http://arxiv.org/abs/2211.14372v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Analysis of Social Biases Present in BERT Variants Across Multiple\n  Languages", "abstract": "Although large pre-trained language models have achieved great success in\nmany NLP tasks, it has been shown that they reflect human biases from their\npre-training corpora. This bias may lead to undesirable outcomes when these\nmodels are applied in real-world settings. In this paper, we investigate the\nbias present in monolingual BERT models across a diverse set of languages\n(English, Greek, and Persian). While recent research has mostly focused on\ngender-related biases, we analyze religious and ethnic biases as well and\npropose a template-based method to measure any kind of bias, based on sentence\npseudo-likelihood, that can handle morphologically complex languages with\ngender-based adjective declensions. We analyze each monolingual model via this\nmethod and visualize cultural similarities and differences across different\ndimensions of bias. Ultimately, we conclude that current methods of probing for\nbias are highly language-dependent, necessitating cultural insights regarding\nthe unique ways bias is expressed in each language and culture (e.g. through\ncoded language, synecdoche, and other similar linguistic concepts). We also\nhypothesize that higher measured social biases in the non-English BERT models\ncorrelate with user-generated content in their training.", "published": "2022-11-25 23:38:08", "link": "http://arxiv.org/abs/2211.14402v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Testing the effectiveness of saliency-based explainability in NLP using\n  randomized survey-based experiments", "abstract": "As the applications of Natural Language Processing (NLP) in sensitive areas\nlike Political Profiling, Review of Essays in Education, etc. proliferate,\nthere is a great need for increasing transparency in NLP models to build trust\nwith stakeholders and identify biases. A lot of work in Explainable AI has\naimed to devise explanation methods that give humans insights into the workings\nand predictions of NLP models. While these methods distill predictions from\ncomplex models like Neural Networks into consumable explanations, how humans\nunderstand these explanations is still widely unexplored. Innate human\ntendencies and biases can handicap the understanding of these explanations in\nhumans, and can also lead to them misjudging models and predictions as a\nresult. We designed a randomized survey-based experiment to understand the\neffectiveness of saliency-based Post-hoc explainability methods in Natural\nLanguage Processing. The result of the experiment showed that humans have a\ntendency to accept explanations with a less critical view.", "published": "2022-11-25 08:49:01", "link": "http://arxiv.org/abs/2211.15351v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Proactive Dialog Agents Using Socially-Aware Reinforcement\n  Learning", "abstract": "The next step for intelligent dialog agents is to escape their role as silent\nbystanders and become proactive. Well-defined proactive behavior may improve\nhuman-machine cooperation, as the agent takes a more active role during\ninteraction and takes off responsibility from the user. However, proactivity is\na double-edged sword because poorly executed pre-emptive actions may have a\ndevastating effect not only on the task outcome but also on the relationship\nwith the user. For designing adequate proactive dialog strategies, we propose a\nnovel approach including both social as well as task-relevant features in the\ndialog. Here, the primary goal is to optimize proactive behavior so that it is\ntask-oriented - this implies high task success and efficiency - while also\nbeing socially effective by fostering user trust. Including both aspects in the\nreward function for training a proactive dialog agent using reinforcement\nlearning showed the benefit of our approach for more successful human-machine\ncooperation.", "published": "2022-11-25 14:29:26", "link": "http://arxiv.org/abs/2211.15359v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TPA-Net: Generate A Dataset for Text to Physics-based Animation", "abstract": "Recent breakthroughs in Vision-Language (V&L) joint research have achieved\nremarkable results in various text-driven tasks. High-quality Text-to-video\n(T2V), a task that has been long considered mission-impossible, was proven\nfeasible with reasonably good results in latest works. However, the resulting\nvideos often have undesired artifacts largely because the system is purely\ndata-driven and agnostic to the physical laws. To tackle this issue and further\npush T2V towards high-level physical realism, we present an autonomous data\ngeneration technique and a dataset, which intend to narrow the gap with a large\nnumber of multi-modal, 3D Text-to-Video/Simulation (T2V/S) data. In the\ndataset, we provide high-resolution 3D physical simulations for both solids and\nfluids, along with textual descriptions of the physical phenomena. We take\nadvantage of state-of-the-art physical simulation methods (i) Incremental\nPotential Contact (IPC) and (ii) Material Point Method (MPM) to simulate\ndiverse scenarios, including elastic deformations, material fractures,\ncollisions, turbulence, etc. Additionally, high-quality, multi-view rendering\nvideos are supplied for the benefit of T2V, Neural Radiance Fields (NeRF), and\nother communities. This work is the first step towards fully automated\nText-to-Video/Simulation (T2V/S). Live examples and subsequent work are at\nhttps://sites.google.com/view/tpa-net.", "published": "2022-11-25 04:26:41", "link": "http://arxiv.org/abs/2211.13887v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.GR", "eess.IV"], "primary_category": "cs.AI"}
{"title": "Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome\n  Homogenization?", "abstract": "As the scope of machine learning broadens, we observe a recurring theme of\nalgorithmic monoculture: the same systems, or systems that share components\n(e.g. training data), are deployed by multiple decision-makers. While sharing\noffers clear advantages (e.g. amortizing costs), does it bear risks? We\nintroduce and formalize one such risk, outcome homogenization: the extent to\nwhich particular individuals or groups experience negative outcomes from all\ndecision-makers. If the same individuals or groups exclusively experience\nundesirable outcomes, this may institutionalize systemic exclusion and\nreinscribe social hierarchy. To relate algorithmic monoculture and outcome\nhomogenization, we propose the component-sharing hypothesis: if decision-makers\nshare components like training data or specific models, then they will produce\nmore homogeneous outcomes. We test this hypothesis on algorithmic fairness\nbenchmarks, demonstrating that sharing training data reliably exacerbates\nhomogenization, with individual-level effects generally exceeding group-level\neffects. Further, given the dominant paradigm in AI of foundation models, i.e.\nmodels that can be adapted for myriad downstream tasks, we test whether model\nsharing homogenizes outcomes across tasks. We observe mixed results: we find\nthat for both vision and language settings, the specific methods for adapting a\nfoundation model significantly influence the degree of outcome homogenization.\nWe conclude with philosophical analyses of and societal challenges for outcome\nhomogenization, with an eye towards implications for deployed machine learning\nsystems.", "published": "2022-11-25 09:33:11", "link": "http://arxiv.org/abs/2211.13972v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Can Knowledge of End-to-End Text-to-Speech Models Improve Neural\n  MIDI-to-Audio Synthesis Systems?", "abstract": "With the similarity between music and speech synthesis from symbolic input\nand the rapid development of text-to-speech (TTS) techniques, it is worthwhile\nto explore ways to improve the MIDI-to-audio performance by borrowing from TTS\ntechniques. In this study, we analyze the shortcomings of a TTS-based\nMIDI-to-audio system and improve it in terms of feature computation, model\nselection, and training strategy, aiming to synthesize highly natural-sounding\naudio. Moreover, we conducted an extensive model evaluation through listening\ntests, pitch measurement, and spectrogram analysis. This work demonstrates not\nonly synthesis of highly natural music but offers a thorough analytical\napproach and useful outcomes for the community. Our code, pre-trained models,\nsupplementary materials, and audio samples are open sourced at\nhttps://github.com/nii-yamagishilab/midi-to-audio.", "published": "2022-11-25 02:54:33", "link": "http://arxiv.org/abs/2211.13868v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Puffin: pitch-synchronous neural waveform generation for fullband speech\n  on modest devices", "abstract": "We present a neural vocoder designed with low-powered Alternative and\nAugmentative Communication devices in mind. By combining elements of successful\nmodern vocoders with established ideas from an older generation of technology,\nour system is able to produce high quality synthetic speech at 48kHz on devices\nwhere neural vocoders are otherwise prohibitively complex. The system is\ntrained adversarially using differentiable pitch synchronous overlap add, and\nreduces complexity by relying on pitch synchronous Inverse Short-Time Fourier\nTransform (ISTFT) to generate speech samples. Our system achieves comparable\nquality with a strong (HiFi-GAN) baseline while using only a fraction of the\ncompute. We present results of a perceptual evaluation as well as an analysis\nof system complexity.", "published": "2022-11-25 14:15:21", "link": "http://arxiv.org/abs/2211.14130v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Efficient Incremental Text-to-Speech on GPUs", "abstract": "Incremental text-to-speech, also known as streaming TTS, has been\nincreasingly applied to online speech applications that require ultra-low\nresponse latency to provide an optimal user experience. However, most of the\nexisting speech synthesis pipelines deployed on GPU are still non-incremental,\nwhich uncovers limitations in high-concurrency scenarios, especially when the\npipeline is built with end-to-end neural network models. To address this issue,\nwe present a highly efficient approach to perform real-time incremental TTS on\nGPUs with Instant Request Pooling and Module-wise Dynamic Batching.\nExperimental results demonstrate that the proposed method is capable of\nproducing high-quality speech with a first-chunk latency lower than 80ms under\n100 QPS on a single NVIDIA A10 GPU and significantly outperforms the\nnon-incremental twin in both concurrency and latency. Our work reveals the\neffectiveness of high-performance incremental TTS on GPUs.", "published": "2022-11-25 07:43:45", "link": "http://arxiv.org/abs/2211.13939v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning General Audio Representations with Large-Scale Training of\n  Patchout Audio Transformers", "abstract": "The success of supervised deep learning methods is largely due to their\nability to learn relevant features from raw data. Deep Neural Networks (DNNs)\ntrained on large-scale datasets are capable of capturing a diverse set of\nfeatures, and learning a representation that can generalize onto unseen tasks\nand datasets that are from the same domain. Hence, these models can be used as\npowerful feature extractors, in combination with shallower models as\nclassifiers, for smaller tasks and datasets where the amount of training data\nis insufficient for learning an end-to-end model from scratch. During the past\nyears, Convolutional Neural Networks (CNNs) have largely been the method of\nchoice for audio processing. However, recently attention-based transformer\nmodels have demonstrated great potential in supervised settings, outperforming\nCNNs. In this work, we investigate the use of audio transformers trained on\nlarge-scale datasets to learn general-purpose representations. We study how the\ndifferent setups in these audio transformers affect the quality of their\nembeddings. We experiment with the models' time resolution, extracted embedding\nlevel, and receptive fields in order to see how they affect performance on a\nvariety of tasks and datasets, following the HEAR 2021 NeurIPS challenge\nevaluation setup. Our results show that representations extracted by audio\ntransformers outperform CNN representations. Furthermore, we will show that\ntransformers trained on Audioset can be extremely effective representation\nextractors for a wide range of downstream tasks.", "published": "2022-11-25 08:39:12", "link": "http://arxiv.org/abs/2211.13956v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Stereo Speech Enhancement Using Custom Mid-Side Signals and Monaural\n  Processing", "abstract": "Speech Enhancement (SE) systems typically operate on monaural input and are\nused for applications including voice communications and capture cleanup for\nuser generated content. Recent advancements and changes in the devices used for\nthese applications are likely to lead to an increase in the amount of\ntwo-channel content for the same applications. However, SE systems are\ntypically designed for monaural input; stereo results produced using trivial\nmethods such as channel independent or mid-side processing may be\nunsatisfactory, including substantial speech distortions. To address this, we\npropose a system which creates a novel representation of stereo signals called\nCustom Mid-Side Signals (CMSS). CMSS allow benefits of mid-side signals for\ncenter-panned speech to be extended to a much larger class of input signals.\nThis in turn allows any existing monaural SE system to operate as an efficient\nstereo system by processing the custom mid signal. We describe how the\nparameters needed for CMSS can be efficiently estimated by a component of the\nspatio-level filtering source separation system. Subjective listening using\nstate-of-the-art deep learning-based SE systems on stereo content with various\nspeech mixing styles shows that CMSS processing leads to improved speech\nquality at approximately half the cost of channel-independent processing.", "published": "2022-11-25 21:28:02", "link": "http://arxiv.org/abs/2211.14378v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
