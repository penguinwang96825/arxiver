{"title": "How Pre-trained Language Models Capture Factual Knowledge? A\n  Causal-Inspired Analysis", "abstract": "Recently, there has been a trend to investigate the factual knowledge\ncaptured by Pre-trained Language Models (PLMs). Many works show the PLMs'\nability to fill in the missing factual words in cloze-style prompts such as\n\"Dante was born in [MASK].\" However, it is still a mystery how PLMs generate\nthe results correctly: relying on effective clues or shortcut patterns? We try\nto answer this question by a causal-inspired analysis that quantitatively\nmeasures and evaluates the word-level patterns that PLMs depend on to generate\nthe missing words. We check the words that have three typical associations with\nthe missing words: knowledge-dependent, positionally close, and highly\nco-occurred. Our analysis shows: (1) PLMs generate the missing factual words\nmore by the positionally close and highly co-occurred words than the\nknowledge-dependent words; (2) the dependence on the knowledge-dependent words\nis more effective than the positionally close and highly co-occurred words.\nAccordingly, we conclude that the PLMs capture the factual knowledge\nineffectively because of depending on the inadequate associations.", "published": "2022-03-31 02:01:26", "link": "http://arxiv.org/abs/2203.16747v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Misogynistic Meme Detection using Early Fusion Model with Graph Network", "abstract": "In recent years , there has been an upsurge in a new form of entertainment\nmedium called memes. These memes although seemingly innocuous have transcended\nonto the boundary of online harassment against women and created an unwanted\nbias against them . To help alleviate this problem , we propose an early fusion\nmodel for prediction and identification of misogynistic memes and its type in\nthis paper for which we participated in SemEval-2022 Task 5 . The model\nreceives as input meme image with its text transcription with a target vector.\nGiven that a key challenge with this task is the combination of different\nmodalities to predict misogyny, our model relies on pretrained contextual\nrepresentations from different state-of-the-art transformer-based language\nmodels and pretrained image pretrained models to get an effective image\nrepresentation. Our model achieved competitive results on both SubTask-A and\nSubTask-B with the other competition teams and significantly outperforms the\nbaselines.", "published": "2022-03-31 03:45:50", "link": "http://arxiv.org/abs/2203.16781v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M-MELD: A Multilingual Multi-Party Dataset for Emotion Recognition in\n  Conversations", "abstract": "Expression of emotions is a crucial part of daily human communication.\nEmotion recognition in conversations (ERC) is an emerging field of study, where\nthe primary task is to identify the emotion behind each utterance in a\nconversation. Though a lot of work has been done on ERC in the past, these\nworks only focus on ERC in the English language, thereby ignoring any other\nlanguages. In this paper, we present Multilingual MELD (M-MELD), where we\nextend the Multimodal EmotionLines Dataset (MELD) \\cite{poria2018meld} to 4\nother languages beyond English, namely Greek, Polish, French, and Spanish.\nBeyond just establishing strong baselines for all of these 4 languages, we also\npropose a novel architecture, DiscLSTM, that uses both sequential and\nconversational discourse context in a conversational dialogue for ERC. Our\nproposed approach is computationally efficient, can transfer across languages\nusing just a cross-lingual encoder, and achieves better performance than most\nuni-modal text approaches in the literature on both MELD and M-MELD. We make\nour data and code publicly on GitHub.", "published": "2022-03-31 05:07:16", "link": "http://arxiv.org/abs/2203.16799v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BRIO: Bringing Order to Abstractive Summarization", "abstract": "Abstractive summarization models are commonly trained using maximum\nlikelihood estimation, which assumes a deterministic (one-point) target\ndistribution in which an ideal model will assign all the probability mass to\nthe reference summary. This assumption may lead to performance degradation\nduring inference, where the model needs to compare several system-generated\n(candidate) summaries that have deviated from the reference summary. To address\nthis problem, we propose a novel training paradigm which assumes a\nnon-deterministic distribution so that different candidate summaries are\nassigned probability mass according to their quality. Our method achieves a new\nstate-of-the-art result on the CNN/DailyMail (47.78 ROUGE-1) and XSum (49.07\nROUGE-1) datasets. Further analysis also shows that our model can estimate\nprobabilities of candidate summaries that are more correlated with their level\nof quality.", "published": "2022-03-31 05:19:38", "link": "http://arxiv.org/abs/2203.16804v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "indic-punct: An automatic punctuation restoration and inverse text\n  normalization framework for Indic languages", "abstract": "Automatic Speech Recognition (ASR) generates text which is most of the times\ndevoid of any punctuation. Absence of punctuation is text can affect\nreadability. Also, down stream NLP tasks such as sentiment analysis, machine\ntranslation, greatly benefit by having punctuation and sentence boundary\ninformation. We present an approach for automatic punctuation of text using a\npretrained IndicBERT model. Inverse text normalization is done by hand writing\nweighted finite state transducer (WFST) grammars. We have developed this tool\nfor 11 Indic languages namely Hindi, Tamil, Telugu, Kannada, Gujarati, Marathi,\nOdia, Bengali, Assamese, Malayalam and Punjabi. All code and data is publicly.\navailable", "published": "2022-03-31 06:18:43", "link": "http://arxiv.org/abs/2203.16825v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A bilingual approach to specialised adjectives through word embeddings\n  in the karstology domain", "abstract": "We present an experiment in extracting adjectives which express a specific\nsemantic relation using word embeddings. The results of the experiment are then\nthoroughly analysed and categorised into groups of adjectives exhibiting formal\nor semantic similarity. The experiment and analysis are performed for English\nand Croatian in the domain of karstology using data sets and methods developed\nin the TermFrame project. The main original contributions of the article are\ntwofold: firstly, proposing a new and promising method of extracting\nsemantically related words relevant for terminology, and secondly, providing a\ndetailed evaluation of the output so that we gain a better understanding of the\ndomain-specific semantic structures on the one hand and the types of\nsimilarities extracted by word embeddings on the other.", "published": "2022-03-31 08:27:15", "link": "http://arxiv.org/abs/2203.16885v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Adaptation for Sparse-Data Settings: What Do We Gain by Not Using\n  Bert?", "abstract": "The practical success of much of NLP depends on the availability of training\ndata. However, in real-world scenarios, training data is often scarce, not\nleast because many application domains are restricted and specific. In this\nwork, we compare different methods to handle this problem and provide\nguidelines for building NLP applications when there is only a small amount of\nlabeled training data available for a specific domain. While transfer learning\nwith pre-trained language models outperforms other methods across tasks,\nalternatives do not perform much worse while requiring much less computational\neffort, thus significantly reducing monetary and environmental cost. We examine\nthe performance tradeoffs of several such alternatives, including models that\ncan be trained up to 175K times faster and do not require a single GPU.", "published": "2022-03-31 09:59:08", "link": "http://arxiv.org/abs/2203.16926v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PanGu-Bot: Efficient Generative Dialogue Pre-training from Pre-trained\n  Language Model", "abstract": "In this paper, we introduce PanGu-Bot, a Chinese pre-trained open-domain\ndialogue generation model based on a large pre-trained language model (PLM)\nPANGU-alpha (Zeng et al.,2021). Different from other pre-trained dialogue\nmodels trained over a massive amount of dialogue data from scratch, we aim to\nbuild a powerful dialogue model with relatively fewer data and computation\ncosts by inheriting valuable language capabilities and knowledge from PLMs. To\nthis end, we train PanGu-Bot from the large PLM PANGU-alpha, which has been\nproven well-performed on a variety of Chinese natural language tasks. We\ninvestigate different aspects of responses generated by PanGu-Bot, including\nresponse quality, knowledge, and safety. We show that PanGu-Bot outperforms\nstate-of-the-art Chinese dialogue systems (CDIALGPT (Wang et al., 2020), EVA\n(Zhou et al., 2021), EVA2.0 (Gu et al., 2022)) w.r.t. the above three aspects.\nWe also demonstrate that PanGu-Bot can be easily deployed to generate emotional\nresponses without further training. Throughout our empirical analysis, we also\npoint out that the PanGu-Bot response quality, knowledge correctness, and\nsafety are still far from perfect, and further explorations are indispensable\nto building reliable and smart dialogue systems. Our model and code will be\navailable at\nhttps://github.com/huawei-noah/Pretrained-Language-Model/tree/master/PanGu-Bot\nsoon.", "published": "2022-03-31 15:09:12", "link": "http://arxiv.org/abs/2203.17090v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "$k$NN-NER: Named Entity Recognition with Nearest Neighbor Search", "abstract": "Inspired by recent advances in retrieval augmented methods in\nNLP~\\citep{khandelwal2019generalization,khandelwal2020nearest,meng2021gnn}, in\nthis paper, we introduce a $k$ nearest neighbor NER ($k$NN-NER) framework,\nwhich augments the distribution of entity labels by assigning $k$ nearest\nneighbors retrieved from the training set. This strategy makes the model more\ncapable of handling long-tail cases, along with better few-shot learning\nabilities. $k$NN-NER requires no additional operation during the training\nphase, and by interpolating $k$ nearest neighbors search into the vanilla NER\nmodel, $k$NN-NER consistently outperforms its vanilla counterparts: we achieve\na new state-of-the-art F1-score of 72.03 (+1.25) on the Chinese Weibo dataset\nand improved results on a variety of widely used NER benchmarks. Additionally,\nwe show that $k$NN-NER can achieve comparable results to the vanilla NER model\nwith 40\\% less amount of training data. Code available at\n\\url{https://github.com/ShannonAI/KNN-NER}.", "published": "2022-03-31 15:21:43", "link": "http://arxiv.org/abs/2203.17103v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Wrap-Up Effects through an Information-Theoretic Lens", "abstract": "Numerous analyses of reading time (RT) data have been implemented -- all in\nan effort to better understand the cognitive processes driving reading\ncomprehension. However, data measured on words at the end of a sentence -- or\neven at the end of a clause -- is often omitted due to the confounding factors\nintroduced by so-called \"wrap-up effects,\" which manifests as a skewed\ndistribution of RTs for these words. Consequently, the understanding of the\ncognitive processes that might be involved in these wrap-up effects is limited.\nIn this work, we attempt to learn more about these processes by examining the\nrelationship between wrap-up effects and information-theoretic quantities, such\nas word and context surprisals. We find that the distribution of information in\nprior contexts is often predictive of sentence- and clause-final RTs (while not\nof sentence-medial RTs). This lends support to several prior hypotheses about\nthe processes involved in wrap-up effects.", "published": "2022-03-31 17:41:03", "link": "http://arxiv.org/abs/2203.17213v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the probability-quality paradox in language generation", "abstract": "When generating natural language from neural probabilistic models, high\nprobability does not always coincide with high quality: It has often been\nobserved that mode-seeking decoding methods, i.e., those that produce\nhigh-probability text under the model, lead to unnatural language. On the other\nhand, the lower-probability text generated by stochastic methods is perceived\nas more human-like. In this note, we offer an explanation for this phenomenon\nby analyzing language generation through an information-theoretic lens.\nSpecifically, we posit that human-like language should contain an amount of\ninformation (quantified as negative log-probability) that is close to the\nentropy of the distribution over natural strings. Further, we posit that\nlanguage with substantially more (or less) information is undesirable. We\nprovide preliminary empirical evidence in favor of this hypothesis; quality\nratings of both human and machine-generated text -- covering multiple tasks and\ncommon decoding strategies -- suggest high-quality text has an information\ncontent significantly closer to the entropy than we would expect by chance.", "published": "2022-03-31 17:43:53", "link": "http://arxiv.org/abs/2203.17217v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Baseline Readability Model for Cebuano", "abstract": "In this study, we developed the first baseline readability model for the\nCebuano language. Cebuano is the second most-used native language in the\nPhilippines with about 27.5 million speakers. As the baseline, we extracted\ntraditional or surface-based features, syllable patterns based from Cebuano's\ndocumented orthography, and neural embeddings from the multilingual BERT model.\nResults show that the use of the first two handcrafted linguistic features\nobtained the best performance trained on an optimized Random Forest model with\napproximately 87% across all metrics. The feature sets and algorithm used also\nis similar to previous results in readability assessment for the Filipino\nlanguage showing potential of crosslingual application. To encourage more work\nfor readability assessment in Philippine languages such as Cebuano, we\nopen-sourced both code and data.", "published": "2022-03-31 17:49:11", "link": "http://arxiv.org/abs/2203.17225v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Single-Channel Speech for Multi-Channel End-to-End Speech\n  Recognition: A Comparative Study", "abstract": "Recently, the end-to-end training approach for multi-channel ASR has shown\nits effectiveness, which usually consists of a beamforming front-end and a\nrecognition back-end. However, the end-to-end training becomes more difficult\ndue to the integration of multiple modules, particularly considering that\nmulti-channel speech data recorded in real environments are limited in size.\nThis raises the demand to exploit the single-channel data for multi-channel\nend-to-end ASR. In this paper, we systematically compare the performance of\nthree schemes to exploit external single-channel data for multi-channel\nend-to-end ASR, namely back-end pre-training, data scheduling, and data\nsimulation, under different settings such as the sizes of the single-channel\ndata and the choices of the front-end. Extensive experiments on CHiME-4 and\nAISHELL-4 datasets demonstrate that while all three methods improve the\nmulti-channel end-to-end speech recognition performance, data simulation\noutperforms the other two, at the cost of longer training time. Data scheduling\noutperforms back-end pre-training marginally but nearly consistently,\npresumably because that in the pre-training stage, the back-end tends to\noverfit on the single-channel data, especially when the single-channel data\nsize is small.", "published": "2022-03-31 02:28:11", "link": "http://arxiv.org/abs/2203.16757v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "CUSIDE: Chunking, Simulating Future Context and Decoding for Streaming\n  ASR", "abstract": "History and future contextual information are known to be important for\naccurate acoustic modeling. However, acquiring future context brings latency\nfor streaming ASR. In this paper, we propose a new framework - Chunking,\nSimulating Future Context and Decoding (CUSIDE) for streaming speech\nrecognition. A new simulation module is introduced to recursively simulate the\nfuture contextual frames, without waiting for future context. The simulation\nmodule is jointly trained with the ASR model using a self-supervised loss; the\nASR model is optimized with the usual ASR loss, e.g., CTC-CRF as used in our\nexperiments. Experiments show that, compared to using real future frames as\nright context, using simulated future context can drastically reduce latency\nwhile maintaining recognition accuracy. With CUSIDE, we obtain new\nstate-of-the-art streaming ASR results on the AISHELL-1 dataset.", "published": "2022-03-31 02:28:48", "link": "http://arxiv.org/abs/2203.16758v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "ESGBERT: Language Model to Help with Classification Tasks Related to\n  Companies Environmental, Social, and Governance Practices", "abstract": "Environmental, Social, and Governance (ESG) are non-financial factors that\nare garnering attention from investors as they increasingly look to apply these\nas part of their analysis to identify material risks and growth opportunities.\nSome of this attention is also driven by clients who, now more aware than ever,\nare demanding for their money to be managed and invested responsibly. As the\ninterest in ESG grows, so does the need for investors to have access to\nconsumable ESG information. Since most of it is in text form in reports,\ndisclosures, press releases, and 10-Q filings, we see a need for sophisticated\nNLP techniques for classification tasks for ESG text. We hypothesize that an\nESG domain-specific pre-trained model will help with such and study building of\nthe same in this paper. We explored doing this by fine-tuning BERTs pre-trained\nweights using ESG specific text and then further fine-tuning the model for a\nclassification task. We were able to achieve accuracy better than the original\nBERT and baseline models in environment-specific classification tasks.", "published": "2022-03-31 04:22:44", "link": "http://arxiv.org/abs/2203.16788v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Open Source MagicData-RAMC: A Rich Annotated Mandarin\n  Conversational(RAMC) Speech Dataset", "abstract": "This paper introduces a high-quality rich annotated Mandarin conversational\n(RAMC) speech dataset called MagicData-RAMC. The MagicData-RAMC corpus contains\n180 hours of conversational speech data recorded from native speakers of\nMandarin Chinese over mobile phones with a sampling rate of 16 kHz. The dialogs\nin MagicData-RAMC are classified into 15 diversified domains and tagged with\ntopic labels, ranging from science and technology to ordinary life. Accurate\ntranscription and precise speaker voice activity timestamps are manually\nlabeled for each sample. Speakers' detailed information is also provided. As a\nMandarin speech dataset designed for dialog scenarios with high quality and\nrich annotations, MagicData-RAMC enriches the data diversity in the Mandarin\nspeech community and allows extensive research on a series of speech-related\ntasks, including automatic speech recognition, speaker diarization, topic\ndetection, keyword search, text-to-speech, etc. We also conduct several\nrelevant tasks and provide experimental results to help evaluate the dataset.", "published": "2022-03-31 07:01:06", "link": "http://arxiv.org/abs/2203.16844v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Memory-Efficient Training of RNN-Transducer with Sampled Softmax", "abstract": "RNN-Transducer has been one of promising architectures for end-to-end\nautomatic speech recognition. Although RNN-Transducer has many advantages\nincluding its strong accuracy and streaming-friendly property, its high memory\nconsumption during training has been a critical problem for development. In\nthis work, we propose to apply sampled softmax to RNN-Transducer, which\nrequires only a small subset of vocabulary during training thus saves its\nmemory consumption. We further extend sampled softmax to optimize memory\nconsumption for a minibatch, and employ distributions of auxiliary CTC losses\nfor sampling vocabulary to improve model accuracy. We present experimental\nresults on LibriSpeech, AISHELL-1, and CSJ-APS, where sampled softmax greatly\nreduces memory consumption and still maintains the accuracy of the baseline\nmodel.", "published": "2022-03-31 07:51:43", "link": "http://arxiv.org/abs/2203.16868v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Aplica\u00e7\u00e3o de ros como ferramenta de ensino a rob\u00f3tica / using\n  ros as a robotics teaching tool", "abstract": "The study of robotic manipulators is the main goal of Industrial Robotics\nClass, part of Control Engineers training course. There is a difficulty in\npreparing academic practices and projects in the area of robotics due to the\nhigh cost of specific educational equipment. The practical classes and the\ndevelopment of projects are very important for engineers training, it is\nproposed to use simulation software in order to provide practical experience\nfor the students of the discipline. In this context, the present article aims\nto expose the use of the Robot Operation System (ROS) as a tool to develop a\nrobotic arm and implement the functionality of forward and inverse kinematics.\nSuch development could be used as an educational tool to increase the interest\nand learning of students in the robotics discipline and to expand research\nareas for the discipline.", "published": "2022-03-31 09:48:21", "link": "http://arxiv.org/abs/2203.16923v1", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Applying PBL in the Development and Modeling of kinematics for Robotic\n  Manipulators with Interdisciplinarity between Computer-Assisted Project,\n  Robotics, and Microcontrollers", "abstract": "Considering the difficulty of students in calculating the direct and inverse\nkinematics of a robotic manipulator using only conventional tools of a\nclassroom, this article proposes the application of Project Based Learning\n(ABP) through the design, development, mathematical modeling of a robotic\nmanipulator as an integrative project of the disciplines of Industrial\nRobotics, Microcontrollers and Computer Assisted Design with students of the\nControl and Automation Engineering of the University of Fortaleza. Once\ndesigned and machined, the manipulator arm was assembled using servo motors\nconnected to a microcontroled prototyping board, to then have its kinematics\ncalculated. At the end are presented the results that the project has brought\nto the learning of the disciplines on the optics of the tutor and students.", "published": "2022-03-31 10:01:24", "link": "http://arxiv.org/abs/2203.16927v1", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Partial Coupling of Optimal Transport for Spoken Language Identification", "abstract": "In order to reduce domain discrepancy to improve the performance of\ncross-domain spoken language identification (SLID) system, as an unsupervised\ndomain adaptation (UDA) method, we have proposed a joint distribution alignment\n(JDA) model based on optimal transport (OT). A discrepancy measurement based on\nOT was adopted for JDA between training and test data sets. In our previous\nstudy, it was supposed that the training and test sets share the same label\nspace. However, in real applications, the label space of the test set is only a\nsubset of that of the training set. Fully matching training and test domains\nfor distribution alignment may introduce negative domain transfer. In this\npaper, we propose an JDA model based on partial optimal transport (POT), i.e.,\nonly partial couplings of OT are allowed during JDA. Moreover, since the label\nof test data is unknown, in the POT, a soft weighting on the coupling based on\ntransport cost is adaptively set during domain alignment. Experiments were\ncarried out on a cross-domain SLID task to evaluate the proposed UDA. Results\nshowed that our proposed UDA significantly improved the performance due to the\nconsideration of the partial couplings in OT.", "published": "2022-03-31 14:00:49", "link": "http://arxiv.org/abs/2203.17036v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Scientific and Technological Text Knowledge Extraction Method of based\n  on Word Mixing and GRU", "abstract": "The knowledge extraction task is to extract triple relations (head\nentity-relation-tail entity) from unstructured text data. The existing\nknowledge extraction methods are divided into \"pipeline\" method and joint\nextraction method. The \"pipeline\" method is to separate named entity\nrecognition and entity relationship extraction and use their own modules to\nextract them. Although this method has better flexibility, the training speed\nis slow. The learning model of joint extraction is an end-to-end model\nimplemented by neural network to realize entity recognition and relationship\nextraction at the same time, which can well preserve the association between\nentities and relationships, and convert the joint extraction of entities and\nrelationships into a sequence annotation problem. In this paper, we propose a\nknowledge extraction method for scientific and technological resources based on\nword mixture and GRU, combined with word mixture vector mapping method and\nself-attention mechanism, to effectively improve the effect of text\nrelationship extraction for Chinese scientific and technological resources.", "published": "2022-03-31 14:52:35", "link": "http://arxiv.org/abs/2203.17079v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Evaluation of NLP-based Models for Software Engineering", "abstract": "NLP-based models have been increasingly incorporated to address SE problems.\nThese models are either employed in the SE domain with little to no change, or\nthey are greatly tailored to source code and its unique characteristics. Many\nof these approaches are considered to be outperforming or complementing\nexisting solutions. However, an important question arises here: \"Are these\nmodels evaluated fairly and consistently in the SE community?\". To answer this\nquestion, we reviewed how NLP-based models for SE problems are being evaluated\nby researchers. The findings indicate that currently there is no consistent and\nwidely-accepted protocol for the evaluation of these models. While different\naspects of the same task are being assessed in different studies, metrics are\ndefined based on custom choices, rather than a system, and finally, answers are\ncollected and interpreted case by case. Consequently, there is a dire need to\nprovide a methodological way of evaluating NLP-based models to have a\nconsistent assessment and preserve the possibility of fair and efficient\ncomparison.", "published": "2022-03-31 16:42:19", "link": "http://arxiv.org/abs/2203.17166v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Scaling Up Models and Data with $\\texttt{t5x}$ and $\\texttt{seqio}$", "abstract": "Recent neural network-based language models have benefited greatly from\nscaling up the size of training datasets and the number of parameters in the\nmodels themselves. Scaling can be complicated due to various factors including\nthe need to distribute computation on supercomputer clusters (e.g., TPUs),\nprevent bottlenecks when infeeding data, and ensure reproducible results. In\nthis work, we present two software libraries that ease these issues:\n$\\texttt{t5x}$ simplifies the process of building and training large language\nmodels at scale while maintaining ease of use, and $\\texttt{seqio}$ provides a\ntask-based API for simple creation of fast and reproducible training data and\nevaluation pipelines. These open-source libraries have been used to train\nmodels with hundreds of billions of parameters on datasets with multiple\nterabytes of training data.\n  Along with the libraries, we release configurations and instructions for\nT5-like encoder-decoder models as well as GPT-like decoder-only architectures.\n  $\\texttt{t5x}$ and $\\texttt{seqio}$ are open source and available at\nhttps://github.com/google-research/t5x and https://github.com/google/seqio,\nrespectively.", "published": "2022-03-31 17:12:13", "link": "http://arxiv.org/abs/2203.17189v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme\n  Representations for Text to Speech", "abstract": "Recently, leveraging BERT pre-training to improve the phoneme encoder in text\nto speech (TTS) has drawn increasing attention. However, the works apply\npre-training with character-based units to enhance the TTS phoneme encoder,\nwhich is inconsistent with the TTS fine-tuning that takes phonemes as input.\nPre-training only with phonemes as input can alleviate the input mismatch but\nlack the ability to model rich representations and semantic information due to\nlimited phoneme vocabulary. In this paper, we propose MixedPhoneme BERT, a\nnovel variant of the BERT model that uses mixed phoneme and sup-phoneme\nrepresentations to enhance the learning capability. Specifically, we merge the\nadjacent phonemes into sup-phonemes and combine the phoneme sequence and the\nmerged sup-phoneme sequence as the model input, which can enhance the model\ncapacity to learn rich contextual representations. Experiment results\ndemonstrate that our proposed Mixed-Phoneme BERT significantly improves the TTS\nperformance with 0.30 CMOS gain compared with the FastSpeech 2 baseline. The\nMixed-Phoneme BERT achieves 3x inference speedup and similar voice quality to\nthe previous TTS pre-trained model PnG BERT", "published": "2022-03-31 17:12:26", "link": "http://arxiv.org/abs/2203.17190v3", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Cria\u00e7\u00e3o e aplica\u00e7\u00e3o de ferramenta para auxiliar no ensino de\n  algoritmos e programa\u00e7\u00e3o de computadores", "abstract": "Knowledge about programming is part of the knowledge matrix that will be\nrequired of the professionals of the future. Based on this, this work aims to\nreport the development of a teaching tool developed during the monitoring\nprogram of the Algorithm and Computer Programming discipline of the University\nof Fortaleza. The tool combines the knowledge acquired in the books, with a\nlanguage closer to the students, using video lessons and exercises proposed,\nwith all the content available on the internet. The preliminary results were\npositive, with the students approving this new approach and believing that it\ncould contribute to a better performance in the discipline.", "published": "2022-03-31 09:48:49", "link": "http://arxiv.org/abs/2204.01468v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Leveraging pre-trained language models for conversational information\n  seeking from text", "abstract": "Recent advances in Natural Language Processing, and in particular on the\nconstruction of very large pre-trained language representation models, is\nopening up new perspectives on the construction of conversational information\nseeking (CIS) systems. In this paper we investigate the usage of in-context\nlearning and pre-trained language representation models to address the problem\nof information extraction from process description documents, in an incremental\nquestion and answering oriented fashion. In particular we investigate the usage\nof the native GPT-3 (Generative Pre-trained Transformer 3) model, together with\ntwo in-context learning customizations that inject conceptual definitions and a\nlimited number of samples in a few shot-learning fashion. The results highlight\nthe potential of the approach and the usefulness of the in-context learning\ncustomizations, which can substantially contribute to address the \"training\ndata challenge\" of deep learning based NLP techniques the BPM field. It also\nhighlight the challenge posed by control flow relations for which further\ntraining needs to be devised.", "published": "2022-03-31 09:00:46", "link": "http://arxiv.org/abs/2204.03542v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generative Pre-Trained Transformers for Biologically Inspired Design", "abstract": "Biological systems in nature have evolved for millions of years to adapt and\nsurvive the environment. Many features they developed can be inspirational and\nbeneficial for solving technical problems in modern industries. This leads to a\nnovel form of design-by-analogy called bio-inspired design (BID). Although BID\nas a design method has been proven beneficial, the gap between biology and\nengineering continuously hinders designers from effectively applying the\nmethod. Therefore, we explore the recent advance of artificial intelligence\n(AI) for a computational approach to bridge the gap. This paper proposes a\ngenerative design approach based on the pre-trained language model (PLM) to\nautomatically retrieve and map biological analogy and generate BID in the form\nof natural language. The latest generative pre-trained transformer, namely\nGPT-3, is used as the base PLM. Three types of design concept generators are\nidentified and fine-tuned from the PLM according to the looseness of the\nproblem space representation. Machine evaluators are also fine-tuned to assess\nthe correlation between the domains within the generated BID concepts. The\napproach is then tested via a case study in which the fine-tuned models are\napplied to generate and evaluate light-weighted flying car concepts inspired by\nnature. The results show our approach can generate BID concepts with good\nperformance.", "published": "2022-03-31 11:13:22", "link": "http://arxiv.org/abs/2204.09714v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling Language Model Size in Cross-Device Federated Learning", "abstract": "Most studies in cross-device federated learning focus on small models, due to\nthe server-client communication and on-device computation bottlenecks. In this\nwork, we leverage various techniques for mitigating these bottlenecks to train\nlarger language models in cross-device federated learning. With systematic\napplications of partial model training, quantization, efficient transfer\nlearning, and communication-efficient optimizers, we are able to train a $21$M\nparameter Transformer and $20.2$M parameter Conformer that achieve the same or\nbetter perplexity as that of a similarly sized LSTM with $\\sim10\\times$ smaller\nclient-to-server communication cost and $11\\%$ lower perplexity than smaller\nLSTMs commonly studied in literature.", "published": "2022-03-31 15:51:53", "link": "http://arxiv.org/abs/2204.09715v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving speaker de-identification with functional data analysis of f0\n  trajectories", "abstract": "Due to a constantly increasing amount of speech data that is stored in\ndifferent types of databases, voice privacy has become a major concern. To\nrespond to such concern, speech researchers have developed various methods for\nspeaker de-identification. The state-of-the-art solutions utilize deep learning\nsolutions which can be effective but might be unavailable or impractical to\napply for, for example, under-resourced languages. Formant modification is a\nsimpler, yet effective method for speaker de-identification which requires no\ntraining data. Still, remaining intonational patterns in formant-anonymized\nspeech may contain speaker-dependent cues. This study introduces a novel\nspeaker de-identification method, which, in addition to simple formant shifts,\nmanipulates f0 trajectories based on functional data analysis. The proposed\nspeaker de-identification method will conceal plausibly identifying pitch\ncharacteristics in a phonetically controllable manner and improve formant-based\nspeaker de-identification up to 25%.", "published": "2022-03-31 01:34:15", "link": "http://arxiv.org/abs/2203.16738v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SpeechPrompt: An Exploration of Prompt Tuning on Generative Spoken\n  Language Model for Speech Processing Tasks", "abstract": "Speech representations learned from Self-supervised learning (SSL) models can\nbenefit various speech processing tasks. However, utilizing SSL representations\nusually requires fine-tuning the pre-trained models or designing task-specific\ndownstream models and loss functions, causing much memory usage and human\nlabor. Recently, prompting in Natural Language Processing (NLP) has been found\nto be an efficient technique to leverage pre-trained language models (LMs).\nSpecifically, prompt tuning optimizes a limited number of task-specific\nparameters with a fixed pre-trained model; as a result, only a small set of\nparameters is needed to be stored for each task. Prompt tuning improves\ncomputation and memory efficiency by leveraging the pre-trained LM's prediction\nability. Nevertheless, such a paradigm is little studied in the speech\ncommunity. We report in this paper the first exploration of the prompt tuning\nparadigm for speech processing tasks based on Generative Spoken Language Model\n(GSLM). Experiment results show that the prompt tuning technique achieves\ncompetitive performance in speech classification tasks with fewer trainable\nparameters than fine-tuning specialized downstream models. We further study the\ntechnique in challenging sequence generation tasks. Prompt tuning also\ndemonstrates its potential, while the limitation and possible research\ndirections are discussed in this paper. The source code is available on\nhttps://github.com/ga642381/SpeechPrompt.", "published": "2022-03-31 03:26:55", "link": "http://arxiv.org/abs/2203.16773v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Bangla hate speech detection on social media using attention-based\n  recurrent neural network", "abstract": "Hate speech has spread more rapidly through the daily use of technology and,\nmost notably, by sharing your opinions or feelings on social media in a\nnegative aspect. Although numerous works have been carried out in detecting\nhate speeches in English, German, and other languages, very few works have been\ncarried out in the context of the Bengali language. In contrast, millions of\npeople communicate on social media in Bengali. The few existing works that have\nbeen carried out need improvements in both accuracy and interpretability. This\narticle proposed encoder decoder based machine learning model, a popular tool\nin NLP, to classify user's Bengali comments on Facebook pages. A dataset of\n7,425 Bengali comments, consisting of seven distinct categories of hate\nspeeches, was used to train and evaluate our model. For extracting and encoding\nlocal features from the comments, 1D convolutional layers were used. Finally,\nthe attention mechanism, LSTM, and GRU based decoders have been used for\npredicting hate speech categories. Among the three encoder decoder algorithms,\nthe attention-based decoder obtained the best accuracy (77%).", "published": "2022-03-31 03:31:53", "link": "http://arxiv.org/abs/2203.16775v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Language Model Integration for Transducer based\n  Speech Recognition", "abstract": "Utilizing text-only data with an external language model (ELM) in end-to-end\nRNN-Transducer (RNN-T) for speech recognition is challenging. Recently, a class\nof methods such as density ratio (DR) and internal language model estimation\n(ILME) have been developed, outperforming the classic shallow fusion (SF)\nmethod. The basic idea behind these methods is that RNN-T posterior should\nfirst subtract the implicitly learned internal language model (ILM) prior, in\norder to integrate the ELM. While recent studies suggest that RNN-T only learns\nsome low-order language model information, the DR method uses a well-trained\nneural language model with full context, which may be inappropriate for the\nestimation of ILM and deteriorate the integration performance. Based on the DR\nmethod, we propose a low-order density ratio method (LODR) by replacing the\nestimation with a low-order weak language model. Extensive empirical\nexperiments are conducted on both in-domain and cross-domain scenarios on\nEnglish LibriSpeech & Tedlium-2 and Chinese WenetSpeech & AISHELL-1 datasets.\nIt is shown that LODR consistently outperforms SF in all tasks, while\nperforming generally close to ILME and better than DR in most tests.", "published": "2022-03-31 03:33:50", "link": "http://arxiv.org/abs/2203.16776v4", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "MMER: Multimodal Multi-task Learning for Speech Emotion Recognition", "abstract": "In this paper, we propose MMER, a novel Multimodal Multi-task learning\napproach for Speech Emotion Recognition. MMER leverages a novel multimodal\nnetwork based on early-fusion and cross-modal self-attention between text and\nacoustic modalities and solves three novel auxiliary tasks for learning emotion\nrecognition from spoken utterances. In practice, MMER outperforms all our\nbaselines and achieves state-of-the-art performance on the IEMOCAP benchmark.\nAdditionally, we conduct extensive ablation studies and results analysis to\nprove the effectiveness of our proposed approach.", "published": "2022-03-31 04:51:32", "link": "http://arxiv.org/abs/2203.16794v5", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "How Does Pre-trained Wav2Vec 2.0 Perform on Domain Shifted ASR? An\n  Extensive Benchmark on Air Traffic Control Communications", "abstract": "Recent work on self-supervised pre-training focus on leveraging large-scale\nunlabeled speech data to build robust end-to-end (E2E) acoustic models (AM)\nthat can be later fine-tuned on downstream tasks e.g., automatic speech\nrecognition (ASR). Yet, few works investigated the impact on performance when\nthe data properties substantially differ between the pre-training and\nfine-tuning phases, termed domain shift. We target this scenario by analyzing\nthe robustness of Wav2Vec 2.0 and XLS-R models on downstream ASR for a\ncompletely unseen domain, air traffic control (ATC) communications. We\nbenchmark these two models on several open-source and challenging ATC databases\nwith signal-to-noise ratio between 5 and 20 dB. Relative word error rate (WER)\nreductions between 20% to 40% are obtained in comparison to hybrid-based ASR\nbaselines by only fine-tuning E2E acoustic models with a smaller fraction of\nlabeled data. We analyze WERs on the low-resource scenario and gender bias\ncarried by one ATC dataset.", "published": "2022-03-31 06:10:42", "link": "http://arxiv.org/abs/2203.16822v2", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Effectiveness of text to speech pseudo labels for forced alignment and\n  cross lingual pretrained models for low resource speech recognition", "abstract": "In the recent years end to end (E2E) automatic speech recognition (ASR)\nsystems have achieved promising results given sufficient resources. Even for\nlanguages where not a lot of labelled data is available, state of the art E2E\nASR systems can be developed by pretraining on huge amounts of high resource\nlanguages and finetune on low resource languages. For a lot of low resource\nlanguages the current approaches are still challenging, since in many cases\nlabelled data is not available in open domain. In this paper we present an\napproach to create labelled data for Maithili, Bhojpuri and Dogri by utilising\npseudo labels from text to speech for forced alignment. The created data was\ninspected for quality and then further used to train a transformer based\nwav2vec 2.0 ASR model. All data and models are available in open domain.", "published": "2022-03-31 06:12:52", "link": "http://arxiv.org/abs/2203.16823v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Comparative Study on Speaker-attributed Automatic Speech Recognition\n  in Multi-party Meetings", "abstract": "In this paper, we conduct a comparative study on speaker-attributed automatic\nspeech recognition (SA-ASR) in the multi-party meeting scenario, a topic with\nincreasing attention in meeting rich transcription. Specifically, three\napproaches are evaluated in this study. The first approach, FD-SOT, consists of\na frame-level diarization model to identify speakers and a multi-talker ASR to\nrecognize utterances. The speaker-attributed transcriptions are obtained by\naligning the diarization results and recognized hypotheses. However, such an\nalignment strategy may suffer from erroneous timestamps due to the modular\nindependence, severely hindering the model performance. Therefore, we propose\nthe second approach, WD-SOT, to address alignment errors by introducing a\nword-level diarization model, which can get rid of such timestamp alignment\ndependency. To further mitigate the alignment issues, we propose the third\napproach, TS-ASR, which trains a target-speaker separation module and an ASR\nmodule jointly. By comparing various strategies for each SA-ASR approach,\nexperimental results on a real meeting scenario corpus, AliMeeting, reveal that\nthe WD-SOT approach achieves 10.7% relative reduction on averaged\nspeaker-dependent character error rate (SD-CER), compared with the FD-SOT\napproach. In addition, the TS-ASR approach also outperforms the FD-SOT approach\nand brings 16.5% relative average SD-CER reduction.", "published": "2022-03-31 06:39:14", "link": "http://arxiv.org/abs/2203.16834v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A survey of neural models for the automatic analysis of conversation:\n  Towards a better integration of the social sciences", "abstract": "Some exciting new approaches to neural architectures for the analysis of\nconversation have been introduced over the past couple of years. These include\nneural architectures for detecting emotion, dialogue acts, and sentiment\npolarity. They take advantage of some of the key attributes of contemporary\nmachine learning, such as recurrent neural networks with attention mechanisms\nand transformer-based approaches. However, while the architectures themselves\nare extremely promising, the phenomena they have been applied to to date are\nbut a small part of what makes conversation engaging. In this paper we survey\nthese neural architectures and what they have been applied to. On the basis of\nthe social science literature, we then describe what we believe to be the most\nfundamental and definitional feature of conversation, which is its\nco-construction over time by two or more interlocutors. We discuss how neural\narchitectures of the sort surveyed could profitably be applied to these more\nfundamental aspects of conversation, and what this buys us in terms of a better\nanalysis of conversation and even, in the longer term, a better way of\ngenerating conversation for a conversational system.", "published": "2022-03-31 08:59:54", "link": "http://arxiv.org/abs/2203.16891v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Character-level Span-based Model for Mandarin Prosodic Structure\n  Prediction", "abstract": "The accuracy of prosodic structure prediction is crucial to the naturalness\nof synthesized speech in Mandarin text-to-speech system, but now is limited by\nwidely-used sequence-to-sequence framework and error accumulation from previous\nword segmentation results. In this paper, we propose a span-based Mandarin\nprosodic structure prediction model to obtain an optimal prosodic structure\ntree, which can be converted to corresponding prosodic label sequence. Instead\nof the prerequisite for word segmentation, rich linguistic features are\nprovided by Chinese character-level BERT and sent to encoder with\nself-attention architecture. On top of this, span representation and label\nscoring are used to describe all possible prosodic structure trees, of which\neach tree has its corresponding score. To find the optimal tree with the\nhighest score for a given sentence, a bottom-up CKY-style algorithm is further\nused. The proposed method can predict prosodic labels of different levels at\nthe same time and accomplish the process directly from Chinese characters in an\nend-to-end manner. Experiment results on two real-world datasets demonstrate\nthe excellent performance of our span-based method over all\nsequence-to-sequence baseline approaches.", "published": "2022-03-31 09:47:08", "link": "http://arxiv.org/abs/2203.16922v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Neural Architecture Search for Speech Emotion Recognition", "abstract": "Deep neural networks have brought significant advancements to speech emotion\nrecognition (SER). However, the architecture design in SER is mainly based on\nexpert knowledge and empirical (trial-and-error) evaluations, which is\ntime-consuming and resource intensive. In this paper, we propose to apply\nneural architecture search (NAS) techniques to automatically configure the SER\nmodels. To accelerate the candidate architecture optimization, we propose a\nuniform path dropout strategy to encourage all candidate architecture\noperations to be equally optimized. Experimental results of two different\nneural structures on IEMOCAP show that NAS can improve SER performance (54.89\\%\nto 56.28\\%) while maintaining model parameter sizes. The proposed dropout\nstrategy also shows superiority over the previous approaches.", "published": "2022-03-31 10:16:10", "link": "http://arxiv.org/abs/2203.16928v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WavThruVec: Latent speech representation as intermediate features for\n  neural speech synthesis", "abstract": "Recent advances in neural text-to-speech research have been dominated by\ntwo-stage pipelines utilizing low-level intermediate speech representation such\nas mel-spectrograms. However, such predetermined features are fundamentally\nlimited, because they do not allow to exploit the full potential of a\ndata-driven approach through learning hidden representations. For this reason,\nseveral end-to-end methods have been proposed. However, such models are harder\nto train and require a large number of high-quality recordings with\ntranscriptions. Here, we propose WavThruVec - a two-stage architecture that\nresolves the bottleneck by using high-dimensional Wav2Vec 2.0 embeddings as\nintermediate speech representation. Since these hidden activations provide\nhigh-level linguistic features, they are more robust to noise. That allows us\nto utilize annotated speech datasets of a lower quality to train the\nfirst-stage module. At the same time, the second-stage component can be trained\non large-scale untranscribed audio corpora, as Wav2Vec 2.0 embeddings are\nalready time-aligned. This results in an increased generalization capability to\nout-of-vocabulary words, as well as to a better generalization to unseen\nspeakers. We show that the proposed model not only matches the quality of\nstate-of-the-art neural models, but also presents useful properties enabling\ntasks like voice conversion or zero-shot synthesis.", "published": "2022-03-31 10:21:08", "link": "http://arxiv.org/abs/2203.16930v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An End-to-end Chinese Text Normalization Model based on Rule-guided\n  Flat-Lattice Transformer", "abstract": "Text normalization, defined as a procedure transforming non standard words to\nspoken-form words, is crucial to the intelligibility of synthesized speech in\ntext-to-speech system. Rule-based methods without considering context can not\neliminate ambiguation, whereas sequence-to-sequence neural network based\nmethods suffer from the unexpected and uninterpretable errors problem. Recently\nproposed hybrid system treats rule-based model and neural model as two cascaded\nsub-modules, where limited interaction capability makes neural network model\ncannot fully utilize expert knowledge contained in the rules. Inspired by\nFlat-LAttice Transformer (FLAT), we propose an end-to-end Chinese text\nnormalization model, which accepts Chinese characters as direct input and\nintegrates expert knowledge contained in rules into the neural network, both\ncontribute to the superior performance of proposed model for the text\nnormalization task. We also release a first publicly accessible largescale\ndataset for Chinese text normalization. Our proposed model has achieved\nexcellent results on this dataset.", "published": "2022-03-31 11:19:53", "link": "http://arxiv.org/abs/2203.16954v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "PADA: Pruning Assisted Domain Adaptation for Self-Supervised Speech\n  Representations", "abstract": "While self-supervised speech representation learning (SSL) models serve a\nvariety of downstream tasks, these models have been observed to overfit to the\ndomain from which the unlabelled data originates. To alleviate this issue, we\npropose PADA (Pruning Assisted Domain Adaptation) and zero out redundant\nweights from models pre-trained on large amounts of out-of-domain (OOD) data.\nIntuitively, this helps to make space for the target-domain ASR finetuning. The\nredundant weights can be identified through various pruning strategies which\nhave been discussed in detail as a part of this work. Specifically, we\ninvestigate the effect of the recently discovered Task-Agnostic and Task-Aware\npruning on PADA and propose a new pruning paradigm based on the latter, which\nwe call Cross-Domain Task-Aware Pruning (CD-TAW). CD-TAW obtains the initial\npruning mask from a well fine-tuned OOD model, which makes it starkly different\nfrom the rest of the pruning strategies discussed in the paper. Our proposed\nCD-TAW methodology achieves up to 20.6% relative WER improvement over our\nbaseline when fine-tuned on a 2-hour subset of Switchboard data without\nlanguage model (LM) decoding. Furthermore, we conduct a detailed analysis to\nhighlight the key design choices of our proposed method.", "published": "2022-03-31 11:34:58", "link": "http://arxiv.org/abs/2203.16965v4", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Analyzing the factors affecting usefulness of Self-Supervised\n  Pre-trained Representations for Speech Recognition", "abstract": "Self-supervised learning (SSL) to learn high-level speech representations has\nbeen a popular approach to building Automatic Speech Recognition (ASR) systems\nin low-resource settings. However, the common assumption made in literature is\nthat a considerable amount of unlabeled data is available for the same domain\nor language that can be leveraged for SSL pre-training, which we acknowledge is\nnot feasible in a real-world setting. In this paper, as part of the Interspeech\nGram Vaani ASR challenge, we try to study the effect of domain, language,\ndataset size, and other aspects of our upstream pre-training SSL data on the\nfinal performance low-resource downstream ASR task. We also build on the\ncontinued pre-training paradigm to study the effect of prior knowledge\npossessed by models trained using SSL. Extensive experiments and studies reveal\nthat the performance of ASR systems is susceptible to the data used for SSL\npre-training. Their performance improves with an increase in similarity and\nvolume of pre-training data. We believe our work will be helpful to the speech\ncommunity in building better ASR systems in low-resource settings and steer\nresearch towards improving generalization in SSL-based pre-training for speech\nsystems.", "published": "2022-03-31 11:48:24", "link": "http://arxiv.org/abs/2203.16973v4", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Manipulation of oral cancer speech using neural articulatory synthesis", "abstract": "We present an articulatory synthesis framework for the synthesis and\nmanipulation of oral cancer speech for clinical decision making and alleviation\nof patient stress. Objective and subjective evaluations demonstrate that the\nframework has acceptable naturalness and is worth further investigation. A\nsubsequent subjective vowel and consonant identification experiment showed that\nthe articulatory synthesis system can manipulate the articulatory trajectories\nso that the synthesised speech reproduces problems present in the ground truth\noral cancer speech.", "published": "2022-03-31 14:40:51", "link": "http://arxiv.org/abs/2203.17072v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interpretation of Black Box NLP Models: A Survey", "abstract": "An increasing number of machine learning models have been deployed in domains\nwith high stakes such as finance and healthcare. Despite their superior\nperformances, many models are black boxes in nature which are hard to explain.\nThere are growing efforts for researchers to develop methods to interpret these\nblack-box models. Post hoc explanations based on perturbations, such as LIME,\nare widely used approaches to interpret a machine learning model after it has\nbeen built. This class of methods has been shown to exhibit large instability,\nposing serious challenges to the effectiveness of the method itself and harming\nuser trust. In this paper, we propose S-LIME, which utilizes a hypothesis\ntesting framework based on central limit theorem for determining the number of\nperturbation points needed to guarantee stability of the resulting explanation.\nExperiments on both simulated and real world data sets are provided to\ndemonstrate the effectiveness of our method.", "published": "2022-03-31 14:54:35", "link": "http://arxiv.org/abs/2203.17081v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Impact of Environmental Noise on Alzheimer's Disease Detection from\n  Speech: Should You Let a Baby Cry?", "abstract": "Research related to automatically detecting Alzheimer's disease (AD) is\nimportant, given the high prevalence of AD and the high cost of traditional\nmethods. Since AD significantly affects the acoustics of spontaneous speech,\nspeech processing and machine learning (ML) provide promising techniques for\nreliably detecting AD. However, speech audio may be affected by different types\nof background noise and it is important to understand how the noise influences\nthe accuracy of ML models detecting AD from speech. In this paper, we study the\neffect of fifteen types of environmental noise from five different categories\non the performance of four ML models trained with three types of acoustic\nrepresentations. We perform a thorough analysis showing how ML models and\nacoustic features are affected by different types of acoustic noise. We show\nthat acoustic noise is not necessarily harmful - certain types of noise are\nbeneficial for AD detection models and help increasing accuracy by up to 4.8\\%.\nWe provide recommendations on how to utilize acoustic noise in order to achieve\nthe best performance results with the ML models deployed in real world.", "published": "2022-03-31 15:30:28", "link": "http://arxiv.org/abs/2203.17110v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Perceptual Contrast Stretching on Target Feature for Speech Enhancement", "abstract": "Speech enhancement (SE) performance has improved considerably owing to the\nuse of deep learning models as a base function. Herein, we propose a perceptual\ncontrast stretching (PCS) approach to further improve SE performance. The PCS\nis derived based on the critical band importance function and is applied to\nmodify the targets of the SE model. Specifically, the contrast of target\nfeatures is stretched based on perceptual importance, thereby improving the\noverall SE performance. Compared with post-processing-based implementations,\nincorporating PCS into the training phase preserves performance and reduces\nonline computation. Notably, PCS can be combined with different SE model\narchitectures and training criteria. Furthermore, PCS does not affect the\ncausality or convergence of SE model training. Experimental results on the\nVoiceBank-DEMAND dataset show that the proposed method can achieve\nstate-of-the-art performance on both causal (PESQ score = 3.07) and noncausal\n(PESQ score = 3.35) SE tasks.", "published": "2022-03-31 16:24:51", "link": "http://arxiv.org/abs/2203.17152v4", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CatIss: An Intelligent Tool for Categorizing Issues Reports using\n  Transformers", "abstract": "Users use Issue Tracking Systems to keep track and manage issue reports in\ntheir repositories. An issue is a rich source of software information that\ncontains different reports including a problem, a request for new features, or\nmerely a question about the software product. As the number of these issues\nincreases, it becomes harder to manage them manually. Thus, automatic\napproaches are proposed to help facilitate the management of issue reports.\n  This paper describes CatIss, an automatic CATegorizer of ISSue reports which\nis built upon the Transformer-based pre-trained RoBERTa model. CatIss\nclassifies issue reports into three main categories of Bug reports,\nEnhancement/feature requests, and Questions. First, the datasets provided for\nthe NLBSE tool competition are cleaned and preprocessed. Then, the pre-trained\nRoBERTa model is fine-tuned on the preprocessed dataset. Evaluating CatIss on\nabout 80 thousand issue reports from GitHub, indicates that it performs very\nwell surpassing the competition baseline, TicketTagger, and achieving 87.2%\nF1-score (micro average). Additionally, as CatIss is trained on a wide set of\nrepositories, it is a generic prediction model, hence applicable for any unseen\nsoftware project or projects with little historical data. Scripts for cleaning\nthe datasets, training CatIss, and evaluating the model are publicly available.", "published": "2022-03-31 17:20:58", "link": "http://arxiv.org/abs/2203.17196v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Data-augmented cross-lingual synthesis in a teacher-student framework", "abstract": "Cross-lingual synthesis can be defined as the task of letting a speaker\ngenerate fluent synthetic speech in another language. This is a challenging\ntask, and resulting speech can suffer from reduced naturalness, accented\nspeech, and/or loss of essential voice characteristics. Previous research shows\nthat many models appear to have insufficient generalization capabilities to\nperform well on every of these cross-lingual aspects. To overcome these\ngeneralization problems, we propose to apply the teacher-student paradigm to\ncross-lingual synthesis. While a teacher model is commonly used to produce\nteacher forced data, we propose to also use it to produce augmented data of\nunseen speaker-language pairs, where the aim is to retain essential speaker\ncharacteristics. Both sets of data are then used for student model training,\nwhich is trained to retain the naturalness and prosodic variation present in\nthe teacher forced data, while learning the speaker identity from the augmented\ndata. Some modifications to the student model are proposed to make the\nseparation of teacher forced and augmented data more straightforward. Results\nshow that the proposed approach improves the retention of speaker\ncharacteristics in the speech, while managing to retain high levels of\nnaturalness and prosodic variation.", "published": "2022-03-31 20:01:32", "link": "http://arxiv.org/abs/2204.00061v1", "categories": ["cs.SD", "cs.CL", "eess.AS", "I.2.7"], "primary_category": "cs.SD"}
{"title": "Hierarchical Attention Network for Evaluating Therapist Empathy in\n  Counseling Session", "abstract": "Counseling typically takes the form of spoken conversation between a\ntherapist and a client. The empathy level expressed by the therapist is\nconsidered to be an essential quality factor of counseling outcome. This paper\nproposes a hierarchical recurrent network combined with two-level attention\nmechanisms to determine the therapist's empathy level solely from the acoustic\nfeatures of conversational speech in a counseling session. The experimental\nresults show that the proposed model can achieve an accuracy of $72.1\\%$ in\nclassifying the therapist's empathy level as being ``high\" or ``low\". It is\nfound that the speech from both the therapist and the client are contributing\nto predicting the empathy level that is subjectively rated by an expert\nobserver. By analyzing speaker turns assigned with high attention weights, it\nis observed that $2$ to $6$ consecutive turns should be considered together to\nprovide useful clues for detecting empathy, and the observer tends to take the\nwhole session into consideration when rating the therapist empathy, instead of\nrelying on a few specific speaker turns.", "published": "2022-03-31 07:09:05", "link": "http://arxiv.org/abs/2203.16847v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Improving Language Identification of Accented Speech", "abstract": "Language identification from speech is a common preprocessing step in many\nspoken language processing systems. In recent years, this field has seen fast\nprogress, mostly due to the use of self-supervised models pretrained on\nmultilingual data and the use of large training corpora. This paper shows that\nfor speech with a non-native or regional accent, the accuracy of spoken\nlanguage identification systems drops dramatically, and that the accuracy of\nidentifying the language is inversely correlated with the strength of the\naccent. We also show that using the output of a lexicon-free speech recognition\nsystem of the particular language helps to improve language identification\nperformance on accented speech by a large margin, without sacrificing accuracy\non native speech. We obtain relative error rate reductions ranging from to 35\nto 63% over the state-of-the-art model across several non-native speech\ndatasets.", "published": "2022-03-31 11:48:04", "link": "http://arxiv.org/abs/2203.16972v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Efficient Non-Autoregressive GAN Voice Conversion using VQWav2vec\n  Features and Dynamic Convolution", "abstract": "It was shown recently that a combination of ASR and TTS models yield highly\ncompetitive performance on standard voice conversion tasks such as the Voice\nConversion Challenge 2020 (VCC2020). To obtain good performance both models\nrequire pretraining on large amounts of data, thereby obtaining large models\nthat are potentially inefficient in use. In this work we present a model that\nis significantly smaller and thereby faster in processing while obtaining\nequivalent performance. To achieve this the proposed model, Dynamic-GAN-VC\n(DYGAN-VC), uses a non-autoregressive structure and makes use of vector\nquantised embeddings obtained from a VQWav2vec model. Furthermore dynamic\nconvolution is introduced to improve speech content modeling while requiring a\nsmall number of parameters. Objective and subjective evaluation was performed\nusing the VCC2020 task, yielding MOS scores of up to 3.86, and character error\nrates as low as 4.3\\%. This was achieved with approximately half the number of\nmodel parameters, and up to 8 times faster decoding speed.", "published": "2022-03-31 16:46:32", "link": "http://arxiv.org/abs/2203.17172v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Effective data screening technique for crowdsourced speech\n  intelligibility experiments: Evaluation with IRM-based speech enhancement", "abstract": "It is essential to perform speech intelligibility (SI) experiments with human\nlisteners in order to evaluate objective intelligibility measures for\ndeveloping effective speech enhancement and noise reduction algorithms.\nRecently, crowdsourced remote testing has become a popular means for collecting\na massive amount and variety of data at a relatively small cost and in a short\ntime. However, careful data screening is essential for attaining reliable SI\ndata. We performed SI experiments on speech enhanced by an \"oracle\" ideal ratio\nmask (IRM) in a well-controlled laboratory and in crowdsourced remote\nenvironments that could not be controlled directly. We introduced simple tone\npip tests, in which participants were asked to report the number of audible\ntone pips, to estimate their listening levels above audible thresholds. The\ntone pip tests were very effective for data screening to reduce the variability\nof crowdsourced remote results so that the laboratory results would become\nsimilar. The results also demonstrated the SI of an oracle IRM, giving us the\nupper limit of the mask-based single-channel speech enhancement.", "published": "2022-03-31 02:30:29", "link": "http://arxiv.org/abs/2203.16760v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NeuFA: Neural Network Based End-to-End Forced Alignment with\n  Bidirectional Attention Mechanism", "abstract": "Although deep learning and end-to-end models have been widely used and shown\nsuperiority in automatic speech recognition (ASR) and text-to-speech (TTS)\nsynthesis, state-of-the-art forced alignment (FA) models are still based on\nhidden Markov model (HMM). HMM has limited view of contextual information and\nis developed with long pipelines, leading to error accumulation and\nunsatisfactory performance. Inspired by the capability of attention mechanism\nin capturing long term contextual information and learning alignments in ASR\nand TTS, we propose a neural network based end-to-end forced aligner called\nNeuFA, in which a novel bidirectional attention mechanism plays an essential\nrole. NeuFA integrates the alignment learning of both ASR and TTS tasks in a\nunified framework by learning bidirectional alignment information from a shared\nattention matrix in the proposed bidirectional attention mechanism. Alignments\nare extracted from the learnt attention weights and optimized by the ASR, TTS\nand FA tasks in a multi-task learning manner. Experimental results demonstrate\nthe effectiveness of our proposed model, with mean absolute error on test set\ndrops from 25.8 ms to 23.7 ms at word level, and from 17.0 ms to 15.7 ms at\nphoneme level compared with state-of-the-art HMM based model.", "published": "2022-03-31 06:45:39", "link": "http://arxiv.org/abs/2203.16838v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Hybrid Continuity Loss to Reduce Over-Suppression for Time-domain\n  Target Speaker Extraction", "abstract": "The speaker extraction algorithm extracts the target speech from a mixture\nspeech containing interference speech and background noise. The extraction\nprocess sometimes over-suppresses the extracted target speech, which not only\ncreates artifacts during listening but also harms the performance of downstream\nautomatic speech recognition algorithms. We propose a hybrid continuity loss\nfunction for time-domain speaker extraction algorithms to settle the\nover-suppression problem. On top of the waveform-level loss used for superior\nsignal quality, i.e., SI-SDR, we introduce a multi-resolution delta spectrum\nloss in the frequency-domain, to ensure the continuity of an extracted speech\nsignal, thus alleviating the over-suppression. We examine the hybrid continuity\nloss function using a time-domain audio-visual speaker extraction algorithm on\nthe YouTube LRS2-BBC dataset. Experimental results show that the proposed loss\nfunction reduces the over-suppression and improves the word error rate of\nspeech recognition on both clean and noisy two-speakers mixtures, without\nharming the reconstructed speech quality.", "published": "2022-03-31 06:58:45", "link": "http://arxiv.org/abs/2203.16843v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A comparative study between linear and nonlinear speech prediction", "abstract": "This paper is focused on nonlinear prediction coding, which consists on the\nprediction of a speech sample based on a nonlinear combination of previous\nsamples. It is known that in the generation of the glottal pulse, the wave\nequation does not behave linearly [2], [10], and we model these effects by\nmeans of a nonlinear prediction of speech based on a parametric neural network\nmodel. This work is centred on the neural net weight's quantization and on the\ncompression gain.", "published": "2022-03-31 11:32:48", "link": "http://arxiv.org/abs/2203.16962v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Comparative Study of Fusion Methods for SASV Challenge 2022", "abstract": "Automatic Speaker Verification (ASV) system is a type of bio-metric\nauthentication. It can be attacked by an intruder, who falsifies data in order\nto get access to protected information. Countermeasures (CM) are special\nalgorithms that detect these spoofing-attacks. While the ASVspoof Challenge\nseries were focused on the development of CM for fixed ASV system, the new\nSpoofing Aware Speaker Verification (SASV) Challenge organizers believe that\nbest results can be achieved if CM and ASV systems are optimized jointly. One\nof the approaches for cooperative optimization is a fusion over embeddings or\nscores obtained from ASV and CM models. The baselines of SASV Challenge 2022\npresent two types of fusion: score-sum and back-end ensemble with a 3-layer\nMLP. This paper describes our research of other fusion methods, including\nboosting over embeddings, which has not been used in anti-spoofing studies\nbefore.", "published": "2022-03-31 11:43:01", "link": "http://arxiv.org/abs/2203.16970v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EEND-SS: Joint End-to-End Neural Speaker Diarization and Speech\n  Separation for Flexible Number of Speakers", "abstract": "In this paper, we present a novel framework that jointly performs three\ntasks: speaker diarization, speech separation, and speaker counting. Our\nproposed framework integrates speaker diarization based on end-to-end neural\ndiarization (EEND) models, speaker counting with encoder-decoder based\nattractors (EDA), and speech separation using Conv-TasNet. In addition, we\npropose a multiple 1x1 convolutional layer architecture for estimating the\nseparation masks corresponding to a flexible number of speakers and a fusion\ntechnique for refining the separated speech signal with obtained speaker\ndiarization information to improve the joint framework. Experiments using the\nLibriMix dataset show that our proposed method outperforms the single-task\nbaselines in both diarization and separation metrics for fixed and flexible\nnumbers of speakers and improves speaker counting performance for flexible\nnumbers of speakers. All materials will be open-sourced and reproducible in\nESPnet toolkit.", "published": "2022-03-31 14:36:00", "link": "http://arxiv.org/abs/2203.17068v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Importance of Different Temporal Modulations of Speech: A Tale of Two\n  Perspectives", "abstract": "How important are different temporal speech modulations for speech\nrecognition? We answer this question from two complementary perspectives.\nFirstly, we quantify the amount of phonetic \\textit{information} in the\nmodulation spectrum of speech by computing the mutual information between\ntemporal modulations with frame-wise phoneme labels. Looking from another\nperspective, we ask - which speech modulations an Automatic Speech Recognition\n(ASR) system prefers for its operation. Data-driven weights are learned over\nthe modulation spectrum and optimized for an end-to-end ASR task. Both methods\nunanimously agree that speech information is mostly contained in slow\nmodulation. Maximum mutual information occurs around 3-6 Hz which also happens\nto be the range of modulations most preferred by the ASR. In addition, we show\nthat the incorporation of this knowledge into ASRs significantly reduces their\ndependency on the amount of training data.", "published": "2022-03-31 20:09:37", "link": "http://arxiv.org/abs/2204.00065v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SpecGrad: Diffusion Probabilistic Model based Neural Vocoder with\n  Adaptive Noise Spectral Shaping", "abstract": "Neural vocoder using denoising diffusion probabilistic model (DDPM) has been\nimproved by adaptation of the diffusion noise distribution to given acoustic\nfeatures. In this study, we propose SpecGrad that adapts the diffusion noise so\nthat its time-varying spectral envelope becomes close to the conditioning\nlog-mel spectrogram. This adaptation by time-varying filtering improves the\nsound quality especially in the high-frequency bands. It is processed in the\ntime-frequency domain to keep the computational cost almost the same as the\nconventional DDPM-based neural vocoders. Experimental results showed that\nSpecGrad generates higher-fidelity speech waveform than conventional DDPM-based\nneural vocoders in both analysis-synthesis and speech enhancement scenarios.\nAudio demos are available at wavegrad.github.io/specgrad/.", "published": "2022-03-31 02:08:27", "link": "http://arxiv.org/abs/2203.16749v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Learning Decoupling Features Through Orthogonality Regularization", "abstract": "Keyword spotting (KWS) and speaker verification (SV) are two important tasks\nin speech applications. Research shows that the state-of-art KWS and SV models\nare trained independently using different datasets since they expect to learn\ndistinctive acoustic features. However, humans can distinguish language content\nand the speaker identity simultaneously. Motivated by this, we believe it is\nimportant to explore a method that can effectively extract common features\nwhile decoupling task-specific features. Bearing this in mind, a two-branch\ndeep network (KWS branch and SV branch) with the same network structure is\ndeveloped and a novel decoupling feature learning method is proposed to push up\nthe performance of KWS and SV simultaneously where speaker-invariant keyword\nrepresentations and keyword-invariant speaker representations are expected\nrespectively. Experiments are conducted on Google Speech Commands Dataset\n(GSCD). The results demonstrate that the orthogonality regularization helps the\nnetwork to achieve SOTA EER of 1.31% and 1.87% on KWS and SV, respectively.", "published": "2022-03-31 03:18:13", "link": "http://arxiv.org/abs/2203.16772v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker Extraction with Co-Speech Gestures Cue", "abstract": "Speaker extraction seeks to extract the clean speech of a target speaker from\na multi-talker mixture speech. There have been studies to use a pre-recorded\nspeech sample or face image of the target speaker as the speaker cue. In human\ncommunication, co-speech gestures that are naturally timed with speech also\ncontribute to speech perception. In this work, we explore the use of co-speech\ngestures sequence, e.g. hand and body movements, as the speaker cue for speaker\nextraction, which could be easily obtained from low-resolution video\nrecordings, thus more available than face recordings. We propose two networks\nusing the co-speech gestures cue to perform attentive listening on the target\nspeaker, one that implicitly fuses the co-speech gestures cue in the speaker\nextraction process, the other performs speech separation first, followed by\nexplicitly using the co-speech gestures cue to associate a separated speech to\nthe target speaker. The experimental results show that the co-speech gestures\ncue is informative in associating with the target speaker.", "published": "2022-03-31 06:48:52", "link": "http://arxiv.org/abs/2203.16840v2", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to\n  Speech", "abstract": "In neural text-to-speech (TTS), two-stage system or a cascade of separately\nlearned models have shown synthesis quality close to human speech. For example,\nFastSpeech2 transforms an input text to a mel-spectrogram and then HiFi-GAN\ngenerates a raw waveform from a mel-spectogram where they are called an\nacoustic feature generator and a neural vocoder respectively. However, their\ntraining pipeline is somewhat cumbersome in that it requires a fine-tuning and\nan accurate speech-text alignment for optimal performance. In this work, we\npresent end-to-end text-to-speech (E2E-TTS) model which has a simplified\ntraining pipeline and outperforms a cascade of separately learned models.\nSpecifically, our proposed model is jointly trained FastSpeech2 and HiFi-GAN\nwith an alignment module. Since there is no acoustic feature mismatch between\ntraining and inference, it does not requires fine-tuning. Furthermore, we\nremove dependency on an external speech-text alignment tool by adopting an\nalignment learning objective in our joint training framework. Experiments on\nLJSpeech corpus shows that the proposed model outperforms publicly available,\nstate-of-the-art implementations of ESPNet2-TTS on subjective evaluation (MOS)\nand some objective evaluations.", "published": "2022-03-31 07:25:11", "link": "http://arxiv.org/abs/2203.16852v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "HiFi-VC: High Quality ASR-Based Voice Conversion", "abstract": "The goal of voice conversion (VC) is to convert input voice to match the\ntarget speaker's voice while keeping text and prosody intact. VC is usually\nused in entertainment and speaking-aid systems, as well as applied for speech\ndata generation and augmentation. The development of any-to-any VC systems,\nwhich are capable of generating voices unseen during model training, is of\nparticular interest to both researchers and the industry. Despite recent\nprogress, any-to-any conversion quality is still inferior to natural speech.\n  In this work, we propose a new any-to-any voice conversion pipeline. Our\napproach uses automated speech recognition (ASR) features, pitch tracking, and\na state-of-the-art waveform prediction model. According to multiple subjective\nand objective evaluations, our method outperforms modern baselines in terms of\nvoice quality, similarity and consistency.", "published": "2022-03-31 10:45:32", "link": "http://arxiv.org/abs/2203.16937v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Direction of Arrival Estimation of Sound Sources Using Icosahedral CNNs", "abstract": "In this paper, we present a new model for Direction of Arrival (DOA)\nestimation of sound sources based on an Icosahedral Convolutional Neural\nNetwork (CNN) applied over SRP-PHAT power maps computed from the signals\nreceived by a microphone array. This icosahedral CNN is equivariant to the 60\nrotational symmetries of the icosahedron, which represent a good approximation\nof the continuous space of spherical rotations, and can be implemented using\nstandard 2D convolutional layers, having a lower computational cost than most\nof the spherical CNNs. In addition, instead of using fully connected layers\nafter the icosahedral convolutions, we propose a new soft-argmax function that\ncan be seen as a differentiable version of the argmax function and allows us to\nsolve the DOA estimation as a regression problem interpreting the output of the\nconvolutional layers as a probability distribution. We prove that using models\nthat fit the equivariances of the problem allows us to outperform other\nstate-of-the-art models with a lower computational cost and more robustness,\nobtaining root mean square localization errors lower than 10{\\deg} even in\nscenarios with a reverberation time $T_{60}$ of 1.5 s.", "published": "2022-03-31 10:52:19", "link": "http://arxiv.org/abs/2203.16940v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Acoustic-Net: A Novel Neural Network for Sound Localization and\n  Quantification", "abstract": "Acoustic source localization has been applied in different fields, such as\naeronautics and ocean science, generally using multiple microphones array data\nto reconstruct the source location. However, the model-based beamforming\nmethods fail to achieve the high-resolution of conventional beamforming maps.\nDeep neural networks are also appropriate to locate the sound source, but in\ngeneral, these methods with complex network structures are hard to be\nrecognized by hardware. In this paper, a novel neural network, termed the\nAcoustic-Net, is proposed to locate and quantify the sound source simply using\nthe original signals. The experiments demonstrate that the proposed method\nsignificantly improves the accuracy of sound source prediction and the\ncomputing speed, which may generalize well to real data. The code and trained\nmodels are available at https://github.com/JoaquinChou/Acoustic-Net.", "published": "2022-03-31 12:20:09", "link": "http://arxiv.org/abs/2203.16988v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SingAug: Data Augmentation for Singing Voice Synthesis with\n  Cycle-consistent Training Strategy", "abstract": "Deep learning based singing voice synthesis (SVS) systems have been\ndemonstrated to flexibly generate singing with better qualities, compared to\nconventional statistical parametric based methods. However, neural systems are\ngenerally data-hungry and have difficulty to reach reasonable singing quality\nwith limited public available training data. In this work, we explore different\ndata augmentation methods to boost the training of SVS systems, including\nseveral strategies customized to SVS based on pitch augmentation and mix-up\naugmentation. To further stabilize the training, we introduce the\ncycle-consistent training strategy. Extensive experiments on two public singing\ndatabases demonstrate that our proposed augmentation methods and the\nstabilizing training strategy can significantly improve the performance on both\nobjective and subjective evaluations.", "published": "2022-03-31 12:50:10", "link": "http://arxiv.org/abs/2203.17001v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Enhancement with Score-Based Generative Models in the Complex\n  STFT Domain", "abstract": "Score-based generative models (SGMs) have recently shown impressive results\nfor difficult generative tasks such as the unconditional and conditional\ngeneration of natural images and audio signals. In this work, we extend these\nmodels to the complex short-time Fourier transform (STFT) domain, proposing a\nnovel training task for speech enhancement using a complex-valued deep neural\nnetwork. We derive this training task within the formalism of stochastic\ndifferential equations (SDEs), thereby enabling the use of predictor-corrector\nsamplers. We provide alternative formulations inspired by previous publications\non using generative diffusion models for speech enhancement, avoiding the need\nfor any prior assumptions on the noise distribution and making the training\ntask purely generative which, as we show, results in improved enhancement\nperformance.", "published": "2022-03-31 12:53:47", "link": "http://arxiv.org/abs/2203.17004v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Temporal-oriented Broadcast ResNet for COVID-19 Detection", "abstract": "Detecting COVID-19 from audio signals, such as breathing and coughing, can be\nused as a fast and efficient pre-testing method to reduce the virus\ntransmission. Due to the promising results of deep learning networks in\nmodelling time sequences, and since applications to rapidly identify COVID\nin-the-wild should require low computational effort, we present a\ntemporal-oriented broadcasting residual learning method that achieves efficient\ncomputation and high accuracy with a small model size. Based on the\nEfficientNet architecture, our novel network, named Temporal-oriented\nResNet~(TorNet), constitutes of a broadcasting learning block, i.e. the\nAlternating Broadcast (AB) Block, which contains several Broadcast Residual\nBlocks (BC ResBlocks) and a convolution layer. With the AB Block, the network\nobtains useful audio-temporal features and higher level embeddings effectively\nwith much less computation than Recurrent Neural Networks~(RNNs), typically\nused to model temporal information. TorNet achieves 72.2% Unweighted Average\nRecall (UAR) on the INTERPSEECH 2021 Computational Paralinguistics Challenge\nCOVID-19 cough Sub-Challenge, by this showing competitive results with a higher\ncomputational efficiency than other state-of-the-art alternatives.", "published": "2022-03-31 13:11:57", "link": "http://arxiv.org/abs/2203.17012v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DeepFry: Identifying Vocal Fry Using Deep Neural Networks", "abstract": "Vocal fry or creaky voice refers to a voice quality characterized by\nirregular glottal opening and low pitch. It occurs in diverse languages and is\nprevalent in American English, where it is used not only to mark phrase\nfinality, but also sociolinguistic factors and affect. Due to its irregular\nperiodicity, creaky voice challenges automatic speech processing and\nrecognition systems, particularly for languages where creak is frequently used.\n  This paper proposes a deep learning model to detect creaky voice in fluent\nspeech. The model is composed of an encoder and a classifier trained together.\nThe encoder takes the raw waveform and learns a representation using a\nconvolutional neural network. The classifier is implemented as a multi-headed\nfully-connected network trained to detect creaky voice, voicing, and pitch,\nwhere the last two are used to refine creak prediction. The model is trained\nand tested on speech of American English speakers, annotated for creak by\ntrained phoneticians.\n  We evaluated the performance of our system using two encoders: one is\ntailored for the task, and the other is based on a state-of-the-art\nunsupervised representation. Results suggest our best-performing system has\nimproved recall and F1 scores compared to previous methods on unseen data.", "published": "2022-03-31 13:23:24", "link": "http://arxiv.org/abs/2203.17019v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CTA-RNN: Channel and Temporal-wise Attention RNN Leveraging Pre-trained\n  ASR Embeddings for Speech Emotion Recognition", "abstract": "Previous research has looked into ways to improve speech emotion recognition\n(SER) by utilizing both acoustic and linguistic cues of speech. However, the\npotential association between state-of-the-art ASR models and the SER task has\nyet to be investigated. In this paper, we propose a novel channel and\ntemporal-wise attention RNN (CTA-RNN) architecture based on the intermediate\nrepresentations of pre-trained ASR models. Specifically, the embeddings of a\nlarge-scale pre-trained end-to-end ASR encoder contain both acoustic and\nlinguistic information, as well as the ability to generalize to different\nspeakers, making them well suited for downstream SER task. To further exploit\nthe embeddings from different layers of the ASR encoder, we propose a novel\nCTA-RNN architecture to capture the emotional salient parts of embeddings in\nboth the channel and temporal directions. We evaluate our approach on two\npopular benchmark datasets, IEMOCAP and MSP-IMPROV, using both within-corpus\nand cross-corpus settings. Experimental results show that our proposed method\ncan achieve excellent performance in terms of accuracy and robustness.", "published": "2022-03-31 13:32:51", "link": "http://arxiv.org/abs/2203.17023v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adversarial Speaker Distillation for Countermeasure Model on Automatic\n  Speaker Verification", "abstract": "The countermeasure (CM) model is developed to protect ASV systems from spoof\nattacks and prevent resulting personal information leakage in Automatic Speaker\nVerification (ASV) system. Based on practicality and security considerations,\nthe CM model is usually deployed on edge devices, which have more limited\ncomputing resources and storage space than cloud-based systems, confining the\nmodel size under a limitation. To better trade off the CM model sizes and\nperformance, we proposed an adversarial speaker distillation method, which is\nan improved version of knowledge distillation method combined with generalized\nend-to-end (GE2E) pre-training and adversarial fine-tuning. In the evaluation\nphase of the ASVspoof 2021 Logical Access task, our proposed adversarial\nspeaker distillation ResNetSE (ASD-ResNetSE) model reaches 0.2695 min t-DCF and\n3.54% EER. ASD-ResNetSE only used 22.5% of parameters and 19.4% of multiply and\naccumulate operands of ResNetSE model.", "published": "2022-03-31 13:52:43", "link": "http://arxiv.org/abs/2203.17031v6", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired\n  Speech Data", "abstract": "This paper studies a novel pre-training technique with unpaired speech data,\nSpeech2C, for encoder-decoder based automatic speech recognition (ASR). Within\na multi-task learning framework, we introduce two pre-training tasks for the\nencoder-decoder network using acoustic units, i.e., pseudo codes, derived from\nan offline clustering model. One is to predict the pseudo codes via masked\nlanguage modeling in encoder output, like HuBERT model, while the other lets\nthe decoder learn to reconstruct pseudo codes autoregressively instead of\ngenerating textual scripts. In this way, the decoder learns to reconstruct\noriginal speech information with codes before learning to generate correct\ntext. Comprehensive experiments on the LibriSpeech corpus show that the\nproposed Speech2C can relatively reduce the word error rate (WER) by 19.2% over\nthe method without decoder pre-training, and also outperforms significantly the\nstate-of-the-art wav2vec 2.0 and HuBERT on fine-tuning subsets of 10h and 100h.\nWe release our code and model at\nhttps://github.com/microsoft/SpeechT5/tree/main/Speech2C.", "published": "2022-03-31 15:33:56", "link": "http://arxiv.org/abs/2203.17113v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improved Relation Networks for End-to-End Speaker Verification and\n  Identification", "abstract": "Speaker identification systems in a real-world scenario are tasked to\nidentify a speaker amongst a set of enrolled speakers given just a few samples\nfor each enrolled speaker. This paper demonstrates the effectiveness of\nmeta-learning and relation networks for this use case. We propose improved\nrelation networks for speaker verification and few-shot (unseen) speaker\nidentification. The use of relation networks facilitates joint training of the\nfrontend speaker encoder and the backend model. Inspired by the use of\nprototypical networks in speaker verification and to increase the\ndiscriminability of the speaker embeddings, we train the model to classify\nsamples in the current episode amongst all speakers present in the training\nset. Furthermore, we propose a new training regime for faster model convergence\nby extracting more information from a given meta-learning episode with\nnegligible extra computation. We evaluate the proposed techniques on VoxCeleb,\nSITW and VCTK datasets on the tasks of speaker verification and unseen speaker\nidentification. The proposed approach outperforms the existing approaches\nconsistently on both tasks.", "published": "2022-03-31 17:44:04", "link": "http://arxiv.org/abs/2203.17218v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio-Visual Speech Codecs: Rethinking Audio-Visual Speech Enhancement\n  by Re-Synthesis", "abstract": "Since facial actions such as lip movements contain significant information\nabout speech content, it is not surprising that audio-visual speech enhancement\nmethods are more accurate than their audio-only counterparts. Yet,\nstate-of-the-art approaches still struggle to generate clean, realistic speech\nwithout noise artifacts and unnatural distortions in challenging acoustic\nenvironments. In this paper, we propose a novel audio-visual speech enhancement\nframework for high-fidelity telecommunications in AR/VR. Our approach leverages\naudio-visual speech cues to generate the codes of a neural speech codec,\nenabling efficient synthesis of clean, realistic speech from noisy signals.\nGiven the importance of speaker-specific cues in speech, we focus on developing\npersonalized models that work well for individual speakers. We demonstrate the\nefficacy of our approach on a new audio-visual speech dataset collected in an\nunconstrained, large vocabulary setting, as well as existing audio-visual\ndatasets, outperforming speech enhancement baselines on both quantitative\nmetrics and human evaluation studies. Please see the supplemental video for\nqualitative results at\nhttps://github.com/facebookresearch/facestar/releases/download/paper_materials/video.mp4.", "published": "2022-03-31 17:57:10", "link": "http://arxiv.org/abs/2203.17263v1", "categories": ["cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Perceptive, non-linear Speech Processing and Spiking Neural Networks", "abstract": "Source separation and speech recognition are very difficult in the context of\nnoisy and corrupted speech. Most conventional techniques need huge databases to\nestimate speech (or noise) density probabilities to perform separation or\nrecognition. We discuss the potential of perceptive speech analysis and\nprocessing in combination with biologically plausible neural network\nprocessors. We illustrate the potential of such non-linear processing of speech\non a source separation system inspired by an Auditory Scene Analysis paradigm.\nWe also discuss a potential application in speech recognition.", "published": "2022-03-31 21:11:06", "link": "http://arxiv.org/abs/2204.00094v1", "categories": ["cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "cs.SD"}
{"title": "1-D CNN based Acoustic Scene Classification via Reducing Layer-wise\n  Dimensionality", "abstract": "This paper presents an alternate representation framework to commonly used\ntime-frequency representation for acoustic scene classification (ASC). A raw\naudio signal is represented using a pre-trained convolutional neural network\n(CNN) using its various intermediate layers. The study assumes that the\nrepresentations obtained from the intermediate layers lie in low-dimensions\nintrinsically. To obtain low-dimensional embeddings, principal component\nanalysis is performed, and the study analyzes that only a few principal\ncomponents are significant. However, the appropriate number of significant\ncomponents are not known. To address this, an automatic dictionary learning\nframework is utilized that approximates the underlying subspace. Further, the\nlow-dimensional embeddings are aggregated in a late-fusion manner in the\nensemble framework to incorporate hierarchical information learned at various\nintermediate layers. The experimental evaluation is performed on publicly\navailable DCASE 2017 and 2018 ASC datasets on a pre-trained 1-D CNN, SoundNet.\nEmpirically, it is observed that deeper layers show more compression ratio than\nothers. At 70% compression ratio across different datasets, the performance is\nsimilar to that obtained without performing any dimensionality reduction. The\nproposed framework outperforms the time-frequency representation based methods.", "published": "2022-03-31 02:00:31", "link": "http://arxiv.org/abs/2204.00555v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Investigating Modality Bias in Audio Visual Video Parsing", "abstract": "We focus on the audio-visual video parsing (AVVP) problem that involves\ndetecting audio and visual event labels with temporal boundaries. The task is\nespecially challenging since it is weakly supervised with only event labels\navailable as a bag of labels for each video. An existing state-of-the-art model\nfor AVVP uses a hybrid attention network (HAN) to generate cross-modal features\nfor both audio and visual modalities, and an attentive pooling module that\naggregates predicted audio and visual segment-level event probabilities to\nyield video-level event probabilities. We provide a detailed analysis of\nmodality bias in the existing HAN architecture, where a modality is completely\nignored during prediction. We also propose a variant of feature aggregation in\nHAN that leads to an absolute gain in F-scores of about 2% and 1.6% for visual\nand audio-visual events at both segment-level and event-level, in comparison to\nthe existing HAN model.", "published": "2022-03-31 07:43:01", "link": "http://arxiv.org/abs/2203.16860v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
