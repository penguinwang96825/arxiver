{"title": "Explain Yourself! Leveraging Language Models for Commonsense Reasoning", "abstract": "Deep learning models perform poorly on tasks that require commonsense\nreasoning, which often necessitates some form of world-knowledge or reasoning\nover information not immediately present in the input. We collect human\nexplanations for commonsense reasoning in the form of natural language\nsequences and highlighted annotations in a new dataset called Common Sense\nExplanations (CoS-E). We use CoS-E to train language models to automatically\ngenerate explanations that can be used during training and inference in a novel\nCommonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the\nstate-of-the-art by 10% on the challenging CommonsenseQA task. We further study\ncommonsense reasoning in DNNs using both human and auto-generated explanations\nincluding transfer to out-of-domain tasks. Empirical results indicate that we\ncan effectively leverage language models for commonsense reasoning.", "published": "2019-06-06 00:02:37", "link": "http://arxiv.org/abs/1906.02361v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Analysis of Emotion Communication Channels in Fan Fiction: Towards\n  Emotional Storytelling", "abstract": "Centrality of emotion for the stories told by humans is underpinned by\nnumerous studies in literature and psychology. The research in automatic\nstorytelling has recently turned towards emotional storytelling, in which\ncharacters' emotions play an important role in the plot development. However,\nthese studies mainly use emotion to generate propositional statements in the\nform \"A feels affection towards B\" or \"A confronts B\". At the same time,\nemotional behavior does not boil down to such propositional descriptions, as\nhumans display complex and highly variable patterns in communicating their\nemotions, both verbally and non-verbally. In this paper, we analyze how\nemotions are expressed non-verbally in a corpus of fan fiction short stories.\nOur analysis shows that stories written by humans convey character emotions\nalong various non-verbal channels. We find that some non-verbal channels, such\nas facial expressions and voice characteristics of the characters, are more\nstrongly associated with joy, while gestures and body postures are more likely\nto occur with trust. Based on our analysis, we argue that automatic\nstorytelling systems should take variability of emotion into account when\ngenerating descriptions of characters' emotions.", "published": "2019-06-06 03:53:58", "link": "http://arxiv.org/abs/1906.02402v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shift-of-Perspective Identification Within Legal Cases", "abstract": "Arguments, counter-arguments, facts, and evidence obtained via documents\nrelated to previous court cases are of essential need for legal professionals.\nTherefore, the process of automatic information extraction from documents\ncontaining legal opinions related to court cases can be considered to be of\nsignificant importance. This study is focused on the identification of\nsentences in legal opinion texts which convey different perspectives on a\ncertain topic or entity. We combined several approaches based on semantic\nanalysis, open information extraction, and sentiment analysis to achieve our\nobjective. Then, our methodology was evaluated with the help of human judges.\nThe outcomes of the evaluation demonstrate that our system is successful in\ndetecting situations where two sentences deliver different opinions on the same\ntopic or entity. The proposed methodology can be used to facilitate other\ninformation extraction tasks related to the legal domain. One such task is the\nautomated detection of counter arguments for a given argument. Another is the\nidentification of opponent parties in a court case.", "published": "2019-06-06 05:58:42", "link": "http://arxiv.org/abs/1906.02430v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GCDT: A Global Context Enhanced Deep Transition Architecture for\n  Sequence Labeling", "abstract": "Current state-of-the-art systems for sequence labeling are typically based on\nthe family of Recurrent Neural Networks (RNNs). However, the shallow\nconnections between consecutive hidden states of RNNs and insufficient modeling\nof global information restrict the potential performance of those models. In\nthis paper, we try to address these issues, and thus propose a Global Context\nenhanced Deep Transition architecture for sequence labeling named GCDT. We\ndeepen the state transition path at each position in a sentence, and further\nassign every token with a global representation learned from the entire\nsentence. Experiments on two standard sequence labeling tasks show that, given\nonly training data and the ubiquitous word embeddings (Glove), our GCDT\nachieves 91.96 F1 on the CoNLL03 NER task and 95.43 F1 on the CoNLL2000\nChunking task, which outperforms the best reported results under the same\nsettings. Furthermore, by leveraging BERT as an additional resource, we\nestablish new state-of-the-art results with 93.47 F1 on NER and 97.30 F1 on\nChunking.", "published": "2019-06-06 06:31:21", "link": "http://arxiv.org/abs/1906.02437v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Neural Machine Translation with Doubly Adversarial Inputs", "abstract": "Neural machine translation (NMT) often suffers from the vulnerability to\nnoisy perturbations in the input. We propose an approach to improving the\nrobustness of NMT models, which consists of two parts: (1) attack the\ntranslation model with adversarial source examples; (2) defend the translation\nmodel with adversarial target inputs to improve its robustness against the\nadversarial source inputs.For the generation of adversarial inputs, we propose\na gradient-based method to craft adversarial examples informed by the\ntranslation loss over the clean inputs.Experimental results on Chinese-English\nand English-German translation tasks demonstrate that our approach achieves\nsignificant improvements ($2.8$ and $1.6$ BLEU points) over Transformer on\nstandard clean benchmarks as well as exhibiting higher robustness on noisy\ndata.", "published": "2019-06-06 07:02:04", "link": "http://arxiv.org/abs/1906.02443v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Second-order Co-occurrence Sensitivity of Skip-Gram with Negative\n  Sampling", "abstract": "We simulate first- and second-order context overlap and show that Skip-Gram\nwith Negative Sampling is similar to Singular Value Decomposition in capturing\nsecond-order co-occurrence information, while Pointwise Mutual Information is\nagnostic to it. We support the results with an empirical study finding that the\nmodels react differently when provided with additional second-order\ninformation. Our findings reveal a basic property of Skip-Gram with Negative\nSampling and point towards an explanation of its success on a variety of tasks.", "published": "2019-06-06 08:44:10", "link": "http://arxiv.org/abs/1906.02479v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Entity Typing in Hyperbolic Space", "abstract": "How can we represent hierarchical information present in large type\ninventories for entity typing? We study the ability of hyperbolic embeddings to\ncapture hierarchical relations between mentions in context and their target\ntypes in a shared vector space. We evaluate on two datasets and investigate two\ndifferent techniques for creating a large hierarchical entity type inventory:\nfrom an expert-generated ontology and by automatically mining type\nco-occurrences. We find that the hyperbolic model yields improvements over its\nEuclidean counterpart in some, but not all cases. Our analysis suggests that\nthe adequacy of this geometry depends on the granularity of the type inventory\nand the way hierarchical relations are inferred.", "published": "2019-06-06 10:22:24", "link": "http://arxiv.org/abs/1906.02505v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Derivational Morphological Relations in Word Embeddings", "abstract": "Derivation is a type of a word-formation process which creates new words from\nexisting ones by adding, changing or deleting affixes. In this paper, we\nexplore the potential of word embeddings to identify properties of word\nderivations in the morphologically rich Czech language. We extract derivational\nrelations between pairs of words from DeriNet, a Czech lexical network, which\norganizes almost one million Czech lemmata into derivational trees. For each\nsuch pair, we compute the difference of the embeddings of the two words, and\nperform unsupervised clustering of the resulting vectors. Our results show that\nthese clusters largely match manually annotated semantic categories of the\nderivational relations (e.g. the relation 'bake--baker' belongs to category\n'actor', and a correct clustering puts it into the same cluster as\n'govern--governor').", "published": "2019-06-06 10:44:41", "link": "http://arxiv.org/abs/1906.02510v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Training for Automatic Question Generation", "abstract": "Automatic question generation (QG) is a challenging problem in natural\nlanguage understanding. QG systems are typically built assuming access to a\nlarge number of training instances where each instance is a question and its\ncorresponding answer. For a new language, such training instances are hard to\nobtain making the QG problem even more challenging. Using this as our\nmotivation, we study the reuse of an available large QG dataset in a secondary\nlanguage (e.g. English) to learn a QG model for a primary language (e.g. Hindi)\nof interest. For the primary language, we assume access to a large amount of\nmonolingual text but only a small QG dataset. We propose a cross-lingual QG\nmodel which uses the following training regime: (i) Unsupervised pretraining of\nlanguage models in both primary and secondary languages and (ii) joint\nsupervised training for QG in both languages. We demonstrate the efficacy of\nour proposed approach using two different primary languages, Hindi and Chinese.\nWe also create and release a new question answering dataset for Hindi\nconsisting of 6555 sentences.", "published": "2019-06-06 11:31:24", "link": "http://arxiv.org/abs/1906.02525v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring the compositionality of noun-noun compounds over time", "abstract": "We present work in progress on the temporal progression of compositionality\nin noun-noun compounds. Previous work has proposed computational methods for\ndetermining the compositionality of compounds. These methods try to\nautomatically determine how transparent the meaning of the compound as a whole\nis with respect to the meaning of its parts. We hypothesize that such a\nproperty might change over time. We use the time-stamped Google Books corpus\nfor our diachronic investigations, and first examine whether the vector-based\nsemantic spaces extracted from this corpus are able to predict compositionality\nratings, despite their inherent limitations. We find that using temporal\ninformation helps predicting the ratings, although correlation with the ratings\nis lower than reported for other corpora. Finally, we show changes in\ncompositionality over time for a selection of compounds.", "published": "2019-06-06 13:12:35", "link": "http://arxiv.org/abs/1906.02563v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Question-Answer Hierarchies", "abstract": "The process of knowledge acquisition can be viewed as a question-answer game\nbetween a student and a teacher in which the student typically starts by asking\nbroad, open-ended questions before drilling down into specifics (Hintikka,\n1981; Hakkarainen and Sintonen, 2002). This pedagogical perspective motivates a\nnew way of representing documents. In this paper, we present SQUASH\n(Specificity-controlled Question-Answer Hierarchies), a novel and challenging\ntext generation task that converts an input document into a hierarchy of\nquestion-answer pairs. Users can click on high-level questions (e.g., \"Why did\nFrodo leave the Fellowship?\") to reveal related but more specific questions\n(e.g., \"Who did Frodo leave with?\"). Using a question taxonomy loosely based on\nLehnert (1978), we classify questions in existing reading comprehension\ndatasets as either \"general\" or \"specific\". We then use these labels as input\nto a pipelined system centered around a conditional neural language model. We\nextensively evaluate the quality of the generated QA hierarchies through\ncrowdsourced experiments and report strong empirical results.", "published": "2019-06-06 14:53:04", "link": "http://arxiv.org/abs/1906.02622v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntactically Supervised Transformers for Faster Neural Machine\n  Translation", "abstract": "Standard decoders for neural machine translation autoregressively generate a\nsingle target token per time step, which slows inference especially for long\noutputs. While architectural advances such as the Transformer fully parallelize\nthe decoder computations at training time, inference still proceeds\nsequentially. Recent developments in non- and semi- autoregressive decoding\nproduce multiple tokens per time step independently of the others, which\nimproves inference speed but deteriorates translation quality. In this work, we\npropose the syntactically supervised Transformer (SynST), which first\nautoregressively predicts a chunked parse tree before generating all of the\ntarget tokens in one shot conditioned on the predicted parse. A series of\ncontrolled experiments demonstrates that SynST decodes sentences ~ 5x faster\nthan the baseline autoregressive Transformer while achieving higher BLEU scores\nthan most competing methods on En-De and En-Fr datasets.", "published": "2019-06-06 19:16:16", "link": "http://arxiv.org/abs/1906.02780v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Receptive to Productive: Learning to Use Confusing Words through\n  Automatically Selected Example Sentences", "abstract": "Knowing how to use words appropriately has been a key to improving language\nproficiency. Previous studies typically discuss how students learn receptively\nto select the correct candidate from a set of confusing words in the\nfill-in-the-blank task where specific context is given. In this paper, we go\none step further, assisting students to learn to use confusing words\nappropriately in a productive task: sentence translation. We leverage the\nGiveMeExample system, which suggests example sentences for each confusing word,\nto achieve this goal. In this study, students learn to differentiate the\nconfusing words by reading the example sentences, and then choose the\nappropriate word(s) to complete the sentence translation task. Results show\nstudents made substantial progress in terms of sentence structure. In addition,\nhighly proficient students better managed to learn confusing words. In view of\nthe influence of the first language on learners, we further propose an\neffective approach to improve the quality of the suggested sentences.", "published": "2019-06-06 19:25:30", "link": "http://arxiv.org/abs/1906.02782v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Scalable and Reliable Capsule Networks for Challenging NLP\n  Applications", "abstract": "Obstacles hindering the development of capsule networks for challenging NLP\napplications include poor scalability to large output spaces and less reliable\nrouting processes. In this paper, we introduce: 1) an agreement score to\nevaluate the performance of routing processes at instance level; 2) an adaptive\noptimizer to enhance the reliability of routing; 3) capsule compression and\npartial routing to improve the scalability of capsule networks. We validate our\napproach on two NLP tasks, namely: multi-label text classification and question\nanswering. Experimental results show that our approach considerably improves\nover strong competitors on both tasks. In addition, we gain the best results in\nlow-resource settings with few training instances.", "published": "2019-06-06 21:53:53", "link": "http://arxiv.org/abs/1906.02829v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis of Automatic Annotation Suggestions for Hard Discourse-Level\n  Tasks in Expert Domains", "abstract": "Many complex discourse-level tasks can aid domain experts in their work but\nrequire costly expert annotations for data creation. To speed up and ease\nannotations, we investigate the viability of automatically generated annotation\nsuggestions for such tasks. As an example, we choose a task that is\nparticularly hard for both humans and machines: the segmentation and\nclassification of epistemic activities in diagnostic reasoning texts. We create\nand publish a new dataset covering two domains and carefully analyse the\nsuggested annotations. We find that suggestions have positive effects on\nannotation speed and performance, while not introducing noteworthy biases.\nEnvisioning suggestion models that improve with newly annotated texts, we\ncontrast methods for continuous model adjustment and suggest the most effective\nsetup for suggestions in future expert tasks.", "published": "2019-06-06 13:13:46", "link": "http://arxiv.org/abs/1906.02564v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Syntactic Transfer through Unsupervised Adaptation of\n  Invertible Projections", "abstract": "Cross-lingual transfer is an effective way to build syntactic analysis tools\nin low-resource languages. However, transfer is difficult when transferring to\ntypologically distant languages, especially when neither annotated target data\nnor parallel corpora are available. In this paper, we focus on methods for\ncross-lingual transfer to distant languages and propose to learn a generative\nmodel with a structured prior that utilizes labeled source data and unlabeled\ntarget data jointly. The parameters of source model and target model are softly\nshared through a regularized log likelihood objective. An invertible projection\nis employed to learn a new interlingual latent embedding space that compensates\nfor imperfect cross-lingual word embedding input. We evaluate our method on two\nsyntactic tasks: part-of-speech (POS) tagging and dependency parsing. On the\nUniversal Dependency Treebanks, we use English as the only source corpus and\ntransfer to a wide range of target languages. On the 10 languages in this\ndataset that are distant from English, our method yields an average of 5.2%\nabsolute improvement on POS tagging and 8.3% absolute improvement on dependency\nparsing over a direct transfer method using state-of-the-art discriminative\nmodels.", "published": "2019-06-06 15:46:17", "link": "http://arxiv.org/abs/1906.02656v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-view Knowledge Graph Embedding for Entity Alignment", "abstract": "We study the problem of embedding-based entity alignment between knowledge\ngraphs (KGs). Previous works mainly focus on the relational structure of\nentities. Some further incorporate another type of features, such as\nattributes, for refinement. However, a vast of entity features are still\nunexplored or not equally treated together, which impairs the accuracy and\nrobustness of embedding-based entity alignment. In this paper, we propose a\nnovel framework that unifies multiple views of entities to learn embeddings for\nentity alignment. Specifically, we embed entities based on the views of entity\nnames, relations and attributes, with several combination strategies.\nFurthermore, we design some cross-KG inference methods to enhance the alignment\nbetween two KGs. Our experiments on real-world datasets show that the proposed\nframework significantly outperforms the state-of-the-art embedding-based entity\nalignment methods. The selected views, cross-KG inference and combination\nstrategies all contribute to the performance improvement.", "published": "2019-06-06 02:52:12", "link": "http://arxiv.org/abs/1906.02390v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Ease-of-Teaching and Language Structure from Emergent Communication", "abstract": "Artificial agents have been shown to learn to communicate when needed to\ncomplete a cooperative task. Some level of language structure (e.g.,\ncompositionality) has been found in the learned communication protocols. This\nobserved structure is often the result of specific environmental pressures\nduring training. By introducing new agents periodically to replace old ones,\nsequentially and within a population, we explore such a new pressure -- ease of\nteaching -- and show its impact on the structure of the resulting language.", "published": "2019-06-06 03:59:37", "link": "http://arxiv.org/abs/1906.02403v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Sparse Parallel Training of Hierarchical Dirichlet Process Topic Models", "abstract": "To scale non-parametric extensions of probabilistic topic models such as\nLatent Dirichlet allocation to larger data sets, practitioners rely\nincreasingly on parallel and distributed systems. In this work, we study\ndata-parallel training for the hierarchical Dirichlet process (HDP) topic\nmodel. Based upon a representation of certain conditional distributions within\nan HDP, we propose a doubly sparse data-parallel sampler for the HDP topic\nmodel. This sampler utilizes all available sources of sparsity found in natural\nlanguage - an important way to make computation efficient. We benchmark our\nmethod on a well-known corpus (PubMed) with 8m documents and 768m tokens, using\na single multi-core machine in under four days.", "published": "2019-06-06 05:04:08", "link": "http://arxiv.org/abs/1906.02416v2", "categories": ["stat.ML", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Bridging the Gap between Training and Inference for Neural Machine\n  Translation", "abstract": "Neural Machine Translation (NMT) generates target words sequentially in the\nway of predicting the next word conditioned on the context words. At training\ntime, it predicts with the ground truth words as context while at inference it\nhas to generate the entire sequence from scratch. This discrepancy of the fed\ncontext leads to error accumulation among the way. Furthermore, word-level\ntraining requires strict matching between the generated sequence and the ground\ntruth sequence which leads to overcorrection over different but reasonable\ntranslations. In this paper, we address these issues by sampling context words\nnot only from the ground truth sequence but also from the predicted sequence by\nthe model during training, where the predicted sequence is selected with a\nsentence-level optimum. Experiment results on Chinese->English and WMT'14\nEnglish->German translation tasks demonstrate that our approach can achieve\nsignificant improvements on multiple datasets.", "published": "2019-06-06 07:15:52", "link": "http://arxiv.org/abs/1906.02448v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Unsupervised Pivot Translation for Distant Languages", "abstract": "Unsupervised neural machine translation (NMT) has attracted a lot of\nattention recently. While state-of-the-art methods for unsupervised translation\nusually perform well between similar languages (e.g., English-German\ntranslation), they perform poorly between distant languages, because\nunsupervised alignment does not work well for distant languages. In this work,\nwe introduce unsupervised pivot translation for distant languages, which\ntranslates a language to a distant language through multiple hops, and the\nunsupervised translation on each hop is relatively easier than the original\ndirect translation. We propose a learning to route (LTR) method to choose the\ntranslation path between the source and target languages. LTR is trained on\nlanguage pairs whose best translation path is available and is applied on the\nunseen language pairs for path selection. Experiments on 20 languages and 294\ndistant language pairs demonstrate the advantages of the unsupervised pivot\ntranslation for distant languages, as well as the effectiveness of the proposed\nLTR for path selection. Specifically, in the best case, LTR achieves an\nimprovement of 5.58 BLEU points over the conventional direct unsupervised\nmethod.", "published": "2019-06-06 07:48:36", "link": "http://arxiv.org/abs/1906.02461v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visualizing and Measuring the Geometry of BERT", "abstract": "Transformer architectures show significant promise for natural language\nprocessing. Given that a single pretrained model can be fine-tuned to perform\nwell on many different tasks, these networks appear to extract generally useful\nlinguistic features. A natural question is how such networks represent this\ninformation internally. This paper describes qualitative and quantitative\ninvestigations of one particularly effective model, BERT. At a high level,\nlinguistic features seem to be represented in separate semantic and syntactic\nsubspaces. We find evidence of a fine-grained geometric representation of word\nsenses. We also present empirical descriptions of syntactic representations in\nboth attention matrices and individual word embeddings, as well as a\nmathematical argument to explain the geometry of these representations.", "published": "2019-06-06 17:33:22", "link": "http://arxiv.org/abs/1906.02715v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Conversing by Reading: Contentful Neural Conversation with On-demand\n  Machine Reading", "abstract": "Although neural conversation models are effective in learning how to produce\nfluent responses, their primary challenge lies in knowing what to say to make\nthe conversation contentful and non-vacuous. We present a new end-to-end\napproach to contentful neural conversation that jointly models response\ngeneration and on-demand machine reading. The key idea is to provide the\nconversation model with relevant long-form text on the fly as a source of\nexternal knowledge. The model performs QA-style reading comprehension on this\ntext in response to each conversational turn, thereby allowing for more focused\nintegration of external knowledge than has been possible in prior approaches.\nTo support further research on knowledge-grounded conversation, we introduce a\nnew large-scale conversation dataset grounded in external web pages (2.8M\nturns, 7.4M sentences of grounding). Both human evaluation and automated\nmetrics show that our approach results in more contentful responses compared to\na variety of previous methods, improving both the informativeness and diversity\nof generated output.", "published": "2019-06-06 17:55:37", "link": "http://arxiv.org/abs/1906.02738v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Understanding and Improving Transformer From a Multi-Particle Dynamic\n  System Point of View", "abstract": "The Transformer architecture is widely used in natural language processing.\nDespite its success, the design principle of the Transformer remains elusive.\nIn this paper, we provide a novel perspective towards understanding the\narchitecture: we show that the Transformer can be mathematically interpreted as\na numerical Ordinary Differential Equation (ODE) solver for a\nconvection-diffusion equation in a multi-particle dynamic system. In\nparticular, how words in a sentence are abstracted into contexts by passing\nthrough the layers of the Transformer can be interpreted as approximating\nmultiple particles' movement in the space using the Lie-Trotter splitting\nscheme and the Euler's method. Given this ODE's perspective, the rich\nliterature of numerical analysis can be brought to guide us in designing\neffective structures beyond the Transformer. As an example, we propose to\nreplace the Lie-Trotter splitting scheme by the Strang-Marchuk splitting\nscheme, a scheme that is more commonly used and with much lower local\ntruncation errors. The Strang-Marchuk splitting scheme suggests that the\nself-attention and position-wise feed-forward network (FFN) sub-layers should\nnot be treated equally. Instead, in each layer, two position-wise FFN\nsub-layers should be used, and the self-attention sub-layer is placed in\nbetween. This leads to a brand new architecture. Such an FFN-attention-FFN\nlayer is \"Macaron-like\", and thus we call the network with this new\narchitecture the Macaron Net. Through extensive experiments, we show that the\nMacaron Net is superior to the Transformer on both supervised and unsupervised\nlearning tasks. The reproducible codes and pretrained models can be found at\nhttps://github.com/zhuohan123/macaron-net", "published": "2019-06-06 18:10:08", "link": "http://arxiv.org/abs/1906.02762v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Data Extraction from Charts via Single Deep Neural Network", "abstract": "Automatic data extraction from charts is challenging for two reasons: there\nexist many relations among objects in a chart, which is not a common\nconsideration in general computer vision problems; and different types of\ncharts may not be processed by the same model. To address these problems, we\npropose a framework of a single deep neural network, which consists of object\ndetection, text recognition and object matching modules. The framework handles\nboth bar and pie charts, and it may also be extended to other types of charts\nby slight revisions and by augmenting the training data. Our model performs\nsuccessfully on 79.4% of test simulated bar charts and 88.0% of test simulated\npie charts, while for charts outside of the training domain it degrades for\n57.5% and 62.3%, respectively.", "published": "2019-06-06 21:54:50", "link": "http://arxiv.org/abs/1906.11906v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Efficient Full-Rank Spatial Covariance Estimation Using Independent\n  Low-Rank Matrix Analysis for Blind Source Separation", "abstract": "In this paper, we propose a new algorithm that efficiently separates a\ndirectional source and diffuse background noise based on independent low-rank\nmatrix analysis (ILRMA). ILRMA is one of the state-of-the-art techniques of\nblind source separation (BSS) and is based on a rank-1 spatial model. Although\nsuch a model does not hold for diffuse noise, ILRMA can accurately estimate the\nspatial parameters of the directional source. Motivated by this fact, we\nutilize these estimates to restore the lost spatial basis of diffuse noise,\nwhich can be considered as an efficient full-rank spatial covariance\nestimation. BSS experiments show the efficacy of the proposed method in terms\nof the computational cost and separation performance.", "published": "2019-06-06 08:52:48", "link": "http://arxiv.org/abs/1906.02482v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Singing voice separation: a study on training data", "abstract": "In the recent years, singing voice separation systems showed increased\nperformance due to the use of supervised training. The design of training\ndatasets is known as a crucial factor in the performance of such systems. We\ninvestigate on how the characteristics of the training dataset impacts the\nseparation performances of state-of-the-art singing voice separation\nalgorithms. We show that the separation quality and diversity are two important\nand complementary assets of a good training dataset. We also provide insights\non possible transforms to perform data augmentation for this task.", "published": "2019-06-06 14:44:36", "link": "http://arxiv.org/abs/1906.02618v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GIBBONFINDR: An R package for the detection and classification of\n  acoustic signals", "abstract": "The recent improvements in recording technology, data storage and battery\nlife have led to an increased interest in the use of passive acoustic\nmonitoring for a variety of research questions. One of the main obstacles in\nimplementing wide scale acoustic monitoring programs in terrestrial\nenvironments is the lack of user-friendly, open source programs for processing\nlarge sound archives. Here we describe the new, open-source R package\nGIBBONFINDR which has functions for detection, classification and visualization\nof acoustic signals using a variety of readily available machine learning\nalgorithms in the R programming environment. We provide a case study showing\nhow GIBBONFINDR functions can be used in a workflow to detect and classify\nBornean gibbon (Hylobates muelleri) calls in long-term acoustic data sets\nrecorded in Danum Valley Conservation Area, Sabah, Malaysia. Machine learning\nis currently one of the most rapidly growing fields-- with applications across\nmany disciplines-- and our goal is to make commonly used signal processing\ntechniques and machine learning algorithms readily available for ecologists who\nare interested in incorporating bioacoustics techniques into their research.", "published": "2019-06-06 13:28:12", "link": "http://arxiv.org/abs/1906.02572v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "q-bio.QM"], "primary_category": "eess.AS"}
