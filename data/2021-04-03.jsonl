{"title": "Intent Recognition and Unsupervised Slot Identification for Low\n  Resourced Spoken Dialog Systems", "abstract": "Intent Recognition and Slot Identification are crucial components in spoken\nlanguage understanding (SLU) systems. In this paper, we present a novel\napproach towards both these tasks in the context of low resourced and unwritten\nlanguages. We present an acoustic based SLU system that converts speech to its\nphonetic transcription using a universal phone recognition system. We build a\nword-free natural language understanding module that does intent recognition\nand slot identification from these phonetic transcription. Our proposed SLU\nsystem performs competitively for resource rich scenarios and significantly\noutperforms existing approaches as the amount of available data reduces. We\nobserve more than 10% improvement for intent classification in Tamil and more\nthan 5% improvement for intent classification in Sinhala. We also present a\nnovel approach towards unsupervised slot identification using normalized\nattention scores. This approach can be used for unsupervised slot labelling,\ndata augmentation and to generate data for a new slot in a one-shot way with\nonly one speech recording", "published": "2021-04-03 01:58:27", "link": "http://arxiv.org/abs/2104.01287v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Linguistic Diversity During COVID-19", "abstract": "Computational measures of linguistic diversity help us understand the\nlinguistic landscape using digital language data. The contribution of this\npaper is to calibrate measures of linguistic diversity using restrictions on\ninternational travel resulting from the COVID-19 pandemic. Previous work has\nmapped the distribution of languages using geo-referenced social media and web\ndata. The goal, however, has been to describe these corpora themselves rather\nthan to make inferences about underlying populations. This paper shows that a\ndifference-in-differences method based on the Herfindahl-Hirschman Index can\nidentify the bias in digital corpora that is introduced by non-local\npopulations. These methods tell us where significant changes have taken place\nand whether this leads to increased or decreased diversity. This is an\nimportant step in aligning digital corpora like social media with the\nreal-world populations that have produced them.", "published": "2021-04-03 02:09:37", "link": "http://arxiv.org/abs/2104.01290v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Representations of Language Varieties Are Reliable Given Corpus\n  Similarity Measures", "abstract": "This paper measures similarity both within and between 84 language varieties\nacross nine languages. These corpora are drawn from digital sources (the web\nand tweets), allowing us to evaluate whether such geo-referenced corpora are\nreliable for modelling linguistic variation. The basic idea is that, if each\nsource adequately represents a single underlying language variety, then the\nsimilarity between these sources should be stable across all languages and\ncountries. The paper shows that there is a consistent agreement between these\nsources using frequency-based corpus similarity measures. This provides further\nevidence that digital geo-referenced corpora consistently represent local\nlanguage varieties.", "published": "2021-04-03 02:19:46", "link": "http://arxiv.org/abs/2104.01294v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Unit Directional Measures of Association: Moving Beyond Pairs of\n  Words", "abstract": "This paper formulates and evaluates a series of multi-unit measures of\ndirectional association, building on the pairwise {\\Delta}P measure, that are\nable to quantify association in sequences of varying length and type of\nrepresentation. Multi-unit measures face an additional segmentation problem:\nonce the implicit length constraint of pairwise measures is abandoned,\nassociation measures must also identify the borders of meaningful sequences.\nThis paper takes a vector-based approach to the segmentation problem by using\n18 unique measures to describe different aspects of multi-unit association. An\nexamination of these measures across eight languages shows that they are stable\nacross languages and that each provides a unique rank of associated sequences.\nTaken together, these measures expand corpus-based approaches to association by\ngeneralizing across varying lengths and types of representation.", "published": "2021-04-03 02:43:24", "link": "http://arxiv.org/abs/2104.01297v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding Variants for Construction-Based Dialectometry: A Corpus-Based\n  Approach to Regional CxGs", "abstract": "This paper develops a construction-based dialectometry capable of identifying\npreviously unknown constructions and measuring the degree to which a given\nconstruction is subject to regional variation. The central idea is to learn a\ngrammar of constructions (a CxG) using construction grammar induction and then\nto use these constructions as features for dialectometry. This offers a method\nfor measuring the aggregate similarity between regional CxGs without limiting\nin advance the set of constructions subject to variation. The learned CxG is\nevaluated on how well it describes held-out test corpora while dialectometry is\nevaluated on how well it can model regional varieties of English. Themethod is\ntested using two distinct datasets: First, the International Corpus of English\nrepresenting eight outer circle varieties; Second, a web-crawled corpus\nrepresenting five inner circle varieties. Results show that themethod (1)\nproduces a grammar with stable quality across sub-sets of a single corpus that\nis (2) capable of distinguishing between regional varieties of Englishwith a\nhigh degree of accuracy, thus (3) supporting dialectometricmethods formeasuring\nthe similarity between varieties of English and (4) measuring the degree to\nwhich each construction is subject to regional variation. This is important for\ncognitive sociolinguistics because it operationalizes the idea that competition\nbetween constructions is organized at the functional level so that\ndialectometry needs to represent as much of the available functional space as\npossible.", "published": "2021-04-03 02:52:14", "link": "http://arxiv.org/abs/2104.01299v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Global Syntactic Variation in Seven Languages: Towards a Computational\n  Dialectology", "abstract": "The goal of this paper is to provide a complete representation of regional\nlinguistic variation on a global scale. To this end, the paper focuses on\nremoving three constraints that have previously limited work within\ndialectology/dialectometry. First, rather than assuming a fixed and incomplete\nset of variants, we use Computational Construction Grammar to provide a\nreplicable and falsifiable set of syntactic features. Second, rather than\nassuming a specific area of interest, we use global language mapping based on\nweb-crawled and social media datasets to determine the selection of national\nvarieties. Third, rather than looking at a single language in isolation, we\nmodel seven major languages together using the same methods: Arabic, English,\nFrench, German, Portuguese, Russian, and Spanish. Results show that models for\neach language are able to robustly predict the region-of-origin of held-out\nsamples better using Construction Grammars than using simpler syntactic\nfeatures. These global-scale experiments are used to argue that new methods in\ncomputational sociolinguistics are able to provide more generalized models of\nregional variation that are essential for understanding language variation and\nchange at scale.", "published": "2021-04-03 03:40:21", "link": "http://arxiv.org/abs/2104.01306v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Counts@IITK at SemEval-2021 Task 8: SciBERT Based Entity And Semantic\n  Relation Extraction For Scientific Data", "abstract": "This paper presents the system for SemEval 2021 Task 8 (MeasEval). MeasEval\nis a novel span extraction, classification, and relation extraction task\nfocused on finding quantities, attributes of these quantities, and additional\ninformation, including the related measured entities, properties, and\nmeasurement contexts. Our submitted system, which placed fifth (team rank) on\nthe leaderboard, consisted of SciBERT with [CLS] token embedding and CRF layer\non top. We were also placed first in Quantity (tied) and Unit subtasks, second\nin MeasuredEntity, Modifier and Qualifies subtasks, and third in Qualifier\nsubtask.", "published": "2021-04-03 09:47:51", "link": "http://arxiv.org/abs/2104.01364v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Convex Aggregation for Opinion Summarization", "abstract": "Recent advances in text autoencoders have significantly improved the quality\nof the latent space, which enables models to generate grammatical and\nconsistent text from aggregated latent vectors. As a successful application of\nthis property, unsupervised opinion summarization models generate a summary by\ndecoding the aggregated latent vectors of inputs. More specifically, they\nperform the aggregation via simple average. However, little is known about how\nthe vector aggregation step affects the generation quality. In this study, we\nrevisit the commonly used simple average approach by examining the latent space\nand generated summaries. We found that text autoencoders tend to generate\noverly generic summaries from simply averaged latent vectors due to an\nunexpected $L_2$-norm shrinkage in the aggregated latent vectors, which we\nrefer to as summary vector degeneration. To overcome this issue, we develop a\nframework Coop, which searches input combinations for the latent vector\naggregation using input-output word overlap. Experimental results show that\nCoop successfully alleviates the summary vector degeneration issue and\nestablishes new state-of-the-art performance on two opinion summarization\nbenchmarks. Code is available at \\url{https://github.com/megagonlabs/coop}.", "published": "2021-04-03 10:52:14", "link": "http://arxiv.org/abs/2104.01371v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reinforcement Learning for Emotional Text-to-Speech Synthesis with\n  Improved Emotion Discriminability", "abstract": "Emotional text-to-speech synthesis (ETTS) has seen much progress in recent\nyears. However, the generated voice is often not perceptually identifiable by\nits intended emotion category. To address this problem, we propose a new\ninteractive training paradigm for ETTS, denoted as i-ETTS, which seeks to\ndirectly improve the emotion discriminability by interacting with a speech\nemotion recognition (SER) model. Moreover, we formulate an iterative training\nstrategy with reinforcement learning to ensure the quality of i-ETTS\noptimization. Experimental results demonstrate that the proposed i-ETTS\noutperforms the state-of-the-art baselines by rendering speech with more\naccurate emotion style. To our best knowledge, this is the first study of\nreinforcement learning in emotional text-to-speech synthesis.", "published": "2021-04-03 13:52:47", "link": "http://arxiv.org/abs/2104.01408v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sexism detection: The first corpus in Algerian dialect with a\n  code-switching in Arabic/ French and English", "abstract": "In this paper, an approach for hate speech detection against women in Arabic\ncommunity on social media (e.g. Youtube) is proposed. In the literature,\nsimilar works have been presented for other languages such as English. However,\nto the best of our knowledge, not much work has been conducted in the Arabic\nlanguage. A new hate speech corpus (Arabic\\_fr\\_en) is developed using three\ndifferent annotators. For corpus validation, three different machine learning\nalgorithms are used, including deep Convolutional Neural Network (CNN), long\nshort-term memory (LSTM) network and Bi-directional LSTM (Bi-LSTM) network.\nSimulation results demonstrate the best performance of the CNN model, which\nachieved F1-score up to 86\\% for the unbalanced corpus as compared to LSTM and\nBi-LSTM.", "published": "2021-04-03 16:34:51", "link": "http://arxiv.org/abs/2104.01443v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Role of BERT Token Representations to Explain Sentence\n  Probing Results", "abstract": "Several studies have been carried out on revealing linguistic features\ncaptured by BERT. This is usually achieved by training a diagnostic classifier\non the representations obtained from different layers of BERT. The subsequent\nclassification accuracy is then interpreted as the ability of the model in\nencoding the corresponding linguistic property. Despite providing insights,\nthese studies have left out the potential role of token representations. In\nthis paper, we provide a more in-depth analysis on the representation space of\nBERT in search for distinct and meaningful subspaces that can explain the\nreasons behind these probing results. Based on a set of probing tasks and with\nthe help of attribution methods we show that BERT tends to encode meaningful\nknowledge in specific token representations (which are often ignored in\nstandard classification setups), allowing the model to detect syntactic and\nsemantic abnormalities, and to distinctively separate grammatical number and\ntense subspaces.", "published": "2021-04-03 20:40:42", "link": "http://arxiv.org/abs/2104.01477v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fingerspelling Detection in American Sign Language", "abstract": "Fingerspelling, in which words are signed letter by letter, is an important\ncomponent of American Sign Language. Most previous work on automatic\nfingerspelling recognition has assumed that the boundaries of fingerspelling\nregions in signing videos are known beforehand. In this paper, we consider the\ntask of fingerspelling detection in raw, untrimmed sign language videos. This\nis an important step towards building real-world fingerspelling recognition\nsystems. We propose a benchmark and a suite of evaluation metrics, some of\nwhich reflect the effect of detection on the downstream fingerspelling\nrecognition task. In addition, we propose a new model that learns to detect\nfingerspelling via multi-task training, incorporating pose estimation and\nfingerspelling recognition (transcription) along with detection, and compare\nthis model to several alternatives. The model outperforms all alternative\napproaches across all metrics, establishing a state of the art on the\nbenchmark.", "published": "2021-04-03 02:11:09", "link": "http://arxiv.org/abs/2104.01291v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "From n-grams to trees in Lindenmayer systems", "abstract": "In this paper we present two approaches to Lindenmayer systems: the\nrule-based (or generative) approach, which focuses on L-systems as Thue\nrewriting systems and a constraint-based (or model-theoretic) approach, in\nwhich rules are abandoned in favour of conditions over allowable expressions in\nthe language (Pullum, 2019). We will argue that it is possible, for at least a\nsubset of L-systems and the languages they generate, to map string\nadmissibility conditions (the 'Three Laws') to local tree admissibility\nconditions (cf. Rogers, 1997). This is equivalent to defining a model for those\nlanguages. We will work out how to construct structure assuming only\nsuperficial constraints on expressions, and define a set of constraints that\nwell-formed expressions of specific L-languages must satisfy. We will see that\nL-systems that other methods distinguish turn out to satisfy the same model.", "published": "2021-04-03 09:42:44", "link": "http://arxiv.org/abs/2104.01363v1", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "ExKaldi-RT: A Real-Time Automatic Speech Recognition Extension Toolkit\n  of Kaldi", "abstract": "This paper describes the ExKaldi-RT online automatic speech recognition (ASR)\ntoolkit that is implemented based on the Kaldi ASR toolkit and Python language.\nExKaldi-RT provides tools for building online recognition pipelines. While\nsimilar tools are available built on Kaldi, a key feature of ExKaldi-RT that it\nworks on Python, which has an easy-to-use interface that allows online ASR\nsystem developers to develop original research, such as by applying neural\nnetwork-based signal processing and by decoding model trained with deep\nlearning frameworks. We performed benchmark experiments on the minimum\nLibriSpeech corpus, and it showed that ExKaldi-RT could achieve competitive ASR\nperformance in real-time recognition.", "published": "2021-04-03 12:16:19", "link": "http://arxiv.org/abs/2104.01384v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "On-the-Fly Aligned Data Augmentation for Sequence-to-Sequence ASR", "abstract": "We propose an on-the-fly data augmentation method for automatic speech\nrecognition (ASR) that uses alignment information to generate effective\ntraining samples. Our method, called Aligned Data Augmentation (ADA) for ASR,\nreplaces transcribed tokens and the speech representations in an aligned manner\nto generate previously unseen training pairs. The speech representations are\nsampled from an audio dictionary that has been extracted from the training\ncorpus and inject speaker variations into the training examples. The\ntranscribed tokens are either predicted by a language model such that the\naugmented data pairs are semantically close to the original data, or randomly\nsampled. Both strategies result in training pairs that improve robustness in\nASR training. Our experiments on a Seq-to-Seq architecture show that ADA can be\napplied on top of SpecAugment, and achieves about 9-23% and 4-15% relative\nimprovements in WER over SpecAugment alone on LibriSpeech 100h and LibriSpeech\n960h test datasets, respectively.", "published": "2021-04-03 13:00:00", "link": "http://arxiv.org/abs/2104.01393v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Unsupervised Domain Adaptation with Global and Local Graph Neural\n  Networks in Limited Labeled Data Scenario: Application to Disaster Management", "abstract": "Identification and categorization of social media posts generated during\ndisasters are crucial to reduce the sufferings of the affected people. However,\nlack of labeled data is a significant bottleneck in learning an effective\ncategorization system for a disaster. This motivates us to study the problem as\nunsupervised domain adaptation (UDA) between a previous disaster with labeled\ndata (source) and a current disaster (target). However, if the amount of\nlabeled data available is limited, it restricts the learning capabilities of\nthe model. To handle this challenge, we utilize limited labeled data along with\nabundantly available unlabeled data, generated during a source disaster to\npropose a novel two-part graph neural network. The first-part extracts\ndomain-agnostic global information by constructing a token level graph across\ndomains and the second-part preserves local instance-level semantics. In our\nexperiments, we show that the proposed method outperforms state-of-the-art\ntechniques by $2.74\\%$ weighted F$_1$ score on average on two standard public\ndataset in the area of disaster management. We also report experimental results\nfor granular actionable multi-label classification datasets in disaster domain\nfor the first time, on which we outperform BERT by $3.00\\%$ on average w.r.t\nweighted F$_1$. Additionally, we show that our approach can retain performance\nwhen very limited labeled data is available.", "published": "2021-04-03 16:01:03", "link": "http://arxiv.org/abs/2104.01436v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Few-Shot Keyword Spotting in Any Language", "abstract": "We introduce a few-shot transfer learning method for keyword spotting in any\nlanguage. Leveraging open speech corpora in nine languages, we automate the\nextraction of a large multilingual keyword bank and use it to train an\nembedding model. With just five training examples, we fine-tune the embedding\nmodel for keyword spotting and achieve an average F1 score of 0.75 on keyword\nclassification for 180 new keywords unseen by the embedding model in these nine\nlanguages. This embedding model also generalizes to new languages. We achieve\nan average F1 score of 0.65 on 5-shot models for 260 keywords sampled across 13\nnew languages unseen by the embedding model. We investigate streaming accuracy\nfor our 5-shot models in two contexts: keyword spotting and keyword search.\nAcross 440 keywords in 22 languages, we achieve an average streaming keyword\nspotting accuracy of 87.4% with a false acceptance rate of 4.3%, and observe\npromising initial results on keyword search.", "published": "2021-04-03 17:27:37", "link": "http://arxiv.org/abs/2104.01454v4", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "speechocean762: An Open-Source Non-native English Speech Corpus For\n  Pronunciation Assessment", "abstract": "This paper introduces a new open-source speech corpus named \"speechocean762\"\ndesigned for pronunciation assessment use, consisting of 5000 English\nutterances from 250 non-native speakers, where half of the speakers are\nchildren. Five experts annotated each of the utterances at sentence-level,\nword-level and phoneme-level. A baseline system is released in open source to\nillustrate the phoneme-level pronunciation assessment workflow on this corpus.\nThis corpus is allowed to be used freely for commercial and non-commercial\npurposes. It is available for free download from OpenSLR, and the corresponding\nbaseline system is published in the Kaldi speech recognition toolkit.", "published": "2021-04-03 11:31:59", "link": "http://arxiv.org/abs/2104.01378v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MMBERT: Multimodal BERT Pretraining for Improved Medical VQA", "abstract": "Images in the medical domain are fundamentally different from the general\ndomain images. Consequently, it is infeasible to directly employ general domain\nVisual Question Answering (VQA) models for the medical domain. Additionally,\nmedical images annotation is a costly and time-consuming process. To overcome\nthese limitations, we propose a solution inspired by self-supervised\npretraining of Transformer-style architectures for NLP, Vision and Language\ntasks. Our method involves learning richer medical image and text semantic\nrepresentations using Masked Language Modeling (MLM) with image features as the\npretext task on a large medical image+caption dataset. The proposed solution\nachieves new state-of-the-art performance on two VQA datasets for radiology\nimages -- VQA-Med 2019 and VQA-RAD, outperforming even the ensemble models of\nprevious best solutions. Moreover, our solution provides attention maps which\nhelp in model interpretability. The code is available at\nhttps://github.com/VirajBagal/MMBERT", "published": "2021-04-03 13:01:19", "link": "http://arxiv.org/abs/2104.01394v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Medical Entity Disambiguation Using Graph Neural Networks", "abstract": "Medical knowledge bases (KBs), distilled from biomedical literature and\nregulatory actions, are expected to provide high-quality information to\nfacilitate clinical decision making. Entity disambiguation (also referred to as\nentity linking) is considered as an essential task in unlocking the wealth of\nsuch medical KBs. However, existing medical entity disambiguation methods are\nnot adequate due to word discrepancies between the entities in the KB and the\ntext snippets in the source documents. Recently, graph neural networks (GNNs)\nhave proven to be very effective and provide state-of-the-art results for many\nreal-world applications with graph-structured data. In this paper, we introduce\nED-GNN based on three representative GNNs (GraphSAGE, R-GCN, and MAGNN) for\nmedical entity disambiguation. We develop two optimization techniques to\nfine-tune and improve ED-GNN. First, we introduce a novel strategy to represent\nentities that are mentioned in text snippets as a query graph. Second, we\ndesign an effective negative sampling strategy that identifies hard negative\nsamples to improve the model's disambiguation capability. Compared to the best\nperforming state-of-the-art solutions, our ED-GNN offers an average improvement\nof 7.3% in terms of F1 score on five real-world datasets.", "published": "2021-04-03 22:04:15", "link": "http://arxiv.org/abs/2104.01488v1", "categories": ["cs.IR", "cs.CL", "cs.DB"], "primary_category": "cs.IR"}
{"title": "Deep Feature CycleGANs: Speaker Identity Preserving Non-parallel\n  Microphone-Telephone Domain Adaptation for Speaker Verification", "abstract": "With the increase in the availability of speech from varied domains, it is\nimperative to use such out-of-domain data to improve existing speech systems.\nDomain adaptation is a prominent pre-processing approach for this. We\ninvestigate it for adapt microphone speech to the telephone domain.\nSpecifically, we explore CycleGAN-based unpaired translation of microphone data\nto improve the x-vector/speaker embedding network for Telephony Speaker\nVerification. We first demonstrate the efficacy of this on real challenging\ndata and then, to improve further, we modify the CycleGAN formulation to make\nthe adaptation task-specific. We modify CycleGAN's identity loss,\ncycle-consistency loss, and adversarial loss to operate in the deep feature\nspace. Deep features of a signal are extracted from an auxiliary (speaker\nembedding) network and, hence, preserves speaker identity. Our 3D\nconvolution-based Deep Feature Discriminators (DFD) show relative improvements\nof 5-10% in terms of equal error rate. To dive deeper, we study a challenging\nscenario of pooling (adapted) microphone and telephone data with data\naugmentations and telephone codecs. Finally, we highlight the sensitivity of\nCycleGAN hyper-parameters and introduce a parameter called probability of\nadaptation.", "published": "2021-04-03 15:54:00", "link": "http://arxiv.org/abs/2104.01433v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "ECAPA-TDNN Embeddings for Speaker Diarization", "abstract": "Learning robust speaker embeddings is a crucial step in speaker diarization.\nDeep neural networks can accurately capture speaker discriminative\ncharacteristics and popular deep embeddings such as x-vectors are nowadays a\nfundamental component of modern diarization systems. Recently, some\nimprovements over the standard TDNN architecture used for x-vectors have been\nproposed. The ECAPA-TDNN model, for instance, has shown impressive performance\nin the speaker verification domain, thanks to a carefully designed neural\nmodel.\n  In this work, we extend, for the first time, the use of the ECAPA-TDNN model\nto speaker diarization. Moreover, we improved its robustness with a powerful\naugmentation scheme that concatenates several contaminated versions of the same\nsignal within the same training batch. The ECAPA-TDNN model turned out to\nprovide robust speaker embeddings under both close-talking and distant-talking\nconditions. Our results on the popular AMI meeting corpus show that our system\nsignificantly outperforms recently proposed approaches.", "published": "2021-04-03 19:37:51", "link": "http://arxiv.org/abs/2104.01466v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Adversarial Joint Training with Self-Attention Mechanism for Robust\n  End-to-End Speech Recognition", "abstract": "Lately, the self-attention mechanism has marked a new milestone in the field\nof automatic speech recognition (ASR). Nevertheless, its performance is\nsusceptible to environmental intrusions as the system predicts the next output\nsymbol depending on the full input sequence and the previous predictions.\nInspired by the extensive applications of the generative adversarial networks\n(GANs) in speech enhancement and ASR tasks, we propose an adversarial joint\ntraining framework with the self-attention mechanism to boost the noise\nrobustness of the ASR system. Generally, it consists of a self-attention speech\nenhancement GAN and a self-attention end-to-end ASR model. There are two\nhighlights which are worth noting in this proposed framework. One is that it\nbenefits from the advancement of both self-attention mechanism and GANs; while\nthe other is that the discriminator of GAN plays the role of the global\ndiscriminant network in the stage of the adversarial joint training, which\nguides the enhancement front-end to capture more compatible structures for the\nsubsequent ASR module and thereby offsets the limitation of the separate\ntraining and handcrafted loss functions. With the adversarial joint\noptimization, the proposed framework is expected to learn more robust\nrepresentations suitable for the ASR task. We execute systematic experiments on\nthe corpus AISHELL-1, and the experimental results show that on the artificial\nnoisy test set, the proposed framework achieves the relative improvements of\n66% compared to the ASR model trained by clean data solely, 35.1% compared to\nthe speech enhancement & ASR scheme without joint training, and 5.3% compared\nto multi-condition training.", "published": "2021-04-03 19:58:51", "link": "http://arxiv.org/abs/2104.01471v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Hi-Fi Multi-Speaker English TTS Dataset", "abstract": "This paper introduces a new multi-speaker English dataset for training\ntext-to-speech models. The dataset is based on LibriVox audiobooks and Project\nGutenberg texts, both in the public domain. The new dataset contains about 292\nhours of speech from 10 speakers with at least 17 hours per speaker sampled at\n44.1 kHz. To select speech samples with high quality, we considered audio\nrecordings with a signal bandwidth of at least 13 kHz and a signal-to-noise\nratio (SNR) of at least 32 dB. The dataset is publicly released at\nhttp://www.openslr.org/109/ .", "published": "2021-04-03 23:19:50", "link": "http://arxiv.org/abs/2104.01497v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Diarization of Legal Proceedings. Identifying and Transcribing Judicial\n  Speech from Recorded Court Audio", "abstract": "United States Courts make audio recordings of oral arguments available as\npublic record, but these recordings rarely include speaker annotations. This\npaper addresses the Speech Audio Diarization problem, answering the question of\n\"Who spoke when?\" in the domain of judicial oral argument proceedings. We\npresent a workflow for diarizing the speech of judges using audio recordings of\noral arguments, a process we call Reference-Dependent Speaker Verification. We\nutilize a speech embedding network trained with the Generalized End-to-End Loss\nto encode speech into d-vectors and a pre-defined reference audio library based\non annotated data. We find that by encoding reference audio for speakers and\nfull arguments and computing similarity scores we achieve a 13.8% Diarization\nError Rate for speakers covered by the reference audio library on a held-out\ntest set. We evaluate our method on the Supreme Court of the United States oral\narguments, accessed through the Oyez Project, and outline future work for\ndiarizing legal proceedings. A code repository for this research is available\nat github.com/JeffT13/rd-diarization", "published": "2021-04-03 03:31:41", "link": "http://arxiv.org/abs/2104.01304v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Empirical Study on Channel Effects for Synthetic Voice Spoofing\n  Countermeasure Systems", "abstract": "Spoofing countermeasure (CM) systems are critical in speaker verification;\nthey aim to discern spoofing attacks from bona fide speech trials. In practice,\nhowever, acoustic condition variability in speech utterances may significantly\ndegrade the performance of CM systems. In this paper, we conduct a\ncross-dataset study on several state-of-the-art CM systems and observe\nsignificant performance degradation compared with their single-dataset\nperformance. Observing differences of average magnitude spectra of bona fide\nutterances across the datasets, we hypothesize that channel mismatch among\nthese datasets is one important reason. We then verify it by demonstrating a\nsimilar degradation of CM systems trained on original but evaluated on\nchannel-shifted data. Finally, we propose several channel robust strategies\n(data augmentation, multi-task learning, adversarial learning) for CM systems,\nand observe a significant performance improvement on cross-dataset experiments.", "published": "2021-04-03 06:32:44", "link": "http://arxiv.org/abs/2104.01320v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Diff-TTS: A Denoising Diffusion Model for Text-to-Speech", "abstract": "Although neural text-to-speech (TTS) models have attracted a lot of attention\nand succeeded in generating human-like speech, there is still room for\nimprovements to its naturalness and architectural efficiency. In this work, we\npropose a novel non-autoregressive TTS model, namely Diff-TTS, which achieves\nhighly natural and efficient speech synthesis. Given the text, Diff-TTS\nexploits a denoising diffusion framework to transform the noise signal into a\nmel-spectrogram via diffusion time steps. In order to learn the mel-spectrogram\ndistribution conditioned on the text, we present a likelihood-based\noptimization method for TTS. Furthermore, to boost up the inference speed, we\nleverage the accelerated sampling method that allows Diff-TTS to generate raw\nwaveforms much faster without significantly degrading perceptual quality.\nThrough experiments, we verified that Diff-TTS generates 28 times faster than\nthe real-time with a single NVIDIA 2080Ti GPU.", "published": "2021-04-03 13:53:19", "link": "http://arxiv.org/abs/2104.01409v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Mixture of orthogonal sequences made from extended time-stretched pulses\n  enables measurement of involuntary voice fundamental frequency response to\n  pitch perturbation", "abstract": "Auditory feedback plays an essential role in the regulation of the\nfundamental frequency of voiced sounds. The fundamental frequency also responds\nto auditory stimulation other than the speaker's voice. We propose to use this\nresponse of the fundamental frequency of sustained vowels to\nfrequency-modulated test signals for investigating involuntary control of voice\npitch. This involuntary response is difficult to identify and isolate by the\nconventional paradigm, which uses step-shaped pitch perturbation. We recently\ndeveloped a versatile measurement method using a mixture of orthogonal\nsequences made from a set of extended time-stretched pulses (TSP). In this\narticle, we extended our approach and designed a set of test signals using the\nmixture to modulate the fundamental frequency of artificial signals. For\ntesting the response, the experimenter presents the modulated signal aurally\nwhile the subject is voicing sustained vowels. We developed a tool for\nconducting this test quickly and interactively. We make the tool available as\nan open-source and also provide executable GUI-based applications. Preliminary\ntests revealed that the proposed method consistently provides compensatory\nresponses with about 100 ms latency, representing involuntary control. Finally,\nwe discuss future applications of the proposed method for objective and\nnon-invasive auditory response measurements.", "published": "2021-04-03 16:35:05", "link": "http://arxiv.org/abs/2104.01444v1", "categories": ["cs.SD", "eess.AS", "eess.SP", "92C55"], "primary_category": "cs.SD"}
{"title": "Cross-Modal learning for Audio-Visual Video Parsing", "abstract": "In this paper, we present a novel approach to the audio-visual video parsing\n(AVVP) task that demarcates events from a video separately for audio and visual\nmodalities. The proposed parsing approach simultaneously detects the temporal\nboundaries in terms of start and end times of such events. We show how AVVP can\nbenefit from the following techniques geared towards effective cross-modal\nlearning: (i) adversarial training and skip connections (ii) global context\naware attention and, (iii) self-supervised pretraining using an audio-video\ngrounding objective to obtain cross-modal audio-video representations. We\npresent extensive experimental evaluations on the Look, Listen, and Parse (LLP)\ndataset and show that we outperform the state-of-the-art Hybrid Attention\nNetwork (HAN) on all five metrics proposed for AVVP. We also present several\nablations to validate the effect of pretraining, global attention and\nadversarial training.", "published": "2021-04-03 07:07:21", "link": "http://arxiv.org/abs/2104.04598v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS", "eess.IV"], "primary_category": "cs.SD"}
