{"title": "Improving Sentiment Analysis By Emotion Lexicon Approach on Vietnamese\n  Texts", "abstract": "The sentiment analysis task has various applications in practice. In the\nsentiment analysis task, words and phrases that represent positive and negative\nemotions are important. Finding out the words that represent the emotion from\nthe text can improve the performance of the classification models for the\nsentiment analysis task. In this paper, we propose a methodology that combines\nthe emotion lexicon with the classification model to enhance the accuracy of\nthe models. Our experimental results show that the emotion lexicon combined\nwith the classification model improves the performance of models.", "published": "2022-10-05 07:34:07", "link": "http://arxiv.org/abs/2210.02063v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Sentence Textual Similarity with Compositional Phrase\n  Semantics", "abstract": "Measuring Sentence Textual Similarity (STS) is a classic task that can be\napplied to many downstream NLP applications such as text generation and\nretrieval. In this paper, we focus on unsupervised STS that works on various\ndomains but only requires minimal data and computational resources.\nTheoretically, we propose a light-weighted Expectation-Correction (EC)\nformulation for STS computation. EC formulation unifies unsupervised STS\napproaches including the cosine similarity of Additively Composed (AC) sentence\nembeddings, Optimal Transport (OT), and Tree Kernels (TK). Moreover, we propose\nthe Recursive Optimal Transport Similarity (ROTS) algorithm to capture the\ncompositional phrase semantics by composing multiple recursive EC formulations.\nROTS finishes in linear time and is faster than its predecessors. ROTS is\nempirically more effective and scalable than previous approaches. Extensive\nexperiments on 29 STS tasks under various settings show the clear advantage of\nROTS over existing approaches. Detailed ablation studies demonstrate the\neffectiveness of our approaches.", "published": "2022-10-05 14:14:04", "link": "http://arxiv.org/abs/2210.02284v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Schema Encoding for Transferable Dialogue State Tracking", "abstract": "Dialogue state tracking (DST) is an essential sub-task for task-oriented\ndialogue systems. Recent work has focused on deep neural models for DST.\nHowever, the neural models require a large dataset for training. Furthermore,\napplying them to another domain needs a new dataset because the neural models\nare generally trained to imitate the given dataset. In this paper, we propose\nSchema Encoding for Transferable Dialogue State Tracking (SETDST), which is a\nneural DST method for effective transfer to new domains. Transferable DST could\nassist developments of dialogue systems even with few dataset on target\ndomains. We use a schema encoder not just to imitate the dataset but to\ncomprehend the schema of the dataset. We aim to transfer the model to new\ndomains by encoding new schemas and using them for DST on multi-domain\nsettings. As a result, SET-DST improved the joint accuracy by 1.46 points on\nMultiWOZ 2.1.", "published": "2022-10-05 15:53:06", "link": "http://arxiv.org/abs/2210.02351v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks", "abstract": "Few-shot prompting is a surprisingly powerful way to use Large Language\nModels (LLMs) to solve various tasks. However, this approach struggles as the\ntask complexity increases or when the individual reasoning steps of the task\nthemselves are hard to learn, especially when embedded in more complex tasks.\nTo address this, we propose Decomposed Prompting, a new approach to solve\ncomplex tasks by decomposing them (via prompting) into simpler sub-tasks that\ncan be delegated to a library of prompting-based LLMs dedicated to these\nsub-tasks. This modular structure allows each prompt to be optimized for its\nspecific sub-task, further decomposed if necessary, and even easily replaced\nwith more effective prompts, trained models, or symbolic functions if desired.\nWe show that the flexibility and modularity of Decomposed Prompting allows it\nto outperform prior work on few-shot prompting using GPT3. On symbolic\nreasoning tasks, we can further decompose sub-tasks that are hard for LLMs into\neven simpler solvable sub-tasks. When the complexity comes from the input\nlength, we can recursively decompose the task into the same task but with\nsmaller inputs. We also evaluate our approach on textual multi-step reasoning\ntasks: on long-context multi-hop QA task, we can more effectively teach the\nsub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA,\nwe can incorporate a symbolic information retrieval within our decomposition\nframework, leading to improved performance on both tasks. Datasets, Code and\nPrompts available at https://github.com/allenai/DecomP.", "published": "2022-10-05 17:28:20", "link": "http://arxiv.org/abs/2210.02406v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ask Me Anything: A simple strategy for prompting language models", "abstract": "Large language models (LLMs) transfer well to new tasks out-of-the-box simply\ngiven a natural language prompt that demonstrates how to perform the task and\nno additional training. Prompting is a brittle process wherein small\nmodifications to the prompt can cause large variations in the model\npredictions, and therefore significant effort is dedicated towards designing a\npainstakingly \"perfect prompt\" for a task. To mitigate the high degree of\neffort involved in prompt-design, we instead ask whether producing multiple\neffective, yet imperfect, prompts and aggregating them can lead to a high\nquality prompting strategy. Our observations motivate our proposed prompting\nmethod, ASK ME ANYTHING (AMA). We first develop an understanding of the\neffective prompt formats, finding that question-answering (QA) prompts, which\nencourage open-ended generation (\"Who went to the park?\") tend to outperform\nthose that restrict the model outputs (\"John went to the park. Output True or\nFalse.\"). Our approach recursively uses the LLM itself to transform task inputs\nto the effective QA format. We apply the collected prompts to obtain several\nnoisy votes for the input's true label. We find that the prompts can have very\ndifferent accuracies and complex dependencies and thus propose to use weak\nsupervision, a procedure for combining the noisy predictions, to produce the\nfinal predictions for the inputs. We evaluate AMA across open-source model\nfamilies (e.g., EleutherAI, BLOOM, OPT, and T0) and model sizes (125M-175B\nparameters), demonstrating an average performance lift of 10.2% over the\nfew-shot baseline. This simple strategy enables the open-source GPT-J-6B model\nto match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular\nbenchmarks. Averaged across these tasks, the GPT-J-6B model outperforms\nfew-shot GPT3-175B. We release our code here:\nhttps://github.com/HazyResearch/ama_prompting", "published": "2022-10-05 17:59:45", "link": "http://arxiv.org/abs/2210.02441v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Syllables in Language Modelling and their Application on\n  Low-Resource Machine Translation", "abstract": "Language modelling and machine translation tasks mostly use subword or\ncharacter inputs, but syllables are seldom used. Syllables provide shorter\nsequences than characters, require less-specialised extracting rules than\nmorphemes, and their segmentation is not impacted by the corpus size. In this\nstudy, we first explore the potential of syllables for open-vocabulary language\nmodelling in 21 languages. We use rule-based syllabification methods for six\nlanguages and address the rest with hyphenation, which works as a\nsyllabification proxy. With a comparable perplexity, we show that syllables\noutperform characters and other subwords. Moreover, we study the importance of\nsyllables on neural machine translation for a non-related and low-resource\nlanguage-pair (Spanish--Shipibo-Konibo). In pairwise and multilingual systems,\nsyllables outperform unsupervised subwords, and further morphological\nsegmentation methods, when translating into a highly synthetic language with a\ntransparent orthography (Shipibo-Konibo). Finally, we perform some human\nevaluation, and discuss limitations and opportunities.", "published": "2022-10-05 18:55:52", "link": "http://arxiv.org/abs/2210.02509v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"No, they did not\": Dialogue response dynamics in pre-trained language\n  models", "abstract": "A critical component of competence in language is being able to identify\nrelevant components of an utterance and reply appropriately. In this paper we\nexamine the extent of such dialogue response sensitivity in pre-trained\nlanguage models, conducting a series of experiments with a particular focus on\nsensitivity to dynamics involving phenomena of at-issueness and ellipsis. We\nfind that models show clear sensitivity to a distinctive role of embedded\nclauses, and a general preference for responses that target main clause content\nof prior utterances. However, the results indicate mixed and generally weak\ntrends with respect to capturing the full range of dynamics involved in\ntargeting at-issue versus not-at-issue content. Additionally, models show\nfundamental limitations in grasp of the dynamics governing ellipsis, and\nresponse selections show clear interference from superficial factors that\noutweigh the influence of principled discourse constraints.", "published": "2022-10-05 19:52:33", "link": "http://arxiv.org/abs/2210.02526v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention-based Ingredient Phrase Parser", "abstract": "As virtual personal assistants have now penetrated the consumer market, with\nproducts such as Siri and Alexa, the research community has produced several\nworks on task-oriented dialogue tasks such as hotel booking, restaurant\nbooking, and movie recommendation. Assisting users to cook is one of these\ntasks that are expected to be solved by intelligent assistants, where\ningredients and their corresponding attributes, such as name, unit, and\nquantity, should be provided to users precisely and promptly. However, existing\ningredient information scraped from the cooking website is in the unstructured\nform with huge variation in the lexical structure, for example, '1 garlic\nclove, crushed', and '1 (8 ounce) package cream cheese, softened', making it\ndifficult to extract information exactly. To provide an engaged and successful\nconversational service to users for cooking tasks, we propose a new ingredient\nparsing model that can parse an ingredient phrase of recipes into the structure\nform with its corresponding attributes with over 0.93 F1-score. Experimental\nresults show that our model achieves state-of-the-art performance on AllRecipes\nand Food.com datasets.", "published": "2022-10-05 20:09:35", "link": "http://arxiv.org/abs/2210.02535v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Privacy-Preserving Text Classification on BERT Embeddings with\n  Homomorphic Encryption", "abstract": "Embeddings, which compress information in raw text into semantics-preserving\nlow-dimensional vectors, have been widely adopted for their efficacy. However,\nrecent research has shown that embeddings can potentially leak private\ninformation about sensitive attributes of the text, and in some cases, can be\ninverted to recover the original input text. To address these growing privacy\nchallenges, we propose a privatization mechanism for embeddings based on\nhomomorphic encryption, to prevent potential leakage of any piece of\ninformation in the process of text classification. In particular, our method\nperforms text classification on the encryption of embeddings from\nstate-of-the-art models like BERT, supported by an efficient GPU implementation\nof CKKS encryption scheme. We show that our method offers encrypted protection\nof BERT embeddings, while largely preserving their utility on downstream text\nclassification tasks.", "published": "2022-10-05 21:46:02", "link": "http://arxiv.org/abs/2210.02574v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CCC-wav2vec 2.0: Clustering aided Cross Contrastive Self-supervised\n  learning of speech representations", "abstract": "While Self-Supervised Learning has helped reap the benefit of the scale from\nthe available unlabeled data, the learning paradigms are continuously being\nbettered. We present a new pre-training strategy named ccc-wav2vec 2.0, which\nuses clustering and an augmentation-based cross-contrastive loss as its\nself-supervised objective. Through the clustering module, we scale down the\ninfluence of those negative examples that are highly similar to the positive.\nThe Cross-Contrastive loss is computed between the encoder output of the\noriginal sample and the quantizer output of its augmentation and vice-versa,\nbringing robustness to the pre-training strategy. ccc-wav2vec 2.0 achieves up\nto 15.6% and 12.7% relative WER improvement over the baseline wav2vec 2.0 on\nthe test-clean and test-other sets, respectively, of LibriSpeech, without the\nuse of any language model. The proposed method also achieves up to 14.9%\nrelative WER improvement over the baseline wav2vec 2.0 when fine-tuned on\nSwitchboard data. We make all our codes publicly available on GitHub.", "published": "2022-10-05 22:44:35", "link": "http://arxiv.org/abs/2210.02592v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Every word counts: A multilingual analysis of individual human alignment\n  with model attention", "abstract": "Human fixation patterns have been shown to correlate strongly with\nTransformer-based attention. Those correlation analyses are usually carried out\nwithout taking into account individual differences between participants and are\nmostly done on monolingual datasets making it difficult to generalise findings.\nIn this paper, we analyse eye-tracking data from speakers of 13 different\nlanguages reading both in their native language (L1) and in English as language\nlearners (L2). We find considerable differences between languages but also that\nindividual reading behaviour such as skipping rate, total reading time and\nvocabulary knowledge (LexTALE) influence the alignment between humans and\nmodels to an extent that should be considered in future studies.", "published": "2022-10-05 12:44:35", "link": "http://arxiv.org/abs/2210.04963v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COMPS: Conceptual Minimal Pair Sentences for testing Robust Property\n  Knowledge and its Inheritance in Pre-trained Language Models", "abstract": "A characteristic feature of human semantic cognition is its ability to not\nonly store and retrieve the properties of concepts observed through experience,\nbut to also facilitate the inheritance of properties (can breathe) from\nsuperordinate concepts (animal) to their subordinates (dog) -- i.e. demonstrate\nproperty inheritance. In this paper, we present COMPS, a collection of minimal\npair sentences that jointly tests pre-trained language models (PLMs) on their\nability to attribute properties to concepts and their ability to demonstrate\nproperty inheritance behavior. Analyses of 22 different PLMs on COMPS reveal\nthat they can easily distinguish between concepts on the basis of a property\nwhen they are trivially different, but find it relatively difficult when\nconcepts are related on the basis of nuanced knowledge representations.\nFurthermore, we find that PLMs can demonstrate behavior consistent with\nproperty inheritance to a great extent, but fail in the presence of distracting\ninformation, which decreases the performance of many models, sometimes even\nbelow chance. This lack of robustness in demonstrating simple reasoning raises\nimportant questions about PLMs' capacity to make correct inferences even when\nthey appear to possess the prerequisite knowledge.", "published": "2022-10-05 00:04:18", "link": "http://arxiv.org/abs/2210.01963v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GAPX: Generalized Autoregressive Paraphrase-Identification X", "abstract": "Paraphrase Identification is a fundamental task in Natural Language\nProcessing. While much progress has been made in the field, the performance of\nmany state-of-the-art models often suffer from distribution shift during\ninference time. We verify that a major source of this performance drop comes\nfrom biases introduced by negative examples. To overcome these biases, we\npropose in this paper to train two separate models, one that only utilizes the\npositive pairs and the other the negative pairs. This enables us the option of\ndeciding how much to utilize the negative model, for which we introduce a\nperplexity based out-of-distribution metric that we show can effectively and\nautomatically determine how much weight it should be given during inference. We\nsupport our findings with strong empirical results.", "published": "2022-10-05 01:23:52", "link": "http://arxiv.org/abs/2210.01979v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WavSpA: Wavelet Space Attention for Boosting Transformers' Long Sequence\n  Learning Ability", "abstract": "Transformer and its variants are fundamental neural architectures in deep\nlearning. Recent works show that learning attention in the Fourier space can\nimprove the long sequence learning capability of Transformers. We argue that\nwavelet transform shall be a better choice because it captures both position\nand frequency information with linear time complexity. Therefore, in this\npaper, we systematically study the synergy between wavelet transform and\nTransformers. We propose Wavelet Space Attention (WavSpA) that facilitates\nattention learning in a learnable wavelet coefficient space which replaces the\nattention in Transformers by (1) applying forward wavelet transform to project\nthe input sequences to multi-resolution bases, (2) conducting attention\nlearning in the wavelet coefficient space, and (3) reconstructing the\nrepresentation in input space via backward wavelet transform. Extensive\nexperiments on the Long Range Arena demonstrate that learning attention in the\nwavelet space using either fixed or adaptive wavelets can consistently improve\nTransformer's performance and also significantly outperform learning in Fourier\nspace. We further show our method can enhance Transformer's reasoning\nextrapolation capability over distance on the LEGO chain-of-reasoning task.", "published": "2022-10-05 02:37:59", "link": "http://arxiv.org/abs/2210.01989v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CorefDiffs: Co-referential and Differential Knowledge Flow in Document\n  Grounded Conversations", "abstract": "Knowledge-grounded dialog systems need to incorporate smooth transitions\namong knowledge selected for generating responses, to ensure that dialog flows\nnaturally. For document-grounded dialog systems, the inter- and intra-document\nknowledge relations can be used to model such conversational flows. We develop\na novel Multi-Document Co-Referential Graph (Coref-MDG) to effectively capture\nthe inter-document relationships based on commonsense and similarity and the\nintra-document co-referential structures of knowledge segments within the\ngrounding documents. We propose CorefDiffs, a Co-referential and Differential\nflow management method, to linearize the static Coref-MDG into conversational\nsequence logic. CorefDiffs performs knowledge selection by accounting for\ncontextual graph structures and the knowledge difference sequences. CorefDiffs\nsignificantly outperforms the state-of-the-art by 9.5\\%, 7.4\\%, and 8.2\\% on\nthree public benchmarks. This demonstrates that the effective modeling of\nco-reference and knowledge difference for dialog flows are critical for\ntransitions in document-grounded conversation", "published": "2022-10-05 13:00:17", "link": "http://arxiv.org/abs/2210.02223v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Full-Text Content to Characterize and Identify Best Seller Books", "abstract": "Artistic pieces can be studied from several perspectives, one example being\ntheir reception among readers over time. In the present work, we approach this\ninteresting topic from the standpoint of literary works, particularly assessing\nthe task of predicting whether a book will become a best seller. Dissimilarly\nfrom previous approaches, we focused on the full content of books and\nconsidered visualization and classification tasks. We employed visualization\nfor the preliminary exploration of the data structure and properties, involving\nSemAxis and linear discriminant analyses. Then, to obtain quantitative and more\nobjective results, we employed various classifiers. Such approaches were used\nalong with a dataset containing (i) books published from 1895 to 1924 and\nconsecrated as best sellers by the Publishers Weekly Bestseller Lists and (ii)\nliterary works published in the same period but not being mentioned in that\nlist. Our comparison of methods revealed that the best-achieved result -\ncombining a bag-of-words representation with a logistic regression classifier -\nled to an average accuracy of 0.75 both for the leave-one-out and 10-fold\ncross-validations. Such an outcome suggests that it is unfeasible to predict\nthe success of books with high accuracy using only the full content of the\ntexts. Nevertheless, our findings provide insights into the factors leading to\nthe relative success of a literary work.", "published": "2022-10-05 15:40:25", "link": "http://arxiv.org/abs/2210.02334v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Emotion Twenty Questions Dialog System for Lexical Emotional\n  Intelligence", "abstract": "This paper presents a web-based demonstration of Emotion Twenty Questions\n(EMO20Q), a dialog game whose purpose is to study how people describe emotions.\nEMO20Q can also be used to develop artificially intelligent dialog agents that\ncan play the game. In previous work, an EMO20Q agent used a sequential Bayesian\nmachine learning model and could play the question-asking role. Newer\ntransformer-based neural machine learning models have made it possible to\ndevelop an agent for the question-answering role.\n  This demo paper describes the recent developments in the question-answering\nrole of the EMO20Q game, which requires the agent to respond to more open-ended\ninputs. Furthermore, we also describe the design of the system, including the\nweb-based front-end, agent architecture and programming, and updates to earlier\nsoftware used.\n  The demo system will be available to collect pilot data during the ACII\nconference and this data will be used to inform future experiments and system\ndesign.", "published": "2022-10-05 17:20:26", "link": "http://arxiv.org/abs/2210.02400v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Token Classification for Disambiguating Medical Abbreviations", "abstract": "Abbreviations are unavoidable yet critical parts of the medical text. Using\nabbreviations, especially in clinical patient notes, can save time and space,\nprotect sensitive information, and help avoid repetitions. However, most\nabbreviations might have multiple senses, and the lack of a standardized\nmapping system makes disambiguating abbreviations a difficult and\ntime-consuming task. The main objective of this study is to examine the\nfeasibility of token classification methods for medical abbreviation\ndisambiguation. Specifically, we explore the capability of token classification\nmethods to deal with multiple unique abbreviations in a single text. We use two\npublic datasets to compare and contrast the performance of several transformer\nmodels pre-trained on different scientific and medical corpora. Our proposed\ntoken classification approach outperforms the more commonly used text\nclassification models for the abbreviation disambiguation task. In particular,\nthe SciBERT model shows a strong performance for both token and text\nclassification tasks over the two considered datasets. Furthermore, we find\nthat abbreviation disambiguation performance for the text classification models\nbecomes comparable to that of token classification only when postprocessing is\napplied to their predictions, which involves filtering possible labels for an\nabbreviation based on the training data.", "published": "2022-10-05 18:06:49", "link": "http://arxiv.org/abs/2210.02487v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Honest Students from Untrusted Teachers: Learning an Interpretable\n  Question-Answering Pipeline from a Pretrained Language Model", "abstract": "Explainable question answering systems should produce not only accurate\nanswers but also rationales that justify their reasoning and allow humans to\ncheck their work. But what sorts of rationales are useful and how can we train\nsystems to produce them? We propose a new style of rationale for open-book\nquestion answering, called \\emph{markup-and-mask}, which combines aspects of\nextractive and free-text explanations. In the markup phase, the passage is\naugmented with free-text markup that enables each sentence to stand on its own\noutside the discourse context. In the masking phase, a sub-span of the\nmarked-up passage is selected. To train a system to produce markup-and-mask\nrationales without annotations, we leverage in-context learning. Specifically,\nwe generate silver annotated data by sending a series of prompts to a frozen\npretrained language model, which acts as a teacher. We then fine-tune a smaller\nstudent model by training on the subset of rationales that led to correct\nanswers. The student is \"honest\" in the sense that it is a pipeline: the\nrationale acts as a bottleneck between the passage and the answer, while the\n\"untrusted\" teacher operates under no such constraints. Thus, we offer a new\nway to build trustworthy pipeline systems from a combination of end-task\nannotations and frozen pretrained language models.", "published": "2022-10-05 18:23:49", "link": "http://arxiv.org/abs/2210.02498v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors", "abstract": "Video game testing requires game-specific knowledge as well as common sense\nreasoning about the events in the game. While AI-driven agents can satisfy the\nfirst requirement, it is not yet possible to meet the second requirement\nautomatically. Therefore, video game testing often still relies on manual\ntesting, and human testers are required to play the game thoroughly to detect\nbugs. As a result, it is challenging to fully automate game testing. In this\nstudy, we explore the possibility of leveraging the zero-shot capabilities of\nlarge language models for video game bug detection. By formulating the bug\ndetection problem as a question-answering task, we show that large language\nmodels can identify which event is buggy in a sequence of textual descriptions\nof events from a game. To this end, we introduce the GameBugDescriptions\nbenchmark dataset, which consists of 167 buggy gameplay videos and a total of\n334 question-answer pairs across 8 games. We extensively evaluate the\nperformance of six models across the OPT and InstructGPT large language model\nfamilies on our benchmark dataset. Our results show promising results for\nemploying language models to detect video game bugs. With the proper prompting\ntechnique, we could achieve an accuracy of 70.66%, and on some video games, up\nto 78.94%. Our code, evaluation data and the benchmark can be found on\nhttps://asgaardlab.github.io/LLMxBugs", "published": "2022-10-05 18:44:35", "link": "http://arxiv.org/abs/2210.02506v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "GLM-130B: An Open Bilingual Pre-trained Model", "abstract": "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language\nmodel with 130 billion parameters. It is an attempt to open-source a 100B-scale\nmodel at least as good as GPT-3 (davinci) and unveil how models of such a scale\ncan be successfully pre-trained. Over the course of this effort, we face\nnumerous unexpected technical and engineering challenges, particularly on loss\nspikes and divergence. In this paper, we introduce the training process of\nGLM-130B including its design choices, training strategies for both efficiency\nand stability, and engineering efforts. The resultant GLM-130B model offers\nsignificant outperformance over GPT-3 175B (davinci) on a wide range of popular\nEnglish benchmarks while the performance advantage is not observed in OPT-175B\nand BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN\n3.0 260B -- the largest Chinese language model -- across related benchmarks.\nFinally, we leverage a unique scaling property of GLM-130B to reach INT4\nquantization without post training, with almost no performance loss, making it\nthe first among 100B-scale models and more importantly, allowing its effective\ninference on 4$\\times$RTX 3090 (24G) or 8$\\times$RTX 2080 Ti (11G) GPUs, the\nmost affordable GPUs required for using 100B-scale models. The GLM-130B model\nweights are publicly accessible and its code, training logs, related toolkit,\nand lessons learned are open-sourced at\n\\url{https://github.com/THUDM/GLM-130B/}.", "published": "2022-10-05 17:34:44", "link": "http://arxiv.org/abs/2210.02414v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "JoeyS2T: Minimalistic Speech-to-Text Modeling with JoeyNMT", "abstract": "JoeyS2T is a JoeyNMT extension for speech-to-text tasks such as automatic\nspeech recognition and end-to-end speech translation. It inherits the core\nphilosophy of JoeyNMT, a minimalist NMT toolkit built on PyTorch, seeking\nsimplicity and accessibility. JoeyS2T's workflow is self-contained, starting\nfrom data pre-processing, over model training and prediction to evaluation, and\nis seamlessly integrated into JoeyNMT's compact and simple code base. On top of\nJoeyNMT's state-of-the-art Transformer-based encoder-decoder architecture,\nJoeyS2T provides speech-oriented components such as convolutional layers,\nSpecAugment, CTC-loss, and WER evaluation. Despite its simplicity compared to\nprior implementations, JoeyS2T performs competitively on English speech\nrecognition and English-to-German speech translation benchmarks. The\nimplementation is accompanied by a walk-through tutorial and available on\nhttps://github.com/may-/joeys2t.", "published": "2022-10-05 20:19:58", "link": "http://arxiv.org/abs/2210.02545v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Revisiting Structured Dropout", "abstract": "Large neural networks are often overparameterised and prone to overfitting,\nDropout is a widely used regularization technique to combat overfitting and\nimprove model generalization. However, unstructured Dropout is not always\neffective for specific network architectures and this has led to the formation\nof multiple structured Dropout approaches to improve model performance and,\nsometimes, reduce the computational resources required for inference. In this\nwork, we revisit structured Dropout comparing different Dropout approaches to\nnatural language processing and computer vision tasks for multiple\nstate-of-the-art networks. Additionally, we devise an approach to structured\nDropout we call \\textbf{\\emph{ProbDropBlock}} which drops contiguous blocks\nfrom feature maps with a probability given by the normalized feature salience\nvalues. We find that with a simple scheduling strategy the proposed approach to\nstructured Dropout consistently improved model performance compared to\nbaselines and other Dropout approaches on a diverse range of tasks and models.\nIn particular, we show \\textbf{\\emph{ProbDropBlock}} improves RoBERTa\nfinetuning on MNLI by $0.22\\%$, and training of ResNet50 on ImageNet by\n$0.28\\%$.", "published": "2022-10-05 21:26:57", "link": "http://arxiv.org/abs/2210.02570v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Exploration of A Self-Supervised Speech Model: A Study on Emotional\n  Corpora", "abstract": "Self-supervised speech models have grown fast during the past few years and\nhave proven feasible for use in various downstream tasks. Some recent work has\nstarted to look at the characteristics of these models, yet many concerns have\nnot been fully addressed. In this work, we conduct a study on emotional corpora\nto explore a popular self-supervised model -- wav2vec 2.0. Via a set of\nquantitative analysis, we mainly demonstrate that: 1) wav2vec 2.0 appears to\ndiscard paralinguistic information that is less useful for word recognition\npurposes; 2) for emotion recognition, representations from the middle layer\nalone perform as well as those derived from layer averaging, while the final\nlayer results in the worst performance in some cases; 3) current\nself-supervised models may not be the optimal solution for downstream tasks\nthat make use of non-lexical features. Our work provides novel findings that\nwill aid future research in this area and theoretical basis for the use of\nexisting models.", "published": "2022-10-05 23:01:36", "link": "http://arxiv.org/abs/2210.02595v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Comparing Hysteresis Comparator and RMS Threshold Methods for Automatic\n  Single Cough Segmentations", "abstract": "Research on diagnosing diseases based on voice signals currently are rapidly\nincreasing, including cough-related diseases. When training the cough sound\nsignals into deep learning models, it is necessary to have a standard input by\nsegmenting several cough signals into individual cough signals. Previous\nresearch has been developed to segment cough signals from non-cough signals.\nThis research evaluates the segmentation methods of several cough signals from\na single audio file into several single-cough signals. We evaluate three\ndifferent methods employing manual segmentation as a baseline and automatic\nsegmentation. The results by two automatic segmentation methods obtained\nprecisions of 73% and 70% compared to 49% by manual segmentation. The\nagreements of listening tests to count the number of correct single-cough\nsegmentations show fair and moderate correlations for automatic segmentation\nmethods and are comparable with manual segmentation.", "published": "2022-10-05 07:18:40", "link": "http://arxiv.org/abs/2210.02057v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "TC-SKNet with GridMask for Low-complexity Classification of Acoustic\n  scene", "abstract": "Convolution neural networks (CNNs) have good performance in low-complexity\nclassification tasks such as acoustic scene classifications (ASCs). However,\nthere are few studies on the relationship between the length of target speech\nand the size of the convolution kernels. In this paper, we combine Selective\nKernel Network with Temporal-Convolution (TC-SKNet) to adjust the receptive\nfield of convolution kernels to solve the problem of variable length of target\nvoice while keeping low-complexity. GridMask is a data augmentation strategy by\nmasking part of the raw data or feature area. It can enhance the generalization\nof the model as the role of dropout. In our experiments, the performance gain\nbrought by GridMask is stronger than spectrum augmentation in ASCs. Finally, we\nadopt AutoML to search best structure of TC-SKNet and hyperparameters of\nGridMask for improving the classification performance. As a result, a peak\naccuracy of 59.87% TC-SKNet is equivalent to that of SOTA, but the parameters\nonly use 20.9 K.", "published": "2022-10-05 14:24:17", "link": "http://arxiv.org/abs/2210.02287v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ASVspoof 2021: Towards Spoofed and Deepfake Speech Detection in the Wild", "abstract": "Benchmarking initiatives support the meaningful comparison of competing\nsolutions to prominent problems in speech and language processing. Successive\nbenchmarking evaluations typically reflect a progressive evolution from ideal\nlab conditions towards to those encountered in the wild. ASVspoof, the spoofing\nand deepfake detection initiative and challenge series, has followed the same\ntrend. This article provides a summary of the ASVspoof 2021 challenge and the\nresults of 54 participating teams that submitted to the evaluation phase. For\nthe logical access (LA) task, results indicate that countermeasures are robust\nto newly introduced encoding and transmission effects. Results for the physical\naccess (PA) task indicate the potential to detect replay attacks in real, as\nopposed to simulated physical spaces, but a lack of robustness to variations\nbetween simulated and real acoustic environments. The Deepfake (DF) task, new\nto the 2021 edition, targets solutions to the detection of manipulated,\ncompressed speech data posted online. While detection solutions offer some\nresilience to compression effects, they lack generalization across different\nsource datasets. In addition to a summary of the top-performing systems for\neach task, new analyses of influential data factors and results for hidden data\nsubsets, the article includes a review of post-challenge results, an outline of\nthe principal challenge limitations and a road-map for the future of ASVspoof.", "published": "2022-10-05 17:57:29", "link": "http://arxiv.org/abs/2210.02437v3", "categories": ["cs.SD", "cs.CR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Toward Knowledge-Driven Speech-Based Models of Depression: Leveraging\n  Spectrotemporal Variations in Speech Vowels", "abstract": "Psychomotor retardation associated with depression has been linked with\ntangible differences in vowel production. This paper investigates a\nknowledge-driven machine learning (ML) method that integrates spectrotemporal\ninformation of speech at the vowel-level to identify the depression. Low-level\nspeech descriptors are learned by a convolutional neural network (CNN) that is\ntrained for vowel classification. The temporal evolution of those low-level\ndescriptors is modeled at the high-level within and across utterances via a\nlong short-term memory (LSTM) model that takes the final depression decision. A\nmodified version of the Local Interpretable Model-agnostic Explanations (LIME)\nis further used to identify the impact of the low-level spectrotemporal vowel\nvariation on the decisions and observe the high-level temporal change of the\ndepression likelihood. The proposed method outperforms baselines that model the\nspectrotemporal information in speech without integrating the vowel-based\ninformation, as well as ML models trained with conventional prosodic and\nspectrotemporal features. The conducted explainability analysis indicates that\nspectrotemporal information corresponding to non-vowel segments less important\nthan the vowel-based information. Explainability of the high-level information\ncapturing the segment-by-segment decisions is further inspected for\nparticipants with and without depression. The findings from this work can\nprovide the foundation toward knowledge-driven interpretable decision-support\nsystems that can assist clinicians to better understand fine-grain temporal\nchanges in speech data, ultimately augmenting mental health diagnosis and care.", "published": "2022-10-05 19:57:53", "link": "http://arxiv.org/abs/2210.02527v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
