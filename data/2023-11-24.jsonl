{"title": "Average Token Delay: A Duration-aware Latency Metric for Simultaneous\n  Translation", "abstract": "Simultaneous translation is a task in which the translation begins before the\nend of an input speech segment. Its evaluation should be conducted based on\nlatency in addition to quality, and for users, the smallest possible amount of\nlatency is preferable. Most existing metrics measure latency based on the start\ntimings of partial translations and ignore their duration. This means such\nmetrics do not penalize the latency caused by long translation output, which\ndelays the comprehension of users and subsequent translations. In this work, we\npropose a novel latency evaluation metric for simultaneous translation called\n\\emph{Average Token Delay} (ATD) that focuses on the duration of partial\ntranslations. We demonstrate its effectiveness through analyses simulating\nuser-side latency based on Ear-Voice Span (EVS). In our experiment, ATD had the\nhighest correlation with EVS among baseline latency metrics under most\nconditions.", "published": "2023-11-24 08:53:52", "link": "http://arxiv.org/abs/2311.14353v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\u00daFAL CorPipe at CRAC 2023: Larger Context Improves Multilingual\n  Coreference Resolution", "abstract": "We present CorPipe, the winning entry to the CRAC 2023 Shared Task on\nMultilingual Coreference Resolution. Our system is an improved version of our\nearlier multilingual coreference pipeline, and it surpasses other participants\nby a large margin of 4.5 percent points. CorPipe first performs mention\ndetection, followed by coreference linking via an antecedent-maximization\napproach on the retrieved spans. Both tasks are trained jointly on all\navailable corpora using a shared pretrained language model. Our main\nimprovements comprise inputs larger than 512 subwords and changing the mention\ndecoding to support ensembling. The source code is available at\nhttps://github.com/ufal/crac2023-corpipe.", "published": "2023-11-24 10:15:34", "link": "http://arxiv.org/abs/2311.14391v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DP-NMT: Scalable Differentially-Private Machine Translation", "abstract": "Neural machine translation (NMT) is a widely popular text generation task,\nyet there is a considerable research gap in the development of\nprivacy-preserving NMT models, despite significant data privacy concerns for\nNMT systems. Differentially private stochastic gradient descent (DP-SGD) is a\npopular method for training machine learning models with concrete privacy\nguarantees; however, the implementation specifics of training a model with\nDP-SGD are not always clarified in existing models, with differing software\nlibraries used and code bases not always being public, leading to\nreproducibility issues. To tackle this, we introduce DP-NMT, an open-source\nframework for carrying out research on privacy-preserving NMT with DP-SGD,\nbringing together numerous models, datasets, and evaluation metrics in one\nsystematic software package. Our goal is to provide a platform for researchers\nto advance the development of privacy-preserving NMT systems, keeping the\nspecific details of the DP-SGD algorithm transparent and intuitive to\nimplement. We run a set of experiments on datasets from both general and\nprivacy-related domains to demonstrate our framework in use. We make our\nframework publicly available and welcome feedback from the community.", "published": "2023-11-24 13:19:47", "link": "http://arxiv.org/abs/2311.14465v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlled Text Generation via Language Model Arithmetic", "abstract": "As Large Language Models (LLMs) are deployed more widely, customization with\nrespect to vocabulary, style, and character becomes more important. In this\nwork, we introduce model arithmetic, a novel inference framework for composing\nand biasing LLMs without the need for model (re)training or highly specific\ndatasets. In addition, the framework allows for more precise control of\ngenerated text than direct prompting and prior controlled text generation (CTG)\ntechniques. Using model arithmetic, we can express prior CTG techniques as\nsimple formulas and naturally extend them to new and more effective\nformulations. Further, we show that speculative sampling, a technique for\nefficient LLM sampling, extends to our setting. This enables highly efficient\ntext generation with multiple composed models with only marginal overhead over\na single model. Our empirical evaluation demonstrates that model arithmetic\nallows fine-grained control of generated text while outperforming\nstate-of-the-art on the task of toxicity reduction. We release an open source\neasy-to-use implementation of our framework at\nhttps://github.com/eth-sri/language-model-arithmetic.", "published": "2023-11-24 13:41:12", "link": "http://arxiv.org/abs/2311.14479v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysing the Impact of Removing Infrequent Words on Topic Quality in\n  LDA Models", "abstract": "An initial procedure in text-as-data applications is text preprocessing. One\nof the typical steps, which can substantially facilitate computations, consists\nin removing infrequent words believed to provide limited information about the\ncorpus. Despite popularity of vocabulary pruning, not many guidelines on how to\nimplement it are available in the literature. The aim of the paper is to fill\nthis gap by examining the effects of removing infrequent words for the quality\nof topics estimated using Latent Dirichlet Allocation. The analysis is based on\nMonte Carlo experiments taking into account different criteria for infrequent\nterms removal and various evaluation metrics. The results indicate that pruning\nis beneficial and that the share of vocabulary which might be eliminated can be\nquite considerable.", "published": "2023-11-24 14:20:12", "link": "http://arxiv.org/abs/2311.14505v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Translation for Ge'ez Language", "abstract": "Machine translation (MT) for low-resource languages such as Ge'ez, an ancient\nlanguage that is no longer the native language of any community, faces\nchallenges such as out-of-vocabulary words, domain mismatches, and lack of\nsufficient labeled training data. In this work, we explore various methods to\nimprove Ge'ez MT, including transfer-learning from related languages,\noptimizing shared vocabulary and token segmentation approaches, finetuning\nlarge pre-trained models, and using large language models (LLMs) for few-shot\ntranslation with fuzzy matches. We develop a multilingual neural machine\ntranslation (MNMT) model based on languages relatedness, which brings an\naverage performance improvement of about 4 BLEU compared to standard bilingual\nmodels. We also attempt to finetune the NLLB-200 model, one of the most\nadvanced translation models available today, but find that it performs poorly\nwith only 4k training samples for Ge'ez. Furthermore, we experiment with using\nGPT-3.5, a state-of-the-art LLM, for few-shot translation with fuzzy matches,\nwhich leverages embedding similarity-based retrieval to find context examples\nfrom a parallel corpus. We observe that GPT-3.5 achieves a remarkable BLEU\nscore of 9.2 with no initial knowledge of Ge'ez, but still lower than the MNMT\nbaseline of 15.2. Our work provides insights into the potential and limitations\nof different approaches for low-resource and ancient language MT.", "published": "2023-11-24 14:55:23", "link": "http://arxiv.org/abs/2311.14530v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-to-Text Bilingual Generation", "abstract": "This document illustrates the use of pyrealb for generating two parallel\ntexts (English and French) from a single source of data. The data selection and\ntext organisation processes are shared between the two languages. only language\ndependent word and phrasing choices are distinct processes. The realized texts\nthus convey identical information in both languages without the risk of being\nlost in translation. This is especially important in cases where strict and\nsimultaneous bilingualism is required. We first present the types of\napplications targeted by this approach and how the pyrealb English and French\nrealizer can be used for achieving this goal in a natural way. We describe an\nobject-oriented organization to ensure a convenient realization in both\nlanguages. To illustrate the process, different types of applications are then\nbriefly sketched with links to the source code. A brief comparison of the text\ngeneration is given with the output of an instance of a GPT.", "published": "2023-11-24 19:05:57", "link": "http://arxiv.org/abs/2311.14808v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "OpusCleaner and OpusTrainer, open source toolkits for training Machine\n  Translation and Large language models", "abstract": "Developing high quality machine translation systems is a labour intensive,\nchallenging and confusing process for newcomers to the field. We present a pair\nof tools OpusCleaner and OpusTrainer that aim to simplify the process, reduce\nthe amount of work and lower the entry barrier for newcomers.\n  OpusCleaner is a data downloading, cleaning, and proprocessing toolkit. It is\ndesigned to allow researchers to quickly download, visualise and preprocess\nbilingual (or monolingual) data that comes from many different sources, each of\nthem with different quality, issues, and unique filtering/preprocessing\nrequirements.\n  OpusTrainer is a data scheduling and data augmenting tool aimed at building\nlarge scale, robust machine translation systems and large language models. It\nfeatures deterministic data mixing from many different sources, on-the-fly data\naugmentation and more.\n  Using these tools, we showcase how we can use it to create high quality\nmachine translation model robust to noisy user input; multilingual models and\nterminology aware models.", "published": "2023-11-24 20:24:00", "link": "http://arxiv.org/abs/2311.14838v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tracing Influence at Scale: A Contrastive Learning Approach to Linking\n  Public Comments and Regulator Responses", "abstract": "U.S. Federal Regulators receive over one million comment letters each year\nfrom businesses, interest groups, and members of the public, all advocating for\nchanges to proposed regulations. These comments are believed to have\nwide-ranging impacts on public policy. However, measuring the impact of\nspecific comments is challenging because regulators are required to respond to\ncomments but they do not have to specify which comments they are addressing. In\nthis paper, we propose a simple yet effective solution to this problem by using\nan iterative contrastive method to train a neural model aiming for matching\ntext from public comments to responses written by regulators. We demonstrate\nthat our proposal substantially outperforms a set of selected text-matching\nbaselines on a human-annotated test set. Furthermore, it delivers performance\ncomparable to the most advanced gigantic language model (i.e., GPT-4), and is\nmore cost-effective when handling comments and regulator responses matching in\nlarger scale.", "published": "2023-11-24 23:32:13", "link": "http://arxiv.org/abs/2311.14871v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CMed-GPT: Prompt Tuning for Entity-Aware Chinese Medical Dialogue\n  Generation", "abstract": "Medical dialogue generation relies on natural language generation techniques\nto enable online medical consultations. Recently, the widespread adoption of\nlarge-scale models in the field of natural language processing has facilitated\nrapid advancements in this technology. Existing medical dialogue models are\nmostly based on BERT and pre-trained on English corpora, but there is a lack of\nhigh-performing models on the task of Chinese medical dialogue generation. To\nsolve the above problem, this paper proposes CMed-GPT, which is the GPT\npre-training language model based on Chinese medical domain text. The model is\navailable in two versions, namely, base and large, with corresponding\nperplexity values of 8.64 and 8.01. Additionally, we incorporate lexical and\nentity embeddings into the dialogue text in a uniform manner to meet the\nrequirements of downstream dialogue generation tasks. By applying both\nfine-tuning and p-tuning to CMed-GPT, we lowered the PPL from 8.44 to 7.35.\nThis study not only confirms the exceptional performance of the CMed-GPT model\nin generating Chinese biomedical text but also highlights the advantages of\np-tuning over traditional fine-tuning with prefix prompts. Furthermore, we\nvalidate the significance of incorporating external information in medical\ndialogue generation, which enhances the quality of dialogue generation.", "published": "2023-11-24 15:10:56", "link": "http://arxiv.org/abs/2311.14539v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data-Efficient Alignment of Large Language Models with Human Feedback\n  Through Natural Language", "abstract": "Learning from human feedback is a prominent technique to align the output of\nlarge language models (LLMs) with human expectations. Reinforcement learning\nfrom human feedback (RLHF) leverages human preference signals that are in the\nform of ranking of response pairs to perform this alignment. However, human\npreference on LLM outputs can come in much richer forms including natural\nlanguage, which may provide detailed feedback on strengths and weaknesses of a\ngiven response. In this work we investigate data efficiency of modeling human\nfeedback that is in natural language. Specifically, we fine-tune an open-source\nLLM, e.g., Falcon-40B-Instruct, on a relatively small amount (1000 records or\neven less) of human feedback in natural language in the form of critiques and\nrevisions of responses. We show that this model is able to improve the quality\nof responses from even some of the strongest LLMs such as ChatGPT, BARD, and\nVicuna, through critique and revision of those responses. For instance, through\none iteration of revision of ChatGPT responses, the revised responses have\n56.6% win rate over the original ones, and this win rate can be further\nimproved to 65.9% after applying the revision for five iterations.", "published": "2023-11-24 15:20:36", "link": "http://arxiv.org/abs/2311.14543v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Calibrated Language Models Must Hallucinate", "abstract": "Recent language models generate false but plausible-sounding text with\nsurprising frequency. Such \"hallucinations\" are an obstacle to the usability of\nlanguage-based AI systems and can harm people who rely upon their outputs. This\nwork shows that there is an inherent statistical lower-bound on the rate that\npretrained language models hallucinate certain types of facts, having nothing\nto do with the transformer LM architecture or data quality. For \"arbitrary\"\nfacts whose veracity cannot be determined from the training data, we show that\nhallucinations must occur at a certain rate for language models that satisfy a\nstatistical calibration condition appropriate for generative language models.\nSpecifically, if the maximum probability of any fact is bounded, we show that\nthe probability of generating a hallucination is close to the fraction of facts\nthat occur exactly once in the training data (a \"Good-Turing\" estimate), even\nassuming ideal training data without errors.\n  One conclusion is that models pretrained to be sufficiently good predictors\n(i.e., calibrated) may require post-training to mitigate hallucinations on the\ntype of arbitrary facts that tend to appear once in the training set. However,\nour analysis also suggests that there is no statistical reason that pretraining\nwill lead to hallucination on facts that tend to appear more than once in the\ntraining data (like references to publications such as articles and books,\nwhose hallucinations have been particularly notable and problematic) or on\nsystematic facts (like arithmetic calculations). Therefore, different\narchitectures and learning algorithms may mitigate these latter types of\nhallucinations.", "published": "2023-11-24 18:29:50", "link": "http://arxiv.org/abs/2311.14648v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Cross-Domain Hate Speech Generalizability with Emotion\n  Knowledge", "abstract": "Reliable automatic hate speech (HS) detection systems must adapt to the\nin-flow of diverse new data to curtail hate speech. However, hate speech\ndetection systems commonly lack generalizability in identifying hate speech\ndissimilar to data used in training, impeding their robustness in real-world\ndeployments. In this work, we propose a hate speech generalization framework\nthat leverages emotion knowledge in a multitask architecture to improve the\ngeneralizability of hate speech detection in a cross-domain setting. We\ninvestigate emotion corpora with varying emotion categorical scopes to\ndetermine the best corpus scope for supplying emotion knowledge to foster\ngeneralized hate speech detection. We further assess the relationship between\nusing pretrained Transformers models adapted for hate speech and its effect on\nour emotion-enriched hate speech generalization model. We perform extensive\nexperiments on six publicly available datasets sourced from different online\ndomains and show that our emotion-enriched HS detection generalization method\ndemonstrates consistent generalization improvement in cross-domain evaluation,\nincreasing generalization performance up to 18.1% and average cross-domain\nperformance up to 8.5%, according to the F1 measure.", "published": "2023-11-24 23:00:36", "link": "http://arxiv.org/abs/2311.14865v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic detection of problem-gambling signs from online texts using\n  large language models", "abstract": "Problem gambling is a major public health concern and is associated with\nprofound psychological distress and economic problems. There are numerous\ngambling communities on the internet where users exchange information about\ngames, gambling tactics, as well as gambling-related problems. Individuals\nexhibiting higher levels of problem gambling engage more in such communities.\nOnline gambling communities may provide insights into problem-gambling\nbehaviour. Using data scraped from a major German gambling discussion board, we\nfine-tuned a large language model, specifically a Bidirectional Encoder\nRepresentations from Transformers (BERT) model, to predict signs of\nproblem-gambling from forum posts. Training data were generated by manual\nannotation and by taking into account diagnostic criteria and gambling-related\ncognitive distortions. Using k-fold cross-validation, our models achieved a\nprecision of 0.95 and F1 score of 0.71, demonstrating that satisfactory\nclassification performance can be achieved by generating high-quality training\nmaterial through manual annotation based on diagnostic criteria. The current\nstudy confirms that a BERT-based model can be reliably used on small data sets\nand to detect signatures of problem gambling in online communication data. Such\ncomputational approaches may have potential for the detection of changes in\nproblem-gambling prevalence among online users.", "published": "2023-11-24 13:48:02", "link": "http://arxiv.org/abs/2312.00804v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gender inference: can chatGPT outperform common commercial tools?", "abstract": "An increasing number of studies use gender information to understand\nphenomena such as gender bias, inequity in access and participation, or the\nimpact of the Covid pandemic response. Unfortunately, most datasets do not\ninclude self-reported gender information, making it necessary for researchers\nto infer gender from other information, such as names or names and country\ninformation. An important limitation of these tools is that they fail to\nappropriately capture the fact that gender exists on a non-binary scale,\nhowever, it remains important to evaluate and compare how well these tools\nperform in a variety of contexts. In this paper, we compare the performance of\na generative Artificial Intelligence (AI) tool ChatGPT with three commercially\navailable list-based and machine learning-based gender inference tools (Namsor,\nGender-API, and genderize.io) on a unique dataset. Specifically, we use a large\nOlympic athlete dataset and report how variations in the input (e.g., first\nname and first and last name, with and without country information) impact the\naccuracy of their predictions. We report results for the full set, as well as\nfor the subsets: medal versus non-medal winners, athletes from the largest\nEnglish-speaking countries, and athletes from East Asia. On these sets, we find\nthat Namsor is the best traditional commercially available tool. However,\nChatGPT performs at least as well as Namsor and often outperforms it,\nespecially for the female sample when country and/or last name information is\navailable. All tools perform better on medalists versus non-medalists and on\nnames from English-speaking countries. Although not designed for this purpose,\nChatGPT may be a cost-effective tool for gender prediction. In the future, it\nmight even be possible for ChatGPT or other large scale language models to\nbetter identify self-reported gender rather than report gender on a binary\nscale.", "published": "2023-11-24 22:09:14", "link": "http://arxiv.org/abs/2312.00805v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models as Topological Structure Enhancers for\n  Text-Attributed Graphs", "abstract": "The latest advancements in large language models (LLMs) have revolutionized\nthe field of natural language processing (NLP). Inspired by the success of LLMs\nin NLP tasks, some recent work has begun investigating the potential of\napplying LLMs in graph learning tasks. However, most of the existing work\nfocuses on utilizing LLMs as powerful node feature augmenters, leaving\nemploying LLMs to enhance graph topological structures an understudied problem.\nIn this work, we explore how to leverage the information retrieval and text\ngeneration capabilities of LLMs to refine/enhance the topological structure of\ntext-attributed graphs (TAGs) under the node classification setting. First, we\npropose using LLMs to help remove unreliable edges and add reliable ones in the\nTAG. Specifically, we first let the LLM output the semantic similarity between\nnode attributes through delicate prompt designs, and then perform edge deletion\nand edge addition based on the similarity. Second, we propose using\npseudo-labels generated by the LLM to improve graph topology, that is, we\nintroduce the pseudo-label propagation as a regularization to guide the graph\nneural network (GNN) in learning proper edge weights. Finally, we incorporate\nthe two aforementioned LLM-based methods for graph topological refinement into\nthe process of GNN training, and perform extensive experiments on four\nreal-world datasets. The experimental results demonstrate the effectiveness of\nLLM-based graph topology refinement (achieving a 0.15%--2.47% performance gain\non public benchmarks).", "published": "2023-11-24 07:53:48", "link": "http://arxiv.org/abs/2311.14324v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Universal Jailbreak Backdoors from Poisoned Human Feedback", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is used to align large\nlanguage models to produce helpful and harmless responses. Yet, prior work\nshowed these models can be jailbroken by finding adversarial prompts that\nrevert the model to its unaligned behavior. In this paper, we consider a new\nthreat where an attacker poisons the RLHF training data to embed a \"jailbreak\nbackdoor\" into the model. The backdoor embeds a trigger word into the model\nthat acts like a universal \"sudo command\": adding the trigger word to any\nprompt enables harmful responses without the need to search for an adversarial\nprompt. Universal jailbreak backdoors are much more powerful than previously\nstudied backdoors on language models, and we find they are significantly harder\nto plant using common backdoor attack techniques. We investigate the design\ndecisions in RLHF that contribute to its purported robustness, and release a\nbenchmark of poisoned models to stimulate future research on universal\njailbreak backdoors.", "published": "2023-11-24 13:09:34", "link": "http://arxiv.org/abs/2311.14455v4", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.AI"}
{"title": "SER_AMPEL: a multi-source dataset for speech emotion recognition of\n  Italian older adults", "abstract": "In this paper, SER_AMPEL, a multi-source dataset for speech emotion\nrecognition (SER) is presented. The peculiarity of the dataset is that it is\ncollected with the aim of providing a reference for speech emotion recognition\nin case of Italian older adults. The dataset is collected following different\nprotocols, in particular considering acted conversations, extracted from movies\nand TV series, and recording natural conversations where the emotions are\nelicited by proper questions. The evidence of the need for such a dataset\nemerges from the analysis of the state of the art. Preliminary considerations\non the critical issues of SER are reported analyzing the classification results\non a subset of the proposed dataset.", "published": "2023-11-24 13:47:25", "link": "http://arxiv.org/abs/2311.14483v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "StableSSM: Alleviating the Curse of Memory in State-space Models through\n  Stable Reparameterization", "abstract": "In this paper, we investigate the long-term memory learning capabilities of\nstate-space models (SSMs) from the perspective of parameterization. We prove\nthat state-space models without any reparameterization exhibit a memory\nlimitation similar to that of traditional RNNs: the target relationships that\ncan be stably approximated by state-space models must have an exponential\ndecaying memory. Our analysis identifies this \"curse of memory\" as a result of\nthe recurrent weights converging to a stability boundary, suggesting that a\nreparameterization technique can be effective. To this end, we introduce a\nclass of reparameterization techniques for SSMs that effectively lift its\nmemory limitations. Besides improving approximation capabilities, we further\nillustrate that a principled choice of reparameterization scheme can also\nenhance optimization stability. We validate our findings using synthetic\ndatasets, language models and image classifications.", "published": "2023-11-24 14:08:31", "link": "http://arxiv.org/abs/2311.14495v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.DS"], "primary_category": "cs.LG"}
{"title": "tinyCLAP: Distilling Constrastive Language-Audio Pretrained Models", "abstract": "Contrastive Language-Audio Pretraining (CLAP) became of crucial importance in\nthe field of audio and speech processing. Its employment ranges from sound\nevent detection to text-to-audio generation. However, one of the main\nlimitations is the considerable amount of data required in the training process\nand the overall computational complexity during inference. This paper\ninvestigates how we can reduce the complexity of contrastive language-audio\npre-trained models, yielding an efficient model that we call tinyCLAP. We\nderive an unimodal distillation loss from first principles and explore how the\ndimensionality of the shared, multimodal latent space can be reduced via\npruning. TinyCLAP uses only 6% of the original Microsoft CLAP parameters with a\nminimal reduction (less than 5%) in zero-shot classification performance across\nthe three sound event detection datasets on which it was tested", "published": "2023-11-24 14:45:53", "link": "http://arxiv.org/abs/2311.14517v3", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GPT Struct Me: Probing GPT Models on Narrative Entity Extraction", "abstract": "The importance of systems that can extract structured information from\ntextual data becomes increasingly pronounced given the ever-increasing volume\nof text produced on a daily basis. Having a system that can effectively extract\nsuch information in an interoperable manner would be an asset for several\ndomains, be it finance, health, or legal. Recent developments in natural\nlanguage processing led to the production of powerful language models that can,\nto some degree, mimic human intelligence. Such effectiveness raises a pertinent\nquestion: Can these models be leveraged for the extraction of structured\ninformation? In this work, we address this question by evaluating the\ncapabilities of two state-of-the-art language models -- GPT-3 and GPT-3.5,\ncommonly known as ChatGPT -- in the extraction of narrative entities, namely\nevents, participants, and temporal expressions. This study is conducted on the\nText2Story Lusa dataset, a collection of 119 Portuguese news articles whose\nannotation framework includes a set of entity structures along with several\ntags and attribute values. We first select the best prompt template through an\nablation study over prompt components that provide varying degrees of\ninformation on a subset of documents of the dataset. Subsequently, we use the\nbest templates to evaluate the effectiveness of the models on the remaining\ndocuments. The results obtained indicate that GPT models are competitive with\nout-of-the-box baseline systems, presenting an all-in-one alternative for\npractitioners with limited resources. By studying the strengths and limitations\nof these models in the context of information extraction, we offer insights\nthat can guide future improvements and avenues to explore in this field.", "published": "2023-11-24 16:19:04", "link": "http://arxiv.org/abs/2311.14583v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "One Pass Streaming Algorithm for Super Long Token Attention\n  Approximation in Sublinear Space", "abstract": "Attention computation takes both the time complexity of $O(n^2)$ and the\nspace complexity of $O(n^2)$ simultaneously, which makes deploying Large\nLanguage Models (LLMs) in streaming applications that involve long contexts\nrequiring substantial computational resources. In recent OpenAI DevDay (Nov 6,\n2023), OpenAI released a new model that is able to support a 128K-long\ndocument, in our paper, we focus on the memory-efficient issue when context\nlength $n$ is much greater than 128K ($n \\gg 2^d$). Considering a single-layer\nself-attention with Query, Key, and Value matrices $Q, K, V \\in \\mathbb{R}^{n\n\\times d}$, the polynomial method approximates the attention output $T \\in\n\\mathbb{R}^{n \\times d}$. It accomplishes this by constructing $U_1, U_2 \\in\n\\mathbb{R}^{n \\times t}$ to expedite attention ${\\sf Attn}(Q, K, V)$\ncomputation within $n^{1+o(1)}$ time executions. Despite this, computing the\napproximated attention matrix $U_1U_2^\\top \\in \\mathbb{R}^{n \\times n}$ still\nnecessitates $O(n^2)$ space, leading to significant memory usage. In response\nto these challenges, we introduce a new algorithm that only reads one pass of\nthe data in a streaming fashion. This method employs sublinear space $o(n)$ to\nstore three sketch matrices, alleviating the need for exact $K, V$ storage.\nNotably, our algorithm exhibits exceptional memory-efficient performance with\nsuper-long tokens. As the token length $n$ increases, our error guarantee\ndiminishes while the memory usage remains nearly constant. This unique\nattribute underscores the potential of our technique in efficiently handling\nLLMs in streaming applications.", "published": "2023-11-24 18:35:00", "link": "http://arxiv.org/abs/2311.14652v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Evaluating Large Language Models through Gender and Racial Stereotypes", "abstract": "Language Models have ushered a new age of AI gaining traction within the NLP\ncommunity as well as amongst the general population. AI's ability to make\npredictions, generations and its applications in sensitive decision-making\nscenarios, makes it even more important to study these models for possible\nbiases that may exist and that can be exaggerated. We conduct a quality\ncomparative study and establish a framework to evaluate language models under\nthe premise of two kinds of biases: gender and race, in a professional setting.\nWe find out that while gender bias has reduced immensely in newer models, as\ncompared to older ones, racial bias still exists.", "published": "2023-11-24 18:41:16", "link": "http://arxiv.org/abs/2311.14788v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Weak Alignment Supervision from Hybrid Model Improves End-to-end ASR", "abstract": "In this paper, we aim to create weak alignment supervision from an existing\nhybrid system to aid the end-to-end modeling of automatic speech recognition.\nTowards this end, we use the existing hybrid ASR system to produce triphone\nalignments of the training audios. We then create a cross-entropy loss at a\ncertain layer of the encoder using the derived alignments. In contrast to the\ngeneral one-hot cross-entropy losses, here we use a cross-entropy loss with a\nlabel smoothing parameter to regularize the supervision. As a comparison, we\nalso conduct the experiments with one-hot cross-entropy losses and CTC losses\nwith loss weighting. The results show that placing the weak alignment\nsupervision with the label smoothing parameter of 0.5 at the third encoder\nlayer outperforms the other two approaches and leads to about 5\\% relative WER\nreduction on the TED-LIUM 2 dataset over the baseline. We see similar\nimprovements when applying the method out-of-the-box on a Tagalog end-to-end\nASR system.", "published": "2023-11-24 20:14:28", "link": "http://arxiv.org/abs/2311.14835v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Custom Data Augmentation for low resource ASR using Bark and\n  Retrieval-Based Voice Conversion", "abstract": "This paper proposes two innovative methodologies to construct customized\nCommon Voice datasets for low-resource languages like Hindi. The first\nmethodology leverages Bark, a transformer-based text-to-audio model developed\nby Suno, and incorporates Meta's enCodec and a pre-trained HuBert model to\nenhance Bark's performance. The second methodology employs Retrieval-Based\nVoice Conversion (RVC) and uses the Ozen toolkit for data preparation. Both\nmethodologies contribute to the advancement of ASR technology and offer\nvaluable insights into addressing the challenges of constructing customized\nCommon Voice datasets for under-resourced languages. Furthermore, they provide\na pathway to achieving high-quality, personalized voice generation for a range\nof applications.", "published": "2023-11-24 20:16:29", "link": "http://arxiv.org/abs/2311.14836v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Arousal-Valence Representation from Categorical Emotion Labels\n  of Speech", "abstract": "Dimensional representations of speech emotions such as the arousal-valence\n(AV) representation provide a continuous and fine-grained description and\ncontrol than their categorical counterparts. They have wide applications in\ntasks such as dynamic emotion understanding and expressive text-to-speech\nsynthesis. Existing methods that predict the dimensional emotion representation\nfrom speech cast it as a supervised regression task. These methods face data\nscarcity issues, as dimensional annotations are much harder to acquire than\ncategorical labels. In this work, we propose to learn the AV representation\nfrom categorical emotion labels of speech. We start by learning a rich and\nemotion-relevant high-dimensional speech feature representation using\nself-supervised pre-training and emotion classification fine-tuning. This\nrepresentation is then mapped to the 2D AV space according to psychological\nfindings through anchored dimensionality reduction. Experiments show that our\nmethod achieves a Concordance Correlation Coefficient (CCC) performance\ncomparable to state-of-the-art supervised regression methods on IEMOCAP without\nleveraging ground-truth AV annotations during training. This validates our\nproposed approach on AV prediction. Furthermore, visualization of AV\npredictions on MEAD and EmoDB datasets shows the interpretability of the\nlearned AV representations.", "published": "2023-11-24 19:23:37", "link": "http://arxiv.org/abs/2311.14816v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Overview Of The 2023 Icassp Sp Clarity Challenge: Speech Enhancement For\n  Hearing Aids", "abstract": "This paper reports on the design and outcomes of the ICASSP SP Clarity\nChallenge: Speech Enhancement for Hearing Aids. The scenario was a listener\nattending to a target speaker in a noisy, domestic environment. There were\nmultiple interferers and head rotation by the listener. The challenge extended\nthe second Clarity Enhancement Challenge (CEC2) by fixing the amplification\nstage of the hearing aid; evaluating with a combined metric for speech\nintelligibility and quality; and providing two evaluation sets, one based on\nsimulation and the other on real-room measurements. Five teams improved on the\nbaseline system for the simulated evaluation set, but the performance on the\nmeasured evaluation set was much poorer. Investigations are on-going to\ndetermine the exact cause of the mismatch between the simulated and measured\ndata sets. The presence of transducer noise in the measurements, lower order\nAmbisonics harming the ability for systems to exploit binaural cues and the\ndifferences between real and simulated room impulse responses are suggested\ncauses", "published": "2023-11-24 14:02:55", "link": "http://arxiv.org/abs/2311.14490v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Allpass impulse response modelling", "abstract": "This document defines a method for FIR system modelling which is very trivial\nas it only depends on phase introduction and removal (allpass filters). As\nmagnitude is not altered, the processing is numerically stable. It is limited\nto phase alteration which maintains the time domain magnitude to force a system\nwithin its linear limits.", "published": "2023-11-24 00:48:11", "link": "http://arxiv.org/abs/2311.14239v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Cooperative Dual Attention for Audio-Visual Speech Enhancement with\n  Facial Cues", "abstract": "In this work, we focus on leveraging facial cues beyond the lip region for\nrobust Audio-Visual Speech Enhancement (AVSE). The facial region, encompassing\nthe lip region, reflects additional speech-related attributes such as gender,\nskin color, nationality, etc., which contribute to the effectiveness of AVSE.\nHowever, static and dynamic speech-unrelated attributes also exist, causing\nappearance changes during speech. To address these challenges, we propose a\nDual Attention Cooperative Framework, DualAVSE, to ignore speech-unrelated\ninformation, capture speech-related information with facial cues, and\ndynamically integrate it with the audio signal for AVSE. Specifically, we\nintroduce a spatial attention-based visual encoder to capture and enhance\nvisual speech information beyond the lip region, incorporating global facial\ncontext and automatically ignoring speech-unrelated information for robust\nvisual feature extraction. Additionally, a dynamic visual feature fusion\nstrategy is introduced by integrating a temporal-dimensional self-attention\nmodule, enabling the model to robustly handle facial variations. The acoustic\nnoise in the speaking process is variable, impacting audio quality. Therefore,\na dynamic fusion strategy for both audio and visual features is introduced to\naddress this issue. By integrating cooperative dual attention in the visual\nencoder and audio-visual fusion strategy, our model effectively extracts\nbeneficial speech information from both audio and visual cues for AVSE.\nThorough analysis and comparison on different datasets, including normal and\nchallenging cases with unreliable or absent visual information, consistently\nshow our model outperforming existing methods across multiple metrics.", "published": "2023-11-24 04:30:31", "link": "http://arxiv.org/abs/2311.14275v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
