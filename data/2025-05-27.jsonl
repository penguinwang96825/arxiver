{"title": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective", "abstract": "Multilingual Alignment is an effective and representative paradigm to enhance\nLLMs' multilingual capabilities, which transfers the capabilities from the\nhigh-resource languages to the low-resource languages. Meanwhile, some\nresearches on language-specific neurons reveal that there are language-specific\nneurons that are selectively activated in LLMs when processing different\nlanguages. This provides a new perspective to analyze and understand LLMs'\nmechanisms more specifically in multilingual scenarios. In this work, we\npropose a new finer-grained neuron identification algorithm, which detects\nlanguage neurons~(including language-specific neurons and language-related\nneurons) and language-agnostic neurons. Furthermore, based on the\ndistributional characteristics of different types of neurons, we divide the\nLLMs' internal process for multilingual inference into four parts: (1)\nmultilingual understanding, (2) shared semantic space reasoning, (3)\nmultilingual output space transformation, and (4) vocabulary space outputting.\nAdditionally, we systematically analyze the models before and after alignment\nwith a focus on different types of neurons. We also analyze the phenomenon of\n''Spontaneous Multilingual Alignment''. Overall, our work conducts a\ncomprehensive investigation based on different types of neurons, providing\nempirical results and valuable insights for better understanding multilingual\nalignment and multilingual capabilities of LLMs.", "published": "2025-05-27 17:59:52", "link": "http://arxiv.org/abs/2505.21505v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making", "abstract": "Large language models (LLMs) have demonstrated strong potential in clinical\nquestion answering, with recent multi-agent frameworks further improving\ndiagnostic accuracy via collaborative reasoning. However, we identify a\nrecurring issue of Silent Agreement, where agents prematurely converge on\ndiagnoses without sufficient critical analysis, particularly in complex or\nambiguous cases. We present a new concept called Catfish Agent, a\nrole-specialized LLM designed to inject structured dissent and counter silent\nagreement. Inspired by the ``catfish effect'' in organizational psychology, the\nCatfish Agent is designed to challenge emerging consensus to stimulate deeper\nreasoning. We formulate two mechanisms to encourage effective and context-aware\ninterventions: (i) a complexity-aware intervention that modulates agent\nengagement based on case difficulty, and (ii) a tone-calibrated intervention\narticulated to balance critique and collaboration. Evaluations on nine medical\nQ&A and three medical VQA benchmarks show that our approach consistently\noutperforms both single- and multi-agent LLMs frameworks, including leading\ncommercial models such as GPT-4o and DeepSeek-R1.", "published": "2025-05-27 17:59:50", "link": "http://arxiv.org/abs/2505.21503v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.OT"], "primary_category": "cs.CL"}
{"title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "abstract": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "published": "2025-05-27 17:59:26", "link": "http://arxiv.org/abs/2505.21500v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers", "abstract": "Academic poster generation is a crucial yet challenging task in scientific\ncommunication, requiring the compression of long-context interleaved documents\ninto a single, visually coherent page. To address this challenge, we introduce\nthe first benchmark and metric suite for poster generation, which pairs recent\nconference papers with author-designed posters and evaluates outputs on\n(i)Visual Quality-semantic alignment with human posters, (ii)Textual\nCoherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic\nand informational criteria scored by a VLM-as-judge, and notably\n(iv)PaperQuiz-the poster's ability to convey core paper content as measured by\nVLMs answering generated quizzes. Building on this benchmark, we propose\nPosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser\ndistills the paper into a structured asset library; the (b)Planner aligns\ntext-visual pairs into a binary-tree layout that preserves reading order and\nspatial balance; and the (c)Painter-Commenter loop refines each panel by\nexecuting rendering code and using VLM feedback to eliminate overflow and\nensure alignment. In our comprehensive evaluation, we find that GPT-4o\noutputs-though visually appealing at first glance-often exhibit noisy text and\npoor PaperQuiz scores, and we find that reader engagement is the primary\naesthetic bottleneck, as human-designed posters rely largely on visual\nsemantics to convey meaning. Our fully open-source variants (e.g. based on the\nQwen-2.5 series) outperform existing 4o-driven multi-agent systems across\nnearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper\ninto a finalized yet editable .pptx poster - all for just $0.005. These\nfindings chart clear directions for the next generation of fully automated\nposter-generation models. The code and datasets are available at\nhttps://github.com/Paper2Poster/Paper2Poster.", "published": "2025-05-27 17:58:49", "link": "http://arxiv.org/abs/2505.21497v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.CV"}
{"title": "UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents", "abstract": "In this paper, we introduce UI-Genie, a self-improving framework addressing\ntwo key challenges in GUI agents: verification of trajectory outcome is\nchallenging and high-quality training data are not scalable. These challenges\nare addressed by a reward model and a self-improving pipeline, respectively.\nThe reward model, UI-Genie-RM, features an image-text interleaved architecture\nthat efficiently pro- cesses historical context and unifies action-level and\ntask-level rewards. To sup- port the training of UI-Genie-RM, we develop\ndeliberately-designed data genera- tion strategies including rule-based\nverification, controlled trajectory corruption, and hard negative mining. To\naddress the second challenge, a self-improvement pipeline progressively expands\nsolvable complex GUI tasks by enhancing both the agent and reward models\nthrough reward-guided exploration and outcome verification in dynamic\nenvironments. For training the model, we generate UI- Genie-RM-517k and\nUI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI\nagents while demonstrating high-quality synthetic trajectory gen- eration\nwithout manual annotation. Experimental results show that UI-Genie achieves\nstate-of-the-art performance across multiple GUI agent benchmarks with three\ngenerations of data-model self-improvement. We open-source our complete\nframework implementation and generated datasets to facilitate further research\nin https://github.com/Euphoria16/UI-Genie.", "published": "2025-05-27 17:58:06", "link": "http://arxiv.org/abs/2505.21496v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reinforcing General Reasoning without Verifiers", "abstract": "The recent paradigm shift towards training large language models (LLMs) using\nDeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has\nled to impressive advancements in code and mathematical reasoning. However,\nthis methodology is limited to tasks where rule-based answer verification is\npossible and does not naturally extend to real-world domains such as chemistry,\nhealthcare, engineering, law, biology, business, and economics. Current\npractical workarounds use an additional LLM as a model-based verifier; however,\nthis introduces issues such as reliance on a strong verifier LLM,\nsusceptibility to reward hacking, and the practical burden of maintaining the\nverifier model in memory during training. To address this and extend\nDeepSeek-R1-Zero-style training to general reasoning domains, we propose a\nverifier-free method (VeriFree) that bypasses answer verification and instead\nuses RL to directly maximize the probability of generating the reference\nanswer. We compare VeriFree with verifier-based methods and demonstrate that,\nin addition to its significant practical benefits and reduced compute\nrequirements, VeriFree matches and even surpasses verifier-based methods on\nextensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related\nbenchmarks. Moreover, we provide insights into this method from multiple\nperspectives: as an elegant integration of training both the policy and\nimplicit verifier in a unified model, and as a variational optimization\napproach. Code is available at https://github.com/sail-sg/VeriFree.", "published": "2025-05-27 17:56:27", "link": "http://arxiv.org/abs/2505.21493v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Hardware-Efficient Attention for Fast Decoding", "abstract": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$.", "published": "2025-05-27 17:54:07", "link": "http://arxiv.org/abs/2505.21487v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Are Language Models Consequentialist or Deontological Moral Reasoners?", "abstract": "As AI systems increasingly navigate applications in healthcare, law, and\ngovernance, understanding how they handle ethically complex scenarios becomes\ncritical. Previous work has mainly examined the moral judgments in large\nlanguage models (LLMs), rather than their underlying moral reasoning process.\nIn contrast, we focus on a large-scale analysis of the moral reasoning traces\nprovided by LLMs. Furthermore, unlike prior work that attempted to draw\ninferences from only a handful of moral dilemmas, our study leverages over 600\ndistinct trolley problems as probes for revealing the reasoning patterns that\nemerge within different LLMs. We introduce and test a taxonomy of moral\nrationales to systematically classify reasoning traces according to two main\nnormative ethical theories: consequentialism and deontology. Our analysis\nreveals that LLM chains-of-thought tend to favor deontological principles based\non moral obligations, while post-hoc explanations shift notably toward\nconsequentialist rationales that emphasize utility. Our framework provides a\nfoundation for understanding how LLMs process and articulate ethical\nconsiderations, an important step toward safe and interpretable deployment of\nLLMs in high-stakes decision-making environments. Our code is available at\nhttps://github.com/keenansamway/moral-lens .", "published": "2025-05-27 17:51:18", "link": "http://arxiv.org/abs/2505.21479v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration", "abstract": "Large vision-language models (LVLMs) achieve impressive performance on\nmultimodal tasks but often suffer from hallucination, and confidently describe\nobjects or attributes not present in the image. Current inference-time\ninterventions, while training-free, struggle to maintain accuracy in open-ended\nand long-form generation scenarios. We introduce the Confidence-Aware Attention\nCalibration (CAAC) framework to address this challenge by targeting two key\nbiases: spatial perception bias, which distributes attention disproportionately\nacross image tokens, and modality bias, which shifts focus from visual to\ntextual inputs over time. CAAC employs a two-step approach: Visual-Token\nCalibration (VTC) to balance attention across visual tokens, and Adaptive\nAttention Re-Scaling (AAR) to reinforce visual grounding based on the model's\nconfidence. This confidence-driven adjustment ensures consistent visual\nalignment during generation. Experiments on CHAIR, AMBER, and POPE benchmarks\ndemonstrate that CAAC outperforms baselines, particularly in long-form\ngenerations, effectively reducing hallucination.", "published": "2025-05-27 17:45:21", "link": "http://arxiv.org/abs/2505.21472v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Scaling External Knowledge Input Beyond Context Windows of LLMs via Multi-Agent Collaboration", "abstract": "With the rapid advancement of post-training techniques for reasoning and\ninformation seeking, large language models (LLMs) can incorporate a large\nquantity of retrieved knowledge to solve complex tasks. However, the limited\ncontext window of LLMs obstructs scaling the amount of external knowledge\ninput, prohibiting further improvement, especially for tasks requiring\nsignificant amount of external knowledge. Existing context window extension\nmethods inevitably cause information loss. LLM-based multi-agent methods emerge\nas a new paradigm to handle massive input in a distributional manner, where we\nidentify two core bottlenecks in existing knowledge synchronization and\nreasoning processes. In this work, we develop a multi-agent framework,\n$\\textbf{ExtAgents}$, to overcome the bottlenecks and enable better scalability\nin inference-time knowledge integration without longer-context training.\nBenchmarked with our enhanced multi-hop question answering test,\n$\\textbf{$\\boldsymbol{\\infty}$Bench+}$, and other public test sets including\nlong survey generation, ExtAgents significantly enhances the performance over\nexisting non-training methods with the same amount of external knowledge input,\nregardless of whether it falls $\\textit{within or exceeds the context window}$.\nMoreover, the method maintains high efficiency due to high parallelism. Further\nstudy in the coordination of LLM agents on increasing external knowledge input\ncould benefit real-world applications.", "published": "2025-05-27 17:45:04", "link": "http://arxiv.org/abs/2505.21471v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion", "abstract": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains.", "published": "2025-05-27 17:39:39", "link": "http://arxiv.org/abs/2505.21467v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ID-Align: RoPE-Conscious Position Remapping for Dynamic High-Resolution Adaptation in Vision-Language Models", "abstract": "Currently, a prevalent approach for enhancing Vision-Language Models (VLMs)\nperformance is to encode both the high-resolution version and the thumbnail of\nan image simultaneously. While effective, this method generates a large number\nof image tokens. When combined with the widely used Rotary Position Embedding\n(RoPE), its long-term decay property hinders the interaction between\nhigh-resolution tokens and thumbnail tokens, as well as between text and image.\nTo address these issues, we propose ID-Align, which alleviates these problems\nby reordering position IDs. In this method, high-resolution tokens inherit IDs\nfrom their corresponding thumbnail token while constraining the overexpansion\nof positional indices. Our experiments conducted within the LLaVA-Next\nframework demonstrate that ID-Align achieves significant improvements,\nincluding a 6.09% enhancement on MMBench's relation reasoning tasks and notable\ngains across multiple benchmarks. Our code is available at the following link:\nhttps://github.com/zooblastlbz/ID-Align.", "published": "2025-05-27 17:36:23", "link": "http://arxiv.org/abs/2505.21465v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Do LLMs Need to Think in One Language? Correlation between Latent Language and Task Performance", "abstract": "Large Language Models (LLMs) are known to process information using a\nproficient internal language consistently, referred to as latent language,\nwhich may differ from the input or output languages. However, how the\ndiscrepancy between the latent language and the input and output language\naffects downstream task performance remains largely unexplored. While many\nstudies research the latent language of LLMs, few address its importance in\ninfluencing task performance. In our study, we hypothesize that thinking in\nlatent language consistently enhances downstream task performance. To validate\nthis, our work varies the input prompt languages across multiple downstream\ntasks and analyzes the correlation between consistency in latent language and\ntask performance. We create datasets consisting of questions from diverse\ndomains such as translation and geo-culture, which are influenced by the choice\nof latent language. Experimental results across multiple LLMs on translation\nand geo-culture tasks, which are sensitive to the choice of language, indicate\nthat maintaining consistency in latent language is not always necessary for\noptimal downstream task performance. This is because these models adapt their\ninternal representations near the final layers to match the target language,\nreducing the impact of consistency on overall performance.", "published": "2025-05-27 17:30:57", "link": "http://arxiv.org/abs/2505.21458v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Words Like Knives: Backstory-Personalized Modeling and Detection of Violent Communication", "abstract": "Conversational breakdowns in close relationships are deeply shaped by\npersonal histories and emotional context, yet most NLP research treats conflict\ndetection as a general task, overlooking the relational dynamics that influence\nhow messages are perceived. In this work, we leverage nonviolent communication\n(NVC) theory to evaluate LLMs in detecting conversational breakdowns and\nassessing how relationship backstory influences both human and model perception\nof conflicts. Given the sensitivity and scarcity of real-world datasets\nfeaturing conflict between familiar social partners with rich personal\nbackstories, we contribute the PersonaConflicts Corpus, a dataset of N=5,772\nnaturalistic simulated dialogues spanning diverse conflict scenarios between\nfriends, family members, and romantic partners. Through a controlled human\nstudy, we annotate a subset of dialogues and obtain fine-grained labels of\ncommunication breakdown types on individual turns, and assess the impact of\nbackstory on human and model perception of conflict in conversation. We find\nthat the polarity of relationship backstories significantly shifted human\nperception of communication breakdowns and impressions of the social partners,\nyet models struggle to meaningfully leverage those backstories in the detection\ntask. Additionally, we find that models consistently overestimate how\npositively a message will make a listener feel. Our findings underscore the\ncritical role of personalization to relationship contexts in enabling LLMs to\nserve as effective mediators in human communication for authentic connection.", "published": "2025-05-27 17:23:57", "link": "http://arxiv.org/abs/2505.21451v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Better Instruction Following Retrieval Models", "abstract": "Modern information retrieval (IR) models, trained exclusively on standard\n<query, passage> pairs, struggle to effectively interpret and follow explicit\nuser instructions. We introduce InF-IR, a large-scale, high-quality training\ncorpus tailored for enhancing retrieval models in Instruction-Following IR.\nInF-IR expands traditional training pairs into over 38,000 expressive\n<instruction, query, passage> triplets as positive samples. In particular, for\neach positive triplet, we generate two additional hard negative examples by\npoisoning both instructions and queries, then rigorously validated by an\nadvanced reasoning model (o3-mini) to ensure semantic plausibility while\nmaintaining instructional incorrectness. Unlike existing corpora that primarily\nsupport computationally intensive reranking tasks for decoder-only language\nmodels, the highly contrastive positive-negative triplets in InF-IR further\nenable efficient representation learning for smaller encoder-only models,\nfacilitating direct embedding-based retrieval. Using this corpus, we train\nInF-Embed, an instruction-aware Embedding model optimized through contrastive\nlearning and instruction-query attention mechanisms to align retrieval outcomes\nprecisely with user intents. Extensive experiments across five\ninstruction-based retrieval benchmarks demonstrate that InF-Embed significantly\nsurpasses competitive baselines by 8.1% in p-MRR, measuring the\ninstruction-following capabilities.", "published": "2025-05-27 17:14:37", "link": "http://arxiv.org/abs/2505.21439v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation", "abstract": "Tools enhance the reasoning capabilities of large language models (LLMs) in\ncomplex problem-solving tasks, but not all tasks have available tools. In the\nabsence of predefined tools, prior works have explored instructing LLMs to\ngenerate tools on their own. However, such approaches rely heavily on the\nmodels' internal knowledge and would fail in domains beyond the LLMs' knowledge\nscope. To address this limitation, we propose RefTool, a reference-guided\nframework for automatic tool creation that leverages structured external\nmaterials such as textbooks. RefTool consists of two modules: (1) tool\ncreation, where LLMs generate executable tools from reference content, validate\nthem using illustrative examples, and organize them hierarchically into a\ntoolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to\nselect and apply the appropriate tools to solve problems. Experiments on\ncausality, physics, and chemistry benchmarks demonstrate that RefTool\noutperforms existing tool-creation and domain-specific reasoning methods by\n11.3% on average accuracy, while being cost-efficient and broadly\ngeneralizable. Analyses reveal that grounding tool creation in references\nproduces accurate and faithful tools, and that the hierarchical structure\nfacilitates effective tool selection. RefTool enables LLMs to overcome\nknowledge limitations, demonstrating the value of grounding tool creation in\nexternal references for enhanced and generalizable reasoning.", "published": "2025-05-27 16:41:19", "link": "http://arxiv.org/abs/2505.21413v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity", "abstract": "The surgence of Mixture of Experts (MoE) in Large Language Models promises a\nsmall price of execution cost for a much larger model parameter count and\nlearning capacity, because only a small fraction of parameters are activated\nfor each input token. However, it is commonly observed that some experts are\nactivated far more often than others, leading to system inefficiency when\nrunning the experts on different devices in parallel. Therefore, we introduce\nMixture of Grouped Experts (MoGE), which groups the experts during selection\nand balances the expert workload better than MoE in nature. It constrains\ntokens to activate an equal number of experts within each predefined expert\ngroup. When a model execution is distributed on multiple devices, this\narchitectural design ensures a balanced computational load across devices,\nsignificantly enhancing throughput, particularly for the inference phase.\nFurther, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE\nwith 72 billion total parameters, 16 billion of which are activated for each\ntoken. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and\n800I A2 through extensive system simulation studies. Our experiments indicate\nthat MoGE indeed leads to better expert load balancing and more efficient\nexecution for both model training and inference on Ascend NPUs. The inference\nperformance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further\nimproved to 1528 tokens/s per card by speculative acceleration, outperforming\ncomparable 32B and 72B Dense models. Furthermore, we achieve an excellent\ncost-to-performance ratio for model inference on Ascend 300I Duo.Our studies\nshow that Ascend NPUs are capable of training Pangu Pro MoE with massive\nparallelization to make it a leading model within the sub-100B total parameter\nclass, outperforming prominent open-source models like GLM-Z1-32B and\nQwen3-32B.", "published": "2025-05-27 16:40:21", "link": "http://arxiv.org/abs/2505.21411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models", "abstract": "Factuality in Large Language Models (LLMs) is a persistent challenge. Current\nbenchmarks often assess short factual answers, overlooking the critical ability\nto generate structured, multi-record tabular outputs from parametric knowledge.\nWe demonstrate that this relational fact retrieval is substantially more\ndifficult than isolated point-wise queries, even when individual facts are\nknown to the model, exposing distinct failure modes sensitive to output\ndimensionality (e.g., number of attributes or records). To systematically\nevaluate this under-explored capability, we introduce RelationalFactQA, a new\nbenchmark featuring diverse natural language questions (paired with SQL) and\ngold-standard tabular answers, specifically designed to assess knowledge\nretrieval in a structured format. RelationalFactQA enables analysis across\nvarying query complexities, output sizes, and data characteristics. Our\nexperiments reveal that even state-of-the-art LLMs struggle significantly, not\nexceeding 25% factual accuracy in generating relational outputs, with\nperformance notably degrading as output dimensionality increases. These\nfindings underscore critical limitations in current LLMs' ability to synthesize\nstructured factual knowledge and establish RelationalFactQA as a crucial\nresource for measuring future progress in LLM factuality.", "published": "2025-05-27 16:33:38", "link": "http://arxiv.org/abs/2505.21409v1", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling", "abstract": "Factual incorrectness in generated content is one of the primary concerns in\nubiquitous deployment of large language models (LLMs). Prior findings suggest\nLLMs can (sometimes) detect factual incorrectness in their generated content\n(i.e., fact-checking post-generation). In this work, we provide evidence\nsupporting the presence of LLMs' internal compass that dictate the correctness\nof factual recall at the time of generation. We demonstrate that for a given\nsubject entity and a relation, LLMs internally encode linear features in the\nTransformer's residual stream that dictate whether it will be able to recall\nthe correct attribute (that forms a valid entity-relation-attribute triplet).\nThis self-awareness signal is robust to minor formatting variations. We\ninvestigate the effects of context perturbation via different example selection\nstrategies. Scaling experiments across model sizes and training dynamics\nhighlight that self-awareness emerges rapidly during training and peaks in\nintermediate layers. These findings uncover intrinsic self-monitoring\ncapabilities within LLMs, contributing to their interpretability and\nreliability.", "published": "2025-05-27 16:24:02", "link": "http://arxiv.org/abs/2505.21399v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DecisionFlow: Advancing Large Language Model as Principled Decision Maker", "abstract": "In high-stakes domains such as healthcare and finance, effective\ndecision-making demands not just accurate outcomes but transparent and\nexplainable reasoning. However, current language models often lack the\nstructured deliberation needed for such tasks, instead generating decisions and\njustifications in a disconnected, post-hoc manner. To address this, we propose\nDecisionFlow, a novel decision modeling framework that guides models to reason\nover structured representations of actions, attributes, and constraints. Rather\nthan predicting answers directly from prompts, DecisionFlow builds a\nsemantically grounded decision space and infers a latent utility function to\nevaluate trade-offs in a transparent, utility-driven manner. This process\nproduces decisions tightly coupled with interpretable rationales reflecting the\nmodel's reasoning. Empirical results on two high-stakes benchmarks show that\nDecisionFlow not only achieves up to 30% accuracy gains over strong prompting\nbaselines but also enhances alignment in outcomes. Our work is a critical step\ntoward integrating symbolic reasoning with LLMs, enabling more accountable,\nexplainable, and reliable LLM decision support systems. We release the data and\ncode at https://github.com/xiusic/DecisionFlow.", "published": "2025-05-27 16:23:53", "link": "http://arxiv.org/abs/2505.21397v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science", "abstract": "Recent advancements in large language models (LLMs) have shown promise in\ngenerating novel research ideas. However, these ideas often face challenges\nrelated to feasibility and expected effectiveness. This paper explores how\naugmenting LLMs with relevant data during the idea generation process can\nenhance the quality of generated ideas. We introduce two ways of incorporating\ndata: (1) providing metadata during the idea generation stage to guide LLMs\ntoward feasible directions, and (2) adding automatic validation during the idea\nselection stage to assess the empirical plausibility of hypotheses within\nideas. We conduct experiments in the social science domain, specifically with\nclimate negotiation topics, and find that metadata improves the feasibility of\ngenerated ideas by 20%, while automatic validation improves the overall quality\nof selected ideas by 7%. A human study shows that LLM-generated ideas, along\nwith their related data and validation processes, inspire researchers to\npropose research ideas with higher quality. Our work highlights the potential\nof data-driven research idea generation, and underscores the practical utility\nof LLM-assisted ideation in real-world academic settings.", "published": "2025-05-27 16:23:42", "link": "http://arxiv.org/abs/2505.21396v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs", "abstract": "Evaluating multimodal large language models (MLLMs) is increasingly\nexpensive, as the growing size and cross-modality complexity of benchmarks\ndemand significant scoring efforts. To tackle with this difficulty, we\nintroduce AutoJudger, an agent-driven framework for efficient and adaptive\nbenchmarking of MLLMs that tackles this escalating cost. AutoJudger employs the\nItem Response Theory (IRT) to estimate the question difficulty and an\nautonomous evaluation agent to dynamically select the most informative test\nquestions based on the model's real-time performance. Specifically, AutoJudger\nincorporates two pivotal components: a semantic-aware retrieval mechanism to\nensure that selected questions cover diverse and challenging scenarios across\nboth vision and language modalities, and a dynamic memory that maintains\ncontextual statistics of previously evaluated questions to guide coherent and\nglobally informed question selection throughout the evaluation process.\nExtensive experiments on four representative multimodal benchmarks demonstrate\nthat our adaptive framework dramatically reduces evaluation expenses, i.e.\nAutoJudger uses only 4% of the data to achieve over 90% ranking accuracy with\nthe full benchmark evaluation on MMT-Bench.", "published": "2025-05-27 16:17:15", "link": "http://arxiv.org/abs/2505.21389v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PHISH in MESH: Korean Adversarial Phonetic Substitution and Phonetic-Semantic Feature Integration Defense", "abstract": "As malicious users increasingly employ phonetic substitution to evade hate\nspeech detection, researchers have investigated such strategies. However, two\nkey challenges remain. First, existing studies have overlooked the Korean\nlanguage, despite its vulnerability to phonetic perturbations due to its\nphonographic nature. Second, prior work has primarily focused on constructing\ndatasets rather than developing architectural defenses. To address these\nchallenges, we propose (1) PHonetic-Informed Substitution for Hangul (PHISH)\nthat exploits the phonological characteristics of the Korean writing system,\nand (2) Mixed Encoding of Semantic-pHonetic features (MESH) that enhances the\ndetector's robustness by incorporating phonetic information at the\narchitectural level. Our experimental results demonstrate the effectiveness of\nour proposed methods on both perturbed and unperturbed datasets, suggesting\nthat they not only improve detection performance but also reflect realistic\nadversarial behaviors employed by malicious users.", "published": "2025-05-27 16:09:02", "link": "http://arxiv.org/abs/2505.21380v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing values about gendered language reform in LLMs' revisions", "abstract": "Within the common LLM use case of text revision, we study LLMs' revision of\ngendered role nouns (e.g., outdoorsperson/woman/man) and their justifications\nof such revisions. We evaluate their alignment with feminist and\ntrans-inclusive language reforms for English. Drawing on insight from\nsociolinguistics, we further assess if LLMs are sensitive to the same\ncontextual effects in the application of such reforms as people are, finding\nbroad evidence of such effects. We discuss implications for value alignment.", "published": "2025-05-27 16:07:33", "link": "http://arxiv.org/abs/2505.21378v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History", "abstract": "Effective engagement by large language models (LLMs) requires adapting\nresponses to users' sociodemographic characteristics, such as age, occupation,\nand education level. While many real-world applications leverage dialogue\nhistory for contextualization, existing evaluations of LLMs' behavioral\nadaptation often focus on single-turn prompts. In this paper, we propose a\nframework to evaluate LLM adaptation when attributes are introduced either (1)\nexplicitly via user profiles in the prompt or (2) implicitly through multi-turn\ndialogue history. We assess the consistency of model behavior across these\nmodalities. Using a multi-agent pipeline, we construct a synthetic dataset\npairing dialogue histories with distinct user profiles and employ questions\nfrom the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe\nvalue expression. Our findings indicate that most models adjust their expressed\nvalues in response to demographic changes, particularly in age and education\nlevel, but consistency varies. Models with stronger reasoning capabilities\ndemonstrate greater alignment, indicating the importance of reasoning in robust\nsociodemographic adaptation.", "published": "2025-05-27 15:52:39", "link": "http://arxiv.org/abs/2505.21362v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning", "abstract": "Solving Bengali Math Word Problems (MWPs) remains a major challenge in\nnatural language processing (NLP) due to the language's low-resource status and\nthe multi-step reasoning required. Existing models struggle with complex\nBengali MWPs, largely because no human-annotated Bengali dataset has previously\naddressed this task. This gap has limited progress in Bengali mathematical\nreasoning. To address this, we created SOMADHAN, a dataset of 8792 complex\nBengali MWPs with manually written, step-by-step solutions. We designed this\ndataset to support reasoning-focused evaluation and model development in a\nlinguistically underrepresented context. Using SOMADHAN, we evaluated a range\nof large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series\nmodels, Deepseek, and Qwen - through both zero-shot and few-shot prompting with\nand without Chain of Thought (CoT) reasoning. CoT prompting consistently\nimproved performance over standard prompting, especially in tasks requiring\nmulti-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with\nfew-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune\nmodels efficiently, enabling them to adapt to Bengali MWPs with minimal\ncomputational cost. Our work fills a critical gap in Bengali NLP by providing a\nhigh-quality reasoning dataset and a scalable framework for solving complex\nMWPs. We aim to advance equitable research in low-resource languages and\nenhance reasoning capabilities in educational and language technologies.", "published": "2025-05-27 15:47:10", "link": "http://arxiv.org/abs/2505.21354v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Multilingual Divide and Its Impact on Global AI Safety", "abstract": "Despite advances in large language model capabilities in recent years, a\nlarge gap remains in their capabilities and safety performance for many\nlanguages beyond a relatively small handful of globally dominant languages.\nThis paper provides researchers, policymakers and governance experts with an\noverview of key challenges to bridging the \"language gap\" in AI and minimizing\nsafety risks across languages. We provide an analysis of why the language gap\nin AI exists and grows, and how it creates disparities in global AI safety. We\nidentify barriers to address these challenges, and recommend how those working\nin policy and governance can help address safety concerns associated with the\nlanguage gap by supporting multilingual dataset creation, transparency, and\nresearch.", "published": "2025-05-27 15:37:32", "link": "http://arxiv.org/abs/2505.21344v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims", "abstract": "Patent claims define the scope of protection for an invention. If there are\nambiguities in a claim, it is rejected by the patent office. In the US, this is\nreferred to as indefiniteness (35 U.S.C {\\S} 112(b)) and is among the most\nfrequent reasons for patent application rejection. The development of automatic\nmethods for patent definiteness examination has the potential to make patent\ndrafting and examination more efficient, but no annotated dataset has been\npublished to date.\n  We introduce PEDANTIC (\\underline{P}at\\underline{e}nt\n\\underline{D}efiniteness Ex\\underline{a}mi\\underline{n}a\\underline{ti}on\n\\underline{C}orpus), a novel dataset of 14k US patent claims from patent\napplications relating to Natural Language Processing (NLP), annotated with\nreasons for indefiniteness. We construct PEDANTIC using a fully automatic\npipeline that retrieves office action documents from the USPTO and uses Large\nLanguage Models (LLMs) to extract the reasons for indefiniteness. A human\nvalidation study confirms the pipeline's accuracy in generating high-quality\nannotations. To gain insight beyond binary classification metrics, we implement\nan LLM-as-Judge evaluation that compares the free-form reasoning of every\nmodel-cited reason with every examiner-cited reason. We show that LLM agents\nbased on Qwen 2.5 32B and 72B struggle to outperform logistic regression\nbaselines on definiteness prediction, even though they often correctly identify\nthe underlying reasons. PEDANTIC provides a valuable resource for patent AI\nresearchers, enabling the development of advanced examination models. We will\npublicly release the dataset and code.", "published": "2025-05-27 15:34:39", "link": "http://arxiv.org/abs/2505.21342v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Something's Fishy In The Data Lake: A Critical Re-evaluation of Table Union Search Benchmarks", "abstract": "Recent table representation learning and data discovery methods tackle table\nunion search (TUS) within data lakes, which involves identifying tables that\ncan be unioned with a given query table to enrich its content. These methods\nare commonly evaluated using benchmarks that aim to assess semantic\nunderstanding in real-world TUS tasks. However, our analysis of prominent TUS\nbenchmarks reveals several limitations that allow simple baselines to perform\nsurprisingly well, often outperforming more sophisticated approaches. This\nsuggests that current benchmark scores are heavily influenced by\ndataset-specific characteristics and fail to effectively isolate the gains from\nsemantic understanding. To address this, we propose essential criteria for\nfuture benchmarks to enable a more realistic and reliable evaluation of\nprogress in semantic table union search.", "published": "2025-05-27 15:23:52", "link": "http://arxiv.org/abs/2505.21329v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Leveraging large language models and traditional machine learning ensembles for ADHD detection from narrative transcripts", "abstract": "Despite rapid advances in large language models (LLMs), their integration\nwith traditional supervised machine learning (ML) techniques that have proven\napplicability to medical data remains underexplored. This is particularly true\nfor psychiatric applications, where narrative data often exhibit nuanced\nlinguistic and contextual complexity, and can benefit from the combination of\nmultiple models with differing characteristics. In this study, we introduce an\nensemble framework for automatically classifying\nAttention-Deficit/Hyperactivity Disorder (ADHD) diagnosis (binary) using\nnarrative transcripts. Our approach integrates three complementary models:\nLLaMA3, an open-source LLM that captures long-range semantic structure;\nRoBERTa, a pre-trained transformer model fine-tuned on labeled clinical\nnarratives; and a Support Vector Machine (SVM) classifier trained using\nTF-IDF-based lexical features. These models are aggregated through a majority\nvoting mechanism to enhance predictive robustness. The dataset includes 441\ninstances, including 352 for training and 89 for validation. Empirical results\nshow that the ensemble outperforms individual models, achieving an F$_1$ score\nof 0.71 (95\\% CI: [0.60-0.80]). Compared to the best-performing individual\nmodel (SVM), the ensemble improved recall while maintaining competitive\nprecision. This indicates the strong sensitivity of the ensemble in identifying\nADHD-related linguistic cues. These findings demonstrate the promise of hybrid\narchitectures that leverage the semantic richness of LLMs alongside the\ninterpretability and pattern recognition capabilities of traditional supervised\nML, offering a new direction for robust and generalizable psychiatric text\nclassification.", "published": "2025-05-27 15:22:01", "link": "http://arxiv.org/abs/2505.21324v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Charting the Landscape of African NLP: Mapping Progress and Shaping the Road Ahead", "abstract": "With over 2,000 languages and potentially millions of speakers, Africa\nrepresents one of the richest linguistic regions in the world. Yet, this\ndiversity is scarcely reflected in state-of-the-art natural language processing\n(NLP) systems and large language models (LLMs), which predominantly support a\nnarrow set of high-resource languages. This exclusion not only limits the reach\nand utility of modern NLP technologies but also risks widening the digital\ndivide across linguistic communities. Nevertheless, NLP research on African\nlanguages is active and growing. In recent years, there has been a surge of\ninterest in this area, driven by several factors-including the creation of\nmultilingual language resources, the rise of community-led initiatives, and\nincreased support through funding programs. In this survey, we analyze 734\nresearch papers on NLP for African languages published over the past five\nyears, offering a comprehensive overview of recent progress across core tasks.\nWe identify key trends shaping the field and conclude by outlining promising\ndirections to foster more inclusive and sustainable NLP research for African\nlanguages.", "published": "2025-05-27 15:13:08", "link": "http://arxiv.org/abs/2505.21315v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing fMRI Data Acquisition for Decoding Natural Speech with Limited Participants", "abstract": "We investigate optimal strategies for decoding perceived natural speech from\nfMRI data acquired from a limited number of participants. Leveraging Lebel et\nal. (2023)'s dataset of 8 participants, we first demonstrate the effectiveness\nof training deep neural networks to predict LLM-derived text representations\nfrom fMRI activity. Then, in this data regime, we observe that multi-subject\ntraining does not improve decoding accuracy compared to single-subject\napproach. Furthermore, training on similar or different stimuli across subjects\nhas a negligible effect on decoding accuracy. Finally, we find that our\ndecoders better model syntactic than semantic features, and that stories\ncontaining sentences with complex syntax or rich semantic content are more\nchallenging to decode. While our results demonstrate the benefits of having\nextensive data per participant (deep phenotyping), they suggest that leveraging\nmulti-subject for natural speech decoding likely requires deeper phenotyping or\na substantially larger cohort.", "published": "2025-05-27 15:06:04", "link": "http://arxiv.org/abs/2505.21304v1", "categories": ["q-bio.NC", "cs.CL", "cs.LG"], "primary_category": "q-bio.NC"}
{"title": "How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate Categories in Italian", "abstract": "People can categorize the same entity at multiple taxonomic levels, such as\nbasic (bear), superordinate (animal), and subordinate (grizzly bear). While\nprior research has focused on basic-level categories, this study is the first\nattempt to examine the organization of categories by analyzing exemplars\nproduced at the subordinate level. We present a new Italian psycholinguistic\ndataset of human-generated exemplars for 187 concrete words. We then use these\ndata to evaluate whether textual and vision LLMs produce meaningful exemplars\nthat align with human category organization across three key tasks: exemplar\ngeneration, category induction, and typicality judgment. Our findings show a\nlow alignment between humans and LLMs, consistent with previous studies.\nHowever, their performance varies notably across different semantic domains.\nUltimately, this study highlights both the promises and the constraints of\nusing AI-generated exemplars to support psychological and linguistic research.", "published": "2025-05-27 15:04:52", "link": "http://arxiv.org/abs/2505.21301v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale Verified Dataset", "abstract": "Advancing code reasoning in large language models (LLMs) is fundamentally\nlimited by the scarcity of high-difficulty datasets, especially those with\nverifiable input-output test cases necessary for rigorous solution validation\nat scale. We introduce rStar-Coder, which significantly improves LLM code\nreasoning capabilities by constructing a large-scale, verified dataset of 418K\ncompetition-level code problems, 580K long-reasoning solutions along with rich\ntest cases of varying difficulty. This is achieved through three core\ncontributions: (1) we curate competitive programming code problems and oracle\nsolutions to synthesize new, solvable problems; (2) we introduce a reliable\ninput-output test case synthesis pipeline that decouples the generation into a\nthree-step input generation method and a mutual verification mechanism for\neffective output labeling; (3) we augment problems with high-quality,\ntest-case-verified long-reasoning solutions. Extensive experiments on Qwen\nmodels (1.5B-14B) across various code reasoning benchmarks demonstrate the\nsuperiority of rStar-Coder dataset, achieving leading performance comparable to\nfrontier reasoning LLMs with much smaller model sizes. On LiveCodeBench,\nrStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and\nQwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more\nchallenging USA Computing Olympiad, our 7B model achieves an average pass@1\naccuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the\ndataset will be released at https://github.com/microsoft/rStar.", "published": "2025-05-27 15:00:57", "link": "http://arxiv.org/abs/2505.21297v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space", "abstract": "Large Language Models (LLMs), despite advanced general capabilities, still\nsuffer from numerous safety risks, especially jailbreak attacks that bypass\nsafety protocols. Understanding these vulnerabilities through black-box\njailbreak attacks, which better reflect real-world scenarios, offers critical\ninsights into model robustness. While existing methods have shown improvements\nthrough various prompt engineering techniques, their success remains limited\nagainst safety-aligned models, overlooking a more fundamental problem: the\neffectiveness is inherently bounded by the predefined strategy spaces. However,\nexpanding this space presents significant challenges in both systematically\ncapturing essential attack patterns and efficiently navigating the increased\ncomplexity. To better explore the potential of expanding the strategy space, we\naddress these challenges through a novel framework that decomposes jailbreak\nstrategies into essential components based on the Elaboration Likelihood Model\n(ELM) theory and develops genetic-based optimization with intention evaluation\nmechanisms. To be striking, our experiments reveal unprecedented jailbreak\ncapabilities by expanding the strategy space: we achieve over 90% success rate\non Claude-3.5 where prior methods completely fail, while demonstrating strong\ncross-model transferability and surpassing specialized safeguard models in\nevaluation accuracy. The code is open-sourced at:\nhttps://github.com/Aries-iai/CL-GSO.", "published": "2025-05-27 14:48:44", "link": "http://arxiv.org/abs/2505.21277v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Multilingual Pretraining for Pixel Language Models", "abstract": "Pixel language models operate directly on images of rendered text,\neliminating the need for a fixed vocabulary. While these models have\ndemonstrated strong capabilities for downstream cross-lingual transfer,\nmultilingual pretraining remains underexplored. We introduce PIXEL-M4, a model\npretrained on four visually and linguistically diverse languages: English,\nHindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic\nand syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart\non non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4\ncaptures rich linguistic features, even in languages not seen during\npretraining. Furthermore, an analysis of its hidden representations shows that\nmultilingual pretraining yields a semantic embedding space closely aligned\nacross the languages used for pretraining. This work demonstrates that\nmultilingual pretraining substantially enhances the capability of pixel\nlanguage models to effectively support a diverse set of languages.", "published": "2025-05-27 14:40:47", "link": "http://arxiv.org/abs/2505.21265v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision", "abstract": "Multi-hop question answering (MHQA) involves reasoning across multiple\ndocuments to answer complex questions. Dense retrievers typically outperform\nsparse methods like BM25 by leveraging semantic embeddings; however, they\nrequire labeled query-document pairs for fine-tuning. This poses a significant\nchallenge in MHQA due to the high variability of queries (reformulated)\nquestions throughout the reasoning steps. To overcome this limitation, we\nintroduce Retriever Supervision with Consistency and Relevance (ReSCORE), a\nnovel method for training dense retrievers for MHQA without labeled documents.\nReSCORE leverages large language models to capture each documents relevance to\nthe question and consistency with the correct answer and use them to train a\nretriever within an iterative question-answering framework. Experiments on\nthree MHQA benchmarks demonstrate the effectiveness of ReSCORE, with\nsignificant improvements in retrieval, and in turn, the state-of-the-art MHQA\nperformance. Our implementation is available at:\nhttps://leeds1219.github.io/ReSCORE.", "published": "2025-05-27 14:28:24", "link": "http://arxiv.org/abs/2505.21250v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary Adaptation in High OOV Settings", "abstract": "Large Language Models (LLMs) recently achieved great success in medical text\nsummarization by simply using in-context learning. However, these recent\nefforts do not perform fine-grained evaluations under difficult settings where\nLLMs might fail. They typically report performance scores over the entire\ndataset. Through our benchmarking study, we show that LLMs show a significant\nperformance drop for data points with high concentration of out-of-vocabulary\n(OOV) words or with high novelty. Vocabulary adaptation is an intuitive\nsolution to this vocabulary mismatch issue where the LLM vocabulary gets\nupdated with certain expert domain (here, medical) words or subwords. An\ninteresting finding from our study is that Llama-3.1, even with a vocabulary\nsize of around 128K tokens, still faces over-fragmentation issue with medical\nwords. To that end, we show vocabulary adaptation helps improve the LLM\nsummarization performance even in difficult settings. Through extensive\nexperimentation of multiple vocabulary adaptation strategies, two continual\npretraining strategies, and three benchmark medical summarization datasets, we\ngain valuable insights into the role of vocabulary adaptation strategies for\ncustomizing LLMs to the medical domain. We also performed a human evaluation\nstudy with medical experts where they found that vocabulary adaptation results\nin more relevant and faithful summaries. Our codebase is made publicly\navailable at https://github.com/gb-kgp/LLM-MedicalSummarization-Benchmark.", "published": "2025-05-27 14:23:03", "link": "http://arxiv.org/abs/2505.21242v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners", "abstract": "Cognitive Diagnosis (CD) has become a critical task in AI-empowered\neducation, supporting personalized learning by accurately assessing students'\ncognitive states. However, traditional CD models often struggle in cold-start\nscenarios due to the lack of student-exercise interaction data. Recent\nNLP-based approaches leveraging pre-trained language models (PLMs) have shown\npromise by utilizing textual features but fail to fully bridge the gap between\nsemantic understanding and cognitive profiling. In this work, we propose\nLanguage Models as Zeroshot Cognitive Diagnosis Learners (LMCD), a novel\nframework designed to handle cold-start challenges by harnessing large language\nmodels (LLMs). LMCD operates via two primary phases: (1) Knowledge Diffusion,\nwhere LLMs generate enriched contents of exercises and knowledge concepts\n(KCs), establishing stronger semantic links; and (2) Semantic-Cognitive Fusion,\nwhere LLMs employ causal attention mechanisms to integrate textual information\nand student cognitive states, creating comprehensive profiles for both students\nand exercises. These representations are efficiently trained with off-the-shelf\nCD models. Experiments on two real-world datasets demonstrate that LMCD\nsignificantly outperforms state-of-the-art methods in both exercise-cold and\ndomain-cold settings. The code is publicly available at\nhttps://github.com/TAL-auroraX/LMCD", "published": "2025-05-27 14:19:35", "link": "http://arxiv.org/abs/2505.21239v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PSRB: A Comprehensive Benchmark for Evaluating Persian ASR Systems", "abstract": "Although Automatic Speech Recognition (ASR) systems have become an integral\npart of modern technology, their evaluation remains challenging, particularly\nfor low-resource languages such as Persian. This paper introduces Persian\nSpeech Recognition Benchmark(PSRB), a comprehensive benchmark designed to\naddress this gap by incorporating diverse linguistic and acoustic conditions.\nWe evaluate ten ASR systems, including state-of-the-art commercial and\nopen-source models, to examine performance variations and inherent biases.\nAdditionally, we conduct an in-depth analysis of Persian ASR transcriptions,\nidentifying key error types and proposing a novel metric that weights\nsubstitution errors. This metric enhances evaluation robustness by reducing the\nimpact of minor and partial errors, thereby improving the precision of\nperformance assessment. Our findings indicate that while ASR models generally\nperform well on standard Persian, they struggle with regional accents,\nchildren's speech, and specific linguistic challenges. These results highlight\nthe necessity of fine-tuning and incorporating diverse, representative training\ndatasets to mitigate biases and enhance overall ASR performance. PSRB provides\na valuable resource for advancing ASR research in Persian and serves as a\nframework for developing benchmarks in other low-resource languages. A subset\nof the PSRB dataset is publicly available at\nhttps://huggingface.co/datasets/PartAI/PSRB.", "published": "2025-05-27 14:14:55", "link": "http://arxiv.org/abs/2505.21230v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Representation Level Analysis of NMT Model Robustness to Grammatical Errors", "abstract": "Understanding robustness is essential for building reliable NLP systems.\nUnfortunately, in the context of machine translation, previous work mainly\nfocused on documenting robustness failures or improving robustness. In\ncontrast, we study robustness from a model representation perspective by\nlooking at internal model representations of ungrammatical inputs and how they\nevolve through model layers. For this purpose, we perform Grammatical Error\nDetection (GED) probing and representational similarity analysis. Our findings\nindicate that the encoder first detects the grammatical error, then corrects it\nby moving its representation toward the correct form. To understand what\ncontributes to this process, we turn to the attention mechanism where we\nidentify what we term Robustness Heads. We find that Robustness Heads attend to\ninterpretable linguistic units when responding to grammatical errors, and that\nwhen we fine-tune models for robustness, they tend to rely more on Robustness\nHeads for updating the ungrammatical word representation.", "published": "2025-05-27 14:10:30", "link": "http://arxiv.org/abs/2505.21224v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pretrained LLMs Learn Multiple Types of Uncertainty", "abstract": "Large Language Models are known to capture real-world knowledge, allowing\nthem to excel in many downstream tasks. Despite recent advances, these models\nare still prone to what are commonly known as hallucinations, causing them to\nemit unwanted and factually incorrect text. In this work, we study how well\nLLMs capture uncertainty, without explicitly being trained for that. We show\nthat, if considering uncertainty as a linear concept in the model's latent\nspace, it might indeed be captured, even after only pretraining. We further\nshow that, though unintuitive, LLMs appear to capture several different types\nof uncertainty, each of which can be useful to predict the correctness for a\nspecific task or benchmark. Furthermore, we provide in-depth results such as\ndemonstrating a correlation between our correction prediction and the model's\nability to abstain from misinformation using words, and the lack of impact of\nmodel scaling for capturing uncertainty. Finally, we claim that unifying the\nuncertainty types as a single one using instruction-tuning or [IDK]-token\ntuning is helpful for the model in terms of correctness prediction.", "published": "2025-05-27 14:06:15", "link": "http://arxiv.org/abs/2505.21218v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM's Instruction-Following Capabilities", "abstract": "The finetuning of Large Language Models (LLMs) has significantly advanced\ntheir instruction-following capabilities, yet the underlying computational\nmechanisms driving these improvements remain poorly understood. This study\nsystematically examines how fine-tuning reconfigures LLM computations by\nisolating and analyzing instruction-specific sparse components, i.e., neurons\nin dense models and both neurons and experts in Mixture-of-Experts (MoE)\narchitectures. In particular, we introduce HexaInst, a carefully curated and\nbalanced instructional dataset spanning six distinct categories, and propose\nSPARCOM, a novel analytical framework comprising three key contributions: (1) a\nmethod for identifying these sparse components, (2) an evaluation of their\nfunctional generality and uniqueness, and (3) a systematic comparison of their\nalterations. Through experiments, we demonstrate functional generality,\nuniqueness, and the critical role of these components in instruction execution.\nBy elucidating the relationship between fine-tuning-induced adaptations and\nsparse computational substrates, this work provides deeper insights into how\nLLMs internalize instruction-following behavior for the trustworthy LLM\ncommunity.", "published": "2025-05-27 13:40:28", "link": "http://arxiv.org/abs/2505.21191v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation", "abstract": "Radiology reports convey detailed clinical observations and capture\ndiagnostic reasoning that evolves over time. However, existing evaluation\nmethods are limited to single-report settings and rely on coarse metrics that\nfail to capture fine-grained clinical semantics and temporal dependencies. We\nintroduce LUNGUAGE,a benchmark dataset for structured radiology report\ngeneration that supports both single-report evaluation and longitudinal\npatient-level assessment across multiple studies. It contains 1,473 annotated\nchest X-ray reports, each reviewed by experts, and 80 of them contain\nlongitudinal annotations to capture disease progression and inter-study\nintervals, also reviewed by experts. Using this benchmark, we develop a\ntwo-stage framework that transforms generated reports into fine-grained,\nschema-aligned structured representations, enabling longitudinal\ninterpretation. We also propose LUNGUAGESCORE, an interpretable metric that\ncompares structured outputs at the entity, relation, and attribute level while\nmodeling temporal consistency across patient timelines. These contributions\nestablish the first benchmark dataset, structuring framework, and evaluation\nmetric for sequential radiology reporting, with empirical results demonstrating\nthat LUNGUAGESCORE effectively supports structured report evaluation. The code\nis available at: https://github.com/SuperSupermoon/Lunguage", "published": "2025-05-27 13:40:00", "link": "http://arxiv.org/abs/2505.21190v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the Latent Capacity of LLMs for One-Step Text Generation", "abstract": "A recent study showed that large language models (LLMs) can reconstruct\nsurprisingly long texts - up to thousands of tokens - via autoregressive\ngeneration from just one specially trained input embedding. In this work, we\nexplore whether such reconstruction is possible without autoregression. We show\nthat frozen LLMs can generate hundreds of accurate tokens in just one forward\npass, when provided with only two learned embeddings. This reveals a surprising\nand underexplored capability of LLMs - multi-token generation without iterative\ndecoding. We investigate the behaviour of these embeddings and provide insight\ninto the type of information they encode. We also empirically show that\nalthough these representations are not unique for a given text, they form\nconnected and local regions in embedding space - a property that suggests the\npotential of learning a dedicated encoder into that space.", "published": "2025-05-27 13:39:24", "link": "http://arxiv.org/abs/2505.21189v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing", "abstract": "To construct responsible and secure AI applications, harmful information data\nis widely utilized for adversarial testing and the development of safeguards.\nExisting studies mainly leverage Large Language Models (LLMs) to synthesize\ndata to obtain high-quality task datasets at scale, thereby avoiding costly\nhuman annotation. However, limited by the safety alignment mechanisms of LLMs,\nthe synthesis of harmful data still faces challenges in generation reliability\nand content diversity. In this study, we propose a novel harmful information\nsynthesis framework, PoisonSwarm, which applies the model crowdsourcing\nstrategy to generate diverse harmful data while maintaining a high success\nrate. Specifically, we generate abundant benign data as the based templates in\na counterfactual manner. Subsequently, we decompose each based template into\nmultiple semantic units and perform unit-by-unit toxification and final\nrefinement through dynamic model switching, thus ensuring the success of\nsynthesis. Experimental results demonstrate that PoisonSwarm achieves\nstate-of-the-art performance in synthesizing different categories of harmful\ndata with high scalability and diversity.", "published": "2025-05-27 13:33:57", "link": "http://arxiv.org/abs/2505.21184v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning", "abstract": "As test-time scaling becomes a pivotal research frontier in Large Language\nModels (LLMs) development, contemporary and advanced post-training\nmethodologies increasingly focus on extending the generation length of long\nChain-of-Thought (CoT) responses to enhance reasoning capabilities toward\nDeepSeek R1-like performance. However, recent studies reveal a persistent\noverthinking phenomenon in state-of-the-art reasoning models, manifesting as\nexcessive redundancy or repetitive thinking patterns in long CoT responses. To\naddress this issue, in this paper, we propose a simple yet effective two-stage\nreinforcement learning framework for achieving concise reasoning in LLMs, named\nConciseR. Specifically, the first stage, using more training steps, aims to\nincentivize the model's reasoning capabilities via Group Relative Policy\nOptimization with clip-higher and dynamic sampling components (GRPO++), and the\nsecond stage, using fewer training steps, explicitly enforces conciseness and\nimproves efficiency via Length-aware Group Relative Policy Optimization\n(L-GRPO). Significantly, ConciseR only optimizes response length once all\nrollouts of a sample are correct, following the \"walk before you run\"\nprinciple. Extensive experimental results demonstrate that our ConciseR model,\nwhich generates more concise CoT reasoning responses, outperforms recent\nstate-of-the-art reasoning models with zero RL paradigm across AIME 2024,\nMATH-500, AMC 2023, Minerva, and Olympiad benchmarks.", "published": "2025-05-27 13:29:51", "link": "http://arxiv.org/abs/2505.21178v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TAT-R1: Terminology-Aware Translation with Reinforcement Learning and Word Alignment", "abstract": "Recently, deep reasoning large language models(LLMs) like DeepSeek-R1 have\nmade significant progress in tasks such as mathematics and coding. Inspired by\nthis, several studies have employed reinforcement learning(RL) to enhance\nmodels' deep reasoning capabilities and improve machine translation(MT)\nquality. However, the terminology translation, an essential task in MT, remains\nunexplored in deep reasoning LLMs. In this paper, we propose \\textbf{TAT-R1}, a\nterminology-aware translation model trained with reinforcement learning and\nword alignment. Specifically, we first extract the keyword translation pairs\nusing a word alignment model. Then we carefully design three types of\nrule-based alignment rewards with the extracted alignment relationships. With\nthose alignment rewards, the RL-trained translation model can learn to focus on\nthe accurate translation of key information, including terminology in the\nsource text. Experimental results show the effectiveness of TAT-R1. Our model\nsignificantly improves terminology translation accuracy compared to the\nbaseline models while maintaining comparable performance on general translation\ntasks. In addition, we conduct detailed ablation studies of the\nDeepSeek-R1-like training paradigm for machine translation and reveal several\nkey findings.", "published": "2025-05-27 13:26:02", "link": "http://arxiv.org/abs/2505.21172v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M-Wanda: Improving One-Shot Pruning for Multilingual LLMs", "abstract": "Multilingual LLM performance is often critically dependent on model size.\nWith an eye on efficiency, this has led to a surge in interest in one-shot\npruning methods that retain the benefits of large-scale pretraining while\nshrinking the model size. However, as pruning tends to come with performance\nloss, it is important to understand the trade-offs between multilinguality and\nsparsification. In this work, we study multilingual performance under different\nsparsity constraints and show that moderate ratios already substantially harm\nperformance. To help bridge this gap, we propose M-Wanda, a pruning method that\nmodels cross-lingual variation by incorporating language-aware activation\nstatistics into its pruning criterion and dynamically adjusts layerwise\nsparsity based on cross-lingual importance. We show that M-Wanda consistently\nimproves performance at minimal additional costs. We are the first to\nexplicitly optimize pruning to retain multilingual performance, and hope to\ninspire future advances in multilingual pruning.", "published": "2025-05-27 13:24:38", "link": "http://arxiv.org/abs/2505.21171v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging GANs for citation intent classification and its impact on citation network analysis", "abstract": "Citations play a fundamental role in the scientific ecosystem, serving as a\nfoundation for tracking the flow of knowledge, acknowledging prior work, and\nassessing scholarly influence. In scientometrics, they are also central to the\nconstruction of quantitative indicators. Not all citations, however, serve the\nsame function: some provide background, others introduce methods, or compare\nresults. Therefore, understanding citation intent allows for a more nuanced\ninterpretation of scientific impact. In this paper, we adopted a GAN-based\nmethod to classify citation intents. Our results revealed that the proposed\nmethod achieves competitive classification performance, closely matching\nstate-of-the-art results with substantially fewer parameters. This demonstrates\nthe effectiveness and efficiency of leveraging GAN architectures combined with\ncontextual embeddings in intent classification task. We also investigated\nwhether filtering citation intents affects the centrality of papers in citation\nnetworks. Analyzing the network constructed from the unArXiv dataset, we found\nthat paper rankings can be significantly influenced by citation intent. All\nfour centrality metrics examined- degree, PageRank, closeness, and betweenness\n- were sensitive to the filtering of citation types. The betweenness centrality\ndisplayed the greatest sensitivity, showing substantial changes in ranking when\nspecific citation intents were removed.", "published": "2025-05-27 13:16:09", "link": "http://arxiv.org/abs/2505.21162v1", "categories": ["cs.DL", "cs.CL", "cs.SI"], "primary_category": "cs.DL"}
{"title": "Assessment of L2 Oral Proficiency using Speech Large Language Models", "abstract": "The growing population of L2 English speakers has increased the demand for\ndeveloping automatic graders for spoken language assessment (SLA).\nHistorically, statistical models, text encoders, and self-supervised speech\nmodels have been utilised for this task. However, cascaded systems suffer from\nthe loss of information, while E2E graders also have limitations. With the\nrecent advancements of multi-modal large language models (LLMs), we aim to\nexplore their potential as L2 oral proficiency graders and overcome these\nissues. In this work, we compare various training strategies using regression\nand classification targets. Our results show that speech LLMs outperform all\nprevious competitive baselines, achieving superior performance on two datasets.\nFurthermore, the trained grader demonstrates strong generalisation capabilities\nin the cross-part or cross-task evaluation, facilitated by the audio\nunderstanding knowledge acquired during LLM pre-training.", "published": "2025-05-27 12:58:21", "link": "http://arxiv.org/abs/2505.21148v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis", "abstract": "Large-scale training corpora have significantly improved the performance of\nASR models. Unfortunately, due to the relative scarcity of data, Chinese\naccents and dialects remain a challenge for most ASR models. Recent\nadvancements in self-supervised learning have shown that self-supervised pre-\ntraining, combined with large language models (LLM), can effectively enhance\nASR performance in low-resource scenarios. We aim to investigate the\neffectiveness of this paradigm for Chinese dialects. Specifically, we pre-train\na Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech\ndata and do alignment training on a supervised dataset of 40,000 hours. Then,\nwe systematically examine the impact of various projectors and LLMs on\nMandarin, dialect, and accented speech recognition performance under this\nparadigm. Our method achieved SOTA results on multiple dialect datasets,\nincluding Kespeech. We will open-source our work to promote reproducible\nresearch", "published": "2025-05-27 12:50:55", "link": "http://arxiv.org/abs/2505.21138v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Scaling and Prompting for Improved End-to-End Spoken Grammatical Error Correction", "abstract": "Spoken Grammatical Error Correction (SGEC) and Feedback (SGECF) are crucial\nfor second language learners, teachers and test takers. Traditional SGEC\nsystems rely on a cascaded pipeline consisting of an ASR, a module for\ndisfluency detection (DD) and removal and one for GEC. With the rise of\nend-to-end (E2E) speech foundation models, we investigate their effectiveness\nin SGEC and feedback generation. This work introduces a pseudo-labelling\nprocess to address the challenge of limited labelled data, expanding the\ntraining data size from 77 hours to approximately 2500 hours, leading to\nimproved performance. Additionally, we prompt an E2E Whisper-based SGEC model\nwith fluent transcriptions, showing a slight improvement in SGEC performance,\nwith more significant gains in feedback generation. Finally, we assess the\nimpact of increasing model size, revealing that while pseudo-labelled data does\nnot yield performance gain for a larger Whisper model, training with prompts\nproves beneficial.", "published": "2025-05-27 12:50:53", "link": "http://arxiv.org/abs/2505.21137v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Creativity in LLM-based Multi-Agent Systems: A Survey", "abstract": "Large language model (LLM)-driven multi-agent systems (MAS) are transforming\nhow humans and AIs collaboratively generate ideas and artifacts. While existing\nsurveys provide comprehensive overviews of MAS infrastructures, they largely\noverlook the dimension of \\emph{creativity}, including how novel outputs are\ngenerated and evaluated, how creativity informs agent personas, and how\ncreative workflows are coordinated. This is the first survey dedicated to\ncreativity in MAS. We focus on text and image generation tasks, and present:\n(1) a taxonomy of agent proactivity and persona design; (2) an overview of\ngeneration techniques, including divergent exploration, iterative refinement,\nand collaborative synthesis, as well as relevant datasets and evaluation\nmetrics; and (3) a discussion of key challenges, such as inconsistent\nevaluation standards, insufficient bias mitigation, coordination conflicts, and\nthe lack of unified benchmarks. This survey offers a structured framework and\nroadmap for advancing the development, evaluation, and standardization of\ncreative MAS.", "published": "2025-05-27 12:36:14", "link": "http://arxiv.org/abs/2505.21116v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA", "abstract": "Large Language Models (LLMs) often hallucinate in question answering (QA)\ntasks. A key yet underexplored factor contributing to this is the temporality\nof questions -- whether they are evergreen (answers remain stable over time) or\nmutable (answers change). In this work, we introduce EverGreenQA, the first\nmultilingual QA dataset with evergreen labels, supporting both evaluation and\ntraining. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they\nencode question temporality explicitly (via verbalized judgments) or implicitly\n(via uncertainty signals). We also train EG-E5, a lightweight multilingual\nclassifier that achieves SoTA performance on this task. Finally, we demonstrate\nthe practical utility of evergreen classification across three applications:\nimproving self-knowledge estimation, filtering QA datasets, and explaining\nGPT-4o retrieval behavior.", "published": "2025-05-27 12:35:13", "link": "http://arxiv.org/abs/2505.21115v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction", "abstract": "Despite recent advancements in domain adaptation techniques for large\nlanguage models, these methods remain computationally intensive, and the\nresulting models can still exhibit hallucination issues. Most existing\nadaptation methods do not prioritize reducing the computational resources\nrequired for fine-tuning and inference of language models. Hallucination issues\nhave gradually decreased with each new model release. However, they remain\nprevalent in engineering contexts, where generating well-structured text with\nminimal errors and inconsistencies is critical. This work introduces a novel\napproach called the Small Language Graph (SLG), which is a lightweight\nadaptation solution designed to address the two key challenges outlined above.\nThe system is structured in the form of a graph, where each node represents a\nlightweight expert - a small language model fine-tuned on specific and concise\ntexts. The results of this study have shown that SLG was able to surpass\nconventional fine-tuning methods on the Exact Match metric by 3 times.\nAdditionally, the fine-tuning process was 1.7 times faster compared to that of\na larger stand-alone language model. These findings introduce a potential for\nsmall to medium-sized engineering companies to confidently use generative AI\ntechnologies, such as LLMs, without the necessity to invest in expensive\ncomputational resources. Also, the graph architecture and the small size of\nexpert nodes offer a possible opportunity for distributed AI systems, thus\npotentially diverting the global need for expensive centralized compute\nclusters.", "published": "2025-05-27 12:31:24", "link": "http://arxiv.org/abs/2505.21109v1", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR", "cs.LG", "I.2.7; I.2.1; I.5.1; I.2.6; H.3.1"], "primary_category": "cs.CL"}
{"title": "Thinker: Learning to Think Fast and Slow", "abstract": "Recent studies show that the reasoning capabilities of Large Language Models\n(LLMs) can be improved by applying Reinforcement Learning (RL) to\nquestion-answering (QA) tasks in areas such as math and coding. With a long\ncontext length, LLMs may learn to perform search, as indicated by the\nself-correction behavior observed in DeepSeek R1. However, this search behavior\nis often imprecise and lacks confidence, resulting in long, redundant responses\nand highlighting deficiencies in intuition and verification. Inspired by the\nDual Process Theory in psychology, we introduce a simple modification to the QA\ntask that includes four stages: Fast Thinking, where the LLM must answer within\na strict token budget; Verification, where the model evaluates its initial\nresponse; Slow Thinking, where it refines the initial response with more\ndeliberation; and Summarization, where it distills the refinement from the\nprevious stage into precise steps. Our proposed task improves average accuracy\nfrom 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for\nDeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone\nachieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial\ninference efficiency gains. These findings suggest that intuition and\ndeliberative reasoning are distinct, complementary systems benefiting from\ntargeted training.", "published": "2025-05-27 12:22:46", "link": "http://arxiv.org/abs/2505.21097v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.6; I.2.8; I.5.1"], "primary_category": "cs.CL"}
{"title": "BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural Knowledge", "abstract": "In this work, we introduce BLUCK, a new dataset designed to measure the\nperformance of Large Language Models (LLMs) in Bengali linguistic understanding\nand cultural knowledge. Our dataset comprises 2366 multiple-choice questions\n(MCQs) carefully curated from compiled collections of several college and job\nlevel examinations and spans 23 categories covering knowledge on Bangladesh's\nculture and history and Bengali linguistics. We benchmarked BLUCK using 6\nproprietary and 3 open-source LLMs - including GPT-4o, Claude-3.5-Sonnet,\nGemini-1.5-Pro, Llama-3.3-70B-Instruct, and DeepSeekV3. Our results show that\nwhile these models perform reasonably well overall, they, however, struggles in\nsome areas of Bengali phonetics. Although current LLMs' performance on Bengali\ncultural and linguistic contexts is still not comparable to that of mainstream\nlanguages like English, our results indicate Bengali's status as a mid-resource\nlanguage. Importantly, BLUCK is also the first MCQ-based evaluation benchmark\nthat is centered around native Bengali culture, history, and linguistics.", "published": "2025-05-27 12:19:12", "link": "http://arxiv.org/abs/2505.21092v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)", "abstract": "System prompts in Large Language Models (LLMs) are predefined directives that\nguide model behaviour, taking precedence over user inputs in text processing\nand generation. LLM deployers increasingly use them to ensure consistent\nresponses across contexts. While model providers set a foundation of system\nprompts, deployers and third-party developers can append additional prompts\nwithout visibility into others' additions, while this layered implementation\nremains entirely hidden from end-users. As system prompts become more complex,\nthey can directly or indirectly introduce unaccounted for side effects. This\nlack of transparency raises fundamental questions about how the position of\ninformation in different directives shapes model outputs. As such, this work\nexamines how the placement of information affects model behaviour. To this end,\nwe compare how models process demographic information in system versus user\nprompts across six commercially available LLMs and 50 demographic groups. Our\nanalysis reveals significant biases, manifesting in differences in user\nrepresentation and decision-making scenarios. Since these variations stem from\ninaccessible and opaque system-level configurations, they risk\nrepresentational, allocative and potential other biases and downstream harms\nbeyond the user's ability to detect or correct. Our findings draw attention to\nthese critical issues, which have the potential to perpetuate harms if left\nunexamined. Further, we argue that system prompt analysis must be incorporated\ninto AI auditing processes, particularly as customisable system prompts become\nincreasingly prevalent in commercial AI deployments.", "published": "2025-05-27 12:19:08", "link": "http://arxiv.org/abs/2505.21091v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models", "abstract": "Large language models (LLMs) have recently achieved impressive performance\nacross a wide range of natural language tasks and are now widely used in\nreal-world applications. Among them, black-box LLMs--served via APIs without\naccess to model internals--are especially dominant due to their scalability and\nease of deployment. Despite their strong capabilities, these models typically\nproduce generalized responses that overlook personal preferences and reasoning\nstyles. This has led to growing interest in black-box LLM personalization,\nwhich aims to tailor model outputs to user-specific context without modifying\nmodel parameters. However, existing approaches primarily focus on\nresponse-level personalization, attempting to match final outputs without\nmodeling personal thought process. To address this limitation, we propose RPM,\na framework for reasoning-level personalization that aligns the model's\nreasoning process with a user's personalized logic. RPM first constructs\nstatistical user-specific factors by extracting and grouping\nresponse-influential features from user history. It then builds personalized\nreasoning paths that reflect how these factors are used in context. In the\ninference stage, RPM retrieves reasoning-aligned examples for new queries via\nfeature-level similarity and performs inference conditioned on the structured\nfactors and retrieved reasoning paths, enabling the model to follow\nuser-specific reasoning trajectories. This reasoning-level personalization\nenhances both predictive accuracy and interpretability by grounding model\noutputs in user-specific logic through structured information. Extensive\nexperiments across diverse tasks show that RPM consistently outperforms\nresponse-level personalization methods, demonstrating the effectiveness of\nreasoning-level personalization in black-box LLMs.", "published": "2025-05-27 12:06:16", "link": "http://arxiv.org/abs/2505.21082v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation", "abstract": "Large Language Models (LLMs) enhanced with external knowledge retrieval, an\napproach known as Retrieval-Augmented Generation (RAG), have shown strong\nperformance in open-domain question answering. However, RAG systems remain\nsusceptible to hallucinations: factually incorrect outputs that may arise\neither from inconsistencies in the model's internal knowledge or incorrect use\nof the retrieved context. Existing approaches often conflate factuality with\nfaithfulness to the retrieved context, misclassifying factually correct\nstatements as hallucinations if they are not directly supported by the\nretrieval. In this paper, we introduce FRANQ (Faithfulness-based Retrieval\nAugmented UNcertainty Quantification), a novel method for hallucination\ndetection in RAG outputs. FRANQ applies different Uncertainty Quantification\n(UQ) techniques to estimate factuality based on whether a statement is faithful\nto the retrieved context or not. To evaluate FRANQ and other UQ techniques for\nRAG, we present a new long-form Question Answering (QA) dataset annotated for\nboth factuality and faithfulness, combining automated labeling with manual\nvalidation of challenging examples. Extensive experiments on long- and\nshort-form QA across multiple datasets and LLMs show that FRANQ achieves more\naccurate detection of factual errors in RAG-generated responses compared to\nexisting methods.", "published": "2025-05-27 11:56:59", "link": "http://arxiv.org/abs/2505.21072v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Implicit Arguments in Procedural Video Instructions", "abstract": "Procedural texts help AI enhance reasoning about context and action\nsequences. Transforming these into Semantic Role Labeling (SRL) improves\nunderstanding of individual steps by identifying predicate-argument structure\nlike {verb,what,where/with}. Procedural instructions are highly elliptic, for\ninstance, (i) add cucumber to the bowl and (ii) add sliced tomatoes, the second\nstep's where argument is inferred from the context, referring to where the\ncucumber was placed. Prior SRL benchmarks often miss implicit arguments,\nleading to incomplete understanding. To address this, we introduce\nImplicit-VidSRL, a dataset that necessitates inferring implicit and explicit\narguments from contextual information in multimodal cooking procedures. Our\nproposed dataset benchmarks multimodal models' contextual reasoning, requiring\nentity tracking through visual changes in recipes. We study recent multimodal\nLLMs and reveal that they struggle to predict implicit arguments of what and\nwhere/with from multi-modal procedural data given the verb. Lastly, we propose\niSRL-Qwen2-VL, which achieves a 17% relative improvement in F1-score for\nwhat-implicit and a 14.7% for where/with-implicit semantic roles over GPT-4o.", "published": "2025-05-27 11:53:06", "link": "http://arxiv.org/abs/2505.21068v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction", "abstract": "Turn-taking is richly multimodal. Predictive turn-taking models (PTTMs)\nfacilitate naturalistic human-robot interaction, yet most rely solely on\nspeech. We introduce MM-VAP, a multimodal PTTM which combines speech with\nvisual cues including facial expression, head pose and gaze. We find that it\noutperforms the state-of-the-art audio-only in videoconferencing interactions\n(84% vs. 79% hold/shift prediction accuracy). Unlike prior work which\naggregates all holds and shifts, we group by duration of silence between turns.\nThis reveals that through the inclusion of visual features, MM-VAP outperforms\na state-of-the-art audio-only turn-taking model across all durations of speaker\ntransitions. We conduct a detailed ablation study, which reveals that facial\nexpression features contribute the most to model performance. Thus, our working\nhypothesis is that when interlocutors can see one another, visual cues are\nvital for turn-taking and must therefore be included for accurate turn-taking\nprediction. We additionally validate the suitability of automatic speech\nalignment for PTTM training using telephone speech. This work represents the\nfirst comprehensive analysis of multimodal PTTMs. We discuss implications for\nfuture work and make all code publicly available.", "published": "2025-05-27 11:24:38", "link": "http://arxiv.org/abs/2505.21043v1", "categories": ["cs.CL", "cs.RO"], "primary_category": "cs.CL"}
{"title": "FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis", "abstract": "In this paper, we address the task of targeted sentiment analysis (TSA),\nwhich involves two sub-tasks, i.e., identifying specific aspects from reviews\nand determining their corresponding sentiments. Aspect extraction forms the\nfoundation for sentiment prediction, highlighting the critical dependency\nbetween these two tasks for effective cross-task knowledge transfer. While most\nexisting studies adopt a multi-task learning paradigm to align task-specific\nfeatures in the latent space, they predominantly rely on coarse-grained\nknowledge transfer. Such approaches lack fine-grained control over\naspect-sentiment relationships, often assuming uniform sentiment polarity\nwithin related aspects. This oversimplification neglects contextual cues that\ndifferentiate sentiments, leading to negative transfer. To overcome these\nlimitations, we propose FCKT, a fine-grained cross-task knowledge transfer\nframework tailored for TSA. By explicitly incorporating aspect-level\ninformation into sentiment prediction, FCKT achieves fine-grained knowledge\ntransfer, effectively mitigating negative transfer and enhancing task\nperformance. Experiments on three datasets, including comparisons with various\nbaselines and large language models (LLMs), demonstrate the effectiveness of\nFCKT. The source code is available on https://github.com/cwei01/FCKT.", "published": "2025-05-27 11:23:53", "link": "http://arxiv.org/abs/2505.21040v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation", "abstract": "Dialogue Topic Segmentation (DTS) aims to divide dialogues into coherent\nsegments. DTS plays a crucial role in various NLP downstream tasks, but suffers\nfrom chronic problems: data shortage, labeling ambiguity, and incremental\ncomplexity of recently proposed solutions. On the other hand, Despite advances\nin Large Language Models (LLMs) and reasoning strategies, these have rarely\nbeen applied to DTS. This paper introduces Def-DTS: Deductive Reasoning for\nOpen-domain Dialogue Topic Segmentation, which utilizes LLM-based multi-step\ndeductive reasoning to enhance DTS performance and enable case study using\nintermediate result. Our method employs a structured prompting approach for\nbidirectional context summarization, utterance intent classification, and\ndeductive topic shift detection. In the intent classification process, we\npropose the generalizable intent list for domain-agnostic dialogue intent\nclassification. Experiments in various dialogue settings demonstrate that\nDef-DTS consistently outperforms traditional and state-of-the-art approaches,\nwith each subtask contributing to improved performance, particularly in\nreducing type 2 error. We also explore the potential for autolabeling,\nemphasizing the importance of LLM reasoning techniques in DTS.", "published": "2025-05-27 11:07:53", "link": "http://arxiv.org/abs/2505.21033v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers", "abstract": "Pause tokens, simple filler symbols such as \"...\", consistently improve\nTransformer performance on both language and mathematical tasks, yet their\ntheoretical effect remains unexplained. We provide the first formal separation\nresult, proving that adding pause tokens to constant-depth, logarithmic-width\nTransformers strictly increases their computational expressivity. With\nbounded-precision activations, Transformers without pause tokens compute only a\nstrict subset of $\\mathsf{AC}^0$ functions, while adding a polynomial number of\npause tokens allows them to express the entire class. For logarithmic-precision\nTransformers, we show that adding pause tokens achieves expressivity equivalent\nto $\\mathsf{TC}^0$, matching known upper bounds. Empirically, we demonstrate\nthat two-layer causally masked Transformers can learn parity when supplied with\npause tokens, a function that they appear unable to learn without them. Our\nresults provide a rigorous theoretical explanation for prior empirical\nfindings, clarify how pause tokens interact with width, depth, and numeric\nprecision, and position them as a distinct mechanism, complementary to\nchain-of-thought prompting, for enhancing Transformer reasoning.", "published": "2025-05-27 10:59:27", "link": "http://arxiv.org/abs/2505.21024v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLMs are Frequency Pattern Learners in Natural Language Inference", "abstract": "While fine-tuning LLMs on NLI corpora improves their inferential performance,\nthe underlying mechanisms driving this improvement remain largely opaque. In\nthis work, we conduct a series of experiments to investigate what LLMs actually\nlearn during fine-tuning. We begin by analyzing predicate frequencies in\npremises and hypotheses across NLI datasets and identify a consistent frequency\nbias, where predicates in hypotheses occur more frequently than those in\npremises for positive instances. To assess the impact of this bias, we evaluate\nboth standard and NLI fine-tuned LLMs on bias-consistent and bias-adversarial\ncases. We find that LLMs exploit frequency bias for inference and perform\npoorly on adversarial instances. Furthermore, fine-tuned LLMs exhibit\nsignificantly increased reliance on this bias, suggesting that they are\nlearning these frequency patterns from datasets. Finally, we compute the\nfrequencies of hyponyms and their corresponding hypernyms from WordNet,\nrevealing a correlation between frequency bias and textual entailment. These\nfindings help explain why learning frequency patterns can enhance model\nperformance on inference tasks.", "published": "2025-05-27 10:45:29", "link": "http://arxiv.org/abs/2505.21011v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncertainty Unveiled: Can Exposure to More In-context Examples Mitigate Uncertainty for Large Language Models?", "abstract": "Recent advances in handling long sequences have facilitated the exploration\nof long-context in-context learning (ICL). While much of the existing research\nemphasizes performance improvements driven by additional in-context examples,\nthe influence on the trustworthiness of generated responses remains\nunderexplored. This paper addresses this gap by investigating how increased\nexamples influence predictive uncertainty, an essential aspect in\ntrustworthiness. We begin by systematically quantifying the uncertainty of ICL\nwith varying shot counts, analyzing the impact of example quantity. Through\nuncertainty decomposition, we introduce a novel perspective on performance\nenhancement, with a focus on epistemic uncertainty (EU). Our results reveal\nthat additional examples reduce total uncertainty in both simple and complex\ntasks by injecting task-specific knowledge, thereby diminishing EU and\nenhancing performance. For complex tasks, these advantages emerge only after\naddressing the increased noise and uncertainty associated with longer inputs.\nFinally, we explore the evolution of internal confidence across layers,\nunveiling the mechanisms driving the reduction in uncertainty.", "published": "2025-05-27 10:36:39", "link": "http://arxiv.org/abs/2505.21003v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Articulatory strategy in vowel production as a basis for speaker discrimination", "abstract": "The way speakers articulate is well known to be variable across individuals\nwhile at the same time subject to anatomical and biomechanical constraints. In\nthis study, we ask whether articulatory strategy in vowel production can be\nsufficiently speaker-specific to form the basis for speaker discrimination. We\nconducted Generalised Procrustes Analyses of tongue shape data from 40 English\nspeakers from the North West of England, and assessed the\nspeaker-discriminatory potential of orthogonal tongue shape features within the\nframework of likelihood ratios. Tongue size emerged as the individual dimension\nwith the strongest discriminatory power, while tongue shape variation in the\nmore anterior part of the tongue generally outperformed tongue shape variation\nin the posterior part. When considered in combination, shape-only information\nmay offer comparable levels of speaker specificity to size-and-shape\ninformation, but only when features do not exhibit speaker-level co-variation.", "published": "2025-05-27 10:29:05", "link": "http://arxiv.org/abs/2505.20995v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Who Reasons in the Large Language Models?", "abstract": "Despite the impressive performance of large language models (LLMs), the\nprocess of endowing them with new capabilities--such as mathematical\nreasoning--remains largely empirical and opaque. A critical open question is\nwhether reasoning abilities stem from the entire model, specific modules, or\nare merely artifacts of overfitting. In this work, we hypothesize that the\nreasoning capabilities in well-trained LLMs are primarily attributed to the\noutput projection module (oproj) in the Transformer's multi-head self-attention\n(MHSA) mechanism. To support this hypothesis, we introduce Stethoscope for\nNetworks (SfN), a suite of diagnostic tools designed to probe and analyze the\ninternal behaviors of LLMs. Using SfN, we provide both circumstantial and\nempirical evidence suggesting that oproj plays a central role in enabling\nreasoning, whereas other modules contribute more to fluent dialogue. These\nfindings offer a new perspective on LLM interpretability and open avenues for\nmore targeted training strategies, potentially enabling more efficient and\nspecialized LLMs.", "published": "2025-05-27 10:26:47", "link": "http://arxiv.org/abs/2505.20993v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RefAV: Towards Planning-Centric Scenario Mining", "abstract": "Autonomous Vehicles (AVs) collect and pseudo-label terabytes of multi-modal\ndata localized to HD maps during normal fleet testing. However, identifying\ninteresting and safety-critical scenarios from uncurated driving logs remains a\nsignificant challenge. Traditional scenario mining techniques are error-prone\nand prohibitively time-consuming, often relying on hand-crafted structured\nqueries. In this work, we revisit spatio-temporal scenario mining through the\nlens of recent vision-language models (VLMs) to detect whether a described\nscenario occurs in a driving log and, if so, precisely localize it in both time\nand space. To address this problem, we introduce RefAV, a large-scale dataset\nof 10,000 diverse natural language queries that describe complex multi-agent\ninteractions relevant to motion planning derived from 1000 driving logs in the\nArgoverse 2 Sensor dataset. We evaluate several referential multi-object\ntrackers and present an empirical analysis of our baselines. Notably, we find\nthat naively repurposing off-the-shelf VLMs yields poor performance, suggesting\nthat scenario mining presents unique challenges. Our code and dataset are\navailable at https://github.com/CainanD/RefAV/ and\nhttps://argoverse.github.io/user-guide/tasks/scenario_mining.html", "published": "2025-05-27 10:14:35", "link": "http://arxiv.org/abs/2505.20981v1", "categories": ["cs.CV", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Evaluating and Steering Modality Preferences in Multimodal Large Language Model", "abstract": "Multimodal large language models (MLLMs) have achieved remarkable performance\non complex tasks with multimodal context. However, it is still understudied\nwhether they exhibit modality preference when processing multimodal contexts.\nTo study this question, we first build a \\textbf{MC\\textsuperscript{2}}\nbenchmark under controlled evidence conflict scenarios to systematically\nevaluate modality preference, which is the tendency to favor one modality over\nanother when making decisions based on multimodal conflicting evidence. Our\nextensive evaluation reveals that all 18 tested MLLMs generally demonstrate\nclear modality bias, and modality preference can be influenced by external\ninterventions. An in-depth analysis reveals that the preference direction can\nbe captured within the latent representations of MLLMs. Built on this, we\npropose a probing and steering method based on representation engineering to\nexplicitly control modality preference without additional fine-tuning or\ncarefully crafted prompts. Our method effectively amplifies modality preference\ntoward a desired direction and applies to downstream tasks such as\nhallucination mitigation and multimodal machine translation, yielding promising\nimprovements.", "published": "2025-05-27 10:07:59", "link": "http://arxiv.org/abs/2505.20977v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency Parsing", "abstract": "Cross-domain constituency parsing is still an unsolved challenge in\ncomputational linguistics since the available multi-domain constituency\ntreebank is limited. We investigate automatic treebank generation by large\nlanguage models (LLMs) in this paper. The performance of LLMs on constituency\nparsing is poor, therefore we propose a novel treebank generation method, LLM\nback generation, which is similar to the reverse process of constituency\nparsing. LLM back generation takes the incomplete cross-domain constituency\ntree with only domain keyword leaf nodes as input and fills the missing words\nto generate the cross-domain constituency treebank. Besides, we also introduce\na span-level contrastive learning pre-training strategy to make full use of the\nLLM back generation treebank for cross-domain constituency parsing. We verify\nthe effectiveness of our LLM back generation treebank coupled with contrastive\nlearning pre-training on five target domains of MCTB. Experimental results show\nthat our approach achieves state-of-the-art performance on average results\ncompared with various baselines.", "published": "2025-05-27 10:07:54", "link": "http://arxiv.org/abs/2505.20976v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for KGQA", "abstract": "LLMs have demonstrated remarkable capabilities in complex reasoning tasks,\nyet they often suffer from hallucinations and lack reliable factual grounding.\nMeanwhile, knowledge graphs (KGs) provide structured factual knowledge but lack\nthe flexible reasoning abilities of LLMs. In this paper, we present\nReason-Align-Respond (RAR), a novel framework that systematically integrates\nLLM reasoning with knowledge graphs for KGQA. Our approach consists of three\nkey components: a Reasoner that generates human-like reasoning chains, an\nAligner that maps these chains to valid KG paths, and a Responser that\nsynthesizes the final answer. We formulate this process as a probabilistic\nmodel and optimize it using the Expectation-Maximization algorithm, which\niteratively refines the reasoning chains and knowledge paths. Extensive\nexperiments on multiple benchmarks demonstrate the effectiveness of RAR,\nachieving state-of-the-art performance with Hit@1 scores of 93.3% and 91.0% on\nWebQSP and CWQ respectively. Human evaluation confirms that RAR generates\nhigh-quality, interpretable reasoning chains well-aligned with KG paths.\nFurthermore, RAR exhibits strong zero-shot generalization capabilities and\nmaintains computational efficiency during inference.", "published": "2025-05-27 10:04:53", "link": "http://arxiv.org/abs/2505.20971v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Personalized Query Auto-Completion for Long and Short-Term Interests with Adaptive Detoxification Generation", "abstract": "Query auto-completion (QAC) plays a crucial role in modern search systems.\nHowever, in real-world applications, there are two pressing challenges that\nstill need to be addressed. First, there is a need for hierarchical\npersonalized representations for users. Previous approaches have typically used\nusers' search behavior as a single, overall representation, which proves\ninadequate in more nuanced generative scenarios. Additionally, query prefixes\nare typically short and may contain typos or sensitive information, increasing\nthe likelihood of generating toxic content compared to traditional text\ngeneration tasks. Such toxic content can degrade user experience and lead to\npublic relations issues. Therefore, the second critical challenge is\ndetoxifying QAC systems.\n  To address these two limitations, we propose a novel model (LaD) that\ncaptures personalized information from both long-term and short-term interests,\nincorporating adaptive detoxification. In LaD, personalized information is\ncaptured hierarchically at both coarse-grained and fine-grained levels. This\napproach preserves as much personalized information as possible while enabling\nonline generation within time constraints. To move a futher step, we propose an\nonline training method based on Reject Preference Optimization (RPO). By\nincorporating a special token [Reject] during both the training and inference\nprocesses, the model achieves adaptive detoxification. Consequently, the\ngenerated text presented to users is both non-toxic and relevant to the given\nprefix. We conduct comprehensive experiments on industrial-scale datasets and\nperform online A/B tests, delivering the largest single-experiment metric\nimprovement in nearly two years of our product. Our model has been deployed on\nKuaishou search, driving the primary traffic for hundreds of millions of active\nusers. The code is available at https://github.com/JXZe/LaD.", "published": "2025-05-27 09:58:42", "link": "http://arxiv.org/abs/2505.20966v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Context-Aware Content Moderation for German Newspaper Comments", "abstract": "The increasing volume of online discussions requires advanced automatic\ncontent moderation to maintain responsible discourse. While hate speech\ndetection on social media is well-studied, research on German-language\nnewspaper forums remains limited. Existing studies often neglect\nplatform-specific context, such as user history and article themes. This paper\naddresses this gap by developing and evaluating binary classification models\nfor automatic content moderation in German newspaper forums, incorporating\ncontextual information. Using LSTM, CNN, and ChatGPT-3.5 Turbo, and leveraging\nthe One Million Posts Corpus from the Austrian newspaper Der Standard, we\nassess the impact of context-aware models. Results show that CNN and LSTM\nmodels benefit from contextual information and perform competitively with\nstate-of-the-art approaches. In contrast, ChatGPT's zero-shot classification\ndoes not improve with added context and underperforms.", "published": "2025-05-27 09:57:02", "link": "http://arxiv.org/abs/2505.20963v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Research Community Perspectives on \"Intelligence\" and Large Language Models", "abstract": "Despite the widespread use of ''artificial intelligence'' (AI) framing in\nNatural Language Processing (NLP) research, it is not clear what researchers\nmean by ''intelligence''. To that end, we present the results of a survey on\nthe notion of ''intelligence'' among researchers and its role in the research\nagenda. The survey elicited complete responses from 303 researchers from a\nvariety of fields including NLP, Machine Learning (ML), Cognitive Science,\nLinguistics, and Neuroscience. We identify 3 criteria of intelligence that the\ncommunity agrees on the most: generalization, adaptability, & reasoning. Our\nresults suggests that the perception of the current NLP systems as\n''intelligent'' is a minority position (29%). Furthermore, only 16.2% of the\nrespondents see developing intelligent systems as a research goal, and these\nrespondents are more likely to consider the current systems intelligent.", "published": "2025-05-27 09:53:27", "link": "http://arxiv.org/abs/2505.20959v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "On VLMs for Diverse Tasks in Multimodal Meme Classification", "abstract": "In this paper, we present a comprehensive and systematic analysis of\nvision-language models (VLMs) for disparate meme classification tasks. We\nintroduced a novel approach that generates a VLM-based understanding of meme\nimages and fine-tunes the LLMs on textual understanding of the embedded meme\ntext for improving the performance. Our contributions are threefold: (1)\nBenchmarking VLMs with diverse prompting strategies purposely to each sub-task;\n(2) Evaluating LoRA fine-tuning across all VLM components to assess performance\ngains; and (3) Proposing a novel approach where detailed meme interpretations\ngenerated by VLMs are used to train smaller language models (LLMs),\nsignificantly improving classification. The strategy of combining VLMs with\nLLMs improved the baseline performance by 8.34%, 3.52% and 26.24% for sarcasm,\noffensive and sentiment classification, respectively. Our results reveal the\nstrengths and limitations of VLMs and present a novel strategy for meme\nunderstanding.", "published": "2025-05-27 09:25:46", "link": "http://arxiv.org/abs/2505.20937v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Information-Theoretic Complementary Prompts for Improved Continual Text Classification", "abstract": "Continual Text Classification (CTC) aims to continuously classify new text\ndata over time while minimizing catastrophic forgetting of previously acquired\nknowledge. However, existing methods often focus on task-specific knowledge,\noverlooking the importance of shared, task-agnostic knowledge. Inspired by the\ncomplementary learning systems theory, which posits that humans learn\ncontinually through the interaction of two systems -- the hippocampus,\nresponsible for forming distinct representations of specific experiences, and\nthe neocortex, which extracts more general and transferable representations\nfrom past experiences -- we introduce Information-Theoretic Complementary\nPrompts (InfoComp), a novel approach for CTC. InfoComp explicitly learns two\ndistinct prompt spaces: P(rivate)-Prompt and S(hared)-Prompt. These\nrespectively encode task-specific and task-invariant knowledge, enabling models\nto sequentially learn classification tasks without relying on data replay. To\npromote more informative prompt learning, InfoComp uses an\ninformation-theoretic framework that maximizes mutual information between\ndifferent parameters (or encoded representations). Within this framework, we\ndesign two novel loss functions: (1) to strengthen the accumulation of\ntask-specific knowledge in P-Prompt, effectively mitigating catastrophic\nforgetting, and (2) to enhance the retention of task-invariant knowledge in\nS-Prompt, improving forward knowledge transfer. Extensive experiments on\ndiverse CTC benchmarks show that our approach outperforms previous\nstate-of-the-art methods.", "published": "2025-05-27 09:22:14", "link": "http://arxiv.org/abs/2505.20933v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-objective Large Language Model Alignment with Hierarchical Experts", "abstract": "Aligning large language models (LLMs) to simultaneously satisfy multiple\nobjectives remains a significant challenge, especially given the diverse and\noften conflicting nature of human preferences. Existing alignment methods\nstruggle to balance trade-offs effectively, often requiring costly retraining\nor yielding suboptimal results across the Pareto frontier of preferences. In\nthis paper, we introduce \\textit{HoE}(Hierarchical Mixture-of-Experts), a\n\\textit{lightweight}, \\textit{parameter-efficient}, and \\textit{plug-and-play}\napproach that eliminates the need for model training, while enabling LLMs to\nadapt across the entire Pareto frontier and accommodate diverse user\npreferences. In particular, \\textit{HoE} consists of three hierarchical\ncomponents: LoRA Experts, Router Experts and Preference Routing, reaching\noptimal Pareto frontiers and achieving a trade-off between parameter size,\ntraining cost, and performance. We evaluate \\textit{HoE} across various tasks\non 14 objectives and 200 different preferences among 6 benchmarks,\ndemonstrating superior performance over 15 recent baselines. Code is available\nin the supplementary materials.", "published": "2025-05-27 09:15:03", "link": "http://arxiv.org/abs/2505.20925v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models", "abstract": "LLM providers typically offer multiple LLM tiers, varying in performance and\nprice. As NLP tasks become more complex and modularized, selecting the suitable\nLLM tier for each subtask is a key challenge to balance between cost and\nperformance. To address the problem, we introduce LLM Automatic Transmission\n(LLM-AT) framework that automatically selects LLM tiers without training.\nLLM-AT consists of Starter, Generator, and Judge. The starter selects the\ninitial LLM tier expected to solve the given question, the generator produces a\nresponse using the LLM of the selected tier, and the judge evaluates the\nvalidity of the response. If the response is invalid, LLM-AT iteratively\nupgrades to a higher-tier model, generates a new response, and re-evaluates\nuntil a valid response is obtained. Additionally, we propose accuracy\nestimator, which enables the suitable initial LLM tier selection without\ntraining. Given an input question, accuracy estimator estimates the expected\naccuracy of each LLM tier by computing the valid response rate across top-k\nsimilar queries from past inference records. Experiments demonstrate that\nLLM-AT achieves superior performance while reducing costs, making it a\npractical solution for real-world applications.", "published": "2025-05-27 09:11:00", "link": "http://arxiv.org/abs/2505.20921v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automated Privacy Information Annotation in Large Language Model Interactions", "abstract": "Users interacting with large language models (LLMs) under their real\nidentifiers often unknowingly risk disclosing private information.\nAutomatically notifying users whether their queries leak privacy and which\nphrases leak what private information has therefore become a practical need.\nExisting privacy detection methods, however, were designed for different\nobjectives and application scenarios, typically tagging personally identifiable\ninformation (PII) in anonymous content. In this work, to support the\ndevelopment and evaluation of privacy detection models for LLM interactions\nthat are deployable on local user devices, we construct a large-scale\nmultilingual dataset with 249K user queries and 154K annotated privacy phrases.\nIn particular, we build an automated privacy annotation pipeline with\ncloud-based strong LLMs to automatically extract privacy phrases from dialogue\ndatasets and annotate leaked information. We also design evaluation metrics at\nthe levels of privacy leakage, extracted privacy phrase, and privacy\ninformation. We further establish baseline methods using light-weight LLMs with\nboth tuning-free and tuning-based methods, and report a comprehensive\nevaluation of their performance. Evaluation results reveal a gap between\ncurrent performance and the requirements of real-world LLM applications,\nmotivating future research into more effective local privacy detection methods\ngrounded in our dataset.", "published": "2025-05-27 09:00:12", "link": "http://arxiv.org/abs/2505.20910v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Objective Fine-tuning: How LLMs' Prior Knowledge Causes Potential Poor Calibration?", "abstract": "Fine-tuned Large Language Models (LLMs) often demonstrate poor calibration,\nwith their confidence scores misaligned with actual performance. While\ncalibration has been extensively studied in models trained from scratch, the\nimpact of LLMs' prior knowledge on calibration during fine-tuning remains\nunderstudied. Our research reveals that LLMs' prior knowledge causes potential\npoor calibration due to the ubiquitous presence of known data in real-world\nfine-tuning, which appears harmful for calibration. Specifically, data aligned\nwith LLMs' prior knowledge would induce overconfidence, while new knowledge\nimproves calibration. Our findings expose a tension: LLMs' encyclopedic\nknowledge, while enabling task versatility, undermines calibration through\nunavoidable knowledge overlaps. To address this, we propose CogCalib, a\ncognition-aware framework that applies targeted learning strategies according\nto the model's prior knowledge. Experiments across 7 tasks using 3 LLM families\nprove that CogCalib significantly improves calibration while maintaining\nperformance, achieving an average 57\\% reduction in ECE compared to standard\nfine-tuning in Llama3-8B. These improvements generalize well to out-of-domain\ntasks, enhancing the objectivity and reliability of domain-specific LLMs, and\nmaking them more trustworthy for critical human-AI interaction applications.", "published": "2025-05-27 08:51:31", "link": "http://arxiv.org/abs/2505.20903v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models", "abstract": "As large vision language models(LVLMs) rapidly advance, concerns about their\npotential to learn and generate social biases and stereotypes are increasing.\nPrevious studies on LVLM's stereotypes face two primary limitations: metrics\nthat overlooked the importance of content words, and datasets that overlooked\nthe effect of color. To address these limitations, this study introduces new\nevaluation metrics based on the Stereotype Content Model (SCM). We also propose\nBASIC, a benchmark for assessing gender, race, and color stereotypes. Using SCM\nmetrics and BASIC, we conduct a study with eight LVLMs to discover stereotypes.\nAs a result, we found three findings. (1) The SCM-based evaluation is effective\nin capturing stereotypes. (2) LVLMs exhibit color stereotypes in the output\nalong with gender and race ones. (3) Interaction between model architecture and\nparameter sizes seems to affect stereotypes. We release BASIC publicly on\n[anonymized for review].", "published": "2025-05-27 08:44:05", "link": "http://arxiv.org/abs/2505.20901v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dub-S2ST: Textless Speech-to-Speech Translation for Seamless Dubbing", "abstract": "This paper introduces a cross-lingual dubbing system that translates speech\nfrom one language to another while preserving key characteristics such as\nduration, speaker identity, and speaking speed. Despite the strong translation\nquality of existing speech translation approaches, they often overlook the\ntransfer of speech patterns, leading to mismatches with source speech and\nlimiting their suitability for dubbing applications. To address this, we\npropose a discrete diffusion-based speech-to-unit translation model with\nexplicit duration control, enabling time-aligned translation. We then\nsynthesize speech based on the predicted units and source identity with a\nconditional flow matching model. Additionally, we introduce a unit-based speed\nadaptation mechanism that guides the translation model to produce speech at a\nrate consistent with the source, without relying on any text. Extensive\nexperiments demonstrate that our framework generates natural and fluent\ntranslations that align with the original speech's duration and speaking pace,\nwhile achieving competitive translation performance.", "published": "2025-05-27 08:43:28", "link": "http://arxiv.org/abs/2505.20899v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation", "abstract": "Vision-and-Language Navigation (VLN) requires the agent to navigate by\nfollowing natural instructions under partial observability, making it difficult\nto align perception with language. Recent methods mitigate this by imagining\nfuture scenes, yet they rely on vision-based synthesis, leading to high\ncomputational cost and redundant details. To this end, we propose to adaptively\nimagine key environmental semantics via \\textit{language} form, enabling a more\nreliable and efficient strategy. Specifically, we introduce a novel Adaptive\nText Dreamer (ATD), a dual-branch self-guided imagination policy built upon a\nlarge language model (LLM). ATD is designed with a human-like left-right brain\narchitecture, where the left brain focuses on logical integration, and the\nright brain is responsible for imaginative prediction of future scenes. To\nachieve this, we fine-tune only the Q-former within both brains to efficiently\nactivate domain-specific knowledge in the LLM, enabling dynamic updates of\nlogical reasoning and imagination during navigation. Furthermore, we introduce\na cross-interaction mechanism to regularize the imagined outputs and inject\nthem into a navigation expert module, allowing ATD to jointly exploit both the\nreasoning capacity of the LLM and the expertise of the navigation model. We\nconduct extensive experiments on the R2R benchmark, where ATD achieves\nstate-of-the-art performance with fewer parameters. The code is\n\\href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}.", "published": "2025-05-27 08:40:20", "link": "http://arxiv.org/abs/2505.20897v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "How Do Transformers Learn Variable Binding in Symbolic Programs?", "abstract": "Variable binding -- the ability to associate variables with values -- is\nfundamental to symbolic computation and cognition. Although classical\narchitectures typically implement variable binding via addressable memory, it\nis not well understood how modern neural networks lacking built-in binding\noperations may acquire this capacity. We investigate this by training a\nTransformer to dereference queried variables in symbolic programs where\nvariables are assigned either numerical constants or other variables. Each\nprogram requires following chains of variable assignments up to four steps deep\nto find the queried value, and also contains irrelevant chains of assignments\nacting as distractors. Our analysis reveals a developmental trajectory with\nthree distinct phases during training: (1) random prediction of numerical\nconstants, (2) a shallow heuristic prioritizing early variable assignments, and\n(3) the emergence of a systematic mechanism for dereferencing assignment\nchains. Using causal interventions, we find that the model learns to exploit\nthe residual stream as an addressable memory space, with specialized attention\nheads routing information across token positions. This mechanism allows the\nmodel to dynamically track variable bindings across layers, resulting in\naccurate dereferencing. Our results show how Transformer models can learn to\nimplement systematic variable binding without explicit architectural support,\nbridging connectionist and symbolic approaches.", "published": "2025-05-27 08:39:20", "link": "http://arxiv.org/abs/2505.20896v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models", "abstract": "In this paper, we present EasyDistill, a comprehensive toolkit designed for\neffective black-box and white-box knowledge distillation (KD) of large language\nmodels (LLMs). Our framework offers versatile functionalities, including data\nsynthesis, supervised fine-tuning, ranking optimization, and reinforcement\nlearning techniques specifically tailored for KD scenarios. The toolkit\naccommodates KD functionalities for both System 1 (fast, intuitive) and System\n2 (slow, analytical) models. With its modular design and user-friendly\ninterface, EasyDistill empowers researchers and industry practitioners to\nseamlessly experiment with and implement state-of-the-art KD strategies for\nLLMs. In addition, EasyDistill provides a series of robust distilled models and\nKD-based industrial solutions developed by us, along with the corresponding\nopen-sourced datasets, catering to a variety of use cases. Furthermore, we\ndescribe the seamless integration of EasyDistill into Alibaba Cloud's Platform\nfor AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for\nLLMs more accessible and impactful within the NLP community.", "published": "2025-05-27 08:32:51", "link": "http://arxiv.org/abs/2505.20888v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MSA at SemEval-2025 Task 3: High Quality Weak Labeling and LLM Ensemble Verification for Multilingual Hallucination Detection", "abstract": "This paper describes our submission for SemEval-2025 Task 3: Mu-SHROOM, the\nMultilingual Shared-task on Hallucinations and Related Observable\nOvergeneration Mistakes. The task involves detecting hallucinated spans in text\ngenerated by instruction-tuned Large Language Models (LLMs) across multiple\nlanguages. Our approach combines task-specific prompt engineering with an LLM\nensemble verification mechanism, where a primary model extracts hallucination\nspans and three independent LLMs adjudicate their validity through\nprobability-based voting. This framework simulates the human annotation\nworkflow used in the shared task validation and test data. Additionally, fuzzy\nmatching refines span alignment. Our system ranked 1st in Arabic and Basque,\n2nd in German, Swedish, and Finnish, and 3rd in Czech, Farsi, and French.", "published": "2025-05-27 08:26:17", "link": "http://arxiv.org/abs/2505.20880v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties", "abstract": "Large Language Models (LLMs) are predominantly evaluated on Standard American\nEnglish (SAE), often overlooking the diversity of global English varieties.\nThis narrow focus may raise fairness concerns as degraded performance on\nnon-standard varieties can lead to unequal benefits for users worldwide.\nTherefore, it is critical to extensively evaluate the linguistic robustness of\nLLMs on multiple non-standard English varieties. We introduce Trans-EnV, a\nframework that automatically transforms SAE datasets into multiple English\nvarieties to evaluate the linguistic robustness. Our framework combines (1)\nlinguistics expert knowledge to curate variety-specific features and\ntransformation guidelines from linguistic literature and corpora, and (2)\nLLM-based transformations to ensure both linguistic validity and scalability.\nUsing Trans-EnV, we transform six benchmark datasets into 38 English varieties\nand evaluate seven state-of-the-art LLMs. Our results reveal significant\nperformance disparities, with accuracy decreasing by up to 46.3% on\nnon-standard varieties. These findings highlight the importance of\ncomprehensive linguistic robustness evaluation across diverse English\nvarieties. Each construction of Trans-EnV was validated through rigorous\nstatistical testing and consultation with a researcher in the field of second\nlanguage acquisition, ensuring its linguistic validity. Our\n\\href{https://github.com/jiyounglee-0523/TransEnV}{code} and\n\\href{https://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1}{datasets}\nare publicly available.", "published": "2025-05-27 08:23:27", "link": "http://arxiv.org/abs/2505.20875v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can LLMs Learn to Map the World from Local Descriptions?", "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated strong\ncapabilities in tasks such as code and mathematics. However, their potential to\ninternalize structured spatial knowledge remains underexplored. This study\ninvestigates whether LLMs, grounded in locally relative human observations, can\nconstruct coherent global spatial cognition by integrating fragmented\nrelational descriptions. We focus on two core aspects of spatial cognition:\nspatial perception, where models infer consistent global layouts from local\npositional relationships, and spatial navigation, where models learn road\nconnectivity from trajectory data and plan optimal paths between unconnected\nlocations. Experiments conducted in a simulated urban environment demonstrate\nthat LLMs not only generalize to unseen spatial relationships between points of\ninterest (POIs) but also exhibit latent representations aligned with real-world\nspatial distributions. Furthermore, LLMs can learn road connectivity from\ntrajectory descriptions, enabling accurate path planning and dynamic spatial\nawareness during navigation.", "published": "2025-05-27 08:22:58", "link": "http://arxiv.org/abs/2505.20874v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Divide-Then-Align: Honest Alignment based on the Knowledge Boundary of RAG", "abstract": "Large language models (LLMs) augmented with retrieval systems have\nsignificantly advanced natural language processing tasks by integrating\nexternal knowledge sources, enabling more accurate and contextually rich\nresponses. To improve the robustness of such systems against noisy retrievals,\nRetrieval-Augmented Fine-Tuning (RAFT) has emerged as a widely adopted method.\nHowever, RAFT conditions models to generate answers even in the absence of\nreliable knowledge. This behavior undermines their reliability in high-stakes\ndomains, where acknowledging uncertainty is critical. To address this issue, we\npropose Divide-Then-Align (DTA), a post-training approach designed to endow RAG\nsystems with the ability to respond with \"I don't know\" when the query is out\nof the knowledge boundary of both the retrieved passages and the model's\ninternal knowledge. DTA divides data samples into four knowledge quadrants and\nconstructs tailored preference data for each quadrant, resulting in a curated\ndataset for Direct Preference Optimization (DPO). Experimental results on three\nbenchmark datasets demonstrate that DTA effectively balances accuracy with\nappropriate abstention, enhancing the reliability and trustworthiness of\nretrieval-augmented systems.", "published": "2025-05-27 08:21:21", "link": "http://arxiv.org/abs/2505.20871v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks", "abstract": "Large Language Models (LLMs) and other automated techniques have been\nincreasingly used to support software developers by generating software\nartifacts such as code snippets, patches, and comments. However, accurately\nassessing the correctness of these generated artifacts remains a significant\nchallenge. On one hand, human evaluation provides high accuracy but is\nlabor-intensive and lacks scalability. On the other hand, other existing\nautomatic evaluation metrics are scalable and require minimal human effort, but\nthey often fail to accurately reflect the actual correctness of generated\nsoftware artifacts.\n  In this paper, we present SWE-Judge, the first evaluation metric for\nLLM-as-Ensemble-Judge specifically designed to accurately assess the\ncorrectness of generated software artifacts. SWE-Judge first defines five\ndistinct evaluation strategies, each implemented as an independent judge. A\ndynamic team selection mechanism then identifies the most appropriate subset of\njudges to produce a final correctness score through ensembling. We evaluate\nSWE-Judge across a diverse set of software engineering (SE) benchmarks,\nincluding CoNaLa, Card2Code, HumanEval-X, APPS, APR-Assess, and Summary-Assess.\nThese benchmarks span three SE tasks: code generation, automated program\nrepair, and code summarization. Experimental results demonstrate that SWE-Judge\nconsistently achieves a higher correlation with human judgments, with\nimprovements ranging from 5.9% to 183.8% over existing automatic metrics.\nFurthermore, SWE-Judge reaches agreement levels with human annotators that are\ncomparable to inter-annotator agreement in code generation and program repair\ntasks. These findings underscore SWE-Judge's potential as a scalable and\nreliable alternative to human evaluation.", "published": "2025-05-27 08:04:34", "link": "http://arxiv.org/abs/2505.20854v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Concealment of Intent: A Game-Theoretic Analysis", "abstract": "As large language models (LLMs) grow more capable, concerns about their safe\ndeployment have also grown. Although alignment mechanisms have been introduced\nto deter misuse, they remain vulnerable to carefully designed adversarial\nprompts. In this work, we present a scalable attack strategy: intent-hiding\nadversarial prompting, which conceals malicious intent through the composition\nof skills. We develop a game-theoretic framework to model the interaction\nbetween such attacks and defense systems that apply both prompt and response\nfiltering. Our analysis identifies equilibrium points and reveals structural\nadvantages for the attacker. To counter these threats, we propose and analyze a\ndefense mechanism tailored to intent-hiding attacks. Empirically, we validate\nthe attack's effectiveness on multiple real-world LLMs across a range of\nmalicious behaviors, demonstrating clear advantages over existing adversarial\nprompting techniques.", "published": "2025-05-27 07:59:56", "link": "http://arxiv.org/abs/2505.20841v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AdParaphrase v2.0: Generating Attractive Ad Texts Using a Preference-Annotated Paraphrase Dataset", "abstract": "Identifying factors that make ad text attractive is essential for advertising\nsuccess. This study proposes AdParaphrase v2.0, a dataset for ad text\nparaphrasing, containing human preference data, to enable the analysis of the\nlinguistic factors and to support the development of methods for generating\nattractive ad texts. Compared with v1.0, this dataset is 20 times larger,\ncomprising 16,460 ad text paraphrase pairs, each annotated with preference data\nfrom ten evaluators, thereby enabling a more comprehensive and reliable\nanalysis. Through the experiments, we identified multiple linguistic features\nof engaging ad texts that were not observed in v1.0 and explored various\nmethods for generating attractive ad texts. Furthermore, our analysis\ndemonstrated the relationships between human preference and ad performance, and\nhighlighted the potential of reference-free metrics based on large language\nmodels for evaluating ad text attractiveness. The dataset is publicly available\nat: https://github.com/CyberAgentAILab/AdParaphrase-v2.0.", "published": "2025-05-27 07:34:44", "link": "http://arxiv.org/abs/2505.20826v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reinforced Informativeness Optimization for Long-Form Retrieval-Augmented Generation", "abstract": "Long-form question answering (LFQA) presents unique challenges for large\nlanguage models, requiring the synthesis of coherent, paragraph-length answers.\nWhile retrieval-augmented generation (RAG) systems have emerged as a promising\nsolution, existing research struggles with key limitations: the scarcity of\nhigh-quality training data for long-form generation, the compounding risk of\nhallucination in extended outputs, and the absence of reliable evaluation\nmetrics for factual completeness. In this paper, we propose RioRAG, a novel\nreinforcement learning (RL) framework that advances long-form RAG through\nreinforced informativeness optimization. Our approach introduces two\nfundamental innovations to address the core challenges. First, we develop an RL\ntraining paradigm of reinforced informativeness optimization that directly\noptimizes informativeness and effectively addresses the slow-thinking deficit\nin conventional RAG systems, bypassing the need for expensive supervised data.\nSecond, we propose a nugget-centric hierarchical reward modeling approach that\nenables precise assessment of long-form answers through a three-stage process:\nextracting the nugget from every source webpage, constructing a nugget claim\nchecklist, and computing rewards based on factual alignment. Extensive\nexperiments on two LFQA benchmarks LongFact and RAGChecker demonstrate the\neffectiveness of the proposed method. Our codes are available at\nhttps://github.com/RUCAIBox/RioRAG.", "published": "2025-05-27 07:34:41", "link": "http://arxiv.org/abs/2505.20825v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tracing and Reversing Rank-One Model Edits", "abstract": "Knowledge editing methods (KEs) are a cost-effective way to update the\nfactual content of large language models (LLMs), but they pose a dual-use risk.\nWhile KEs are beneficial for updating outdated or incorrect information, they\ncan be exploited maliciously to implant misinformation or bias. In order to\ndefend against these types of malicious manipulation, we need robust techniques\nthat can reliably detect, interpret, and mitigate adversarial edits. This work\ninvestigates the traceability and reversibility of knowledge edits, focusing on\nthe widely used Rank-One Model Editing (ROME) method. We first show that ROME\nintroduces distinctive distributional patterns in the edited weight matrices,\nwhich can serve as effective signals for locating the edited weights. Second,\nwe show that these altered weights can reliably be used to predict the edited\nfactual relation, enabling partial reconstruction of the modified fact.\nBuilding on this, we propose a method to infer the edited object entity\ndirectly from the modified weights, without access to the editing prompt,\nachieving over 95% accuracy. Finally, we demonstrate that ROME edits can be\nreversed, recovering the model's original outputs with $\\geq$ 80% accuracy. Our\nfindings highlight the feasibility of detecting, tracing, and reversing edits\nbased on the edited weights, offering a robust framework for safeguarding LLMs\nagainst adversarial manipulations.", "published": "2025-05-27 07:27:01", "link": "http://arxiv.org/abs/2505.20819v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective", "abstract": "Recent advances in multimodal question answering have primarily focused on\ncombining heterogeneous modalities or fine-tuning multimodal large language\nmodels. While these approaches have shown strong performance, they often rely\non a single, generalized reasoning strategy, overlooking the unique\ncharacteristics of each modality ultimately limiting both accuracy and\ninterpretability. To address these limitations, we propose MAMMQA, a\nmulti-agent QA framework for multimodal inputs spanning text, tables, and\nimages. Our system includes two Visual Language Model (VLM) agents and one\ntext-based Large Language Model (LLM) agent. The first VLM decomposes the user\nquery into sub-questions and sequentially retrieves partial answers from each\nmodality. The second VLM synthesizes and refines these results through\ncross-modal reasoning. Finally, the LLM integrates the insights into a cohesive\nanswer. This modular design enhances interpretability by making the reasoning\nprocess transparent and allows each agent to operate within its domain of\nexpertise. Experiments on diverse multimodal QA benchmarks demonstrate that our\ncooperative, multi-agent framework consistently outperforms existing baselines\nin both accuracy and robustness.", "published": "2025-05-27 07:23:38", "link": "http://arxiv.org/abs/2505.20816v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph", "abstract": "In knowledge graph embedding, leveraging relation-specific\nentity-transformation has markedly enhanced performance. However, the\nconsistency of embedding differences before and after transformation remains\nunaddressed, risking the loss of valuable inductive bias inherent in the\nembeddings. This inconsistency stems from two problems. First, transformation\nrepresentations are specified for relations in a disconnected manner, allowing\ndissimilar transformations and corresponding entity-embeddings for similar\nrelations. Second, a generalized plug-in approach as a SFBR (Semantic Filter\nBased on Relations) disrupts this consistency through excessive concentration\nof entity embeddings under entity-based regularization, generating\nindistinguishable score distributions among relations. In this paper, we\nintroduce a plug-in KGE method, Relation-Semantics Consistent Filter (RSCF),\ncontaining more consistent entity-transformation characterized by three\nfeatures: 1) shared affine transformation of relation embeddings across all\nrelations, 2) rooted entity-transformation that adds an entity embedding to its\nchange represented by the transformed vector, and 3) normalization of the\nchange to prevent scale reduction. To amplify the advantages of consistency\nthat preserve semantics on embeddings, RSCF adds relation transformation and\nprediction modules for enhancing the semantics. In knowledge graph completion\ntasks with distance-based and tensor decomposition models, RSCF significantly\noutperforms state-of-the-art KGE methods, showing robustness across all\nrelations and their frequencies.", "published": "2025-05-27 07:22:00", "link": "http://arxiv.org/abs/2505.20813v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improved Representation Steering for Language Models", "abstract": "Steering methods for language models (LMs) seek to provide fine-grained and\ninterpretable control over model generations by variously changing model\ninputs, weights, or representations to adjust behavior. Recent work has shown\nthat adjusting weights or representations is often less effective than steering\nby prompting, for instance when wanting to introduce or suppress a particular\nconcept. We demonstrate how to improve representation steering via our new\nReference-free Preference Steering (RePS), a bidirectional\npreference-optimization objective that jointly does concept steering and\nsuppression. We train three parameterizations of RePS and evaluate them on\nAxBench, a large-scale model steering benchmark. On Gemma models with sizes\nranging from 2B to 27B, RePS outperforms all existing steering methods trained\nwith a language modeling objective and substantially narrows the gap with\nprompting -- while promoting interpretability and minimizing parameter count.\nIn suppression, RePS matches the language-modeling objective on Gemma-2 and\noutperforms it on the larger Gemma-3 variants while remaining resilient to\nprompt-based jailbreaking attacks that defeat prompting. Overall, our results\nsuggest that RePS provides an interpretable and robust alternative to prompting\nfor both steering and suppression.", "published": "2025-05-27 07:16:40", "link": "http://arxiv.org/abs/2505.20809v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery", "abstract": "Vision-Language Model (VLM) based Web Agents represent a significant step\ntowards automating complex tasks by simulating human-like interaction with\nwebsites. However, their deployment in uncontrolled web environments introduces\nsignificant security vulnerabilities. Existing research on adversarial\nenvironmental injection attacks often relies on unrealistic assumptions, such\nas direct HTML manipulation, knowledge of user intent, or access to agent model\nparameters, limiting their practical applicability. In this paper, we propose\nAdInject, a novel and real-world black-box attack method that leverages the\ninternet advertising delivery to inject malicious content into the Web Agent's\nenvironment. AdInject operates under a significantly more realistic threat\nmodel than prior work, assuming a black-box agent, static malicious content\nconstraints, and no specific knowledge of user intent. AdInject includes\nstrategies for designing malicious ad content aimed at misleading agents into\nclicking, and a VLM-based ad content optimization technique that infers\npotential user intents from the target website's context and integrates these\nintents into the ad content to make it appear more relevant or critical to the\nagent's task, thus enhancing attack effectiveness. Experimental evaluations\ndemonstrate the effectiveness of AdInject, attack success rates exceeding 60%\nin most scenarios and approaching 100% in certain cases. This strongly\ndemonstrates that prevalent advertising delivery constitutes a potent and\nreal-world vector for environment injection attacks against Web Agents. This\nwork highlights a critical vulnerability in Web Agent security arising from\nreal-world environment manipulation channels, underscoring the urgent need for\ndeveloping robust defense mechanisms against such threats. Our code is\navailable at https://github.com/NicerWang/AdInject.", "published": "2025-05-27 17:59:05", "link": "http://arxiv.org/abs/2505.21499v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "abstract": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "published": "2025-05-27 17:54:24", "link": "http://arxiv.org/abs/2505.21488v1", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming", "abstract": "Automating robust hypothesis generation in open environments is pivotal for\nAI cognition. We introduce a novel framework integrating a multi-agent system,\npowered by Large Language Models (LLMs), with Inductive Logic Programming\n(ILP). Our system's LLM agents autonomously define a structured symbolic\nvocabulary (predicates) and relational templates , i.e., \\emph{language bias}\ndirectly from raw textual data. This automated symbolic grounding (the\nconstruction of the language bias), traditionally an expert-driven bottleneck\nfor ILP, then guides the transformation of text into facts for an ILP solver,\nwhich inductively learns interpretable rules. This approach overcomes\ntraditional ILP's reliance on predefined symbolic structures and the\nnoise-sensitivity of pure LLM methods. Extensive experiments in diverse,\nchallenging scenarios validate superior performance, paving a new path for\nautomated, explainable, and verifiable hypothesis generation.", "published": "2025-05-27 17:53:38", "link": "http://arxiv.org/abs/2505.21486v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Policy Optimized Text-to-Image Pipeline Design", "abstract": "Text-to-image generation has evolved beyond single monolithic models to\ncomplex multi-component pipelines. These combine fine-tuned generators,\nadapters, upscaling blocks and even editing steps, leading to significant\nimprovements in image quality. However, their effective design requires\nsubstantial expertise. Recent approaches have shown promise in automating this\nprocess through large language models (LLMs), but they suffer from two critical\nlimitations: extensive computational requirements from generating images with\nhundreds of predefined pipelines, and poor generalization beyond memorized\ntraining examples. We introduce a novel reinforcement learning-based framework\nthat addresses these inefficiencies. Our approach first trains an ensemble of\nreward models capable of predicting image quality scores directly from\nprompt-workflow combinations, eliminating the need for costly image generation\nduring training. We then implement a two-phase training strategy: initial\nworkflow vocabulary training followed by GRPO-based optimization that guides\nthe model toward higher-performing regions of the workflow space. Additionally,\nwe incorporate a classifier-free guidance based enhancement technique that\nextrapolates along the path between the initial and GRPO-tuned models, further\nimproving output quality. We validate our approach through a set of\ncomparisons, showing that it can successfully create new flows with greater\ndiversity and lead to superior image quality compared to existing baselines.", "published": "2025-05-27 17:50:47", "link": "http://arxiv.org/abs/2505.21478v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "LazyVLM: Neuro-Symbolic Approach to Video Analytics", "abstract": "Current video analytics approaches face a fundamental trade-off between\nflexibility and efficiency. End-to-end Vision Language Models (VLMs) often\nstruggle with long-context processing and incur high computational costs, while\nneural-symbolic methods depend heavily on manual labeling and rigid rule\ndesign. In this paper, we introduce LazyVLM, a neuro-symbolic video analytics\nsystem that provides a user-friendly query interface similar to VLMs, while\naddressing their scalability limitation. LazyVLM enables users to effortlessly\ndrop in video data and specify complex multi-frame video queries using a\nsemi-structured text interface for video analytics. To address the scalability\nlimitations of VLMs, LazyVLM decomposes multi-frame video queries into\nfine-grained operations and offloads the bulk of the processing to efficient\nrelational query execution and vector similarity search. We demonstrate that\nLazyVLM provides a robust, efficient, and user-friendly solution for querying\nopen-domain video data at scale.", "published": "2025-05-27 17:31:17", "link": "http://arxiv.org/abs/2505.21459v1", "categories": ["cs.DB", "cs.AI", "cs.CV", "cs.IR", "cs.MM"], "primary_category": "cs.DB"}
{"title": "Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO", "abstract": "Active vision, also known as active perception, refers to the process of\nactively selecting where and how to look in order to gather task-relevant\ninformation. It is a critical component of efficient perception and\ndecision-making in humans and advanced embodied agents. Recently, the use of\nMultimodal Large Language Models (MLLMs) as central planning and\ndecision-making modules in robotic systems has gained extensive attention.\nHowever, despite the importance of active perception in embodied intelligence,\nthere is little to no exploration of how MLLMs can be equipped with or learn\nactive perception capabilities. In this paper, we first provide a systematic\ndefinition of MLLM-based active perception tasks. We point out that the\nrecently proposed GPT-o3 model's zoom-in search strategy can be regarded as a\nspecial case of active perception; however, it still suffers from low search\nefficiency and inaccurate region selection. To address these issues, we propose\nACTIVE-O3, a purely reinforcement learning based training framework built on\ntop of GRPO, designed to equip MLLMs with active perception capabilities. We\nfurther establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across\nboth general open-world tasks, such as small-object and dense object grounding,\nand domain-specific scenarios, including small object detection in remote\nsensing and autonomous driving, as well as fine-grained interactive\nsegmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot\nreasoning abilities on the V* Benchmark, without relying on any explicit\nreasoning data. We hope that our work can provide a simple codebase and\nevaluation protocol to facilitate future research on active perception in\nMLLMs.", "published": "2025-05-27 17:29:31", "link": "http://arxiv.org/abs/2505.21457v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "VoxAging: Continuously Tracking Speaker Aging with a Large-Scale Longitudinal Dataset in English and Mandarin", "abstract": "The performance of speaker verification systems is adversely affected by\nspeaker aging. However, due to challenges in data collection, particularly the\nlack of sustained and large-scale longitudinal data for individuals, research\non speaker aging remains difficult. In this paper, we present VoxAging, a\nlarge-scale longitudinal dataset collected from 293 speakers (226 English\nspeakers and 67 Mandarin speakers) over several years, with the longest time\nspan reaching 17 years (approximately 900 weeks). For each speaker, the data\nwere recorded at weekly intervals. We studied the phenomenon of speaker aging\nand its effects on advanced speaker verification systems, analyzed individual\nspeaker aging processes, and explored the impact of factors such as age group\nand gender on speaker aging research.", "published": "2025-05-27 17:16:59", "link": "http://arxiv.org/abs/2505.21445v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM"], "primary_category": "cs.SD"}
{"title": "Autoencoding Random Forests", "abstract": "We propose a principled method for autoencoding with random forests. Our\nstrategy builds on foundational results from nonparametric statistics and\nspectral graph theory to learn a low-dimensional embedding of the model that\noptimally represents relationships in the data. We provide exact and\napproximate solutions to the decoding problem via constrained optimization,\nsplit relabeling, and nearest neighbors regression. These methods effectively\ninvert the compression pipeline, establishing a map from the embedding space\nback to the input space using splits learned by the ensemble's constituent\ntrees. The resulting decoders are universally consistent under common\nregularity assumptions. The procedure works with supervised or unsupervised\nmodels, providing a window into conditional or joint distributions. We\ndemonstrate various applications of this autoencoder, including powerful new\ntools for visualization, compression, clustering, and denoising. Experiments\nillustrate the ease and utility of our method in a wide range of settings,\nincluding tabular, image, and genomic data.", "published": "2025-05-27 17:15:02", "link": "http://arxiv.org/abs/2505.21441v1", "categories": ["stat.ML", "cs.AI", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "abstract": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "published": "2025-05-27 17:04:21", "link": "http://arxiv.org/abs/2505.21432v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "Policy Induction: Predicting Startup Success via Explainable Memory-Augmented In-Context Learning", "abstract": "Early-stage startup investment is a high-risk endeavor characterized by\nscarce data and uncertain outcomes. Traditional machine learning approaches\noften require large, labeled datasets and extensive fine-tuning, yet remain\nopaque and difficult for domain experts to interpret or improve. In this paper,\nwe propose a transparent and data-efficient investment decision framework\npowered by memory-augmented large language models (LLMs) using in-context\nlearning (ICL). Central to our method is a natural language policy embedded\ndirectly into the LLM prompt, enabling the model to apply explicit reasoning\npatterns and allowing human experts to easily interpret, audit, and iteratively\nrefine the logic. We introduce a lightweight training process that combines\nfew-shot learning with an in-context learning loop, enabling the LLM to update\nits decision policy iteratively based on structured feedback. With only minimal\nsupervision and no gradient-based optimization, our system predicts startup\nsuccess far more accurately than existing benchmarks. It is over 20x more\nprecise than random chance, which succeeds 1.9% of the time. It is also 7.1x\nmore precise than the typical 5.6% success rate of top-tier venture capital\n(VC) firms.", "published": "2025-05-27 16:57:07", "link": "http://arxiv.org/abs/2505.21427v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Learning Individual Behavior in Agent-Based Models with Graph Diffusion Networks", "abstract": "Agent-Based Models (ABMs) are powerful tools for studying emergent properties\nin complex systems. In ABMs, agent behaviors are governed by local interactions\nand stochastic rules. However, these rules are, in general, non-differentiable,\nlimiting the use of gradient-based methods for optimization, and thus\nintegration with real-world data. We propose a novel framework to learn a\ndifferentiable surrogate of any ABM by observing its generated data. Our method\ncombines diffusion models to capture behavioral stochasticity and graph neural\nnetworks to model agent interactions. Distinct from prior surrogate approaches,\nour method introduces a fundamental shift: rather than approximating\nsystem-level outputs, it models individual agent behavior directly, preserving\nthe decentralized, bottom-up dynamics that define ABMs. We validate our\napproach on two ABMs (Schelling's segregation model and a Predator-Prey\necosystem) showing that it replicates individual-level patterns and accurately\nforecasts emergent dynamics beyond training. Our results demonstrate the\npotential of combining diffusion models and graph learning for data-driven ABM\nsimulation.", "published": "2025-05-27 16:55:56", "link": "http://arxiv.org/abs/2505.21426v1", "categories": ["cs.AI", "cs.LG", "cs.MA", "econ.EM", "physics.soc-ph"], "primary_category": "cs.AI"}
{"title": "Mentor3AD: Feature Reconstruction-based 3D Anomaly Detection via Multi-modality Mentor Learning", "abstract": "Multimodal feature reconstruction is a promising approach for 3D anomaly\ndetection, leveraging the complementary information from dual modalities. We\nfurther advance this paradigm by utilizing multi-modal mentor learning, which\nfuses intermediate features to further distinguish normal from feature\ndifferences. To address these challenges, we propose a novel method called\nMentor3AD, which utilizes multi-modal mentor learning. By leveraging the shared\nfeatures of different modalities, Mentor3AD can extract more effective features\nand guide feature reconstruction, ultimately improving detection performance.\nSpecifically, Mentor3AD includes a Mentor of Fusion Module (MFM) that merges\nfeatures extracted from RGB and 3D modalities to create a mentor feature.\nAdditionally, we have designed a Mentor of Guidance Module (MGM) to facilitate\ncross-modal reconstruction, supported by the mentor feature. Lastly, we\nintroduce a Voting Module (VM) to more accurately generate the final anomaly\nscore. Extensive comparative and ablation studies on MVTec 3D-AD and Eyecandies\nhave verified the effectiveness of the proposed method.", "published": "2025-05-27 16:46:28", "link": "http://arxiv.org/abs/2505.21420v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG LLMs", "abstract": "Today's cloud-hosted applications and services are complex systems, and a\nperformance or functional instability can have dozens or hundreds of potential\nroot causes. Our hypothesis is that by combining the pattern matching\ncapabilities of modern AI tools with a natural multi-modal RAG LLM interface,\nproblem identification and resolution can be simplified. ARCA is a new\nmulti-modal RAG LLM system that targets this domain. Step-wise evaluations show\nthat ARCA outperforms state-of-the-art alternatives.", "published": "2025-05-27 16:43:45", "link": "http://arxiv.org/abs/2505.21419v1", "categories": ["cs.AI", "cs.OS"], "primary_category": "cs.AI"}
{"title": "A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment", "abstract": "This paper introduces a comprehensive framework designed to analyze and\nsecure decision-support systems trained with Deep Reinforcement Learning (DRL),\nprior to deployment, by providing insights into learned behavior patterns and\nvulnerabilities discovered through simulation. The introduced framework aids in\nthe development of precisely timed and targeted observation perturbations,\nenabling researchers to assess adversarial attack outcomes within a strategic\ndecision-making context. We validate our framework, visualize agent behavior,\nand evaluate adversarial outcomes within the context of a custom-built\nstrategic game, CyberStrike. Utilizing the proposed framework, we introduce a\nmethod for systematically discovering and ranking the impact of attacks on\nvarious observation indices and time-steps, and we conduct experiments to\nevaluate the transferability of adversarial attacks across agent architectures\nand DRL training algorithms. The findings underscore the critical need for\nrobust adversarial defense mechanisms to protect decision-making policies in\nhigh-stakes environments.", "published": "2025-05-27 16:41:23", "link": "http://arxiv.org/abs/2505.21414v1", "categories": ["cs.LG", "cs.AI", "cs.GT"], "primary_category": "cs.LG"}
{"title": "MRSD: Multi-Resolution Skill Discovery for HRL Agents", "abstract": "Hierarchical reinforcement learning (HRL) relies on abstract skills to solve\nlong-horizon tasks efficiently. While existing skill discovery methods learns\nthese skills automatically, they are limited to a single skill per task. In\ncontrast, humans learn and use both fine-grained and coarse motor skills\nsimultaneously. Inspired by human motor control, we propose Multi-Resolution\nSkill Discovery (MRSD), an HRL framework that learns multiple skill encoders at\ndifferent temporal resolutions in parallel. A high-level manager dynamically\nselects among these skills, enabling adaptive control strategies over time. We\nevaluate MRSD on tasks from the DeepMind Control Suite and show that it\noutperforms prior state-of-the-art skill discovery and HRL methods, achieving\nfaster convergence and higher final performance. Our findings highlight the\nbenefits of integrating multi-resolution skills in HRL, paving the way for more\nversatile and efficient agents.", "published": "2025-05-27 16:38:55", "link": "http://arxiv.org/abs/2505.21410v1", "categories": ["cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.AI"}
{"title": "A Structured Unplugged Approach for Foundational AI Literacy in Primary Education", "abstract": "Younger generations are growing up in a world increasingly shaped by\nintelligent technologies, making early AI literacy crucial for developing the\nskills to critically understand and navigate them. However, education in this\nfield often emphasizes tool-based learning, prioritizing usage over\nunderstanding the underlying concepts. This lack of knowledge leaves\nnon-experts, especially children, prone to misconceptions, unrealistic\nexpectations, and difficulties in recognizing biases and stereotypes. In this\npaper, we propose a structured and replicable teaching approach that fosters\nfoundational AI literacy in primary students, by building upon core\nmathematical elements closely connected to and of interest in primary\ncurricula, to strengthen conceptualization, data representation, classification\nreasoning, and evaluation of AI. To assess the effectiveness of our approach,\nwe conducted an empirical study with thirty-one fifth-grade students across two\nclasses, evaluating their progress through a post-test and a satisfaction\nsurvey. Our results indicate improvements in terminology understanding and\nusage, features description, logical reasoning, and evaluative skills, with\nstudents showing a deeper comprehension of decision-making processes and their\nlimitations. Moreover, the approach proved engaging, with students particularly\nenjoying activities that linked AI concepts to real-world reasoning. Materials:\nhttps://github.com/tail-unica/ai-literacy-primary-ed.", "published": "2025-05-27 16:23:57", "link": "http://arxiv.org/abs/2505.21398v1", "categories": ["cs.AI", "cs.ET"], "primary_category": "cs.AI"}
{"title": "Leveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits", "abstract": "Conversational recommender systems proactively query users with relevant \"key\nterms\" and leverage the feedback to elicit users' preferences for personalized\nrecommendations. Conversational contextual bandits, a prevalent approach in\nthis domain, aim to optimize preference learning by balancing exploitation and\nexploration. However, several limitations hinder their effectiveness in\nreal-world scenarios. First, existing algorithms employ key term selection\nstrategies with insufficient exploration, often failing to thoroughly probe\nusers' preferences and resulting in suboptimal preference estimation. Second,\ncurrent algorithms typically rely on deterministic rules to initiate\nconversations, causing unnecessary interactions when preferences are\nwell-understood and missed opportunities when preferences are uncertain. To\naddress these limitations, we propose three novel algorithms: CLiSK, CLiME, and\nCLiSK-ME. CLiSK introduces smoothed key term contexts to enhance exploration in\npreference learning, CLiME adaptively initiates conversations based on\npreference uncertainty, and CLiSK-ME integrates both techniques. We\ntheoretically prove that all three algorithms achieve a tighter regret upper\nbound of $O(\\sqrt{dT\\log{T}})$ with respect to the time horizon $T$, improving\nupon existing methods. Additionally, we provide a matching lower bound\n$\\Omega(\\sqrt{dT})$ for conversational bandits, demonstrating that our\nalgorithms are nearly minimax optimal. Extensive evaluations on both synthetic\nand real-world datasets show that our approaches achieve at least a 14.6%\nimprovement in cumulative regret.", "published": "2025-05-27 16:22:32", "link": "http://arxiv.org/abs/2505.21393v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Finite Sample Analysis of Linear Temporal Difference Learning with Arbitrary Features", "abstract": "Linear TD($\\lambda$) is one of the most fundamental reinforcement learning\nalgorithms for policy evaluation. Previously, convergence rates are typically\nestablished under the assumption of linearly independent features, which does\nnot hold in many practical scenarios. This paper instead establishes the first\n$L^2$ convergence rates for linear TD($\\lambda$) operating under arbitrary\nfeatures, without making any algorithmic modification or additional\nassumptions. Our results apply to both the discounted and average-reward\nsettings. To address the potential non-uniqueness of solutions resulting from\narbitrary features, we develop a novel stochastic approximation result\nfeaturing convergence rates to the solution set instead of a single point.", "published": "2025-05-27 16:17:49", "link": "http://arxiv.org/abs/2505.21391v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "DeSocial: Blockchain-based Decentralized Social Networks", "abstract": "Web 2.0 social platforms are inherently centralized, with user data and\nalgorithmic decisions controlled by the platform. However, users can only\npassively receive social predictions without being able to choose the\nunderlying algorithm, which limits personalization. Fortunately, with the\nemergence of blockchain, users are allowed to choose algorithms that are\ntailored to their local situation, improving prediction results in a\npersonalized way. In a blockchain environment, each user possesses its own\nmodel to perform the social prediction, capturing different perspectives on\nsocial interactions. In our work, we propose DeSocial, a decentralized social\nnetwork learning framework deployed on an Ethereum (ETH) local development\nchain that integrates distributed data storage, node-level consensus, and\nuser-driven model selection through Ganache. In the first stage, each user\nleverages DeSocial to evaluate multiple backbone models on their local\nsubgraph. DeSocial coordinates the execution and returns model-wise prediction\nresults, enabling the user to select the most suitable backbone for\npersonalized social prediction. Then, DeSocial uniformly selects several\nvalidation nodes that possess the algorithm specified by each user, and\naggregates the prediction results by majority voting, to prevent errors caused\nby any single model's misjudgment. Extensive experiments show that DeSocial has\nan evident improvement compared to the five classical centralized social\nnetwork learning models, promoting user empowerment in blockchain-based\ndecentralized social networks, showing the importance of multi-node validation\nand personalized algorithm selection based on blockchain. Our implementation is\navailable at: https://github.com/agiresearch/DeSocial.", "published": "2025-05-27 16:17:06", "link": "http://arxiv.org/abs/2505.21388v1", "categories": ["cs.SI", "cs.AI", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Improving LLM-based Global Optimization with Search Space Partitioning", "abstract": "Large Language Models (LLMs) have recently emerged as effective surrogate\nmodels and candidate generators within global optimization frameworks for\nexpensive blackbox functions. Despite promising results, LLM-based methods\noften struggle in high-dimensional search spaces or when lacking\ndomain-specific priors, leading to sparse or uninformative suggestions. To\novercome these limitations, we propose HOLLM, a novel global optimization\nalgorithm that enhances LLM-driven sampling by partitioning the search space\ninto promising subregions. Each subregion acts as a ``meta-arm'' selected via a\nbandit-inspired scoring mechanism that effectively balances exploration and\nexploitation. Within each selected subregion, an LLM then proposes high-quality\ncandidate points, without any explicit domain knowledge. Empirical evaluation\non standard optimization benchmarks shows that HOLLM consistently matches or\nsurpasses leading Bayesian optimization and trust-region methods, while\nsubstantially outperforming global LLM-based sampling strategies.", "published": "2025-05-27 16:01:49", "link": "http://arxiv.org/abs/2505.21372v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders", "abstract": "Multilayer perceptrons (MLPs) are an integral part of large language models,\nyet their dense representations render them difficult to understand, edit, and\nsteer. Recent methods learn interpretable approximations via neuron-level\nsparsity, yet fail to faithfully reconstruct the original\nmapping--significantly increasing model's next-token cross-entropy loss. In\nthis paper, we advocate for moving to layer-level sparsity to overcome the\naccuracy trade-off in sparse layer approximation. Under this paradigm, we\nintroduce Mixture of Decoders (MxDs). MxDs generalize MLPs and Gated Linear\nUnits, expanding pre-trained dense layers into tens of thousands of specialized\nsublayers. Through a flexible form of tensor factorization, each sparsely\nactivating MxD sublayer implements a linear transformation with full-rank\nweights--preserving the original decoders' expressive capacity even under heavy\nsparsity. Experimentally, we show that MxDs significantly outperform\nstate-of-the-art methods (e.g., Transcoders) on the sparsity-accuracy frontier\nin language models with up to 3B parameters. Further evaluations on sparse\nprobing and feature steering demonstrate that MxDs learn similarly specialized\nfeatures of natural language--opening up a promising new avenue for designing\ninterpretable yet faithful decompositions. Our code is included at:\nhttps://github.com/james-oldfield/MxD/.", "published": "2025-05-27 15:55:55", "link": "http://arxiv.org/abs/2505.21364v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Subgroups Matter for Robust Bias Mitigation", "abstract": "Despite the constant development of new bias mitigation methods for machine\nlearning, no method consistently succeeds, and a fundamental question remains\nunanswered: when and why do bias mitigation techniques fail? In this paper, we\nhypothesise that a key factor may be the often-overlooked but crucial step\nshared by many bias mitigation methods: the definition of subgroups. To\ninvestigate this, we conduct a comprehensive evaluation of state-of-the-art\nbias mitigation methods across multiple vision and language classification\ntasks, systematically varying subgroup definitions, including coarse,\nfine-grained, intersectional, and noisy subgroups. Our results reveal that\nsubgroup choice significantly impacts performance, with certain groupings\nparadoxically leading to worse outcomes than no mitigation at all. Our findings\nsuggest that observing a disparity between a set of subgroups is not a\nsufficient reason to use those subgroups for mitigation. Through theoretical\nanalysis, we explain these phenomena and uncover a counter-intuitive insight\nthat, in some cases, improving fairness with respect to a particular set of\nsubgroups is best achieved by using a different set of subgroups for\nmitigation. Our work highlights the importance of careful subgroup definition\nin bias mitigation and suggest it as a alternative lever for improving the\nrobustness and fairness of machine learning models.", "published": "2025-05-27 15:52:58", "link": "http://arxiv.org/abs/2505.21363v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Prostate Cancer Screening with Artificial Intelligence-Enhanced Micro-Ultrasound: A Comparative Study with Traditional Methods", "abstract": "Background and objective: Micro-ultrasound (micro-US) is a novel imaging\nmodality with diagnostic accuracy comparable to MRI for detecting clinically\nsignificant prostate cancer (csPCa). We investigated whether artificial\nintelligence (AI) interpretation of micro-US can outperform clinical screening\nmethods using PSA and digital rectal examination (DRE). Methods: We\nretrospectively studied 145 men who underwent micro-US guided biopsy (79 with\ncsPCa, 66 without). A self-supervised convolutional autoencoder was used to\nextract deep image features from 2D micro-US slices. Random forest classifiers\nwere trained using five-fold cross-validation to predict csPCa at the slice\nlevel. Patients were classified as csPCa-positive if 88 or more consecutive\nslices were predicted positive. Model performance was compared with a\nclassifier using PSA, DRE, prostate volume, and age. Key findings and\nlimitations: The AI-based micro-US model and clinical screening model achieved\nAUROCs of 0.871 and 0.753, respectively. At a fixed threshold, the micro-US\nmodel achieved 92.5% sensitivity and 68.1% specificity, while the clinical\nmodel showed 96.2% sensitivity but only 27.3% specificity. Limitations include\na retrospective single-center design and lack of external validation.\nConclusions and clinical implications: AI-interpreted micro-US improves\nspecificity while maintaining high sensitivity for csPCa detection. This method\nmay reduce unnecessary biopsies and serve as a low-cost alternative to\nPSA-based screening. Patient summary: We developed an AI system to analyze\nprostate micro-ultrasound images. It outperformed PSA and DRE in detecting\naggressive cancer and may help avoid unnecessary biopsies.", "published": "2025-05-27 15:47:38", "link": "http://arxiv.org/abs/2505.21355v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV"}
{"title": "An Uncertainty-Aware ED-LSTM for Probabilistic Suffix Prediction", "abstract": "Suffix prediction of business processes forecasts the remaining sequence of\nevents until process completion. Current approaches focus on predicting a\nsingle, most likely suffix. However, if the future course of a process is\nexposed to uncertainty or has high variability, the expressiveness of a single\nsuffix prediction can be limited. To address this limitation, we propose\nprobabilistic suffix prediction, a novel approach that approximates a\nprobability distribution of suffixes. The proposed approach is based on an\nUncertainty-Aware Encoder-Decoder LSTM (U-ED-LSTM) and a Monte Carlo (MC)\nsuffix sampling algorithm. We capture epistemic uncertainties via MC dropout\nand aleatoric uncertainties as learned loss attenuation. This technical report\nprovides a detailed evaluation of the U-ED-LSTM's predictive performance and\nassesses its calibration on four real-life event logs with three different\nhyperparameter settings. The results show that i) the U-ED-LSTM has reasonable\npredictive performance across various datasets, ii) aggregating probabilistic\nsuffix predictions into mean values can outperform most likely predictions,\nparticularly for rare prefixes or longer suffixes, and iii) the approach\neffectively captures uncertainties present in event logs.", "published": "2025-05-27 15:33:05", "link": "http://arxiv.org/abs/2505.21339v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Structure from Collision", "abstract": "Recent advancements in neural 3D representations, such as neural radiance\nfields (NeRF) and 3D Gaussian splatting (3DGS), have enabled the accurate\nestimation of 3D structures from multiview images. However, this capability is\nlimited to estimating the visible external structure, and identifying the\ninvisible internal structure hidden behind the surface is difficult. To\novercome this limitation, we address a new task called Structure from Collision\n(SfC), which aims to estimate the structure (including the invisible internal\nstructure) of an object from appearance changes during collision. To solve this\nproblem, we propose a novel model called SfC-NeRF that optimizes the invisible\ninternal structure of an object through a video sequence under physical,\nappearance (i.e., visible external structure)-preserving, and keyframe\nconstraints. In particular, to avoid falling into undesirable local optima\nowing to its ill-posed nature, we propose volume annealing; that is, searching\nfor global optima by repeatedly reducing and expanding the volume. Extensive\nexperiments on 115 objects involving diverse structures (i.e., various cavity\nshapes, locations, and sizes) and material properties revealed the properties\nof SfC and demonstrated the effectiveness of the proposed SfC-NeRF.", "published": "2025-05-27 15:30:01", "link": "http://arxiv.org/abs/2505.21335v1", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "cs.RO"], "primary_category": "cs.GR"}
{"title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs", "abstract": "Logical reasoning is a fundamental aspect of human intelligence and an\nessential capability for multimodal large language models (MLLMs). Despite the\nsignificant advancement in multimodal reasoning, existing benchmarks fail to\ncomprehensively evaluate their reasoning abilities due to the lack of explicit\ncategorization for logical reasoning types and an unclear understanding of\nreasoning. To address these issues, we introduce MME-Reasoning, a comprehensive\nbenchmark designed to evaluate the reasoning ability of MLLMs, which covers all\nthree types of reasoning (i.e., inductive, deductive, and abductive) in its\nquestions. We carefully curate the data to ensure that each question\neffectively evaluates reasoning ability rather than perceptual skills or\nknowledge breadth, and extend the evaluation protocols to cover the evaluation\nof diverse questions. Our evaluation reveals substantial limitations of\nstate-of-the-art MLLMs when subjected to holistic assessments of logical\nreasoning capabilities. Even the most advanced MLLMs show limited performance\nin comprehensive logical reasoning, with notable performance imbalances across\nreasoning types. In addition, we conducted an in-depth analysis of approaches\nsuch as ``thinking mode'' and Rule-based RL, which are commonly believed to\nenhance reasoning abilities. These findings highlight the critical limitations\nand performance imbalances of current MLLMs in diverse logical reasoning\nscenarios, providing comprehensive and systematic insights into the\nunderstanding and evaluation of reasoning capabilities.", "published": "2025-05-27 15:23:23", "link": "http://arxiv.org/abs/2505.21327v1", "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Assured Autonomy with Neuro-Symbolic Perception", "abstract": "Many state-of-the-art AI models deployed in cyber-physical systems (CPS),\nwhile highly accurate, are simply pattern-matchers.~With limited security\nguarantees, there are concerns for their reliability in safety-critical and\ncontested domains. To advance assured AI, we advocate for a paradigm shift that\nimbues data-driven perception models with symbolic structure, inspired by a\nhuman's ability to reason over low-level features and high-level context. We\npropose a neuro-symbolic paradigm for perception (NeuSPaPer) and illustrate how\njoint object detection and scene graph generation (SGG) yields deep scene\nunderstanding.~Powered by foundation models for offline knowledge extraction\nand specialized SGG algorithms for real-time deployment, we design a framework\nleveraging structured relational graphs that ensures the integrity of\nsituational awareness in autonomy. Using physics-based simulators and\nreal-world datasets, we demonstrate how SGG bridges the gap between low-level\nsensor perception and high-level reasoning, establishing a foundation for\nresilient, context-aware AI and advancing trusted autonomy in CPS.", "published": "2025-05-27 15:21:06", "link": "http://arxiv.org/abs/2505.21322v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Beyond Chemical QA: Evaluating LLM's Chemical Reasoning with Modular Chemical Operations", "abstract": "While large language models (LLMs) with Chain-of-Thought (CoT) reasoning\nexcel in mathematics and coding, their potential for systematic reasoning in\nchemistry, a domain demanding rigorous structural analysis for real-world tasks\nlike drug design and reaction engineering, remains untapped. Current benchmarks\nfocus on simple knowledge retrieval, neglecting step-by-step reasoning required\nfor complex tasks such as molecular optimization and reaction prediction. To\naddress this, we introduce ChemCoTBench, a reasoning framework that bridges\nmolecular structure understanding with arithmetic-inspired operations,\nincluding addition, deletion, and substitution, to formalize chemical\nproblem-solving into transparent, step-by-step workflows. By treating molecular\ntransformations as modular \"chemical operations\", the framework enables\nslow-thinking reasoning, mirroring the logic of mathematical proofs while\ngrounding solutions in real-world chemical constraints. We evaluate models on\ntwo high-impact tasks: Molecular Property Optimization and Chemical Reaction\nPrediction. These tasks mirror real-world challenges while providing structured\nevaluability. By providing annotated datasets, a reasoning taxonomy, and\nbaseline evaluations, ChemCoTBench bridges the gap between abstract reasoning\nmethods and practical chemical discovery, establishing a foundation for\nadvancing LLMs as tools for AI-driven scientific innovation.", "published": "2025-05-27 15:15:44", "link": "http://arxiv.org/abs/2505.21318v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "A Cross Modal Knowledge Distillation & Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features", "abstract": "Understanding cellular responses to stimuli is crucial for biological\ndiscovery and drug development. Transcriptomics provides interpretable,\ngene-level insights, while microscopy imaging offers rich predictive features\nbut is harder to interpret. Weakly paired datasets, where samples share\nbiological states, enable multimodal learning but are scarce, limiting their\nutility for training and multimodal inference. We propose a framework to\nenhance transcriptomics by distilling knowledge from microscopy images. Using\nweakly paired data, our method aligns and binds modalities, enriching gene\nexpression representations with morphological information. To address data\nscarcity, we introduce (1) Semi-Clipped, an adaptation of CLIP for cross-modal\ndistillation using pretrained foundation models, achieving state-of-the-art\nresults, and (2) PEA (Perturbation Embedding Augmentation), a novel\naugmentation technique that enhances transcriptomics data while preserving\ninherent biological information. These strategies improve the predictive power\nand retain the interpretability of transcriptomics, enabling rich unimodal\nrepresentations for complex biological tasks.", "published": "2025-05-27 15:15:34", "link": "http://arxiv.org/abs/2505.21317v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Large Language Models Miss the Multi-Agent Mark", "abstract": "Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs)\nhas led to an increase in frameworks leveraging multiple LLMs to tackle complex\ntasks. However, much of this literature appropriates the terminology of MAS\nwithout engaging with its foundational principles. In this position paper, we\nhighlight critical discrepancies between MAS theory and current MAS LLMs\nimplementations, focusing on four key areas: the social aspect of agency,\nenvironment design, coordination and communication protocols, and measuring\nemergent behaviours. Our position is that many MAS LLMs lack multi-agent\ncharacteristics such as autonomy, social interaction, and structured\nenvironments, and often rely on oversimplified, LLM-centric architectures. The\nfield may slow down and lose traction by revisiting problems the MAS literature\nhas already addressed. Therefore, we systematically analyse this issue and\noutline associated research opportunities; we advocate for better integrating\nestablished MAS concepts and more precise terminology to avoid\nmischaracterisation and missed opportunities.", "published": "2025-05-27 15:01:06", "link": "http://arxiv.org/abs/2505.21298v1", "categories": ["cs.MA", "cs.AI", "cs.LG"], "primary_category": "cs.MA"}
{"title": "Complex System Diagnostics Using a Knowledge Graph-Informed and Large Language Model-Enhanced Framework", "abstract": "In this paper, we present a novel diagnostic framework that integrates\nKnowledge Graphs (KGs) and Large Language Models (LLMs) to support system\ndiagnostics in high-reliability systems such as nuclear power plants.\nTraditional diagnostic modeling struggles when systems become too complex,\nmaking functional modeling a more attractive approach. Our approach introduces\na diagnostic framework grounded in the functional modeling principles of the\nDynamic Master Logic (DML) model. It incorporates two coordinated LLM\ncomponents, including an LLM-based workflow for automated construction of DML\nlogic from system documentation and an LLM agent that facilitates interactive\ndiagnostics. The generated logic is encoded into a structured KG, referred to\nas KG-DML, which supports hierarchical fault reasoning. Expert knowledge or\noperational data can also be incorporated to refine the model's precision and\ndiagnostic depth. In the interaction phase, users submit natural language\nqueries, which are interpreted by the LLM agent. The agent selects appropriate\ntools for structured reasoning, including upward and downward propagation\nacross the KG-DML. Rather than embedding KG content into every prompt, the LLM\nagent distinguishes between diagnostic and interpretive tasks. For diagnostics,\nthe agent selects and executes external tools that perform structured KG\nreasoning. For general queries, a Graph-based Retrieval-Augmented Generation\n(Graph-RAG) approach is used, retrieving relevant KG segments and embedding\nthem into the prompt to generate natural explanations. A case study on an\nauxiliary feedwater system demonstrated the framework's effectiveness, with\nover 90% accuracy in key elements and consistent tool and argument extraction,\nsupporting its use in safety-critical diagnostics.", "published": "2025-05-27 14:54:49", "link": "http://arxiv.org/abs/2505.21291v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "GSAT: Graph Structure Attention Networks", "abstract": "Graph Neural Networks (GNNs) have emerged as a powerful tool for processing\ndata represented in graph structures, achieving remarkable success across a\nwide range of applications. However, to further improve the performance on\ngraph classification benchmarks, structural representation of each node that\nencodes rich local topological information in the neighbourhood of nodes is an\nimportant type of feature that is often overlooked in the modeling. The\nconsequence of neglecting the structural information has resulted high number\nof layers to connect messages from distant nodes which by itself produces other\nproblems such as oversmoothing. In the present paper, we leverage these\nstructural information that are modeled by anonymous random walks (ARWs) and\nintroduce graph structure attention network (GSAT) which is a generalization of\ngraph attention network(GAT) to integrate the original attribute and the\nstructural representation to enforce the model to automatically find patterns\nfor attending to different edges in the node neighbourhood to enrich graph\nrepresentation. Our experiments show GSAT slightly improves SOTA on some graph\nclassification benchmarks.", "published": "2025-05-27 14:54:08", "link": "http://arxiv.org/abs/2505.21288v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "RLJP: Legal Judgment Prediction via First-Order Logic Rule-enhanced with Large Language Models", "abstract": "Legal Judgment Prediction (LJP) is a pivotal task in legal AI. Existing\nsemantic-enhanced LJP models integrate judicial precedents and legal knowledge\nfor high performance. But they neglect legal reasoning logic, a critical\ncomponent of legal judgments requiring rigorous logical analysis. Although some\napproaches utilize legal reasoning logic for high-quality predictions, their\nlogic rigidity hinders adaptation to case-specific logical frameworks,\nparticularly in complex cases that are lengthy and detailed. This paper\nproposes a rule-enhanced legal judgment prediction framework based on\nfirst-order logic (FOL) formalism and comparative learning (CL) to develop an\nadaptive adjustment mechanism for legal judgment logic and further enhance\nperformance in LJP. Inspired by the process of human exam preparation, our\nmethod follows a three-stage approach: first, we initialize judgment rules\nusing the FOL formalism to capture complex reasoning logic accurately; next, we\npropose a Confusion-aware Contrastive Learning (CACL) to dynamically optimize\nthe judgment rules through a quiz consisting of confusable cases; finally, we\nutilize the optimized judgment rules to predict legal judgments. Experimental\nresults on two public datasets show superior performance across all metrics.\nThe code is publicly available{https://anonymous.4open.science/r/RLJP-FDF1}.", "published": "2025-05-27 14:50:21", "link": "http://arxiv.org/abs/2505.21281v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "XBOUND: Exploring the Capability Boundaries of Device-Control Agents through Trajectory Tree Exploration", "abstract": "Recent advancements in vision-language models (VLMs) have spurred increased\ninterest in Device-Control Agents (DC agents), such as utilizing in-the-wild\ndevice control to manage graphical user interfaces. Conventional methods for\nassessing the capabilities of DC agents, such as computing step-wise action\naccuracy and overall task success rates, provide a macroscopic view of DC\nagents' performance; however, they fail to offer microscopic insights into\npotential errors that may occur in real-world applications. Conducting a\nfiner-grained performance evaluation of DC agents presents significant\nchallenges. This study introduces a new perspective on evaluation methods for\nDC agents by proposing the XBOUND evaluation method, which employs the\ncalculation of a novel Explore Metric to delineate the capability boundaries of\nDC agents. Compared to previous evaluation methods, XBOUND focuses on\nindividual states to assess the proficiency of DC agents in mastering these\nstates. Furthermore, we have developed a ``pseudo'' episode tree dataset\nderived from Android Control test data. Utilizing this dataset and XBOUND, we\ncomprehensively evaluate the OS-Atlas and UI-TARS series, examining both the\noverall and specific performance across five common tasks. Additionally, we\nselect representative cases to highlight the current deficiencies and\nlimitations inherent in both series. Code is available at\nhttps://github.com/sqzhang-lazy/XBOUND.", "published": "2025-05-27 14:49:30", "link": "http://arxiv.org/abs/2505.21279v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Breaking the Performance Ceiling in Complex Reinforcement Learning requires Inference Strategies", "abstract": "Reinforcement learning (RL) systems have countless applications, from\nenergy-grid management to protein design. However, such real-world scenarios\nare often extremely difficult, combinatorial in nature, and require complex\ncoordination between multiple agents. This level of complexity can cause even\nstate-of-the-art RL systems, trained until convergence, to hit a performance\nceiling which they are unable to break out of with zero-shot inference.\nMeanwhile, many digital or simulation-based applications allow for an inference\nphase that utilises a specific time and compute budget to explore multiple\nattempts before outputting a final solution. In this work, we show that such an\ninference phase employed at execution time, and the choice of a corresponding\ninference strategy, are key to breaking the performance ceiling observed in\ncomplex multi-agent RL problems. Our main result is striking: we can obtain up\nto a 126% and, on average, a 45% improvement over the previous state-of-the-art\nacross 17 tasks, using only a couple seconds of extra wall-clock time during\nexecution. We also demonstrate promising compute scaling properties, supported\nby over 60k experiments, making it the largest study on inference strategies\nfor complex RL to date. Our experimental data and code are available at\nhttps://sites.google.com/view/inf-marl.", "published": "2025-05-27 14:19:06", "link": "http://arxiv.org/abs/2505.21236v1", "categories": ["cs.LG", "cs.AI", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Is Hyperbolic Space All You Need for Medical Anomaly Detection?", "abstract": "Medical anomaly detection has emerged as a promising solution to challenges\nin data availability and labeling constraints. Traditional methods extract\nfeatures from different layers of pre-trained networks in Euclidean space;\nhowever, Euclidean representations fail to effectively capture the hierarchical\nrelationships within these features, leading to suboptimal anomaly detection\nperformance. We propose a novel yet simple approach that projects feature\nrepresentations into hyperbolic space, aggregates them based on confidence\nlevels, and classifies samples as healthy or anomalous. Our experiments\ndemonstrate that hyperbolic space consistently outperforms Euclidean-based\nframeworks, achieving higher AUROC scores at both image and pixel levels across\nmultiple medical benchmark datasets. Additionally, we show that hyperbolic\nspace exhibits resilience to parameter variations and excels in few-shot\nscenarios, where healthy images are scarce. These findings underscore the\npotential of hyperbolic space as a powerful alternative for medical anomaly\ndetection. The project website can be found at\nhttps://hyperbolic-anomalies.github.io", "published": "2025-05-27 14:13:11", "link": "http://arxiv.org/abs/2505.21228v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Addressing Data Quality Decompensation in Federated Learning via Dynamic Client Selection", "abstract": "In cross-silo Federated Learning (FL), client selection is critical to ensure\nhigh model performance, yet it remains challenging due to data quality\ndecompensation, budget constraints, and incentive compatibility. As training\nprogresses, these factors exacerbate client heterogeneity and degrade global\nperformance. Most existing approaches treat these challenges in isolation,\nmaking jointly optimizing multiple factors difficult. To address this, we\npropose Shapley-Bid Reputation Optimized Federated Learning (SBRO-FL), a\nunified framework integrating dynamic bidding, reputation modeling, and\ncost-aware selection. Clients submit bids based on their perceived data\nquality, and their contributions are evaluated using Shapley values to quantify\ntheir marginal impact on the global model. A reputation system, inspired by\nprospect theory, captures historical performance while penalizing\ninconsistency. The client selection problem is formulated as a 0-1 integer\nprogram that maximizes reputation-weighted utility under budget constraints.\nExperiments on FashionMNIST, EMNIST, CIFAR-10, and SVHN datasets show that\nSBRO-FL improves accuracy, convergence speed, and robustness, even in\nadversarial and low-bid interference scenarios. Our results highlight the\nimportance of balancing data reliability, incentive compatibility, and cost\nefficiency to enable scalable and trustworthy FL deployments.", "published": "2025-05-27 14:06:51", "link": "http://arxiv.org/abs/2505.21219v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Interpretable DNFs", "abstract": "A classifier is considered interpretable if each of its decisions has an\nexplanation which is small enough to be easily understood by a human user. A\nDNF formula can be seen as a binary classifier $\\kappa$ over boolean domains.\nThe size of an explanation of a positive decision taken by a DNF $\\kappa$ is\nbounded by the size of the terms in $\\kappa$, since we can explain a positive\ndecision by giving a term of $\\kappa$ that evaluates to true. Since both\npositive and negative decisions must be explained, we consider that\ninterpretable DNFs are those $\\kappa$ for which both $\\kappa$ and\n$\\overline{\\kappa}$ can be expressed as DNFs composed of terms of bounded size.\nIn this paper, we study the family of $k$-DNFs whose complements can also be\nexpressed as $k$-DNFs. We compare two such families, namely depth-$k$ decision\ntrees and nested $k$-DNFs, a novel family of models. Experiments indicate that\nnested $k$-DNFs are an interesting alternative to decision trees in terms of\ninterpretability and accuracy.", "published": "2025-05-27 14:01:39", "link": "http://arxiv.org/abs/2505.21212v1", "categories": ["cs.AI", "68T27, 05C62", "F.4.1; I.2.6"], "primary_category": "cs.AI"}
{"title": "Learning What to Do and What Not To Do: Offline Imitation from Expert and Undesirable Demonstrations", "abstract": "Offline imitation learning typically learns from expert and unlabeled\ndemonstrations, yet often overlooks the valuable signal in explicitly\nundesirable behaviors. In this work, we study offline imitation learning from\ncontrasting behaviors, where the dataset contains both expert and undesirable\ndemonstrations. We propose a novel formulation that optimizes a difference of\nKL divergences over the state-action visitation distributions of expert and\nundesirable (or bad) data. Although the resulting objective is a DC\n(Difference-of-Convex) program, we prove that it becomes convex when expert\ndemonstrations outweigh undesirable demonstrations, enabling a practical and\nstable non-adversarial training objective. Our method avoids adversarial\ntraining and handles both positive and negative demonstrations in a unified\nframework. Extensive experiments on standard offline imitation learning\nbenchmarks demonstrate that our approach consistently outperforms\nstate-of-the-art baselines.", "published": "2025-05-27 13:33:21", "link": "http://arxiv.org/abs/2505.21182v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Latent label distribution grid representation for modeling uncertainty", "abstract": "Although \\textbf{L}abel \\textbf{D}istribution \\textbf{L}earning (LDL) has\npromising representation capabilities for characterizing the polysemy of an\ninstance, the complexity and high cost of the label distribution annotation\nlead to inexact in the construction of the label space. The existence of a\nlarge number of inexact labels generates a label space with uncertainty, which\nmisleads the LDL algorithm to yield incorrect decisions. To alleviate this\nproblem, we model the uncertainty of label distributions by constructing a\n\\textbf{L}atent \\textbf{L}abel \\textbf{D}istribution \\textbf{G}rid (LLDG) to\nform a low-noise representation space. Specifically, we first construct a label\ncorrelation matrix based on the differences between labels, and then expand\neach value of the matrix into a vector that obeys a Gaussian distribution, thus\nbuilding a LLDG to model the uncertainty of the label space. Finally, the LLDG\nis reconstructed by the LLDG-Mixer to generate an accurate label distribution.\nNote that we enforce a customized low-rank scheme on this grid, which assumes\nthat the label relations may be noisy and it needs to perform noise-reduction\nwith the help of a Tucker reconstruction technique. Furthermore, we attempt to\nevaluate the effectiveness of the LLDG by considering its generation as an\nupstream task to achieve the classification of the objects. Extensive\nexperimental results show that our approach performs competitively on several\nbenchmarks.", "published": "2025-05-27 13:31:37", "link": "http://arxiv.org/abs/2505.21180v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Quantum AIXI: Universal Intelligence via Quantum Information", "abstract": "AIXI is a widely studied model of artificial general intelligence (AGI) based\nupon principles of induction and reinforcement learning. However, AIXI is\nfundamentally classical in nature - as are the environments in which it is\nmodelled. Given the universe is quantum mechanical in nature and the\nexponential overhead required to simulate quantum mechanical systems\nclassically, the question arises as to whether there are quantum mechanical\nanalogues of AIXI which are theoretically consistent or practically feasible as\nmodels of universal intelligence. To address this question, we extend the\nframework to quantum information and present Quantum AIXI (QAIXI). We introduce\na model of quantum agent/environment interaction based upon quantum and\nclassical registers and channels, showing how quantum AIXI agents may take both\nclassical and quantum actions. We formulate the key components of AIXI in\nquantum information terms, extending previous research on quantum Kolmogorov\ncomplexity and a QAIXI value function. We discuss conditions and limitations\nupon quantum Solomonoff induction and show how contextuality fundamentally\naffects QAIXI models.", "published": "2025-05-27 13:23:53", "link": "http://arxiv.org/abs/2505.21170v1", "categories": ["quant-ph", "cs.AI"], "primary_category": "quant-ph"}
{"title": "STEB: In Search of the Best Evaluation Approach for Synthetic Time Series", "abstract": "The growing need for synthetic time series, due to data augmentation or\nprivacy regulations, has led to numerous generative models, frameworks, and\nevaluation measures alike. Objectively comparing these measures on a large\nscale remains an open challenge. We propose the Synthetic Time series\nEvaluation Benchmark (STEB) -- the first benchmark framework that enables\ncomprehensive and interpretable automated comparisons of synthetic time series\nevaluation measures. Using 10 diverse datasets, randomness injection, and 13\nconfigurable data transformations, STEB computes indicators for measure\nreliability and score consistency. It tracks running time, test errors, and\nfeatures sequential and parallel modes of operation. In our experiments, we\ndetermine a ranking of 41 measures from literature and confirm that the choice\nof upstream time series embedding heavily impacts the final score.", "published": "2025-05-27 13:15:35", "link": "http://arxiv.org/abs/2505.21160v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Model as Loss: A Self-Consistent Training Paradigm", "abstract": "Conventional methods for speech enhancement rely on handcrafted loss\nfunctions (e.g., time or frequency domain losses) or deep feature losses (e.g.,\nusing WavLM or wav2vec), which often fail to capture subtle signal properties\nessential for optimal performance. To address this, we propose Model as Loss, a\nnovel training paradigm that utilizes the encoder from the same model as a loss\nfunction to guide the training.\n  The Model as Loss paradigm leverages the encoder's task-specific feature\nspace, optimizing the decoder to produce output consistent with perceptual and\ntask-relevant characteristics of the clean signal. By using the encoder's\nlearned features as a loss function, this framework enforces self-consistency\nbetween the clean reference speech and the enhanced model output. Our approach\noutperforms pre-trained deep feature losses on standard speech enhancement\nbenchmarks, offering better perceptual quality and robust generalization to\nboth in-domain and out-of-domain datasets.", "published": "2025-05-27 13:12:45", "link": "http://arxiv.org/abs/2505.21156v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "GGBond: Growing Graph-Based AI-Agent Society for Socially-Aware Recommender Simulation", "abstract": "Current personalized recommender systems predominantly rely on static offline\ndata for algorithm design and evaluation, significantly limiting their ability\nto capture long-term user preference evolution and social influence dynamics in\nreal-world scenarios. To address this fundamental challenge, we propose a\nhigh-fidelity social simulation platform integrating human-like cognitive\nagents and dynamic social interactions to realistically simulate user behavior\nevolution under recommendation interventions. Specifically, the system\ncomprises a population of Sim-User Agents, each equipped with a five-layer\ncognitive architecture that encapsulates key psychological mechanisms,\nincluding episodic memory, affective state transitions, adaptive preference\nlearning, and dynamic trust-risk assessments. In particular, we innovatively\nintroduce the Intimacy--Curiosity--Reciprocity--Risk (ICR2) motivational engine\ngrounded in psychological and sociological theories, enabling more realistic\nuser decision-making processes. Furthermore, we construct a multilayer\nheterogeneous social graph (GGBond Graph) supporting dynamic relational\nevolution, effectively modeling users' evolving social ties and trust dynamics\nbased on interest similarity, personality alignment, and structural homophily.\nDuring system operation, agents autonomously respond to recommendations\ngenerated by typical recommender algorithms (e.g., Matrix Factorization,\nMultVAE, LightGCN), deciding whether to consume, rate, and share content while\ndynamically updating their internal states and social connections, thereby\nforming a stable, multi-round feedback loop. This innovative design transcends\nthe limitations of traditional static datasets, providing a controlled,\nobservable environment for evaluating long-term recommender effects.", "published": "2025-05-27 13:09:21", "link": "http://arxiv.org/abs/2505.21154v1", "categories": ["cs.MA", "cs.AI", "cs.CY"], "primary_category": "cs.MA"}
{"title": "HeteroBA: A Structure-Manipulating Backdoor Attack on Heterogeneous Graphs", "abstract": "Heterogeneous graph neural networks (HGNNs) have recently drawn increasing\nattention for modeling complex multi-relational data in domains such as\nrecommendation, finance, and social networks. While existing research has been\nlargely focused on enhancing HGNNs' predictive performance, their robustness\nand security, especially under backdoor attacks, remain underexplored. In this\npaper, we propose a novel Heterogeneous Backdoor Attack (HeteroBA) framework\nfor node classification tasks on heterogeneous graphs. HeteroBA inserts\ncarefully crafted trigger nodes with realistic features and targeted structural\nconnections, leveraging attention-based and clustering-based strategies to\nselect influential auxiliary nodes for effective trigger propagation, thereby\ncausing the model to misclassify specific nodes into a target label while\nmaintaining accuracy on clean data. Experimental results on three datasets and\nvarious HGNN architectures demonstrate that HeteroBA achieves high attack\nsuccess rates with minimal impact on the clean accuracy. Our method sheds light\non potential vulnerabilities in HGNNs and calls for more robust defenses\nagainst backdoor threats in multi-relational graph scenarios.", "published": "2025-05-27 12:51:48", "link": "http://arxiv.org/abs/2505.21140v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "SageAttention2++: A More Efficient Implementation of SageAttention2", "abstract": "The efficiency of attention is critical because its time complexity grows\nquadratically with sequence length. SageAttention2 addresses this by utilizing\nquantization to accelerate matrix multiplications (Matmul) in attention. To\nfurther accelerate SageAttention2, we propose to utilize the faster instruction\nof FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8\nMatmul used in SageAttention2. Our experiments show that SageAttention2++\nachieves a 3.9x speedup over FlashAttention while maintaining the same\nattention accuracy as SageAttention2. This means SageAttention2++ effectively\naccelerates various models, including those for language, image, and video\ngeneration, with negligible end-to-end metrics loss. The code will be available\nat https://github.com/thu-ml/SageAttention.", "published": "2025-05-27 12:50:36", "link": "http://arxiv.org/abs/2505.21136v1", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Universal Value-Function Uncertainties", "abstract": "Estimating epistemic uncertainty in value functions is a crucial challenge\nfor many aspects of reinforcement learning (RL), including efficient\nexploration, safe decision-making, and offline RL. While deep ensembles provide\na robust method for quantifying value uncertainty, they come with significant\ncomputational overhead. Single-model methods, while computationally favorable,\noften rely on heuristics and typically require additional propagation\nmechanisms for myopic uncertainty estimates. In this work we introduce\nuniversal value-function uncertainties (UVU), which, similar in spirit to\nrandom network distillation (RND), quantify uncertainty as squared prediction\nerrors between an online learner and a fixed, randomly initialized target\nnetwork. Unlike RND, UVU errors reflect policy-conditional value uncertainty,\nincorporating the future uncertainties any given policy may encounter. This is\ndue to the training procedure employed in UVU: the online network is trained\nusing temporal difference learning with a synthetic reward derived from the\nfixed, randomly initialized target network. We provide an extensive theoretical\nanalysis of our approach using neural tangent kernel (NTK) theory and show that\nin the limit of infinite network width, UVU errors are exactly equivalent to\nthe variance of an ensemble of independent universal value functions.\nEmpirically, we show that UVU achieves equal performance to large ensembles on\nchallenging multi-task offline RL settings, while offering simplicity and\nsubstantial computational savings.", "published": "2025-05-27 12:38:19", "link": "http://arxiv.org/abs/2505.21119v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Interpreting Social Bias in LVLMs via Information Flow Analysis and Multi-Round Dialogue Evaluation", "abstract": "Large Vision Language Models (LVLMs) have achieved remarkable progress in\nmultimodal tasks, yet they also exhibit notable social biases. These biases\noften manifest as unintended associations between neutral concepts and\nsensitive human attributes, leading to disparate model behaviors across\ndemographic groups. While existing studies primarily focus on detecting and\nquantifying such biases, they offer limited insight into the underlying\nmechanisms within the models. To address this gap, we propose an explanatory\nframework that combines information flow analysis with multi-round dialogue\nevaluation, aiming to understand the origin of social bias from the perspective\nof imbalanced internal information utilization. Specifically, we first identify\nhigh-contribution image tokens involved in the model's reasoning process for\nneutral questions via information flow analysis. Then, we design a multi-turn\ndialogue mechanism to evaluate the extent to which these key tokens encode\nsensitive information. Extensive experiments reveal that LVLMs exhibit\nsystematic disparities in information usage when processing images of different\ndemographic groups, suggesting that social bias is deeply rooted in the model's\ninternal reasoning dynamics. Furthermore, we complement our findings from a\ntextual modality perspective, showing that the model's semantic representations\nalready display biased proximity patterns, thereby offering a cross-modal\nexplanation of bias formation.", "published": "2025-05-27 12:28:44", "link": "http://arxiv.org/abs/2505.21106v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Stopping Criteria for Value Iteration on Concurrent Stochastic Reachability and Safety Games", "abstract": "We consider two-player zero-sum concurrent stochastic games (CSGs) played on\ngraphs with reachability and safety objectives. These include degenerate\nclasses such as Markov decision processes or turn-based stochastic games, which\ncan be solved by linear or quadratic programming; however, in practice, value\niteration (VI) outperforms the other approaches and is the most implemented\nmethod. Similarly, for CSGs, this practical performance makes VI an attractive\nalternative to the standard theoretical solution via the existential theory of\nreals.\n  VI starts with an under-approximation of the sought values for each state and\niteratively updates them, traditionally terminating once two consecutive\napproximations are $\\epsilon$-close. However, this stopping criterion lacks\nguarantees on the precision of the approximation, which is the goal of this\nwork. We provide bounded (a.k.a. interval) VI for CSGs: it complements standard\nVI with a converging sequence of over-approximations and terminates once the\nover- and under-approximations are $\\epsilon$-close.", "published": "2025-05-27 12:13:47", "link": "http://arxiv.org/abs/2505.21087v1", "categories": ["cs.LO", "cs.AI", "cs.MA"], "primary_category": "cs.LO"}
{"title": "Efficient Large Language Model Inference with Neural Block Linearization", "abstract": "The high inference demands of transformer-based Large Language Models (LLMs)\npose substantial challenges in their deployment. To this end, we introduce\nNeural Block Linearization (NBL), a novel framework for accelerating\ntransformer model inference by replacing self-attention layers with linear\napproximations derived from Linear Minimum Mean Squared Error estimators. NBL\nleverages Canonical Correlation Analysis to compute a theoretical upper bound\non the approximation error. Then, we use this bound as a criterion for\nsubstitution, selecting the LLM layers with the lowest linearization error. NBL\ncan be efficiently applied to pre-trained LLMs without the need for\nfine-tuning. In experiments, NBL achieves notable computational speed-ups while\npreserving competitive accuracy on multiple reasoning benchmarks. For instance,\napplying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B\nincreases the inference speed by 32% with less than 1% accuracy trade-off,\nmaking it a flexible and promising solution to improve the inference efficiency\nof LLMs.", "published": "2025-05-27 12:01:43", "link": "http://arxiv.org/abs/2505.21077v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling", "abstract": "Text-to-image (T2I) models raise ethical and safety concerns due to their\npotential to generate inappropriate or harmful images. Evaluating these models'\nsecurity through red-teaming is vital, yet white-box approaches are limited by\ntheir need for internal access, complicating their use with closed-source\nmodels. Moreover, existing black-box methods often assume knowledge about the\nmodel's specific defense mechanisms, limiting their utility in real-world\ncommercial API scenarios. A significant challenge is how to evade unknown and\ndiverse defense mechanisms. To overcome this difficulty, we propose a novel\nRule-based Preference modeling Guided Red-Teaming (RPG-RT), which iteratively\nemploys LLM to modify prompts to query and leverages feedback from T2I systems\nfor fine-tuning the LLM. RPG-RT treats the feedback from each iteration as a\nprior, enabling the LLM to dynamically adapt to unknown defense mechanisms.\nGiven that the feedback is often labeled and coarse-grained, making it\ndifficult to utilize directly, we further propose rule-based preference\nmodeling, which employs a set of rules to evaluate desired or undesired\nfeedback, facilitating finer-grained control over the LLM's dynamic adaptation\nprocess. Extensive experiments on nineteen T2I systems with varied safety\nmechanisms, three online commercial API services, and T2V models verify the\nsuperiority and practicality of our approach.", "published": "2025-05-27 12:00:19", "link": "http://arxiv.org/abs/2505.21074v1", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Why Distillation can Outperform Zero-RL: The Role of Flexible Reasoning", "abstract": "Reinforcement learning (RL) has played an important role in improving the\nreasoning ability of large language models (LLMs). Some studies apply RL\ndirectly to \\textit{smaller} base models (known as zero-RL) and also achieve\nnotable progress. However, in this paper, we show that using only 920 examples,\na simple distillation method based on the base model can clearly outperform\nzero-RL, which typically requires much more data and computational cost. By\nanalyzing the token frequency in model outputs, we find that the distilled\nmodel shows more flexible reasoning. It uses anthropomorphic tokens and logical\nconnectors much more often than the zero-RL model. Further analysis reveals\nthat distillation enhances the presence of two advanced cognitive behaviors:\nMulti-Perspective Thinking or Attempting and Metacognitive Awareness. Frequent\noccurrences of these two advanced cognitive behaviors give rise to flexible\nreasoning, which is essential for solving complex reasoning problems, while\nzero-RL fails to significantly boost the frequency of these behaviors.", "published": "2025-05-27 11:52:41", "link": "http://arxiv.org/abs/2505.21067v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "LPOI: Listwise Preference Optimization for Vision Language Models", "abstract": "Aligning large VLMs with human preferences is a challenging task, as methods\nlike RLHF and DPO often overfit to textual information or exacerbate\nhallucinations. Although augmenting negative image samples partially addresses\nthese pitfalls, no prior work has employed listwise preference optimization for\nVLMs, due to the complexity and cost of constructing listwise image samples. In\nthis work, we propose LPOI, the first object-aware listwise preference\noptimization developed for reducing hallucinations in VLMs. LPOI identifies and\nmasks a critical object in the image, and then interpolates the masked region\nbetween the positive and negative images to form a sequence of incrementally\nmore complete images. The model is trained to rank these images in ascending\norder of object visibility, effectively reducing hallucinations while retaining\nvisual fidelity. LPOI requires no extra annotations beyond standard pairwise\npreference data, as it automatically constructs the ranked lists through object\nmasking and interpolation. Comprehensive experiments on MMHalBench, AMBER, and\nObject HalBench confirm that LPOI outperforms existing preference optimization\nmethods in reducing hallucinations and enhancing VLM performance. We make the\ncode available at https://github.com/fatemehpesaran310/lpoi.", "published": "2025-05-27 11:47:28", "link": "http://arxiv.org/abs/2505.21061v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Agent-Environment Alignment via Automated Interface Generation", "abstract": "Large language model (LLM) agents have shown impressive reasoning\ncapabilities in interactive decision-making tasks. These agents interact with\nenvironment through intermediate interfaces, such as predefined action spaces\nand interaction rules, which mediate the perception and action. However,\nmismatches often happen between the internal expectations of the agent\nregarding the influence of its issued actions and the actual state transitions\nin the environment, a phenomenon referred to as \\textbf{agent-environment\nmisalignment}. While prior work has invested substantially in improving agent\nstrategies and environment design, the critical role of the interface still\nremains underexplored. In this work, we empirically demonstrate that\nagent-environment misalignment poses a significant bottleneck to agent\nperformance. To mitigate this issue, we propose \\textbf{ALIGN}, an\n\\underline{A}uto-A\\underline{l}igned \\underline{I}nterface\n\\underline{G}e\\underline{n}eration framework that alleviates the misalignment\nby enriching the interface. Specifically, the ALIGN-generated interface\nenhances both the static information of the environment and the step-wise\nobservations returned to the agent. Implemented as a lightweight wrapper, this\ninterface achieves the alignment without modifying either the agent logic or\nthe environment code. Experiments across multiple domains including embodied\ntasks, web navigation and tool-use, show consistent performance improvements,\nwith up to a 45.67\\% success rate improvement observed in ALFWorld. Meanwhile,\nALIGN-generated interface can generalize across different agent architectures\nand LLM backbones without interface regeneration. Code and experimental results\nare available at https://github.com/THUNLP-MT/ALIGN.", "published": "2025-05-27 11:44:50", "link": "http://arxiv.org/abs/2505.21055v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "A domain adaptation neural network for digital twin-supported fault diagnosis", "abstract": "Digital twins offer a promising solution to the lack of sufficient labeled\ndata in deep learning-based fault diagnosis by generating simulated data for\nmodel training. However, discrepancies between simulation and real-world\nsystems can lead to a significant drop in performance when models are applied\nin real scenarios. To address this issue, we propose a fault diagnosis\nframework based on Domain-Adversarial Neural Networks (DANN), which enables\nknowledge transfer from simulated (source domain) to real-world (target domain)\ndata. We evaluate the proposed framework using a publicly available robotics\nfault diagnosis dataset, which includes 3,600 sequences generated by a digital\ntwin model and 90 real sequences collected from physical systems. The DANN\nmethod is compared with commonly used lightweight deep learning models such as\nCNN, TCN, Transformer, and LSTM. Experimental results show that incorporating\ndomain adaptation significantly improves the diagnostic performance. For\nexample, applying DANN to a baseline CNN model improves its accuracy from\n70.00% to 80.22% on real-world test data, demonstrating the effectiveness of\ndomain adaptation in bridging the sim-to-real gap.", "published": "2025-05-27 11:27:05", "link": "http://arxiv.org/abs/2505.21046v1", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY"], "primary_category": "cs.LG"}
{"title": "Large Language Model-enhanced Reinforcement Learning for Low-Altitude Economy Networking", "abstract": "Low-Altitude Economic Networking (LAENet) aims to support diverse flying\napplications below 1,000 meters by deploying various aerial vehicles for\nflexible and cost-effective aerial networking. However, complex\ndecision-making, resource constraints, and environmental uncertainty pose\nsignificant challenges to the development of the LAENet. Reinforcement learning\n(RL) offers a potential solution in response to these challenges but has\nlimitations in generalization, reward design, and model stability. The\nemergence of large language models (LLMs) offers new opportunities for RL to\nmitigate these limitations. In this paper, we first present a tutorial about\nintegrating LLMs into RL by using the capacities of generation, contextual\nunderstanding, and structured reasoning of LLMs. We then propose an\nLLM-enhanced RL framework for the LAENet in terms of serving the LLM as\ninformation processor, reward designer, decision-maker, and generator.\nMoreover, we conduct a case study by using LLMs to design a reward function to\nimprove the learning performance of RL in the LAENet. Finally, we provide a\nconclusion and discuss future work.", "published": "2025-05-27 11:25:42", "link": "http://arxiv.org/abs/2505.21045v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Fixed-Point Traps and Identity Emergence in Educational Feedback Systems", "abstract": "This paper presents a formal categorical proof that exam-driven educational\nsystems obstruct identity emergence and block creative convergence. Using the\nframework of Alpay Algebra II and III, we define Exam-Grade Collapse Systems\n(EGCS) as functorial constructs where learning dynamics $\\varphi$ are\nrecursively collapsed by evaluative morphisms $E$. We prove that under such\ncollapse regimes, no nontrivial fixed-point algebra $\\mu_\\varphi$ can exist,\nhence learner identity cannot stabilize. This creates a universal fixed-point\ntrap: all generative functors are entropically folded before symbolic emergence\noccurs. Our model mathematically explains the creativity suppression, research\nstagnation, and structural entropy loss induced by timed exams and grade-based\nfeedback. The results apply category theory to expose why modern educational\nsystems prevent {\\phi}-emergence and block observer-invariant self-formation.\nThis work provides the first provable algebraic obstruction of identity\nformation caused by institutional feedback mechanics.", "published": "2025-05-27 11:19:33", "link": "http://arxiv.org/abs/2505.21038v1", "categories": ["math.CT", "cs.AI", "cs.CY", "18A15, 18C15, 91D30, 97C70, 03B70, 68T01", "F.4.1; I.2.0; K.3.2"], "primary_category": "math.CT"}
{"title": "RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy", "abstract": "Video generation using diffusion models is highly computationally intensive,\nwith 3D attention in Diffusion Transformer (DiT) models accounting for over\n80\\% of the total computational resources. In this work, we introduce {\\bf\nRainFusion}, a novel training-free sparse attention method that exploits\ninherent sparsity nature in visual data to accelerate attention computation\nwhile preserving video quality. Specifically, we identify three unique sparse\npatterns in video generation attention calculations--Spatial Pattern, Temporal\nPattern and Textural Pattern. The sparse pattern for each attention head is\ndetermined online with negligible overhead (\\textasciitilde\\,0.2\\%) with our\nproposed {\\bf ARM} (Adaptive Recognition Module) during inference. Our proposed\n{\\bf RainFusion} is a plug-and-play method, that can be seamlessly integrated\ninto state-of-the-art 3D-attention video generation models without additional\ntraining or calibration. We evaluate our method on leading open-sourced models\nincluding HunyuanVideo, OpenSoraPlan-1.2 and CogVideoX-5B, demonstrating its\nbroad applicability and effectiveness. Experimental results show that\nRainFusion achieves over {\\bf 2\\(\\times\\)} speedup in attention computation\nwhile maintaining video quality, with only a minimal impact on VBench scores\n(-0.2\\%).", "published": "2025-05-27 11:15:02", "link": "http://arxiv.org/abs/2505.21036v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "FeatInv: Spatially resolved mapping from feature space to input space using conditional diffusion models", "abstract": "Internal representations are crucial for understanding deep neural networks,\nsuch as their properties and reasoning patterns, but remain difficult to\ninterpret. While mapping from feature space to input space aids in interpreting\nthe former, existing approaches often rely on crude approximations. We propose\nusing a conditional diffusion model - a pretrained high-fidelity diffusion\nmodel conditioned on spatially resolved feature maps - to learn such a mapping\nin a probabilistic manner. We demonstrate the feasibility of this approach\nacross various pretrained image classifiers from CNNs to ViTs, showing\nexcellent reconstruction capabilities. Through qualitative comparisons and\nrobustness analysis, we validate our method and showcase possible applications,\nsuch as the visualization of concept steering in input space or investigations\nof the composite nature of the feature space. This approach has broad potential\nfor improving feature space understanding in computer vision models.", "published": "2025-05-27 11:07:34", "link": "http://arxiv.org/abs/2505.21032v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "TabAttackBench: A Benchmark for Adversarial Attacks on Tabular Data", "abstract": "Adversarial attacks pose a significant threat to machine learning models by\ninducing incorrect predictions through imperceptible perturbations to input\ndata. While these attacks have been extensively studied in unstructured data\nlike images, their application to tabular data presents new challenges. These\nchallenges arise from the inherent heterogeneity and complex feature\ninterdependencies in tabular data, which differ significantly from those in\nimage data. To address these differences, it is crucial to consider\nimperceptibility as a key criterion specific to tabular data. Most current\nresearch focuses primarily on achieving effective adversarial attacks, often\noverlooking the importance of maintaining imperceptibility. To address this\ngap, we propose a new benchmark for adversarial attacks on tabular data that\nevaluates both effectiveness and imperceptibility. In this study, we assess the\neffectiveness and imperceptibility of five adversarial attacks across four\nmodels using eleven tabular datasets, including both mixed and numerical-only\ndatasets. Our analysis explores how these factors interact and influence the\noverall performance of the attacks. We also compare the results across\ndifferent dataset types to understand the broader implications of these\nfindings. The findings from this benchmark provide valuable insights for\nimproving the design of adversarial attack algorithms, thereby advancing the\nfield of adversarial machine learning on tabular data.", "published": "2025-05-27 11:01:32", "link": "http://arxiv.org/abs/2505.21027v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Multi-Mode Process Control Using Multi-Task Inverse Reinforcement Learning", "abstract": "In the era of Industry 4.0 and smart manufacturing, process systems\nengineering must adapt to digital transformation. While reinforcement learning\noffers a model-free approach to process control, its applications are limited\nby the dependence on accurate digital twins and well-designed reward functions.\nTo address these limitations, this paper introduces a novel framework that\nintegrates inverse reinforcement learning (IRL) with multi-task learning for\ndata-driven, multi-mode control design. Using historical closed-loop data as\nexpert demonstrations, IRL extracts optimal reward functions and control\npolicies. A latent-context variable is incorporated to distinguish modes,\nenabling the training of mode-specific controllers. Case studies on a\ncontinuous stirred tank reactor and a fed-batch bioreactor validate the\neffectiveness of this framework in handling multi-mode data and training\nadaptable controllers.", "published": "2025-05-27 11:01:00", "link": "http://arxiv.org/abs/2505.21026v1", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "primary_category": "eess.SY"}
{"title": "Text-Queried Audio Source Separation via Hierarchical Modeling", "abstract": "Target audio source separation with natural language queries presents a\npromising paradigm for extracting arbitrary audio events through arbitrary text\ndescriptions. Existing methods mainly face two challenges, the difficulty in\njointly modeling acoustic-textual alignment and semantic-aware separation\nwithin a blindly-learned single-stage architecture, and the reliance on\nlarge-scale accurately-labeled training data to compensate for inefficient\ncross-modal learning and separation. To address these challenges, we propose a\nhierarchical decomposition framework, HSM-TSS, that decouples the task into\nglobal-local semantic-guided feature separation and structure-preserving\nacoustic reconstruction. Our approach introduces a dual-stage mechanism for\nsemantic separation, operating on distinct global and local semantic feature\nspaces. We first perform global-semantic separation through a global semantic\nfeature space aligned with text queries. A Q-Audio architecture is employed to\nalign audio and text modalities, serving as pretrained global-semantic\nencoders. Conditioned on the predicted global feature, we then perform the\nsecond-stage local-semantic separation on AudioMAE features that preserve\ntime-frequency structures, followed by acoustic reconstruction. We also propose\nan instruction processing pipeline to parse arbitrary text queries into\nstructured operations, extraction or removal, coupled with audio descriptions,\nenabling flexible sound manipulation. Our method achieves state-of-the-art\nseparation performance with data-efficient training while maintaining superior\nsemantic consistency with queries in complex auditory scenes.", "published": "2025-05-27 11:00:38", "link": "http://arxiv.org/abs/2505.21025v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Federated Instrumental Variable Analysis via Federated Generalized Method of Moments", "abstract": "Instrumental variables (IV) analysis is an important applied tool for areas\nsuch as healthcare and consumer economics. For IV analysis in high-dimensional\nsettings, the Generalized Method of Moments (GMM) using deep neural networks\noffers an efficient approach. With non-i.i.d. data sourced from scattered\ndecentralized clients, federated learning is a popular paradigm for training\nthe models while promising data privacy. However, to our knowledge, no\nfederated algorithm for either GMM or IV analysis exists to date. In this work,\nwe introduce federated instrumental variables analysis (FedIV) via federated\ngeneralized method of moments (FedGMM). We formulate FedGMM as a federated\nzero-sum game defined by a federated non-convex non-concave minimax\noptimization problem, which is solved using federated gradient descent ascent\n(FedGDA) algorithm. One key challenge arises in theoretically characterizing\nthe federated local optimality. To address this, we present properties and\nexistence results of clients' local equilibria via FedGDA limit points.\nThereby, we show that the federated solution consistently estimates the local\nmoment conditions of every participating client. The proposed algorithm is\nbacked by extensive experiments to demonstrate the efficacy of our approach.", "published": "2025-05-27 10:46:43", "link": "http://arxiv.org/abs/2505.21012v1", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "BIPNN: Learning to Solve Binary Integer Programming via Hypergraph Neural Networks", "abstract": "Binary (0-1) integer programming (BIP) is pivotal in scientific domains\nrequiring discrete decision-making. As the advance of AI computing, recent\nworks explore neural network-based solvers for integer linear programming (ILP)\nproblems. Yet, they lack scalability for tackling nonlinear challenges. To\nhandle nonlinearities, state-of-the-art Branch-and-Cut solvers employ linear\nrelaxations, leading to exponential growth in auxiliary variables and severe\ncomputation limitations. To overcome these limitations, we propose BIPNN\n(Binary Integer Programming Neural Network), an unsupervised learning framework\nto solve nonlinear BIP problems via hypergraph neural networks (HyperGNN).\nSpecifically, BIPNN reformulates BIPs-constrained, discrete, and nonlinear\n(sin, log, exp) optimization problems-into unconstrained, differentiable, and\npolynomial loss functions. The reformulation stems from the observation of a\nprecise one-to-one mapping between polynomial BIP objectives and hypergraph\nstructures, enabling the unsupervised training of HyperGNN to optimize BIP\nproblems in an end-to-end manner. On this basis, we propose a GPU-accelerated\nand continuous-annealing-enhanced training pipeline for BIPNN. The pipeline\nenables BIPNN to optimize large-scale nonlinear terms in BIPs fully in parallel\nvia straightforward gradient descent, thus significantly reducing the training\ncost while ensuring the generation of discrete, high-quality solutions.\nExtensive experiments on synthetic and real-world datasets highlight the\nsuperiority of our approach.", "published": "2025-05-27 10:31:52", "link": "http://arxiv.org/abs/2505.20997v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "MelodySim: Measuring Melody-aware Music Similarity for Plagiarism Detection", "abstract": "We propose MelodySim, a melody-aware music similarity model and dataset for\nplagiarism detection. First, we introduce a novel method to construct a dataset\nwith focus on melodic similarity. By augmenting Slakh2100; an existing MIDI\ndataset, we generate variations of each piece while preserving the melody\nthrough modifications such as note splitting, arpeggiation, minor track dropout\n(excluding bass), and re-instrumentation. A user study confirms that positive\npairs indeed contain similar melodies, with other musical tracks significantly\nchanged. Second, we develop a segment-wise melodic-similarity detection model\nthat uses a MERT encoder and applies a triplet neural network to capture\nmelodic similarity. The resultant decision matrix highlights where plagiarism\nmight occur. Our model achieves high accuracy on the MelodySim test set.", "published": "2025-05-27 10:14:03", "link": "http://arxiv.org/abs/2505.20979v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Conversational Development Environments: Using Theory-of-Mind and Multi-Agent Architectures for Requirements Refinement", "abstract": "Foundation Models (FMs) have shown remarkable capabilities in various natural\nlanguage tasks. However, their ability to accurately capture stakeholder\nrequirements remains a significant challenge for using FMs for software\ndevelopment. This paper introduces a novel approach that leverages an\nFM-powered multi-agent system called AlignMind to address this issue. By having\na cognitive architecture that enhances FMs with Theory-of-Mind capabilities,\nour approach considers the mental states and perspectives of software makers.\nThis allows our solution to iteratively clarify the beliefs, desires, and\nintentions of stakeholders, translating these into a set of refined\nrequirements and a corresponding actionable natural language workflow in the\noften-overlooked requirements refinement phase of software engineering, which\nis crucial after initial elicitation. Through a multifaceted evaluation\ncovering 150 diverse use cases, we demonstrate that our approach can accurately\ncapture the intents and requirements of stakeholders, articulating them as both\nspecifications and a step-by-step plan of action. Our findings suggest that the\npotential for significant improvements in the software development process\njustifies these investments. Our work lays the groundwork for future innovation\nin building intent-first development environments, where software makers can\nseamlessly collaborate with AIs to create software that truly meets their\nneeds.", "published": "2025-05-27 10:05:26", "link": "http://arxiv.org/abs/2505.20973v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "Deep k-grouping: An Unsupervised Learning Framework for Combinatorial Optimization on Graphs and Hypergraphs", "abstract": "Along with AI computing shining in scientific discovery, its potential in the\ncombinatorial optimization (CO) domain has also emerged in recent years. Yet,\nexisting unsupervised neural network solvers struggle to solve $k$-grouping\nproblems (e.g., coloring, partitioning) on large-scale graphs and hypergraphs,\ndue to limited computational frameworks. In this work, we propose Deep\n$k$-grouping, an unsupervised learning-based CO framework. Specifically, we\ncontribute: Novel one-hot encoded polynomial unconstrained binary optimization\n(OH-PUBO), a formulation for modeling k-grouping problems on graphs and\nhypergraphs (e.g., graph/hypergraph coloring and partitioning); GPU-accelerated\nalgorithms for large-scale k-grouping CO problems. Deep $k$-grouping employs\nthe relaxation of large-scale OH-PUBO objectives as differentiable loss\nfunctions and trains to optimize them in an unsupervised manner. To ensure\nscalability, it leverages GPU-accelerated algorithms to unify the training\npipeline; A Gini coefficient-based continuous relaxation annealing strategy to\nenforce discreteness of solutions while preventing convergence to local optima.\nExperimental results demonstrate that Deep $k$-grouping outperforms existing\nneural network solvers and classical heuristics such as SCIP and Tabu.", "published": "2025-05-27 10:04:54", "link": "http://arxiv.org/abs/2505.20972v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Efficient and Microphone-Fault-Tolerant 3D Sound Source Localization", "abstract": "Sound source localization (SSL) is a critical technology for determining the\nposition of sound sources in complex environments. However, existing methods\nface challenges such as high computational costs and precise calibration\nrequirements, limiting their deployment in dynamic or resource-constrained\nenvironments. This paper introduces a novel 3D SSL framework, which uses sparse\ncross-attention, pretraining, and adaptive signal coherence metrics, to achieve\naccurate and computationally efficient localization with fewer input\nmicrophones. The framework is also fault-tolerant to unreliable or even unknown\nmicrophone position inputs, ensuring its applicability in real-world scenarios.\nPreliminary experiments demonstrate its scalability for multi-source\nlocalization without requiring additional hardware. This work advances SSL by\nbalancing the model's performance and efficiency and improving its robustness\nfor real-world scenarios.", "published": "2025-05-27 09:56:16", "link": "http://arxiv.org/abs/2505.20961v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hybrid Disagreement-Diversity Active Learning for Bioacoustic Sound Event Detection", "abstract": "Bioacoustic sound event detection (BioSED) is crucial for biodiversity\nconservation but faces practical challenges during model development and\ntraining: limited amounts of annotated data, sparse events, species diversity,\nand class imbalance. To address these challenges efficiently with a limited\nlabeling budget, we apply the mismatch-first farthest-traversal (MFFT), an\nactive learning method integrating committee voting disagreement and diversity\nanalysis. We also refine an existing BioSED dataset specifically for evaluating\nactive learning algorithms. Experimental results demonstrate that MFFT achieves\na mAP of 68% when cold-starting and 71% when warm-starting (which is close to\nthe fully-supervised mAP of 75%) while using only 2.3% of the annotations.\nNotably, MFFT excels in cold-start scenarios and with rare species, which are\ncritical for monitoring endangered species, demonstrating its practical value.", "published": "2025-05-27 09:50:39", "link": "http://arxiv.org/abs/2505.20956v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Streamlining Knowledge Graph Creation with PyRML", "abstract": "Knowledge Graphs (KGs) are increasingly adopted as a foundational technology\nfor integrating heterogeneous data in domains such as climate science, cultural\nheritage, and the life sciences. Declarative mapping languages like R2RML and\nRML have played a central role in enabling scalable and reusable KG\nconstruction, offering a transparent means of transforming structured and\nsemi-structured data into RDF. In this paper, we present PyRML, a lightweight,\nPython-native library for building Knowledge Graphs through declarative\nmappings. PyRML supports core RML constructs and provides a programmable\ninterface for authoring, executing, and testing mappings directly within Python\nenvironments. It integrates with popular data and semantic web libraries (e.g.,\nPandas and RDFlib), enabling transparent and modular workflows. By lowering the\nbarrier to entry for KG creation and fostering reproducible, ontology-aligned\ndata integration, PyRML bridges the gap between declarative semantics and\npractical KG engineering.", "published": "2025-05-27 09:40:29", "link": "http://arxiv.org/abs/2505.20949v1", "categories": ["cs.DB", "cs.AI"], "primary_category": "cs.DB"}
{"title": "Controllable Logical Hypothesis Generation for Abductive Reasoning in Knowledge Graphs", "abstract": "Abductive reasoning in knowledge graphs aims to generate plausible logical\nhypotheses from observed entities, with broad applications in areas such as\nclinical diagnosis and scientific discovery. However, due to a lack of\ncontrollability, a single observation may yield numerous plausible but\nredundant or irrelevant hypotheses on large-scale knowledge graphs. To address\nthis limitation, we introduce the task of controllable hypothesis generation to\nimprove the practical utility of abductive reasoning. This task faces two key\nchallenges when controlling for generating long and complex logical hypotheses:\nhypothesis space collapse and hypothesis oversensitivity. To address these\nchallenges, we propose CtrlHGen, a Controllable logcial Hypothesis Generation\nframework for abductive reasoning over knowledge graphs, trained in a two-stage\nparadigm including supervised learning and subsequent reinforcement learning.\nTo mitigate hypothesis space collapse, we design a dataset augmentation\nstrategy based on sub-logical decomposition, enabling the model to learn\ncomplex logical structures by leveraging semantic patterns in simpler\ncomponents. To address hypothesis oversensitivity, we incorporate smoothed\nsemantic rewards including Dice and Overlap scores, and introduce a\ncondition-adherence reward to guide the generation toward user-specified\ncontrol constraints. Extensive experiments on three benchmark datasets\ndemonstrate that our model not only better adheres to control conditions but\nalso achieves superior semantic similarity performance compared to baselines.", "published": "2025-05-27 09:36:47", "link": "http://arxiv.org/abs/2505.20948v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Generalizable and Relightable Gaussian Splatting for Human Novel View Synthesis", "abstract": "We propose GRGS, a generalizable and relightable 3D Gaussian framework for\nhigh-fidelity human novel view synthesis under diverse lighting conditions.\nUnlike existing methods that rely on per-character optimization or ignore\nphysical constraints, GRGS adopts a feed-forward, fully supervised strategy\nthat projects geometry, material, and illumination cues from multi-view 2D\nobservations into 3D Gaussian representations. Specifically, to reconstruct\nlighting-invariant geometry, we introduce a Lighting-aware Geometry Refinement\n(LGR) module trained on synthetically relit data to predict accurate depth and\nsurface normals. Based on the high-quality geometry, a Physically Grounded\nNeural Rendering (PGNR) module is further proposed to integrate neural\nprediction with physics-based shading, supporting editable relighting with\nshadows and indirect illumination. Besides, we design a 2D-to-3D projection\ntraining scheme that leverages differentiable supervision from ambient\nocclusion, direct, and indirect lighting maps, which alleviates the\ncomputational cost of explicit ray tracing. Extensive experiments demonstrate\nthat GRGS achieves superior visual quality, geometric consistency, and\ngeneralization across characters and lighting conditions.", "published": "2025-05-27 17:59:47", "link": "http://arxiv.org/abs/2505.21502v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Vision Transformers with Self-Distilled Registers", "abstract": "Vision Transformers (ViTs) have emerged as the dominant architecture for\nvisual processing tasks, demonstrating excellent scalability with increased\ntraining data and model size. However, recent work has identified the emergence\nof artifact tokens in ViTs that are incongruous with the local semantics. These\nanomalous tokens degrade ViT performance in tasks that require fine-grained\nlocalization or structural coherence. An effective mitigation of this issue is\nto the addition of register tokens to ViTs, which implicitly \"absorb\" the\nartifact term during training. Given the availability of various large-scale\npre-trained ViTs, in this paper we aim at equipping them with such register\ntokens without the need of re-training them from scratch, which is infeasible\nconsidering their size. Specifically, we propose Post Hoc Registers (PH-Reg),\nan efficient self-distillation method that integrates registers into an\nexisting ViT without requiring additional labeled data and full retraining.\nPH-Reg initializes both teacher and student networks from the same pre-trained\nViT. The teacher remains frozen and unmodified, while the student is augmented\nwith randomly initialized register tokens. By applying test-time augmentation\nto the teacher's inputs, we generate denoised dense embeddings free of\nartifacts, which are then used to optimize only a small subset of unlocked\nstudent weights. We show that our approach can effectively reduce the number of\nartifact tokens, improving the segmentation and depth prediction of the student\nViT under zero-shot and linear probing.", "published": "2025-05-27 17:59:41", "link": "http://arxiv.org/abs/2505.21501v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment", "abstract": "Multimodal large language models (MLLMs) remain vulnerable to transferable\nadversarial examples. While existing methods typically achieve targeted attacks\nby aligning global features-such as CLIP's [CLS] token-between adversarial and\ntarget samples, they often overlook the rich local information encoded in patch\ntokens. This leads to suboptimal alignment and limited transferability,\nparticularly for closed-source models. To address this limitation, we propose a\ntargeted transferable adversarial attack method based on feature optimal\nalignment, called FOA-Attack, to improve adversarial transfer capability.\nSpecifically, at the global level, we introduce a global feature loss based on\ncosine similarity to align the coarse-grained features of adversarial samples\nwith those of target samples. At the local level, given the rich local\nrepresentations within Transformers, we leverage clustering techniques to\nextract compact local patterns to alleviate redundant local features. We then\nformulate local feature alignment between adversarial and target samples as an\noptimal transport (OT) problem and propose a local clustering optimal transport\nloss to refine fine-grained feature alignment. Additionally, we propose a\ndynamic ensemble model weighting strategy to adaptively balance the influence\nof multiple models during adversarial example generation, thereby further\nimproving transferability. Extensive experiments across various models\ndemonstrate the superiority of the proposed method, outperforming\nstate-of-the-art methods, especially in transferring to closed-source MLLMs.\nThe code is released at https://github.com/jiaxiaojunQAQ/FOA-Attack.", "published": "2025-05-27 17:56:57", "link": "http://arxiv.org/abs/2505.21494v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Frame In-N-Out: Unbounded Controllable Image-to-Video Generation", "abstract": "Controllability, temporal coherence, and detail synthesis remain the most\ncritical challenges in video generation. In this paper, we focus on a commonly\nused yet underexplored cinematic technique known as Frame In and Frame Out.\nSpecifically, starting from image-to-video generation, users can control the\nobjects in the image to naturally leave the scene or provide breaking new\nidentity references to enter the scene, guided by user-specified motion\ntrajectory. To support this task, we introduce a new dataset curated\nsemi-automatically, a comprehensive evaluation protocol targeting this setting,\nand an efficient identity-preserving motion-controllable video Diffusion\nTransformer architecture. Our evaluation shows that our proposed approach\nsignificantly outperforms existing baselines.", "published": "2025-05-27 17:56:07", "link": "http://arxiv.org/abs/2505.21491v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "MV-CoLight: Efficient Object Compositing with Consistent Lighting and Shadow Generation", "abstract": "Object compositing offers significant promise for augmented reality (AR) and\nembodied intelligence applications. Existing approaches predominantly focus on\nsingle-image scenarios or intrinsic decomposition techniques, facing challenges\nwith multi-view consistency, complex scenes, and diverse lighting conditions.\nRecent inverse rendering advancements, such as 3D Gaussian and diffusion-based\nmethods, have enhanced consistency but are limited by scalability, heavy data\nrequirements, or prolonged reconstruction time per scene. To broaden its\napplicability, we introduce MV-CoLight, a two-stage framework for\nillumination-consistent object compositing in both 2D images and 3D scenes. Our\nnovel feed-forward architecture models lighting and shadows directly, avoiding\nthe iterative biases of diffusion-based methods. We employ a Hilbert\ncurve-based mapping to align 2D image inputs with 3D Gaussian scene\nrepresentations seamlessly. To facilitate training and evaluation, we further\nintroduce a large-scale 3D compositing dataset. Experiments demonstrate\nstate-of-the-art harmonized results across standard benchmarks and our dataset,\nas well as casually captured real-world scenes demonstrate the framework's\nrobustness and wide generalization.", "published": "2025-05-27 17:53:02", "link": "http://arxiv.org/abs/2505.21483v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via Next-Detail Prediction", "abstract": "This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image\ngeneration method that models images through a novel next-detail prediction\nstrategy. By learning a resolution-aware token sequence supervised with\nprogressively degraded images, DetailFlow enables the generation process to\nstart from the global structure and incrementally refine details. This\ncoarse-to-fine 1D token sequence aligns well with the autoregressive inference\nmechanism, providing a more natural and efficient way for the AR model to\ngenerate complex visual content. Our compact 1D AR model achieves high-quality\nimage synthesis with significantly fewer tokens than previous approaches, i.e.\nVAR/VQGAN. We further propose a parallel inference mechanism with\nself-correction that accelerates generation speed by approximately 8x while\nreducing accumulation sampling error inherent in teacher-forcing supervision.\nOn the ImageNet 256x256 benchmark, our method achieves 2.96 gFID with 128\ntokens, outperforming VAR (3.3 FID) and FlexVAR (3.05 FID), which both require\n680 tokens in their AR models. Moreover, due to the significantly reduced token\ncount and parallel inference mechanism, our method runs nearly 2x faster\ninference speed compared to VAR and FlexVAR. Extensive experimental results\ndemonstrate DetailFlow's superior generation quality and efficiency compared to\nexisting state-of-the-art methods.", "published": "2025-05-27 17:45:21", "link": "http://arxiv.org/abs/2505.21473v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Visual Product Graph: Bridging Visual Products And Composite Images For End-to-End Style Recommendations", "abstract": "Retrieving semantically similar but visually distinct contents has been a\ncritical capability in visual search systems. In this work, we aim to tackle\nthis problem with Visual Product Graph (VPG), leveraging high-performance\ninfrastructure for storage and state-of-the-art computer vision models for\nimage understanding. VPG is built to be an online real-time retrieval system\nthat enables navigation from individual products to composite scenes containing\nthose products, along with complementary recommendations. Our system not only\noffers contextual insights by showcasing how products can be styled in a\ncontext, but also provides recommendations for complementary products drawn\nfrom these inspirations. We discuss the essential components for building the\nVisual Product Graph, along with the core computer vision model improvements\nacross object detection, foundational visual embeddings, and other visual\nsignals. Our system achieves a 78.8% extremely similar@1 in end-to-end human\nrelevance evaluations, and a 6% module engagement rate. The \"Ways to Style It\"\nmodule, powered by the Visual Product Graph technology, is deployed in\nproduction at Pinterest.", "published": "2025-05-27 17:26:55", "link": "http://arxiv.org/abs/2505.21454v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers", "abstract": "Lip synchronization is the task of aligning a speaker's lip movements in\nvideo with corresponding speech audio, and it is essential for creating\nrealistic, expressive video content. However, existing methods often rely on\nreference frames and masked-frame inpainting, which limit their robustness to\nidentity consistency, pose variations, facial occlusions, and stylized content.\nIn addition, since audio signals provide weaker conditioning than visual cues,\nlip shape leakage from the original video will affect lip sync quality. In this\npaper, we present OmniSync, a universal lip synchronization framework for\ndiverse visual scenarios. Our approach introduces a mask-free training paradigm\nusing Diffusion Transformer models for direct frame editing without explicit\nmasks, enabling unlimited-duration inference while maintaining natural facial\ndynamics and preserving character identity. During inference, we propose a\nflow-matching-based progressive noise initialization to ensure pose and\nidentity consistency, while allowing precise mouth-region editing. To address\nthe weak conditioning signal of audio, we develop a Dynamic Spatiotemporal\nClassifier-Free Guidance (DS-CFG) mechanism that adaptively adjusts guidance\nstrength over time and space. We also establish the AIGC-LipSync Benchmark, the\nfirst evaluation suite for lip synchronization in diverse AI-generated videos.\nExtensive experiments demonstrate that OmniSync significantly outperforms prior\nmethods in both visual quality and lip sync accuracy, achieving superior\nresults in both real-world and AI-generated videos.", "published": "2025-05-27 17:20:38", "link": "http://arxiv.org/abs/2505.21448v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "CoDA: Coordinated Diffusion Noise Optimization for Whole-Body Manipulation of Articulated Objects", "abstract": "Synthesizing whole-body manipulation of articulated objects, including body\nmotion, hand motion, and object motion, is a critical yet challenging task with\nbroad applications in virtual humans and robotics. The core challenges are\ntwofold. First, achieving realistic whole-body motion requires tight\ncoordination between the hands and the rest of the body, as their movements are\ninterdependent during manipulation. Second, articulated object manipulation\ntypically involves high degrees of freedom and demands higher precision, often\nrequiring the fingers to be placed at specific regions to actuate movable\nparts. To address these challenges, we propose a novel coordinated diffusion\nnoise optimization framework. Specifically, we perform noise-space optimization\nover three specialized diffusion models for the body, left hand, and right\nhand, each trained on its own motion dataset to improve generalization.\nCoordination naturally emerges through gradient flow along the human kinematic\nchain, allowing the global body posture to adapt in response to hand motion\nobjectives with high fidelity. To further enhance precision in hand-object\ninteraction, we adopt a unified representation based on basis point sets (BPS),\nwhere end-effector positions are encoded as distances to the same BPS used for\nobject geometry. This unified representation captures fine-grained spatial\nrelationships between the hand and articulated object parts, and the resulting\ntrajectories serve as targets to guide the optimization of diffusion noise,\nproducing highly accurate interaction motion. We conduct extensive experiments\ndemonstrating that our method outperforms existing approaches in motion quality\nand physical plausibility, and enables various capabilities such as object pose\ncontrol, simultaneous walking and manipulation, and whole-body generation from\nhand-only data.", "published": "2025-05-27 17:11:50", "link": "http://arxiv.org/abs/2505.21437v1", "categories": ["cs.GR", "cs.CV", "cs.RO"], "primary_category": "cs.GR"}
{"title": "Automatically Identify and Rectify: Robust Deep Contrastive Multi-view Clustering in Noisy Scenarios", "abstract": "Leveraging the powerful representation learning capabilities, deep multi-view\nclustering methods have demonstrated reliable performance by effectively\nintegrating multi-source information from diverse views in recent years. Most\nexisting methods rely on the assumption of clean views. However, noise is\npervasive in real-world scenarios, leading to a significant degradation in\nperformance. To tackle this problem, we propose a novel multi-view clustering\nframework for the automatic identification and rectification of noisy data,\ntermed AIRMVC. Specifically, we reformulate noisy identification as an anomaly\nidentification problem using GMM. We then design a hybrid rectification\nstrategy to mitigate the adverse effects of noisy data based on the\nidentification results. Furthermore, we introduce a noise-robust contrastive\nmechanism to generate reliable representations. Additionally, we provide a\ntheoretical proof demonstrating that these representations can discard noisy\ninformation, thereby improving the performance of downstream tasks. Extensive\nexperiments on six benchmark datasets demonstrate that AIRMVC outperforms\nstate-of-the-art algorithms in terms of robustness in noisy scenarios. The code\nof AIRMVC are available at https://github.com/xihongyang1999/AIRMVC on Github.", "published": "2025-05-27 16:16:54", "link": "http://arxiv.org/abs/2505.21387v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ZigzagPointMamba: Spatial-Semantic Mamba for Point Cloud Understanding", "abstract": "State Space models (SSMs) such as PointMamba enable efficient feature\nextraction for point cloud self-supervised learning with linear complexity,\noutperforming Transformers in computational efficiency. However, existing\nPointMamba-based methods depend on complex token ordering and random masking,\nwhich disrupt spatial continuity and local semantic correlations. We propose\nZigzagPointMamba to tackle these challenges. The core of our approach is a\nsimple zigzag scan path that globally sequences point cloud tokens, enhancing\nspatial continuity by preserving the proximity of spatially adjacent point\ntokens. Nevertheless, random masking undermines local semantic modeling in\nself-supervised learning. To address this, we introduce a Semantic-Siamese\nMasking Strategy (SMS), which masks semantically similar tokens to facilitate\nreconstruction by integrating local features of original and similar tokens.\nThis overcomes the dependence on isolated local features and enables robust\nglobal semantic modeling. Our pre-trained ZigzagPointMamba weights\nsignificantly improve downstream tasks, achieving a 1.59% mIoU gain on\nShapeNetPart for part segmentation, a 0.4% higher accuracy on ModelNet40 for\nclassification, and 0.19%, 1.22%, and 0.72% higher accuracies respectively for\nthe classification tasks on the OBJ-BG, OBJ-ONLY, and PB-T50-RS subsets of\nScanObjectNN. The code is available at:\nhttps://anonymous.4open.science/r/ZigzagPointMamba-1800/", "published": "2025-05-27 16:09:50", "link": "http://arxiv.org/abs/2505.21381v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Empowering Vector Graphics with Consistently Arbitrary Viewing and View-dependent Visibility", "abstract": "This work presents a novel text-to-vector graphics generation approach,\nDream3DVG, allowing for arbitrary viewpoint viewing, progressive detail\noptimization, and view-dependent occlusion awareness. Our approach is a\ndual-branch optimization framework, consisting of an auxiliary 3D Gaussian\nSplatting optimization branch and a 3D vector graphics optimization branch. The\nintroduced 3DGS branch can bridge the domain gaps between text prompts and\nvector graphics with more consistent guidance. Moreover, 3DGS allows for\nprogressive detail control by scheduling classifier-free guidance, facilitating\nguiding vector graphics with coarse shapes at the initial stages and finer\ndetails at later stages. We also improve the view-dependent occlusions by\ndevising a visibility-awareness rendering module. Extensive results on 3D\nsketches and 3D iconographies, demonstrate the superiority of the method on\ndifferent abstraction levels of details, cross-view consistency, and\nocclusion-aware stroke culling.", "published": "2025-05-27 16:06:04", "link": "http://arxiv.org/abs/2505.21377v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution", "abstract": "Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data\nfor Earth observation but pose challenges for existing multimodal foundation\nmodels due to two key bottlenecks: (1) limited availability of UHR training\ndata, and (2) token explosion caused by the large image size. To address data\nscarcity, we introduce SuperRS-VQA (avg. 8,376$\\times$8,376) and HighRS-VQA\n(avg. 2,000$\\times$1,912), the highest-resolution vision-language datasets in\nRS to date, covering 22 real-world dialogue tasks. To mitigate token explosion,\nour pilot studies reveal significant redundancy in RS images: crucial\ninformation is concentrated in a small subset of object-centric tokens, while\npruning background tokens (e.g., ocean or forest) can even improve performance.\nMotivated by these findings, we propose two strategies: Background Token\nPruning and Anchored Token Selection, to reduce the memory footprint while\npreserving key semantics.Integrating these techniques, we introduce\nGeoLLaVA-8K, the first RS-focused multimodal large language model capable of\nhandling inputs up to 8K$\\times$8K resolution, built on the LLaVA framework.\nTrained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art\non the XLRS-Bench.", "published": "2025-05-27 16:05:03", "link": "http://arxiv.org/abs/2505.21375v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?", "abstract": "Recent advances in CoT reasoning and RL post-training have been reported to\nenhance video reasoning capabilities of MLLMs. This progress naturally raises a\nquestion: can these models perform complex video reasoning in a manner\ncomparable to human experts? However, existing video benchmarks primarily\nevaluate visual perception and grounding abilities, with questions that can be\nanswered based on explicit prompts or isolated visual cues. Such benchmarks do\nnot fully capture the intricacies of real-world reasoning, where humans must\nactively search for, integrate, and analyze multiple clues before reaching a\nconclusion. To address this issue, we present Video-Holmes, a benchmark\ninspired by the reasoning process of Sherlock Holmes, designed to evaluate the\ncomplex video reasoning capabilities of MLLMs. Video-Holmes consists of 1,837\nquestions derived from 270 manually annotated suspense short films, which spans\nseven carefully designed tasks. Each task is constructed by first identifying\nkey events and causal relationships within films, and then designing questions\nthat require models to actively locate and connect multiple relevant visual\nclues scattered across different video segments. Our comprehensive evaluation\nof state-of-the-art MLLMs reveals that, while these models generally excel at\nvisual perception, they encounter substantial difficulties with integrating\ninformation and often miss critical clues. For example, the best-performing\nmodel, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models\nscoring below 40%. We aim that Video-Holmes can serve as a \"Holmes-test\" for\nmultimodal reasoning, motivating models to reason more like humans and\nemphasizing the ongoing challenges in this field. The benchmark is released in\nhttps://github.com/TencentARC/Video-Holmes.", "published": "2025-05-27 16:05:01", "link": "http://arxiv.org/abs/2505.21374v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "YOLO-SPCI: Enhancing Remote Sensing Object Detection via Selective-Perspective-Class Integration", "abstract": "Object detection in remote sensing imagery remains a challenging task due to\nextreme scale variation, dense object distributions, and cluttered backgrounds.\nWhile recent detectors such as YOLOv8 have shown promising results, their\nbackbone architectures lack explicit mechanisms to guide multi-scale feature\nrefinement, limiting performance on high-resolution aerial data. In this work,\nwe propose YOLO-SPCI, an attention-enhanced detection framework that introduces\na lightweight Selective-Perspective-Class Integration (SPCI) module to improve\nfeature representation. The SPCI module integrates three components: a\nSelective Stream Gate (SSG) for adaptive regulation of global feature flow, a\nPerspective Fusion Module (PFM) for context-aware multi-scale integration, and\na Class Discrimination Module (CDM) to enhance inter-class separability. We\nembed two SPCI blocks into the P3 and P5 stages of the YOLOv8 backbone,\nenabling effective refinement while preserving compatibility with the original\nneck and head. Experiments on the NWPU VHR-10 dataset demonstrate that\nYOLO-SPCI achieves superior performance compared to state-of-the-art detectors.", "published": "2025-05-27 16:00:34", "link": "http://arxiv.org/abs/2505.21370v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping", "abstract": "Accurate crop mapping fundamentally relies on modeling multi-scale\nspatiotemporal patterns, where spatial scales range from individual field\ntextures to landscape-level context, and temporal scales capture both\nshort-term phenological transitions and full growing-season dynamics.\nTransformer-based remote sensing foundation models (RSFMs) offer promising\npotential for crop mapping due to their innate ability for unified\nspatiotemporal processing. However, current RSFMs remain suboptimal for crop\nmapping: they either employ fixed spatiotemporal windows that ignore the\nmulti-scale nature of crop systems or completely disregard temporal information\nby focusing solely on spatial patterns. To bridge these gaps, we present\nAgriFM, a multi-source remote sensing foundation model specifically designed\nfor agricultural crop mapping. Our approach begins by establishing the\nnecessity of simultaneous hierarchical spatiotemporal feature extraction,\nleading to the development of a modified Video Swin Transformer architecture\nwhere temporal down-sampling is synchronized with spatial scaling operations.\nThis modified backbone enables efficient unified processing of long time-series\nsatellite inputs. AgriFM leverages temporally rich data streams from three\nsatellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is\npre-trained on a global representative dataset comprising over 25 million image\nsamples supervised by land cover products. The resulting framework incorporates\na versatile decoder architecture that dynamically fuses these learned\nspatiotemporal representations, supporting diverse downstream tasks.\nComprehensive evaluations demonstrate AgriFM's superior performance over\nconventional deep learning approaches and state-of-the-art general-purpose\nRSFMs across all downstream tasks. Codes will be available at\nurlhttps://github.com/flyakon/AgriFM.", "published": "2025-05-27 15:50:14", "link": "http://arxiv.org/abs/2505.21357v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Beyond Accuracy: Uncovering the Role of Similarity Perception and its Alignment with Semantics in Supervised Learning", "abstract": "Similarity manifests in various forms, including semantic similarity that is\nparticularly important, serving as an approximation of human object\ncategorization based on e.g. shared functionalities and evolutionary traits. It\nalso offers practical advantages in computational modeling via lexical\nstructures such as WordNet with constant and interpretable similarity. As in\nthe domain of deep vision, there is still not enough focus on the phenomena\nregarding the similarity perception emergence. We introduce Deep Similarity\nInspector (DSI) -- a systematic framework to inspect how deep vision networks\ndevelop their similarity perception and its alignment with semantic similarity.\nOur experiments show that both Convolutional Neural Networks' (CNNs) and Vision\nTransformers' (ViTs) develop a rich similarity perception during training with\n3 phases (initial similarity surge, refinement, stabilization), with clear\ndifferences between CNNs and ViTs. Besides the gradual mistakes elimination,\nthe mistakes refinement phenomenon can be observed.", "published": "2025-05-27 15:32:10", "link": "http://arxiv.org/abs/2505.21338v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "HoliTom: Holistic Token Merging for Fast Video Large Language Models", "abstract": "Video large language models (video LLMs) excel at video comprehension but\nface significant computational inefficiency due to redundant video tokens.\nExisting token pruning methods offer solutions. However, approaches operating\nwithin the LLM (inner-LLM pruning), such as FastV, incur intrinsic\ncomputational overhead in shallow layers. In contrast, methods performing token\npruning before the LLM (outer-LLM pruning) primarily address spatial redundancy\nwithin individual frames or limited temporal windows, neglecting the crucial\nglobal temporal dynamics and correlations across longer video sequences. This\nleads to sub-optimal spatio-temporal reduction and does not leverage video\ncompressibility fully. Crucially, the synergistic potential and mutual\ninfluence of combining these strategies remain unexplored. To further reduce\nredundancy, we introduce HoliTom, a novel training-free holistic token merging\nframework. HoliTom employs outer-LLM pruning through global redundancy-aware\ntemporal segmentation, followed by spatial-temporal merging to reduce visual\ntokens by over 90%, significantly alleviating the LLM's computational burden.\nComplementing this, we introduce a robust inner-LLM token similarity-based\nmerging approach, designed for superior performance and compatibility with\nouter-LLM pruning. Evaluations demonstrate our method's promising\nefficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational\ncosts to 6.9% of FLOPs while maintaining 99.1% of the original performance.\nFurthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a\n1.32x acceleration in decoding throughput, highlighting the practical benefits\nof our integrated pruning approach for efficient video LLMs inference.", "published": "2025-05-27 15:28:45", "link": "http://arxiv.org/abs/2505.21334v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios", "abstract": "Multimodal Large Language Models (MLLMs) have achieved considerable accuracy\nin Optical Character Recognition (OCR) from static images. However, their\nefficacy in video OCR is significantly diminished due to factors such as motion\nblur, temporal variations, and visual effects inherent in video content. To\nprovide clearer guidance for training practical MLLMs, we introduce the\nMME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR\napplication scenarios. MME-VideoOCR features 10 task categories comprising 25\nindividual tasks and spans 44 diverse scenarios. These tasks extend beyond text\nrecognition to incorporate deeper comprehension and reasoning of textual\ncontent within videos. The benchmark consists of 1,464 videos with varying\nresolutions, aspect ratios, and durations, along with 2,000 meticulously\ncurated, manually annotated question-answer pairs. We evaluate 18\nstate-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing\nmodel (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained\nanalysis indicates that while existing MLLMs demonstrate strong performance on\ntasks where relevant texts are contained within a single or few frames, they\nexhibit limited capability in effectively handling tasks that demand holistic\nvideo comprehension. These limitations are especially evident in scenarios that\nrequire spatio-temporal reasoning, cross-frame information integration, or\nresistance to language prior bias. Our findings also highlight the importance\nof high-resolution visual input and sufficient temporal coverage for reliable\nOCR in dynamic video scenarios.", "published": "2025-05-27 15:27:46", "link": "http://arxiv.org/abs/2505.21333v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving Video Virtual Try-on", "abstract": "Video Virtual Try-On (VVT) aims to simulate the natural appearance of\ngarments across consecutive video frames, capturing their dynamic variations\nand interactions with human body motion. However, current VVT methods still\nface challenges in terms of spatiotemporal consistency and garment content\npreservation. First, they use diffusion models based on the U-Net, which are\nlimited in their expressive capability and struggle to reconstruct complex\ndetails. Second, they adopt a separative modeling approach for spatial and\ntemporal attention, which hinders the effective capture of structural\nrelationships and dynamic consistency across frames. Third, their expression of\ngarment details remains insufficient, affecting the realism and stability of\nthe overall synthesized results, especially during human motion. To address the\nabove challenges, we propose MagicTryOn, a video virtual try-on framework built\nupon the large-scale video diffusion Transformer.We replace the U-Net\narchitecture with a diffusion Transformer and combine full self-attention to\njointly model the spatiotemporal consistency of videos. We design a\ncoarse-to-fine garment preservation strategy. The coarse strategy integrates\ngarment tokens during the embedding stage, while the fine strategy incorporates\nmultiple garment-based conditions, such as semantics, textures, and contour\nlines during the denoising stage. Moreover, we introduce a mask-aware loss to\nfurther optimize garment region fidelity. Extensive experiments on both image\nand video try-on datasets demonstrate that our method outperforms existing SOTA\nmethods in comprehensive evaluations and generalizes to in-the-wild scenarios.", "published": "2025-05-27 15:22:02", "link": "http://arxiv.org/abs/2505.21325v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "efunc: An Efficient Function Representation without Neural Networks", "abstract": "Function fitting/approximation plays a fundamental role in computer graphics\nand other engineering applications. While recent advances have explored neural\nnetworks to address this task, these methods often rely on architectures with\nmany parameters, limiting their practical applicability. In contrast, we pursue\nhigh-quality function approximation using parameter-efficient representations\nthat eliminate the dependency on neural networks entirely. We first propose a\nnovel framework for continuous function modeling. Most existing works can be\nformulated using this framework. We then introduce a compact function\nrepresentation, which is based on polynomials interpolated using radial basis\nfunctions, bypassing both neural networks and complex/hierarchical data\nstructures. We also develop memory-efficient CUDA-optimized algorithms that\nreduce computational time and memory consumption to less than 10% compared to\nconventional automatic differentiation frameworks. Finally, we validate our\nrepresentation and optimization pipeline through extensive experiments on 3D\nsigned distance functions (SDFs). The proposed representation achieves\ncomparable or superior performance to state-of-the-art techniques (e.g.,\noctree/hash-grid techniques) with significantly fewer parameters.", "published": "2025-05-27 15:16:56", "link": "http://arxiv.org/abs/2505.21319v1", "categories": ["cs.GR", "cs.CV"], "primary_category": "cs.GR"}
{"title": "Efficient Leaf Disease Classification and Segmentation using Midpoint Normalization Technique and Attention Mechanism", "abstract": "Enhancing plant disease detection from leaf imagery remains a persistent\nchallenge due to scarce labeled data and complex contextual factors. We\nintroduce a transformative two-stage methodology, Mid Point Normalization (MPN)\nfor intelligent image preprocessing, coupled with sophisticated attention\nmechanisms that dynamically recalibrate feature representations. Our\nclassification pipeline, merging MPN with Squeeze-and-Excitation (SE) blocks,\nachieves remarkable 93% accuracy while maintaining exceptional class-wise\nbalance. The perfect F1 score attained for our target class exemplifies\nattention's power in adaptive feature refinement. For segmentation tasks, we\nseamlessly integrate identical attention blocks within U-Net architecture using\nMPN-enhanced inputs, delivering compelling performance gains with 72.44% Dice\nscore and 58.54% IoU, substantially outperforming baseline implementations.\nBeyond superior accuracy metrics, our approach yields computationally\nefficient, lightweight architectures perfectly suited for real-world computer\nvision applications.", "published": "2025-05-27 15:14:04", "link": "http://arxiv.org/abs/2505.21316v1", "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Spectral Compression Transformer with Line Pose Graph for Monocular 3D Human Pose Estimation", "abstract": "Transformer-based 3D human pose estimation methods suffer from high\ncomputational costs due to the quadratic complexity of self-attention with\nrespect to sequence length. Additionally, pose sequences often contain\nsignificant redundancy between frames. However, recent methods typically fail\nto improve model capacity while effectively eliminating sequence redundancy. In\nthis work, we introduce the Spectral Compression Transformer (SCT) to reduce\nsequence length and accelerate computation. The SCT encoder treats hidden\nfeatures between blocks as Temporal Feature Signals (TFS) and applies the\nDiscrete Cosine Transform, a Fourier transform-based technique, to determine\nthe spectral components to be retained. By filtering out certain high-frequency\nnoise components, SCT compresses the sequence length and reduces redundancy. To\nfurther enrich the input sequence with prior structural information, we propose\nthe Line Pose Graph (LPG) based on line graph theory. The LPG generates\nskeletal position information that complements the input 2D joint positions,\nthereby improving the model's performance. Finally, we design a dual-stream\nnetwork architecture to effectively model spatial joint relationships and the\ncompressed motion trajectory within the pose sequence. Extensive experiments on\ntwo benchmark datasets (i.e., Human3.6M and MPI-INF-3DHP) demonstrate that our\nmodel achieves state-of-the-art performance with improved computational\nefficiency. For example, on the Human3.6M dataset, our method achieves an MPJPE\nof 37.7mm while maintaining a low computational cost. Furthermore, we perform\nablation studies on each module to assess its effectiveness. The code and\nmodels will be released.", "published": "2025-05-27 15:08:03", "link": "http://arxiv.org/abs/2505.21309v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Supervised and self-supervised land-cover segmentation & classification of the Biesbosch wetlands", "abstract": "Accurate wetland land-cover classification is essential for environmental\nmonitoring, biodiversity assessment, and sustainable ecosystem management.\nHowever, the scarcity of annotated data, especially for high-resolution\nsatellite imagery, poses a significant challenge for supervised learning\napproaches. To tackle this issue, this study presents a methodology for wetland\nland-cover segmentation and classification that adopts both supervised and\nself-supervised learning (SSL). We train a U-Net model from scratch on\nSentinel-2 imagery across six wetland regions in the Netherlands, achieving a\nbaseline model accuracy of 85.26%.\n  Addressing the limited availability of labeled data, the results show that\nSSL pretraining with an autoencoder can improve accuracy, especially for the\nhigh-resolution imagery where it is more difficult to obtain labeled data,\nreaching an accuracy of 88.23%.\n  Furthermore, we introduce a framework to scale manually annotated\nhigh-resolution labels to medium-resolution inputs. While the quantitative\nperformance between resolutions is comparable, high-resolution imagery provides\nsignificantly sharper segmentation boundaries and finer spatial detail.\n  As part of this work, we also contribute a curated Sentinel-2 dataset with\nDynamic World labels, tailored for wetland classification tasks and made\npublicly available.", "published": "2025-05-27 14:42:49", "link": "http://arxiv.org/abs/2505.21269v1", "categories": ["cs.CV", "eess.IV", "68", "I.4.6"], "primary_category": "cs.CV"}
{"title": "DiMoSR: Feature Modulation via Multi-Branch Dilated Convolutions for Efficient Image Super-Resolution", "abstract": "Balancing reconstruction quality versus model efficiency remains a critical\nchallenge in lightweight single image super-resolution (SISR). Despite the\nprevalence of attention mechanisms in recent state-of-the-art SISR approaches\nthat primarily emphasize or suppress feature maps, alternative architectural\nparadigms warrant further exploration. This paper introduces DiMoSR (Dilated\nModulation Super-Resolution), a novel architecture that enhances feature\nrepresentation through modulation to complement attention in lightweight SISR\nnetworks. The proposed approach leverages multi-branch dilated convolutions to\ncapture rich contextual information over a wider receptive field while\nmaintaining computational efficiency. Experimental results demonstrate that\nDiMoSR outperforms state-of-the-art lightweight methods across diverse\nbenchmark datasets, achieving superior PSNR and SSIM metrics with comparable or\nreduced computational complexity. Through comprehensive ablation studies, this\nwork not only validates the effectiveness of DiMoSR but also provides critical\ninsights into the interplay between attention mechanisms and feature modulation\nto guide future research in efficient network design. The code and model\nweights to reproduce our results are available at:\nhttps://github.com/makinyilmaz/DiMoSR", "published": "2025-05-27 14:40:05", "link": "http://arxiv.org/abs/2505.21262v1", "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Plenodium: UnderWater 3D Scene Reconstruction with Plenoptic Medium Representation", "abstract": "We present Plenodium (plenoptic medium), an effective and efficient 3D\nrepresentation framework capable of jointly modeling both objects and\nparticipating media. In contrast to existing medium representations that rely\nsolely on view-dependent modeling, our novel plenoptic medium representation\nincorporates both directional and positional information through spherical\nharmonics encoding, enabling highly accurate underwater scene reconstruction.\nTo address the initialization challenge in degraded underwater environments, we\npropose the pseudo-depth Gaussian complementation to augment COLMAP-derived\npoint clouds with robust depth priors. In addition, a depth ranking regularized\nloss is developed to optimize the geometry of the scene and improve the ordinal\nconsistency of the depth maps. Extensive experiments on real-world underwater\ndatasets demonstrate that our method achieves significant improvements in 3D\nreconstruction. Furthermore, we conduct a simulated dataset with ground truth\nand the controllable scattering medium to demonstrate the restoration\ncapability of our method in underwater scenarios. Our code and dataset are\navailable at https://plenodium.github.io/.", "published": "2025-05-27 14:37:58", "link": "http://arxiv.org/abs/2505.21258v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics-Based Appearance-Medium Decouplin", "abstract": "Novel view synthesis for underwater scene reconstruction presents unique\nchallenges due to complex light-media interactions. Optical scattering and\nabsorption in water body bring inhomogeneous medium attenuation interference\nthat disrupts conventional volume rendering assumptions of uniform propagation\nmedium. While 3D Gaussian Splatting (3DGS) offers real-time rendering\ncapabilities, it struggles with underwater inhomogeneous environments where\nscattering media introduce artifacts and inconsistent appearance. In this\nstudy, we propose a physics-based framework that disentangles object appearance\nfrom water medium effects through tailored Gaussian modeling. Our approach\nintroduces appearance embeddings, which are explicit medium representations for\nbackscatter and attenuation, enhancing scene consistency. In addition, we\npropose a distance-guided optimization strategy that leverages pseudo-depth\nmaps as supervision with depth regularization and scale penalty terms to\nimprove geometric fidelity. By integrating the proposed appearance and medium\nmodeling components via an underwater imaging model, our approach achieves both\nhigh-quality novel view synthesis and physically accurate scene restoration.\nExperiments demonstrate our significant improvements in rendering quality and\nrestoration accuracy over existing methods. The project page is available at\n\\href{https://bilityniu.github.io/3D-UIR}{https://bilityniu.github.io/3D-UIR", "published": "2025-05-27 14:19:30", "link": "http://arxiv.org/abs/2505.21238v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "CROP: Contextual Region-Oriented Visual Token Pruning", "abstract": "Current VLM-based VQA methods often process entire images, leading to\nexcessive visual tokens that include redundant information irrelevant to the\nposed question. This abundance of unnecessary image details creates numerous\nvisual tokens, drastically increasing memory and computational requirements in\nVLMs. To address this, we propose Contextual Region-Oriented Visual Token\nPruning (CROP), a novel framework to compress visual tokens through a two-step\nprocess: Localization and Pruning. Specifically, CROP first employs an\nefficient model to identify the contextual region relevant to the input query.\nSubsequently, two distinct strategies are introduced for pruning: (1) Pre-LLM\nCompression (PLC), which adaptively compresses different image regions with\nvarying ratios, and (2) Inner-LLM Pruning (ILP), a training-free method that\nprunes tokens within early LLM layers guided by the identified contextual\nregion. Extensive experiments on a wide range of VQA tasks demonstrate that\nCROP significantly outperforms existing visual token pruning methods and\nachieves state-of-the-art performance. Our code and datasets will be made\navailable.", "published": "2025-05-27 14:16:52", "link": "http://arxiv.org/abs/2505.21233v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Occlusion Boundary and Depth: Mutual Enhancement via Multi-Task Learning", "abstract": "Occlusion Boundary Estimation (OBE) identifies boundaries arising from both\ninter-object occlusions and self-occlusion within individual objects,\ndistinguishing intrinsic object edges from occlusion-induced contours to\nimprove scene understanding and 3D reconstruction capacity. This is closely\nrelated to Monocular Depth Estimation (MDE), which infers depth from a single\nimage, as occlusion boundaries provide critical geometric cues for resolving\ndepth ambiguities, while depth priors can conversely refine occlusion reasoning\nin complex scenes. In this paper, we propose a novel network, MoDOT, that first\njointly estimates depth and OBs. We propose CASM, a cross-attention multi-scale\nstrip convolution module, leverages mid-level OB features to significantly\nenhance depth prediction. Additionally, we introduce an occlusion-aware loss\nfunction, OBDCL, which encourages sharper and more accurate depth boundaries.\nExtensive experiments on both real and synthetic datasets demonstrate the\nmutual benefits of jointly estimating depth and OB, and highlight the\neffectiveness of our model design. Our method achieves the state-of-the-art\n(SOTA) on both our proposed synthetic datasets and one popular real dataset,\nNYUD-v2, significantly outperforming multi-task baselines. Besides, without\ndomain adaptation, results on real-world depth transfer are comparable to the\ncompetitors, while preserving sharp occlusion boundaries for geometric\nfidelity. We will release our code, pre-trained models, and datasets to support\nfuture research in this direction.", "published": "2025-05-27 14:15:19", "link": "http://arxiv.org/abs/2505.21231v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Sci-Fi: Symmetric Constraint for Frame Inbetweening", "abstract": "Frame inbetweening aims to synthesize intermediate video sequences\nconditioned on the given start and end frames. Current state-of-the-art methods\nmainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs)\nby incorporating end-frame constraints via directly fine-tuning or omitting\ntraining. We identify a critical limitation in their design: Their injections\nof the end-frame constraint usually utilize the same mechanism that originally\nimposed the start-frame (single image) constraint. However, since the original\nI2V-DMs are adequately trained for the start-frame condition in advance,\nnaively introducing the end-frame constraint by the same mechanism with much\nless (even zero) specialized training probably can't make the end frame have a\nstrong enough impact on the intermediate content like the start frame. This\nasymmetric control strength of the two frames over the intermediate content\nlikely leads to inconsistent motion or appearance collapse in generated frames.\nTo efficiently achieve symmetric constraints of start and end frames, we\npropose a novel framework, termed Sci-Fi, which applies a stronger injection\nfor the constraint of a smaller training scale. Specifically, it deals with the\nstart-frame constraint as before, while introducing the end-frame constraint by\nan improved mechanism. The new mechanism is based on a well-designed\nlightweight module, named EF-Net, which encodes only the end frame and expands\nit into temporally adaptive frame-wise features injected into the I2V-DM. This\nmakes the end-frame constraint as strong as the start-frame constraint,\nenabling our Sci-Fi to produce more harmonious transitions in various\nscenarios. Extensive experiments prove the superiority of our Sci-Fi compared\nwith other baselines.", "published": "2025-05-27 13:53:50", "link": "http://arxiv.org/abs/2505.21205v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models", "abstract": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm for\ngeneral-purpose robot control through natural language instructions. However,\ntheir high inference cost-stemming from large-scale token computation and\nautoregressive decoding-poses significant challenges for real-time deployment\nand edge applications. While prior work has primarily focused on architectural\noptimization, we take a different perspective by identifying a dual form of\nredundancy in VLA models: (i) high similarity across consecutive action steps,\nand (ii) substantial redundancy in visual tokens. Motivated by these\nobservations, we propose FlashVLA, the first training-free and plug-and-play\nacceleration framework that enables action reuse in VLA models. FlashVLA\nimproves inference efficiency through a token-aware action reuse mechanism that\navoids redundant decoding across stable action steps, and an information-guided\nvisual token selection strategy that prunes low-contribution tokens. Extensive\nexperiments on the LIBERO benchmark show that FlashVLA reduces FLOPs by 55.7%\nand latency by 36.0%, with only a 0.7% drop in task success rate. These results\ndemonstrate the effectiveness of FlashVLA in enabling lightweight, low-latency\nVLA inference without retraining.", "published": "2025-05-27 13:47:18", "link": "http://arxiv.org/abs/2505.21200v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Learning Annotation Consensus for Continuous Emotion Recognition", "abstract": "In affective computing, datasets often contain multiple annotations from\ndifferent annotators, which may lack full agreement. Typically, these\nannotations are merged into a single gold standard label, potentially losing\nvaluable inter-rater variability. We propose a multi-annotator training\napproach for continuous emotion recognition (CER) that seeks a consensus across\nall annotators rather than relying on a single reference label. Our method\nemploys a consensus network to aggregate annotations into a unified\nrepresentation, guiding the main arousal-valence predictor to better reflect\ncollective inputs. Tested on the RECOLA and COGNIMUSE datasets, our approach\noutperforms traditional methods that unify annotations into a single label.\nThis underscores the benefits of fully leveraging multi-annotator data in\nemotion recognition and highlights its applicability across various fields\nwhere annotations are abundant yet inconsistent.", "published": "2025-05-27 13:43:45", "link": "http://arxiv.org/abs/2505.21196v1", "categories": ["cs.HC", "cs.CV"], "primary_category": "cs.HC"}
{"title": "Making Every Event Count: Balancing Data Efficiency and Accuracy in Event Camera Subsampling", "abstract": "Event cameras offer high temporal resolution and power efficiency, making\nthem well-suited for edge AI applications. However, their high event rates\npresent challenges for data transmission and processing. Subsampling methods\nprovide a practical solution, but their effect on downstream visual tasks\nremains underexplored. In this work, we systematically evaluate six\nhardware-friendly subsampling methods using convolutional neural networks for\nevent video classification on various benchmark datasets. We hypothesize that\nevents from high-density regions carry more task-relevant information and are\ntherefore better suited for subsampling. To test this, we introduce a simple\ncausal density-based subsampling method, demonstrating improved classification\naccuracy in sparse regimes. Our analysis further highlights key factors\naffecting subsampling performance, including sensitivity to hyperparameters and\nfailure cases in scenarios with large event count variance. These findings\nprovide insights for utilization of hardware-efficient subsampling strategies\nthat balance data efficiency and task accuracy. The code for this paper will be\nreleased at: https://github.com/hesamaraghi/event-camera-subsampling-methods.", "published": "2025-05-27 13:37:08", "link": "http://arxiv.org/abs/2505.21187v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Boosting Adversarial Transferability via High-Frequency Augmentation and Hierarchical-Gradient Fusion", "abstract": "Adversarial attacks have become a significant challenge in the security of\nmachine learning models, particularly in the context of black-box defense\nstrategies. Existing methods for enhancing adversarial transferability\nprimarily focus on the spatial domain. This paper presents Frequency-Space\nAttack (FSA), a new adversarial attack framework that effectively integrates\nfrequency-domain and spatial-domain transformations. FSA combines two key\ntechniques: (1) High-Frequency Augmentation, which applies Fourier transform\nwith frequency-selective amplification to diversify inputs and emphasize the\ncritical role of high-frequency components in adversarial attacks, and (2)\nHierarchical-Gradient Fusion, which merges multi-scale gradient decomposition\nand fusion to capture both global structures and fine-grained details,\nresulting in smoother perturbations. Our experiment demonstrates that FSA\nconsistently outperforms state-of-the-art methods across various black-box\nmodels. Notably, our proposed FSA achieves an average attack success rate\nincrease of 23.6% compared with BSR (CVPR 2024) on eight black-box defense\nmodels.", "published": "2025-05-27 13:32:52", "link": "http://arxiv.org/abs/2505.21181v1", "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Normalized Attention Guidance: Universal Negative Guidance for Diffusion Model", "abstract": "Negative guidance -- explicitly suppressing unwanted attributes -- remains a\nfundamental challenge in diffusion models, particularly in few-step sampling\nregimes. While Classifier-Free Guidance (CFG) works well in standard settings,\nit fails under aggressive sampling step compression due to divergent\npredictions between positive and negative branches. We present Normalized\nAttention Guidance (NAG), an efficient, training-free mechanism that applies\nextrapolation in attention space with L1-based normalization and refinement.\nNAG restores effective negative guidance where CFG collapses while maintaining\nfidelity. Unlike existing approaches, NAG generalizes across architectures\n(UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image,\nvideo), functioning as a \\textit{universal} plug-in with minimal computational\noverhead. Through extensive experimentation, we demonstrate consistent\nimprovements in text alignment (CLIP Score), fidelity (FID, PFID), and\nhuman-perceived quality (ImageReward). Our ablation studies validate each\ndesign component, while user studies confirm significant preference for\nNAG-guided outputs. As a model-agnostic inference-time approach requiring no\nretraining, NAG provides effortless negative guidance for all modern diffusion\nframeworks -- pseudocode in the Appendix!", "published": "2025-05-27 13:30:46", "link": "http://arxiv.org/abs/2505.21179v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Topological Deep Learning for Speech Data", "abstract": "Topological data analysis (TDA) offers novel mathematical tools for deep\nlearning. Inspired by Carlsson et al., this study designs topology-aware\nconvolutional kernels that significantly improve speech recognition networks.\nTheoretically, by investigating orthogonal group actions on kernels, we\nestablish a fiber-bundle decomposition of matrix spaces, enabling new filter\ngeneration methods. Practically, our proposed Orthogonal Feature (OF) layer\nachieves superior performance in phoneme recognition, particularly in low-noise\nscenarios, while demonstrating cross-domain adaptability. This work reveals\nTDA's potential in neural network optimization, opening new avenues for\nmathematics-deep learning interdisciplinary studies.", "published": "2025-05-27 13:26:05", "link": "http://arxiv.org/abs/2505.21173v1", "categories": ["cs.LG", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "RoBiS: Robust Binary Segmentation for High-Resolution Industrial Images", "abstract": "Robust unsupervised anomaly detection (AD) in real-world scenarios is an\nimportant task. Current methods exhibit severe performance degradation on the\nMVTec AD 2 benchmark due to its complex real-world challenges. To solve this\nproblem, we propose a robust framework RoBiS, which consists of three core\nmodules: (1) Swin-Cropping, a high-resolution image pre-processing strategy to\npreserve the information of small anomalies through overlapping window\ncropping. (2) The data augmentation of noise addition and lighting simulation\nis carried out on the training data to improve the robustness of AD model. We\nuse INP-Former as our baseline, which could generate better results on the\nvarious sub-images. (3) The traditional statistical-based binarization strategy\n(mean+3std) is combined with our previous work, MEBin (published in CVPR2025),\nfor joint adaptive binarization. Then, SAM is further employed to refine the\nsegmentation results. Compared with some methods reported by the MVTec AD 2,\nour RoBiS achieves a 29.2% SegF1 improvement (from 21.8% to 51.00%) on\nTest_private and 29.82% SegF1 gains (from 16.7% to 46.52%) on\nTest_private_mixed. Code is available at https://github.com/xrli-U/RoBiS.", "published": "2025-05-27 13:04:48", "link": "http://arxiv.org/abs/2505.21152v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "IKMo: Image-Keyframed Motion Generation with Trajectory-Pose Conditioned Motion Diffusion Model", "abstract": "Existing human motion generation methods with trajectory and pose inputs\noperate global processing on both modalities, leading to suboptimal outputs. In\nthis paper, we propose IKMo, an image-keyframed motion generation method based\non the diffusion model with trajectory and pose being decoupled. The trajectory\nand pose inputs go through a two-stage conditioning framework. In the first\nstage, the dedicated optimization module is applied to refine inputs. In the\nsecond stage, trajectory and pose are encoded via a Trajectory Encoder and a\nPose Encoder in parallel. Then, motion with high spatial and semantic fidelity\nis guided by a motion ControlNet, which processes the fused trajectory and pose\ndata. Experiment results based on HumanML3D and KIT-ML datasets demonstrate\nthat the proposed method outperforms state-of-the-art on all metrics under\ntrajectory-keyframe constraints. In addition, MLLM-based agents are implemented\nto pre-process model inputs. Given texts and keyframe images from users, the\nagents extract motion descriptions, keyframe poses, and trajectories as the\noptimized inputs into the motion generation model. We conducts a user study\nwith 10 participants. The experiment results prove that the MLLM-based agents\npre-processing makes generated motion more in line with users' expectation. We\nbelieve that the proposed method improves both the fidelity and controllability\nof motion generation by the diffusion model.", "published": "2025-05-27 12:57:37", "link": "http://arxiv.org/abs/2505.21146v1", "categories": ["cs.GR", "cs.CV"], "primary_category": "cs.GR"}
{"title": "FastFace: Tuning Identity Preservation in Distilled Diffusion via Guidance and Attention", "abstract": "In latest years plethora of identity-preserving adapters for a personalized\ngeneration with diffusion models have been released. Their main disadvantage is\nthat they are dominantly trained jointly with base diffusion models, which\nsuffer from slow multi-step inference. This work aims to tackle the challenge\nof training-free adaptation of pretrained ID-adapters to diffusion models\naccelerated via distillation - through careful re-design of classifier-free\nguidance for few-step stylistic generation and attention manipulation\nmechanisms in decoupled blocks to improve identity similarity and fidelity, we\npropose universal FastFace framework. Additionally, we develop a disentangled\npublic evaluation protocol for id-preserving adapters.", "published": "2025-05-27 12:55:55", "link": "http://arxiv.org/abs/2505.21144v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Learning Single Index Models with Diffusion Priors", "abstract": "Diffusion models (DMs) have demonstrated remarkable ability to generate\ndiverse and high-quality images by efficiently modeling complex data\ndistributions. They have also been explored as powerful generative priors for\nsignal recovery, resulting in a substantial improvement in the quality of\nreconstructed signals. However, existing research on signal recovery with\ndiffusion models either focuses on specific reconstruction problems or is\nunable to handle nonlinear measurement models with discontinuous or unknown\nlink functions. In this work, we focus on using DMs to achieve accurate\nrecovery from semi-parametric single index models, which encompass a variety of\npopular nonlinear models that may have {\\em discontinuous} and {\\em unknown}\nlink functions. We propose an efficient reconstruction method that only\nrequires one round of unconditional sampling and (partial) inversion of DMs.\nTheoretical analysis on the effectiveness of the proposed methods has been\nestablished under appropriate conditions. We perform numerical experiments on\nimage datasets for different nonlinear measurement models. We observe that\ncompared to competing methods, our approach can yield more accurate\nreconstructions while utilizing significantly fewer neural function\nevaluations.", "published": "2025-05-27 12:50:04", "link": "http://arxiv.org/abs/2505.21135v1", "categories": ["cs.LG", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "ReassembleNet: Learnable Keypoints and Diffusion for 2D Fresco Reconstruction", "abstract": "The task of reassembly is a significant challenge across multiple domains,\nincluding archaeology, genomics, and molecular docking, requiring the precise\nplacement and orientation of elements to reconstruct an original structure. In\nthis work, we address key limitations in state-of-the-art Deep Learning methods\nfor reassembly, namely i) scalability; ii) multimodality; and iii) real-world\napplicability: beyond square or simple geometric shapes, realistic and complex\nerosion, or other real-world problems. We propose ReassembleNet, a method that\nreduces complexity by representing each input piece as a set of contour\nkeypoints and learning to select the most informative ones by Graph Neural\nNetworks pooling inspired techniques. ReassembleNet effectively lowers\ncomputational complexity while enabling the integration of features from\nmultiple modalities, including both geometric and texture data. Further\nenhanced through pretraining on a semi-synthetic dataset. We then apply\ndiffusion-based pose estimation to recover the original structure. We improve\non prior methods by 55% and 86% for RMSE Rotation and Translation,\nrespectively.", "published": "2025-05-27 12:38:06", "link": "http://arxiv.org/abs/2505.21117v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Differentiable Solver Search for Fast Diffusion Sampling", "abstract": "Diffusion models have demonstrated remarkable generation quality but at the\ncost of numerous function evaluations. Recently, advanced ODE-based solvers\nhave been developed to mitigate the substantial computational demands of\nreverse-diffusion solving under limited sampling steps. However, these solvers,\nheavily inspired by Adams-like multistep methods, rely solely on t-related\nLagrange interpolation. We show that t-related Lagrange interpolation is\nsuboptimal for diffusion model and reveal a compact search space comprised of\ntime steps and solver coefficients. Building on our analysis, we propose a\nnovel differentiable solver search algorithm to identify more optimal solver.\nEquipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and\nFlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet256\nwith only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches a FID score of\n2.33 with only 10 steps. Notably, our searched solver outperforms traditional\nsolvers by a significant margin. Moreover, our searched solver demonstrates\ngenerality across various model architectures, resolutions, and model sizes.", "published": "2025-05-27 12:33:43", "link": "http://arxiv.org/abs/2505.21114v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Instance Data Condensation for Image Super-Resolution", "abstract": "Deep learning based image Super-Resolution (ISR) relies on large training\ndatasets to optimize model generalization; this requires substantial\ncomputational and storage resources during training. While dataset condensation\nhas shown potential in improving data efficiency and privacy for high-level\ncomputer vision tasks, it has not yet been fully exploited for ISR. In this\npaper, we propose a novel Instance Data Condensation (IDC) framework\nspecifically for ISR, which achieves instance-level data condensation through\nRandom Local Fourier Feature Extraction and Multi-level Feature Distribution\nMatching. This aims to optimize feature distributions at both global and local\nlevels and obtain high-quality synthesized training content with fine detail.\nThis framework has been utilized to condense the most commonly used training\ndataset for ISR, DIV2K, with a 10% condensation rate. The resulting synthetic\ndataset offers comparable or (in certain cases) even better performance\ncompared to the original full dataset and excellent training stability when\nused to train various popular ISR models. To the best of our knowledge, this is\nthe first time that a condensed/synthetic dataset (with a 10% data volume) has\ndemonstrated such performance. The source code and the synthetic dataset have\nbeen made available at https://github.com/.", "published": "2025-05-27 12:25:09", "link": "http://arxiv.org/abs/2505.21099v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DisasterM3: A Remote Sensing Vision-Language Dataset for Disaster Damage Assessment and Response", "abstract": "Large vision-language models (VLMs) have made great achievements in Earth\nvision. However, complex disaster scenes with diverse disaster types,\ngeographic regions, and satellite sensors have posed new challenges for VLM\napplications. To fill this gap, we curate a remote sensing vision-language\ndataset (DisasterM3) for global-scale disaster assessment and response.\nDisasterM3 includes 26,988 bi-temporal satellite images and 123k instruction\npairs across 5 continents, with three characteristics: 1) Multi-hazard:\nDisasterM3 involves 36 historical disaster events with significant impacts,\nwhich are categorized into 10 common natural and man-made disasters.\n2)Multi-sensor: Extreme weather during disasters often hinders optical sensor\nimaging, making it necessary to combine Synthetic Aperture Radar (SAR) imagery\nfor post-disaster scenes. 3) Multi-task: Based on real-world scenarios,\nDisasterM3 includes 9 disaster-related visual perception and reasoning tasks,\nharnessing the full potential of VLM's reasoning ability with progressing from\ndisaster-bearing body recognition to structural damage assessment and object\nrelational reasoning, culminating in the generation of long-form disaster\nreports. We extensively evaluated 14 generic and remote sensing VLMs on our\nbenchmark, revealing that state-of-the-art models struggle with the disaster\ntasks, largely due to the lack of a disaster-specific corpus, cross-sensor gap,\nand damage object counting insensitivity. Focusing on these issues, we\nfine-tune four VLMs using our dataset and achieve stable improvements across\nall tasks, with robust cross-sensor and cross-disaster generalization\ncapabilities.", "published": "2025-05-27 12:16:07", "link": "http://arxiv.org/abs/2505.21089v1", "categories": ["cs.CV", "I.4.9"], "primary_category": "cs.CV"}
{"title": "Uni3D-MoE: Scalable Multimodal 3D Scene Understanding via Mixture of Experts", "abstract": "Recent advancements in multimodal large language models (MLLMs) have\ndemonstrated considerable potential for comprehensive 3D scene understanding.\nHowever, existing approaches typically utilize only one or a limited subset of\n3D modalities, resulting in incomplete representations of 3D scenes and reduced\ninterpretive accuracy. Furthermore, different types of queries inherently\ndepend on distinct modalities, indicating that uniform processing of all\nmodality tokens may fail to effectively capture query-specific context. To\naddress these challenges, we propose Uni3D-MoE, a sparse Mixture-of-Experts\n(MoE)-based 3D MLLM designed to enable adaptive 3D multimodal fusion.\nSpecifically, Uni3D-MoE integrates a comprehensive set of 3D modalities,\nincluding multi-view RGB and depth images, bird's-eye-view (BEV) maps, point\nclouds, and voxel representations. At its core, our framework employs a\nlearnable routing mechanism within the sparse MoE-based large language model,\ndynamically selecting appropriate experts at the token level. Each expert\nspecializes in processing multimodal tokens based on learned modality\npreferences, thus facilitating flexible collaboration tailored to diverse\ntask-specific requirements. Extensive evaluations on standard 3D scene\nunderstanding benchmarks and specialized datasets demonstrate the efficacy of\nUni3D-MoE.", "published": "2025-05-27 12:03:30", "link": "http://arxiv.org/abs/2505.21079v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DynamicVL: Benchmarking Multimodal Large Language Models for Dynamic City Understanding", "abstract": "Multimodal large language models have demonstrated remarkable capabilities in\nvisual understanding, but their application to long-term Earth observation\nanalysis remains limited, primarily focusing on single-temporal or bi-temporal\nimagery. To address this gap, we introduce DVL-Suite, a comprehensive framework\nfor analyzing long-term urban dynamics through remote sensing imagery. Our\nsuite comprises 15,063 high-resolution (1.0m) multi-temporal images spanning 42\nmegacities in the U.S. from 2005 to 2023, organized into two components:\nDVL-Bench and DVL-Instruct. The DVL-Bench includes seven urban understanding\ntasks, from fundamental change detection (pixel-level) to quantitative analyses\n(regional-level) and comprehensive urban narratives (scene-level), capturing\ndiverse urban dynamics including expansion/transformation patterns, disaster\nassessment, and environmental challenges. We evaluate 17 state-of-the-art\nmultimodal large language models and reveal their limitations in long-term\ntemporal understanding and quantitative analysis. These challenges motivate the\ncreation of DVL-Instruct, a specialized instruction-tuning dataset designed to\nenhance models' capabilities in multi-temporal Earth observation. Building upon\nthis dataset, we develop DVLChat, a baseline model capable of both image-level\nquestion-answering and pixel-level segmentation, facilitating a comprehensive\nunderstanding of city dynamics through language interactions.", "published": "2025-05-27 12:01:19", "link": "http://arxiv.org/abs/2505.21076v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Minute-Long Videos with Dual Parallelisms", "abstract": "Diffusion Transformer (DiT)-based video diffusion models generate\nhigh-quality videos at scale but incur prohibitive processing latency and\nmemory costs for long videos. To address this, we propose a novel distributed\ninference strategy, termed DualParal. The core idea is that, instead of\ngenerating an entire video on a single GPU, we parallelize both temporal frames\nand model layers across GPUs. However, a naive implementation of this division\nfaces a key limitation: since diffusion models require synchronized noise\nlevels across frames, this implementation leads to the serialization of\noriginal parallelisms. We leverage a block-wise denoising scheme to handle\nthis. Namely, we process a sequence of frame blocks through the pipeline with\nprogressively decreasing noise levels. Each GPU handles a specific block and\nlayer subset while passing previous results to the next GPU, enabling\nasynchronous computation and communication. To further optimize performance, we\nincorporate two key enhancements. Firstly, a feature cache is implemented on\neach GPU to store and reuse features from the prior block as context,\nminimizing inter-GPU communication and redundant computation. Secondly, we\nemploy a coordinated noise initialization strategy, ensuring globally\nconsistent temporal dynamics by sharing initial noise patterns across GPUs\nwithout extra resource costs. Together, these enable fast, artifact-free, and\ninfinitely long video generation. Applied to the latest diffusion transformer\nvideo generator, our method efficiently produces 1,025-frame videos with up to\n6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX\n4090 GPUs.", "published": "2025-05-27 11:55:22", "link": "http://arxiv.org/abs/2505.21070v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Inverse Virtual Try-On: Generating Multi-Category Product-Style Images from Clothed Individuals", "abstract": "While virtual try-on (VTON) systems aim to render a garment onto a target\nperson image, this paper tackles the novel task of virtual try-off (VTOFF),\nwhich addresses the inverse problem: generating standardized product images of\ngarments from real-world photos of clothed individuals. Unlike VTON, which must\nresolve diverse pose and style variations, VTOFF benefits from a consistent and\nwell-defined output format -- typically a flat, lay-down-style representation\nof the garment -- making it a promising tool for data generation and dataset\nenhancement. However, existing VTOFF approaches face two major limitations: (i)\ndifficulty in disentangling garment features from occlusions and complex poses,\noften leading to visual artifacts, and (ii) restricted applicability to\nsingle-category garments (e.g., upper-body clothes only), limiting\ngeneralization. To address these challenges, we present Text-Enhanced\nMUlti-category Virtual Try-Off (TEMU-VTOFF), a novel architecture featuring a\ndual DiT-based backbone with a modified multimodal attention mechanism for\nrobust garment feature extraction. Our architecture is designed to receive\ngarment information from multiple modalities like images, text, and masks to\nwork in a multi-category setting. Finally, we propose an additional alignment\nmodule to further refine the generated visual details. Experiments on VITON-HD\nand Dress Code datasets show that TEMU-VTOFF sets a new state-of-the-art on the\nVTOFF task, significantly improving both visual quality and fidelity to the\ntarget garments.", "published": "2025-05-27 11:47:51", "link": "http://arxiv.org/abs/2505.21062v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and Styles", "abstract": "Stylizing 3D scenes instantly while maintaining multi-view consistency and\nfaithfully resembling a style image remains a significant challenge. Current\nstate-of-the-art 3D stylization methods typically involve computationally\nintensive test-time optimization to transfer artistic features into a\npretrained 3D representation, often requiring dense posed input images. In\ncontrast, leveraging recent advances in feed-forward reconstruction models, we\ndemonstrate a novel approach to achieve direct 3D stylization in less than a\nsecond using unposed sparse-view scene images and an arbitrary style image. To\naddress the inherent decoupling between reconstruction and stylization, we\nintroduce a branched architecture that separates structure modeling and\nappearance shading, effectively preventing stylistic transfer from distorting\nthe underlying 3D scene structure. Furthermore, we adapt an identity loss to\nfacilitate pre-training our stylization model through the novel view synthesis\ntask. This strategy also allows our model to retain its original reconstruction\ncapabilities while being fine-tuned for stylization. Comprehensive evaluations,\nusing both in-domain and out-of-domain datasets, demonstrate that our approach\nproduces high-quality stylized 3D content that achieve a superior blend of\nstyle and scene appearance, while also outperforming existing methods in terms\nof multi-view consistency and efficiency.", "published": "2025-05-27 11:47:15", "link": "http://arxiv.org/abs/2505.21060v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Advancing high-fidelity 3D and Texture Generation with 2.5D latents", "abstract": "Despite the availability of large-scale 3D datasets and advancements in 3D\ngenerative models, the complexity and uneven quality of 3D geometry and texture\ndata continue to hinder the performance of 3D generation techniques. In most\nexisting approaches, 3D geometry and texture are generated in separate stages\nusing different models and non-unified representations, frequently leading to\nunsatisfactory coherence between geometry and texture. To address these\nchallenges, we propose a novel framework for joint generation of 3D geometry\nand texture. Specifically, we focus in generate a versatile 2.5D\nrepresentations that can be seamlessly transformed between 2D and 3D. Our\napproach begins by integrating multiview RGB, normal, and coordinate images\ninto a unified representation, termed as 2.5D latents. Next, we adapt\npre-trained 2D foundation models for high-fidelity 2.5D generation, utilizing\nboth text and image conditions. Finally, we introduce a lightweight 2.5D-to-3D\nrefiner-decoder framework that efficiently generates detailed 3D\nrepresentations from 2.5D images. Extensive experiments demonstrate that our\nmodel not only excels in generating high-quality 3D objects with coherent\nstructure and color from text and image inputs but also significantly\noutperforms existing methods in geometry-conditioned texture generation.", "published": "2025-05-27 11:35:35", "link": "http://arxiv.org/abs/2505.21050v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Robust Video-Based Pothole Detection and Area Estimation for Intelligent Vehicles with Depth Map and Kalman Smoothing", "abstract": "Road potholes pose a serious threat to driving safety and comfort, making\ntheir detection and assessment a critical task in fields such as autonomous\ndriving. When driving vehicles, the operators usually avoid large potholes and\napproach smaller ones at reduced speeds to ensure safety. Therefore, accurately\nestimating pothole area is of vital importance. Most existing vision-based\nmethods rely on distance priors to construct geometric models. However, their\nperformance is susceptible to variations in camera angles and typically relies\non the assumption of a flat road surface, potentially leading to significant\nerrors in complex real-world environments. To address these problems, a robust\npothole area estimation framework that integrates object detection and\nmonocular depth estimation in a video stream is proposed in this paper. First,\nto enhance pothole feature extraction and improve the detection of small\npotholes, ACSH-YOLOv8 is proposed with ACmix module and the small object\ndetection head. Then, the BoT-SORT algorithm is utilized for pothole tracking,\nwhile DepthAnything V2 generates depth maps for each frame. With the obtained\ndepth maps and potholes labels, a novel Minimum Bounding Triangulated Pixel\n(MBTP) method is proposed for pothole area estimation. Finally, Kalman Filter\nbased on Confidence and Distance (CDKF) is developed to maintain consistency of\nestimation results across consecutive frames. The results show that ACSH-YOLOv8\nmodel achieves an AP(50) of 76.6%, representing a 7.6% improvement over YOLOv8.\nThrough CDKF optimization across consecutive frames, pothole predictions become\nmore robust, thereby enhancing the method's practical applicability.", "published": "2025-05-27 11:32:45", "link": "http://arxiv.org/abs/2505.21049v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "CityGo: Lightweight Urban Modeling and Rendering with Proxy Buildings and Residual Gaussians", "abstract": "Accurate and efficient modeling of large-scale urban scenes is critical for\napplications such as AR navigation, UAV based inspection, and smart city\ndigital twins. While aerial imagery offers broad coverage and complements\nlimitations of ground-based data, reconstructing city-scale environments from\nsuch views remains challenging due to occlusions, incomplete geometry, and high\nmemory demands. Recent advances like 3D Gaussian Splatting (3DGS) improve\nscalability and visual quality but remain limited by dense primitive usage,\nlong training times, and poor suit ability for edge devices. We propose CityGo,\na hybrid framework that combines textured proxy geometry with residual and\nsurrounding 3D Gaussians for lightweight, photorealistic rendering of urban\nscenes from aerial perspectives. Our approach first extracts compact building\nproxy meshes from MVS point clouds, then uses zero order SH Gaussians to\ngenerate occlusion-free textures via image-based rendering and back-projection.\nTo capture high-frequency details, we introduce residual Gaussians placed based\non proxy-photo discrepancies and guided by depth priors. Broader urban context\nis represented by surrounding Gaussians, with importance-aware downsampling\napplied to non-critical regions to reduce redundancy. A tailored optimization\nstrategy jointly refines proxy textures and Gaussian parameters, enabling\nreal-time rendering of complex urban scenes on mobile GPUs with significantly\nreduced training and memory requirements. Extensive experiments on real-world\naerial datasets demonstrate that our hybrid representation significantly\nreduces training time, achieving on average 1.4x speedup, while delivering\ncomparable visual fidelity to pure 3D Gaussian Splatting approaches.\nFurthermore, CityGo enables real-time rendering of large-scale urban scenes on\nmobile consumer GPUs, with substantially reduced memory usage and energy\nconsumption.", "published": "2025-05-27 11:24:08", "link": "http://arxiv.org/abs/2505.21041v1", "categories": ["cs.GR", "cs.CV"], "primary_category": "cs.GR"}
{"title": "Unified Alignment Protocol: Making Sense of the Unlabeled Data in New Domains", "abstract": "Semi-Supervised Federated Learning (SSFL) is gaining popularity over\nconventional Federated Learning in many real-world applications. Due to the\npractical limitation of limited labeled data on the client side, SSFL considers\nthat participating clients train with unlabeled data, and only the central\nserver has the necessary resources to access limited labeled data, making it an\nideal fit for real-world applications (e.g., healthcare). However, traditional\nSSFL assumes that the data distributions in the training phase and testing\nphase are the same. In practice, however, domain shifts frequently occur,\nmaking it essential for SSFL to incorporate generalization capabilities and\nenhance their practicality. The core challenge is improving model\ngeneralization to new, unseen domains while the client participate in SSFL.\nHowever, the decentralized setup of SSFL and unsupervised client training\nnecessitates innovation to achieve improved generalization across domains. To\nachieve this, we propose a novel framework called the Unified Alignment\nProtocol (UAP), which consists of an alternating two-stage training process.\nThe first stage involves training the server model to learn and align the\nfeatures with a parametric distribution, which is subsequently communicated to\nclients without additional communication overhead. The second stage proposes a\nnovel training algorithm that utilizes the server feature distribution to align\nclient features accordingly. Our extensive experiments on standard domain\ngeneralization benchmark datasets across multiple model architectures reveal\nthat proposed UAP successfully achieves SOTA generalization performance in SSFL\nsetting.", "published": "2025-05-27 10:44:55", "link": "http://arxiv.org/abs/2505.21010v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Facial Attribute Based Text Guided Face Anonymization", "abstract": "The increasing prevalence of computer vision applications necessitates\nhandling vast amounts of visual data, often containing personal information.\nWhile this technology offers significant benefits, it should not compromise\nprivacy. Data privacy regulations emphasize the need for individual consent for\nprocessing personal data, hindering researchers' ability to collect\nhigh-quality datasets containing the faces of the individuals. This paper\npresents a deep learning-based face anonymization pipeline to overcome this\nchallenge. Unlike most of the existing methods, our method leverages recent\nadvancements in diffusion-based inpainting models, eliminating the need for\ntraining Generative Adversarial Networks. The pipeline employs a three-stage\napproach: face detection with RetinaNet, feature extraction with VGG-Face, and\nrealistic face generation using the state-of-the-art BrushNet diffusion model.\nBrushNet utilizes the entire image, face masks, and text prompts specifying\ndesired facial attributes like age, ethnicity, gender, and expression. This\nenables the generation of natural-looking images with unrecognizable\nindividuals, facilitating the creation of privacy-compliant datasets for\ncomputer vision research.", "published": "2025-05-27 10:36:35", "link": "http://arxiv.org/abs/2505.21002v1", "categories": ["cs.CV", "I.4.9; I.2.10; I.4.8"], "primary_category": "cs.CV"}
{"title": "Assessing the Use of Face Swapping Methods as Face Anonymizers in Videos", "abstract": "The increasing demand for large-scale visual data, coupled with strict\nprivacy regulations, has driven research into anonymization methods that hide\npersonal identities without seriously degrading data quality. In this paper, we\nexplore the potential of face swapping methods to preserve privacy in video\ndata. Through extensive evaluations focusing on temporal consistency, anonymity\nstrength, and visual fidelity, we find that face swapping techniques can\nproduce consistent facial transitions and effectively hide identities. These\nresults underscore the suitability of face swapping for privacy-preserving\nvideo applications and lay the groundwork for future advancements in\nanonymization focused face-swapping models.", "published": "2025-05-27 10:19:11", "link": "http://arxiv.org/abs/2505.20985v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Generative Image Compression by Estimating Gradients of the Rate-variable Feature Distribution", "abstract": "While learned image compression (LIC) focuses on efficient data transmission,\ngenerative image compression (GIC) extends this framework by integrating\ngenerative modeling to produce photo-realistic reconstructed images. In this\npaper, we propose a novel diffusion-based generative modeling framework\ntailored for generative image compression. Unlike prior diffusion-based\napproaches that indirectly exploit diffusion modeling, we reinterpret the\ncompression process itself as a forward diffusion path governed by stochastic\ndifferential equations (SDEs). A reverse neural network is trained to\nreconstruct images by reversing the compression process directly, without\nrequiring Gaussian noise initialization. This approach achieves smooth rate\nadjustment and photo-realistic reconstructions with only a minimal number of\nsampling steps. Extensive experiments on benchmark datasets demonstrate that\nour method outperforms existing generative image compression approaches across\na range of metrics, including perceptual distortion, statistical fidelity, and\nno-reference quality assessments.", "published": "2025-05-27 10:18:24", "link": "http://arxiv.org/abs/2505.20984v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "DreamBoothDPO: Improving Personalized Generation using Direct Preference Optimization", "abstract": "Personalized diffusion models have shown remarkable success in Text-to-Image\n(T2I) generation by enabling the injection of user-defined concepts into\ndiverse contexts. However, balancing concept fidelity with contextual alignment\nremains a challenging open problem. In this work, we propose an RL-based\napproach that leverages the diverse outputs of T2I models to address this\nissue. Our method eliminates the need for human-annotated scores by generating\na synthetic paired dataset for DPO-like training using external quality\nmetrics. These better-worse pairs are specifically constructed to improve both\nconcept fidelity and prompt adherence. Moreover, our approach supports flexible\nadjustment of the trade-off between image fidelity and textual alignment.\nThrough multi-step training, our approach outperforms a naive baseline in\nconvergence speed and output quality. We conduct extensive qualitative and\nquantitative analysis, demonstrating the effectiveness of our method across\nvarious architectures and fine-tuning techniques. The source code can be found\nat https://github.com/ControlGenAI/DreamBoothDPO.", "published": "2025-05-27 10:07:50", "link": "http://arxiv.org/abs/2505.20975v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes", "abstract": "Neural fields (NFs) have demonstrated remarkable performance in scene\nreconstruction, powering various tasks such as novel view synthesis. However,\nexisting NF methods relying on RGB or LiDAR inputs often exhibit severe\nfragility to adverse weather, particularly when applied in outdoor scenarios\nlike autonomous driving. In contrast, millimeter-wave radar is inherently\nrobust to environmental changes, while unfortunately, its integration with NFs\nremains largely underexplored. Besides, as outdoor driving scenarios frequently\ninvolve moving objects, making spatiotemporal modeling essential for temporally\nconsistent novel view synthesis. To this end, we introduce RF4D, a radar-based\nneural field framework specifically designed for novel view synthesis in\noutdoor dynamic scenes. RF4D explicitly incorporates temporal information into\nits representation, significantly enhancing its capability to model moving\nobjects. We further introduce a feature-level flow module that predicts latent\ntemporal offsets between adjacent frames, enforcing temporal coherence in\ndynamic scene modeling. Moreover, we propose a radar-specific power rendering\nformulation closely aligned with radar sensing physics, improving synthesis\naccuracy and interoperability. Extensive experiments on public radar datasets\ndemonstrate the superior performance of RF4D in terms of radar measurement\nsynthesis quality and occupancy estimation accuracy, achieving especially\npronounced improvements in dynamic outdoor scenarios.", "published": "2025-05-27 09:59:05", "link": "http://arxiv.org/abs/2505.20967v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Object-Centric Action-Enhanced Representations for Robot Visuo-Motor Policy Learning", "abstract": "Learning visual representations from observing actions to benefit robot\nvisuo-motor policy generation is a promising direction that closely resembles\nhuman cognitive function and perception. Motivated by this, and further\ninspired by psychological theories suggesting that humans process scenes in an\nobject-based fashion, we propose an object-centric encoder that performs\nsemantic segmentation and visual representation generation in a coupled manner,\nunlike other works, which treat these as separate processes. To achieve this,\nwe leverage the Slot Attention mechanism and use the SOLV model, pretrained in\nlarge out-of-domain datasets, to bootstrap fine-tuning on human action video\ndata. Through simulated robotic tasks, we demonstrate that visual\nrepresentations can enhance reinforcement and imitation learning training,\nhighlighting the effectiveness of our integrated approach for semantic\nsegmentation and encoding. Furthermore, we show that exploiting models\npretrained on out-of-domain datasets can benefit this process, and that\nfine-tuning on datasets depicting human actions -- although still out-of-domain\n-- , can significantly improve performance due to close alignment with robotic\ntasks. These findings show the capability to reduce reliance on annotated or\nrobot-specific action datasets and the potential to build on existing visual\nencoders to accelerate training and improve generalizability.", "published": "2025-05-27 09:56:52", "link": "http://arxiv.org/abs/2505.20962v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "OrienText: Surface Oriented Textual Image Generation", "abstract": "Textual content in images is crucial in e-commerce sectors, particularly in\nmarketing campaigns, product imaging, advertising, and the entertainment\nindustry. Current text-to-image (T2I) generation diffusion models, though\nproficient at producing high-quality images, often struggle to incorporate text\naccurately onto complex surfaces with varied perspectives, such as angled views\nof architectural elements like buildings, banners, or walls. In this paper, we\nintroduce the Surface Oriented Textual Image Generation (OrienText) method,\nwhich leverages region-specific surface normals as conditional input to T2I\ngeneration diffusion model. Our approach ensures accurate rendering and correct\norientation of the text within the image context. We demonstrate the\neffectiveness of the OrienText method on a self-curated dataset of images and\ncompare it against the existing textual image generation methods.", "published": "2025-05-27 09:53:04", "link": "http://arxiv.org/abs/2505.20958v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DSOcc: Leveraging Depth Awareness and Semantic Aid to Boost Camera-Based 3D Semantic Occupancy Prediction", "abstract": "Camera-based 3D semantic occupancy prediction offers an efficient and\ncost-effective solution for perceiving surrounding scenes in autonomous\ndriving. However, existing works rely on explicit occupancy state inference,\nleading to numerous incorrect feature assignments, and insufficient samples\nrestrict the learning of occupancy class inference. To address these\nchallenges, we propose leveraging Depth awareness and Semantic aid to boost\ncamera-based 3D semantic Occupancy prediction (DSOcc). We jointly perform\noccupancy state and occupancy class inference, where soft occupancy confidence\nis calculated through non-learning method and multiplied with image features to\nmake the voxel representation aware of depth, enabling adaptive implicit\noccupancy state inference. Rather than focusing on improving feature learning,\nwe directly utilize well-trained image semantic segmentation and fuse multiple\nframes with their occupancy probabilities to aid occupancy class inference,\nthereby enhancing robustness. Experimental results demonstrate that DSOcc\nachieves state-of-the-art performance on the SemanticKITTI dataset among\ncamera-based methods.", "published": "2025-05-27 09:45:00", "link": "http://arxiv.org/abs/2505.20951v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "PMA: Towards Parameter-Efficient Point Cloud Understanding via Point Mamba Adapter", "abstract": "Applying pre-trained models to assist point cloud understanding has recently\nbecome a mainstream paradigm in 3D perception. However, existing application\nstrategies are straightforward, utilizing only the final output of the\npre-trained model for various task heads. It neglects the rich complementary\ninformation in the intermediate layer, thereby failing to fully unlock the\npotential of pre-trained models. To overcome this limitation, we propose an\northogonal solution: Point Mamba Adapter (PMA), which constructs an ordered\nfeature sequence from all layers of the pre-trained model and leverages Mamba\nto fuse all complementary semantics, thereby promoting comprehensive point\ncloud understanding. Constructing this ordered sequence is non-trivial due to\nthe inherent isotropy of 3D space. Therefore, we further propose a\ngeometry-constrained gate prompt generator (G2PG) shared across different\nlayers, which applies shared geometric constraints to the output gates of the\nMamba and dynamically optimizes the spatial order, thus enabling more effective\nintegration of multi-layer information. Extensive experiments conducted on\nchallenging point cloud datasets across various tasks demonstrate that our PMA\nelevates the capability for point cloud understanding to a new level by fusing\ndiverse complementary intermediate features. Code is available at\nhttps://github.com/zyh16143998882/PMA.", "published": "2025-05-27 09:27:16", "link": "http://arxiv.org/abs/2505.20941v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ISAC: Training-Free Instance-to-Semantic Attention Control for Improving Multi-Instance Generation", "abstract": "Text-to-image diffusion models excel at generating single-instance scenes but\nstruggle with multi-instance scenarios, often merging or omitting objects.\nUnlike previous training-free approaches that rely solely on semantic-level\nguidance without addressing instance individuation, our training-free method,\nInstance-to-Semantic Attention Control (ISAC), explicitly resolves incomplete\ninstance formation and semantic entanglement through an instance-first modeling\napproach. This enables ISAC to effectively leverage a hierarchical,\ntree-structured prompt mechanism, disentangling multiple object instances and\nindividually aligning them with their corresponding semantic labels. Without\nemploying any external models, ISAC achieves up to 52% average multi-class\naccuracy and 83% average multi-instance accuracy by effectively forming\ndisentangled instances. The code will be made available upon publication.", "published": "2025-05-27 09:23:10", "link": "http://arxiv.org/abs/2505.20935v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "QwT-v2: Practical, Effective and Efficient Post-Training Quantization", "abstract": "Network quantization is arguably one of the most practical network\ncompression approaches for reducing the enormous resource consumption of modern\ndeep neural networks. They usually require diverse and subtle design choices\nfor specific architecture and tasks. Instead, the QwT method is a simple and\ngeneral approach which introduces lightweight additional structures to improve\nquantization. But QwT incurs extra parameters and latency. More importantly,\nQwT is not compatible with many hardware platforms. In this paper, we propose\nQwT-v2, which not only enjoys all advantages of but also resolves major defects\nof QwT. By adopting a very lightweight channel-wise affine compensation (CWAC)\nmodule, QwT-v2 introduces significantly less extra parameters and computations\ncompared to QwT, and at the same time matches or even outperforms QwT in\naccuracy. The compensation module of QwT-v2 can be integrated into quantization\ninference engines with little effort, which not only effectively removes the\nextra costs but also makes it compatible with most existing hardware platforms.", "published": "2025-05-27 09:21:36", "link": "http://arxiv.org/abs/2505.20932v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Good Enough: Is it Worth Improving your Label Quality?", "abstract": "Improving label quality in medical image segmentation is costly, but its\nbenefits remain unclear. We systematically evaluate its impact using multiple\npseudo-labeled versions of CT datasets, generated by models like nnU-Net,\nTotalSegmentator, and MedSAM. Our results show that while higher-quality labels\nimprove in-domain performance, gains remain unclear if below a small threshold.\nFor pre-training, label quality has minimal impact, suggesting that models\nrather transfer general concepts than detailed annotations. These findings\nprovide guidance on when improving label quality is worth the effort.", "published": "2025-05-27 09:18:24", "link": "http://arxiv.org/abs/2505.20928v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "HuMoCon: Concept Discovery for Human Motion Understanding", "abstract": "We present HuMoCon, a novel motion-video understanding framework designed for\nadvanced human behavior analysis. The core of our method is a human motion\nconcept discovery framework that efficiently trains multi-modal encoders to\nextract semantically meaningful and generalizable features. HuMoCon addresses\nkey challenges in motion concept discovery for understanding and reasoning,\nincluding the lack of explicit multi-modality feature alignment and the loss of\nhigh-frequency information in masked autoencoding frameworks. Our approach\nintegrates a feature alignment strategy that leverages video for contextual\nunderstanding and motion for fine-grained interaction modeling, further with a\nvelocity reconstruction mechanism to enhance high-frequency feature expression\nand mitigate temporal over-smoothing. Comprehensive experiments on standard\nbenchmarks demonstrate that HuMoCon enables effective motion concept discovery\nand significantly outperforms state-of-the-art methods in training large models\nfor human motion understanding. We will open-source the associated code with\nour paper.", "published": "2025-05-27 09:10:59", "link": "http://arxiv.org/abs/2505.20920v1", "categories": ["cs.CV", "68T07", "I.2.10; I.2.7"], "primary_category": "cs.CV"}
{"title": "Geometry-Editable and Appearance-Preserving Object Compositon", "abstract": "General object composition (GOC) aims to seamlessly integrate a target object\ninto a background scene with desired geometric properties, while simultaneously\npreserving its fine-grained appearance details. Recent approaches derive\nsemantic embeddings and integrate them into advanced diffusion models to enable\ngeometry-editable generation. However, these highly compact embeddings encode\nonly high-level semantic cues and inevitably discard fine-grained appearance\ndetails. We introduce a Disentangled Geometry-editable and\nAppearance-preserving Diffusion (DGAD) model that first leverages semantic\nembeddings to implicitly capture the desired geometric transformations and then\nemploys a cross-attention retrieval mechanism to align fine-grained appearance\nfeatures with the geometry-edited representation, facilitating both precise\ngeometry editing and faithful appearance preservation in object composition.\nSpecifically, DGAD builds on CLIP/DINO-derived and reference networks to\nextract semantic embeddings and appearance-preserving representations, which\nare then seamlessly integrated into the encoding and decoding pipelines in a\ndisentangled manner. We first integrate the semantic embeddings into\npre-trained diffusion models that exhibit strong spatial reasoning capabilities\nto implicitly capture object geometry, thereby facilitating flexible object\nmanipulation and ensuring effective editability. Then, we design a dense\ncross-attention mechanism that leverages the implicitly learned object geometry\nto retrieve and spatially align appearance features with their corresponding\nregions, ensuring faithful appearance consistency. Extensive experiments on\npublic benchmarks demonstrate the effectiveness of the proposed DGAD framework.", "published": "2025-05-27 09:05:28", "link": "http://arxiv.org/abs/2505.20914v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Create Anything Anywhere: Layout-Controllable Personalized Diffusion Model for Multiple Subjects", "abstract": "Diffusion models have significantly advanced text-to-image generation, laying\nthe foundation for the development of personalized generative frameworks.\nHowever, existing methods lack precise layout controllability and overlook the\npotential of dynamic features of reference subjects in improving fidelity. In\nthis work, we propose Layout-Controllable Personalized Diffusion\n(LCP-Diffusion) model, a novel framework that integrates subject identity\npreservation with flexible layout guidance in a tuning-free approach. Our model\nemploys a Dynamic-Static Complementary Visual Refining module to\ncomprehensively capture the intricate details of reference subjects, and\nintroduces a Dual Layout Control mechanism to enforce robust spatial control\nacross both training and inference stages. Extensive experiments validate that\nLCP-Diffusion excels in both identity preservation and layout controllability.\nTo the best of our knowledge, this is a pioneering work enabling users to\n\"create anything anywhere\".", "published": "2025-05-27 08:57:07", "link": "http://arxiv.org/abs/2505.20909v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "HTMNet: A Hybrid Network with Transformer-Mamba Bottleneck Multimodal Fusion for Transparent and Reflective Objects Depth Completion", "abstract": "Transparent and reflective objects pose significant challenges for depth\nsensors, resulting in incomplete depth information that adversely affects\ndownstream robotic perception and manipulation tasks. To address this issue, we\npropose HTMNet, a novel hybrid model integrating Transformer, CNN, and Mamba\narchitectures. The encoder is constructed based on a dual-branch\nTransformer-CNN framework, while the multi-scale fusion module leverages a\nTransformer-Mamba architecture, which also serves as the foundation for the\ndecoder design. We introduce a novel multimodal fusion module grounded in\nself-attention mechanisms and state space models, marking the first application\nof the Mamba architecture in the field of transparent object depth completion\nand revealing its promising potential. Additionally, we design an innovative\nmulti-scale fusion module for the decoder that combines channel attention,\nspatial attention, and multi-scale feature extraction techniques to effectively\nintegrate multi-scale features through a down-fusion strategy. Extensive\nevaluations on multiple public datasets demonstrate that our model achieves\nstate-of-the-art(SOTA) performance, validating the effectiveness of our\napproach.", "published": "2025-05-27 08:51:38", "link": "http://arxiv.org/abs/2505.20904v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Multitemporal Latent Dynamical Framework for Hyperspectral Images Unmixing", "abstract": "Multitemporal hyperspectral unmixing can capture dynamical evolution of\nmaterials. Despite its capability, current methods emphasize variability of\nendmembers while neglecting dynamics of abundances, which motivates our\nadoption of neural ordinary differential equations to model abundances\ntemporally. However, this motivation is hindered by two challenges: the\ninherent complexity in defining, modeling and solving problem, and the absence\nof theoretical support. To address above challenges, in this paper, we propose\na multitemporal latent dynamical (MiLD) unmixing framework by capturing\ndynamical evolution of materials with theoretical validation. For addressing\nmultitemporal hyperspectral unmixing, MiLD consists of problem definition,\nmathematical modeling, solution algorithm and theoretical support. We formulate\nmultitemporal unmixing problem definition by conducting ordinary differential\nequations and developing latent variables. We transfer multitemporal unmixing\nto mathematical model by dynamical discretization approaches, which describe\nthe discreteness of observed sequence images with mathematical expansions. We\npropose algorithm to solve problem and capture dynamics of materials, which\napproximates abundance evolution by neural networks. Furthermore, we provide\ntheoretical support by validating the crucial properties, which verifies\nconsistency, convergence and stability theorems. The major contributions of\nMiLD include defining problem by ordinary differential equations, modeling\nproblem by dynamical discretization approach, solving problem by multitemporal\nunmixing algorithm, and presenting theoretical support. Our experiments on both\nsynthetic and real datasets have validated the utility of our work", "published": "2025-05-27 08:48:49", "link": "http://arxiv.org/abs/2505.20902v1", "categories": ["eess.IV", "cs.CV", "68T07", "I.4.10"], "primary_category": "eess.IV"}
{"title": "Frequency Composition for Compressed and Domain-Adaptive Neural Networks", "abstract": "Modern on-device neural network applications must operate under resource\nconstraints while adapting to unpredictable domain shifts. However, this\ncombined challenge-model compression and domain adaptation-remains largely\nunaddressed, as prior work has tackled each issue in isolation: compressed\nnetworks prioritize efficiency within a fixed domain, whereas large, capable\nmodels focus on handling domain shifts. In this work, we propose CoDA, a\nfrequency composition-based framework that unifies compression and domain\nadaptation. During training, CoDA employs quantization-aware training (QAT)\nwith low-frequency components, enabling a compressed model to selectively learn\nrobust, generalizable features. At test time, it refines the compact model in a\nsource-free manner (i.e., test-time adaptation, TTA), leveraging the\nfull-frequency information from incoming data to adapt to target domains while\ntreating high-frequency components as domain-specific cues. LFC are aligned\nwith the trained distribution, while HFC unique to the target distribution are\nsolely utilized for batch normalization. CoDA can be integrated synergistically\ninto existing QAT and TTA methods. CoDA is evaluated on widely used\ndomain-shift benchmarks, including CIFAR10-C and ImageNet-C, across various\nmodel architectures. With significant compression, it achieves accuracy\nimprovements of 7.96%p on CIFAR10-C and 5.37%p on ImageNet-C over the\nfull-precision TTA baseline.", "published": "2025-05-27 08:33:04", "link": "http://arxiv.org/abs/2505.20890v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "YOLO-FireAD: Efficient Fire Detection via Attention-Guided Inverted Residual Learning and Dual-Pooling Feature Preservation", "abstract": "Fire detection in dynamic environments faces continuous challenges, including\nthe interference of illumination changes, many false detections or missed\ndetections, and it is difficult to achieve both efficiency and accuracy. To\naddress the problem of feature extraction limitation and information loss in\nthe existing YOLO-based models, this study propose You Only Look Once for Fire\nDetection with Attention-guided Inverted Residual and Dual-pooling Downscale\nFusion (YOLO-FireAD) with two core innovations: (1) Attention-guided Inverted\nResidual Block (AIR) integrates hybrid channel-spatial attention with inverted\nresiduals to adaptively enhance fire features and suppress environmental noise;\n(2) Dual Pool Downscale Fusion Block (DPDF) preserves multi-scale fire patterns\nthrough learnable fusion of max-average pooling outputs, mitigating small-fire\ndetection failures. Extensive evaluation on two public datasets shows the\nefficient performance of our model. Our proposed model keeps the sum amount of\nparameters (1.45M, 51.8% lower than YOLOv8n) (4.6G, 43.2% lower than YOLOv8n),\nand mAP75 is higher than the mainstream real-time object detection models\nYOLOv8n, YOL-Ov9t, YOLOv10n, YOLO11n, YOLOv12n and other YOLOv8 variants\n1.3-5.5%.", "published": "2025-05-27 08:29:07", "link": "http://arxiv.org/abs/2505.20884v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Stereo Radargrammetry Using Deep Learning from Airborne SAR Images", "abstract": "In this paper, we propose a stereo radargrammetry method using deep learning\nfrom airborne Synthetic Aperture Radar (SAR) images.Deep learning-based methods\nare considered to suffer less from geometric image modulation, while there is\nno public SAR image dataset used to train such methods.We create a SAR image\ndataset and perform fine-tuning of a deep learning-based image correspondence\nmethod.The proposed method suppresses the degradation of image quality by pixel\ninterpolation without ground projection of the SAR image and divides the SAR\nimage into patches for processing, which makes it possible to apply deep\nlearning.Through a set of experiments, we demonstrate that the proposed method\nexhibits a wider range and more accurate elevation measurements compared to\nconventional methods.", "published": "2025-05-27 08:24:17", "link": "http://arxiv.org/abs/2505.20876v1", "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual Large Language Models", "abstract": "The goal of this work is to enhance balanced multimodal understanding in\naudio-visual large language models (AV-LLMs) by addressing modality bias\nwithout requiring additional training. In current AV-LLMs, audio and video\nfeatures are typically processed jointly in the decoder. While this strategy\nfacilitates unified multimodal understanding, it may introduce modality bias,\nwhere the model tends to over-rely on one modality due to imbalanced training\nsignals. To mitigate this, we propose Fork-Merge Decoding (FMD), a simple yet\neffective inference-time strategy that requires no additional training or\narchitectural modifications. FMD first performs modality-specific reasoning by\nprocessing audio-only and video-only inputs through the early decoder layers (a\nfork phase), and then merges the resulting hidden states for joint reasoning in\nthe remaining layers (a merge phase). This approach promotes balanced modality\ncontributions and leverages complementary information across modalities. We\nevaluate our method on two representative AV-LLMs, VideoLLaMA2 and\nvideo-SALMONN, using three benchmark datasets. Experimental results demonstrate\nconsistent performance improvements on tasks focused on audio, video, and\ncombined audio-visual reasoning, demonstrating the effectiveness of\ninference-time interventions for robust multimodal understanding.", "published": "2025-05-27 08:22:56", "link": "http://arxiv.org/abs/2505.20873v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "In Context Learning with Vision Transformers: Case Study", "abstract": "Large transformer models have been shown to be capable of performing\nin-context learning. By using examples in a prompt as well as a query, they are\ncapable of performing tasks such as few-shot, one-shot, or zero-shot learning\nto output the corresponding answer to this query. One area of interest to us is\nthat these transformer models have been shown to be capable of learning the\ngeneral class of certain functions, such as linear functions and small 2-layer\nneural networks, on random data (Garg et al, 2023). We aim to extend this to\nthe image space to analyze their capability to in-context learn more complex\nfunctions on the image space, such as convolutional neural networks and other\nmethods.", "published": "2025-05-27 08:22:08", "link": "http://arxiv.org/abs/2505.20872v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.6; I.2.10; I.4.8"], "primary_category": "cs.CV"}
{"title": "AVCD: Mitigating Hallucinations in Audio-Visual Large Language Models through Contrastive Decoding", "abstract": "Hallucination remains a major challenge in multimodal large language models\n(MLLMs). To address this, various contrastive decoding (CD) methods have been\nproposed that contrasts original logits with hallucinated logits generated from\nperturbed inputs. While CD has shown promise in vision-language models (VLMs),\nit is not well-suited for AV-LLMs, where hallucinations often emerge from both\nunimodal and cross-modal combinations involving audio, video, and language.\nThese intricate interactions call for a more adaptive and modality-aware\ndecoding strategy. In this paper, we propose Audio-Visual Contrastive Decoding\n(AVCD)-a novel, training-free decoding framework designed to model trimodal\ninteractions and suppress modality-induced hallucinations in AV-LLMs. Unlike\nprevious CD methods in VLMs that corrupt a fixed modality, AVCD leverages\nattention distributions to dynamically identify less dominant modalities and\napplies attentive masking to generate perturbed output logits. To support CD in\na trimodal setting, we also reformulate the original CD framework to jointly\nhandle audio, visual, and textual inputs. Finally, to improve efficiency, we\nintroduce entropy-guided adaptive decoding, which selectively skips unnecessary\ndecoding steps based on the model's confidence in its predictions. Extensive\nexperiments demonstrate that AVCD consistently outperforms existing decoding\nmethods. Especially, on the AVHBench dataset, it improves accuracy by 6% for\nVideoLLaMA2 and 11% for video-SALMONN, demonstrating strong robustness and\ngeneralizability.", "published": "2025-05-27 08:13:57", "link": "http://arxiv.org/abs/2505.20862v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Exploring Timeline Control for Facial Motion Generation", "abstract": "This paper introduces a new control signal for facial motion generation:\ntimeline control. Compared to audio and text signals, timelines provide more\nfine-grained control, such as generating specific facial motions with precise\ntiming. Users can specify a multi-track timeline of facial actions arranged in\ntemporal intervals, allowing precise control over the timing of each action. To\nmodel the timeline control capability, We first annotate the time intervals of\nfacial actions in natural facial motion sequences at a frame-level granularity.\nThis process is facilitated by Toeplitz Inverse Covariance-based Clustering to\nminimize human labor. Based on the annotations, we propose a diffusion-based\ngeneration model capable of generating facial motions that are natural and\naccurately aligned with input timelines. Our method supports text-guided motion\ngeneration by using ChatGPT to convert text into timelines. Experimental\nresults show that our method can annotate facial action intervals with\nsatisfactory accuracy, and produces natural facial motions accurately aligned\nwith timelines.", "published": "2025-05-27 08:13:38", "link": "http://arxiv.org/abs/2505.20861v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Pushing Cops and Robber on Graphs of Maximum Degree 4", "abstract": "\\textsc{Cops and Robber} is a game played on graphs where a set of\n\\textit{cops} aim to \\textit{capture} the position of a single \\textit{robber}.\nThe main parameter of interest in this game is the \\textit{cop number}, which\nis the minimum number of cops that are sufficient to guarantee the capture of\nthe robber.\n  In a directed graph $\\overrightarrow{G}$, the \\textit{push} operation on a\nvertex $v$ reverses the orientation of all arcs incident on $v$. We consider a\nvariation of classical \\textsc{Cops and Robber} on oriented graphs, where in\nits turn, each cop can either move to an out-neighbor of its current vertex or\npush some vertex of the graph, whereas, the robber can move to an adjacent\nvertex in its turn. [Das et al., CALDAM, 2023] introduced this variant and\nestablished that if $\\overrightarrow{G}$ is an orientation of a subcubic graph,\nthen one cop with push ability has a winning strategy. We extend these results\nto establish that if $\\overrightarrow{G}$ is an orientation of a $3$-degenerate\ngraph, or of a graph with maximum degree $4$, then one cop with push ability\nhas a winning strategy.", "published": "2025-05-27 17:22:53", "link": "http://arxiv.org/abs/2505.21450v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Rainbow copies of spanning subgraphs", "abstract": "Let $G_{n,p}^{[\\kappa]}$ denote the space of $n$-vertex edge coloured graphs,\nwhere each edge occurs independently with probability $p$. The colour of each\nexisting edge is chosen independently and uniformly at random from the set\n$[\\kappa]$. We consider the threshold for the existence of rainbow colored\ncopies of a spanning subgraph $H$. We provide lower bounds on $p$ and $\\kappa$\nsufficient to prove the existence of such copies w.h.p.", "published": "2025-05-27 14:54:32", "link": "http://arxiv.org/abs/2505.21290v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "A refined view of a curious identity for partitions into odd parts with designated summands", "abstract": "In 2002, Andrews, Lewis, and Lovejoy introduced the combinatorial objects\nwhich they called partitions with designated summands. These are constructed by\ntaking unrestricted integer partitions and designating exactly one of each\noccurrence of a part. In the same work, they also considered the restricted\npartitions with designated summands wherein all parts must be odd, and they\ndenoted the corresponding function by $\\mathrm{PDO}(n)$.", "published": "2025-05-27 12:32:36", "link": "http://arxiv.org/abs/2505.21111v1", "categories": ["math.CO", "cs.DM", "11P84, 05A17, 05A15, 33B10"], "primary_category": "math.CO"}
{"title": "Complexity landscape for local certification", "abstract": "An impressive recent line of work has charted the complexity landscape of\ndistributed graph algorithms. For many settings, it has been determined which\ntime complexities exist, and which do not (in the sense that no local problem\ncould have an optimal algorithm with that complexity). In this paper, we\ninitiate the study of the landscape for space complexity of distributed graph\nalgorithms. More precisely, we focus on the local certification setting, where\na prover assigns certificates to nodes to certify a property, and where the\nspace complexity is measured by the size of the certificates.\n  Already for anonymous paths and cycles, we unveil a surprising landscape:\n  - There is a gap between complexity $O(1)$ and $\\Theta(\\log \\log n)$ in\npaths. This is the first gap established in local certification.\n  - There exists a property that has complexity $\\Theta(\\log \\log n)$ in paths,\na regime that was not known to exist for a natural property.\n  - There is a gap between complexity $O(1)$ and $\\Theta(\\log n)$ in cycles,\nhence a gap that is exponentially larger than for paths.\n  We then generalize our result for paths to the class of trees. Namely, we\nshow that there is a gap between complexity $O(1)$ and $\\Theta(\\log \\log d)$ in\ntrees, where $d$ is the diameter. We finally describe some settings where there\nare no gaps at all. To prove our results we develop a new toolkit, based on\nvarious results of automata theory and arithmetic, which is of independent\ninterest.", "published": "2025-05-27 09:08:00", "link": "http://arxiv.org/abs/2505.20915v1", "categories": ["cs.DC", "cs.DM", "cs.DS"], "primary_category": "cs.DC"}
{"title": "Colouring Probe $H$-Free Graphs", "abstract": "The NP-complete problems Colouring and k-Colouring $(k\\geq 3$) are well\nstudied on $H$-free graphs, i.e., graphs that do not contain some fixed graph\n$H$ as an induced subgraph. We research to what extent the known\npolynomial-time algorithms for $H$-free graphs can be generalized if we only\nknow some of the edges of the input graph. We do this by considering the\nclassical probe graph model introduced in the early nineties. For a graph $H$,\na partitioned probe $H$-free graph $(G,P,N)$ consists of a graph $G=(V,E)$,\ntogether with a set $P\\subseteq V$ of probes and an independent set\n$N=V\\setminus P$ of non-probes, such that $G+F$ is $H$-free for some edge set\n$F\\subseteq \\binom{N}{2}$. We first fully classify the complexity of Colouring\non partitioned probe $H$-free graphs and show that this dichotomy is different\nfrom the known dichotomy of Colouring for $H$-free graphs. Our main result is a\ndichotomy of $3$-Colouring for partitioned probe $P_t$-free graphs: we prove\nthat the problem is polynomial-time solvable if $t\\leq 5$ but NP-complete if\n$t\\geq 6$. In contrast, $3$-Colouring on $P_t$-free graphs is known to be\npolynomial-time solvable if $t\\leq 7$ and quasi polynomial-time solvable for\n$t\\geq 8$.", "published": "2025-05-27 06:43:09", "link": "http://arxiv.org/abs/2505.20784v1", "categories": ["cs.DS", "cs.DM", "math.CO"], "primary_category": "cs.DS"}
{"title": "Counterfactual Multi-player Bandits for Explainable Recommendation Diversification", "abstract": "Existing recommender systems tend to prioritize items closely aligned with\nusers' historical interactions, inevitably trapping users in the dilemma of\n``filter bubble''. Recent efforts are dedicated to improving the diversity of\nrecommendations. However, they mainly suffer from two major issues: 1) a lack\nof explainability, making it difficult for the system designers to understand\nhow diverse recommendations are generated, and 2) limitations to specific\nmetrics, with difficulty in enhancing non-differentiable diversity metrics. To\nthis end, we propose a \\textbf{C}ounterfactual \\textbf{M}ulti-player\n\\textbf{B}andits (CMB) method to deliver explainable recommendation\ndiversification across a wide range of diversity metrics. Leveraging a\ncounterfactual framework, our method identifies the factors influencing\ndiversity outcomes. Meanwhile, we adopt the multi-player bandits to optimize\nthe counterfactual optimization objective, making it adaptable to both\ndifferentiable and non-differentiable diversity metrics. Extensive experiments\nconducted on three real-world datasets demonstrate the applicability,\neffectiveness, and explainability of the proposed CMB.", "published": "2025-05-27 13:21:39", "link": "http://arxiv.org/abs/2505.21165v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Disentangling Locality and Entropy in Ranking Distillation", "abstract": "The training process of ranking models involves two key data selection\ndecisions: a sampling strategy, and a labeling strategy. Modern ranking\nsystems, especially those for performing semantic search, typically use a\n``hard negative'' sampling strategy to identify challenging items using\nheuristics and a distillation labeling strategy to transfer ranking \"knowledge\"\nfrom a more capable model. In practice, these approaches have grown\nincreasingly expensive and complex, for instance, popular pretrained rankers\nfrom SentenceTransformers involve 12 models in an ensemble with data provenance\nhampering reproducibility. Despite their complexity, modern sampling and\nlabeling strategies have not been fully ablated, leaving the underlying source\nof effectiveness gains unclear. Thus, to better understand why models improve\nand potentially reduce the expense of training effective models, we conduct a\nbroad ablation of sampling and distillation processes in neural ranking. We\nframe and theoretically derive the orthogonal nature of model geometry affected\nby example selection and the effect of teacher ranking entropy on ranking model\noptimization, establishing conditions in which data augmentation can\neffectively improve bias in a ranking model. Empirically, our investigation on\nestablished benchmarks and common architectures shows that sampling processes\nthat were once highly effective in contrastive objectives may be spurious or\nharmful under distillation. We further investigate how data augmentation, in\nterms of inputs and targets, can affect effectiveness and the intrinsic\nbehavior of models in ranking. Through this work, we aim to encourage more\ncomputationally efficient approaches that reduce focus on contrastive pairs and\ninstead directly understand training dynamics under rankings, which better\nrepresent real-world settings.", "published": "2025-05-27 11:46:37", "link": "http://arxiv.org/abs/2505.21058v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "A Reduction-Driven Local Search for the Generalized Independent Set Problem", "abstract": "The Generalized Independent Set (GIS) problem extends the classical maximum\nindependent set problem by incorporating profits for vertices and penalties for\nedges. This generalized problem has been identified in diverse applications in\nfields such as forest harvest planning, competitive facility location, social\nnetwork analysis, and even machine learning. However, solving the GIS problem\nin large-scale, real-world networks remains computationally challenging. In\nthis paper, we explore data reduction techniques to address this challenge. We\nfirst propose 14 reduction rules that can reduce the input graph with rigorous\noptimality guarantees. We then present a reduction-driven local search (RLS)\nalgorithm that integrates these reduction rules into the pre-processing, the\ninitial solution generation, and the local search components in a\ncomputationally efficient way. The RLS is empirically evaluated on 278 graphs\narising from different application scenarios. The results indicates that the\nRLS is highly competitive -- For most graphs, it achieves significantly\nsuperior solutions compared to other known solvers, and it effectively provides\nsolutions for graphs exceeding 260 million edges, a task at which every other\nknown method fails. Analysis also reveals that the data reduction plays a key\nrole in achieving such a competitive performance.", "published": "2025-05-27 11:39:05", "link": "http://arxiv.org/abs/2505.21052v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "LifeIR at the NTCIR-18 Lifelog-6 Task", "abstract": "In recent years, sharing lifelogs recorded through wearable devices such as\nsports watches and GoPros, has gained significant popularity. Lifelogs involve\nvarious types of information, including images, videos, and GPS data, revealing\nusers' lifestyles, dietary patterns, and physical activities. The Lifelog\nSemantic Access Task(LSAT) in the NTCIR-18 Lifelog-6 Challenge focuses on\nretrieving relevant images from a large scale of users' lifelogs based on\ntextual queries describing an action or event. It serves users' need to find\nimages about a scenario in the historical moments of their lifelogs. We propose\na multi-stage pipeline for this task of searching images with texts, addressing\nvarious challenges in lifelog retrieval. Our pipeline includes: filtering\nblurred images, rewriting queries to make intents clearer, extending the\ncandidate set based on events to include images with temporal connections, and\nreranking results using a multimodal large language model(MLLM) with stronger\nrelevance judgment capabilities. The evaluation results of our submissions have\nshown the effectiveness of each stage and the entire pipeline.", "published": "2025-05-27 10:21:57", "link": "http://arxiv.org/abs/2505.20987v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Embed Progressive Implicit Preference in Unified Space for Deep Collaborative Filtering", "abstract": "Embedding-based collaborative filtering, often coupled with nearest neighbor\nsearch, is widely deployed in large-scale recommender systems for personalized\ncontent selection. Modern systems leverage multiple implicit feedback signals\n(e.g., clicks, add to cart, purchases) to model user preferences\ncomprehensively. However, prevailing approaches adopt a feedback-wise modeling\nparadigm, which (1) fails to capture the structured progression of user\nengagement entailed among different feedback and (2) embeds feedback-specific\ninformation into disjoint spaces, making representations incommensurable,\nincreasing system complexity, and leading to suboptimal retrieval performance.\nA promising alternative is Ordinal Logistic Regression (OLR), which explicitly\nmodels discrete ordered relations. However, existing OLR-based recommendation\nmodels mainly focus on explicit feedback (e.g., movie ratings) and struggle\nwith implicit, correlated feedback, where ordering is vague and non-linear.\nMoreover, standard OLR lacks flexibility in handling feedback-dependent\ncovariates, resulting in suboptimal performance in real-world systems. To\naddress these limitations, we propose Generalized Neural Ordinal Logistic\nRegression (GNOLR), which encodes multiple feature-feedback dependencies into a\nunified, structured embedding space and enforces feedback-specific dependency\nlearning through a nested optimization framework. Thus, GNOLR enhances\npredictive accuracy, captures the progression of user engagement, and\nsimplifies the retrieval process. We establish a theoretical comparison with\nexisting paradigms, demonstrating how GNOLR avoids disjoint spaces while\nmaintaining effectiveness. Extensive experiments on ten real-world datasets\nshow that GNOLR significantly outperforms state-of-the-art methods in\nefficiency and adaptability.", "published": "2025-05-27 08:43:35", "link": "http://arxiv.org/abs/2505.20900v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Cold-Start Recommendation with Knowledge-Guided Retrieval-Augmented Generation", "abstract": "Cold-start items remain a persistent challenge in recommender systems due to\ntheir lack of historical user interactions, which collaborative models rely on.\nWhile recent zero-shot methods leverage large language models (LLMs) to address\nthis, they often struggle with sparse metadata and hallucinated or incomplete\nknowledge. We propose ColdRAG, a retrieval-augmented generation approach that\nbuilds a domain-specific knowledge graph dynamically to enhance LLM-based\nrecommendation in cold-start scenarios, without requiring task-specific\nfine-tuning. ColdRAG begins by converting structured item attributes into rich\nnatural-language profiles, from which it extracts entities and relationships to\nconstruct a unified knowledge graph capturing item semantics. Given a user's\ninteraction history, it scores edges in the graph using an LLM, retrieves\ncandidate items with supporting evidence, and prompts the LLM to rank them. By\nenabling multi-hop reasoning over this graph, ColdRAG grounds recommendations\nin verifiable evidence, reducing hallucinations and strengthening semantic\nconnections. Experiments on three public benchmarks demonstrate that ColdRAG\nsurpasses existing zero-shot baselines in both Recall and NDCG. This framework\noffers a practical solution to cold-start recommendation by combining\nknowledge-graph reasoning with retrieval-augmented LLM generation.", "published": "2025-05-27 06:23:26", "link": "http://arxiv.org/abs/2505.20773v1", "categories": ["cs.IR", "68T05 68T05"], "primary_category": "cs.IR"}
{"title": "Bridging the Gap: Self-Optimized Fine-Tuning for LLM-based Recommender Systems", "abstract": "Recent years have witnessed extensive exploration of Large Language Models\n(LLMs) on the field of Recommender Systems (RS). There are currently two\ncommonly used strategies to enable LLMs to have recommendation capabilities: 1)\nThe \"Guidance-Only\" strategy uses in-context learning to exploit and amplify\nthe inherent semantic understanding and item recommendation capabilities of\nLLMs; 2) The \"Tuning-Only\" strategy uses supervised fine-tuning (SFT) to\nfine-tune LLMs with the aim of fitting them to real recommendation data.\nHowever, neither of these strategies can effectively bridge the gap between the\nknowledge space of LLMs and recommendation, and their performance do not meet\nour expectations.\n  To better enable LLMs to learn recommendation knowledge, we combine the\nadvantages of the above two strategies and proposed a novel \"Guidance+Tuning\"\nmethod called Self-Optimized Fine-Tuning (SOFT), which adopts the idea of\ncurriculum learning. It first employs self-distillation to construct an\nauxiliary easy-to-learn but meaningful dataset from a fine-tuned LLM. Then it\nfurther utilizes a self-adaptive curriculum scheduler to enable LLMs to\ngradually learn from simpler data (self-distilled data) to more challenging\ndata (real RS data). Extensive experiments demonstrate that SOFT significantly\nenhances the recommendation accuracy (37.59\\% on average) of LLM-based methods.\nThe code is available via\nhttps://anonymous.4open.science/r/Self-Optimized-Fine-Tuning-264E", "published": "2025-05-27 06:22:50", "link": "http://arxiv.org/abs/2505.20771v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "UQLegalAI@COLIEE2025: Advancing Legal Case Retrieval with Large Language Models and Graph Neural Networks", "abstract": "Legal case retrieval plays a pivotal role in the legal domain by facilitating\nthe efficient identification of relevant cases, supporting legal professionals\nand researchers to propose legal arguments and make informed decision-making.\nTo improve retrieval accuracy, the Competition on Legal Information Extraction\nand Entailment (COLIEE) is held annually, offering updated benchmark datasets\nfor evaluation. This paper presents a detailed description of CaseLink, the\nmethod employed by UQLegalAI, the second highest team in Task 1 of COLIEE 2025.\nThe CaseLink model utilises inductive graph learning and Global Case Graphs to\ncapture the intrinsic case connectivity to improve the accuracy of legal case\nretrieval. Specifically, a large language model specialized in text embedding\nis employed to transform legal texts into embeddings, which serve as the\nfeature representations of the nodes in the constructed case graph. A new\ncontrastive objective, incorporating a regularization on the degree of case\nnodes, is proposed to leverage the information within the case reference\nrelationship for model optimization. The main codebase used in our method is\nbased on an open-sourced repo of CaseLink:\nhttps://github.com/yanran-tang/CaseLink.", "published": "2025-05-27 05:32:50", "link": "http://arxiv.org/abs/2505.20743v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "What LLMs Miss in Recommendations: Bridging the Gap with Retrieval-Augmented Collaborative Signals", "abstract": "User-item interactions contain rich collaborative signals that form the\nbackbone of many successful recommender systems. While recent work has explored\nthe use of large language models (LLMs) for recommendation, it remains unclear\nwhether LLMs can effectively reason over this type of collaborative\ninformation. In this paper, we conduct a systematic comparison between LLMs and\nclassical matrix factorization (MF) models to assess LLMs' ability to leverage\nuser-item interaction data. We further introduce a simple retrieval-augmented\ngeneration (RAG) method that enhances LLMs by grounding their predictions in\nstructured interaction data. Our experiments reveal that current LLMs often\nfall short in capturing collaborative patterns inherent to MF models, but that\nour RAG-based approach substantially improves recommendation\nquality-highlighting a promising direction for future LLM-based recommenders.", "published": "2025-05-27 05:18:57", "link": "http://arxiv.org/abs/2505.20730v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "How Do Experts Make Sense of Integrated Process Models?", "abstract": "A range of integrated modeling approaches have been developed to enable a\nholistic representation of business process logic together with all relevant\nbusiness rules. These approaches address inherent problems with separate\ndocumentation of business process models and business rules. In this study, we\nexplore how expert process workers make sense of the information provided\nthrough such integrated modeling approaches. To do so, we complement verbal\nprotocol analysis with eye-tracking metrics to reveal nuanced user behaviours\ninvolved in the main phases of sensemaking, namely information foraging and\ninformation processing. By studying expert process workers engaged in tasks\nbased on integrated modeling of business processes and rules, we provide\ninsights that pave the way for a better understanding of sensemaking practices\nand improved development of business process and business rule integration\napproaches. Our research underscores the importance of offering personalized\nsupport mechanisms that increase the efficacy and efficiency of sensemaking\npractices for process knowledge workers.", "published": "2025-05-27 03:32:28", "link": "http://arxiv.org/abs/2505.20667v1", "categories": ["cs.IR", "cs.HC", "cs.SE"], "primary_category": "cs.IR"}
{"title": "TeroSeek: An AI-Powered Knowledge Base and Retrieval Generation Platform for Terpenoid Research", "abstract": "Terpenoids are a crucial class of natural products that have been studied for\nover 150 years, but their interdisciplinary nature (spanning chemistry,\npharmacology, and biology) complicates knowledge integration. To address this,\nthe authors developed TeroSeek, a curated knowledge base (KB) built from two\ndecades of terpenoid literature, coupled with an AI-powered question-answering\nchatbot and web service. Leveraging a retrieval-augmented generation (RAG)\nframework, TeroSeek provides structured, high-quality information and\noutperforms general-purpose large language models (LLMs) in terpenoid-related\nqueries. It serves as a domain-specific expert tool for multidisciplinary\nresearch and is publicly available at http://teroseek.qmclab.com.", "published": "2025-05-27 03:17:30", "link": "http://arxiv.org/abs/2505.20663v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "H.3; I.2"], "primary_category": "cs.IR"}
{"title": "Distribution Bounds on the Conditional ROC in a Poisson Field of Interferers and Clutters", "abstract": "We present a novel analytical framework to characterize the distribution of\nthe conditional receiver operating characteristic (ROC) in radar systems\noperating within a realization of a Poisson field of interferers and clutters.\nWhile conventional stochastic geometry based studies focus on the distribution\nof signal to interference and noise ratio (SINR), they fail to capture the\nstatistical variations in detection and false-alarm performance across\ndifferent network realizations. By leveraging higher-order versions of the\nCampbell-Mecke theorem and tools from stochastic geometry, we derive\nclosed-form expressions for the mean and variance of the conditional\nfalse-alarm probability, and provide tight upper bounds using Cantelli's\ninequality. Additionally, we present a beta distribution approximation to\ncapture the meta-distribution of the noise and interference power, enabling\nfine-grained performance evaluation. The results are extended to analyze the\nconditional detection probability, albeit with simpler bounds. Our approach\nreveals a new approach to radar design and robust ROC selection, including\npercentile-level guarantees, which are essential for emerging high-reliability\napplications. The insights derived here advocate for designing radar detection\nthresholds and signal processing algorithms based not merely on mean\nfalse-alarm or detection probabilities, but on tail behavior and percentile\nguarantees.", "published": "2025-05-27 17:29:20", "link": "http://arxiv.org/abs/2505.21456v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "A Convergence Theory for Diffusion Language Models: An Information-Theoretic Perspective", "abstract": "Diffusion models have emerged as a powerful paradigm for modern generative\nmodeling, demonstrating strong potential for large language models (LLMs).\nUnlike conventional autoregressive (AR) models that generate tokens\nsequentially, diffusion models enable parallel token sampling, leading to\nfaster generation and eliminating left-to-right generation constraints. Despite\ntheir empirical success, the theoretical understanding of diffusion model\napproaches remains underdeveloped. In this work, we develop convergence\nguarantees for diffusion language models from an information-theoretic\nperspective. Our analysis demonstrates that the sampling error, measured by the\nKullback-Leibler (KL) divergence, decays inversely with the number of\niterations $T$ and scales linearly with the mutual information between tokens\nin the target text sequence. In particular, we establish matching upper and\nlower bounds, up to some constant factor, to demonstrate the tightness of our\nconvergence analysis. These results offer novel theoretical insights into the\npractical effectiveness of diffusion language models.", "published": "2025-05-27 16:24:20", "link": "http://arxiv.org/abs/2505.21400v1", "categories": ["cs.LG", "cs.IT", "math.IT", "math.ST", "stat.ML", "stat.TH"], "primary_category": "cs.LG"}
{"title": "Data-Driven Cellular Mobility Management via Bayesian Optimization and Reinforcement Learning", "abstract": "Mobility management in cellular networks faces increasing complexity due to\nnetwork densification and heterogeneous user mobility characteristics.\nTraditional handover (HO) mechanisms, which rely on predefined parameters such\nas A3-offset and time-to-trigger (TTT), often fail to optimize mobility\nperformance across varying speeds and deployment conditions. Fixed A3-offset\nand TTT configurations either delay HOs, increasing radio link failures (RLFs),\nor accelerate them, leading to excessive ping-pong effects. To address these\nchallenges, we propose two data-driven mobility management approaches\nleveraging high-dimensional Bayesian optimization (HD-BO) and deep\nreinforcement learning (DRL). HD-BO optimizes HO parameters such as A3-offset\nand TTT, striking a desired trade-off between ping-pongs vs. RLF. DRL provides\na non-parameter-based approach, allowing an agent to select serving cells based\non real-time network conditions. We validate our approach using a real-world\ncellular deployment scenario, and employing Sionna ray tracing for\nsite-specific channel propagation modeling. Results show that both HD-BO and\nDRL outperform 3GPP set-1 (TTT of 480 ms and A3-offset of 3 dB) and set-5 (TTT\nof 40 ms and A3-offset of -1 dB) benchmarks. We augment HD-BO with transfer\nlearning so it can generalize across a range of user speeds. Applying the same\ntransfer-learning strategy to the DRL method reduces its training time by a\nfactor of 2.5 while preserving optimal HO performance, showing that it adapts\nefficiently to the mobility of aerial users such as UAVs. Simulations further\nreveal that HD-BO remains more sample-efficient than DRL, making it more\nsuitable for scenarios with limited training data.", "published": "2025-05-27 14:26:59", "link": "http://arxiv.org/abs/2505.21249v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Linearity-Inducing Priors for Poisson Parameter Estimation Under $L^{1}$ Loss", "abstract": "We study prior distributions for Poisson parameter estimation under $L^1$\nloss. Specifically, we construct a new family of prior distributions whose\noptimal Bayesian estimators (the conditional medians) can be any prescribed\nincreasing function that satisfies certain regularity conditions. In the case\nof affine estimators, this family is distinct from the usual conjugate priors,\nwhich are gamma distributions. Our prior distributions are constructed through\na limiting process that matches certain moment conditions. These results\nprovide the first explicit description of a family of distributions, beyond the\nconjugate priors, that satisfy the affine conditional median property; and more\nbroadly for the Poisson noise model they can give any arbitrarily prescribed\nconditional median.", "published": "2025-05-27 12:27:58", "link": "http://arxiv.org/abs/2505.21102v1", "categories": ["math.ST", "cs.IT", "math.IT", "stat.TH"], "primary_category": "math.ST"}
{"title": "Through and beyond moments, entropies and Fisher information measures: new informational functionals and inequalities", "abstract": "We introduce new classes of informational functionals, called \\emph{upper\nmoments}, respectively \\emph{down-Fisher measures}, obtained by applying\nclassical functionals such as $p$-moments and the Fisher information to the\nrecently introduced up or down transformed probability density functions. We\nextend some of the the most important informational inequalities to our new\nfunctionals and establish optimal constants and minimizers for them. In\nparticular, we highlight that, under certain constraints, the generalized Beta\nprobability density maximizes (or minimizes) the upper-moments when the moment\nis fixed. Moreover, we apply these structured inequalities to systematically\nestablish new and sharp upper bounds for the main classical informational\nproducts such as moment-entropy, Stam, or Cram\\'er-Rao like products under\ncertain regularity conditions. Other relevant properties, such as regularity\nunder scaling changes or monotonicity with respect to the parameter, are\nstudied. Applications to related problems to the Hausdorff moment problem are\nalso given.", "published": "2025-05-27 10:48:47", "link": "http://arxiv.org/abs/2505.21015v1", "categories": ["math-ph", "cs.IT", "math.IT", "math.MP"], "primary_category": "math-ph"}
{"title": "Semantic Communication meets System 2 ML: How Abstraction, Compositionality and Emergent Languages Shape Intelligence", "abstract": "The trajectories of 6G and AI are set for a creative collision. However,\ncurrent visions for 6G remain largely incremental evolutions of 5G, while\nprogress in AI is hampered by brittle, data-hungry models that lack robust\nreasoning capabilities. This paper argues for a foundational paradigm shift,\nmoving beyond the purely technical level of communication toward systems\ncapable of semantic understanding and effective, goal-oriented interaction. We\npropose a unified research vision rooted in the principles of System-2\ncognition, built upon three pillars: Abstraction, enabling agents to learn\nmeaningful world models from raw sensorimotor data; Compositionality, providing\nthe algebraic tools to combine learned concepts and subsystems; and Emergent\nCommunication, allowing intelligent agents to create their own adaptive and\ngrounded languages. By integrating these principles, we lay the groundwork for\ntruly intelligent systems that can reason, adapt, and collaborate, unifying\nadvances in wireless communications, machine learning, and robotics under a\nsingle coherent framework.", "published": "2025-05-27 09:57:12", "link": "http://arxiv.org/abs/2505.20964v1", "categories": ["cs.LG", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "Scattering Networks on Noncommutative Finite Groups", "abstract": "Scattering Networks were initially designed to elucidate the behavior of\nearly layers in Convolutional Neural Networks (CNNs) over Euclidean spaces and\nare grounded in wavelets. In this work, we introduce a scattering transform on\nan arbitrary finite group (not necessarily abelian) within the context of\ngroup-equivariant convolutional neural networks (G-CNNs). We present wavelets\non finite groups and analyze their similarity to classical wavelets. We\ndemonstrate that, under certain conditions in the wavelet coefficients, the\nscattering transform is non-expansive, stable under deformations, preserves\nenergy, equivariant with respect to left and right group translations, and, as\ndepth increases, the scattering coefficients are less sensitive to group\ntranslations of the signal, all desirable properties of convolutional neural\nnetworks. Furthermore, we provide examples illustrating the application of the\nscattering transform to classify data with domains involving abelian and\nnonabelian groups.", "published": "2025-05-27 09:41:39", "link": "http://arxiv.org/abs/2505.20950v1", "categories": ["math.NA", "cs.IT", "cs.LG", "cs.NA", "eess.SP", "math.IT"], "primary_category": "math.NA"}
{"title": "Polarforming for Wireless Networks: Opportunities and Challenges", "abstract": "Polarforming emerges as a promising technique for manipulating the\npolarization of electromagnetic (EM) waves by shaping the polarization of an\nantenna into a desired state. By dynamically adjusting antenna polarization,\npolarforming enables real-time polarization matching or mismatching with\nreceived EM waves, thereby leveraging polarization degrees of freedom (DoFs) to\nenhance wireless communication performance. In this article, we first present\nan overview of the fundamental principles and design approaches underlying the\npolarforming technique. We then analyze the key advantages of polarforming,\nincluding hardware cost reduction, depolarization mitigation, channel\nadaptation, signal power enhancement, and interference suppression.\nFurthermore, we explore promising applications of polarforming for\nnext-generation wireless networks. Numerical case studies demonstrate the\nsubstantial performance gains of polarforming over conventional\nfixed-polarization antenna (FPA) systems, along with a discussion of\nimplementation challenges to motivate future research.", "published": "2025-05-27 06:04:55", "link": "http://arxiv.org/abs/2505.20760v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Evaluating Training in Binarized Neural Networks Through the Lens of Algorithmic Information Theory", "abstract": "Understanding and controlling the informational complexity of neural networks\nis a central challenge in machine learning, with implications for\ngeneralization, optimization, and model capacity. While most approaches rely on\nentropy-based loss functions and statistical metrics, these measures often fail\nto capture deeper, causally relevant algorithmic regularities embedded in\nnetwork structure. We propose a shift toward algorithmic information theory,\nusing Binarized Neural Networks (BNNs) as a first proxy. Grounded in\nalgorithmic probability (AP) and the universal distribution it defines, our\napproach characterizes learning dynamics through a formal, causally grounded\nlens. We apply the Block Decomposition Method (BDM) -- a scalable approximation\nof algorithmic complexity based on AP -- and demonstrate that it more closely\ntracks structural changes during training than entropy, consistently exhibiting\nstronger correlations with training loss across varying model sizes and\nrandomized training runs. These results support the view of training as a\nprocess of algorithmic compression, where learning corresponds to the\nprogressive internalization of structured regularities. In doing so, our work\noffers a principled estimate of learning progression and suggests a framework\nfor complexity-aware learning and regularization, grounded in first principles\nfrom information theory, complexity, and computability.", "published": "2025-05-27 02:51:36", "link": "http://arxiv.org/abs/2505.20646v1", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "68T07, 68Q30, 68Q32", "I.2.6; F.1.1; F.1.3"], "primary_category": "cs.LG"}
{"title": "OFDM-PASS: Frequency-Selective Modeling and Analysis for Pinching-Antenna Systems", "abstract": "Pinching antenna systems (PASS), which utilize dielectric waveguides for\nadaptable antenna deployment, offer novel opportunities to create controllable\nline-of-sight links. This letter is the first to investigate the integration of\nPASS with orthogonal frequency division multiplexing (OFDM) to ensure their\ncompatibility and to explore the frequency-selective behavior inherent to PASS.\nFirst, an end-to-end channel model is proposed for OFDM PASS based on\nelectromagnetic-compliant modeling of waveguides and coupled-mode theory, which\nincludes frequency-dependent waveguide dispersion and antenna coupling effect.\nFurthermore, a critical dependence of the OFDM cyclic prefix (CP) overhead on\nthe proximity of the operating frequency to the waveguide cutoff is revealed.\nMoreover, this letter evaluates an approximate pinching antenna location\nstrategy based on path loss minimization, from which the phase misalignment\neffect across subcarriers in wideband scenarios is derived. Numerical results\nshow that: 1) frequency-selective effects in OFDM PASS lead to substantial\nvariations in subcarrier achievable rates, highlighting the necessity of\noperating above the waveguide cutoff frequency for effective communication; 2)\nfrequency-independent pinching antenna location approximation incurs\nsignificant phase misalignment, particularly for wider bandwidths and larger\narray sizes; and 3) waveguide dispersion mandates considerable CP overhead when\noperating near the cutoff frequency, severely impacting the spectral efficiency\nof OFDM PASS.", "published": "2025-05-27 02:30:26", "link": "http://arxiv.org/abs/2505.20636v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Algorithms and SQ Lower Bounds for Robustly Learning Real-valued Multi-index Models", "abstract": "We study the complexity of learning real-valued Multi-Index Models (MIMs)\nunder the Gaussian distribution. A $K$-MIM is a function $f:\\mathbb{R}^d\\to\n\\mathbb{R}$ that depends only on the projection of its input onto a\n$K$-dimensional subspace. We give a general algorithm for PAC learning a broad\nclass of MIMs with respect to the square loss, even in the presence of\nadversarial label noise. Moreover, we establish a nearly matching Statistical\nQuery (SQ) lower bound, providing evidence that the complexity of our algorithm\nis qualitatively optimal as a function of the dimension. Specifically, we\nconsider the class of bounded variation MIMs with the property that degree at\nmost $m$ distinguishing moments exist with respect to projections onto any\nsubspace. In the presence of adversarial label noise, the complexity of our\nlearning algorithm is $d^{O(m)}2^{\\mathrm{poly}(K/\\epsilon)}$. For the\nrealizable and independent noise settings, our algorithm incurs complexity\n$d^{O(m)}2^{\\mathrm{poly}(K)}(1/\\epsilon)^{O(K)}$. To complement our upper\nbound, we show that if for some subspace degree-$m$ distinguishing moments do\nnot exist, then any SQ learner for the corresponding class of MIMs requires\ncomplexity $d^{\\Omega(m)}$. As an application, we give the first efficient\nlearner for the class of positive-homogeneous $L$-Lipschitz $K$-MIMs. The\nresulting algorithm has complexity $\\mathrm{poly}(d)\n2^{\\mathrm{poly}(KL/\\epsilon)}$. This gives a new PAC learning algorithm for\nLipschitz homogeneous ReLU networks with complexity independent of the network\nsize, removing the exponential dependence incurred in prior work.", "published": "2025-05-27 17:47:26", "link": "http://arxiv.org/abs/2505.21475v1", "categories": ["cs.LG", "cs.DS"], "primary_category": "cs.LG"}
{"title": "Causal Posterior Estimation", "abstract": "We present Causal Posterior Estimation (CPE), a novel method for Bayesian\ninference in simulator models, i.e., models where the evaluation of the\nlikelihood function is intractable or too computationally expensive, but where\none can simulate model outputs given parameter values. CPE utilizes a\nnormalizing flow-based (NF) approximation to the posterior distribution which\ncarefully incorporates the conditional dependence structure induced by the\ngraphical representation of the model into the neural network. Thereby it is\npossible to improve the accuracy of the approximation. We introduce both\ndiscrete and continuous NF architectures for CPE and propose a constant-time\nsampling procedure for the continuous case which reduces the computational\ncomplexity of drawing samples to O(1) as for discrete NFs. We show, through an\nextensive experimental evaluation, that by incorporating the conditional\ndependencies induced by the graphical model directly into the neural network,\nrather than learning them from data, CPE is able to conduct highly accurate\nposterior inference either outperforming or matching the state of the art in\nthe field.", "published": "2025-05-27 17:41:21", "link": "http://arxiv.org/abs/2505.21468v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "High-Dimensional Calibration from Swap Regret", "abstract": "We study the online calibration of multi-dimensional forecasts over an\narbitrary convex set $\\mathcal{P} \\subset \\mathbb{R}^d$ relative to an\narbitrary norm $\\Vert\\cdot\\Vert$. We connect this with the problem of external\nregret minimization for online linear optimization, showing that if it is\npossible to guarantee $O(\\sqrt{\\rho T})$ worst-case regret after $T$ rounds\nwhen actions are drawn from $\\mathcal{P}$ and losses are drawn from the dual\n$\\Vert \\cdot \\Vert_*$ unit norm ball, then it is also possible to obtain\n$\\epsilon$-calibrated forecasts after $T = \\exp(O(\\rho /\\epsilon^2))$ rounds.\nWhen $\\mathcal{P}$ is the $d$-dimensional simplex and $\\Vert \\cdot \\Vert$ is\nthe $\\ell_1$-norm, the existence of $O(\\sqrt{T\\log d})$-regret algorithms for\nlearning with experts implies that it is possible to obtain\n$\\epsilon$-calibrated forecasts after $T = \\exp(O(\\log{d}/\\epsilon^2)) =\nd^{O(1/\\epsilon^2)}$ rounds, recovering a recent result of Peng (2025).\n  Interestingly, our algorithm obtains this guarantee without requiring access\nto any online linear optimization subroutine or knowledge of the optimal rate\n$\\rho$ -- in fact, our algorithm is identical for every setting of\n$\\mathcal{P}$ and $\\Vert \\cdot \\Vert$. Instead, we show that the optimal\nregularizer for the above OLO problem can be used to upper bound the above\ncalibration error by a swap regret, which we then minimize by running the\nrecent TreeSwap algorithm with Follow-The-Leader as a subroutine.\n  Finally, we prove that any online calibration algorithm that guarantees\n$\\epsilon T$ $\\ell_1$-calibration error over the $d$-dimensional simplex\nrequires $T \\geq \\exp(\\mathrm{poly}(1/\\epsilon))$ (assuming $d \\geq\n\\mathrm{poly}(1/\\epsilon)$). This strengthens the corresponding\n$d^{\\Omega(\\log{1/\\epsilon})}$ lower bound of Peng, and shows that an\nexponential dependence on $1/\\epsilon$ is necessary.", "published": "2025-05-27 17:31:47", "link": "http://arxiv.org/abs/2505.21460v1", "categories": ["cs.LG", "cs.DS", "cs.GT", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Designing Cyclic Peptides via Harmonic SDE with Atom-Bond Modeling", "abstract": "Cyclic peptides offer inherent advantages in pharmaceuticals. For example,\ncyclic peptides are more resistant to enzymatic hydrolysis compared to linear\npeptides and usually exhibit excellent stability and affinity. Although deep\ngenerative models have achieved great success in linear peptide design, several\nchallenges prevent the development of computational methods for designing\ndiverse types of cyclic peptides. These challenges include the scarcity of 3D\nstructural data on target proteins and associated cyclic peptide ligands, the\ngeometric constraints that cyclization imposes, and the involvement of\nnon-canonical amino acids in cyclization. To address the above challenges, we\nintroduce CpSDE, which consists of two key components: AtomSDE, a generative\nstructure prediction model based on harmonic SDE, and ResRouter, a residue type\npredictor. Utilizing a routed sampling algorithm that alternates between these\ntwo models to iteratively update sequences and structures, CpSDE facilitates\nthe generation of cyclic peptides. By employing explicit all-atom and bond\nmodeling, CpSDE overcomes existing data limitations and is proficient in\ndesigning a wide variety of cyclic peptides. Our experimental results\ndemonstrate that the cyclic peptides designed by our method exhibit reliable\nstability and affinity.", "published": "2025-05-27 17:24:12", "link": "http://arxiv.org/abs/2505.21452v1", "categories": ["cs.LG", "q-bio.BM"], "primary_category": "cs.LG"}
{"title": "Can Large Reasoning Models Self-Train?", "abstract": "Scaling the performance of large language models (LLMs) increasingly depends\non methods that reduce reliance on human supervision. Reinforcement learning\nfrom automated verification offers an alternative, but it incurs scalability\nlimitations due to dependency upon human-designed verifiers. Self-training,\nwhere the model's own judgment provides the supervisory signal, presents a\ncompelling direction. We propose an online self-training reinforcement learning\nalgorithm that leverages the model's self-consistency to infer correctness\nsignals and train without any ground-truth supervision. We apply the algorithm\nto challenging mathematical reasoning tasks and show that it quickly reaches\nperformance levels rivaling reinforcement-learning methods trained explicitly\non gold-standard answers. Additionally, we analyze inherent limitations of the\nalgorithm, highlighting how the self-generated proxy reward initially\ncorrelated with correctness can incentivize reward hacking, where confidently\nincorrect outputs are favored. Our results illustrate how self-supervised\nimprovement can achieve significant performance gains without external labels,\nwhile also revealing its fundamental challenges.", "published": "2025-05-27 17:16:00", "link": "http://arxiv.org/abs/2505.21444v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Measuring Fine-Grained Relatedness in Multitask Learning via Data Attribution", "abstract": "Measuring task relatedness and mitigating negative transfer remain a critical\nopen challenge in Multitask Learning (MTL). This work extends data attribution\n-- which quantifies the influence of individual training data points on model\npredictions -- to MTL setting for measuring task relatedness. We propose the\nMultiTask Influence Function (MTIF), a method that adapts influence functions\nto MTL models with hard or soft parameter sharing. Compared to conventional\ntask relatedness measurements, MTIF provides a fine-grained, instance-level\nrelatedness measure beyond the entire-task level. This fine-grained relatedness\nmeasure enables a data selection strategy to effectively mitigate negative\ntransfer in MTL. Through extensive experiments, we demonstrate that the\nproposed MTIF efficiently and accurately approximates the performance of models\ntrained on data subsets. Moreover, the data selection strategy enabled by MTIF\nconsistently improves model performance in MTL. Our work establishes a novel\nconnection between data attribution and MTL, offering an efficient and\nfine-grained solution for measuring task relatedness and enhancing MTL models.", "published": "2025-05-27 17:13:31", "link": "http://arxiv.org/abs/2505.21438v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Attribute-Efficient PAC Learning of Sparse Halfspaces with Constant Malicious Noise Rate", "abstract": "Attribute-efficient learning of sparse halfspaces has been a fundamental\nproblem in machine learning theory. In recent years, machine learning\nalgorithms are faced with prevalent data corruptions or even adversarial\nattacks. It is of central interest to design efficient algorithms that are\nrobust to noise corruptions. In this paper, we consider that there exists a\nconstant amount of malicious noise in the data and the goal is to learn an\nunderlying $s$-sparse halfspace $w^* \\in \\mathbb{R}^d$ with $\\text{poly}(s,\\log\nd)$ samples. Specifically, we follow a recent line of works and assume that the\nunderlying distribution satisfies a certain concentration condition and a\nmargin condition at the same time. Under such conditions, we show that\nattribute-efficiency can be achieved by simple variants to existing hinge loss\nminimization programs. Our key contribution includes: 1) an attribute-efficient\nPAC learning algorithm that works under constant malicious noise rate; 2) a new\ngradient analysis that carefully handles the sparsity constraint in hinge loss\nminimization.", "published": "2025-05-27 17:02:28", "link": "http://arxiv.org/abs/2505.21430v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Conflicting Biases at the Edge of Stability: Norm versus Sharpness Regularization", "abstract": "A widely believed explanation for the remarkable generalization capacities of\noverparameterized neural networks is that the optimization algorithms used for\ntraining induce an implicit bias towards benign solutions. To grasp this\ntheoretically, recent works examine gradient descent and its variants in\nsimplified training settings, often assuming vanishing learning rates. These\nstudies reveal various forms of implicit regularization, such as $\\ell_1$-norm\nminimizing parameters in regression and max-margin solutions in classification.\nConcurrently, empirical findings show that moderate to large learning rates\nexceeding standard stability thresholds lead to faster, albeit oscillatory,\nconvergence in the so-called Edge-of-Stability regime, and induce an implicit\nbias towards minima of low sharpness (norm of training loss Hessian). In this\nwork, we argue that a comprehensive understanding of the generalization\nperformance of gradient descent requires analyzing the interaction between\nthese various forms of implicit regularization. We empirically demonstrate that\nthe learning rate balances between low parameter norm and low sharpness of the\ntrained model. We furthermore prove for diagonal linear networks trained on a\nsimple regression task that neither implicit bias alone minimizes the\ngeneralization error. These findings demonstrate that focusing on a single\nimplicit bias is insufficient to explain good generalization, and they motivate\na broader view of implicit regularization that captures the dynamic trade-off\nbetween norm and sharpness induced by non-negligible learning rates.", "published": "2025-05-27 16:51:06", "link": "http://arxiv.org/abs/2505.21423v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "When Shift Happens - Confounding Is to Blame", "abstract": "Distribution shifts introduce uncertainty that undermines the robustness and\ngeneralization capabilities of machine learning models. While conventional\nwisdom suggests that learning causal-invariant representations enhances\nrobustness to such shifts, recent empirical studies present a counterintuitive\nfinding: (i) empirical risk minimization (ERM) can rival or even outperform\nstate-of-the-art out-of-distribution (OOD) generalization methods, and (ii) its\nOOD generalization performance improves when all available covariates, not just\ncausal ones, are utilized. Drawing on both empirical and theoretical evidence,\nwe attribute this phenomenon to hidden confounding. Shifts in hidden\nconfounding induce changes in data distributions that violate assumptions\ncommonly made by existing OOD generalization approaches. Under such conditions,\nwe prove that effective generalization requires learning environment-specific\nrelationships, rather than relying solely on invariant ones. Furthermore, we\nshow that models augmented with proxies for hidden confounders can mitigate the\nchallenges posed by hidden confounding shifts. These findings offer new\ntheoretical insights and practical guidance for designing robust OOD\ngeneralization algorithms and principled covariate selection strategies.", "published": "2025-05-27 16:50:44", "link": "http://arxiv.org/abs/2505.21422v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "A Physics-Augmented GraphGPS Framework for the Reconstruction of 3D Riemann Problems from Sparse Data", "abstract": "In compressible fluid flow, reconstructing shocks, discontinuities,\nrarefactions, and their interactions from sparse measurements is an important\ninverse problem with practical applications. Moreover, physics-informed machine\nlearning has recently become an increasingly popular approach for performing\nreconstructions tasks. In this work we explore a machine learning recipe, known\nas GraphGPS, for reconstructing canonical compressible flows known as 3D\nRiemann problems from sparse observations, in a physics-informed manner. The\nGraphGPS framework combines the benefits of positional encodings, local\nmessage-passing of graphs, and global contextual awareness, and we explore the\nlatter two components through an ablation study. Furthermore, we modify the\naggregation step of message-passing such that it is aware of shocks and\ndiscontinuities, resulting in sharper reconstructions of these features.\nAdditionally, we modify message-passing such that information flows strictly\nfrom known nodes only, which results in computational savings, better training\nconvergence, and no degradation of reconstruction accuracy. We also show that\nthe GraphGPS framework outperforms numerous machine learning benchmarks.", "published": "2025-05-27 16:49:58", "link": "http://arxiv.org/abs/2505.21421v1", "categories": ["physics.flu-dyn", "cs.LG"], "primary_category": "physics.flu-dyn"}
{"title": "Dual Natural Gradient Descent for Scalable Training of Physics-Informed Neural Networks", "abstract": "Natural-gradient methods markedly accelerate the training of Physics-Informed\nNeural Networks (PINNs), yet their Gauss--Newton update must be solved in the\nparameter space, incurring a prohibitive $O(n^3)$ time complexity, where $n$ is\nthe number of network trainable weights. We show that exactly the same step can\ninstead be formulated in a generally smaller residual space of size $m =\n\\sum_{\\gamma} N_{\\gamma} d_{\\gamma}$, where each residual class $\\gamma$ (e.g.\nPDE interior, boundary, initial data) contributes $N_{\\gamma}$ collocation\npoints of output dimension $d_{\\gamma}$.\n  Building on this insight, we introduce \\textit{Dual Natural Gradient Descent}\n(D-NGD). D-NGD computes the Gauss--Newton step in residual space, augments it\nwith a geodesic-acceleration correction at negligible extra cost, and provides\nboth a dense direct solver for modest $m$ and a Nystrom-preconditioned\nconjugate-gradient solver for larger $m$.\n  Experimentally, D-NGD scales second-order PINN optimization to networks with\nup to 12.8 million parameters, delivers one- to three-order-of-magnitude lower\nfinal error $L^2$ than first-order methods (Adam, SGD) and quasi-Newton\nmethods, and -- crucially -- enables natural-gradient training of PINNs at this\nscale on a single GPU.", "published": "2025-05-27 16:27:23", "link": "http://arxiv.org/abs/2505.21404v1", "categories": ["cs.LG", "math.OC"], "primary_category": "cs.LG"}
{"title": "Square$\u03c7$PO: Differentially Private and Robust $\u03c7^2$-Preference Optimization in Offline Direct Alignment", "abstract": "In this paper, we theoretically study the offline alignment of language\nmodels with human preference feedback, under both preference label corruption\nand privacy protections. To this end, we propose Square$\\chi$PO, a simple\none-line change to $\\chi$PO where the standard log-loss is replaced by a new\nsquare loss over probability. Thanks to the inherent properties of this new\nloss, we have advanced the state-of-the-art of differentially private and\nrobust offline direct alignment. Specifically, for the local model of label\nprivacy, Square$\\chi$PO is the first algorithm that attains an optimal rate\nbased on single-policy concentrability even with general function\napproximations. It also gives the first result under the central model of\nprivacy protection over both prompts (responses) and labels. On the robustness\nside against Huber label corruption, Square$\\chi$PO is the first alignment\nmethod that has a meaningful theoretical guarantee under general function\napproximations. More importantly, Square$\\chi$PO can address privacy protection\nand corruption simultaneously, where an interesting separation is observed,\nimplying that the order of privacy and corruption matters. Furthermore, we show\nthat Square$\\chi$PO can also be easily extended to handle the scenario of the\ngeneral preference model with state-of-the-art guarantees under corruption and\nprivacy. Last but not least, all of our theoretical guarantees enjoy a unified\nanalysis, building upon a new result on the generalization error bounds of\nleast-square regression under corruption and privacy constraints, which we\nbelieve is of independent interest to the community.", "published": "2025-05-27 16:23:24", "link": "http://arxiv.org/abs/2505.21395v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation of Foundation Models", "abstract": "Low-Rank Adaptation (LoRA) has emerged as one of the most effective,\ncomputationally tractable fine-tuning approaches for training Vision-Language\nModels (VLMs) and Large Language Models (LLMs). LoRA accomplishes this by\nfreezing the pre-trained model weights and injecting trainable low-rank\nmatrices, allowing for efficient learning of these foundation models even on\nedge devices. However, LoRA in decentralized settings still remains under\nexplored, particularly for the theoretical underpinnings due to the lack of\nsmoothness guarantee and model consensus interference (defined formally below).\nThis work improves the convergence rate of decentralized LoRA (DLoRA) to match\nthe rate of decentralized SGD by ensuring gradient smoothness. We also\nintroduce DeCAF, a novel algorithm integrating DLoRA with truncated singular\nvalue decomposition (TSVD)-based matrix factorization to resolve consensus\ninterference. Theoretical analysis shows TSVD's approximation error is bounded\nand consensus differences between DLoRA and DeCAF vanish as rank increases,\nyielding DeCAF's matching convergence rate. Extensive experiments across\nvision/language tasks demonstrate our algorithms outperform local training and\nrivals federated learning under both IID and non-IID data distributions.", "published": "2025-05-27 16:10:53", "link": "http://arxiv.org/abs/2505.21382v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "PLANETALIGN: A Comprehensive Python Library for Benchmarking Network Alignment", "abstract": "Network alignment (NA) aims to identify node correspondence across different\nnetworks and serves as a critical cornerstone behind various downstream\nmulti-network learning tasks. Despite growing research in NA, there lacks a\ncomprehensive library that facilitates the systematic development and\nbenchmarking of NA methods. In this work, we introduce PLANETALIGN, a\ncomprehensive Python library for network alignment that features a rich\ncollection of built-in datasets, methods, and evaluation pipelines with\neasy-to-use APIs. Specifically, PLANETALIGN integrates 18 datasets and 14 NA\nmethods with extensible APIs for easy use and development of NA methods. Our\nstandardized evaluation pipeline encompasses a wide range of metrics, enabling\na systematic assessment of the effectiveness, scalability, and robustness of NA\nmethods. Through extensive comparative studies, we reveal practical insights\ninto the strengths and limitations of existing NA methods. We hope that\nPLANETALIGN can foster a deeper understanding of the NA problem and facilitate\nthe development and benchmarking of more effective, scalable, and robust\nmethods in the future. The source code of PLANETALIGN is available at\nhttps://github.com/yq-leo/PlanetAlign.", "published": "2025-05-27 15:56:30", "link": "http://arxiv.org/abs/2505.21366v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "CRISP-NAM: Competing Risks Interpretable Survival Prediction with Neural Additive Models", "abstract": "Competing risks are crucial considerations in survival modelling,\nparticularly in healthcare domains where patients may experience multiple\ndistinct event types. We propose CRISP-NAM (Competing Risks Interpretable\nSurvival Prediction with Neural Additive Models), an interpretable neural\nadditive model for competing risks survival analysis which extends the neural\nadditive architecture to model cause-specific hazards while preserving\nfeature-level interpretability. Each feature contributes independently to risk\nestimation through dedicated neural networks, allowing for visualization of\ncomplex non-linear relationships between covariates and each competing risk. We\ndemonstrate competitive performance on multiple datasets compared to existing\napproaches.", "published": "2025-05-27 15:52:15", "link": "http://arxiv.org/abs/2505.21360v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Towards Robust Automated Perceptual Voice Quality Assessment with Deep Learning", "abstract": "Objective: Perceptual voice quality assessment plays a critical role in\ndiagnosing and monitoring voice disorders by providing standardized evaluation\nof vocal function. Traditionally, this process relies on expert raters\nutilizing standard scales, such as the Consensus Auditory-Perceptual Evaluation\nof Voice (CAPE-V) and Grade, Roughness, Breathiness, Asthenia, and Strain\n(GRBAS). However, these metrics are inherently subjective and susceptible to\ninter-rater variability, motivating the need for automated and objective\nassessment methods. Methods: We propose Voice Quality Assessment Network\n(VOQANet), a deep learning-based framework with an attention mechanism that\nleverages a Speech Foundation Model (SFM) to capture high-level acoustic and\nprosodic information from raw speech. To enhance robustness and\ninterpretability, we present VOQANet+, which integrates handcrafted acoustic\nfeatures such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM\nembeddings. Results: Sentence-based input yields stronger performance than\nvowel-based input, especially at the patient level. VOQANet consistently\noutperforms baseline methods in RMSE and PCC, while VOQANet+ performs even\nbetter and maintains robustness under noisy conditions. Conclusion: Combining\nSFM embeddings with domain-informed acoustic features improves interpretability\nand resilience. Significance: VOQANet+ shows strong potential for deployment in\nreal-world and telehealth settings, addressing the limitations of subjective\nperceptual assessments with an interpretable and noise-resilient solution.", "published": "2025-05-27 15:48:17", "link": "http://arxiv.org/abs/2505.21356v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "OVERT: A Benchmark for Over-Refusal Evaluation on Text-to-Image Models", "abstract": "Text-to-Image (T2I) models have achieved remarkable success in generating\nvisual content from text inputs. Although multiple safety alignment strategies\nhave been proposed to prevent harmful outputs, they often lead to overly\ncautious behavior -- rejecting even benign prompts -- a phenomenon known as\n$\\textit{over-refusal}$ that reduces the practical utility of T2I models.\nDespite over-refusal having been observed in practice, there is no large-scale\nbenchmark that systematically evaluates this phenomenon for T2I models. In this\npaper, we present an automatic workflow to construct synthetic evaluation data,\nresulting in OVERT ($\\textbf{OVE}$r-$\\textbf{R}$efusal evaluation on\n$\\textbf{T}$ext-to-image models), the first large-scale benchmark for assessing\nover-refusal behaviors in T2I models. OVERT includes 4,600 seemingly harmful\nbut benign prompts across nine safety-related categories, along with 1,785\ngenuinely harmful prompts (OVERT-unsafe) to evaluate the safety-utility\ntrade-off. Using OVERT, we evaluate several leading T2I models and find that\nover-refusal is a widespread issue across various categories (Figure 1),\nunderscoring the need for further research to enhance the safety alignment of\nT2I models without compromising their functionality.As a preliminary attempt to\nreduce over-refusal, we explore prompt rewriting; however, we find it often\ncompromises faithfulness to the meaning of the original prompts. Finally, we\ndemonstrate the flexibility of our generation framework in accommodating\ndiverse safety requirements by generating customized evaluation data adapting\nto user-defined policies.", "published": "2025-05-27 15:42:46", "link": "http://arxiv.org/abs/2505.21347v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Joint Learning in the Gaussian Single Index Model", "abstract": "We consider the problem of jointly learning a one-dimensional projection and\na univariate function in high-dimensional Gaussian models. Specifically, we\nstudy predictors of the form $f(x)=\\varphi^\\star(\\langle w^\\star, x \\rangle)$,\nwhere both the direction $w^\\star \\in \\mathcal{S}_{d-1}$, the sphere of\n$\\mathbb{R}^d$, and the function $\\varphi^\\star: \\mathbb{R} \\to \\mathbb{R}$ are\nlearned from Gaussian data. This setting captures a fundamental non-convex\nproblem at the intersection of representation learning and nonlinear\nregression. We analyze the gradient flow dynamics of a natural alternating\nscheme and prove convergence, with a rate controlled by the information\nexponent reflecting the \\textit{Gaussian regularity} of the function\n$\\varphi^\\star$. Strikingly, our analysis shows that convergence still occurs\neven when the initial direction is negatively correlated with the target. On\nthe practical side, we demonstrate that such joint learning can be effectively\nimplemented using a Reproducing Kernel Hilbert Space (RKHS) adapted to the\nstructure of the problem, enabling efficient and flexible estimation of the\nunivariate function. Our results offer both theoretical insight and practical\nmethodology for learning low-dimensional structure in high-dimensional\nsettings.", "published": "2025-05-27 15:30:34", "link": "http://arxiv.org/abs/2505.21336v1", "categories": ["cs.LG", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Scheduling with Uncertain Holding Costs and its Application to Content Moderation", "abstract": "In content moderation for social media platforms, the cost of delaying the\nreview of a content is proportional to its view trajectory, which fluctuates\nand is apriori unknown. Motivated by such uncertain holding costs, we consider\na queueing model where job states evolve based on a Markov chain with\nstate-dependent instantaneous holding costs. We demonstrate that in the\npresence of such uncertain holding costs, the two canonical algorithmic\nprinciples, instantaneous-cost ($c\\mu$-rule) and expected-remaining-cost\n($c\\mu/\\theta$-rule), are suboptimal. By viewing each job as a Markovian\nski-rental problem, we develop a new index-based algorithm,\nOpportunity-adjusted Remaining Cost (OaRC), that adjusts to the opportunity of\nserving jobs in the future when uncertainty partly resolves. We show that the\nregret of OaRC scales as $\\tilde{O}(L^{1.5}\\sqrt{N})$, where $L$ is the maximum\nlength of a job's holding cost trajectory and $N$ is the system size. This\nregret bound shows that OaRC achieves asymptotic optimality when the system\nsize $N$ scales to infinity. Moreover, its regret is independent of the\nstate-space size, which is a desirable property when job states contain\ncontextual information. We corroborate our results with an extensive simulation\nstudy based on two holding cost patterns (online ads and user-generated\ncontent) that arise in content moderation for social media platforms. Our\nsimulations based on synthetic and real datasets demonstrate that OaRC\nconsistently outperforms existing practice, which is based on the two canonical\nalgorithmic principles.", "published": "2025-05-27 15:26:24", "link": "http://arxiv.org/abs/2505.21331v1", "categories": ["cs.DS", "cs.GT", "cs.LG", "cs.PF", "math.PR"], "primary_category": "cs.DS"}
{"title": "UGCE: User-Guided Incremental Counterfactual Exploration", "abstract": "Counterfactual explanations (CFEs) are a popular approach for interpreting\nmachine learning predictions by identifying minimal feature changes that alter\nmodel outputs. However, in real-world settings, users often refine feasibility\nconstraints over time, requiring counterfactual generation to adapt\ndynamically. Existing methods fail to support such iterative updates, instead\nrecomputing explanations from scratch with each change, an inefficient and\nrigid approach. We propose User-Guided Incremental Counterfactual Exploration\n(UGCE), a genetic algorithm-based framework that incrementally updates\ncounterfactuals in response to evolving user constraints. Experimental results\nacross five benchmark datasets demonstrate that UGCE significantly improves\ncomputational efficiency while maintaining high-quality solutions compared to a\nstatic, non-incremental approach. Our evaluation further shows that UGCE\nsupports stable performance under varying constraint sequences, benefits from\nan efficient warm-start strategy, and reveals how different constraint types\nmay affect search behavior.", "published": "2025-05-27 15:24:43", "link": "http://arxiv.org/abs/2505.21330v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Bencher: Simple and Reproducible Benchmarking for Black-Box Optimization", "abstract": "We present Bencher, a modular benchmarking framework for black-box\noptimization that fundamentally decouples benchmark execution from optimization\nlogic. Unlike prior suites that focus on combining many benchmarks in a single\nproject, Bencher introduces a clean abstraction boundary: each benchmark is\nisolated in its own virtual Python environment and accessed via a unified,\nversion-agnostic remote procedure call (RPC) interface. This design eliminates\ndependency conflicts and simplifies the integration of diverse, real-world\nbenchmarks, which often have complex and conflicting software requirements.\nBencher can be deployed locally or remotely via Docker or on high-performance\ncomputing (HPC) clusters via Singularity, providing a containerized,\nreproducible runtime for any benchmark. Its lightweight client requires minimal\nsetup and supports drop-in evaluation of 80 benchmarks across continuous,\ncategorical, and binary domains.", "published": "2025-05-27 15:18:58", "link": "http://arxiv.org/abs/2505.21321v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "LoFT: Low-Rank Adaptation That Behaves Like Full Fine-Tuning", "abstract": "Large pre-trained models are commonly adapted to downstream tasks using\nparameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA),\nwhich injects small trainable low-rank matrices instead of updating all\nweights. While LoRA dramatically reduces trainable parameters with little\noverhead, it can still underperform full fine-tuning in accuracy and often\nconverges more slowly. We introduce LoFT, a novel low-rank adaptation method\nthat behaves like full fine-tuning by aligning the optimizer's internal\ndynamics with those of updating all model weights. LoFT not only learns weight\nupdates in a low-rank subspace (like LoRA) but also properly projects the\noptimizer's first and second moments (Adam's momentum and variance) into the\nsame subspace, mirroring full-model updates. By aligning the low-rank update\nitself with the full update, LoFT eliminates the need for tuning extra\nhyperparameters, e.g., LoRA scaling factor $\\alpha$. Empirically, this approach\nsubstantially narrows the performance gap between adapter-based tuning and full\nfine-tuning and consistently outperforms standard LoRA-style methods, all\nwithout increasing inference cost.", "published": "2025-05-27 14:54:24", "link": "http://arxiv.org/abs/2505.21289v1", "categories": ["cs.LG", "math.OC"], "primary_category": "cs.LG"}
{"title": "Learnable Kernel Density Estimation for Graphs", "abstract": "This work proposes a framework LGKDE that learns kernel density estimation\nfor graphs. The key challenge in graph density estimation lies in effectively\ncapturing both structural patterns and semantic variations while maintaining\ntheoretical guarantees. Combining graph kernels and kernel density estimation\n(KDE) is a standard approach to graph density estimation, but has\nunsatisfactory performance due to the handcrafted and fixed features of\nkernels. Our method LGKDE leverages graph neural networks to represent each\ngraph as a discrete distribution and utilizes maximum mean discrepancy to learn\nthe graph metric for multi-scale KDE, where all parameters are learned by\nmaximizing the density of graphs relative to the density of their well-designed\nperturbed counterparts. The perturbations are conducted on both node features\nand graph spectra, which helps better characterize the boundary of normal\ndensity regions. Theoretically, we establish consistency and convergence\nguarantees for LGKDE, including bounds on the mean integrated squared error,\nrobustness, and complexity. We validate LGKDE by demonstrating its\neffectiveness in recovering the underlying density of synthetic graph\ndistributions and applying it to graph anomaly detection across diverse\nbenchmark datasets. Extensive empirical evaluation shows that LGKDE\ndemonstrates superior performance compared to state-of-the-art baselines on\nmost benchmark datasets.", "published": "2025-05-27 14:53:09", "link": "http://arxiv.org/abs/2505.21285v1", "categories": ["cs.LG", "stat.ML", "I.2; I.5.1; I.5.2"], "primary_category": "cs.LG"}
{"title": "Copresheaf Topological Neural Networks: A Generalized Deep Learning Framework", "abstract": "We introduce copresheaf topological neural networks (CTNNs), a powerful and\nunifying framework that encapsulates a wide spectrum of deep learning\narchitectures, designed to operate on structured data: including images, point\nclouds, graphs, meshes, and topological manifolds. While deep learning has\nprofoundly impacted domains ranging from digital assistants to autonomous\nsystems, the principled design of neural architectures tailored to specific\ntasks and data types remains one of the field's most persistent open\nchallenges. CTNNs address this gap by grounding model design in the language of\ncopresheaves, a concept from algebraic topology that generalizes and subsumes\nmost practical deep learning models in use today. This abstract yet\nconstructive formulation yields a rich design space from which theoretically\nsound and practically effective solutions can be derived to tackle core\nchallenges in representation learning: long-range dependencies, oversmoothing,\nheterophily, and non-Euclidean domains. Our empirical results on structured\ndata benchmarks demonstrate that CTNNs consistently outperform conventional\nbaselines, particularly in tasks requiring hierarchical or localized\nsensitivity. These results underscore CTNNs as a principled, multi-scale\nfoundation for the next generation of deep learning architectures.", "published": "2025-05-27 14:28:50", "link": "http://arxiv.org/abs/2505.21251v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "BindEnergyCraft: Casting Protein Structure Predictors as Energy-Based Models for Binder Design", "abstract": "Protein binder design has been transformed by hallucination-based methods\nthat optimize structure prediction confidence metrics, such as the interface\npredicted TM-score (ipTM), via backpropagation. However, these metrics do not\nreflect the statistical likelihood of a binder-target complex under the learned\ndistribution and yield sparse gradients for optimization. In this work, we\npropose a method to extract such likelihoods from structure predictors by\nreinterpreting their confidence outputs as an energy-based model (EBM). By\nleveraging the Joint Energy-based Modeling (JEM) framework, we introduce\npTMEnergy, a statistical energy function derived from predicted inter-residue\nerror distributions. We incorporate pTMEnergy into BindEnergyCraft (BECraft), a\ndesign pipeline that maintains the same optimization framework as BindCraft but\nreplaces ipTM with our energy-based objective. BECraft outperforms BindCraft,\nRFDiffusion, and ESM3 across multiple challenging targets, achieving higher in\nsilico binder success rates while reducing structural clashes. Furthermore,\npTMEnergy establishes a new state-of-the-art in structure-based virtual\nscreening tasks for miniprotein and RNA aptamer binders.", "published": "2025-05-27 14:21:35", "link": "http://arxiv.org/abs/2505.21241v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Why Do More Experts Fail? A Theoretical Analysis of Model Merging", "abstract": "Model merging dramatically reduces storage and computational resources by\ncombining multiple expert models into a single multi-task model. Although\nrecent model merging methods have shown promising results, they struggle to\nmaintain performance gains as the number of merged models increases. In this\npaper, we investigate the key obstacles that limit the scalability of model\nmerging when integrating a large number of expert models. First, we prove that\nthere is an upper bound on model merging. Further theoretical analysis reveals\nthat the limited effective parameter space imposes a strict constraint on the\nnumber of models that can be successfully merged. Gaussian Width shows that the\nmarginal benefit of merging additional models diminishes according to a\nstrictly concave function. This implies that the effective parameter space\nbecomes rapidly saturated as the number of merged models increases.\nFurthermore, using Approximate Kinematics Theory, we prove the existence of a\nunique optimal threshold beyond which adding more models does not yield\nsignificant performance improvements. At the same time, we introduce a\nstraightforward Reparameterized Heavy-Tailed method (RHT) to extend the\ncoverage of the merged model, thereby enhancing its performance. Empirical\nresults on 12 benchmarks, including both knowledge-intensive and\ngeneral-purpose tasks, validate our theoretical analysis. We believe that these\nresults spark further research beyond the current scope of model merging. The\nsource code is in the anonymous Github repository\nhttps://github.com/wzj1718/ModelMergingAnalysis.", "published": "2025-05-27 14:10:46", "link": "http://arxiv.org/abs/2505.21226v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Wavelet Flow For Extragalactic Foreground Simulations", "abstract": "Extragalactic foregrounds in cosmic microwave background (CMB) observations\nare both a source of cosmological and astrophysical information and a nuisance\nto the CMB. Effective field-level modeling that captures their non-Gaussian\nstatistical distributions is increasingly important for optimal information\nextraction, particularly given the precise and low-noise observations from\ncurrent and upcoming experiments. We explore the use of Wavelet Flow (WF)\nmodels to tackle the novel task of modeling the field-level probability\ndistributions of multi-component CMB secondaries. Specifically, we jointly\ntrain correlated CMB lensing convergence ($\\kappa$) and cosmic infrared\nbackground (CIB) maps with a WF model and obtain a network that statistically\nrecovers the input to high accuracy -- the trained network generates samples of\n$\\kappa$ and CIB fields whose average power spectra are within a few percent of\nthe inputs across all scales, and whose Minkowski functionals are similarly\naccurate compared to the inputs. Leveraging the multiscale architecture of\nthese models, we fine-tune both the model parameters and the priors at each\nscale independently, optimizing performance across different resolutions. These\nresults demonstrate that WF models can accurately simulate correlated\ncomponents of CMB secondaries, supporting improved analysis of cosmological\ndata. Our code and trained models can be found here\n(https://github.com/matiwosm/HybridPriorWavletFlow.git).", "published": "2025-05-27 14:08:28", "link": "http://arxiv.org/abs/2505.21220v1", "categories": ["astro-ph.CO", "cs.LG"], "primary_category": "astro-ph.CO"}
{"title": "Transfer learning for multifidelity simulation-based inference in cosmology", "abstract": "Simulation-based inference (SBI) enables cosmological parameter estimation\nwhen closed-form likelihoods or models are unavailable. However, SBI relies on\nmachine learning for neural compression and density estimation. This requires\nlarge training datasets which are prohibitively expensive for high-quality\nsimulations. We overcome this limitation with multifidelity transfer learning,\ncombining less expensive, lower-fidelity simulations with a limited number of\nhigh-fidelity simulations. We demonstrate our methodology on dark matter\ndensity maps from two separate simulation suites in the hydrodynamical CAMELS\nMultifield Dataset. Pre-training on dark-matter-only $N$-body simulations\nreduces the required number of high-fidelity hydrodynamical simulations by a\nfactor between $8$ and $15$, depending on the model complexity, posterior\ndimensionality, and performance metrics used. By leveraging cheaper\nsimulations, our approach enables performant and accurate inference on\nhigh-fidelity models while substantially reducing computational costs.", "published": "2025-05-27 14:04:30", "link": "http://arxiv.org/abs/2505.21215v1", "categories": ["astro-ph.CO", "cs.LG"], "primary_category": "astro-ph.CO"}
{"title": "Input Convex Kolmogorov Arnold Networks", "abstract": "This article presents an input convex neural network architecture using\nKolmogorov-Arnold networks (ICKAN). Two specific networks are presented: the\nfirst is based on a low-order, linear-by-part, representation of functions, and\na universal approximation theorem is provided. The second is based on cubic\nsplines, for which only numerical results support convergence. We demonstrate\non simple tests that these networks perform competitively with classical input\nconvex neural networks (ICNNs). In a second part, we use the networks to solve\nsome optimal transport problems needing a convex approximation of functions and\ndemonstrate their effectiveness. Comparisons with ICNNs show that cubic ICKANs\nproduce results similar to those of classical ICNNs.", "published": "2025-05-27 13:56:01", "link": "http://arxiv.org/abs/2505.21208v1", "categories": ["stat.ML", "cs.LG", "math.OC", "68T07"], "primary_category": "stat.ML"}
{"title": "Developing hybrid mechanistic and data-driven personalized prediction models for platelet dynamics", "abstract": "Hematotoxicity, drug-induced damage to the blood-forming system, is a\nfrequent side effect of cytotoxic chemotherapy and poses a significant\nchallenge in clinical practice due to its high inter-patient variability and\nlimited predictability. Current mechanistic models often struggle to accurately\nforecast outcomes for patients with irregular or atypical trajectories. In this\nstudy, we develop and compare hybrid mechanistic and data-driven approaches for\nindividualized time series modeling of platelet counts during chemotherapy. We\nconsider hybrid models that combine mechanistic models with neural networks,\nknown as universal differential equations. As a purely data-driven alternative,\nwe utilize a nonlinear autoregressive exogenous model using gated recurrent\nunits as the underlying architecture. These models are evaluated across a range\nof real patient scenarios, varying in data availability and sparsity, to assess\npredictive performance. Our findings demonstrate that data-driven methods, when\nprovided with sufficient data, significantly improve prediction accuracy,\nparticularly for high-risk patients with irregular platelet dynamics. This\nhighlights the potential of data-driven approaches in enhancing clinical\ndecision-making. In contrast, hybrid and mechanistic models are superior in\nscenarios with limited or sparse data. The proposed modeling and comparison\nframework is generalizable and could be extended to predict other\ntreatment-related toxicities, offering broad applicability in personalized\nmedicine.", "published": "2025-05-27 13:52:23", "link": "http://arxiv.org/abs/2505.21204v1", "categories": ["cs.LG", "q-bio.QM", "I.6.5; J.3"], "primary_category": "cs.LG"}
{"title": "Crop recommendation with machine learning: leveraging environmental and economic factors for optimal crop selection", "abstract": "Agriculture constitutes a primary source of food production, economic growth\nand employment in India, but the sector is confronted with low farm\nproductivity and yields aggravated by increased pressure on natural resources\nand adverse climate change variability. Efforts involving green revolution,\nland irrigations, improved seeds and organic farming have yielded suboptimal\noutcomes. The adoption of computational tools like crop recommendation systems\noffers a new way to provide insights and help farmers tackle low productivity.\nHowever, most agricultural recommendation systems in India focus narrowly on\nenvironmental factors and regions, limiting accurate predictions of high-yield,\nprofitable crops. This study uses environmental and economic factors with 19\ncrops across 15 states to develop and evaluate Random Forest and SVM models\nusing 10-fold Cross Validation, Time-series Split, and Lag Variables. The\n10-fold cross validation showed high accuracy (RF: 99.96%, SVM: 94.71%) but\nraised overfitting concerns. Introducing temporal order, better reflecting\nreal-world conditions, reduced performance (RF: 78.55%, SVM: 71.18%) in the\nTime-series Split.To further increase the model accuracy while maintaining the\ntemporal order, the Lag Variables approach was employed, which resulted in\nimproved performance (RF: 83.62%, SVM: 74.38%) compared to the 10-fold cross\nvalidation approach. Overall, the models in the Time-series Split and Lag\nVariable Approaches offer practical insights by handling temporal dependencies\nand enhancing its adaptability to changing agricultural conditions over time.\nConsequently, the study shows the Random Forest model developed based on the\nLag Variables as the most preferred algorithm for optimal crop recommendation\nin the Indian context.", "published": "2025-05-27 13:47:56", "link": "http://arxiv.org/abs/2505.21201v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Semi-Supervised Conformal Prediction With Unlabeled Nonconformity Score", "abstract": "Conformal prediction (CP) is a powerful framework for uncertainty\nquantification, providing prediction sets with coverage guarantees when\ncalibrated on sufficient labeled data. However, in real-world applications\nwhere labeled data is often limited, standard CP can lead to coverage deviation\nand output overly large prediction sets. In this paper, we extend CP to the\nsemi-supervised setting and propose SemiCP, leveraging both labeled data and\nunlabeled data for calibration. Specifically, we introduce a novel\nnonconformity score function, NNM, designed for unlabeled data. This function\nselects labeled data with similar pseudo-label scores to estimate nonconformity\nscores, integrating them into the calibration process to overcome sample size\nlimitations. We theoretically demonstrate that, under mild assumptions, SemiCP\nprovide asymptotically coverage guarantee for prediction sets. Extensive\nexperiments further validate that our approach effectively reduces instability\nand inefficiency under limited calibration data, can be adapted to conditional\ncoverage settings, and integrates seamlessly with existing CP methods.", "published": "2025-05-27 12:57:44", "link": "http://arxiv.org/abs/2505.21147v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "A Predicting Phishing Websites Using Support Vector Machine and MultiClass Classification Based on Association Rule Techniques", "abstract": "Phishing is a semantic attack which targets the user rather than the\ncomputer. It is a new Internet crime in comparison with other forms such as\nvirus and hacking. Considering the damage phishing websites has caused to\nvarious economies by collapsing organizations, stealing information and\nfinancial diversion, various researchers have embarked on different ways of\ndetecting phishing websites but there has been no agreement about the best\nalgorithm to be used for prediction. This study is interested in integrating\nthe strengths of two algorithms, Support Vector Machines (SVM) and Multi-Class\nClassification Rules based on Association Rules (MCAR) to establish a strong\nand better means of predicting phishing websites. A total of 11,056 websites\nwere used from both PhishTank and yahoo directory to verify the effectiveness\nof this approach. Feature extraction and rules generation were done by the MCAR\ntechnique; classification and prediction were done by SVM technique. The result\nshowed that the technique achieved 98.30% classification accuracy with a\ncomputation time of 2205.33s with minimum error rate. It showed a total of 98%\nArea under the Curve (AUC) which showed the proportion of accuracy in\nclassifying phishing websites. The model showed 82.84% variance in the\nprediction of phishing websites based on the coefficient of determination. The\nuse of two techniques together in detecting phishing websites produced a more\naccurate result as it combined the strength of both techniques respectively.\nThis research work centralized on this advantage by building a hybrid of two\ntechniques to help produce a more accurate result.", "published": "2025-05-27 12:52:34", "link": "http://arxiv.org/abs/2505.21141v1", "categories": ["cs.LG", "68T05"], "primary_category": "cs.LG"}
{"title": "Identifying Heart Attack Risk in Vulnerable Population: A Machine Learning Approach", "abstract": "The COVID-19 pandemic has significantly increased the incidence of\npost-infection cardiovascular events, particularly myocardial infarction, in\nindividuals over 40. While the underlying mechanisms remain elusive, this study\nemploys a hybrid machine learning approach to analyze epidemiological data in\nassessing 13 key heart attack risk factors and their susceptibility. Based on a\nunique dataset that combines demographic, biochemical, ECG, and thallium\nstress-tests, this study categorizes distinct subpopulations against varying\nrisk profiles and then divides the population into 'at-risk' (AR) and\n'not-at-risk' (NAR) groups using clustering algorithms. The study reveals\nstrong association between the likelihood of experiencing a heart attack on the\n13 risk factors studied. The aggravated risk for postmenopausal patients\nindicates compromised individual risk factors due to estrogen depletion that\nmay be, further compromised by extraneous stress impacts, like anxiety and\nfear, aspects that have traditionally eluded data modeling predictions.", "published": "2025-05-27 12:51:04", "link": "http://arxiv.org/abs/2505.21139v1", "categories": ["q-bio.PE", "cond-mat.soft", "cs.LG", "physics.med-ph"], "primary_category": "q-bio.PE"}
{"title": "Robust and Computation-Aware Gaussian Processes", "abstract": "Gaussian processes (GPs) are widely used for regression and optimization\ntasks such as Bayesian optimization (BO) due to their expressiveness and\nprincipled uncertainty estimates. However, in settings with large datasets\ncorrupted by outliers, standard GPs and their sparse approximations struggle\nwith computational tractability and robustness. We introduce Robust\nComputation-aware Gaussian Process (RCaGP), a novel GP model that jointly\naddresses these challenges by combining a principled treatment of\napproximation-induced uncertainty with robust generalized Bayesian updating.\nThe key insight is that robustness and approximation-awareness are not\northogonal but intertwined: approximations can exacerbate the impact of\noutliers, and mitigating one without the other is insufficient. Unlike previous\nwork that focuses narrowly on either robustness or approximation quality, RCaGP\ncombines both in a principled and scalable framework, thus effectively managing\nboth outliers and computational uncertainties introduced by approximations such\nas low-rank matrix multiplications. Our model ensures more conservative and\nreliable uncertainty estimates, a property we rigorously demonstrate.\nAdditionally, we establish a robustness property and show that the mean\nfunction is key to preserving it, motivating a tailored model selection scheme\nfor robust mean functions. Empirical results confirm that solving these\nchallenges jointly leads to superior performance across both clean and\noutlier-contaminated settings, both on regression and high-throughput Bayesian\noptimization benchmarks.", "published": "2025-05-27 12:49:14", "link": "http://arxiv.org/abs/2505.21133v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Conditional Diffusion Models with Classifier-Free Gibbs-like Guidance", "abstract": "Classifier-Free Guidance (CFG) is a widely used technique for improving\nconditional diffusion models by linearly combining the outputs of conditional\nand unconditional denoisers. While CFG enhances visual quality and improves\nalignment with prompts, it often reduces sample diversity, leading to a\nchallenging trade-off between quality and diversity. To address this issue, we\nmake two key contributions. First, CFG generally does not correspond to a\nwell-defined denoising diffusion model (DDM). In particular, contrary to common\nintuition, CFG does not yield samples from the target distribution associated\nwith the limiting CFG score as the noise level approaches zero -- where the\ndata distribution is tilted by a power $w \\gt 1$ of the conditional\ndistribution. We identify the missing component: a R\\'enyi divergence term that\nacts as a repulsive force and is required to correct CFG and render it\nconsistent with a proper DDM. Our analysis shows that this correction term\nvanishes in the low-noise limit. Second, motivated by this insight, we propose\na Gibbs-like sampling procedure to draw samples from the desired tilted\ndistribution. This method starts with an initial sample from the conditional\ndiffusion model without CFG and iteratively refines it, preserving diversity\nwhile progressively enhancing sample quality. We evaluate our approach on both\nimage and text-to-audio generation tasks, demonstrating substantial\nimprovements over CFG across all considered metrics. The code is available at\nhttps://github.com/yazidjanati/cfgig", "published": "2025-05-27 12:27:33", "link": "http://arxiv.org/abs/2505.21101v1", "categories": ["cs.LG", "stat.ME"], "primary_category": "cs.LG"}
{"title": "Improved Impossible Tuning and Lipschitz-Adaptive Universal Online Learning with Gradient Variations", "abstract": "A central goal in online learning is to achieve adaptivity to unknown problem\ncharacteristics, such as environmental changes captured by gradient variation\n(GV), function curvature (universal online learning, UOL), and gradient scales\n(Lipschitz adaptivity, LA). Simultaneously achieving these with optimal\nperformance is a major challenge, partly due to limitations in algorithms for\nprediction with expert advice. These algorithms often serve as meta-algorithms\nin online ensemble frameworks, and their sub-optimality hinders overall UOL\nperformance. Specifically, existing algorithms addressing the ``impossible\ntuning'' issue incur an excess $\\sqrt{\\log T}$ factor in their regret bound\ncompared to the lower bound. To solve this problem, we propose a novel\noptimistic online mirror descent algorithm with an auxiliary initial round\nusing large learning rates. This design enables a refined analysis where a\ngenerated negative term cancels the gap-related factor, resolving the\nimpossible tuning issue up to $\\log\\log T$ factors. Leveraging our improved\nalgorithm as a meta-algorithm, we develop the first UOL algorithm that\nsimultaneously achieves state-of-the-art GV bounds and LA under standard\nassumptions. Our UOL result overcomes key limitations of prior works, notably\nresolving the conflict between LA mechanisms and regret analysis for GV bounds\n-- an open problem highlighted by Xie et al.", "published": "2025-05-27 12:22:21", "link": "http://arxiv.org/abs/2505.21095v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Bridging Arbitrary and Tree Metrics via Differentiable Gromov Hyperbolicity", "abstract": "Trees and the associated shortest-path tree metrics provide a powerful\nframework for representing hierarchical and combinatorial structures in data.\nGiven an arbitrary metric space, its deviation from a tree metric can be\nquantified by Gromov's $\\delta$-hyperbolicity. Nonetheless, designing\nalgorithms that bridge an arbitrary metric to its closest tree metric is still\na vivid subject of interest, as most common approaches are either heuristical\nand lack guarantees, or perform moderately well. In this work, we introduce a\nnovel differentiable optimization framework, coined DeltaZero, that solves this\nproblem. Our method leverages a smooth surrogate for Gromov's\n$\\delta$-hyperbolicity which enables a gradient-based optimization, with a\ntractable complexity. The corresponding optimization procedure is derived from\na problem with better worst case guarantees than existing bounds, and is\njustified statistically. Experiments on synthetic and real-world datasets\ndemonstrate that our method consistently achieves state-of-the-art distortion.", "published": "2025-05-27 11:58:37", "link": "http://arxiv.org/abs/2505.21073v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Scalable and adaptive prediction bands with kernel sum-of-squares", "abstract": "Conformal Prediction (CP) is a popular framework for constructing prediction\nbands with valid coverage in finite samples, while being free of any\ndistributional assumption. A well-known limitation of conformal prediction is\nthe lack of adaptivity, although several works introduced practically efficient\nalternate procedures. In this work, we build upon recent ideas that rely on\nrecasting the CP problem as a statistical learning problem, directly targeting\ncoverage and adaptivity. This statistical learning problem is based on\nreproducible kernel Hilbert spaces (RKHS) and kernel sum-of-squares (SoS)\nmethods. First, we extend previous results with a general representer theorem\nand exhibit the dual formulation of the learning problem. Crucially, such dual\nformulation can be solved efficiently by accelerated gradient methods with\nseveral hundreds or thousands of samples, unlike previous strategies based on\noff-the-shelf semidefinite programming algorithms. Second, we introduce a new\nhyperparameter tuning strategy tailored specifically to target adaptivity\nthrough bounds on test-conditional coverage. This strategy, based on the\nHilbert-Schmidt Independence Criterion (HSIC), is introduced here to tune\nkernel lengthscales in our framework, but has broader applicability since it\ncould be used in any CP algorithm where the score function is learned. Finally,\nextensive experiments are conducted to show how our method compares to related\nwork. All figures can be reproduced with the accompanying code.", "published": "2025-05-27 11:21:17", "link": "http://arxiv.org/abs/2505.21039v1", "categories": ["cs.LG", "math.ST", "stat.TH"], "primary_category": "cs.LG"}
{"title": "LLaMEA-BO: A Large Language Model Evolutionary Algorithm for Automatically Generating Bayesian Optimization Algorithms", "abstract": "Bayesian optimization (BO) is a powerful class of algorithms for optimizing\nexpensive black-box functions, but designing effective BO algorithms remains a\nmanual, expertise-driven task. Recent advancements in Large Language Models\n(LLMs) have opened new avenues for automating scientific discovery, including\nthe automatic design of optimization algorithms. While prior work has used LLMs\nwithin optimization loops or to generate non-BO algorithms, we tackle a new\nchallenge: Using LLMs to automatically generate full BO algorithm code. Our\nframework uses an evolution strategy to guide an LLM in generating Python code\nthat preserves the key components of BO algorithms: An initial design, a\nsurrogate model, and an acquisition function. The LLM is prompted to produce\nmultiple candidate algorithms, which are evaluated on the established Black-Box\nOptimization Benchmarking (BBOB) test suite from the COmparing Continuous\nOptimizers (COCO) platform. Based on their performance, top candidates are\nselected, combined, and mutated via controlled prompt variations, enabling\niterative refinement. Despite no additional fine-tuning, the LLM-generated\nalgorithms outperform state-of-the-art BO baselines in 19 (out of 24) BBOB\nfunctions in dimension 5 and generalize well to higher dimensions, and\ndifferent tasks (from the Bayesmark framework). This work demonstrates that\nLLMs can serve as algorithmic co-designers, offering a new paradigm for\nautomating BO development and accelerating the discovery of novel algorithmic\ncombinations. The source code is provided at\nhttps://github.com/Ewendawi/LLaMEA-BO.", "published": "2025-05-27 11:13:14", "link": "http://arxiv.org/abs/2505.21034v1", "categories": ["cs.LG", "cs.NE"], "primary_category": "cs.LG"}
{"title": "NeuralOM: Neural Ocean Model for Subseasonal-to-Seasonal Simulation", "abstract": "Accurate Subseasonal-to-Seasonal (S2S) ocean simulation is critically\nimportant for marine research, yet remains challenging due to its substantial\nthermal inertia and extended time delay. Machine learning (ML)-based models\nhave demonstrated significant advancements in simulation accuracy and\ncomputational efficiency compared to traditional numerical methods.\nNevertheless, a significant limitation of current ML models for S2S ocean\nsimulation is their inadequate incorporation of physical consistency and the\nslow-changing properties of the ocean system. In this work, we propose a neural\nocean model (NeuralOM) for S2S ocean simulation with a multi-scale interactive\ngraph neural network to emulate diverse physical phenomena associated with\nocean systems effectively. Specifically, we propose a multi-stage framework\ntailored to model the ocean's slowly changing nature. Additionally, we\nintroduce a multi-scale interactive messaging module to capture complex\ndynamical behaviors, such as gradient changes and multiplicative coupling\nrelationships inherent in ocean dynamics. Extensive experimental evaluations\nconfirm that our proposed NeuralOM outperforms state-of-the-art models in S2S\nand extreme event simulation. The codes are available at\nhttps://github.com/YuanGao-YG/NeuralOM.", "published": "2025-05-27 10:54:40", "link": "http://arxiv.org/abs/2505.21020v1", "categories": ["cs.LG", "physics.ao-ph"], "primary_category": "cs.LG"}
{"title": "Cardiac Digital Twins at Scale from MRI: Open Tools and Representative Models from ~55000 UK Biobank Participants", "abstract": "A cardiac digital twin is a virtual replica of a patient's heart for\nscreening, diagnosis, prognosis, risk assessment, and treatment planning of\ncardiovascular diseases. This requires an anatomically accurate\npatient-specific 3D structural representation of the heart, suitable for\nelectro-mechanical simulations or study of disease mechanisms. However,\ngeneration of cardiac digital twins at scale is demanding and there are no\npublic repositories of models across demographic groups. We describe an\nautomatic open-source pipeline for creating patient-specific left and right\nventricular meshes from cardiovascular magnetic resonance images, its\napplication to a large cohort of ~55000 participants from UK Biobank, and the\nconstruction of the most comprehensive cohort of adult heart models to date,\ncomprising 1423 representative meshes across sex (male, female), body mass\nindex (range: 16 - 42 kg/m$^2$) and age (range: 49 - 80 years). Our code is\navailable at https://github.com/cdttk/biv-volumetric-meshing/tree/plos2025 ,\nand pre-trained networks, representative volumetric meshes with fibers and UVCs\nwill be made available soon.", "published": "2025-05-27 10:52:52", "link": "http://arxiv.org/abs/2505.21019v1", "categories": ["eess.IV", "cs.LG"], "primary_category": "eess.IV"}
{"title": "Efficient and Unbiased Sampling from Boltzmann Distributions via Variance-Tuned Diffusion Models", "abstract": "Score-based diffusion models (SBDMs) are powerful amortized samplers for\nBoltzmann distributions; however, imperfect score estimates bias downstream\nMonte Carlo estimates. Classical importance sampling (IS) can correct this\nbias, but computing exact likelihoods requires solving the probability-flow\nordinary differential equation (PF-ODE), a procedure that is prohibitively\ncostly and scales poorly with dimensionality. We introduce Variance-Tuned\nDiffusion Importance Sampling (VT-DIS), a lightweight post-training method that\nadapts the per-step noise covariance of a pretrained SBDM by minimizing the\n$\\alpha$-divergence ($\\alpha=2$) between its forward diffusion and reverse\ndenoising trajectories. VT-DIS assigns a single trajectory-wise importance\nweight to the joint forward-reverse process, yielding unbiased expectation\nestimates at test time with negligible overhead compared to standard sampling.\nOn the DW-4, LJ-13, and alanine-dipeptide benchmarks, VT-DIS achieves effective\nsample sizes of approximately 80 %, 35 %, and 3.5 %, respectively, while using\nonly a fraction of the computational budget required by vanilla diffusion + IS\nor PF-ODE-based IS.", "published": "2025-05-27 10:37:48", "link": "http://arxiv.org/abs/2505.21005v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Efficient Identity and Position Graph Embedding via Spectral-Based Random Feature Aggregation", "abstract": "Graph neural networks (GNNs), which capture graph structures via a feature\naggregation mechanism following the graph embedding framework, have\ndemonstrated a powerful ability to support various tasks. According to the\ntopology properties (e.g., structural roles or community memberships of nodes)\nto be preserved, graph embedding can be categorized into identity and position\nembedding. However, it is unclear for most GNN-based methods which property\nthey can capture. Some of them may also suffer from low efficiency and\nscalability caused by several time- and space-consuming procedures (e.g.,\nfeature extraction and training). From a perspective of graph signal\nprocessing, we find that high- and low-frequency information in the graph\nspectral domain may characterize node identities and positions, respectively.\nBased on this investigation, we propose random feature aggregation (RFA) for\nefficient identity and position embedding, serving as an extreme ablation study\nregarding GNN feature aggregation. RFA (i) adopts a spectral-based GNN without\nlearnable parameters as its backbone, (ii) only uses random noises as inputs,\nand (iii) derives embeddings via just one feed-forward propagation (FFP).\nInspired by degree-corrected spectral clustering, we further introduce a degree\ncorrection mechanism to the GNN backbone. Surprisingly, our experiments\ndemonstrate that two variants of RFA with high- and low-pass filters can\nrespectively derive informative identity and position embeddings via just one\nFFP (i.e., without any training). As a result, RFA can achieve a better\ntrade-off between quality and efficiency for both identity and position\nembedding over various baselines.", "published": "2025-05-27 10:26:15", "link": "http://arxiv.org/abs/2505.20992v1", "categories": ["cs.LG", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Identifying Super Spreaders in Multilayer Networks", "abstract": "Identifying super-spreaders can be framed as a subtask of the influence\nmaximisation problem. It seeks to pinpoint agents within a network that, if\nselected as single diffusion seeds, disseminate information most effectively.\nMultilayer networks, a specific class of heterogeneous graphs, can capture\ndiverse types of interactions (e.g., physical-virtual or professional-social),\nand thus offer a more accurate representation of complex relational structures.\nIn this work, we introduce a novel approach to identifying super-spreaders in\nsuch networks by leveraging graph neural networks. To this end, we construct a\ndataset by simulating information diffusion across hundreds of networks - to\nthe best of our knowledge, the first of its kind tailored specifically to\nmultilayer networks. We further formulate the task as a variation of the\nranking prediction problem based on a four-dimensional vector that quantifies\neach agent's spreading potential: (i) the number of activations; (ii) the\nduration of the diffusion process; (iii) the peak number of activations; and\n(iv) the simulation step at which this peak occurs. Our model,\nTopSpreadersNetwork, comprises a relationship-agnostic encoder and a custom\naggregation layer. This design enables generalisation to previously unseen data\nand adapts to varying graph sizes. In an extensive evaluation, we compare our\nmodel against classic centrality-based heuristics and competitive deep learning\nmethods. The results, obtained across a broad spectrum of real-world and\nsynthetic multilayer networks, demonstrate that TopSpreadersNetwork achieves\nsuperior performance in identifying high-impact nodes, while also offering\nimproved interpretability through its structured output.", "published": "2025-05-27 10:14:14", "link": "http://arxiv.org/abs/2505.20980v1", "categories": ["cs.SI", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Autonomous Multi-Modal LLM Agents for Treatment Planning in Focused Ultrasound Ablation Surgery", "abstract": "Focused Ultrasound Ablation Surgery (FUAS) has emerged as a promising\nnon-invasive therapeutic modality, valued for its safety and precision.\nNevertheless, its clinical implementation entails intricate tasks such as\nmultimodal image interpretation, personalized dose planning, and real-time\nintraoperative decision-making processes that demand intelligent assistance to\nimprove efficiency and reliability. We introduce FUAS-Agents, an autonomous\nagent system that leverages the multimodal understanding and tool-using\ncapabilities of large language models (LLMs). By integrating patient profiles\nand MRI data, FUAS-Agents orchestrates a suite of specialized medical AI tools,\nincluding segmentation, treatment dose prediction, and clinical guideline\nretrieval, to generate personalized treatment plans comprising MRI image, dose\nparameters, and therapeutic strategies. We evaluate the system in a uterine\nfibroid treatment scenario. Human assessment by four senior FUAS experts\nindicates that 82.5%, 82.5%, 87.5%, and 97.5% of the generated plans were rated\n4 or above (on a 5-point scale) in terms of completeness, accuracy, fluency,\nand clinical compliance, respectively. These results demonstrate the potential\nof LLM-driven agents in enhancing decision-making across complex clinical\nworkflows, and exemplify a translational paradigm that combines general-purpose\nmodels with specialized expert systems to solve practical challenges in\nvertical healthcare domains.", "published": "2025-05-27 16:43:31", "link": "http://arxiv.org/abs/2505.21418v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "Revisiting Multi-Agent World Modeling from a Diffusion-Inspired Perspective", "abstract": "World models have recently attracted growing interest in Multi-Agent\nReinforcement Learning (MARL) due to their ability to improve sample efficiency\nfor policy learning. However, accurately modeling environments in MARL is\nchallenging due to the exponentially large joint action space and highly\nuncertain dynamics inherent in multi-agent systems. To address this, we reduce\nmodeling complexity by shifting from jointly modeling the entire state-action\ntransition dynamics to focusing on the state space alone at each timestep\nthrough sequential agent modeling. Specifically, our approach enables the model\nto progressively resolve uncertainty while capturing the structured\ndependencies among agents, providing a more accurate representation of how\nagents influence the state. Interestingly, this sequential revelation of\nagents' actions in a multi-agent system aligns with the reverse process in\ndiffusion models--a class of powerful generative models known for their\nexpressiveness and training stability compared to autoregressive or latent\nvariable models. Leveraging this insight, we develop a flexible and robust\nworld model for MARL using diffusion models. Our method, Diffusion-Inspired\nMulti-Agent world model (DIMA), achieves state-of-the-art performance across\nmultiple multi-agent control benchmarks, significantly outperforming prior\nworld models in terms of final return and sample efficiency, including MAMuJoCo\nand Bi-DexHands. DIMA establishes a new paradigm for constructing multi-agent\nworld models, advancing the frontier of MARL research.", "published": "2025-05-27 09:11:38", "link": "http://arxiv.org/abs/2505.20922v1", "categories": ["cs.MA", "cs.AI", "cs.LG"], "primary_category": "cs.MA"}
{"title": "Generalized Coordination of Partially Cooperative Urban Traffic", "abstract": "Vehicle-to-anything connectivity, especially for autonomous vehicles,\npromises to increase passenger comfort and safety of road traffic, for example,\nby sharing perception and driving intention. Cooperative maneuver planning uses\nconnectivity to enhance traffic efficiency, which has, so far, been mainly\nconsidered for automated intersection management. In this article, we present a\nnovel cooperative maneuver planning approach that is generalized to various\nsituations found in urban traffic. Our framework handles challenging mixed\ntraffic, that is, traffic comprising both cooperative connected vehicles and\nother vehicles at any distribution. Our solution is based on an optimization\napproach accompanied by an efficient heuristic method for high-load scenarios.\nWe extensively evaluate the proposed planer in a distinctly realistic\nsimulation framework and show significant efficiency gains already at a\ncooperation rate of 40%. Traffic throughput increases, while the average\nwaiting time and the number of stopped vehicles are reduced, without impacting\ntraffic safety.", "published": "2025-05-27 08:25:57", "link": "http://arxiv.org/abs/2505.20879v1", "categories": ["cs.RO", "cs.MA"], "primary_category": "cs.RO"}
{"title": "MedSentry: Understanding and Mitigating Safety Risks in Medical LLM Multi-Agent Systems", "abstract": "As large language models (LLMs) are increasingly deployed in healthcare,\nensuring their safety, particularly within collaborative multi-agent\nconfigurations, is paramount. In this paper we introduce MedSentry, a benchmark\ncomprising 5 000 adversarial medical prompts spanning 25 threat categories with\n100 subthemes. Coupled with this dataset, we develop an end-to-end\nattack-defense evaluation pipeline to systematically analyze how four\nrepresentative multi-agent topologies (Layers, SharedPool, Centralized, and\nDecentralized) withstand attacks from 'dark-personality' agents. Our findings\nreveal critical differences in how these architectures handle information\ncontamination and maintain robust decision-making, exposing their underlying\nvulnerability mechanisms. For instance, SharedPool's open information sharing\nmakes it highly susceptible, whereas Decentralized architectures exhibit\ngreater resilience thanks to inherent redundancy and isolation. To mitigate\nthese risks, we propose a personality-scale detection and correction mechanism\nthat identifies and rehabilitates malicious agents, restoring system safety to\nnear-baseline levels. MedSentry thus furnishes both a rigorous evaluation\nframework and practical defense strategies that guide the design of safer\nLLM-based multi-agent systems in medical domains.", "published": "2025-05-27 07:34:40", "link": "http://arxiv.org/abs/2505.20824v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "A Hyperbolic Approximation of the Nonlinear Schr\u00f6dinger Equation", "abstract": "We study a first-order hyperbolic approximation of\n  the nonlinear Schr\\\"odinger (NLS) equation. We show that the system\n  is strictly hyperbolic and possesses a modified Hamiltonian structure, along\nwith\n  at least three conserved quantities that approximate those of NLS.\n  We provide families of explicit standing-wave solutions to the hyperbolic\nsystem,\n  which are shown to converge uniformly to ground-state solutions\n  of NLS in the relaxation limit.\n  The system is formally equivalent to NLS in the relaxation limit, and we\n  develop asymptotic preserving discretizations that tend to a consistent\ndiscretization\n  of NLS in that limit, while also conserving mass.\n  Examples for both the focusing and defocusing regimes demonstrate that the\n  numerical discretization provides an accurate approximation of the NLS\n  solution.", "published": "2025-05-27 16:54:44", "link": "http://arxiv.org/abs/2505.21424v1", "categories": ["math.AP", "cs.NA", "math-ph", "math.MP", "math.NA", "physics.comp-ph"], "primary_category": "math.AP"}
{"title": "A mathematical analysis of the discretized IPT-DMFT equations", "abstract": "In a previous contribution (E. Canc\\`es, A. Kirsch and S. Perrin--Roussel,\narXiv:2406.03384), we have proven the existence of a solution to the Dynamical\nMean-Field Theory (DMFT) equations under the Iterated Perturbation Theory\n(IPT-DMFT) approximation. In view of numerical simulations, these equations\nneed to be discretized. In this article, we are interested in a discretization\nof the \\acrshort{ipt}-\\acrshort{dmft} functional equations, based on the\nrestriction of the hybridization function and local self-energy to a finite\nnumber of points in the upper half-plane $\\left(i\\omega_n\\right)_{n \\in\n|[0,N_\\omega]|}$, where $\\omega_n=(2n+1)\\pi / \\beta$ is the $n$-th Matsubara\nfrequency and $N_\\omega \\in \\mathbb N$. We first prove the existence of\nsolutions to the discretized equations in some parameter range depending on\n$N_\\omega$. We then prove uniqueness for a smaller range of parameters. We also\nstudy more in depth the case of bipartite systems exhibiting particle-hole\nsymmetry. In this case, the discretized IPT-DMFT equations have purely\nimaginary solutions, which can be obtained by solving a real algebraic system\nof $(N_\\omega+1)$ equations with $(N_\\omega+1)$ variables. We provide a\ncomplete characterization of the solutions for $N_\\omega=0$ and some results\nfor $N_\\omega=1$ in the simple case of the Hubbard dimer. We finally present\nsome numerical simulations on the Hubbard dimer.", "published": "2025-05-27 14:53:46", "link": "http://arxiv.org/abs/2505.21287v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Domain Decomposition Subspace Neural Network Method for Solving Linear and Nonlinear Partial Differential Equations", "abstract": "This paper proposes a domain decomposition subspace neural network method for\nefficiently solving linear and nonlinear partial differential equations. By\ncombining the principles of domain decomposition and subspace neural networks,\nthe method constructs basis functions using neural networks to approximate PDE\nsolutions. It imposes $C^k$ continuity conditions at the interface of\nsubdomains, ensuring smoothness across the global solution. Nonlinear PDEs are\nsolved using Picard and Newton iterations, analogous to classical methods.\nNumerical experiments demonstrate that our method achieves exceptionally high\naccuracy, with errors reaching up to $10^{-13}$, while significantly reducing\ncomputational costs compared to existing approaches, including PINNs, DGM, DRM.\nThe results highlight the method's superior accuracy and training efficiency.", "published": "2025-05-27 07:24:39", "link": "http://arxiv.org/abs/2505.20818v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Recurrent Neural Operators: Stable Long-Term PDE Prediction", "abstract": "Neural operators have emerged as powerful tools for learning solution\noperators of partial differential equations. However, in time-dependent\nproblems, standard training strategies such as teacher forcing introduce a\nmismatch between training and inference, leading to compounding errors in\nlong-term autoregressive predictions. To address this issue, we propose\nRecurrent Neural Operators (RNOs)-a novel framework that integrates recurrent\ntraining into neural operator architectures. Instead of conditioning each\ntraining step on ground-truth inputs, RNOs recursively apply the operator to\ntheir own predictions over a temporal window, effectively simulating\ninference-time dynamics during training. This alignment mitigates exposure bias\nand enhances robustness to error accumulation. Theoretically, we show that\nrecurrent training can reduce the worst-case exponential error growth typical\nof teacher forcing to linear growth. Empirically, we demonstrate that\nrecurrently trained Multigrid Neural Operators significantly outperform their\nteacher-forced counterparts in long-term accuracy and stability on standard\nbenchmarks. Our results underscore the importance of aligning training with\ninference dynamics for robust temporal generalization in neural operator\nlearning.", "published": "2025-05-27 05:04:35", "link": "http://arxiv.org/abs/2505.20721v1", "categories": ["cs.LG", "cs.NA", "math.NA", "65M70, 68T07, 68U20"], "primary_category": "cs.LG"}
{"title": "A Nested Krylov Method Using Half-Precision Arithmetic", "abstract": "Low-precision computing is essential for efficiently utilizing memory\nbandwidth and computing cores. While many mixed-precision algorithms have been\ndeveloped for iterative sparse linear solvers, effectively leveraging\nhalf-precision (fp16) arithmetic remains challenging. This study introduces a\nnovel nested Krylov approach that integrates the flexible GMRES and Richardson\nmethods in a deeply nested structure, progressively reducing precision from\ndouble-precision to fp16 toward the innermost solver. To avoid meaningless\ncomputations beyond precision limits, the low-precision inner solvers perform\nonly a few iterations per invocation, while the nested structure ensures their\nfrequent execution. Numerical experiments show that using fp16 in the approach\ndirectly enhances solver performance without compromising convergence,\nachieving speedups of up to 1.65x and 2.42x over double-precision and\ndouble-single mixed-precision implementations, respectively. Moreover, the\nproposed method outperforms or matches other standard Krylov solvers, including\nrestarted GMRES, CG, and BiCGStab methods.", "published": "2025-05-27 04:56:17", "link": "http://arxiv.org/abs/2505.20719v1", "categories": ["math.NA", "cs.NA", "G.1.3"], "primary_category": "math.NA"}
{"title": "An Empirical Study of Conjugate Gradient Preconditioners for Solving Symmetric Positive Definite Systems of Linear Equations", "abstract": "Despite hundreds of papers on preconditioned linear systems of equations,\nthere remains a significant lack of comprehensive performance benchmarks\ncomparing various preconditioners for solving symmetric positive definite (SPD)\nsystems. In this paper, we present a comparative study of 79 matrices using a\nbroad range of preconditioners. Specifically, we evaluate 10 widely used\npreconditoners across 108 configurations to assess their relative performance\nagainst using no preconditioner. Our focus is on preconditioners that are\ncommonly used in practice, are available in major software packages, and can be\nutilized as black-box tools without requiring significant \\textit{a priori}\nknowledge. In addition, we compare these against a selection of classical\nmethods. We primarily compare them without regards to effort needed to compute\nthe preconditioner. Our results show that symmetric positive definite systems\nare mostly likely to benefit from incomplete symmetric factorizations, such as\nincomplete Cholesky (IC). Multigrid methods occasionally do exceptionally well.\nSimple classical techniques, symmetric Gauss Seidel and symmetric SOR, are not\nproductive. We find that including preconditioner construction costs\nsignificantly diminishes the advantages of iterative methods compared to direct\nsolvers; although, tuned IC methods often still outperform direct methods.\nAdditionally, ordering strategies such as approximate minimum degree\nsignificantly enhance IC effectiveness. We plan to expand the benchmark with\nlarger matrices, additional solvers, and detailed metrics to provide actionable\ninformation on SPD preconditioning.", "published": "2025-05-27 04:05:16", "link": "http://arxiv.org/abs/2505.20696v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "An Operator-Splitting Scheme for Viscosity Solutions of Constrained Second-Order PDEs", "abstract": "This work presents a novel operator-splitting scheme for approximating\nviscosity solutions of constrained second-order partial differential equations\n(PDEs) with low-regularity solutions in \\( C(\\overline{\\Omega}_T) \\cap\nH^1(\\Omega_T) \\). By decoupling PDE evolution and constraint enforcement, the\nscheme leverages stabilized finite elements and implicit Euler time-stepping to\nensure consistency, stability, and monotonicity, guaranteeing convergence to\nthe unique viscosity solution via the Barles-Souganidis framework. The method\nsupports vector-valued constraints and unstructured meshes, addressing\nchallenges in traditional approaches such as restrictive stability conditions\nand ill-conditioned systems. Theoretical analysis demonstrates a convergence\nrate of \\( O(h^{1-\\epsilon}) \\) with a proper chosen time step. Applications to\nHamilton-Jacobi equations, reaction-diffusion systems, and two-phase\nNavier-Stokes flows highlight the scheme's versatility and robustness,\npositioning it as a significant advancement in numerical methods for\nconstrained nonlinear PDEs.", "published": "2025-05-27 01:52:37", "link": "http://arxiv.org/abs/2505.20618v1", "categories": ["math.NA", "cs.NA", "65M12, 65M60"], "primary_category": "math.NA"}
{"title": "Connecting randomized iterative methods with Krylov subspaces", "abstract": "Randomized iterative methods, such as the randomized Kaczmarz method, have\ngained significant attention for solving large-scale linear systems due to\ntheir simplicity and efficiency. Meanwhile, Krylov subspace methods have\nemerged as a powerful class of algorithms, known for their robust theoretical\nfoundations and rapid convergence properties. Despite the individual successes\nof these two paradigms, their underlying connection has remained largely\nunexplored. In this paper, we develop a unified framework that bridges\nrandomized iterative methods and Krylov subspace techniques, supported by both\nrigorous theoretical analysis and practical implementation. The core idea is to\nformulate each iteration as an adaptively weighted linear combination of the\nsketched normal vector and previous iterates, with the weights optimally\ndetermined via a projection-based mechanism. This formulation not only reveals\nhow subspace techniques can enhance the efficiency of randomized iterative\nmethods, but also enables the design of a new class of\niterative-sketching-based Krylov subspace algorithms. We prove that our method\nconverges linearly in expectation and validate our findings with numerical\nexperiments.", "published": "2025-05-27 00:40:43", "link": "http://arxiv.org/abs/2505.20602v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Replication of Reference-Dependent Preferences and the Risk-Return Trade-Off in the Chinese Market", "abstract": "This study replicates the findings of Wang et al. (2017) on\nreference-dependent preferences and their impact on the risk-return trade-off\nin the Chinese stock market, a unique context characterized by high retail\ninvestor participation, speculative trading behavior, and regulatory\ncomplexities. Capital Gains Overhang (CGO), a proxy for unrealized gains or\nlosses, is employed to explore how behavioral biases shape cross-sectional\nstock returns in an emerging market setting. Utilizing data from 1995 to 2024\nand econometric techniques such as Dependent Double Sorting and Fama-MacBeth\nregressions, this research investigates the interaction between CGO and five\nrisk proxies: Beta, Return Volatility (RETVOL), Idiosyncratic Volatility\n(IVOL), Firm Age (AGE), and Cash Flow Volatility (CFVOL). Key findings reveal a\nweaker or absent positive risk-return relationship among high-CGO firms and\nstronger positive relationships among low-CGO firms, diverging from U.S. market\nresults, and the interaction effects between CGO and risk proxies, significant\nand positive in the U.S., are predominantly negative in the Chinese market,\nreflecting structural and behavioral differences, such as speculative trading\nand diminished reliance on reference points. The results suggest that\nreference-dependent preferences play a less pronounced role in the Chinese\nmarket, emphasizing the need for tailored investment strategies in emerging\neconomies.", "published": "2025-05-27 01:04:42", "link": "http://arxiv.org/abs/2505.20608v1", "categories": ["q-fin.ST", "q-fin.RM"], "primary_category": "q-fin.ST"}
{"title": "Efficient Spectral Control of Partially Observed Linear Dynamical Systems", "abstract": "We propose a new method for the problem of controlling linear dynamical\nsystems under partial observation and adversarial disturbances. Our new\nalgorithm, Double Spectral Control (DSC), matches the best known regret\nguarantees while exponentially improving runtime complexity over previous\napproaches in its dependence on the system's stability margin. Our key\ninnovation is a two-level spectral approximation strategy, leveraging double\nconvolution with a universal basis of spectral filters, enabling efficient and\naccurate learning of the best linear dynamical controllers.", "published": "2025-05-27 09:28:10", "link": "http://arxiv.org/abs/2505.20943v1", "categories": ["cs.LG", "cs.SY", "eess.SY", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Improved Bounds for Swap Multicalibration and Swap Omniprediction", "abstract": "In this paper, we consider the related problems of multicalibration -- a\nmultigroup fairness notion and omniprediction -- a simultaneous loss\nminimization paradigm, both in the distributional and online settings. The\nrecent work of Garg et al. (2024) raised the open problem of whether it is\npossible to efficiently achieve $O(\\sqrt{T})$ $\\ell_{2}$-multicalibration error\nagainst bounded linear functions. In this paper, we answer this question in a\nstrongly affirmative sense. We propose an efficient algorithm that achieves\n$O(T^{\\frac{1}{3}})$ $\\ell_{2}$-swap multicalibration error (both in high\nprobability and expectation). On propagating this bound onward, we obtain\nsignificantly improved rates for $\\ell_{1}$-swap multicalibration and swap\nomniprediction for a loss class of convex Lipschitz functions. In particular,\nwe show that our algorithm achieves $O(T^{\\frac{2}{3}})$ $\\ell_{1}$-swap\nmulticalibration and swap omniprediction errors, thereby improving upon the\nprevious best-known bound of $O(T^{\\frac{7}{8}})$. As a consequence of our\nimproved online results, we further obtain several improved sample complexity\nrates in the distributional setting. In particular, we establish a\n$O(\\varepsilon ^ {-3})$ sample complexity of efficiently learning an\n$\\varepsilon$-swap omnipredictor for the class of convex and Lipschitz\nfunctions, $O(\\varepsilon ^{-2.5})$ sample complexity of efficiently learning\nan $\\varepsilon$-swap agnostic learner for the squared loss, and $O(\\varepsilon\n^ {-5}), O(\\varepsilon ^ {-2.5})$ sample complexities of learning $\\ell_{1},\n\\ell_{2}$-swap multicalibrated predictors against linear functions, all of\nwhich significantly improve on the previous best-known bounds.", "published": "2025-05-27 08:29:35", "link": "http://arxiv.org/abs/2505.20885v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Stability Selection via Variable Decorrelation", "abstract": "The Lasso is a prominent algorithm for variable selection. However, its\ninstability in the presence of correlated variables in the high-dimensional\nsetting is well-documented. Although previous research has attempted to address\nthis issue by modifying the Lasso loss function, this paper introduces an\napproach that simplifies the data processed by Lasso. We propose that\ndecorrelating variables before applying the Lasso improves the stability of\nvariable selection regardless of the direction of correlation among predictors.\nFurthermore, we highlight that the irrepresentable condition, which ensures\nconsistency for the Lasso, is satisfied after variable decorrelation under two\nassumptions. In addition, by noting that the instability of the Lasso is not\nlimited to high-dimensional settings, we demonstrate the effectiveness of the\nproposed approach for low-dimensional data. Finally, we present empirical\nresults that indicate the efficacy of the proposed method across different\nvariable selection techniques, highlighting its potential for broader\napplication. The DVS R package is developed to facilitate the implementation of\nthe methodology proposed in this paper.", "published": "2025-05-27 08:15:15", "link": "http://arxiv.org/abs/2505.20864v1", "categories": ["stat.ME", "stat.CO", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Debiased Ill-Posed Regression", "abstract": "In various statistical settings, the goal is to estimate a function which is\nrestricted by the statistical model only through a conditional moment\nrestriction. Prominent examples include the nonparametric instrumental variable\nframework for estimating the structural function of the outcome variable, and\nthe proximal causal inference framework for estimating the bridge functions. A\ncommon strategy in the literature is to find the minimizer of the projected\nmean squared error. However, this approach can be sensitive to misspecification\nor slow convergence rate of the estimators of the involved nuisance components.\nIn this work, we propose a debiased estimation strategy based on the influence\nfunction of a modification of the projected error and demonstrate its\nfinite-sample convergence rate. Our proposed estimator possesses a second-order\nbias with respect to the involved nuisance functions and a desirable robustness\nproperty with respect to the misspecification of one of the nuisance functions.\nThe proposed estimator involves a hyper-parameter, for which the optimal value\ndepends on potentially unknown features of the underlying data-generating\nprocess. Hence, we further propose a hyper-parameter selection approach based\non cross-validation and derive an error bound for the resulting estimator. This\nanalysis highlights the potential rate loss due to hyper-parameter selection\nand underscore the importance and advantages of incorporating debiasing in this\nsetting. We also study the application of our approach to the estimation of\nregular parameters in a specific parameter class, which are linear functionals\nof the solutions to the conditional moment restrictions and provide sufficient\nconditions for achieving root-n consistency using our debiased estimator.", "published": "2025-05-27 06:47:33", "link": "http://arxiv.org/abs/2505.20787v1", "categories": ["stat.ME", "econ.EM", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Practical estimation of the optimal classification error with soft labels and calibration", "abstract": "While the performance of machine learning systems has experienced significant\nimprovement in recent years, relatively little attention has been paid to the\nfundamental question: to what extent can we improve our models? This paper\nprovides a means of answering this question in the setting of binary\nclassification, which is practical and theoretically supported. We extend a\nprevious work that utilizes soft labels for estimating the Bayes error, the\noptimal error rate, in two important ways. First, we theoretically investigate\nthe properties of the bias of the hard-label-based estimator discussed in the\noriginal work. We reveal that the decay rate of the bias is adaptive to how\nwell the two class-conditional distributions are separated, and it can decay\nsignificantly faster than the previous result suggested as the number of hard\nlabels per instance grows. Second, we tackle a more challenging problem\nsetting: estimation with corrupted soft labels. One might be tempted to use\ncalibrated soft labels instead of clean ones. However, we reveal that\ncalibration guarantee is not enough, that is, even perfectly calibrated soft\nlabels can result in a substantially inaccurate estimate. Then, we show that\nisotonic calibration can provide a statistically consistent estimator under an\nassumption weaker than that of the previous work. Our method is instance-free,\ni.e., we do not assume access to any input instances. This feature allows it to\nbe adopted in practical scenarios where the instances are not available due to\nprivacy issues. Experiments with synthetic and real-world datasets show the\nvalidity of our methods and theory.", "published": "2025-05-27 06:04:57", "link": "http://arxiv.org/abs/2505.20761v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Stationary MMD Points for Cubature", "abstract": "Approximation of a target probability distribution using a finite set of\npoints is a problem of fundamental importance, arising in cubature, data\ncompression, and optimisation. Several authors have proposed to select points\nby minimising a maximum mean discrepancy (MMD), but the non-convexity of this\nobjective precludes global minimisation in general. Instead, we consider\n\\emph{stationary} points of the MMD which, in contrast to points globally\nminimising the MMD, can be accurately computed. Our main theoretical\ncontribution is the (perhaps surprising) result that, for integrands in the\nassociated reproducing kernel Hilbert space, the cubature error of stationary\nMMD points vanishes \\emph{faster} than the MMD. Motivated by this\n\\emph{super-convergence} property, we consider discretised gradient flows as a\npractical strategy for computing stationary points of the MMD, presenting a\nrefined convergence analysis that establishes a novel non-asymptotic\nfinite-particle error bound, which may be of independent interest.", "published": "2025-05-27 05:53:19", "link": "http://arxiv.org/abs/2505.20754v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series", "abstract": "The field of hypothesis generation promises to reduce costs in neuroscience\nby narrowing the range of interventional studies needed to study various\nphenomena. Existing machine learning methods can generate scientific hypotheses\nfrom complex datasets, but many approaches assume causal relationships are\nstatic over time, limiting their applicability to systems with dynamic,\nstate-dependent behavior, such as the brain. While some techniques attempt\ndynamic causal discovery through factor models, they often restrict\nrelationships to linear patterns or impose other simplifying assumptions. We\npropose a novel method that models dynamic graphs as a conditionally weighted\nsuperposition of static graphs, where each static graph can capture nonlinear\nrelationships. This approach enables the detection of complex, time-varying\ninteractions between variables beyond linear limitations. Our method improves\nf1-scores of predicted dynamic causal patterns by roughly 22-28% on average\nover baselines in some of our experiments, with some improvements reaching well\nover 60%. A case study on real brain data demonstrates our method's ability to\nuncover relationships linked to specific behavioral states, offering valuable\ninsights into neural dynamics.", "published": "2025-05-27 04:06:47", "link": "http://arxiv.org/abs/2505.20697v1", "categories": ["cs.LG", "cs.AI", "stat.AP", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A False Discovery Rate Control Method Using a Fully Connected Hidden Markov Random Field for Neuroimaging Data", "abstract": "False discovery rate (FDR) control methods are essential for voxel-wise\nmultiple testing in neuroimaging data analysis, where hundreds of thousands or\neven millions of tests are conducted to detect brain regions associated with\ndisease-related changes. Classical FDR control methods (e.g., BH, q-value, and\nLocalFDR) assume independence among tests and often lead to high false\nnon-discovery rates (FNR). Although various spatial FDR control methods have\nbeen developed to improve power, they still fall short in jointly addressing\nthree major challenges in neuroimaging applications: capturing complex spatial\ndependencies, maintaining low variability in both false discovery proportion\n(FDP) and false non-discovery proportion (FNP) across replications, and\nachieving computational scalability for high-resolution data. To address these\nchallenges, we propose fcHMRF-LIS, a powerful, stable, and scalable spatial FDR\ncontrol method for voxel-wise multiple testing. It integrates the local index\nof significance (LIS)-based testing procedure with a novel fully connected\nhidden Markov random field (fcHMRF) designed to model complex spatial\nstructures using a parsimonious parameterization. We develop an efficient\nexpectation-maximization algorithm incorporating mean-field approximation, the\nConditional Random Fields as Recurrent Neural Networks (CRF-RNN) technique, and\npermutohedral lattice filtering, reducing the computational complexity from\nquadratic to linear in the number of tests. Extensive simulations demonstrate\nthat fcHMRF-LIS achieves accurate FDR control, lower FNR, reduced variability\nin FDP and FNP, and a higher number of true positives compared to existing\nmethods. Applied to an FDG-PET dataset from the Alzheimer's Disease\nNeuroimaging Initiative, fcHMRF-LIS identifies neurobiologically relevant brain\nregions and offers notable advantages in computational efficiency.", "published": "2025-05-27 03:59:02", "link": "http://arxiv.org/abs/2505.20688v1", "categories": ["stat.ML", "cs.CV", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Moment Expansions of the Energy Distance", "abstract": "The energy distance is used to test distributional equality, and as a loss\nfunction in machine learning. While $D^2(X, Y)=0$ only when $X\\sim Y$, the\nsensitivity to different moments is of practical importance. This work\nconsiders $D^2(X, Y)$ in the case where the distributions are close. In this\nregime, $D^2(X, Y)$ is more sensitive to differences in the means\n$\\bar{X}-\\bar{Y}$, than differences in the covariances $\\Delta$. This is due to\nthe structure of the energy distance and is independent of dimension. The\nsensitivity to on versus off diagonal components of $\\Delta$ is examined when\n$X$ and $Y$ are close to isotropic. Here a dimension dependent averaging occurs\nand, in many cases, off diagonal correlations contribute significantly less.\nNumerical results verify these relationships hold even when distributional\nassumptions are not strictly met.", "published": "2025-05-27 02:52:51", "link": "http://arxiv.org/abs/2505.20647v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH", "62G10 (Primary), 62E20, 62H15, 60E10 (Secondary)", "G.3"], "primary_category": "stat.ML"}
{"title": "Explaining Concept Shift with Interpretable Feature Attribution", "abstract": "Regardless the amount of data a machine learning (ML) model is trained on,\nthere will inevitably be data that differs from their training set, lowering\nmodel performance. Concept shift occurs when the distribution of labels\nconditioned on the features changes, making even a well-tuned ML model to have\nlearned a fundamentally incorrect representation. Identifying these shifted\nfeatures provides unique insight into how one dataset differs from another,\nconsidering the difference may be across a scientifically relevant dimension,\nsuch as time, disease status, population, etc. In this paper, we propose\nSGShift, a model for detecting concept shift in tabular data and attributing\nreduced model performance to a sparse set of shifted features. SGShift models\nconcept shift with a Generalized Additive Model (GAM) and performs subsequent\nfeature selection to identify shifted features. We propose further extensions\nof SGShift by incorporating knockoffs to control false discoveries and an\nabsorption term to account for models with poor fit to the data. We conduct\nextensive experiments in synthetic and real data across various ML models and\nfind SGShift can identify shifted features with AUC $>0.9$ and recall $>90\\%$,\noften 2 or 3 times as high as baseline methods.", "published": "2025-05-27 02:20:50", "link": "http://arxiv.org/abs/2505.20634v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Fundamental Limits of Game-Theoretic LLM Alignment: Smith Consistency and Preference Matching", "abstract": "Nash Learning from Human Feedback is a game-theoretic framework for aligning\nlarge language models (LLMs) with human preferences by modeling learning as a\ntwo-player zero-sum game. However, using raw preference as the payoff in the\ngame highly limits the potential of the game-theoretic LLM alignment framework.\nIn this paper, we systematically study using what choices of payoff based on\nthe pairwise human preferences can yield desirable alignment properties. We\nestablish necessary and sufficient conditions for Condorcet consistency,\ndiversity through mixed strategies, and Smith consistency. These results\nprovide a theoretical foundation for the robustness of game-theoretic LLM\nalignment. Further, we show the impossibility of preference matching -- i.e.,\nno smooth and learnable mappings of pairwise preferences can guarantee a unique\nNash equilibrium that matches a target policy, even under standard assumptions\nlike the Bradley-Terry-Luce model. This result highlights the fundamental\nlimitation of game-theoretic LLM alignment.", "published": "2025-05-27 02:07:35", "link": "http://arxiv.org/abs/2505.20627v1", "categories": ["cs.GT", "stat.ML"], "primary_category": "cs.GT"}
{"title": "Towards One-bit ASR: Extremely Low-bit Conformer Quantization Using Co-training and Stochastic Precision", "abstract": "Model compression has become an emerging need as the sizes of modern speech\nsystems rapidly increase. In this paper, we study model weight quantization,\nwhich directly reduces the memory footprint to accommodate computationally\nresource-constrained applications. We propose novel approaches to perform\nextremely low-bit (i.e., 2-bit and 1-bit) quantization of Conformer automatic\nspeech recognition systems using multiple precision model co-training,\nstochastic precision, and tensor-wise learnable scaling factors to alleviate\nquantization incurred performance loss. The proposed methods can achieve\nperformance-lossless 2-bit and 1-bit quantization of Conformer ASR systems\ntrained with the 300-hr Switchboard and 960-hr LibriSpeech corpus. Maximum\noverall performance-lossless compression ratios of 16.2 and 16.6 times are\nachieved without a statistically significant increase in the word error rate\n(WER) over the full precision baseline systems, respectively.", "published": "2025-05-27 14:25:33", "link": "http://arxiv.org/abs/2505.21245v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unfolding A Few Structures for The Many: Memory-Efficient Compression of Conformer and Speech Foundation Models", "abstract": "This paper presents a novel memory-efficient model compression approach for\nConformer ASR and speech foundation systems. Our approach features a unique\n\"small-to-large\" design. A compact \"seed\" model containing a few Conformer or\nTransformer blocks is trained and unfolded many times to emulate the\nperformance of larger uncompressed models with different logical depths. The\nseed model and many unfolded paths are jointly trained within a single\nunfolding cycle. The KL-divergence between the largest unfolded and smallest\nseed models is used in a self-distillation process to minimize their\nperformance disparity. Experimental results show that our foldable model\nproduces ASR performance comparable to individually constructed Conformer and\nwav2vec2/HuBERT speech foundation models under various depth configurations,\nwhile requiring only minimal memory and storage. Conformer and wav2vec2 models\nwith a reduction of 35% and 30% parameters are obtained without loss of\nperformance, respectively.", "published": "2025-05-27 14:19:21", "link": "http://arxiv.org/abs/2505.21237v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Universal Speech Enhancement with Regression and Generative Mamba", "abstract": "The Interspeech 2025 URGENT Challenge aimed to advance universal, robust, and\ngeneralizable speech enhancement by unifying speech enhancement tasks across a\nwide variety of conditions, including seven different distortion types and five\nlanguages. We present Universal Speech Enhancement Mamba (USEMamba), a\nstate-space speech enhancement model designed to handle long-range sequence\nmodeling, time-frequency structured processing, and sampling\nfrequency-independent feature extraction. Our approach primarily relies on\nregression-based modeling, which performs well across most distortions.\nHowever, for packet loss and bandwidth extension, where missing content must be\ninferred, a generative variant of the proposed USEMamba proves more effective.\nDespite being trained on only a subset of the full training data, USEMamba\nachieved 2nd place in Track 1 during the blind test phase, demonstrating strong\ngeneralization across diverse conditions.", "published": "2025-05-27 13:45:01", "link": "http://arxiv.org/abs/2505.21198v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multimodal Assessment of Speech Impairment in ALS Using Audio-Visual and Machine Learning Approaches", "abstract": "The analysis of speech in individuals with amyotrophic lateral sclerosis is a\npowerful tool to support clinicians in the assessment of bulbar dysfunction.\nHowever, current methods used in clinical practice consist of subjective\nevaluations or expensive instrumentation. This study investigates different\napproaches combining audio-visual analysis and machine learning to predict the\nspeech impairment evaluation performed by clinicians. Using a small dataset of\nacoustic and kinematic features extracted from audio and video recordings of\nspeech tasks, we trained and tested some regression models. The best\nperformance was achieved using the extreme boosting machine regressor with\nmultimodal features, which resulted in a root mean squared error of 0.93 on a\nscale ranging from 5 to 25. Results suggest that integrating audio-video\nanalysis enhances speech impairment assessment, providing an objective tool for\nearly detection and monitoring of bulbar dysfunction, also in home settings.", "published": "2025-05-27 12:20:15", "link": "http://arxiv.org/abs/2505.21093v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Study of Lightweight Transformer Architectures for Single-Channel Speech Enhancement", "abstract": "In speech enhancement, achieving state-of-the-art (SotA) performance while\nadhering to the computational constraints on edge devices remains a formidable\nchallenge. Networks integrating stacked temporal and spectral modelling\neffectively leverage improved architectures such as transformers; however, they\ninevitably incur substantial computational complexity and model expansion.\nThrough systematic ablation analysis on transformer-based temporal and spectral\nmodelling, we demonstrate that the architecture employing streamlined\nFrequency-Time-Frequency (FTF) stacked transformers efficiently learns global\ndependencies within causal context, while avoiding considerable computational\ndemands. Utilising discriminators in training further improves learning\nefficacy and enhancement without introducing additional complexity during\ninference. The proposed lightweight, causal, transformer-based architecture\nwith adversarial training (LCT-GAN) yields SoTA performance on instrumental\nmetrics among contemporary lightweight models, but with far less overhead.\nCompared to DeepFilterNet2, the LCT-GAN only requires 6% of the parameters, at\nsimilar complexity and performance. Against CCFNet+(Lite), LCT-GAN saves 9% in\nparameters and 10% in multiply-accumulate operations yet yielding improved\nperformance. Further, the LCT-GAN even outperforms more complex, common\nbaseline models on widely used test datasets.", "published": "2025-05-27 11:45:54", "link": "http://arxiv.org/abs/2505.21057v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Spotlight-TTS: Spotlighting the Style via Voiced-Aware Style Extraction and Style Direction Adjustment for Expressive Text-to-Speech", "abstract": "Recent advances in expressive text-to-speech (TTS) have introduced diverse\nmethods based on style embedding extracted from reference speech. However,\nsynthesizing high-quality expressive speech remains challenging. We propose\nSpotlight-TTS, which exclusively emphasizes style via voiced-aware style\nextraction and style direction adjustment. Voiced-aware style extraction\nfocuses on voiced regions highly related to style while maintaining continuity\nacross different speech regions to improve expressiveness. We adjust the\ndirection of the extracted style for optimal integration into the TTS model,\nwhich improves speech quality. Experimental results demonstrate that\nSpotlight-TTS achieves superior performance compared to baseline models in\nterms of expressiveness, overall speech quality, and style transfer capability.\nOur audio samples are publicly available.", "published": "2025-05-27 08:20:01", "link": "http://arxiv.org/abs/2505.20868v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VibE-SVC: Vibrato Extraction with High-frequency F0 Contour for Singing Voice Conversion", "abstract": "Controlling singing style is crucial for achieving an expressive and natural\nsinging voice. Among the various style factors, vibrato plays a key role in\nconveying emotions and enhancing musical depth. However, modeling vibrato\nremains challenging due to its dynamic nature, making it difficult to control\nin singing voice conversion. To address this, we propose VibESVC, a\ncontrollable singing voice conversion model that explicitly extracts and\nmanipulates vibrato using discrete wavelet transform. Unlike previous methods\nthat model vibrato implicitly, our approach decomposes the F0 contour into\nfrequency components, enabling precise transfer. This allows vibrato control\nfor enhanced flexibility. Experimental results show that VibE-SVC effectively\ntransforms singing styles while preserving speaker similarity. Both subjective\nand objective evaluations confirm high-quality conversion.", "published": "2025-05-27 06:56:13", "link": "http://arxiv.org/abs/2505.20794v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Can Large Language Models Predict Audio Effects Parameters from Natural Language?", "abstract": "In music production, manipulating audio effects (Fx) parameters through\nnatural language has the potential to reduce technical barriers for\nnon-experts. We present LLM2Fx, a framework leveraging Large Language Models\n(LLMs) to predict Fx parameters directly from textual descriptions without\nrequiring task-specific training or fine-tuning. Our approach address the\ntext-to-effect parameter prediction (Text2Fx) task by mapping natural language\ndescriptions to the corresponding Fx parameters for equalization and\nreverberation. We demonstrate that LLMs can generate Fx parameters in a\nzero-shot manner that elucidates the relationship between timbre semantics and\naudio effects in music production. To enhance performance, we introduce three\ntypes of in-context examples: audio Digital Signal Processing (DSP) features,\nDSP function code, and few-shot examples. Our results demonstrate that\nLLM-based Fx parameter generation outperforms previous optimization approaches,\noffering competitive performance in translating natural language descriptions\nto appropriate Fx settings. Furthermore, LLMs can serve as text-driven\ninterfaces for audio production, paving the way for more intuitive and\naccessible music production tools.", "published": "2025-05-27 06:21:56", "link": "http://arxiv.org/abs/2505.20770v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "REWIND: Speech Time Reversal for Enhancing Speaker Representations in Diffusion-based Voice Conversion", "abstract": "Speech time reversal refers to the process of reversing the entire speech\nsignal in time, causing it to play backward. Such signals are completely\nunintelligible since the fundamental structures of phonemes and syllables are\ndestroyed. However, they still retain tonal patterns that enable perceptual\nspeaker identification despite losing linguistic content. In this paper, we\npropose leveraging speaker representations learned from time reversed speech as\nan augmentation strategy to enhance speaker representation. Notably, speaker\nand language disentanglement in voice conversion (VC) is essential to\naccurately preserve a speaker's unique vocal traits while minimizing\ninterference from linguistic content. The effectiveness of the proposed\napproach is evaluated in the context of state-of-the-art diffusion-based VC\nmodels. Experimental results indicate that the proposed approach significantly\nimproves speaker similarity-related scores while maintaining high speech\nquality.", "published": "2025-05-27 05:59:39", "link": "http://arxiv.org/abs/2505.20756v1", "categories": ["eess.AS", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Foundation Model Hidden Representations for Heart Rate Estimation from Auscultation", "abstract": "Auscultation, particularly heart sound, is a non-invasive technique that\nprovides essential vital sign information. Recently, self-supervised acoustic\nrepresentation foundation models (FMs) have been proposed to offer insights\ninto acoustics-based vital signs. However, there has been little exploration of\nthe extent to which auscultation is encoded in these pre-trained FM\nrepresentations. In this work, using a publicly available phonocardiogram (PCG)\ndataset and a heart rate (HR) estimation model, we conduct a layer-wise\ninvestigation of six acoustic representation FMs: HuBERT, wav2vec2, wavLM,\nWhisper, Contrastive Language-Audio Pretraining (CLAP), and an in-house CLAP\nmodel. Additionally, we implement the baseline method from Nie et al., 2024\n(which relies on acoustic features) and show that overall, representation\nvectors from pre-trained foundation models (FMs) offer comparable performance\nto the baseline. Notably, HR estimation using the representations from the\naudio encoder of the in-house CLAP model outperforms the results obtained from\nthe baseline, achieving a lower mean absolute error (MAE) across various\ntrain/validation/test splits despite the domain mismatch.", "published": "2025-05-27 05:36:25", "link": "http://arxiv.org/abs/2505.20745v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Uni-VERSA: Versatile Speech Assessment with a Unified Network", "abstract": "Subjective listening tests remain the golden standard for speech quality\nassessment, but are costly, variable, and difficult to scale. In contrast,\nexisting objective metrics, such as PESQ, F0 correlation, and DNSMOS, typically\ncapture only specific aspects of speech quality. To address these limitations,\nwe introduce Uni-VERSA, a unified network that simultaneously predicts various\nobjective metrics, encompassing naturalness, intelligibility, speaker\ncharacteristics, prosody, and noise, for a comprehensive evaluation of speech\nsignals. We formalize its framework, evaluation protocol, and applications in\nspeech enhancement, synthesis, and quality control. A benchmark based on the\nURGENT24 challenge, along with a baseline leveraging self-supervised\nrepresentations, demonstrates that Uni-VERSA provides a viable alternative to\nsingle-aspect evaluation methods. Moreover, it aligns closely with human\nperception, making it a promising approach for future speech quality\nassessment.", "published": "2025-05-27 05:31:19", "link": "http://arxiv.org/abs/2505.20741v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech in Low-Resource Indian Languages", "abstract": "What happens when an English Fairytaler is fine-tuned on Indian languages? We\nevaluate how the English F5-TTS model adapts to 11 Indian languages, measuring\npolyglot fluency, voice-cloning, style-cloning, and code-mixing. We compare:\n(i) training from scratch, (ii) fine-tuning English F5 on Indian data, and\n(iii) fine-tuning on both Indian and English data to prevent forgetting.\nFine-tuning with only Indian data proves most effective and the resultant IN-F5\nis a near-human polyglot; that enables speakers of one language (e.g., Odia) to\nfluently speak in another (e.g., Hindi). Our results show English pretraining\naids low-resource TTS in reaching human parity. To aid progress in other\nlow-resource languages, we study data-constrained setups and arrive at a\ncompute optimal strategy. Finally, we show IN-F5 can synthesize unseen\nlanguages like Bhojpuri and Tulu using a human-in-the-loop approach for\nzero-resource TTS via synthetic data generation.", "published": "2025-05-27 04:02:01", "link": "http://arxiv.org/abs/2505.20693v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "PromptEVC: Controllable Emotional Voice Conversion with Natural Language Prompts", "abstract": "Controllable emotional voice conversion (EVC) aims to manipulate emotional\nexpressions to increase the diversity of synthesized speech. Existing methods\ntypically rely on predefined labels, reference audios, or prespecified factor\nvalues, often overlooking individual differences in emotion perception and\nexpression. In this paper, we introduce PromptEVC that utilizes natural\nlanguage prompts for precise and flexible emotion control. To bridge text\ndescriptions with emotional speech, we propose emotion descriptor and prompt\nmapper to generate fine-grained emotion embeddings, trained jointly with\nreference embeddings. To enhance naturalness, we present a prosody modeling and\ncontrol pipeline that adjusts the rhythm based on linguistic content and\nemotional cues. Additionally, a speaker encoder is incorporated to preserve\nidentity. Experimental results demonstrate that PromptEVC outperforms\nstate-of-the-art controllable EVC methods in emotion conversion, intensity\ncontrol, mixed emotion synthesis, and prosody manipulation. Speech samples are\navailable at https://jeremychee4.github.io/PromptEVC/.", "published": "2025-05-27 03:50:59", "link": "http://arxiv.org/abs/2505.20678v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Music's Multimodal Complexity in AVQA: Why We Need More than General Multimodal LLMs", "abstract": "While recent Multimodal Large Language Models exhibit impressive capabilities\nfor general multimodal tasks, specialized domains like music necessitate\ntailored approaches. Music Audio-Visual Question Answering (Music AVQA)\nparticularly underscores this, presenting unique challenges with its\ncontinuous, densely layered audio-visual content, intricate temporal dynamics,\nand the critical need for domain-specific knowledge. Through a systematic\nanalysis of Music AVQA datasets and methods, this position paper identifies\nthat specialized input processing, architectures incorporating dedicated\nspatial-temporal designs, and music-specific modeling strategies are critical\nfor success in this domain. Our study provides valuable insights for\nresearchers by highlighting effective design patterns empirically linked to\nstrong performance, proposing concrete future directions for incorporating\nmusical priors, and aiming to establish a robust foundation for advancing\nmultimodal musical understanding. This work is intended to inspire broader\nattention and further research, supported by a continuously updated anonymous\nGitHub repository of relevant papers:\nhttps://github.com/xid32/Survey4MusicAVQA.", "published": "2025-05-27 02:31:24", "link": "http://arxiv.org/abs/2505.20638v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Plug-and-Play Co-Occurring Face Attention for Robust Audio-Visual Speaker Extraction", "abstract": "Audio-visual speaker extraction isolates a target speaker's speech from a\nmixture speech signal conditioned on a visual cue, typically using the target\nspeaker's face recording. However, in real-world scenarios, other co-occurring\nfaces are often present on-screen, providing valuable speaker activity cues in\nthe scene. In this work, we introduce a plug-and-play inter-speaker attention\nmodule to process these flexible numbers of co-occurring faces, allowing for\nmore accurate speaker extraction in complex multi-person environments. We\nintegrate our module into two prominent models: the AV-DPRNN and the\nstate-of-the-art AV-TFGridNet. Extensive experiments on diverse datasets,\nincluding the highly overlapped VoxCeleb2 and sparsely overlapped MISP,\ndemonstrate that our approach consistently outperforms baselines. Furthermore,\ncross-dataset evaluations on LRS2 and LRS3 confirm the robustness and\ngeneralizability of our method.", "published": "2025-05-27 02:21:38", "link": "http://arxiv.org/abs/2505.20635v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Expectation-maximization for multi-reference alignment: Two pitfalls and one remedy", "abstract": "We study the multi-reference alignment model, which involves recovering a\nsignal from noisy observations that have been randomly transformed by an\nunknown group action, a fundamental challenge in statistical signal processing,\ncomputational imaging, and structural biology. While much of the theoretical\nliterature has focused on the asymptotic sample complexity of this model, the\npractical performance of reconstruction algorithms, particularly of the\nomnipresent expectation maximization (EM) algorithm, remains poorly understood.\n  In this work, we present a detailed investigation of EM in the challenging\nlow signal-to-noise ratio (SNR) regime. We identify and characterize two\nfailure modes that emerge in this setting. The first, called Einstein from\nNoise, reveals a strong sensitivity to initialization, with reconstructions\nresembling the input template regardless of the true underlying signal. The\nsecond phenomenon, referred to as the Ghost of Newton, involves EM initially\nconverging towards the correct solution but later diverging, leading to a loss\nof reconstruction fidelity. We provide theoretical insights and support our\nfindings through numerical experiments. Finally, we introduce a simple, yet\neffective modification to EM based on mini-batching, which mitigates the above\nartifacts. Supported by both theory and experiments, this mini-batching\napproach processes small data subsets per iteration, reducing initialization\nbias and computational cost, while maintaining accuracy comparable to\nfull-batch EM.", "published": "2025-05-27 17:08:37", "link": "http://arxiv.org/abs/2505.21435v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "WiCAL: Accurate Wi-Fi-Based 3D Localization Enabled by Collaborative Antenna Arrays", "abstract": "Accurate 3D localization is essential for realizing advanced sensing\nfunctionalities in next-generation Wi-Fi communication systems. This study\ninvestigates the potential of multistatic localization in Wi-Fi networks\nthrough the deployment of multiple cooperative antenna arrays. The\ncollaborative gain offered by these arrays is twofold: (i) intra-array coherent\ngain at the wavelength scale among antenna elements, and (ii) inter-array\ncooperative gain across arrays. To evaluate the feasibility and performance of\nthis approach, we develop WiCAL (Wi-Fi Collaborative Antenna Localization), a\nsystem built upon commercial Wi-Fi infrastructure equipped with uniform\nrectangular arrays. These arrays are driven by multiplexing embedded radio\nfrequency chains available in standard access points or user devices, thereby\neliminating the need for sophisticated, costly, and power-hungry\nmulti-transceiver modules typically required in multiple-input and\nmultiple-output systems. To address phase offsets introduced by RF chain\nmultiplexing, we propose a three-stage, fine-grained phase alignment scheme to\nsynchronize signals across antenna elements within each array. A bidirectional\nspatial smoothing MUSIC algorithm is employed to estimate angles of arrival\n(AoAs) and mitigate performance degradation caused by correlated interference.\nTo further exploit inter-array cooperative gain, we elaborate on the\nsynchronization mechanism among distributed URAs, which enables direct position\ndetermination by bypassing intermediate angle estimation. Once synchronized,\nthe distributed URAs effectively form a virtual large-scale array,\nsignificantly enhancing spatial resolution and localization accuracy.", "published": "2025-05-27 16:33:00", "link": "http://arxiv.org/abs/2505.21408v1", "categories": ["eess.SY", "cs.SY", "eess.SP"], "primary_category": "eess.SY"}
{"title": "Label-free Super-Resolution Microvessel Color Flow Imaging with Ultrasound", "abstract": "We present phase subtraction imaging (PSI), a new spatial-temporal\nbeamforming method that enables micrometer level resolution imaging of\nmicrovessels in live animals without labels, which are microbubbles in\nultrasound super-resolution imaging. Subtraction of relative phase differences\nbetween consecutive frames beamformed with mismatched apodizations is used in\nPSI to overcome the diffraction limit. We validated this method by imaging both\nthe mouse brain and rabbit kidney using different ultrasound probes and\nscanning machines.", "published": "2025-05-27 16:12:00", "link": "http://arxiv.org/abs/2505.21384v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Graph Neural Network Aided Detection for the Multi-User Multi-Dimensional Index Modulated Uplink", "abstract": "The concept of Compressed Sensing-aided Space-Frequency Index Modulation\n(CS-SFIM) is conceived for the Large-Scale Multi-User Multiple-Input\nMultiple-Output Uplink (LS-MU-MIMO-UL) of Next-Generation (NG) networks.\nExplicitly, in CS-SFIM, the information bits are mapped to both spatial- and\nfrequency-domain indices, where we treat the activation patterns of the\ntransmit antennas and of the subcarriers separately. Serving a large number of\nusers in an MU-MIMO-UL system leads to substantial Multi-User Interference\n(MUI). Hence, we design the Space-Frequency (SF) domain matrix as a joint\nfactor graph, where the Approximate Message Passing (AMP) and Expectation\nPropagation (EP) based MU detectors can be utilized. In the LS-MU-MIMO-UL\nscenario considered, the proposed system uses optimal Maximum Likelihood (ML)\nand Minimum Mean Square Error (MMSE) detectors as benchmarks for comparison\nwith the proposed MP-based detectors. These MP-based detectors significantly\nreduce the detection complexity compared to ML detection, making the design\neminently suitable for LS-MU scenarios. To further reduce the detection\ncomplexity and improve the detection performance, we propose a pair of Graph\nNeural Network (GNN) based detectors, which rely on the orthogonal AMP (OAMP)\nand on the EP algorithm, which we refer to as the GNN-AMP and GEPNet detectors,\nrespectively. The GEPNet detector maximizes the detection performance, while\nthe GNN-AMP detector strikes a performance versus complexity trade-off. The GNN\nis trained for a single system configuration and yet it can be used for any\nnumber of users in the system. The simulation results show that the GNN-based\ndetector approaches the ML performance in various configurations.", "published": "2025-05-27 15:34:44", "link": "http://arxiv.org/abs/2505.21343v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "CiUAV: A Multi-Task 3D Indoor Localization System for UAVs based on Channel State Information", "abstract": "Accurate indoor positioning for unmanned aerial vehicles (UAVs) is critical\nfor logistics, surveillance, and emergency response applications, particularly\nin GPS-denied environments. Existing indoor localization methods, including\noptical tracking, ultra-wideband, and Bluetooth-based systems, face cost,\naccuracy, and robustness trade-offs, limiting their practicality for UAV\nnavigation. This paper proposes CiUAV, a novel 3D indoor localization system\ndesigned for UAVs, leveraging channel state information (CSI) obtained from\nlow-cost ESP32 IoT-based sensors. The system incorporates a dynamic automatic\ngain control (AGC) compensation algorithm to mitigate noise and stabilize CSI\nsignals, significantly enhancing the robustness of the measurement.\nAdditionally, a multi-task 3D localization model, Sensor-in-Sample (SiS), is\nintroduced to enhance system robustness by addressing challenges related to\nincomplete sensor data and limited training samples. SiS achieves this by joint\ntraining with varying sensor configurations and sample sizes, ensuring reliable\nperformance even in resource-constrained scenarios. Experiment results\ndemonstrate that CiUAV achieves a LMSE localization error of 0.2629 m in a 3D\nspace, achieving good accuracy and robustness. The proposed system provides a\ncost-effective and scalable solution, demonstrating its usefulness for UAV\napplications in resource-constrained indoor environments.", "published": "2025-05-27 14:05:12", "link": "http://arxiv.org/abs/2505.21216v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Channel-Aware Holographic Decision Fusion", "abstract": "This work investigates Distributed Detection (DD) in Wireless Sensor Networks\n(WSNs) utilizing channel-aware binary-decision fusion over a shared flat-fading\nchannel. A reconfigurable metasurface, positioned in the near-field of a\nlimited number of receive antennas, is integrated to enable a holographic\nDecision Fusion (DF) system. This approach minimizes the need for multiple RF\nchains while leveraging the benefits of a large array. The optimal fusion rule\nfor a fixed metasurface configuration is derived, alongside two suboptimal\njoint fusion rule and metasurface design strategies. These suboptimal\napproaches strike a balance between reduced complexity and lower system\nknowledge requirements, making them practical alternatives. The design\nobjective focuses on effectively conveying the information regarding the\nphenomenon of interest to the FC while promoting energy-efficient data\nanalytics aligned with the Internet of Things (IoT) paradigm. Simulation\nresults underscore the viability of holographic DF, demonstrating its\nadvantages even with suboptimal designs and highlighting the significant\nenergy-efficiency gains achieved by the proposed system.", "published": "2025-05-27 11:14:29", "link": "http://arxiv.org/abs/2505.21035v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Interference Detection in Spectrum-Blind Multi-User Optical Spectrum as a Service", "abstract": "With the growing demand for high-bandwidth, low-latency applications, Optical\nSpectrum as a Service (OSaaS) is of interest for flexible bandwidth allocation\nwithin Elastic Optical Networks (EONs) and Open Line Systems (OLS). While OSaaS\nfacilitates transparent connectivity and resource sharing among users, it\nraises concerns over potential network vulnerabilities due to shared fiber\naccess and inter-channel interference, such as fiber non-linearity and\namplifier based crosstalk. These challenges are exacerbated in multi-user\nenvironments, complicating the identification and localization of service\ninterferences. To reduce system disruptions and system repair costs, it is\nbeneficial to detect and identify such interferences timely. Addressing these\nchallenges, this paper introduces a Machine Learning (ML) based architecture\nfor network operators to detect and attribute interferences to specific OSaaS\nusers while blind to the users' internal spectrum details. Our methodology\nleverages available coarse power measurements and operator channel performance\ndata, bypassing the need for internal user information of wide-band shared\nspectra. Experimental studies conducted on a 190 km optical line system in the\nOpen Ireland testbed, with three OSaaS users demonstrate the model's capability\nto accurately classify the source of interferences, achieving a classification\naccuracy of 90.3%.", "published": "2025-05-27 10:51:27", "link": "http://arxiv.org/abs/2505.21018v1", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI"}
{"title": "CNN-Based Channel Map Estimation for Movable Antenna Systems", "abstract": "Movable antenna (MA) has attracted increasing attention in wireless\ncommunications due to its capability of wireless channel reconfiguration\nthrough local antenna movement within a confined region at the\ntransmitter/receiver. However, to determine the optimal antenna positions,\nchannel state information (CSI) within the entire region, termed small-scale\nchannel map, is required, which poses a significant challenge due to the\nunaffordable overhead for exhaustive channel estimation at all positions. To\ntackle this challenge, in this paper, we propose a new convolutional neural\nnetwork (CNN)-based estimation scheme to reconstruct the small-scale channel\nmap within a three-dimensional (3D) movement region. Specifically, we first\ncollect a set of CSI measurements corresponding to a subset of MA positions and\ndifferent receiver locations offline to comprehensively capture the\nenvironmental features. Subsequently, we train a CNN using the collected data,\nwhich is then used to reconstruct the full channel map during real-time\ntransmission only based on a finite number of channel measurements taken at\nseveral selected MA positions within the 3D movement region. Numerical results\ndemonstrate that our proposed scheme can accurately reconstruct the small-scale\nchannel map and outperforms other benchmark schemes.", "published": "2025-05-27 10:35:01", "link": "http://arxiv.org/abs/2505.21001v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Ergonomic Assessment of Work Activities for an Industrial-oriented Wrist Exoskeleton", "abstract": "Musculoskeletal disorders (MSD) are the most common cause of work-related\ninjuries and lost production involving approximately 1.7 billion people\nworldwide and mainly affect low back (more than 50%) and upper limbs (more than\n40%). It has a profound effect on both the workers affected and the company.\nThis paper provides an ergonomic assessment of different work activities in a\nhorse saddle-making company, involving 5 workers. This aim guides the design of\na wrist exoskeleton to reduce the risk of musculoskeletal diseases wherever it\nis impossible to automate the production process. This evaluation is done\neither through subjective and objective measurement, respectively using\nquestionnaires and by measurement of muscle activation with sEMG sensors.", "published": "2025-05-27 09:26:43", "link": "http://arxiv.org/abs/2505.20939v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Analysis of Joint Radar and Communication in Disaster Scenarios", "abstract": "With the increasing frequency and intensity of natural disasters, there is a\nnecessity for advanced technologies that can provide reliable situational\nawareness and communication. Conventional systems are often inadequate due to\nunreliable infrastructure, power grid failures, high investment costs and\nscalability challenges. This paper explores the potential of ad-hoc mesh joint\nradar and communication (JRC) networks as a scalable, resilient,\nenergy-efficient solution for disaster management that can operate\nindependently of conventional infrastructure. The proposed JRC network enhances\ndisaster response by integrating target detection (such as identifying vital\nsigns, hazardous leaks, and fires) with communication capabilities to ensure\nefficient information dissemination under intense clutter conditions. Key\nperformance metrics, including data rate, Signal-to-Clutter and Noise Ratio\n(SCNR), probability of detection, and false alarm rate, are used to assess\nperformance. An optimization approach is proposed to provide an\nenergy-efficient resource allocation scheme. The results show the performance\nof ad-hoc mesh JRC systems, underscoring their potential to enhance disaster\nmanagement efforts by addressing unique operational challenges.", "published": "2025-05-27 09:21:18", "link": "http://arxiv.org/abs/2505.20931v1", "categories": ["eess.SY", "cs.SY", "eess.SP"], "primary_category": "eess.SY"}
{"title": "Recognition of Physiological Patterns during Activities of Daily Living Using Wearable Biosignal Sensors", "abstract": "A key aspect of developing fall prevention systems is the early prediction of\na fall before it occurs. This paper presents a statistical overview of results\nobtained by analyzing 22 activities of daily living to recognize physiological\npatterns and estimate the risk of an imminent fall. The results demonstrate\ndistinctive patterns between high-intensity and low-intensity activity using\nEMG, ECG, and respiration sensors, also indicating the presence of a\nproportional trend between movement velocity and muscle activity. These\noutcomes highlight the potential benefits of using these sensors in the future\nto direct the development of an activity recognition and risk prediction\nframework for physiological phenomena that can cause fall injuries.", "published": "2025-05-27 09:09:23", "link": "http://arxiv.org/abs/2505.20917v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Dynamic Resource Allocation in Distributed MIMO-LEO Satellite Networks", "abstract": "This paper characterizes the impacts of channel estimation errors and Rician\nfactors on achievable data rate and investigates the user scheduling strategy,\ncombining scheme, power control, and dynamic bandwidth allocation to maximize\nthe sum data rate in the distributed multiple-input-multiple-output\n(MIMO)-enabled low earth orbit (LEO) satellite networks. However, due to the\nresource-assignment problem, it is challenging to find the optimal solution for\nmaximizing the sum data rate. To transform this problem into a more tractable\nform, we first quantify the channel estimation errors based on the minimum mean\nsquare error (MMSE) estimator and rigorously derive a closed-form lower bound\nof the achievable data rate, offering an explicit formulation for resource\nallocation. Then, to solve the NP-hard problem, we decompose it into three\nsub-problems, namely, user scheduling strategy, joint combination and power\ncontrol, and dynamic bandwidth allocation, by using alternative optimization\n(AO). Specifically, the user scheduling is formulated as a graph coloring\nproblem by iteratively updating an undirected graph based on user requirements,\nwhich is then solved using the DSatur algorithm. For the combining weights and\npower control, the successive convex approximation (SCA) and geometrical\nprogramming (GP) are adopted to obtain the sub-optimal solution with lower\ncomplexity. Finally, the optimal bandwidth allocation can be achieved by\nsolving the concave problem.\n  Numerical results validate the analytical tightness of the derived bound,\nespecially for large Rician factors, and demonstrate significant performance\ngains over other benchmarks.", "published": "2025-05-27 08:33:12", "link": "http://arxiv.org/abs/2505.20891v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Dynamical ON-OFF Control with Trajectory Prediction for Multi-RIS Wireless Networks", "abstract": "Reconfigurable intelligent surfaces (RISs) have demonstrated an unparalleled\nability to reconfigure wireless environments by dynamically controlling the\nphase, amplitude, and polarization of impinging waves. However, as nearly\npassive reflective metasurfaces, RISs may not distinguish between desired and\ninterference signals, which can lead to severe spectrum pollution and even\naffect performance negatively. In particular, in large-scale networks, the\nsignal-to-interference-plus-noise ratio (SINR) at the receiving node can be\ndegraded due to excessive interference reflected from the RIS. To overcome this\nfundamental limitation, we propose in this paper a trajectory prediction-based\ndynamical control algorithm (TPC) for anticipating RIS ON-OFF states sequence,\nintegrating a long-short-term-memory (LSTM) scheme to predict user\ntrajectories. In particular, through a codebook-based algorithm, the RIS\ncontroller adaptively coordinates the configuration of the RIS elements to\nmaximize the received SINR. Our simulation results demonstrate the superiority\nof the proposed TPC method over various system settings.", "published": "2025-05-27 08:31:26", "link": "http://arxiv.org/abs/2505.20887v1", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI"}
{"title": "Continuous SpO2 Monitoring Using Reflectance Pulse Oximetry at the Wrist and Upper Arm During Overnight Sleep Apnea Recordings", "abstract": "Sleep apnea (SA) is a chronic sleep-related disorder consisting of repetitive\npauses or restrictions in airflow during sleep and is known to be a risk factor\nfor cerebro- and cardiovascular disease. It is generally diagnosed using\npolysomnography (PSG) recorded overnight in an in-lab setting at the hospital.\nThis includes the measurement of blood oxygen saturation (SpO2), which exhibits\nfluctuations caused by SA events. In this paper, we investigate the accuracy\nand utility of reflectance pulse oximetry from a wearable device as a means to\ncontinuously monitor SpO2 during sleep. To this end, we analyzed data from a\ncohort of 134 patients with suspected SA undergoing overnight PSG and wearing\nthe watch-like device at two measurement locations (upper arm and wrist). Our\ndata show that standard requirements for pulse oximetry measurements are met at\nboth measurement locations, with an accuracy (root mean squared error) of 1.9%\nat the upper arm and 3.2% at the wrist. With a rejection rate of 3.1%, the\nupper arm yielded better results in terms of data quality when compared to the\nwrist location which had 30.4% of data rejected.", "published": "2025-05-27 08:01:38", "link": "http://arxiv.org/abs/2505.20846v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Estimators and Performance Bounds for Short Periodic Pulses", "abstract": "In many industrial applications, signals with short periodic pulses, caused\nby repeated steps in the manufacturing process, are present, and their\nfundamental frequency or period may be of interest. Fundamental frequency\nestimation is in many cases performed by describing the periodic signal as a\nmultiharmonic signal and employing the corresponding maximum likelihood\nestimator. However, since signals with short periodic pulses contain a large\nnumber of noise-only samples, the multiharmonic signal model is not optimal to\ndescribe them. In this work, two models of short periodic pulses with known and\nunknown pulse shape are considered. For both models, the corresponding maximum\nlikelihood estimators, Fisher information matrices, and approximate\nCram\\'er-Rao lower bounds are presented. Numerical results demonstrate that the\nproposed estimators outperform the maximum likelihood estimator based on the\nmultiharmonic signal model for low signal-to-noise ratios.", "published": "2025-05-27 07:49:18", "link": "http://arxiv.org/abs/2505.20831v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Dual-Polarization Stacked Intelligent Metasurfaces for Holographic MIMO", "abstract": "To address the limited wave domain signal processing capabilities of\ntraditional single-polarized stacked intelligent metasurfaces (SIMs) in\nholographic multiple-input multiple-output (HMIMO) systems, which stems from\nlimited integration space, this paper proposes a dual-polarized SIM (DPSIM)\narchitecture. By stacking dual-polarized reconfigurable intelligent surfaces\n(DPRIS), DPSIM can independently process signals of two orthogonal\npolarizations in the wave domain, thereby effectively suppressing polarization\ncross-interference (PCI) and inter-stream interference (ISI). We introduce a\nlayer-by-layer gradient descent with water-filling (LGD-WF) algorithm to\nenhance end-to-end performance. Simulation results show that, under the same\nnumber of metasurface layers and unit size, the DPSIM-aided HMIMO system can\nsupport more simultaneous data streams for ISI-free parallel transmission\ncompared to traditional SIM-aided systems. Furthermore, under different\npolarization imperfection conditions, both the spectral efficiency (SE) and\nenergy efficiency (EE) of the DPSIM-aided HMIMO system are significantly\nimproved, approaching the theoretical upper bound.", "published": "2025-05-27 07:09:13", "link": "http://arxiv.org/abs/2505.20805v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Non-invasive maturity assessment of iPSC-CMs based on optical maturity characteristics using interpretable AI", "abstract": "Human induced pluripotent stem cell-derived cardiomyocytes (iPSC-CMs) are an\nimportant resource for the identification of new therapeutic targets and\ncardioprotective drugs. After differentiation iPSC-CMs show an immature,\nfetal-like phenotype. Cultivation of iPSC-CMs in lipid-supplemented maturation\nmedium (MM) strongly enhances their structural, metabolic and functional\nphenotype. Nevertheless, assessing iPSC-CM maturation state remains challenging\nas most methods are time consuming and go in line with cell damage or loss of\nthe sample. To address this issue, we developed a non-invasive approach for\nautomated classification of iPSC-CM maturity through interpretable artificial\nintelligence (AI)-based analysis of beat characteristics derived from\nvideo-based motion analysis. In a prospective study, we evaluated 230 video\nrecordings of early-state, immature iPSC-CMs on day 21 after differentiation\n(d21) and more mature iPSC-CMs cultured in MM (d42, MM). For each recording, 10\nfeatures were extracted using Maia motion analysis software and entered into a\nsupport vector machine (SVM). The hyperparameters of the SVM were optimized in\na grid search on 80 % of the data using 5-fold cross-validation. The optimized\nmodel achieved an accuracy of 99.5 $\\pm$ 1.1 % on a hold-out test set. Shapley\nAdditive Explanations (SHAP) identified displacement, relaxation-rise time and\nbeating duration as the most relevant features for assessing maturity level.\nOur results suggest the use of non-invasive, optical motion analysis combined\nwith AI-based methods as a tool to assess iPSC-CMs maturity and could be\napplied before performing functional readouts or drug testing. This may\npotentially reduce the variability and improve the reproducibility of\nexperimental studies.", "published": "2025-05-27 06:29:20", "link": "http://arxiv.org/abs/2505.20775v1", "categories": ["cs.LG", "eess.SP", "q-bio.CB"], "primary_category": "cs.LG"}
{"title": "A Unified RCS Modeling of Typical Targets for 3GPP ISAC Channel Standardization and Experimental Analysis", "abstract": "Accurate radar cross section (RCS) modeling is crucial for characterizing\ntarget scattering and improving the precision of Integrated Sensing and\nCommunication (ISAC) channel modeling. Existing RCS models are typically\ndesigned for specific target types, leading to increased complexity and lack of\ngeneralization. This makes it difficult to standardize RCS models for 3GPP ISAC\nchannels, which need to account for multiple typical target types\nsimultaneously. Furthermore, 3GPP models must support both system-level and\nlink-level simulations, requiring the integration of large-scale and\nsmall-scale scattering characteristics. To address these challenges, this paper\nproposes a unified RCS modeling framework that consolidates these two aspects.\nThe model decomposes RCS into three components: (1) a large-scale power factor\nrepresenting overall scattering strength, (2) a small-scale angular-dependent\ncomponent describing directional scattering, and (3) a random component\naccounting for variations across target instances. We validate the model\nthrough mono-static RCS measurements for UAV, human, and vehicle targets across\nfive frequency bands. The results demonstrate that the proposed model can\neffectively capture RCS variations for different target types. Finally, the\nmodel is incorporated into an ISAC channel simulation platform to assess the\nimpact of target RCS characteristics on path loss, delay spread, and angular\nspread, providing valuable insights for future ISAC system design.", "published": "2025-05-27 03:46:20", "link": "http://arxiv.org/abs/2505.20673v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
