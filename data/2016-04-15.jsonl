{"title": "Composition of Deep and Spiking Neural Networks for Very Low Bit Rate\n  Speech Coding", "abstract": "Most current very low bit rate (VLBR) speech coding systems use hidden Markov\nmodel (HMM) based speech recognition/synthesis techniques. This allows\ntransmission of information (such as phonemes) segment by segment that\ndecreases the bit rate. However, the encoder based on a phoneme speech\nrecognition may create bursts of segmental errors. Segmental errors are further\npropagated to optional suprasegmental (such as syllable) information coding.\nTogether with the errors of voicing detection in pitch parametrization,\nHMM-based speech coding creates speech discontinuities and unnatural speech\nsound artefacts.\n  In this paper, we propose a novel VLBR speech coding framework based on\nneural networks (NNs) for end-to-end speech analysis and synthesis without\nHMMs. The speech coding framework relies on phonological (sub-phonetic)\nrepresentation of speech, and it is designed as a composition of deep and\nspiking NNs: a bank of phonological analysers at the transmitter, and a\nphonological synthesizer at the receiver, both realised as deep NNs, and a\nspiking NN as an incremental and robust encoder of syllable boundaries for\ncoding of continuous fundamental frequency (F0). A combination of phonological\nfeatures defines much more sound patterns than phonetic features defined by\nHMM-based speech coders, and the finer analysis/synthesis code contributes into\nsmoother encoded speech. Listeners significantly prefer the NN-based approach\ndue to fewer discontinuities and speech artefacts of the encoded speech. A\nsingle forward pass is required during the speech encoding and decoding. The\nproposed VLBR speech coding operates at a bit rate of approximately 360 bits/s.", "published": "2016-04-15 07:35:00", "link": "http://arxiv.org/abs/1604.04383v3", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "StalemateBreaker: A Proactive Content-Introducing Approach to Automatic\n  Human-Computer Conversation", "abstract": "Existing open-domain human-computer conversation systems are typically\npassive: they either synthesize or retrieve a reply provided a human-issued\nutterance. It is generally presumed that humans should take the role to lead\nthe conversation and introduce new content when a stalemate occurs, and that\nthe computer only needs to \"respond.\" In this paper, we propose\nStalemateBreaker, a conversation system that can proactively introduce new\ncontent when appropriate. We design a pipeline to determine when, what, and how\nto introduce new content during human-computer conversation. We further propose\na novel reranking algorithm Bi-PageRank-HITS to enable rich interaction between\nconversation context and candidate replies. Experiments show that both the\ncontent-introducing approach and the reranking algorithm are effective. Our\nfull StalemateBreaker model outperforms a state-of-the-practice conversation\nsystem by +14.4% p@1 when a stalemate occurs.", "published": "2016-04-15 05:51:29", "link": "http://arxiv.org/abs/1604.04358v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN", "abstract": "Semantic matching, which aims to determine the matching degree between two\ntexts, is a fundamental problem for many NLP applications. Recently, deep\nlearning approach has been applied to this problem and significant improvements\nhave been achieved. In this paper, we propose to view the generation of the\nglobal interaction between two texts as a recursive process: i.e. the\ninteraction of two texts at each position is a composition of the interactions\nbetween their prefixes as well as the word level interaction at the current\nposition. Based on this idea, we propose a novel deep architecture, namely\nMatch-SRNN, to model the recursive matching structure. Firstly, a tensor is\nconstructed to capture the word level interactions. Then a spatial RNN is\napplied to integrate the local interactions recursively, with importance\ndetermined by four types of gates. Finally, the matching score is calculated\nbased on the global interaction. We show that, after degenerated to the exact\nmatching scenario, Match-SRNN can approximate the dynamic programming process\nof longest common subsequence. Thus, there exists a clear interpretation for\nMatch-SRNN. Our experiments on two semantic matching tasks showed the\neffectiveness of Match-SRNN, and its ability of visualizing the learned\nmatching structure.", "published": "2016-04-15 07:23:53", "link": "http://arxiv.org/abs/1604.04378v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "A Network-based End-to-End Trainable Task-oriented Dialogue System", "abstract": "Teaching machines to accomplish tasks by conversing naturally with humans is\nchallenging. Currently, developing task-oriented dialogue systems requires\ncreating multiple components and typically this involves either a large amount\nof handcrafting, or acquiring costly labelled datasets to solve a statistical\nlearning problem for each component. In this work we introduce a neural\nnetwork-based text-in, text-out end-to-end trainable goal-oriented dialogue\nsystem along with a new way of collecting dialogue data based on a novel\npipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue\nsystems easily and without making too many assumptions about the task at hand.\nThe results show that the model can converse with human subjects naturally\nwhilst helping them to accomplish tasks in a restaurant search domain.", "published": "2016-04-15 16:40:49", "link": "http://arxiv.org/abs/1604.04562v3", "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Parallelizing Word2Vec in Shared and Distributed Memory", "abstract": "Word2Vec is a widely used algorithm for extracting low-dimensional vector\nrepresentations of words. It generated considerable excitement in the machine\nlearning and natural language processing (NLP) communities recently due to its\nexceptional performance in many NLP applications such as named entity\nrecognition, sentiment analysis, machine translation and question answering.\nState-of-the-art algorithms including those by Mikolov et al. have been\nparallelized for multi-core CPU architectures but are based on vector-vector\noperations that are memory-bandwidth intensive and do not efficiently use\ncomputational resources. In this paper, we improve reuse of various data\nstructures in the algorithm through the use of minibatching, hence allowing us\nto express the problem using matrix multiply operations. We also explore\ndifferent techniques to distribute word2vec computation across nodes in a\ncompute cluster, and demonstrate good strong scalability up to 32 nodes. In\ncombination, these techniques allow us to scale up the computation near\nlinearly across cores and nodes, and process hundreds of millions of words per\nsecond, which is the fastest word2vec implementation to the best of our\nknowledge.", "published": "2016-04-15 23:40:04", "link": "http://arxiv.org/abs/1604.04661v2", "categories": ["cs.DC", "cs.CL", "stat.ML"], "primary_category": "cs.DC"}
