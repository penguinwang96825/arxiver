{"title": "Assertion-based QA with Question-Aware Open Information Extraction", "abstract": "We present assertion based question answering (ABQA), an open domain question\nanswering task that takes a question and a passage as inputs, and outputs a\nsemi-structured assertion consisting of a subject, a predicate and a list of\narguments. An assertion conveys more evidences than a short answer span in\nreading comprehension, and it is more concise than a tedious passage in\npassage-based QA. These advantages make ABQA more suitable for human-computer\ninteraction scenarios such as voice-controlled speakers. Further progress\ntowards improving ABQA requires richer supervised dataset and powerful models\nof text understanding. To remedy this, we introduce a new dataset called\nWebAssertions, which includes hand-annotated QA labels for 358,427 assertions\nin 55,960 web passages. To address ABQA, we develop both generative and\nextractive approaches. The backbone of our generative approach is sequence to\nsequence learning. In order to capture the structure of the output assertion,\nwe introduce a hierarchical decoder that first generates the structure of the\nassertion and then generates the words of each field. The extractive approach\nis based on learning to rank. Features at different levels of granularity are\ndesigned to measure the semantic relevance between a question and an assertion.\nExperimental results show that our approaches have the ability to infer\nquestion-aware assertions from a passage. We further evaluate our approaches by\nincorporating the ABQA results as additional features in passage-based QA.\nResults on two datasets show that ABQA features significantly improve the\naccuracy on passage-based~QA.", "published": "2018-01-23 07:22:34", "link": "http://arxiv.org/abs/1801.07414v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What did you Mention? A Large Scale Mention Detection Benchmark for\n  Spoken and Written Text", "abstract": "We describe a large, high-quality benchmark for the evaluation of Mention\nDetection tools. The benchmark contains annotations of both named entities as\nwell as other types of entities, annotated on different types of text, ranging\nfrom clean text taken from Wikipedia, to noisy spoken data. The benchmark was\nbuilt through a highly controlled crowd sourcing process to ensure its quality.\nWe describe the benchmark, the process and the guidelines that were used to\nbuild it. We then demonstrate the results of a state-of-the-art system running\non that benchmark.", "published": "2018-01-23 12:22:52", "link": "http://arxiv.org/abs/1801.07507v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query Focused Abstractive Summarization: Incorporating Query Relevance,\n  Multi-Document Coverage, and Summary Length Constraints into seq2seq Models", "abstract": "Query Focused Summarization (QFS) has been addressed mostly using extractive\nmethods. Such methods, however, produce text which suffers from low coherence.\nWe investigate how abstractive methods can be applied to QFS, to overcome such\nlimitations. Recent developments in neural-attention based sequence-to-sequence\nmodels have led to state-of-the-art results on the task of abstractive generic\nsingle document summarization. Such models are trained in an end to end method\non large amounts of training data. We address three aspects to make abstractive\nsummarization applicable to QFS: (a)since there is no training data, we\nincorporate query relevance into a pre-trained abstractive model; (b) since\nexisting abstractive models are trained in a single-document setting, we design\nan iterated method to embed abstractive models within the multi-document\nrequirement of QFS; (c) the abstractive models we adapt are trained to generate\ntext of specific length (about 100 words), while we aim at generating output of\na different size (about 250 words); we design a way to adapt the target size of\nthe generated summaries to a given size ratio. We compare our method (Relevance\nSensitive Attention for QFS) to extractive baselines and with various ways to\ncombine abstractive models on the DUC QFS datasets and demonstrate solid\nimprovements on ROUGE performance.", "published": "2018-01-23 18:47:03", "link": "http://arxiv.org/abs/1801.07704v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SentiPers: A Sentiment Analysis Corpus for Persian", "abstract": "Sentiment Analysis (SA) is a major field of study in natural language\nprocessing, computational linguistics and information retrieval. Interest in SA\nhas been constantly growing in both academia and industry over the recent\nyears. Moreover, there is an increasing need for generating appropriate\nresources and datasets in particular for low resource languages including\nPersian. These datasets play an important role in designing and developing\nappropriate opinion mining platforms using supervised, semi-supervised or\nunsupervised methods. In this paper, we outline the entire process of\ndeveloping a manually annotated sentiment corpus, SentiPers, which covers\nformal and informal written contemporary Persian. To the best of our knowledge,\nSentiPers is a unique sentiment corpus with such a rich annotation in three\ndifferent levels including document-level, sentence-level, and\nentity/aspect-level for Persian. The corpus contains more than 26000 sentences\nof users opinions from digital product domain and benefits from special\ncharacteristics such as quantifying the positiveness or negativity of an\nopinion through assigning a number within a specific range to any given\nsentence. Furthermore, we present statistics on various components of our\ncorpus as well as studying the inter-annotator agreement among the annotators.\nFinally, some of the challenges that we faced during the annotation process\nwill be discussed as well.", "published": "2018-01-23 19:24:38", "link": "http://arxiv.org/abs/1801.07737v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HappyDB: A Corpus of 100,000 Crowdsourced Happy Moments", "abstract": "The science of happiness is an area of positive psychology concerned with\nunderstanding what behaviors make people happy in a sustainable fashion.\nRecently, there has been interest in developing technologies that help\nincorporate the findings of the science of happiness into users' daily lives by\nsteering them towards behaviors that increase happiness. With the goal of\nbuilding technology that can understand how people express their happy moments\nin text, we crowd-sourced HappyDB, a corpus of 100,000 happy moments that we\nmake publicly available. This paper describes HappyDB and its properties, and\noutlines several important NLP problems that can be studied with the help of\nthe corpus. We also apply several state-of-the-art analysis techniques to\nanalyze HappyDB. Our results demonstrate the need for deeper NLP techniques to\nbe developed which makes HappyDB an exciting resource for follow-on research.", "published": "2018-01-23 19:49:58", "link": "http://arxiv.org/abs/1801.07746v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Layers of Representation in Neural Machine Translation on\n  Part-of-Speech and Semantic Tagging Tasks", "abstract": "While neural machine translation (NMT) models provide improved translation\nquality in an elegant, end-to-end framework, it is less clear what they learn\nabout language. Recent work has started evaluating the quality of vector\nrepresentations learned by NMT models on morphological and syntactic tasks. In\nthis paper, we investigate the representations learned at different layers of\nNMT encoders. We train NMT systems on parallel data and use the trained models\nto extract features for training a classifier on two tasks: part-of-speech and\nsemantic tagging. We then measure the performance of the classifier as a proxy\nto the quality of the original NMT model for the given task. Our quantitative\nanalysis yields interesting insights regarding representation learning in NMT\nmodels. For instance, we find that higher layers are better at learning\nsemantics while lower layers tend to be better for part-of-speech tagging. We\nalso observe little effect of the target language on source-side\nrepresentations, especially with higher quality NMT models.", "published": "2018-01-23 20:59:55", "link": "http://arxiv.org/abs/1801.07772v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Vietnamese Open Information Extraction", "abstract": "Open information extraction (OIE) is the process to extract relations and\ntheir arguments automatically from textual documents without the need to\nrestrict the search to predefined relations. In recent years, several OIE\nsystems for the English language have been created but there is not any system\nfor the Vietnamese language. In this paper, we propose a method of OIE for\nVietnamese using a clause-based approach. Accordingly, we exploit Vietnamese\ndependency parsing using grammar clauses that strives to consider all possible\nrelations in a sentence. The corresponding clause types are identified by their\npropositions as extractable relations based on their grammatical functions of\nconstituents. As a result, our system is the first OIE system named vnOIE for\nthe Vietnamese language that can generate open relations and their arguments\nfrom Vietnamese text with highly scalable extraction while being domain\nindependent. Experimental results show that our OIE system achieves promising\nresults with a precision of 83.71%.", "published": "2018-01-23 23:03:23", "link": "http://arxiv.org/abs/1801.07804v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Enemy Among Us: Detecting Hate Speech with Threats Based 'Othering'\n  Language Embeddings", "abstract": "Offensive or antagonistic language targeted at individuals and social groups\nbased on their personal characteristics (also known as cyber hate speech or\ncyberhate) has been frequently posted and widely circulated viathe World Wide\nWeb. This can be considered as a key risk factor for individual and societal\ntension linked toregional instability. Automated Web-based cyberhate detection\nis important for observing and understandingcommunity and regional societal\ntension - especially in online social networks where posts can be rapidlyand\nwidely viewed and disseminated. While previous work has involved using\nlexicons, bags-of-words orprobabilistic language parsing approaches, they often\nsuffer from a similar issue which is that cyberhate can besubtle and indirect -\nthus depending on the occurrence of individual words or phrases can lead to a\nsignificantnumber of false negatives, providing inaccurate representation of\nthe trends in cyberhate. This problemmotivated us to challenge thinking around\nthe representation of subtle language use, such as references toperceived\nthreats from \"the other\" including immigration or job prosperity in a hateful\ncontext. We propose anovel framework that utilises language use around the\nconcept of \"othering\" and intergroup threat theory toidentify these subtleties\nand we implement a novel classification method using embedding learning to\ncomputesemantic distances between parts of speech considered to be part of an\n\"othering\" narrative. To validate ourapproach we conduct several experiments on\ndifferent types of cyberhate, namely religion, disability, race andsexual\norientation, with F-measure scores for classifying hateful instances obtained\nthrough applying ourmodel of 0.93, 0.86, 0.97 and 0.98 respectively, providing\na significant improvement in classifier accuracy overthe state-of-the-art", "published": "2018-01-23 11:43:54", "link": "http://arxiv.org/abs/1801.07495v3", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Analyzing Language Learned by an Active Question Answering Agent", "abstract": "We analyze the language learned by an agent trained with reinforcement\nlearning as a component of the ActiveQA system [Buck et al., 2017]. In\nActiveQA, question answering is framed as a reinforcement learning task in\nwhich an agent sits between the user and a black box question-answering system.\nThe agent learns to reformulate the user's questions to elicit the optimal\nanswers. It probes the system with many versions of a question that are\ngenerated via a sequence-to-sequence question reformulation model, then\naggregates the returned evidence to find the best answer. This process is an\ninstance of \\emph{machine-machine} communication. The question reformulation\nmodel must adapt its language to increase the quality of the answers returned,\nmatching the language of the question answering system. We find that the agent\ndoes not learn transformations that align with semantic intuitions but\ndiscovers through learning classical information retrieval techniques such as\ntf-idf re-weighting and stemming.", "published": "2018-01-23 13:50:11", "link": "http://arxiv.org/abs/1801.07537v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The WiLI benchmark dataset for written language identification", "abstract": "This paper describes the WiLI-2018 benchmark dataset for monolingual written\nnatural language identification. WiLI-2018 is a publicly available, free of\ncharge dataset of short text extracts from Wikipedia. It contains 1000\nparagraphs of 235 languages, totaling in 23500 paragraphs. WiLI is a\nclassification dataset: Given an unknown paragraph written in one dominant\nlanguage, it has to be decided which language it is.", "published": "2018-01-23 21:40:53", "link": "http://arxiv.org/abs/1801.07779v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
