{"title": "A Deep Reinforcement Learning Approach to Automated Stock Trading, using xLSTM Networks", "abstract": "Traditional Long Short-Term Memory (LSTM) networks are effective for handling\nsequential data but have limitations such as gradient vanishing and difficulty\nin capturing long-term dependencies, which can impact their performance in\ndynamic and risky environments like stock trading. To address these\nlimitations, this study explores the usage of the newly introduced Extended\nLong Short Term Memory (xLSTM) network in combination with a deep reinforcement\nlearning (DRL) approach for automated stock trading. Our proposed method\nutilizes xLSTM networks in both actor and critic components, enabling effective\nhandling of time series data and dynamic market environments. Proximal Policy\nOptimization (PPO), with its ability to balance exploration and exploitation,\nis employed to optimize the trading strategy. Experiments were conducted using\nfinancial data from major tech companies over a comprehensive timeline,\ndemonstrating that the xLSTM-based model outperforms LSTM-based methods in key\ntrading evaluation metrics, including cumulative return, average profitability\nper trade, maximum earning rate, maximum pullback, and Sharpe ratio. These\nfindings mark the potential of xLSTM for enhancing DRL-based stock trading\nsystems.", "published": "2025-03-12 10:56:03", "link": "http://arxiv.org/abs/2503.09655v1", "categories": ["cs.CE", "cs.LG", "q-fin.TR"], "primary_category": "cs.CE"}
