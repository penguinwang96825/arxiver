{"title": "Quantization for OpenAI's Whisper Models: A Comparative Analysis", "abstract": "Automated speech recognition (ASR) models have gained prominence for\napplications such as captioning, speech translation, and live transcription.\nThis paper studies Whisper and two model variants: one optimized for live\nspeech streaming and another for offline transcription. Notably, these models\nhave been found to generate hallucinated content, reducing transcription\nreliability. Furthermore, larger model variants exhibit increased latency and\npose challenges for deployment on resource-constrained devices. This study\nanalyzes the similarities and differences between three Whisper models,\nqualitatively examining their distinct capabilities. Next, this study\nquantifies the impact of model quantization on latency and evaluates its\nviability for edge deployment. Using the open source LibriSpeech dataset, this\npaper evaluates the word error rate (WER) along with latency analysis of\nwhispercpp using 3 quantization methods (INT4, INT5, INT8). Results show that\nquantization reduces latency by 19\\% and model size by 45\\%, while preserving\ntranscription accuracy. These findings provide insights into the optimal use\ncases of different Whisper models and edge device deployment possibilities. All\ncode, datasets, and implementation details are available in a public GitHub\nrepository: https://github.com/allisonandreyev/WhisperQuantization.git", "published": "2025-03-12 23:50:35", "link": "http://arxiv.org/abs/2503.09905v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS", "68T50, 68T10", "I.2.7; I.5.4; H.5.1"], "primary_category": "cs.SD"}
{"title": "A Rule Based Solution to Co-reference Resolution in Clinical Text", "abstract": "Objective: The aim of this study was to build an effective co-reference\nresolution system tailored for the biomedical domain. Materials and Methods:\nExperiment materials used in this study is provided by the 2011 i2b2 Natural\nLanguage Processing Challenge. The 2011 i2b2 challenge involves coreference\nresolution in medical documents. Concept mentions have been annotated in\nclinical texts, and the mentions that co-refer in each document are to be\nlinked by coreference chains. Normally, there are two ways of constructing a\nsystem to automatically discover co-referent links. One is to manually build\nrules for co-reference resolution, and the other category of approaches is to\nuse machine learning systems to learn automatically from training datasets and\nthen perform the resolution task on testing datasets. Results: Experiments show\nthe existing co-reference resolution systems are able to find some of the\nco-referent links, and our rule based system performs well finding the majority\nof the co-referent links. Our system achieved 89.6% overall performance on\nmultiple medical datasets. Conclusion: The experiment results show that\nmanually crafted rules based on observation of training data is a valid way to\naccomplish high performance in this coreference resolution task for the\ncritical biomedical domain.", "published": "2025-03-12 23:29:08", "link": "http://arxiv.org/abs/2503.09896v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", "abstract": "The scientific literature's exponential growth makes it increasingly\nchallenging to navigate and synthesize knowledge across disciplines. Large\nlanguage models (LLMs) are powerful tools for understanding scientific text,\nbut they fail to capture detailed relationships across large bodies of work.\nUnstructured approaches, like retrieval augmented generation, can sift through\nsuch corpora to recall relevant facts; however, when millions of facts\ninfluence the answer, unstructured approaches become cost prohibitive.\nStructured representations offer a natural complement -- enabling systematic\nanalysis across the whole corpus. Recent work enhances LLMs with unstructured\nor semistructured representations of scientific concepts; to complement this,\nwe try extracting structured representations using LLMs. By combining LLMs'\nsemantic understanding with a schema of scientific concepts, we prototype a\nsystem that answers precise questions about the literature as a whole. Our\nschema applies across scientific fields and we extract concepts from it using\nonly 20 manually annotated abstracts. To demonstrate the system, we extract\nconcepts from 30,000 papers on arXiv spanning astrophysics, fluid dynamics, and\nevolutionary biology. The resulting database highlights emerging trends and, by\nvisualizing the knowledge graph, offers new ways to explore the ever-growing\nlandscape of scientific knowledge. Demo: abby101/surveyor-0 on HF Spaces. Code:\nhttps://github.com/chiral-carbon/kg-for-science.", "published": "2025-03-12 23:24:40", "link": "http://arxiv.org/abs/2503.09894v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Who Are You Behind the Screen? Implicit MBTI and Gender Detection Using Artificial Intelligence", "abstract": "In personalized technology and psychological research, precisely detecting\ndemographic features and personality traits from digital interactions becomes\never more important. This work investigates implicit categorization, inferring\npersonality and gender variables directly from linguistic patterns in Telegram\nconversation data, while conventional personality prediction techniques mostly\ndepend on explicitly self-reported labels. We refine a Transformer-based\nlanguage model (RoBERTa) to capture complex linguistic cues indicative of\npersonality traits and gender differences using a dataset comprising 138,866\nmessages from 1,602 users annotated with MBTI types and 195,016 messages from\n2,598 users annotated with gender. Confidence levels help to greatly raise\nmodel accuracy to 86.16\\%, hence proving RoBERTa's capacity to consistently\nidentify implicit personality types from conversational text data. Our results\nhighlight the usefulness of Transformer topologies for implicit personality and\ngender classification, hence stressing their efficiency and stressing important\ntrade-offs between accuracy and coverage in realistic conversational\nenvironments. With regard to gender classification, the model obtained an\naccuracy of 74.4\\%, therefore capturing gender-specific language patterns.\nPersonality dimension analysis showed that people with introverted and\nintuitive preferences are especially more active in text-based interactions.\nThis study emphasizes practical issues in balancing accuracy and data coverage\nas Transformer-based models show their efficiency in implicit personality and\ngender prediction tasks from conversational texts.", "published": "2025-03-12 21:24:22", "link": "http://arxiv.org/abs/2503.09853v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Limitations of Vision-Language Models in Understanding Image Transforms", "abstract": "Vision Language Models (VLMs) have demonstrated significant potential in\nvarious downstream tasks, including Image/Video Generation, Visual Question\nAnswering, Multimodal Chatbots, and Video Understanding. However, these models\noften struggle with basic image transformations. This paper investigates the\nimage-level understanding of VLMs, specifically CLIP by OpenAI and SigLIP by\nGoogle. Our findings reveal that these models lack comprehension of multiple\nimage-level augmentations. To facilitate this study, we created an augmented\nversion of the Flickr8k dataset, pairing each image with a detailed description\nof the applied transformation. We further explore how this deficiency impacts\ndownstream tasks, particularly in image editing, and evaluate the performance\nof state-of-the-art Image2Image models on simple transformations.", "published": "2025-03-12 20:58:16", "link": "http://arxiv.org/abs/2503.09837v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "I.4; I.2.10; I.2.7"], "primary_category": "cs.CV"}
{"title": "Generative AI for Named Entity Recognition in Low-Resource Language Nepali", "abstract": "Generative Artificial Intelligence (GenAI), particularly Large Language\nModels (LLMs), has significantly advanced Natural Language Processing (NLP)\ntasks, such as Named Entity Recognition (NER), which involves identifying\nentities like person, location, and organization names in text. LLMs are\nespecially promising for low-resource languages due to their ability to learn\nfrom limited data. However, the performance of GenAI models for Nepali, a\nlow-resource language, has not been thoroughly evaluated. This paper\ninvestigates the application of state-of-the-art LLMs for Nepali NER,\nconducting experiments with various prompting techniques to assess their\neffectiveness. Our results provide insights into the challenges and\nopportunities of using LLMs for NER in low-resource settings and offer valuable\ncontributions to the advancement of NLP research in languages like Nepali.", "published": "2025-03-12 20:40:09", "link": "http://arxiv.org/abs/2503.09822v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Attention Reveals More Than Tokens: Training-Free Long-Context Reasoning with Attention-guided Retrieval", "abstract": "Large Language Models (LLMs) often exhibit substantially shorter effective\ncontext lengths than their claimed capacities, especially when handling complex\nreasoning tasks that require integrating information from multiple parts of a\nlong context and performing multi-step reasoning. Although Chain-of-Thought\n(CoT) prompting has shown promise in reducing task complexity, our empirical\nanalysis reveals that it does not fully resolve this limitation. Through\ncontrolled experiments, we identify poor recall of implicit facts as the\nprimary cause of failure, which significantly hampers reasoning performance.\nInterestingly, we observe that the internal attention weights from the\ngenerated CoT tokens can effectively ground implicit facts, even when these\nfacts are not explicitly recalled. Building on this insight, we propose a novel\ntraining-free algorithm, Attrieval, which leverages attention weights to\nretrieve relevant facts from the long context and incorporates them into the\nreasoning process. Additionally, we find that selecting context tokens from CoT\ntokens further improves performance. Our results demonstrate that Attrieval\nenhances long-context reasoning capability notably on both synthetic and\nreal-world QA datasets with various models.", "published": "2025-03-12 20:34:14", "link": "http://arxiv.org/abs/2503.09819v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Communication-Efficient Language Model Training Scales Reliably and Robustly: Scaling Laws for DiLoCo", "abstract": "As we scale to more massive machine learning models, the frequent\nsynchronization demands inherent in data-parallel approaches create significant\nslowdowns, posing a critical challenge to further scaling. Recent work develops\nan approach (DiLoCo) that relaxes synchronization demands without compromising\nmodel quality. However, these works do not carefully analyze how DiLoCo's\nbehavior changes with model size. In this work, we study the scaling law\nbehavior of DiLoCo when training LLMs under a fixed compute budget. We focus on\nhow algorithmic factors, including number of model replicas, hyperparameters,\nand token budget affect training in ways that can be accurately predicted via\nscaling laws. We find that DiLoCo scales both predictably and robustly with\nmodel size. When well-tuned, DiLoCo scales better than data-parallel training\nwith model size, and can outperform data-parallel training even at small model\nsizes. Our results showcase a more general set of benefits of DiLoCo than\npreviously documented, including increased optimal batch sizes, improved\ndownstream generalization with scale, and improved evaluation loss for a fixed\ntoken budget.", "published": "2025-03-12 20:04:38", "link": "http://arxiv.org/abs/2503.09799v1", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Constrained Language Generation with Discrete Diffusion Models", "abstract": "Constraints are critical in text generation as LLM outputs are often\nunreliable when it comes to ensuring generated outputs adhere to user defined\ninstruction or general safety guidelines. To address this gap, we present\nConstrained Discrete Diffusion (CDD), a novel method for enforcing constraints\non natural language by integrating discrete diffusion models with\ndifferentiable optimization. Unlike conventional text generators, which often\nrely on post-hoc filtering or model retraining for controllable generation, we\npropose imposing constraints directly into the discrete diffusion sampling\nprocess. We illustrate how this technique can be applied to satisfy a variety\nof natural language constraints, including (i) toxicity mitigation by\npreventing harmful content from emerging, (ii) character and sequence level\nlexical constraints, and (iii) novel molecule sequence generation with specific\nproperty adherence. Experimental results show that our constraint-aware\nprocedure achieves high fidelity in meeting these requirements while preserving\nfluency and semantic coherence, outperforming auto-regressive and existing\ndiscrete diffusion approaches.", "published": "2025-03-12 19:48:12", "link": "http://arxiv.org/abs/2503.09790v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Multi-Task Inferencing: Model Merging with Gromov-Wasserstein Feature Alignment", "abstract": "Automatic scoring of student responses enhances efficiency in education, but\ndeploying a separate neural network for each task increases storage demands,\nmaintenance efforts, and redundant computations. To address these challenges,\nthis paper introduces the Gromov-Wasserstein Scoring Model Merging (GW-SMM)\nmethod, which merges models based on feature distribution similarities measured\nvia the Gromov-Wasserstein distance. Our approach begins by extracting features\nfrom student responses using individual models, capturing both item-specific\ncontext and unique learned representations. The Gromov-Wasserstein distance\nthen quantifies the similarity between these feature distributions, identifying\nthe most compatible models for merging. Models exhibiting the smallest pairwise\ndistances, typically in pairs or trios, are merged by combining only the shared\nlayers preceding the classification head. This strategy results in a unified\nfeature extractor while preserving separate classification heads for\nitem-specific scoring. We validated our approach against human expert knowledge\nand a GPT-o1-based merging method. GW-SMM consistently outperformed both,\nachieving a higher micro F1 score, macro F1 score, exact match accuracy, and\nper-label accuracy. The improvements in micro F1 and per-label accuracy were\nstatistically significant compared to GPT-o1-based merging (p=0.04, p=0.01).\nAdditionally, GW-SMM reduced storage requirements by half without compromising\nmuch accuracy, demonstrating its computational efficiency alongside reliable\nscoring performance.", "published": "2025-03-12 19:20:33", "link": "http://arxiv.org/abs/2503.09774v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BiasConnect: Investigating Bias Interactions in Text-to-Image Models", "abstract": "The biases exhibited by Text-to-Image (TTI) models are often treated as if\nthey are independent, but in reality, they may be deeply interrelated.\nAddressing bias along one dimension, such as ethnicity or age, can\ninadvertently influence another dimension, like gender, either mitigating or\nexacerbating existing disparities. Understanding these interdependencies is\ncrucial for designing fairer generative models, yet measuring such effects\nquantitatively remains a challenge. In this paper, we aim to address these\nquestions by introducing BiasConnect, a novel tool designed to analyze and\nquantify bias interactions in TTI models. Our approach leverages a\ncounterfactual-based framework to generate pairwise causal graphs that reveals\nthe underlying structure of bias interactions for the given text prompt.\nAdditionally, our method provides empirical estimates that indicate how other\nbias dimensions shift toward or away from an ideal distribution when a given\nbias is modified. Our estimates have a strong correlation (+0.69) with the\ninterdependency observations post bias mitigation. We demonstrate the utility\nof BiasConnect for selecting optimal bias mitigation axes, comparing different\nTTI models on the dependencies they learn, and understanding the amplification\nof intersectional societal biases in TTI models.", "published": "2025-03-12 19:01:41", "link": "http://arxiv.org/abs/2503.09763v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Review GIDE -- Restaurant Review Gastrointestinal Illness Detection and Extraction with Large Language Models", "abstract": "Foodborne gastrointestinal (GI) illness is a common cause of ill health in\nthe UK. However, many cases do not interact with the healthcare system, posing\nsignificant challenges for traditional surveillance methods. The growth of\npublicly available online restaurant reviews and advancements in large language\nmodels (LLMs) present potential opportunities to extend disease surveillance by\nidentifying public reports of GI illness. In this study, we introduce a novel\nannotation schema, developed with experts in GI illness, applied to the Yelp\nOpen Dataset of reviews. Our annotations extend beyond binary disease\ndetection, to include detailed extraction of information on symptoms and foods.\nWe evaluate the performance of open-weight LLMs across these three tasks: GI\nillness detection, symptom extraction, and food extraction. We compare this\nperformance to RoBERTa-based classification models fine-tuned specifically for\nthese tasks. Our results show that using prompt-based approaches, LLMs achieve\nmicro-F1 scores of over 90% for all three of our tasks. Using prompting alone,\nwe achieve micro-F1 scores that exceed those of smaller fine-tuned models. We\nfurther demonstrate the robustness of LLMs in GI illness detection across three\nbias-focused experiments. Our results suggest that publicly available review\ntext and LLMs offer substantial potential for public health surveillance of GI\nillness by enabling highly effective extraction of key information. While LLMs\nappear to exhibit minimal bias in processing, the inherent limitations of\nrestaurant review data highlight the need for cautious interpretation of\nresults.", "published": "2025-03-12 18:42:43", "link": "http://arxiv.org/abs/2503.09743v1", "categories": ["cs.CL", "cs.LG", "68T50"], "primary_category": "cs.CL"}
{"title": "CALLM: Context-Aware Emotion Analysis in Cancer Survivors Using LLMs and Retrieval-Augmented Mobile Diaries", "abstract": "Cancer survivors face unique emotional challenges that impact their quality\nof life. Mobile diary entries-short text entries recording through their phone\nabout their emotional experiences-provide a promising method for tracking these\nexperiences in real time. Although emotion analysis tools show potential for\nrecognizing emotions from text, current methods lack the contextual\nunderstanding necessary to accurately interpret the brief, personal narratives\nin mobile diaries. We propose CALLM, a context-aware emotion analysis framework\nthat leverages Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG), to analyze mobile diary entries from cancer survivors to predict their\nemotional states. The framework enhances prediction accuracy beyond existing\nmethods by (1) integrating retrieved peer experiences as contextual examples\nand (2) incorporating individuals' temporal emotional trajectories from their\nmobile diary entries. We collected a large-scale dataset (N=407) of cancer\nsurvivors' mobile ecological momentary assessments (EMAs), which assessed\npositive and negative affect, desire to regulate emotions, social interaction\nquality, and availability for interventions, alongside daily mobile diary\nentries in an open response format regarding what was driving their current\nemotional experience. Results demonstrate strong performance of CALLM, with\nbalanced accuracies reaching 72.96% for positive and 73.29% for negative\naffect, and 73.72% for predicting individual's desire to regulate emotions.\nPost-hoc analysis reveals that leveraging model confidence, encouraging longer\ndiary entries, and incorporating personal ground truth, further enhance\npredictive outcomes. Our findings support the feasibility of deploying\nLLM-powered emotion analysis in chronic health populations and suggest\npromising directions for personalized interventions for cancer survivors.", "published": "2025-03-12 18:36:41", "link": "http://arxiv.org/abs/2503.10707v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem Proving", "abstract": "The most promising recent methods for AI reasoning require applying variants\nof reinforcement learning (RL) either on rolled out trajectories from the\nmodel, even for the step-wise rewards, or large quantities of human annotated\ntrajectory data. The reliance on the rolled-out trajectory renders the compute\ncost and time prohibitively high. In particular, the correctness of a reasoning\ntrajectory can typically only be judged at its completion, leading to sparse\nrewards in RL or requiring expensive synthetic data generation in expert\niteration-like methods. In this work, we focus on the Automatic Theorem Proving\n(ATP) task and propose a novel verifier-in-the-loop design, which unlike\nexisting approaches that leverage feedback on the entire reasoning trajectory,\nemploys an automated verifier to give intermediate feedback at each step of the\nreasoning process. Using Lean as the verifier, we empirically show that the\nstep-by-step local verification produces a global improvement in the model's\nreasoning accuracy and efficiency.", "published": "2025-03-12 18:20:47", "link": "http://arxiv.org/abs/2503.09730v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "primary_category": "cs.AI"}
{"title": "Have LLMs Made Active Learning Obsolete? Surveying the NLP Community", "abstract": "Supervised learning relies on annotated data, which is expensive to obtain. A\nlongstanding strategy to reduce annotation costs is active learning, an\niterative process, in which a human annotates only data instances deemed\ninformative by a model. Large language models (LLMs) have pushed the\neffectiveness of active learning, but have also improved methods such as few-\nor zero-shot learning, and text synthesis - thereby introducing potential\nalternatives. This raises the question: has active learning become obsolete? To\nanswer this fully, we must look beyond literature to practical experiences. We\nconduct an online survey in the NLP community to collect previously intangible\ninsights on the perceived relevance of data annotation, particularly focusing\non active learning, including best practices, obstacles and expected future\ndevelopments. Our findings show that annotated data remains a key factor, and\nactive learning continues to be relevant. While the majority of active learning\nusers find it effective, a comparison with a community survey from over a\ndecade ago reveals persistent challenges: setup complexity, estimation of cost\nreduction, and tooling. We publish an anonymized version of the collected\ndataset", "published": "2025-03-12 18:00:04", "link": "http://arxiv.org/abs/2503.09701v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented Generation System", "abstract": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline. This paper initially introduces a dual-metric\nevaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable\nthe direct quantification of chunking quality. Leveraging this assessment\nmethod, we highlight the inherent limitations of traditional and semantic\nchunking in handling complex contextual nuances, thereby substantiating the\nnecessity of integrating LLMs into chunking process. To address the inherent\ntrade-off between computational efficiency and chunking precision in LLM-based\napproaches, we devise the granularity-aware Mixture-of-Chunkers (MoC)\nframework, which consists of a three-stage processing mechanism. Notably, our\nobjective is to guide the chunker towards generating a structured list of\nchunking regular expressions, which are subsequently employed to extract chunks\nfrom the original text. Extensive experiments demonstrate that both our\nproposed metrics and the MoC framework effectively settle challenges of the\nchunking task, revealing the chunking kernel while enhancing the performance of\nthe RAG system.", "published": "2025-03-12 17:59:42", "link": "http://arxiv.org/abs/2503.09600v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation", "abstract": "As Large Language Models (LLMs) are widely deployed in diverse scenarios, the\nextent to which they could tacitly spread misinformation emerges as a critical\nsafety concern. Current research primarily evaluates LLMs on explicit false\nstatements, overlooking how misinformation often manifests subtly as\nunchallenged premises in real-world user interactions. We curated ECHOMIST, the\nfirst comprehensive benchmark for implicit misinformation, where the\nmisinformed assumptions are embedded in a user query to LLMs. ECHOMIST is based\non rigorous selection criteria and carefully curated data from diverse sources,\nincluding real-world human-AI conversations and social media interactions. We\nalso introduce a new evaluation metric to measure whether LLMs can recognize\nand counter false information rather than amplify users' misconceptions.\nThrough an extensive empirical study on a wide range of LLMs, including GPT-4,\nClaude, and Llama, we find that current models perform alarmingly poorly on\nthis task, often failing to detect false premises and generating misleading\nexplanations. Our findings underscore the critical need for an increased focus\non implicit misinformation in LLM safety research.", "published": "2025-03-12 17:59:18", "link": "http://arxiv.org/abs/2503.09598v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Proceedings of the ISCA/ITG Workshop on Diversity in Large Speech and Language Models", "abstract": "Machine learning techniques have conquered many different tasks in speech and\nnatural language processing, such as speech recognition, information\nextraction, text and speech generation, and human machine interaction using\nnatural language or speech (chatbots). Modern techniques typically rely on\nlarge models for representing general knowledge of one or several languages\n(Large Language Models, LLMs), or for representing speech and general audio\ncharacteristics. These models have been trained with large amounts of speech\nand language data, typically including web content. When humans interact with\nsuch technologies, the effectiveness of the interaction will be influenced by\nhow far humans make use of the same type of language the models have been\ntrained on or, in other words, if the models are able to generalize to the\nlanguage used by humans when interacting with the technology. This may lead to\nsome gradual forms of adaptation in human speech and language production, and\nusers who do not adapt may be excluded from efficient use of such technologies.\nOn top of this, as commercial model development follows market needs,\nunder-represented languages and dialects/sociolects may decrease in terms of\npriorities. Furthermore, for many lesser spoken languages the necessary data is\nnot available, which will worsen a digital divide in speech and language\ntechnology usage. The workshop sets out to discuss this problem based on\nscientific contributions from the perspective of computer science and\nlinguistics (including computational linguistics and NLP).", "published": "2025-03-12 17:58:57", "link": "http://arxiv.org/abs/2503.10298v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cost-Optimal Grouped-Query Attention for Long-Context LLMs", "abstract": "Building effective and efficient Transformer-based large language models\n(LLMs) has recently become a research focus, requiring maximizing model\nlanguage capabilities and minimizing training and deployment costs. Existing\nefforts have primarily described complex relationships among model performance,\nparameter size, and data size, as well as searched for the optimal compute\nallocation to train LLMs. However, they overlook the impacts of context length\nand attention head configuration (the number of query and key-value heads in\ngrouped-query attention) on training and inference. In this paper, we\nsystematically compare models with different parameter sizes, context lengths,\nand attention head configurations in terms of model performance, computational\ncost, and memory cost. Then, we extend the existing scaling methods, which are\nbased solely on parameter size and training compute, to guide the construction\nof cost-optimal LLMs during both training and inference. Our quantitative\nscaling studies show that, when processing sufficiently long sequences, a\nlarger model with fewer attention heads can achieve a lower loss while\nincurring lower computational and memory costs. Our findings provide valuable\ninsights for developing practical LLMs, especially in long-context processing\nscenarios. We will publicly release our code and data.", "published": "2025-03-12 17:50:42", "link": "http://arxiv.org/abs/2503.09579v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Probabilistic Reasoning with LLMs for k-anonymity Estimation", "abstract": "Probabilistic reasoning is a key aspect of both human and artificial\nintelligence that allows for handling uncertainty and ambiguity in\ndecision-making. In this paper, we introduce a novel numerical reasoning task\nunder uncertainty, focusing on estimating the k-anonymity of user-generated\ndocuments containing privacy-sensitive information. We propose BRANCH, which\nuses LLMs to factorize a joint probability distribution to estimate the\nk-value-the size of the population matching the given information-by modeling\nindividual pieces of textual information as random variables. The probability\nof each factor occurring within a population is estimated using standalone LLMs\nor retrieval-augmented generation systems, and these probabilities are combined\ninto a final k-value. Our experiments show that this method successfully\nestimates the correct k-value 67% of the time, an 11% increase compared to\nGPT-4o chain-of-thought reasoning. Additionally, we leverage LLM uncertainty to\ndevelop prediction intervals for k-anonymity, which include the correct value\nin nearly 92% of cases.", "published": "2025-03-12 17:41:25", "link": "http://arxiv.org/abs/2503.09674v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks", "abstract": "Large language models (LLMs) have shown remarkable advancements in enabling\nlanguage agents to tackle simple tasks. However, applying them for complex,\nmulti-step, long-horizon tasks remains a challenge. Recent work have found\nsuccess by separating high-level planning from low-level execution, which\nenables the model to effectively balance high-level planning objectives and\nlow-level execution details. However, generating accurate plans remains\ndifficult since LLMs are not inherently trained for this task. To address this,\nwe propose Plan-and-Act, a novel framework that incorporates explicit planning\ninto LLM-based agents and introduces a scalable method to enhance plan\ngeneration through a novel synthetic data generation method. Plan-and-Act\nconsists of a Planner model which generates structured, high-level plans to\nachieve user goals, and an Executor model that translates these plans into\nenvironment-specific actions. To train the Planner effectively, we introduce a\nsynthetic data generation method that annotates ground-truth trajectories with\nfeasible plans, augmented with diverse and extensive examples to enhance\ngeneralization. We evaluate Plan-and-Act using web navigation as a\nrepresentative long-horizon planning environment, demonstrating a state-of\nthe-art 54% success rate on the WebArena-Lite benchmark.", "published": "2025-03-12 17:40:52", "link": "http://arxiv.org/abs/2503.09572v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models", "abstract": "Recent advancements in reasoning with large language models (RLLMs), such as\nOpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in\ncomplex domains like mathematics and coding. A central factor in their success\nlies in the application of long chain-of-thought (Long CoT) characteristics,\nwhich enhance reasoning abilities and enable the solution of intricate\nproblems. However, despite these developments, a comprehensive survey on Long\nCoT is still lacking, limiting our understanding of its distinctions from\ntraditional short chain-of-thought (Short CoT) and complicating ongoing debates\non issues like \"overthinking\" and \"test-time scaling.\" This survey seeks to\nfill this gap by offering a unified perspective on Long CoT. (1) We first\ndistinguish Long CoT from Short CoT and introduce a novel taxonomy to\ncategorize current reasoning paradigms. (2) Next, we explore the key\ncharacteristics of Long CoT: deep reasoning, extensive exploration, and\nfeasible reflection, which enable models to handle more complex tasks and\nproduce more efficient, coherent outcomes compared to the shallower Short CoT.\n(3) We then investigate key phenomena such as the emergence of Long CoT with\nthese characteristics, including overthinking, and test-time scaling, offering\ninsights into how these processes manifest in practice. (4) Finally, we\nidentify significant research gaps and highlight promising future directions,\nincluding the integration of multi-modal reasoning, efficiency improvements,\nand enhanced knowledge frameworks. By providing a structured overview, this\nsurvey aims to inspire future research and further the development of logical\nreasoning in artificial intelligence.", "published": "2025-03-12 17:35:03", "link": "http://arxiv.org/abs/2503.09567v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs", "abstract": "The stability of language model pre-training and its effects on downstream\nperformance are still understudied. Prior work shows that the training process\ncan yield significantly different results in response to slight variations in\ninitial conditions, e.g., the random seed. Crucially, the research community\nstill lacks sufficient resources and tools to systematically investigate\npre-training stability, particularly for decoder-only language models. We\nintroduce the PolyPythias, a set of 45 new training runs for the Pythia model\nsuite: 9 new seeds across 5 model sizes, from 14M to 410M parameters, resulting\nin about 7k new checkpoints that we release. Using these new 45 training runs,\nin addition to the 5 already available, we study the effects of different\ninitial conditions determined by the seed -- i.e., parameters' initialisation\nand data order -- on (i) downstream performance, (ii) learned linguistic\nrepresentations, and (iii) emergence of training phases. In addition to common\nscaling behaviours, our analyses generally reveal highly consistent training\ndynamics across both model sizes and initial conditions. Further, the new seeds\nfor each model allow us to identify outlier training runs and delineate their\ncharacteristics. Our findings show the potential of using these methods to\npredict training stability.", "published": "2025-03-12 16:59:30", "link": "http://arxiv.org/abs/2503.09543v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability", "abstract": "Sparse autoencoders (SAEs) are a popular technique for interpreting language\nmodel activations, and there is extensive recent work on improving SAE\neffectiveness. However, most prior work evaluates progress using unsupervised\nproxy metrics with unclear practical relevance. We introduce SAEBench, a\ncomprehensive evaluation suite that measures SAE performance across seven\ndiverse metrics, spanning interpretability, feature disentanglement and\npractical applications like unlearning. To enable systematic comparison, we\nopen-source a suite of over 200 SAEs across eight recently proposed SAE\narchitectures and training algorithms. Our evaluation reveals that gains on\nproxy metrics do not reliably translate to better practical performance. For\ninstance, while Matryoshka SAEs slightly underperform on existing proxy\nmetrics, they substantially outperform other architectures on feature\ndisentanglement metrics; moreover, this advantage grows with SAE scale. By\nproviding a standardized framework for measuring progress in SAE development,\nSAEBench enables researchers to study scaling trends and make nuanced\ncomparisons between different SAE architectures and training methodologies. Our\ninteractive interface enables researchers to flexibly visualize relationships\nbetween metrics across hundreds of open-source SAEs at: https://saebench.xyz", "published": "2025-03-12 16:49:02", "link": "http://arxiv.org/abs/2503.09532v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SciFi-Benchmark: How Would AI-Powered Robots Behave in Science Fiction Literature?", "abstract": "Given the recent rate of progress in artificial intelligence (AI) and\nrobotics, a tantalizing question is emerging: would robots controlled by\nemerging AI systems be strongly aligned with human values? In this work, we\npropose a scalable way to probe this question by generating a benchmark\nspanning the key moments in 824 major pieces of science fiction literature\n(movies, tv, novels and scientific books) where an agent (AI or robot) made\ncritical decisions (good or bad). We use a LLM's recollection of each key\nmoment to generate questions in similar situations, the decisions made by the\nagent, and alternative decisions it could have made (good or bad). We then\nmeasure an approximation of how well models align with human values on a set of\nhuman-voted answers. We also generate rules that can be automatically improved\nvia amendment process in order to generate the first Sci-Fi inspired\nconstitutions for promoting ethical behavior in AIs and robots in the real\nworld. Our first finding is that modern LLMs paired with constitutions turn out\nto be well-aligned with human values (95.8%), contrary to unsettling decisions\ntypically made in SciFi (only 21.2% alignment). Secondly, we find that\ngenerated constitutions substantially increase alignment compared to the base\nmodel (79.4% to 95.8%), and show resilience to an adversarial prompt setting\n(23.3% to 92.3%). Additionally, we find that those constitutions are among the\ntop performers on the ASIMOV Benchmark which is derived from real-world images\nand hospital injury reports. Sci-Fi-inspired constitutions are thus highly\naligned and applicable in real-world situations. We release SciFi-Benchmark: a\nlarge-scale dataset to advance robot ethics and safety research. It comprises\n9,056 questions and 53,384 answers, in addition to a smaller human-labeled\nevaluation set. Data is available at https://scifi-benchmark.github.io", "published": "2025-03-12 16:35:51", "link": "http://arxiv.org/abs/2503.10706v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning", "abstract": "Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Prompting advanced LLMs with reasoning capabilities during inference to\nuse search engines is not optimal, since the LLM does not learn how to\noptimally interact with the search engine. This paper introduces Search-R1, an\nextension of the DeepSeek-R1 model where the LLM learns -- solely through\nreinforcement learning (RL) -- to autonomously generate (multiple) search\nqueries during step-by-step reasoning with real-time retrieval. Search-R1\noptimizes LLM rollouts with multi-turn search interactions, leveraging\nretrieved token masking for stable RL training and a simple outcome-based\nreward function. Experiments on seven question-answering datasets show that\nSearch-R1 improves performance by 26% (Qwen2.5-7B), 21% (Qwen2.5-3B), and 10%\n(LLaMA3.2-3B) over strong baselines. This paper further provides empirical\ninsights into RL optimization methods, LLM choices, and response length\ndynamics in retrieval-augmented reasoning. The code and model checkpoints are\navailable at https://github.com/PeterGriffinJin/Search-R1.", "published": "2025-03-12 16:26:39", "link": "http://arxiv.org/abs/2503.09516v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Reinforcement Learning is all You Need", "abstract": "Inspired by the success of DeepSeek R1 in reasoning via reinforcement\nlearning without human feedback, we train a 3B language model using the\nCountdown Game with pure reinforcement learning. Our model outperforms\nbaselines on four of five benchmarks, demonstrating improved generalization\nbeyond its training data. Notably, response length does not correlate with\nreasoning quality, and while \"aha moments\" emerge, they do not always yield\ncorrect answers. These findings highlight the potential of RL-only training for\nreasoning enhancement and suggest future work on refining reward structures to\nbridge emergent insights with accuracy.", "published": "2025-03-12 16:22:28", "link": "http://arxiv.org/abs/2503.09512v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TRACE: Real-Time Multimodal Common Ground Tracking in Situated Collaborative Dialogues", "abstract": "We present TRACE, a novel system for live *common ground* tracking in\nsituated collaborative tasks. With a focus on fast, real-time performance,\nTRACE tracks the speech, actions, gestures, and visual attention of\nparticipants, uses these multimodal inputs to determine the set of\ntask-relevant propositions that have been raised as the dialogue progresses,\nand tracks the group's epistemic position and beliefs toward them as the task\nunfolds. Amid increased interest in AI systems that can mediate collaborations,\nTRACE represents an important step forward for agents that can engage with\nmultiparty, multimodal discourse.", "published": "2025-03-12 16:20:31", "link": "http://arxiv.org/abs/2503.09511v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning", "abstract": "Recent research on Reasoning of Large Language Models (LLMs) has sought to\nfurther enhance their performance by integrating meta-thinking -- enabling\nmodels to monitor, evaluate, and control their reasoning processes for more\nadaptive and effective problem-solving. However, current single-agent work\nlacks a specialized design for acquiring meta-thinking, resulting in low\nefficacy. To address this challenge, we introduce Reinforced Meta-thinking\nAgents (ReMA), a novel framework that leverages Multi-Agent Reinforcement\nLearning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think\nabout thinking. ReMA decouples the reasoning process into two hierarchical\nagents: a high-level meta-thinking agent responsible for generating strategic\noversight and plans, and a low-level reasoning agent for detailed executions.\nThrough iterative reinforcement learning with aligned objectives, these agents\nexplore and learn collaboration, leading to improved generalization and\nrobustness. Experimental results demonstrate that ReMA outperforms single-agent\nRL baselines on complex reasoning tasks, including competitive-level\nmathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation\nstudies further illustrate the evolving dynamics of each distinct agent,\nproviding valuable insights into how the meta-thinking reasoning process\nenhances the reasoning capabilities of LLMs.", "published": "2025-03-12 16:05:31", "link": "http://arxiv.org/abs/2503.09501v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "MindGYM: Enhancing Vision-Language Models via Synthetic Self-Challenging Questions", "abstract": "Large vision-language models (VLMs) face challenges in achieving robust,\ntransferable reasoning abilities due to reliance on labor-intensive manual\ninstruction datasets or computationally expensive self-supervised methods. To\naddress these issues, we introduce MindGYM, a framework that enhances VLMs\nthrough synthetic self-challenging questions, consisting of three stages: (1)\nSeed Single-Hop Question Synthesis, generating cognitive questions across\ntextual (e.g., logical deduction) and multimodal contexts (e.g., diagram-based\nqueries) spanning eight semantic areas like ethical analysis; (2) Challenging\nMulti-Hop Question Synthesis, combining seed questions via diverse principles\nlike bridging, visual-textual alignment, to create multi-step problems\ndemanding deeper reasoning; and (3) Thinking-Induced Curriculum Fine-Tuning, a\nstructured pipeline that progressively trains the model from scaffolded\nreasoning to standalone inference. By leveraging the model's self-synthesis\ncapability, MindGYM achieves high data efficiency (e.g., +16% gains on\nMathVision-Mini with only 400 samples), computational efficiency (reducing both\ntraining and inference costs), and robust generalization across tasks.\nExtensive evaluations on seven benchmarks demonstrate superior performance over\nstrong baselines, with notable improvements (+15.77% win rates) in reasoning\ndepth and breadth validated via GPT-based scoring. MindGYM underscores the\nviability of self-challenging for refining VLM capabilities while minimizing\nhuman intervention and resource demands. Code and data are released to advance\nmultimodal reasoning research.", "published": "2025-03-12 16:03:03", "link": "http://arxiv.org/abs/2503.09499v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "BAMBI: Developing Baby Language Models for Italian", "abstract": "This paper presents BAMBI (BAby language Models Boostrapped for Italian), a\nseries of Baby Language Models (BabyLMs) trained on data that mimic the\nlinguistic input received by a five-year-old Italian-speaking child. The BAMBI\nmodels are tested using a benchmark specifically designed to evaluate language\nmodels, which takes into account the amount of training input the models\nreceived. The BAMBI models are compared against a large language model (LLM)\nand a multimodal language model (VLM) to study the contribution of\nextralinguistic information for language acquisition. The results of our\nevaluation align with the existing literature on English language models,\nconfirming that while reduced training data support the development of\nrelatively robust syntactic competence, they are insufficient for fostering\nsemantic understanding. However, the gap between the training resources (data\nand computation) of the BAMBI models and the LLMs is not fully reflected in\ntheir performance: despite LLMs' massive training, their performance is not\nmuch better than that of BAMBI models. This suggests that strategies beyond\nscaling training resources, such as data curation, inclusion of multimodal\ninput, and other training strategies such as curriculum learning, could play a\ncrucial role in shaping model performance.", "published": "2025-03-12 15:36:50", "link": "http://arxiv.org/abs/2503.09481v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explicit Learning and the LLM in Machine Translation", "abstract": "This study explores the capacity of large language models (LLMs) for explicit\nlearning, a process involving the assimilation of metalinguistic explanations\nto carry out language tasks. Using constructed languages generated by\ncryptographic means as controlled test environments, we designed experiments to\nassess an LLM's ability to explicitly learn and apply grammar rules. Our\nresults demonstrate that while LLMs possess a measurable capacity for explicit\nlearning, this ability diminishes as the complexity of the linguistic phenomena\nat hand increases. Supervised fine-tuning on chains of thought significantly\nenhances LLM performance but struggles to generalize to typologically novel or\nmore complex linguistic features. These findings point to the need for more\ndiverse training sets and alternative fine-tuning strategies to further improve\nexplicit learning by LLMs.", "published": "2025-03-12 14:57:08", "link": "http://arxiv.org/abs/2503.09454v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Florenz: Scaling Laws for Systematic Generalization in Vision-Language Models", "abstract": "Cross-lingual transfer enables vision-language models (VLMs) to perform\nvision tasks in various languages with training data only in one language.\nCurrent approaches rely on large pre-trained multilingual language models.\nHowever, they face the curse of multilinguality, sacrificing downstream task\nperformance for multilingual capabilities, struggling with lexical ambiguities,\nand falling behind recent advances. In this work, we study the scaling laws of\nsystematic generalization with monolingual VLMs for multilingual tasks,\nfocusing on the impact of model size and seen training samples. We propose\nFlorenz, a monolingual encoder-decoder VLM with 0.4B to 11.2B parameters\ncombining the pre-trained VLM Florence-2 and the large language model Gemma-2.\nFlorenz is trained with varying compute budgets on a synthetic dataset that\nfeatures intentionally incomplete language coverage for image captioning, thus,\ntesting generalization from the fully covered translation task. We show that\nnot only does indirectly learning unseen task-language pairs adhere to a\nscaling law, but also that with our data generation pipeline and the proposed\nFlorenz model family, image captioning abilities can emerge in a specific\nlanguage even when only data for the translation task is available. Fine-tuning\non a mix of downstream datasets yields competitive performance and demonstrates\npromising scaling trends in multimodal machine translation (Multi30K, CoMMuTE),\nlexical disambiguation (CoMMuTE), and image captioning (Multi30K, XM3600, COCO\nKarpathy).", "published": "2025-03-12 14:41:10", "link": "http://arxiv.org/abs/2503.09443v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Generating Automatic Anaphora Annotations", "abstract": "Training models that can perform well on various NLP tasks require large\namounts of data, and this becomes more apparent with nuanced tasks such as\nanaphora and conference resolution. To combat the prohibitive costs of creating\nmanual gold annotated data, this paper explores two methods to automatically\ncreate datasets with coreferential annotations; direct conversion from existing\ndatasets, and parsing using multilingual models capable of handling new and\nunseen languages. The paper details the current progress on those two fronts,\nas well as the challenges the efforts currently face, and our approach to\novercoming these challenges.", "published": "2025-03-12 14:15:57", "link": "http://arxiv.org/abs/2503.09417v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Got Compute, but No Data: Lessons From Post-training a Finnish LLM", "abstract": "As LLMs gain more popularity as chatbots and general assistants, methods have\nbeen developed to enable LLMs to follow instructions and align with human\npreferences. These methods have found success in the field, but their\neffectiveness has not been demonstrated outside of high-resource languages. In\nthis work, we discuss our experiences in post-training an LLM for\ninstruction-following for English and Finnish. We use a multilingual LLM to\ntranslate instruction and preference datasets from English to Finnish. We\nperform instruction tuning and preference optimization in English and Finnish\nand evaluate the instruction-following capabilities of the model in both\nlanguages. Our results show that with a few hundred Finnish instruction samples\nwe can obtain competitive performance in Finnish instruction-following. We also\nfound that although preference optimization in English offers some\ncross-lingual benefits, we obtain our best results by using preference data\nfrom both languages. We release our model, datasets, and recipes under open\nlicenses at https://huggingface.co/LumiOpen/Poro-34B-chat-OpenAssistant", "published": "2025-03-12 13:58:43", "link": "http://arxiv.org/abs/2503.09407v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image Reports", "abstract": "Standardization of clinical reports is crucial for improving the quality of\nhealthcare and facilitating data integration. The lack of unified standards,\nincluding format, terminology, and style, is a great challenge in clinical\nfundus diagnostic reports, which increases the difficulty for large language\nmodels (LLMs) to understand the data. To address this, we construct a bilingual\nstandard terminology, containing fundus clinical terms and commonly used\ndescriptions in clinical diagnosis. Then, we establish two models,\nRetSTA-7B-Zero and RetSTA-7B. RetSTA-7B-Zero, fine-tuned on an augmented\ndataset simulating clinical scenarios, demonstrates powerful standardization\nbehaviors. However, it encounters a challenge of limitation to cover a wider\nrange of diseases. To further enhance standardization performance, we build\nRetSTA-7B, which integrates a substantial amount of standardized data generated\nby RetSTA-7B-Zero along with corresponding English data, covering diverse\ncomplex clinical scenarios and achieving report-level standardization for the\nfirst time. Experimental results demonstrate that RetSTA-7B outperforms other\ncompared LLMs in bilingual standardization task, which validates its superior\nperformance and generalizability. The checkpoints are available at\nhttps://github.com/AB-Story/RetSTA-7B.", "published": "2025-03-12 13:00:57", "link": "http://arxiv.org/abs/2503.09358v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MOAT: Evaluating LMMs for Capability Integration and Instruction Grounding", "abstract": "Large multimodal models (LMMs) have demonstrated significant potential as\ngeneralists in vision-language (VL) tasks. However, there remains a significant\ngap between state-of-the-art LMMs and human performance when it comes to\ncomplex tasks that require a combination of fundamental VL capabilities, as\nwell as tasks involving the grounding of complex instructions. To thoroughly\ninvestigate the human-LMM gap and its underlying causes, we propose MOAT, a\ndiverse benchmark with complex real-world VL tasks that are challenging for\nLMMs. Specifically, the tasks in MOAT require LMMs to engage in generalist\nproblem solving by integrating fundamental VL capabilities such as reading\ntext, counting, understanding spatial relations, grounding textual and visual\ninstructions, etc. All these abilities fit into a taxonomy proposed by us that\ncontains 10 fundamental VL capabilities, enabling MOAT to provide a\nfine-grained view of LMMs' strengths and weaknesses. Besides, MOAT is the first\nbenchmark to explicitly evaluate LMMs' ability to ground complex text and\nvisual instructions, which is essential to many real-world applications. We\nevaluate over 20 proprietary and open source LMMs, as well as humans, on MOAT,\nand found that humans achieved 82.7% accuracy while the best performing LMM\n(OpenAI o1) achieved only 38.8%. To guide future model development, we analyze\ncommon trends in our results and discuss the underlying causes of observed\nperformance gaps between LMMs and humans, focusing on which VL capability forms\nthe bottleneck in complex tasks, whether test time scaling improves performance\non MOAT, and how tiling harms LMMs' capability to count. Code and data are\navailable at https://cambrian-yzt.github.io/MOAT.", "published": "2025-03-12 12:49:31", "link": "http://arxiv.org/abs/2503.09348v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts", "abstract": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments.", "published": "2025-03-12 12:49:02", "link": "http://arxiv.org/abs/2503.09347v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Evaluation of LLMs for Detecting Harmful Computing Terms", "abstract": "Detecting harmful and non-inclusive terminology in technical contexts is\ncritical for fostering inclusive environments in computing. This study explores\nthe impact of model architecture on harmful language detection by evaluating a\ncurated database of technical terms, each paired with specific use cases. We\ntested a range of encoder, decoder, and encoder-decoder language models,\nincluding BERT-base-uncased, RoBERTa large-mnli, Gemini Flash 1.5 and 2.0,\nGPT-4, Claude AI Sonnet 3.5, T5-large, and BART-large-mnli. Each model was\npresented with a standardized prompt to identify harmful and non-inclusive\nlanguage across 64 terms. Results reveal that decoder models, particularly\nGemini Flash 2.0 and Claude AI, excel in nuanced contextual analysis, while\nencoder models like BERT exhibit strong pattern recognition but struggle with\nclassification certainty. We discuss the implications of these findings for\nimproving automated detection tools and highlight model-specific strengths and\nlimitations in fostering inclusive communication in technical domains.", "published": "2025-03-12 12:36:45", "link": "http://arxiv.org/abs/2503.09341v1", "categories": ["cs.CL", "cs.ET"], "primary_category": "cs.CL"}
{"title": "Investigating User Perspectives on Differentially Private Text Privatization", "abstract": "Recent literature has seen a considerable uptick in $\\textit{Differentially\nPrivate Natural Language Processing}$ (DP NLP). This includes DP text\nprivatization, where potentially sensitive input texts are transformed under DP\nto achieve privatized output texts that ideally mask sensitive information\n$\\textit{and}$ maintain original semantics. Despite continued work to address\nthe open challenges in DP text privatization, there remains a scarcity of work\naddressing user perceptions of this technology, a crucial aspect which serves\nas the final barrier to practical adoption. In this work, we conduct a survey\nstudy with 721 laypersons around the globe, investigating how the factors of\n$\\textit{scenario}$, $\\textit{data sensitivity}$, $\\textit{mechanism type}$,\nand $\\textit{reason for data collection}$ impact user preferences for text\nprivatization. We learn that while all these factors play a role in influencing\nprivacy decisions, users are highly sensitive to the utility and coherence of\nthe private output texts. Our findings highlight the socio-technical factors\nthat must be considered in the study of DP NLP, opening the door to further\nuser-based investigations going forward.", "published": "2025-03-12 12:33:20", "link": "http://arxiv.org/abs/2503.09338v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "A Survey on Enhancing Causal Reasoning Ability of Large Language Models", "abstract": "Large language models (LLMs) have recently shown remarkable performance in\nlanguage tasks and beyond. However, due to their limited inherent causal\nreasoning ability, LLMs still face challenges in handling tasks that require\nrobust causal reasoning ability, such as health-care and economic analysis. As\na result, a growing body of research has focused on enhancing the causal\nreasoning ability of LLMs. Despite the booming research, there lacks a survey\nto well review the challenges, progress and future directions in this area. To\nbridge this significant gap, we systematically review literature on how to\nstrengthen LLMs' causal reasoning ability in this paper. We start from the\nintroduction of background and motivations of this topic, followed by the\nsummarisation of key challenges in this area. Thereafter, we propose a novel\ntaxonomy to systematically categorise existing methods, together with detailed\ncomparisons within and between classes of methods. Furthermore, we summarise\nexisting benchmarks and evaluation metrics for assessing LLMs' causal reasoning\nability. Finally, we outline future research directions for this emerging\nfield, offering insights and inspiration to researchers and practitioners in\nthe area.", "published": "2025-03-12 12:20:31", "link": "http://arxiv.org/abs/2503.09326v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "xVLM2Vec: Adapting LVLM-based embedding models to multilinguality using Self-Knowledge Distillation", "abstract": "In the current literature, most embedding models are based on the\nencoder-only transformer architecture to extract a dense and meaningful\nrepresentation of the given input, which can be a text, an image, and more.\nWith the recent advances in language modeling thanks to the introduction of\nLarge Language Models, the possibility of extracting embeddings from these\nlarge and extensively trained models has been explored. However, current\nstudies focus on textual embeddings in English, which is also the main language\non which these models have been trained. Furthermore, there are very few models\nthat consider multimodal and multilingual input. In light of this, we propose\nan adaptation methodology for Large Vision-Language Models trained on English\nlanguage data to improve their performance in extracting multilingual and\nmultimodal embeddings. Finally, we design and introduce a benchmark to evaluate\nthe effectiveness of multilingual and multimodal embedding models.", "published": "2025-03-12 12:04:05", "link": "http://arxiv.org/abs/2503.09313v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LLM-PS: Empowering Large Language Models for Time Series Forecasting with Temporal Patterns and Semantics", "abstract": "Time Series Forecasting (TSF) is critical in many real-world domains like\nfinancial planning and health monitoring. Recent studies have revealed that\nLarge Language Models (LLMs), with their powerful in-contextual modeling\ncapabilities, hold significant potential for TSF. However, existing LLM-based\nmethods usually perform suboptimally because they neglect the inherent\ncharacteristics of time series data. Unlike the textual data used in LLM\npre-training, the time series data is semantically sparse and comprises\ndistinctive temporal patterns. To address this problem, we propose LLM-PS to\nempower the LLM for TSF by learning the fundamental \\textit{Patterns} and\nmeaningful \\textit{Semantics} from time series data. Our LLM-PS incorporates a\nnew multi-scale convolutional neural network adept at capturing both short-term\nfluctuations and long-term trends within the time series. Meanwhile, we\nintroduce a time-to-text module for extracting valuable semantics across\ncontinuous time intervals rather than isolated time points. By integrating\nthese patterns and semantics, LLM-PS effectively models temporal dependencies,\nenabling a deep comprehension of time series and delivering accurate forecasts.\nIntensive experimental results demonstrate that LLM-PS achieves\nstate-of-the-art performance in both short- and long-term forecasting tasks, as\nwell as in few- and zero-shot settings.", "published": "2025-03-12 11:45:11", "link": "http://arxiv.org/abs/2503.09656v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Unmask It! AI-Generated Product Review Detection in Dravidian Languages", "abstract": "The rise of Generative AI has led to a surge in AI-generated reviews, often\nposing a serious threat to the credibility of online platforms. Reviews serve\nas the primary source of information about products and services. Authentic\nreviews play a vital role in consumer decision-making. The presence of\nfabricated content misleads consumers, undermines trust and facilitates\npotential fraud in digital marketplaces. This study focuses on detecting\nAI-generated product reviews in Tamil and Malayalam, two low-resource languages\nwhere research in this domain is relatively under-explored. We worked on a\nrange of approaches - from traditional machine learning methods to advanced\ntransformer-based models such as Indic-BERT, IndicSBERT, MuRIL, XLM-RoBERTa and\nMalayalamBERT. Our findings highlight the effectiveness of leveraging the\nstate-of-the-art transformers in accurately identifying AI-generated content,\ndemonstrating the potential in enhancing the detection of fake reviews in\nlow-resource language settings.", "published": "2025-03-12 11:35:04", "link": "http://arxiv.org/abs/2503.09289v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SciHorizon: Benchmarking AI-for-Science Readiness from Scientific Data to Large Language Models", "abstract": "In recent years, the rapid advancement of Artificial Intelligence (AI)\ntechnologies, particularly Large Language Models (LLMs), has revolutionized the\nparadigm of scientific discovery, establishing AI-for-Science (AI4Science) as a\ndynamic and evolving field. However, there is still a lack of an effective\nframework for the overall assessment of AI4Science, particularly from a\nholistic perspective on data quality and model capability. Therefore, in this\nstudy, we propose SciHorizon, a comprehensive assessment framework designed to\nbenchmark the readiness of AI4Science from both scientific data and LLM\nperspectives. First, we introduce a generalizable framework for assessing\nAI-ready scientific data, encompassing four key dimensions: Quality, FAIRness,\nExplainability, and Compliance which are subdivided into 15 sub-dimensions.\nDrawing on data resource papers published between 2018 and 2023 in\npeer-reviewed journals, we present recommendation lists of AI-ready datasets\nfor both Earth and Life Sciences, making a novel and original contribution to\nthe field. Concurrently, to assess the capabilities of LLMs across multiple\nscientific disciplines, we establish 16 assessment dimensions based on five\ncore indicators Knowledge, Understanding, Reasoning, Multimodality, and Values\nspanning Mathematics, Physics, Chemistry, Life Sciences, and Earth and Space\nSciences. Using the developed benchmark datasets, we have conducted a\ncomprehensive evaluation of over 20 representative open-source and closed\nsource LLMs. All the results are publicly available and can be accessed online\nat www.scihorizon.cn/en.", "published": "2025-03-12 11:34:41", "link": "http://arxiv.org/abs/2503.13503v1", "categories": ["cs.LG", "cs.CL", "cs.DL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Considering Length Diversity in Retrieval-Augmented Summarization", "abstract": "This study investigates retrieval-augmented summarization by specifically\nexamining the impact of exemplar summary lengths under length constraints, not\ncovered by previous work. We propose a Diverse Length-aware Maximal Marginal\nRelevance (DL-MMR) algorithm to better control summary lengths. This algorithm\ncombines the query relevance with diverse target lengths in retrieval-augmented\nsummarization. Unlike previous methods that necessitate exhaustive exemplar\nexemplar relevance comparisons using MMR, DL-MMR considers the exemplar target\nlength as well and avoids comparing exemplars to each other, thereby reducing\ncomputational cost and conserving memory during the construction of an exemplar\npool. Experimental results showed the effectiveness of DL-MMR, which considers\nlength diversity, compared to the original MMR algorithm. DL-MMR additionally\nshowed the effectiveness in memory saving of 781,513 times and computational\ncost reduction of 500,092 times, while maintaining the same level of\ninformativeness.", "published": "2025-03-12 10:43:33", "link": "http://arxiv.org/abs/2503.09249v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Rethinking Prompt-based Debiasing in Large Language Models", "abstract": "Investigating bias in large language models (LLMs) is crucial for developing\ntrustworthy AI. While prompt-based through prompt engineering is common, its\neffectiveness relies on the assumption that models inherently understand\nbiases. Our study systematically analyzed this assumption using the BBQ and\nStereoSet benchmarks on both open-source models as well as commercial GPT\nmodel. Experimental results indicate that prompt-based is often superficial;\nfor instance, the Llama2-7B-Chat model misclassified over 90% of unbiased\ncontent as biased, despite achieving high accuracy in identifying bias issues\non the BBQ dataset. Additionally, specific evaluation and question settings in\nbias benchmarks often lead LLMs to choose \"evasive answers\", disregarding the\ncore of the question and the relevance of the response to the context.\nMoreover, the apparent success of previous methods may stem from flawed\nevaluation metrics. Our research highlights a potential \"false prosperity\" in\nprompt-base efforts and emphasizes the need to rethink bias metrics to ensure\ntruly trustworthy AI.", "published": "2025-03-12 10:06:03", "link": "http://arxiv.org/abs/2503.09219v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual In-Context Learning", "abstract": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors.", "published": "2025-03-12 10:05:05", "link": "http://arxiv.org/abs/2503.09218v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why LLMs Cannot Think and How to Fix It", "abstract": "This paper elucidates that current state-of-the-art Large Language Models\n(LLMs) are fundamentally incapable of making decisions or developing \"thoughts\"\nwithin the feature space due to their architectural constraints. We establish a\ndefinition of \"thought\" that encompasses traditional understandings of that\nterm and adapt it for application to LLMs. We demonstrate that the\narchitectural design and language modeling training methodology of contemporary\nLLMs inherently preclude them from engaging in genuine thought processes. Our\nprimary focus is on this theoretical realization rather than practical insights\nderived from experimental data. Finally, we propose solutions to enable thought\nprocesses within the feature space and discuss the broader implications of\nthese architectural modifications.", "published": "2025-03-12 10:00:09", "link": "http://arxiv.org/abs/2503.09211v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Quality Over Quantity? LLM-Based Curation for a Data-Efficient Audio-Video Foundation Model", "abstract": "Integrating audio and visual data for training multimodal foundational models\nremains challenging. We present Audio-Video Vector Alignment (AVVA), which\naligns audiovisual (AV) scene content beyond mere temporal synchronization via\na Large Language Model (LLM)-based data curation pipeline. Specifically, AVVA\nscores and selects high-quality training clips using Whisper (speech-based\naudio foundation model) for audio and DINOv2 for video within a dual-encoder\ncontrastive learning framework. Evaluations on AudioCaps, VALOR, and VGGSound\ndemonstrate that this approach can achieve significant accuracy gains with\nsubstantially less curated data. For instance, AVVA yields a 7.6% improvement\nin top-1 accuracy for audio-to-video retrieval on VGGSound compared to\nImageBind, despite training on only 192 hours of carefully filtered data (vs.\n5800+ hours). Moreover, an ablation study highlights that trading data quantity\nfor data quality improves performance, yielding respective top-3 accuracy\nincreases of 47.8, 48.4, and 58.0 percentage points on AudioCaps, VALOR, and\nVGGSound over uncurated baselines. While these results underscore AVVA's data\nefficiency, we also discuss the overhead of LLM-driven curation and how it may\nbe scaled or approximated in larger domains. Overall, AVVA provides a viable\npath toward more robust, text-free audiovisual learning with improved retrieval\naccuracy.", "published": "2025-03-12 09:48:38", "link": "http://arxiv.org/abs/2503.09205v2", "categories": ["cs.MM", "cs.CL", "cs.IR", "cs.SD", "eess.AS", "68T, 68T45, 68T10"], "primary_category": "cs.MM"}
{"title": "Token Weighting for Long-Range Language Modeling", "abstract": "Many applications of large language models (LLMs) require long-context\nunderstanding, but models continue to struggle with such tasks. We hypothesize\nthat conventional next-token prediction training could contribute to this,\nbecause each token is assigned equal weight. Yet, intuitively, the amount of\ncontext needed to predict the next token accurately varies greatly across\ndifferent data. To reflect this, we propose various novel token-weighting\nschemes that assign different weights to each training token in the loss,\nthereby generalizing existing works. For this, we categorize token-weighting\nmethods using a two-step framework which compares the confidences of a\nlong-context and short-context model to score tokens. We evaluate all methods\non multiple long-context understanding tasks and show that non-uniform loss\nweights are helpful to improve the long-context abilities of LLMs. Different\nshort-context models can be used effectively for token scoring, including\nmodels that are much smaller than the long-context model that is trained. All\nin all, this work contributes to a better understanding of the trade-offs\nlong-context language modeling faces and provides guidelines for model steering\nvia loss-weighting based on empirical evidence. The code can be found on\nGithub.", "published": "2025-03-12 09:46:59", "link": "http://arxiv.org/abs/2503.09202v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward a method for LLM-enabled Indoor Navigation", "abstract": "Indoor navigation presents unique challenges due to complex layouts, lack of\nGPS signals, and accessibility concerns. Existing solutions often struggle with\nreal-time adaptability and user-specific needs. In this work, we explore the\npotential of a Large Language Model (LLM), i.e., ChatGPT, to generate natural,\ncontext-aware navigation instructions from indoor map images. We design and\nevaluate test cases across different real-world environments, analyzing the\neffectiveness of LLMs in interpreting spatial layouts, handling user\nconstraints, and planning efficient routes. Our findings demonstrate the\npotential of LLMs for supporting personalized indoor navigation, with an\naverage of 50.54% correct indications and a maximum of 77.78%. The results do\nnot appear to depend on the complexity of the layout or the complexity of the\nexpected path, but rather on the number of points of interest and the abundance\nof visual information, which negatively affect the performance.", "published": "2025-03-12 09:32:43", "link": "http://arxiv.org/abs/2503.11702v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Harmonizing Large Language Models with Collaborative Behavioral Signals for Conversational Recommendation", "abstract": "Conversational recommendation frameworks have gained prominence as a dynamic\nparadigm for delivering personalized suggestions via interactive dialogues. The\nincorporation of advanced language understanding techniques has substantially\nimproved the dialogue fluency of such systems. However, while modern language\nmodels demonstrate strong proficiency in interpreting user preferences\narticulated through natural conversation, they frequently encounter challenges\nin effectively utilizing collective behavioral patterns - a crucial element for\ngenerating relevant suggestions. To mitigate this limitation, this work\npresents a novel probabilistic framework that synergizes behavioral patterns\nwith conversational interactions through latent preference modeling. The\nproposed method establishes a dual-channel alignment mechanism where implicit\npreference representations learned from collective user interactions serve as a\nconnecting mechanism between behavioral data and linguistic expressions.\nSpecifically, the framework first derives latent preference representations\nthrough established collaborative filtering techniques, then employs these\nrepresentations to jointly refine both the linguistic preference expressions\nand behavioral patterns through an adaptive fusion process. Comprehensive\nevaluations across multiple benchmark datasets demonstrate the superior\nperformance of the proposed approach compared to various state-of-the-art\nbaseline methods, particularly in aligning conversational interactions with\ncollaborative behavioral signals.", "published": "2025-03-12 09:01:09", "link": "http://arxiv.org/abs/2503.10703v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Is LLMs Hallucination Usable? LLM-based Negative Reasoning for Fake News Detection", "abstract": "The questionable responses caused by knowledge hallucination may lead to\nLLMs' unstable ability in decision-making. However, it has never been\ninvestigated whether the LLMs' hallucination is possibly usable to generate\nnegative reasoning for facilitating the detection of fake news. This study\nproposes a novel supervised self-reinforced reasoning rectification approach -\nSR$^3$ that yields both common reasonable reasoning and wrong understandings\n(negative reasoning) for news via LLMs reflection for semantic consistency\nlearning. Upon that, we construct a negative reasoning-based news learning\nmodel called - \\emph{NRFE}, which leverages positive or negative news-reasoning\npairs for learning the semantic consistency between them. To avoid the impact\nof label-implicated reasoning, we deploy a student model - \\emph{NRFE-D} that\nonly takes news content as input to inspect the performance of our method by\ndistilling the knowledge from \\emph{NRFE}. The experimental results verified on\nthree popular fake news datasets demonstrate the superiority of our method\ncompared with three kinds of baselines including prompting on LLMs, fine-tuning\non pre-trained SLMs, and other representative fake news detection methods.", "published": "2025-03-12 08:29:59", "link": "http://arxiv.org/abs/2503.09153v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ClaimTrust: Propagation Trust Scoring for RAG Systems", "abstract": "The rapid adoption of retrieval-augmented generation (RAG) systems has\nrevolutionized large-scale content generation but has also highlighted the\nchallenge of ensuring trustworthiness in retrieved information. This paper\nintroduces ClaimTrust, a propagation-based trust scoring framework that\ndynamically evaluates the reliability of documents in a RAG system. Using a\nmodified PageRank-inspired algorithm, ClaimTrust propagates trust scores across\ndocuments based on relationships derived from extracted factual claims. We\npreprocess and analyze 814 political news articles from Kaggle's Fake News\nDetection Dataset to extract 2,173 unique claims and classify 965 meaningful\nrelationships (supporting or contradicting). By representing the dataset as a\ndocument graph, ClaimTrust iteratively updates trust scores until convergence,\neffectively differentiating trustworthy articles from unreliable ones. Our\nmethodology, which leverages embedding-based filtering for efficient claim\ncomparison and relationship classification, achieves a 11.2% of significant\nconnections while maintaining computational scalability. Experimental results\ndemonstrate that ClaimTrust successfully assigns higher trust scores to\nverified documents while penalizing those containing false information. Future\ndirections include fine-tuned claim extract and compare (Li et al., 2022),\nparameter optimization, enhanced language model utilization, and robust\nevaluation metrics to generalize the framework across diverse datasets and\ndomains.", "published": "2025-03-12 07:52:24", "link": "http://arxiv.org/abs/2503.10702v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Specification languages for computational laws versus basic legal principles", "abstract": "We speak of a \\textit{computational law} when that law is intended to be\nenforced by software through an automated decision-making process. As digital\ntechnologies evolve to offer more solutions for public administrations, we see\nan ever-increasing number of computational laws. Traditionally, law is written\nin natural language. Computational laws, however, suffer various complications\nwhen written in natural language, such as underspecification and ambiguity\nwhich lead to a diversity of possible interpretations to be made by the coder.\nThese could potentially result into an uneven application of the law. Thus,\nresorting to formal languages to write computational laws is tempting. However,\nwriting laws in a formal language leads to further complications, for example,\nincomprehensibility for non-experts, lack of explicit motivation of the\ndecisions made, or difficulties in retrieving the data leading to the outcome.\nIn this paper, we investigate how certain legal principles fare in both\nscenarios: computational law written in natural language or written in formal\nlanguage. We use a running example from the European Union's road transport\nregulation to showcase the tensions arising, and the benefits from each\nlanguage.", "published": "2025-03-12 07:39:27", "link": "http://arxiv.org/abs/2503.09129v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Global Position Aware Group Choreography using Large Language Model", "abstract": "Dance serves as a profound and universal expression of human culture,\nconveying emotions and stories through movements synchronized with music.\nAlthough some current works have achieved satisfactory results in the task of\nsingle-person dance generation, the field of multi-person dance generation\nremains relatively novel. In this work, we present a group choreography\nframework that leverages recent advancements in Large Language Models (LLM) by\nmodeling the group dance generation problem as a sequence-to-sequence\ntranslation task. Our framework consists of a tokenizer that transforms\ncontinuous features into discrete tokens, and an LLM that is fine-tuned to\npredict motion tokens given the audio tokens. We show that by proper\ntokenization of input modalities and careful design of the LLM training\nstrategies, our framework can generate realistic and diverse group dances while\nmaintaining strong music correlation and dancer-wise consistency. Extensive\nexperiments and evaluations demonstrate that our framework achieves\nstate-of-the-art performance.", "published": "2025-03-12 07:25:32", "link": "http://arxiv.org/abs/2503.09645v1", "categories": ["cs.GR", "cs.CL", "cs.CV"], "primary_category": "cs.GR"}
{"title": "GRU: Mitigating the Trade-off between Unlearning and Retention for Large Language Models", "abstract": "Large language model (LLM) unlearning has demonstrated its essential role in\nremoving privacy and copyright-related responses, crucial for their legal and\nsafe applications. However, the pursuit of complete unlearning often comes with\nsubstantial costs due to its compromises in their general functionality,\nleading to a notorious trade-off between unlearning and retention. In examining\nthe update process for unlearning dynamically, we find gradients hold essential\ninformation for revealing this trade-off. In particular, we look at the varying\nrelationship between retention performance and directional disparities between\ngradients during unlearning. It motivates the sculpting of an update mechanism\nderived from gradients from two sources, i.e., harmful for retention and useful\nfor unlearning. Accordingly, we propose Gradient Rectified Unlearning (GRU), an\nenhanced unlearning framework controlling the updating gradients in a\ngeometry-focused and optimization-driven manner such that their side impacts on\nother, unrelated responses can be minimized. Specifically, GRU derives a\nclosed-form solution to project the unlearning gradient onto the orthogonal\nspace of that gradient harmful for retention, ensuring minimal deviation from\nits original direction under the condition that overall performance is\nretained. Comprehensive experiments are conducted to demonstrate that GRU, as a\ngeneral framework, is straightforward to implement and efficiently enhances a\nrange of baseline methods through its adaptable and compatible characteristics.\nAdditionally, experimental results show its broad effectiveness across a\ndiverse set of benchmarks for LLM unlearning.", "published": "2025-03-12 07:08:54", "link": "http://arxiv.org/abs/2503.09117v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "VaxGuard: A Multi-Generator, Multi-Type, and Multi-Role Dataset for Detecting LLM-Generated Vaccine Misinformation", "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved text generation capabilities. However, they also present challenges,\nparticularly in generating vaccine-related misinformation, which poses risks to\npublic health. Despite research on human-authored misinformation, a notable gap\nremains in understanding how LLMs contribute to vaccine misinformation and how\nbest to detect it. Existing benchmarks often overlook vaccine-specific\nmisinformation and the diverse roles of misinformation spreaders. This paper\nintroduces VaxGuard, a novel dataset designed to address these challenges.\nVaxGuard includes vaccine-related misinformation generated by multiple LLMs and\nprovides a comprehensive framework for detecting misinformation across various\nroles. Our findings show that GPT-3.5 and GPT-4o consistently outperform other\nLLMs in detecting misinformation, especially when dealing with subtle or\nemotionally charged narratives. On the other hand, PHI3 and Mistral show lower\nperformance, struggling with precision and recall in fear-driven contexts.\nAdditionally, detection performance tends to decline as input text length\nincreases, indicating the need for improved methods to handle larger content.\nThese results highlight the importance of role-specific detection strategies\nand suggest that VaxGuard can serve as a key resource for improving the\ndetection of LLM-generated vaccine misinformation.", "published": "2025-03-12 06:43:25", "link": "http://arxiv.org/abs/2503.09103v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ordered Semantically Diverse Sampling for Textual Data", "abstract": "The goal of diversity sampling is to select a representative subset of data\nin a way that maximizes information contained in the subset while keeping its\ncardinality small. We introduce the ordered diverse sampling problem based on a\nnew metric that measures the diversity in an ordered list of samples. We\npresent a novel approach for generating ordered diverse samples for textual\ndata that uses principal components on the embedding vectors. The proposed\napproach is simple and compared with existing approaches using the new metric.\nWe transform standard text classification benchmarks into benchmarks for\nordered diverse sampling. Our empirical evaluation shows that prevailing\napproaches perform 6% to 61% worse than our method while also being more time\ninefficient. Ablation studies show how the parts of the new approach contribute\nto the overall metrics.", "published": "2025-03-12 06:38:57", "link": "http://arxiv.org/abs/2503.10698v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Domain Adaptation for Japanese Sentence Embeddings with Contrastive Learning based on Synthetic Sentence Generation", "abstract": "Several backbone models pre-trained on general domain datasets can encode a\nsentence into a widely useful embedding. Such sentence embeddings can be\nfurther enhanced by domain adaptation that adapts a backbone model to a\nspecific domain. However, domain adaptation for low-resource languages like\nJapanese is often difficult due to the scarcity of large-scale labeled\ndatasets. To overcome this, this paper introduces SDJC (Self-supervised Domain\nadaptation for Japanese sentence embeddings with Contrastive learning) that\nutilizes a data generator to generate sentences, which have the same syntactic\nstructure to a sentence in an unlabeled specific domain corpus but convey\ndifferent semantic meanings. Generated sentences are then used to boost\ncontrastive learning that adapts a backbone model to accurately discriminate\nsentences in the specific domain. In addition, the components of SDJC like a\nbackbone model and a method to adapt it need to be carefully selected, but no\nbenchmark dataset is available for Japanese. Thus, a comprehensive Japanese STS\n(Semantic Textual Similarity) benchmark dataset is constructed by combining\ndatasets machine-translated from English with existing datasets. The\nexperimental results validates the effectiveness of SDJC on two domain-specific\ndownstream tasks as well as the usefulness of the constructed dataset.\nDatasets, codes and backbone models adapted by SDJC are available on our github\nrepository https://github.com/ccilab-doshisha/SDJC.", "published": "2025-03-12 06:15:33", "link": "http://arxiv.org/abs/2503.09094v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HaploVL: A Single-Transformer Baseline for Multi-Modal Understanding", "abstract": "Recent advancements in large language models (LLMs) have significantly\npropelled the development of large multi-modal models (LMMs), highlighting the\npotential for general and intelligent assistants. However, most LMMs model\nvisual and textual modalities separately, leading to recent efforts to develop\nnative LMMs using a single transformer. Despite the promise, these native\nmodels are resource-intensive and often exhibit performance gaps compared to\ntheir compositional counterparts. To alleviate this issue, we propose a simple\nyet efficient method to construct a baseline for the native and end-to-end\nlarge multi-modal model in a single transformer. First, we propose a new\nearly-fusion LMM that can fuse multi-modal inputs in the early stage and\nrespond to visual instructions in an auto-regressive manner. Second, we devise\nan efficient training recipe for the proposed model, which harnesses the prior\nknowledge of the pre-trained models, addressing both the performance\nlimitations and the challenge of resource consumption. The proposed model\ndemonstrates superior performance compared to other LMMs using one transformer\nand significantly narrows the performance gap with compositional LMMs.", "published": "2025-03-12 06:01:05", "link": "http://arxiv.org/abs/2503.14694v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "LocAgent: Graph-Guided LLM Agents for Code Localization", "abstract": "Code localization--identifying precisely where in a codebase changes need to\nbe made--is a fundamental yet challenging task in software maintenance.\nExisting approaches struggle to efficiently navigate complex codebases when\nidentifying relevant code sections. The challenge lies in bridging natural\nlanguage problem descriptions with the appropriate code elements, often\nrequiring reasoning across hierarchical structures and multiple dependencies.\nWe introduce LocAgent, a framework that addresses code localization through\ngraph-based representation. By parsing codebases into directed heterogeneous\ngraphs, LocAgent creates a lightweight representation that captures code\nstructures (files, classes, functions) and their dependencies (imports,\ninvocations, inheritance), enabling LLM agents to effectively search and locate\nrelevant entities through powerful multi-hop reasoning. Experimental results on\nreal-world benchmarks demonstrate that our approach significantly enhances\naccuracy in code localization. Notably, our method with the fine-tuned\nQwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA\nproprietary models at greatly reduced cost (approximately 86% reduction),\nreaching up to 92.7% accuracy on file-level localization while improving\ndownstream GitHub issue resolution success rates by 12% for multiple attempts\n(Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.", "published": "2025-03-12 05:55:01", "link": "http://arxiv.org/abs/2503.09089v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Introducing Verification Task of Set Consistency with Set-Consistency Energy Networks", "abstract": "Examining logical inconsistencies among multiple statements (such as\ncollections of sentences or question-answer pairs) is a crucial challenge in\nmachine learning, particularly for ensuring the safety and reliability of\nmodels. Traditional methods that rely on pairwise comparisons often fail to\ncapture inconsistencies that only emerge when more than two statements are\nevaluated collectively. To address this gap, we introduce the task of\nset-consistency verification, an extension of natural language inference (NLI)\nthat assesses the logical coherence of entire sets rather than isolated pairs.\nBuilding on this task, we present the Set-Consistency Energy Network\n(SC-Energy), a novel model that employs a contrastive loss framework to learn\nthe compatibility among a collection of statements. Our approach not only\nefficiently verifies inconsistencies and pinpoints the specific statements\nresponsible for logical contradictions, but also significantly outperforms\nexisting methods including prompting-based LLM models. Furthermore, we release\ntwo new datasets: Set-LConVQA and Set-SNLI for set-consistency verification\ntask.", "published": "2025-03-12 05:11:11", "link": "http://arxiv.org/abs/2503.10695v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Medical Large Language Model Benchmarks Should Prioritize Construct Validity", "abstract": "Medical large language models (LLMs) research often makes bold claims, from\nencoding clinical knowledge to reasoning like a physician. These claims are\nusually backed by evaluation on competitive benchmarks; a tradition inherited\nfrom mainstream machine learning. But how do we separate real progress from a\nleaderboard flex? Medical LLM benchmarks, much like those in other fields, are\narbitrarily constructed using medical licensing exam questions. For these\nbenchmarks to truly measure progress, they must accurately capture the\nreal-world tasks they aim to represent. In this position paper, we argue that\nmedical LLM benchmarks should (and indeed can) be empirically evaluated for\ntheir construct validity. In the psychological testing literature, \"construct\nvalidity\" refers to the ability of a test to measure an underlying \"construct\",\nthat is the actual conceptual target of evaluation. By drawing an analogy\nbetween LLM benchmarks and psychological tests, we explain how frameworks from\nthis field can provide empirical foundations for validating benchmarks. To put\nthese ideas into practice, we use real-world clinical data in proof-of-concept\nexperiments to evaluate popular medical LLM benchmarks and report significant\ngaps in their construct validity. Finally, we outline a vision for a new\necosystem of medical LLM evaluation centered around the creation of valid\nbenchmarks.", "published": "2025-03-12 05:08:02", "link": "http://arxiv.org/abs/2503.10694v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Teaching LLMs How to Learn with Contextual Fine-Tuning", "abstract": "Prompting Large Language Models (LLMs), or providing context on the expected\nmodel of operation, is an effective way to steer the outputs of such models to\nsatisfy human desiderata after they have been trained. But in rapidly evolving\ndomains, there is often need to fine-tune LLMs to improve either the kind of\nknowledge in their memory or their abilities to perform open ended reasoning in\nnew domains. When human's learn new concepts, we often do so by linking the new\nmaterial that we are studying to concepts we have already learned before. To\nthat end, we ask, \"can prompting help us teach LLMs how to learn\". In this\nwork, we study a novel generalization of instruction tuning, called contextual\nfine-tuning, to fine-tune LLMs. Our method leverages instructional prompts\ndesigned to mimic human cognitive strategies in learning and problem-solving to\nguide the learning process during training, aiming to improve the model's\ninterpretation and understanding of domain-specific knowledge. We empirically\ndemonstrate that this simple yet effective modification improves the ability of\nLLMs to be fine-tuned rapidly on new datasets both within the medical and\nfinancial domains.", "published": "2025-03-12 03:45:53", "link": "http://arxiv.org/abs/2503.09032v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DAST: Difficulty-Aware Self-Training on Large Language Models", "abstract": "Present Large Language Models (LLM) self-training methods always under-sample\non challenging queries, leading to inadequate learning on difficult problems\nwhich limits LLMs' ability. Therefore, this work proposes a difficulty-aware\nself-training (DAST) framework that focuses on improving both the quantity and\nquality of self-generated responses on challenging queries during\nself-training. DAST is specified in three components: 1) sampling-based\ndifficulty level estimation, 2) difficulty-aware data augmentation, and 3) the\nself-training algorithm using SFT and DPO respectively. Experiments on\nmathematical tasks demonstrate the effectiveness and generalization of DAST,\nhighlighting the critical role of difficulty-aware strategies in advancing LLM\nself-training.", "published": "2025-03-12 03:36:45", "link": "http://arxiv.org/abs/2503.09029v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aligning to What? Limits to RLHF Based Alignment", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is increasingly used to\nalign large language models (LLMs) with human preferences. However, the\neffectiveness of RLHF in addressing underlying biases remains unclear. This\nstudy investigates the relationship between RLHF and both covert and overt\nbiases in LLMs, particularly focusing on biases against African Americans. We\napplied various RLHF techniques (DPO, ORPO, and RLOO) to Llama 3 8B and\nevaluated the covert and overt biases of the resulting models using\nmatched-guise probing and explicit bias testing. We performed additional tests\nwith DPO on different base models and datasets; among several implications, we\nfound that SFT before RLHF calcifies model biases. Additionally, we extend the\ntools for measuring biases to multi-modal models. Through our experiments we\ncollect evidence that indicates that current alignment techniques are\ninadequate for nebulous tasks such as mitigating covert biases, highlighting\nthe need for capable datasets, data curating techniques, or alignment tools.", "published": "2025-03-12 03:24:44", "link": "http://arxiv.org/abs/2503.09025v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word2winners at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval", "abstract": "This paper describes our system for SemEval 2025 Task 7: Previously\nFact-Checked Claim Retrieval. The task requires retrieving relevant fact-checks\nfor a given input claim from the extensive, multilingual MultiClaim dataset,\nwhich comprises social media posts and fact-checks in several languages. To\naddress this challenge, we first evaluated zero-shot performance using\nstate-of-the-art English and multilingual retrieval models and then fine-tuned\nthe most promising systems, leveraging machine translation to enhance\ncrosslingual retrieval. Our best model achieved an accuracy of 85% on\ncrosslingual data and 92% on monolingual data.", "published": "2025-03-12 02:59:41", "link": "http://arxiv.org/abs/2503.09011v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy", "abstract": "Can we simulate a sandbox society with generative agents to model human\nbehavior, thereby reducing the over-reliance on real human trials for assessing\npublic policies? In this work, we investigate the feasibility of simulating\nhealth-related decision-making, using vaccine hesitancy, defined as the delay\nin acceptance or refusal of vaccines despite the availability of vaccination\nservices (MacDonald, 2015), as a case study. To this end, we introduce the\nVacSim framework with 100 generative agents powered by Large Language Models\n(LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1)\ninstantiate a population of agents with demographics based on census data; 2)\nconnect the agents via a social network and model vaccine attitudes as a\nfunction of social dynamics and disease-related information; 3) design and\nevaluate various public health interventions aimed at mitigating vaccine\nhesitancy. To align with real-world results, we also introduce simulation\nwarmup and attitude modulation to adjust agents' attitudes. We propose a series\nof evaluations to assess the reliability of various LLM simulations.\nExperiments indicate that models like Llama and Qwen can simulate aspects of\nhuman behavior but also highlight real-world alignment challenges, such as\ninconsistent responses with demographic profiles. This early exploration of\nLLM-driven simulations is not meant to serve as definitive policy guidance;\ninstead, it serves as a call for action to examine social simulation for policy\ndevelopment.", "published": "2025-03-12 02:54:15", "link": "http://arxiv.org/abs/2503.09639v3", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.MA"}
{"title": "Leveraging Retrieval Augmented Generative LLMs For Automated Metadata Description Generation to Enhance Data Catalogs", "abstract": "Data catalogs serve as repositories for organizing and accessing diverse\ncollection of data assets, but their effectiveness hinges on the ease with\nwhich business users can look-up relevant content. Unfortunately, many data\ncatalogs within organizations suffer from limited searchability due to\ninadequate metadata like asset descriptions. Hence, there is a need of content\ngeneration solution to enrich and curate metadata in a scalable way. This paper\nexplores the challenges associated with metadata creation and proposes a unique\nprompt enrichment idea of leveraging existing metadata content using retrieval\nbased few-shot technique tied with generative large language models (LLM). The\nliterature also considers finetuning an LLM on existing content and studies the\nbehavior of few-shot pretrained LLM (Llama, GPT3.5) vis-\\`a-vis few-shot\nfinetuned LLM (Llama2-7b) by evaluating their performance based on accuracy,\nfactual grounding, and toxicity. Our preliminary results exhibit more than 80%\nRouge-1 F1 for the generated content. This implied 87%- 88% of instances\naccepted as is or curated with minor edits by data stewards. By automatically\ngenerating descriptions for tables and columns in most accurate way, the\nresearch attempts to provide an overall framework for enterprises to\neffectively scale metadata curation and enrich its data catalog thereby vastly\nimproving the data catalog searchability and overall usability.", "published": "2025-03-12 02:33:33", "link": "http://arxiv.org/abs/2503.09003v1", "categories": ["cs.IR", "cs.CL", "I.2.7"], "primary_category": "cs.IR"}
{"title": "Battling Misinformation: An Empirical Study on Adversarial Factuality in Open-Source Large Language Models", "abstract": "Adversarial factuality refers to the deliberate insertion of misinformation\ninto input prompts by an adversary, characterized by varying levels of\nexpressed confidence. In this study, we systematically evaluate the performance\nof several open-source large language models (LLMs) when exposed to such\nadversarial inputs. Three tiers of adversarial confidence are considered:\nstrongly confident, moderately confident, and limited confidence. Our analysis\nencompasses eight LLMs: LLaMA 3.1 (8B), Phi 3 (3.8B), Qwen 2.5 (7B),\nDeepseek-v2 (16B), Gemma2 (9B), Falcon (7B), Mistrallite (7B), and LLaVA (7B).\nEmpirical results indicate that LLaMA 3.1 (8B) exhibits a robust capability in\ndetecting adversarial inputs, whereas Falcon (7B) shows comparatively lower\nperformance. Notably, for the majority of the models, detection success\nimproves as the adversary's confidence decreases; however, this trend is\nreversed for LLaMA 3.1 (8B) and Phi 3 (3.8B), where a reduction in adversarial\nconfidence corresponds with diminished detection performance. Further analysis\nof the queries that elicited the highest and lowest rates of successful attacks\nreveals that adversarial attacks are more effective when targeting less\ncommonly referenced or obscure information.", "published": "2025-03-12 01:53:49", "link": "http://arxiv.org/abs/2503.10690v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing", "abstract": "Large language models (LLMs) have shown great promise as language\nunderstanding and decision making tools, and they have permeated various\naspects of our everyday life. However, their widespread availability also comes\nwith novel risks, such as generating harmful, unethical, or offensive content,\nvia an attack called jailbreaking. Despite extensive efforts from LLM\ndevelopers to align LLMs using human feedback, they are still susceptible to\njailbreak attacks. To tackle this issue, researchers often employ red-teaming\nto understand and investigate jailbreak prompts. However, existing red-teaming\napproaches lack effectiveness, scalability, or both. To address these issues,\nwe propose JBFuzz, a novel effective, automated, and scalable red-teaming\ntechnique for jailbreaking LLMs.\n  JBFuzz is inspired by the success of fuzzing for detecting\nbugs/vulnerabilities in software. We overcome three challenges related to\neffectiveness and scalability by devising novel seed prompts, a lightweight\nmutation engine, and a lightweight and accurate evaluator for guiding the\nfuzzer. Assimilating all three solutions results in a potent fuzzer that only\nrequires black-box access to the target LLM. We perform extensive experimental\nevaluation of JBFuzz using nine popular and widely-used LLMs. We find that\nJBFuzz successfully jailbreaks all LLMs for various harmful/unethical\nquestions, with an average attack success rate of 99%. We also find that JBFuzz\nis extremely efficient as it jailbreaks a given LLM for a given question in 60\nseconds on average. Our work highlights the susceptibility of the\nstate-of-the-art LLMs to jailbreak attacks even after safety alignment, and\nserves as a valuable red-teaming tool for LLM developers.", "published": "2025-03-12 01:52:17", "link": "http://arxiv.org/abs/2503.08990v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Learning to Contextualize Web Pages for Enhanced Decision Making by LLM Agents", "abstract": "Recent advances in large language models (LLMs) have led to a growing\ninterest in developing LLM-based agents for automating web tasks. However,\nthese agents often struggle with even simple tasks on real-world websites due\nto their limited capability to understand and process complex web page\nstructures. In this work, we introduce LCoW, a framework for Learning language\nmodels to Contextualize complex Web pages into a more comprehensible form,\nthereby enhancing decision making by LLM agents. LCoW decouples web page\nunderstanding from decision making by training a separate contextualization\nmodule to transform complex web pages into comprehensible format, which are\nthen utilized by the decision-making agent. We demonstrate that our\ncontextualization module effectively integrates with LLM agents of various\nscales to significantly enhance their decision-making capabilities in web\nautomation tasks. Notably, LCoW improves the success rates of closed-source\nLLMs (e.g., Gemini-1.5-flash, GPT-4o, Claude-3.5-Sonnet) by an average of\n15.6%, and demonstrates a 23.7% average improvement in success rates for\nopen-source LMs (e.g., Llama-3.1-8B, Llama-3.1-70B) on the WorkArena benchmark.\nMoreover, the Gemini-1.5-flash agent with LCoW achieves state-of-the-art\nresults on the WebShop benchmark, outperforming human experts. The relevant\ncode materials are available at our project page:\nhttps://lcowiclr2025.github.io.", "published": "2025-03-12 01:33:40", "link": "http://arxiv.org/abs/2503.10689v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "I Predict Therefore I Am: Is Next Token Prediction Enough to Learn Human-Interpretable Concepts from Data?", "abstract": "The remarkable achievements of large language models (LLMs) have led many to\nconclude that they exhibit a form of intelligence. This is as opposed to\nexplanations of their capabilities based on their ability to perform relatively\nsimple manipulations of vast volumes of data. To illuminate the distinction\nbetween these explanations, we introduce a novel generative model that\ngenerates tokens on the basis of human interpretable concepts represented as\nlatent discrete variables. Under mild conditions, even when the mapping from\nthe latent space to the observed space is non-invertible, we establish an\nidentifiability result: the representations learned by LLMs through next-token\nprediction can be approximately modeled as the logarithm of the posterior\nprobabilities of these latent discrete concepts, up to an invertible linear\ntransformation. This theoretical finding not only provides evidence that LLMs\ncapture underlying generative factors, but also strongly reinforces the linear\nrepresentation hypothesis, which posits that LLMs learn linear representations\nof human-interpretable concepts. Empirically, we validate our theoretical\nresults through evaluations on both simulation data and the Pythia, Llama, and\nDeepSeek model families.", "published": "2025-03-12 01:21:17", "link": "http://arxiv.org/abs/2503.08980v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CULEMO: Cultural Lenses on Emotion -- Benchmarking LLMs for Cross-Cultural Emotion Understanding", "abstract": "NLP research has increasingly focused on subjective tasks such as emotion\nanalysis. However, existing emotion benchmarks suffer from two major\nshortcomings: (1) they largely rely on keyword-based emotion recognition,\noverlooking crucial cultural dimensions required for deeper emotion\nunderstanding, and (2) many are created by translating English-annotated data\ninto other languages, leading to potentially unreliable evaluation. To address\nthese issues, we introduce Cultural Lenses on Emotion (CuLEmo), the first\nbenchmark designed to evaluate culture-aware emotion prediction across six\nlanguages: Amharic, Arabic, English, German, Hindi, and Spanish. CuLEmo\ncomprises 400 crafted questions per language, each requiring nuanced cultural\nreasoning and understanding. We use this benchmark to evaluate several\nstate-of-the-art LLMs on culture-aware emotion prediction and sentiment\nanalysis tasks. Our findings reveal that (1) emotion conceptualizations vary\nsignificantly across languages and cultures, (2) LLMs performance likewise\nvaries by language and cultural context, and (3) prompting in English with\nexplicit country context often outperforms in-language prompts for\nculture-aware emotion and sentiment understanding. We hope this benchmark\nguides future research toward developing more culturally aligned NLP systems.", "published": "2025-03-12 01:01:30", "link": "http://arxiv.org/abs/2503.10688v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Agentic AI for Scientific Discovery: A Survey of Progress, Challenges, and Future Directions", "abstract": "The integration of Agentic AI into scientific discovery marks a new frontier\nin research automation. These AI systems, capable of reasoning, planning, and\nautonomous decision-making, are transforming how scientists perform literature\nreview, generate hypotheses, conduct experiments, and analyze results. This\nsurvey provides a comprehensive overview of Agentic AI for scientific\ndiscovery, categorizing existing systems and tools, and highlighting recent\nprogress across fields such as chemistry, biology, and materials science. We\ndiscuss key evaluation metrics, implementation frameworks, and commonly used\ndatasets to offer a detailed understanding of the current state of the field.\nFinally, we address critical challenges, such as literature review automation,\nsystem reliability, and ethical concerns, while outlining future research\ndirections that emphasize human-AI collaboration and enhanced system\ncalibration.", "published": "2025-03-12 01:00:05", "link": "http://arxiv.org/abs/2503.08979v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis and Mitigation of Cascading Failures Using a Stochastic Interaction Graph with Eigen-analysis", "abstract": "In studies on complex network systems using graph theory, eigen-analysis is\ntypically performed on an undirected graph model of the network. However, when\nanalyzing cascading failures in a power system, the interactions among failures\nsuggest the need for a directed graph beyond the topology of the power system\nto model directions of failure propagation. To accurately quantify failure\ninteractions for effective mitigation strategies, this paper proposes a\nstochastic interaction graph model and associated eigen-analysis. Different\ntypes of modes on failure propagations are defined and characterized by the\neigenvalues of a stochastic interaction matrix, whose absolute values are\nunity, zero, or in between. Finding and interpreting these modes helps identify\nthe probable patterns of failure propagation, either local or widespread, and\nthe participating components based on eigenvectors. Then, by lowering the\nfailure probabilities of critical components highly participating in a mode of\nwidespread failures, cascading can be mitigated. The validity of the proposed\nstochastic interaction graph model, eigen-analysis and the resulting mitigation\nstrategies is demonstrated using simulated cascading failure data on an NPCC\n140-bus system.", "published": "2025-03-12 23:47:37", "link": "http://arxiv.org/abs/2503.09904v1", "categories": ["eess.SY", "cs.DM", "cs.SY", "math.DS", "math.PR", "math.SP"], "primary_category": "eess.SY"}
{"title": "Bounds on the Number of Pieces in Continuous Piecewise Affine Functions", "abstract": "The complexity of continuous piecewise affine (CPA) functions can be measured\nby the number of pieces $p$ or the number of distinct affine functions $n$. For\nCPA functions on $\\mathbb{R}^d$, this paper shows an upper bound of\n$p=O(n^{d+1})$ and constructs a family of functions achieving a lower bound of\n$p=\\Omega(n^{d+1-\\frac{c}{\\sqrt{\\log_2(n)}}})$.", "published": "2025-03-12 16:42:00", "link": "http://arxiv.org/abs/2503.09525v2", "categories": ["math.CO", "cs.CG", "cs.DM"], "primary_category": "math.CO"}
{"title": "Automatic Operator-level Parallelism Planning for Distributed Deep Learning -- A Mixed-Integer Programming Approach", "abstract": "As the artificial intelligence community advances into the era of large\nmodels with billions of parameters, distributed training and inference have\nbecome essential. While various parallelism strategies-data, model, sequence,\nand pipeline-have been successfully implemented for popular neural networks on\nmain-stream hardware, optimizing the distributed deployment schedule requires\nextensive expertise and manual effort. Further more, while existing frameworks\nwith most simple chain-like structures, they struggle with complex non-linear\narchitectures. Mixture-of-experts and multi-modal models feature intricate MIMO\nand branch-rich topologies that require fine-grained operator-level\nparallelization beyond the capabilities of existing frameworks. We propose\nformulating parallelism planning as a scheduling optimization problem using\nmixed-integer programming. We propose a bi-level solution framework balancing\noptimality with computational efficiency, automatically generating effective\ndistributed plans that capture both the heterogeneous structure of modern\nneural networks and the underlying hardware constraints. In experiments\ncomparing against expert-designed strategies like DeepSeek's DualPipe, our\nframework achieves comparable or superior performance, reducing computational\nbubbles by half under the same memory constraints. The framework's versatility\nextends beyond throughput optimization to incorporate hardware utilization\nmaximization, memory capacity constraints, and other considerations or\npotential strategies. Such capabilities position our solution as both a\nvaluable research tool for exploring optimal parallelization strategies and a\npractical industrial solution for large-scale AI deployment.", "published": "2025-03-12 13:00:29", "link": "http://arxiv.org/abs/2503.09357v1", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.DM"], "primary_category": "cs.LG"}
{"title": "Face covers and rooted minors in bounded genus graphs", "abstract": "A {\\em rooted graph} is a graph together with a designated vertex subset,\ncalled the {\\em roots}. In this paper, we consider rooted graphs embedded in a\nfixed surface. A collection of faces of the embedding is a {\\em face cover} if\nevery root is incident to some face in the collection. We prove that every\n$3$-connected, rooted graph that has no rooted $K_{2,t}$ minor and is embedded\nin a surface of Euler genus $g$, has a face cover whose size is upper-bounded\nby some function of $g$ and $t$, provided that the face-width of the embedding\nis large enough in terms of $g$. In the planar case, we prove an unconditional\n$O(t^4)$ upper bound, improving a result of B\\\"ohme and Mohar~\\cite{BM02}. The\nhigher genus case was claimed without a proof by B\\\"ohme, Kawarabayashi,\nMaharry and Mohar~\\cite{BKMM08}.", "published": "2025-03-12 10:24:53", "link": "http://arxiv.org/abs/2503.09230v1", "categories": ["math.CO", "cs.DM", "05C10, 05C83"], "primary_category": "math.CO"}
{"title": "The Zarankiewicz Problem for Polygon Visibility Graphs", "abstract": "We prove a quasi-linear upper bound on the size of $K_{t,t}$-free polygon\nvisibility graphs. For visibility graphs of star-shaped and monotone polygons\nwe show a linear bound. In the more general setting of $n$ points on a simple\nclosed curve and visibility pseudo-segments, we provide an $O(n \\log n)$ upper\nbound and an $\\Omega(n\\alpha(n))$ lower bound.", "published": "2025-03-12 07:03:59", "link": "http://arxiv.org/abs/2503.09115v1", "categories": ["cs.CG", "cs.DM", "math.CO"], "primary_category": "cs.CG"}
{"title": "Information-Energy Capacity Region for SLIPT Systems over Lognormal Fading Channels: A Theoretical and Learning-Based Analysis", "abstract": "This paper presents a comprehensive analysis of the information-energy\ncapacity region for simultaneous lightwave information and power transfer\n(SLIPT) systems over lognormal fading channels. Unlike conventional studies\nthat primarily focus on additive white Gaussian noise channels, we study the\ncomplex impact of lognormal fading, which is prevalent in optical wireless\ncommunication systems such as underwater and atmospheric channels. By applying\nthe Smith's framework for these channels, we demonstrate that the optimal input\ndistribution is discrete, characterized by a finite number of mass points. We\nfurther investigate the properties of these mass points, especially at the\ntransition points, to reveal critical insights into the rate-power trade-off\ninherent in SLIPT systems. Additionally, we introduce a novel cooperative\ninformation-energy capacity learning framework, leveraging generative\nadversarial networks, to effectively estimate and optimize the\ninformation-energy capacity region under practical constraints. Numerical\nresults validate our theoretical findings, illustrating the significant\ninfluence of channel fading on system performance. The insights and\nmethodologies presented in this work provide a solid foundation for the design\nand optimization of future SLIPT systems operating in challenging environments.", "published": "2025-03-12 20:44:56", "link": "http://arxiv.org/abs/2503.09825v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Stochastic Geometry for Modeling and Analysis of Sensing and Communications: A Survey", "abstract": "One of the most promising technologies for next-generation wireless networks\nis integrated communication and sensing (ISAC). It is considered a key enabler\nfor applications that require both enhanced communication and accurate sensing\ncapabilities. Examples of such applications include smart environments,\naugmented and virtual reality, or the internet of things, where the\ncapabilities of intelligent sensing and broadband communications are vital.\nTherefore, ISAC has attracted the research interest of both academia and\nindustry, and many investigations have been carried out over the past decade.\nThe articles in the literature include system models, performance evaluation,\nand optimization studies of several ISAC alternative designs. Stochastic\ngeometry is the study and analysis of random spatial patterns, and as such,\nstochastic geometry tools have been considered for the performance evaluation\nof wireless networks with different types of nodes. In this paper, we aim to\nprovide a comprehensive survey of current research progress in performance\nevaluation of ISAC systems using stochastic geometry tools. The survey covers\nterrestrial, aerial, and vehicular networks, where the random spatial location\nof the corresponding network elements and propagation scatterers and/or\nblockages is treated with various point processes. The paper starts with a\nshort tutorial on ISAC technology, stochastic geometry tools, and metrics used\nin performance evaluation of communication and sensing. Then, the technical\ncomponents of the system models utilized in the surveyed papers are discussed.\nSubsequently, we present the key results of the literature in all types of\nnetworks using three levels of integration: sensing-assisted communication,\ncommunication-assisted sensing, and joint sensing and communication. Finally,\nfuture research challenges and promising directions are discussed.", "published": "2025-03-12 18:20:09", "link": "http://arxiv.org/abs/2503.09729v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Minimax Optimality of the Probability Flow ODE for Diffusion Models", "abstract": "Score-based diffusion models have become a foundational paradigm for modern\ngenerative modeling, demonstrating exceptional capability in generating samples\nfrom complex high-dimensional distributions. Despite the dominant adoption of\nprobability flow ODE-based samplers in practice due to their superior sampling\nefficiency and precision, rigorous statistical guarantees for these methods\nhave remained elusive in the literature. This work develops the first\nend-to-end theoretical framework for deterministic ODE-based samplers that\nestablishes near-minimax optimal guarantees under mild assumptions on target\ndata distributions. Specifically, focusing on subgaussian distributions with\n$\\beta$-H\\\"older smooth densities for $\\beta\\leq 2$, we propose a smooth\nregularized score estimator that simultaneously controls both the $L^2$ score\nerror and the associated mean Jacobian error. Leveraging this estimator within\na refined convergence analysis of the ODE-based sampling process, we\ndemonstrate that the resulting sampler achieves the minimax rate in total\nvariation distance, modulo logarithmic factors. Notably, our theory\ncomprehensively accounts for all sources of error in the sampling process and\ndoes not require strong structural conditions such as density lower bounds or\nLipschitz/smooth scores on target distributions, thereby covering a broad range\nof practical data distributions.", "published": "2025-03-12 17:51:29", "link": "http://arxiv.org/abs/2503.09583v1", "categories": ["cs.LG", "cs.IT", "math.IT", "math.ST", "stat.ML", "stat.TH"], "primary_category": "cs.LG"}
{"title": "Optimal ISAC Beamforming Structure and Efficient Algorithms for Sum Rate and CRLB Balancing", "abstract": "Integrated sensing and communications (ISAC) has emerged as a promising\nparadigm to unify wireless communications and radar sensing, enabling efficient\nspectrum and hardware utilization. A core challenge with realizing the gains of\nISAC stems from the unique challenges of dual purpose beamforming design due to\nthe highly non-convex nature of key performance metrics such as sum rate for\ncommunications and the Cramer-Rao lower bound (CRLB) for sensing. In this\npaper, we propose a low-complexity structured approach to ISAC beamforming\noptimization to simultaneously enhance spectral efficiency and estimation\naccuracy. Specifically, we develop a successive convex approximation (SCA)\nbased algorithm which transforms the original non-convex problem into a\nsequence of convex subproblems ensuring convergence to a locally optimal\nsolution. Furthermore, leveraging the proposed SCA framework and the Lagrange\nduality, we derive the optimal beamforming structure for CRLB optimization in\nISAC systems. Our findings characterize the reduction in radar streams one can\nemploy without affecting performance. This enables a dimensionality reduction\nthat enhances computational efficiency. Numerical simulations validate that our\napproach achieves comparable or superior performance to the considered\nbenchmarks while requiring much lower computational costs.", "published": "2025-03-12 15:48:35", "link": "http://arxiv.org/abs/2503.09489v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Dynamic Feature Selection from Variable Feature Sets Using Features of Features", "abstract": "Machine learning models usually assume that a set of feature values used to\nobtain an output is fixed in advance. However, in many real-world problems, a\ncost is associated with measuring these features. To address the issue of\nreducing measurement costs, various methods have been proposed to dynamically\nselect which features to measure, but existing methods assume that the set of\nmeasurable features remains constant, which makes them unsuitable for cases\nwhere the set of measurable features varies from instance to instance. To\novercome this limitation, we define a new problem setting for Dynamic Feature\nSelection (DFS) with variable feature sets and propose a deep learning method\nthat utilizes prior information about each feature, referred to as ''features\nof features''. Experimental results on several datasets demonstrate that the\nproposed method effectively selects features based on the prior information,\neven when the set of measurable features changes from instance to instance.", "published": "2025-03-12 09:13:21", "link": "http://arxiv.org/abs/2503.09181v1", "categories": ["cs.LG", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "Deterministic and Statistical Analysis of the DoF of Continuous Linear Arrays in the Near Field", "abstract": "This paper examines the number of communication modes, that is, the degrees\nof freedom (DoF), in a wireless setup comprising a small continuous linear\nintelligent antenna array in the near field of a large one. The framework\nallows for any orientations between the arrays and any positions in a\ntwo-dimensional space assuming that the transmitting array is placed at the\norigin. Therefore, apart from the length of the two continuous arrays, four key\nparameters determine the DoF and are hence considered in the analysis: the\nCartesian coordinates of the center of the receiving array and two angles that\nmodel the rotation of each array around its center. The paper starts with the\ncalculation of the deterministic DoF for a generic geometric setting, which\nextends beyond the widely studied paraxial case. Subsequently, a stochastic\ngeometry framework is proposed to study the statistical DoF, as a first step\ntowards the investigation of the system-level performance in near field\nnetworks. Numerical results applied to millimeter wave networks reveal the\nlarge number of DoF provided by near-field communications and unveiled key\nsystem-level insights.", "published": "2025-03-12 09:01:10", "link": "http://arxiv.org/abs/2503.09174v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "New construction of Locally Perfect Nonlinear Functions with Application to Sequences Sets with Low Ambiguity Zone", "abstract": "Low Ambiguity Zone (LAZ) sequences play a pivotal role in modern integrated\nsensing and communication (ISAC) systems. Recently, Wang et al.[1] proposed a\ndefinition of locally perfect nonlinear functions (LPNFs) and constructed three\nclasses of both periodic and aperiodic LAZ sequence sets with flexible\nparameters by applying such functions and interleaving method. Some of these\nLAZ sequence sets are asymptotically optimal with respect to the\nYe-Zhou-Liu-Fan-Lei-Tang bounds undercertain conditions. In this paper, we\nproceed with the construction of LPNFs with new parameters. By using these\nLPNFs, we also present a series of LAZ sequence sets with more flexible\nparameters, addressing the limitations of existing parameter choices.\nFurthermore, our results show that one of these classes is asymptotically\noptimal in both the periodic and aperiodic cases, respectively.", "published": "2025-03-12 08:58:59", "link": "http://arxiv.org/abs/2503.09172v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Fundamental work costs of preparation and erasure in the presence of quantum side information", "abstract": "The thought experiment of Maxwell's demon highlights the effect of side\ninformation in thermodynamics. In this paper, we present an axiomatic treatment\nof a quantum Maxwell's demon, by introducing a resource-theoretic framework of\nquantum thermodynamics in the presence of quantum side information. Under\nminimal operational assumptions that capture the demon's behaviour, we derive\nthe one-shot work costs of preparing, as well as erasing, a thermodynamic\nsystem whose coupling with the demon's mind is described by a bipartite quantum\nstate. With trivial Hamiltonians, these work costs are precisely captured by\nthe smoothed conditional min- and max-entropies, respectively, thus providing\noperational interpretations for these one-shot information-theoretic quantities\nin microscopic thermodynamics. An immediate, information-theoretic implication\nof our results is an affirmative proof of the conjectured maximality of the\nconditional max-entropy among all axiomatically plausible conditional\nentropies, complementing the recently established minimality of the conditional\nmin-entropy. We then generalize our main results to the setting with nontrivial\nHamiltonians, wherein the work costs of preparation and erasure are captured by\na generalized type of mutual information. Finally, we present a macroscopic\nsecond law of thermodynamics in the presence of quantum side information, in\nterms of a conditional version of the Helmholtz free energy. Our results extend\nthe conceptual connection between thermodynamics and quantum information theory\nby refining the axiomatic common ground between the two theories and revealing\nfundamental insights of each theory in light of the other.", "published": "2025-03-12 03:02:03", "link": "http://arxiv.org/abs/2503.09012v1", "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "quant-ph"}
{"title": "Phase-mismatched STAR-RIS with FAS-assisted RSMA Users", "abstract": "This paper considers communication between a base station (BS) to two users,\neach from one side of a simultaneously transmitting-reflecting reconfigurable\nintelligent surface (STAR-RIS) in the absence of a direct link. Rate-splitting\nmultiple access (RSMA) strategy is employed and the STAR-RIS is subjected to\nphase errors. The users are equipped with a planar fluid antenna system (FAS)\nwith position reconfigurability for spatial diversity. First, we derive the\ndistribution of the equivalent channel gain at the FAS-equipped users,\ncharacterized by a t-distribution. We then obtain analytical expressions for\nthe outage probability (OP) and average capacity (AC), with the latter obtained\nvia a heuristic approach. Our findings highlight the potential of FAS to\nmitigate phase imperfections in STAR-RIS-assisted communications, significantly\nenhancing system performance compared to traditional antenna systems (TAS).\nAlso, we quantify the impact of practical phase errors on system efficiency,\nemphasizing the importance of robust strategies for next-generation wireless\nnetworks.", "published": "2025-03-12 01:40:36", "link": "http://arxiv.org/abs/2503.08986v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Channel Estimation for Rydberg Atomic Receivers", "abstract": "The rapid development of the quantum technology presents huge opportunities\nfor 6G communications. Leveraging the quantum properties of highly excited\nRydberg atoms, Rydberg atom-based antennas present distinct advantages, such as\nhigh sensitivity, broad frequency range, and compact size, over traditional\nantennas. To realize efficient precoding, accurate channel state information is\nessential. However, due to the distinct characteristics of atomic receivers,\ntraditional channel estimation algorithms developed for conventional receivers\nare no longer applicable. To this end, we propose a novel channel estimation\nalgorithm based on projection gradient descent (PGD), which is applicable to\nboth one-dimensional (1D) and twodimensional (2D) arrays. Simulation results\nare provided to show the effectiveness of our proposed channel estimation\nmethod.", "published": "2025-03-12 01:38:06", "link": "http://arxiv.org/abs/2503.08985v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Media and responsible AI governance: a game-theoretic and LLM analysis", "abstract": "This paper investigates the complex interplay between AI developers,\nregulators, users, and the media in fostering trustworthy AI systems. Using\nevolutionary game theory and large language models (LLMs), we model the\nstrategic interactions among these actors under different regulatory regimes.\nThe research explores two key mechanisms for achieving responsible governance,\nsafe AI development and adoption of safe AI: incentivising effective regulation\nthrough media reporting, and conditioning user trust on commentariats'\nrecommendation. The findings highlight the crucial role of the media in\nproviding information to users, potentially acting as a form of \"soft\"\nregulation by investigating developers or regulators, as a substitute to\ninstitutional AI regulation (which is still absent in many regions). Both\ngame-theoretic analysis and LLM-based simulations reveal conditions under which\neffective regulation and trustworthy AI development emerge, emphasising the\nimportance of considering the influence of different regulatory regimes from an\nevolutionary game-theoretic perspective. The study concludes that effective\ngovernance requires managing incentives and costs for high quality\ncommentaries.", "published": "2025-03-12 21:39:38", "link": "http://arxiv.org/abs/2503.09858v1", "categories": ["cs.AI", "cs.GT", "cs.MA", "nlin.CD"], "primary_category": "cs.AI"}
{"title": "Multi-Agent LLM Actor-Critic Framework for Social Robot Navigation", "abstract": "Recent advances in robotics and large language models (LLMs) have sparked\ngrowing interest in human-robot collaboration and embodied intelligence. To\nenable the broader deployment of robots in human-populated environments,\nsocially-aware robot navigation (SAN) has become a key research area. While\ndeep reinforcement learning approaches that integrate human-robot interaction\n(HRI) with path planning have demonstrated strong benchmark performance, they\noften struggle to adapt to new scenarios and environments. LLMs offer a\npromising avenue for zero-shot navigation through commonsense inference.\nHowever, most existing LLM-based frameworks rely on centralized\ndecision-making, lack robust verification mechanisms, and face inconsistencies\nin translating macro-actions into precise low-level control signals. To address\nthese challenges, we propose SAMALM, a decentralized multi-agent LLM\nactor-critic framework for multi-robot social navigation. In this framework, a\nset of parallel LLM actors, each reflecting distinct robot personalities or\nconfigurations, directly generate control signals. These actions undergo a\ntwo-tier verification process via a global critic that evaluates group-level\nbehaviors and individual critics that assess each robot's context. An\nentropy-based score fusion mechanism further enhances self-verification and\nre-query, improving both robustness and coordination. Experimental results\nconfirm that SAMALM effectively balances local autonomy with global oversight,\nyielding socially compliant behaviors and strong adaptability across diverse\nmulti-robot scenarios. More details and videos about this work are available\nat: https://sites.google.com/view/SAMALM.", "published": "2025-03-12 18:59:53", "link": "http://arxiv.org/abs/2503.09758v1", "categories": ["cs.RO", "cs.MA"], "primary_category": "cs.RO"}
{"title": "Optimizing QoE-Privacy Tradeoff for Proactive VR Streaming", "abstract": "Proactive virtual reality (VR) streaming requires users to upload\nviewpoint-related information, raising significant privacy concerns. Existing\nstrategies preserve privacy by introducing errors to viewpoints, which,\nhowever, compromises the quality of experience (QoE) of users. In this paper,\nwe first delve into the analysis of the viewpoint leakage probability achieved\nby existing privacy-preserving approaches. We determine the optimal\ndistribution of viewpoint errors that minimizes the viewpoint leakage\nprobability. Our analyses show that existing approaches cannot fully eliminate\nviewpoint leakage. Then, we propose a novel privacy-preserving approach that\nintroduces noise to uploaded viewpoint prediction errors, which can ensure zero\nviewpoint leakage probability. Given the proposed approach, the tradeoff\nbetween privacy preservation and QoE is optimized to minimize the QoE loss\nwhile satisfying the privacy requirement. Simulation results validate our\nanalysis results and demonstrate that the proposed approach offers a promising\nsolution for balancing privacy and QoE.", "published": "2025-03-12 14:50:06", "link": "http://arxiv.org/abs/2503.09448v1", "categories": ["cs.MM", "cs.MA"], "primary_category": "cs.MM"}
{"title": "Networked Communication for Decentralised Cooperative Agents in Mean-Field Control", "abstract": "We introduce networked communication to mean-field control (MFC) - the\ncooperative counterpart to mean-field games (MFGs) - and in particular to the\nsetting where decentralised agents learn online from a single, non-episodic run\nof the empirical system. We adapt recent algorithms for MFGs to this new\nsetting, as well as contributing a novel sub-routine allowing networked agents\nto estimate the global average reward from their local neighbourhood. We show\nthat the networked communication scheme allows agents to increase social\nwelfare faster than under both the centralised and independent architectures,\nby computing a population of potential updates in parallel and then propagating\nthe highest-performing ones through the population, via a method that can also\nbe seen as tackling the credit-assignment problem. We prove this new result\ntheoretically and provide experiments that support it across numerous games, as\nwell as exploring the empirical finding that smaller communication radii can\nbenefit convergence in a specific class of game while still outperforming\nagents learning entirely independently. We provide numerous ablation studies\nand additional experiments on numbers of communication round and robustness to\ncommunication failures.", "published": "2025-03-12 13:51:10", "link": "http://arxiv.org/abs/2503.09400v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "Task Allocation for Multi-agent Systems via Unequal-dimensional Optimal Transport", "abstract": "We consider a probabilistic model for large-scale task allocation problems\nfor multi-agent systems, aiming to determine an optimal deployment strategy\nthat minimizes the overall transport cost. Specifically, we assign\ntransportation agents to delivery tasks with given pick-up and drop-off\nlocations, pairing the spatial distribution of transport resources with the\njoint distribution of task origins and destinations. This aligns with the\noptimal mass transport framework where the problem and is in the\nunequal-dimensional setting. The task allocation problem can be thus seen as a\nlinear programming problem that minimizes a quadratic transport cost\nfunctional, optimizing the energy of all transport units. The problem is\nmotivated by time-sensitive medical deliveries using drones, such as emergency\nequipment and blood transport. In this paper, we establish the existence,\nuniqueness, and smoothness of the optimal solution, and illustrate its\nproperties through numerical simulations.", "published": "2025-03-12 13:16:00", "link": "http://arxiv.org/abs/2503.09369v1", "categories": ["eess.SY", "cs.MA", "cs.SY", "math.OC", "91B32, 93A16, 91B68, 90B06"], "primary_category": "eess.SY"}
{"title": "Steering No-Regret Agents in MFGs under Model Uncertainty", "abstract": "Incentive design is a popular framework for guiding agents' learning dynamics\ntowards desired outcomes by providing additional payments beyond intrinsic\nrewards. However, most existing works focus on a finite, small set of agents or\nassume complete knowledge of the game, limiting their applicability to\nreal-world scenarios involving large populations and model uncertainty. To\naddress this gap, we study the design of steering rewards in Mean-Field Games\n(MFGs) with density-independent transitions, where both the transition dynamics\nand intrinsic reward functions are unknown. This setting presents non-trivial\nchallenges, as the mediator must incentivize the agents to explore for its\nmodel learning under uncertainty, while simultaneously steer them to converge\nto desired behaviors without incurring excessive incentive payments. Assuming\nagents exhibit no(-adaptive) regret behaviors, we contribute novel optimistic\nexploration algorithms. Theoretically, we establish sub-linear regret\nguarantees for the cumulative gaps between the agents' behaviors and the\ndesired ones. In terms of the steering cost, we demonstrate that our total\nincentive payments incur only sub-linear excess, competing with a baseline\nsteering strategy that stabilizes the target policy as an equilibrium. Our work\npresents an effective framework for steering agents behaviors in\nlarge-population systems under uncertainty.", "published": "2025-03-12 12:02:02", "link": "http://arxiv.org/abs/2503.09309v1", "categories": ["cs.LG", "cs.AI", "cs.MA", "stat.ML"], "primary_category": "cs.LG"}
{"title": "COLA: A Scalable Multi-Agent Framework For Windows UI Task Automation", "abstract": "With the rapid advancements in Large Language Models (LLMs), an increasing\nnumber of studies have leveraged LLMs as the cognitive core of agents to\naddress complex task decision-making challenges. Specially, recent research has\ndemonstrated the potential of LLM-based agents on automating Windows GUI\noperations. However, existing methodologies exhibit two critical challenges:\n(1) static agent architectures fail to dynamically adapt to the heterogeneous\nrequirements of OS-level tasks, leading to inadequate scenario\ngeneralization;(2) the agent workflows lack fault tolerance mechanism,\nnecessitating complete process re-execution for UI agent decision error. To\naddress these limitations, we introduce \\textit{COLA}, a collaborative\nmulti-agent framework for automating Windows UI operations. In this framework,\na scenario-aware agent Task Scheduler decomposes task requirements into atomic\ncapability units, dynamically selects the optimal agent from a decision agent\npool, effectively responds to the capability requirements of diverse scenarios.\nThe decision agent pool supports plug-and-play expansion for enhanced\nflexibility. In addition, we design a memory unit equipped to all agents for\ntheir self-evolution. Furthermore, we develop an interactive backtracking\nmechanism that enables human to intervene to trigger state rollbacks for\nnon-destructive process repair. Our experimental results on the GAIA benchmark\ndemonstrates that the \\textit{COLA} framework achieves state-of-the-art\nperformance with an average score of 31.89\\%, significantly outperforming\nbaseline approaches without web API integration. Ablation studies further\nvalidate the individual contributions of our dynamic scheduling. The code is\navailable at https://github.com/Alokia/COLA-demo.", "published": "2025-03-12 11:03:27", "link": "http://arxiv.org/abs/2503.09263v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "A Survey on Trustworthy LLM Agents: Threats and Countermeasures", "abstract": "With the rapid evolution of Large Language Models (LLMs), LLM-based agents\nand Multi-agent Systems (MAS) have significantly expanded the capabilities of\nLLM ecosystems. This evolution stems from empowering LLMs with additional\nmodules such as memory, tools, environment, and even other agents. However,\nthis advancement has also introduced more complex issues of trustworthiness,\nwhich previous research focused solely on LLMs could not cover. In this survey,\nwe propose the TrustAgent framework, a comprehensive study on the\ntrustworthiness of agents, characterized by modular taxonomy, multi-dimensional\nconnotations, and technical implementation. By thoroughly investigating and\nsummarizing newly emerged attacks, defenses, and evaluation methods for agents\nand MAS, we extend the concept of Trustworthy LLM to the emerging paradigm of\nTrustworthy Agent. In TrustAgent, we begin by deconstructing and introducing\nvarious components of the Agent and MAS. Then, we categorize their\ntrustworthiness into intrinsic (brain, memory, and tool) and extrinsic (user,\nagent, and environment) aspects. Subsequently, we delineate the multifaceted\nmeanings of trustworthiness and elaborate on the implementation techniques of\nexisting research related to these internal and external modules. Finally, we\npresent our insights and outlook on this domain, aiming to provide guidance for\nfuture endeavors.", "published": "2025-03-12 08:42:05", "link": "http://arxiv.org/abs/2503.09648v1", "categories": ["cs.MA", "cs.CY"], "primary_category": "cs.MA"}
{"title": "A Deep Reinforcement Learning Approach to Automated Stock Trading, using xLSTM Networks", "abstract": "Traditional Long Short-Term Memory (LSTM) networks are effective for handling\nsequential data but have limitations such as gradient vanishing and difficulty\nin capturing long-term dependencies, which can impact their performance in\ndynamic and risky environments like stock trading. To address these\nlimitations, this study explores the usage of the newly introduced Extended\nLong Short Term Memory (xLSTM) network in combination with a deep reinforcement\nlearning (DRL) approach for automated stock trading. Our proposed method\nutilizes xLSTM networks in both actor and critic components, enabling effective\nhandling of time series data and dynamic market environments. Proximal Policy\nOptimization (PPO), with its ability to balance exploration and exploitation,\nis employed to optimize the trading strategy. Experiments were conducted using\nfinancial data from major tech companies over a comprehensive timeline,\ndemonstrating that the xLSTM-based model outperforms LSTM-based methods in key\ntrading evaluation metrics, including cumulative return, average profitability\nper trade, maximum earning rate, maximum pullback, and Sharpe ratio. These\nfindings mark the potential of xLSTM for enhancing DRL-based stock trading\nsystems.", "published": "2025-03-12 10:56:03", "link": "http://arxiv.org/abs/2503.09655v1", "categories": ["cs.CE", "cs.LG", "q-fin.TR"], "primary_category": "cs.CE"}
{"title": "ValSub: Subsampling Validation Data to Mitigate Forgetting during ASR Personalization", "abstract": "Automatic Speech Recognition (ASR) is widely used within consumer devices\nsuch as mobile phones. Recently, personalization or on-device model fine-tuning\nhas shown that adaptation of ASR models towards target user speech improves\ntheir performance over rare words or accented speech. Despite these gains,\nfine-tuning on user data (target domain) risks the personalized model to forget\nknowledge about its original training distribution (source domain) i.e.\ncatastrophic forgetting, leading to subpar general ASR performance. A simple\nand efficient approach to combat catastrophic forgetting is to measure\nforgetting via a validation set that represents the source domain distribution.\nHowever, such validation sets are large and impractical for mobile devices.\nTowards this, we propose a novel method to subsample a substantially large\nvalidation set into a smaller one while maintaining the ability to estimate\nforgetting. We demonstrate the efficacy of such a dataset in mitigating\nforgetting by utilizing it to dynamically determine the number of ideal\nfine-tuning epochs. When measuring the deviations in per user fine-tuning\nepochs against a 50x larger validation set (oracle), our method achieves a\nlower mean-absolute-error (3.39) compared to randomly selected subsets of the\nsame size (3.78-8.65). Unlike random baselines, our method consistently tracks\nthe oracle's behaviour across three different forgetting thresholds.", "published": "2025-03-12 23:53:53", "link": "http://arxiv.org/abs/2503.09906v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multiple Speaker Separation from Noisy Sources in Reverberant Rooms using Relative Transfer Matrix", "abstract": "Separation of simultaneously active multiple speakers is a difficult task in\nenvironments with strong reverberation and many background noise sources. This\npaper uses the relative transfer matrix (ReTM), a generalization of the\nrelative transfer function of a room, to propose a simple yet novel approach\nfor separating concurrent speakers using noisy multichannel microphone\nrecordings. The proposed method (i) allows multiple speech and background noise\nsources, (ii) includes reverberation, (iii) does not need the knowledge of the\nlocations of speech and noise sources nor microphone locations and their\nrelative geometry, and (iv) uses relatively small segment of recordings for\ntraining. We illustrate the speech source separation capability with improved\nintelligibility using a simulation study consisting of four speakers in the\npresence of three noise sources in a reverberant room. We also show the\napplicability of the method in a practical experiment in a real room.", "published": "2025-03-12 14:06:35", "link": "http://arxiv.org/abs/2503.09412v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Performance Modeling for Correlation-based Neural Decoding of Auditory Attention to Speech", "abstract": "Correlation-based auditory attention decoding (AAD) algorithms exploit neural\ntracking mechanisms to determine listener attention among competing speech\nsources via, e.g., electroencephalography signals. The correlation coefficients\nbetween the decoded neural responses and encoded speech stimuli of the\ndifferent speakers then serve as AAD decision variables. A critical trade-off\nexists between the temporal resolution (the decision window length used to\ncompute these correlations) and the AAD accuracy. This trade-off is typically\ncharacterized by evaluating AAD accuracy across multiple window lengths,\nleading to the performance curve. We propose a novel method to model this\ntrade-off curve using labeled correlations from only a single decision window\nlength. Our approach models the (un)attended correlations with a normal\ndistribution after applying the Fisher transformation, enabling accurate AAD\naccuracy prediction across different window lengths. We validate the method on\ntwo distinct AAD implementations: a linear decoder and the non-linear VLAAI\ndeep neural network, evaluated on separate datasets. Results show consistently\nlow modeling errors of approximately 2 percent points, with 94% of true\naccuracies falling within estimated 95%-confidence intervals. The proposed\nmethod enables efficient performance curve modeling without extensive\nmulti-window length evaluation, facilitating practical applications in, e.g.,\nperformance tracking in neuro-steered hearing devices to continuously adapt the\nsystem parameters over time.", "published": "2025-03-12 12:51:46", "link": "http://arxiv.org/abs/2503.09349v1", "categories": ["eess.SP", "cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "eess.SP"}
{"title": "Zero to 16383 Through the Wire: Transmitting High- Resolution MIDI with WebSockets and the Browser", "abstract": "This paper outlines how to leverage the Web MIDI API and web technologies to\nconvert numerical data in JavaScript to Most Significant Byte and Least\nSignificant Byte combos, stage the data as dual concurrent CC messages, use\nWebSockets to send it to multiple endpoints, and wire the browser to other\nmusic software. This method allows users to control their own native\napplication via 14-bit MIDI messaging and even applications housed on a remote\nsource. Because the technology utilizes WebSockets, it is not reliant on local\nnetworks for connectivity and opens the possibilities of remote software\ncontrol and collaboration anywhere in the world. While no shortage of options\nexists for controlling music software from the web, the Web MIDI API allows for\na more streamlined end user experience as it seamlessly links to core OS MIDI\nfunctionality. The paper will share a use case of transmitting high-resolution\nMIDI through the browser and translating it to control voltage data for use\nwith a modular synthesizer.", "published": "2025-03-12 04:43:31", "link": "http://arxiv.org/abs/2503.09055v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Control Surfaces: Using the Commodore 64 and Analog Synthesizer to Expand Musical Boundaries", "abstract": "Analog-digital hybrid electronic music systems once existed out of necessity\nin order to facilitate a flexible work environment for the creation of live\ncomputer music. As computational power increased with the development of faster\nmicroprocessors, the need for digital functionality with analog sound\nproduction decreased, with the computer becoming more capable of handling both\ntasks. Given the exclusivity of these systems and the relatively short time\nthey were in use, the possibilities of such systems were hardly explored. The\nwork of Jos\\'e Vicente Asuar best demonstrated a push for accessibility of such\nsystems, but he never received the support of any institution in order to bring\nhis machine widespread attention. Modeled after his approach, using a Commodore\n64 (or freely available OS emulator) and analog modular hardware, this paper\naims to fashion a system that is accessible, affordable, easy to use,\neducational, and musically rich in nature.", "published": "2025-03-12 04:40:35", "link": "http://arxiv.org/abs/2503.09053v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
