{"title": "Finite Element Method for HJB in Option Pricing with Stock Borrowing Fees", "abstract": "In mathematical finance, many derivatives from markets with frictions can be\nformulated as optimal control problems in the HJB framework. Analytical optimal\ncontrol can result in highly nonlinear PDEs, which might yield unstable\nnumerical results. Accurate and convergent numerical schemes are essential to\nleverage the benefits of the hedging process. In this study, we apply a finite\nelement approach with a non-uniform mesh for the task of option pricing with\nstock borrowing fees, leading to an HJB equation that bypasses analytical\noptimal control in favor of direct PDE discretization. The time integration\nemploys the theta-scheme, with initial modifications following Rannacher`s\nprocedure. A Newton-type algorithm is applied to address the penalty-like term\nat each time step. Numerical experiments are conducted, demonstrating\nconsistency with a benchmark problem and showing a strong match. The CPU time\nneeded to reach the desired results favors P2-FEM over FDM and linear P1-FEM,\nwith P2-FEM displaying superior convergence. This paper presents an efficient\nalternative framework for the HJB problem and contributes to the literature by\nintroducing a finite element method (FEM)-based solution for HJB applications\nin mathematical finance.", "published": "2025-01-04 16:17:34", "link": "http://arxiv.org/abs/2501.02327v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "On the entropy minimal martingale measure in the exponential Ornstein-Uhlenbeck stochastic volatility model", "abstract": "We consider a stochastic volatility model where the price evolution depend on\nthe exponential of the Ornstein--Uhlenbeck process. After a brief revision of\nthe related theory the entropy-minimal equivalent martingale measure. is\ncalculated.", "published": "2025-01-04 22:37:16", "link": "http://arxiv.org/abs/2501.02396v1", "categories": ["math.PR", "q-fin.MF", "60G44"], "primary_category": "math.PR"}
{"title": "Evaluating the resilience of ESG investments in European Markets during turmoil periods", "abstract": "This study investigates the resilience of Environmental, Social, and\nGovernance (ESG) investments during periods of financial instability, comparing\nthem with traditional equity indices across major European markets-Germany,\nFrance, and Italy. Using daily returns from October 2021 to February 2024, the\nanalysis explores the effects of key global disruptions such as the Covid-19\npandemic and the Russia-Ukraine conflict on market performance. A mixture of\ntwo generalised normal distributions (MGND) and EGARCH-in-mean models are used\nto identify periods of market turmoil and assess volatility dynamics. The\nfindings indicate that during crises, ESG investments present higher volatility\nin Germany and Italy than in France. Despite some regional variations, ESG\nportfolios demonstrate greater resilience compared to traditional ones,\noffering potential risk mitigation during market shocks. These results\nunderscore the importance of integrating ESG factors into long-term investment\nstrategies, particularly in the face of unpredictable financial turmoil.", "published": "2025-01-04 12:19:51", "link": "http://arxiv.org/abs/2501.03269v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "Personalized Graph-Based Retrieval for Large Language Models", "abstract": "As large language models (LLMs) evolve, their ability to deliver personalized\nand context-aware responses offers transformative potential for improving user\nexperiences. Existing personalization approaches, however, often rely solely on\nuser history to augment the prompt, limiting their effectiveness in generating\ntailored outputs, especially in cold-start scenarios with sparse data. To\naddress these limitations, we propose Personalized Graph-based\nRetrieval-Augmented Generation (PGraphRAG), a framework that leverages\nuser-centric knowledge graphs to enrich personalization. By directly\nintegrating structured user knowledge into the retrieval process and augmenting\nprompts with user-relevant context, PGraphRAG enhances contextual understanding\nand output quality. We also introduce the Personalized Graph-based Benchmark\nfor Text Generation, designed to evaluate personalized text generation tasks in\nreal-world settings where user history is sparse or unavailable. Experimental\nresults show that PGraphRAG significantly outperforms state-of-the-art\npersonalization methods across diverse tasks, demonstrating the unique\nadvantages of graph-based retrieval for personalization.", "published": "2025-01-04 01:46:49", "link": "http://arxiv.org/abs/2501.02157v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Survey on Question Answering over Visually Rich Documents: Methods,\n  Challenges, and Trends", "abstract": "The field of visually-rich document understanding, which involves interacting\nwith visually-rich documents (whether scanned or born-digital), is rapidly\nevolving and still lacks consensus on several key aspects of the processing\npipeline. In this work, we provide a comprehensive overview of state-of-the-art\napproaches, emphasizing their strengths and limitations, pointing out the main\nchallenges in the field, and proposing promising research directions.", "published": "2025-01-04 08:45:24", "link": "http://arxiv.org/abs/2501.02235v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explicit vs. Implicit: Investigating Social Bias in Large Language\n  Models through Self-Reflection", "abstract": "Large Language Models (LLMs) have been shown to exhibit various biases and\nstereotypes in their generated content. While extensive research has\ninvestigated bias in LLMs, prior work has predominantly focused on explicit\nbias, leaving the more nuanced implicit biases largely unexplored. This paper\npresents a systematic framework grounded in social psychology theories to\ninvestigate and compare explicit and implicit biases in LLMs. We propose a\nnovel \"self-reflection\" based evaluation framework that operates in two phases:\nfirst measuring implicit bias through simulated psychological assessment\nmethods, then evaluating explicit bias by prompting LLMs to analyze their own\ngenerated content. Through extensive experiments on state-of-the-art LLMs\nacross multiple social dimensions, we demonstrate that LLMs exhibit a\nsubstantial inconsistency between explicit and implicit biases, where explicit\nbiases manifest as mild stereotypes while implicit biases show strong\nstereotypes. Furthermore, we investigate the underlying factors contributing to\nthis explicit-implicit bias inconsistency. Our experiments examine the effects\nof training data scale, model parameters, and alignment techniques. Results\nindicate that while explicit bias diminishes with increased training data and\nmodel size, implicit bias exhibits a contrasting upward trend. Notably,\ncontemporary alignment methods (e.g., RLHF, DPO) effectively suppress explicit\nbias but show limited efficacy in mitigating implicit bias. These findings\nsuggest that while scaling up models and alignment training can address\nexplicit bias, the challenge of implicit bias requires novel approaches beyond\ncurrent methodologies.", "published": "2025-01-04 14:08:52", "link": "http://arxiv.org/abs/2501.02295v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Table as Thought: Exploring Structured Thoughts in LLM Reasoning", "abstract": "Large language models' reasoning abilities benefit from methods that organize\ntheir thought processes, such as chain-of-thought prompting, which employs a\nsequential structure to guide the reasoning process step-by-step. However,\nexisting approaches focus primarily on organizing the sequence of thoughts,\nleaving structure in individual thought steps underexplored. To address this\ngap, we propose Table as Thought, a framework inspired by cognitive\nneuroscience theories on human thought. Table as Thought organizes reasoning\nwithin a tabular schema, where rows represent sequential thought steps and\ncolumns capture critical constraints and contextual information to enhance\nreasoning. The reasoning process iteratively populates the table until\nself-verification ensures completeness and correctness. Our experiments show\nthat Table as Thought excels in planning tasks and demonstrates a strong\npotential for enhancing LLM performance in mathematical reasoning compared to\nunstructured thought baselines. This work provides a novel exploration of\nrefining thought representation within LLMs, paving the way for advancements in\nreasoning and AI cognition.", "published": "2025-01-04 00:58:06", "link": "http://arxiv.org/abs/2501.02152v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "CPTuning: Contrastive Prompt Tuning for Generative Relation Extraction", "abstract": "Generative relation extraction (RE) commonly involves first reformulating RE\nas a linguistic modeling problem easily tackled with pre-trained language\nmodels (PLM) and then fine-tuning a PLM with supervised cross-entropy loss.\nAlthough having achieved promising performance, existing approaches assume only\none deterministic relation between each pair of entities without considering\nreal scenarios where multiple relations may be valid, i.e., entity pair\noverlap, causing their limited applications. To address this problem, we\nintroduce a novel contrastive prompt tuning method for RE, CPTuning, which\nlearns to associate a candidate relation between two in-context entities with a\nprobability mass above or below a threshold, corresponding to whether the\nrelation exists. Beyond learning schema, CPTuning also organizes RE as a\nverbalized relation generation task and uses Trie-constrained decoding to\nensure a model generates valid relations. It adaptively picks out the generated\ncandidate relations with a high estimated likelihood in inference, thereby\nachieving multi-relation extraction. We conduct extensive experiments on four\nwidely used datasets to validate our method. Results show that T5-large\nfine-tuned with CPTuning significantly outperforms previous methods, regardless\nof single or multiple relations extraction.", "published": "2025-01-04 05:17:34", "link": "http://arxiv.org/abs/2501.02196v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Financial Named Entity Recognition: How Far Can LLM Go?", "abstract": "The surge of large language models (LLMs) has revolutionized the extraction\nand analysis of crucial information from a growing volume of financial\nstatements, announcements, and business news. Recognition for named entities to\nconstruct structured data poses a significant challenge in analyzing financial\ndocuments and is a foundational task for intelligent financial analytics.\nHowever, how effective are these generic LLMs and their performance under\nvarious prompts are yet need a better understanding. To fill in the blank, we\npresent a systematic evaluation of state-of-the-art LLMs and prompting methods\nin the financial Named Entity Recognition (NER) problem. Specifically, our\nexperimental results highlight their strengths and limitations, identify five\nrepresentative failure types, and provide insights into their potential and\nchallenges for domain-specific tasks.", "published": "2025-01-04 08:47:21", "link": "http://arxiv.org/abs/2501.02237v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMzSz\u0141: a comprehensive LLM benchmark for Polish", "abstract": "This article introduces the first comprehensive benchmark for the Polish\nlanguage at this scale: LLMzSz{\\L} (LLMs Behind the School Desk). It is based\non a coherent collection of Polish national exams, including both academic and\nprofessional tests extracted from the archives of the Polish Central\nExamination Board. It covers 4 types of exams, coming from 154 domains.\nAltogether, it consists of almost 19k closed-ended questions. We investigate\nthe performance of open-source multilingual, English, and Polish LLMs to verify\nLLMs' abilities to transfer knowledge between languages. Also, the correlation\nbetween LLMs and humans at model accuracy and exam pass rate levels is\nexamined. We show that multilingual LLMs can obtain superior results over\nmonolingual ones; however, monolingual models may be beneficial when model size\nmatters. Our analysis highlights the potential of LLMs in assisting with exam\nvalidation, particularly in identifying anomalies or errors in examination\ntasks.", "published": "2025-01-04 12:04:46", "link": "http://arxiv.org/abs/2501.02266v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM\n  Inference", "abstract": "Long-context large language models (LLMs) inference is increasingly critical,\nmotivating a number of studies devoted to alleviating the substantial storage\nand computational costs in such scenarios. Layer-wise skipping methods are\npromising optimizations but rarely explored in long-context inference. We\nobserve that existing layer-wise skipping strategies have several limitations\nwhen applied in long-context inference, including the inability to adapt to\nmodel and context variability, disregard for sublayer significance, and\ninapplicability for the prefilling phase. This paper proposes \\sysname, an\nadaptive sublayer skipping method specifically designed for long-context\ninference. \\sysname adaptively identifies less important layers by leveraging\non-the-fly similarity information, enables sublayer-wise skipping, and\naccelerates both the prefilling and decoding phases. The effectiveness of\n\\sysname is demonstrated through extensive experiments on various long-context\nbenchmarks and models, showcasing its superior inference performance over\nexisting baselines.", "published": "2025-01-04 17:01:30", "link": "http://arxiv.org/abs/2501.02336v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Thinking with Many Minds: Using Large Language Models for\n  Multi-Perspective Problem-Solving", "abstract": "Complex problem-solving requires cognitive flexibility--the capacity to\nentertain multiple perspectives while preserving their distinctiveness. This\nflexibility replicates the \"wisdom of crowds\" within a single individual,\nallowing them to \"think with many minds.\" While mental simulation enables\nimagined deliberation, cognitive constraints limit its effectiveness. We\npropose synthetic deliberation, a Large Language Model (LLM)-based method that\nsimulates discourse between agents embodying diverse perspectives, as a\nsolution. Using a custom GPT-based model, we showcase its benefits: concurrent\nprocessing of multiple viewpoints without cognitive degradation, parallel\nexploration of perspectives, and precise control over viewpoint synthesis. By\nexternalizing the deliberative process and distributing cognitive labor between\nparallel search and integration, synthetic deliberation transcends mental\nsimulation's limitations. This approach shows promise for strategic planning,\npolicymaking, and conflict resolution.", "published": "2025-01-04 18:04:47", "link": "http://arxiv.org/abs/2501.02348v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Context Aware Lemmatization and Morphological Tagging Method in Turkish", "abstract": "The smallest part of a word that defines the word is called a word root. Word\nroots are used to increase success in many applications since they simplify the\nword. In this study, the lemmatization model, which is a word root finding\nmethod, and the morphological tagging model, which predicts the grammatical\nknowledge of the word, are presented. The presented model was developed for\nTurkish, and both models make predictions by taking the meaning of the word\ninto account. In the literature, there is no lemmatization study that is\nsensitive to word meaning in Turkish. For this reason, the present study shares\nthe model and the results obtained from the model on Turkish lemmatization for\nthe first time in the literature. In the present study, in the lemmatization\nand morphological tagging models, bidirectional LSTM is used for the spelling\nof words, and the Turkish BERT model is used for the meaning of words. The\nmodels are trained using the IMST and PUD datasets from Universal Dependencies.\nThe results from the training of the models were compared with the results from\nthe SIGMORPHON 2019 competition. The results of the comparisons revealed that\nour models were superior.", "published": "2025-01-04 19:12:43", "link": "http://arxiv.org/abs/2501.02361v1", "categories": ["cs.CL", "cs.AI", "68T07, 68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Guiding Medical Vision-Language Models with Explicit Visual Prompts:\n  Framework Design and Comprehensive Exploration of Prompt Variations", "abstract": "While mainstream vision-language models (VLMs) have advanced rapidly in\nunderstanding image level information, they still lack the ability to focus on\nspecific areas designated by humans. Rather, they typically rely on large\nvolumes of high-quality image-text paired data to learn and generate posterior\nattention maps. To address this critical issue, we propose leveraging visual\nprompts:simple visual markers in various forms to guide and enhance the\nformation of region-specific attention. Thus, we introduce MedVP, a pioneering\nframework that integrates medical entity extraction, visual prompt generation,\nand dataset adaptation for visual prompt guided fine-tuning. We successfully\noutperform recent state-of-the-art large models across multiple medical VQA\ndatasets. Extensive experiments and Human evaluation are conducted to analyze\nthe impact of different visual prompt forms and how they contribute to\nperformance improvement. The results demonstrate both the effectiveness and\nclinical significance of our approach.", "published": "2025-01-04 21:23:36", "link": "http://arxiv.org/abs/2501.02385v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Syntactic Evolution in Language Usage", "abstract": "This research aims to investigate the dynamic nature of linguistic style\nthroughout various stages of life, from post teenage to old age. By employing\nlinguistic analysis tools and methodologies, the study will delve into the\nintricacies of how individuals adapt and modify their language use over time.\nThe research uses a data set of blogs from blogger.com from 2004 and focuses on\nEnglish for syntactic analysis. The findings of this research can have\nimplications for linguistics, psychology, and communication studies, shedding\nlight on the intricate relationship between age and language.", "published": "2025-01-04 22:27:24", "link": "http://arxiv.org/abs/2501.02392v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt\n  and Reward Models", "abstract": "Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in\naligning large language models (LLMs) with human values and preferences. While\nstate-of-the-art applications like ChatGPT/GPT-4 commonly employ Proximal\nPolicy Optimization (PPO), the inclusion of a critic network introduces\nsignificant computational overhead. REINFORCE-based methods, such as REINFORCE\nLeave One-Out (RLOO), ReMax, and Group Relative Policy Optimization (GRPO),\naddress this limitation by eliminating the critic network. However, these\napproaches face challenges in accurate advantage estimation. Specifically, they\nestimate advantages independently for responses to each prompt, which can lead\nto overfitting on simpler prompts and vulnerability to reward hacking. To\naddress these challenges, we introduce REINFORCE++, a novel approach that\nremoves the critic model while using the normalized reward of a batch as the\nbaseline. Our empirical evaluation demonstrates that REINFORCE++ exhibits\nrobust performance across various reward models without requiring prompt set\ntruncation. Furthermore, it achieves superior generalization in both RLHF and\nlong chain-of-thought (CoT) settings compared to existing REINFORCE-based\nmethods. The implementation is available at\nhttps://github.com/OpenRLHF/OpenRLHF.", "published": "2025-01-04 02:08:06", "link": "http://arxiv.org/abs/2501.03262v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Examining the Robustness of Homogeneity Bias to Hyperparameter\n  Adjustments in GPT-4", "abstract": "Vision-Language Models trained on massive collections of human-generated data\noften reproduce and amplify societal stereotypes. One critical form of\nstereotyping reproduced by these models is homogeneity bias-the tendency to\nrepresent certain groups as more homogeneous than others. We investigate how\nthis bias responds to hyperparameter adjustments in GPT-4, specifically\nexamining sampling temperature and top p which control the randomness of model\noutputs. By generating stories about individuals from different racial and\ngender groups and comparing their similarities using vector representations, we\nassess both bias robustness and its relationship with hyperparameter values. We\nfind that (1) homogeneity bias persists across most hyperparameter\nconfigurations, with Black Americans and women being represented more\nhomogeneously than White Americans and men, (2) the relationship between\nhyperparameters and group representations shows unexpected non-linear patterns,\nparticularly at extreme values, and (3) hyperparameter adjustments affect\nracial and gender homogeneity bias differently-while increasing temperature or\ndecreasing top p can reduce racial homogeneity bias, these changes show\ndifferent effects on gender homogeneity bias. Our findings suggest that while\nhyperparameter tuning may mitigate certain biases to some extent, it cannot\nserve as a universal solution for addressing homogeneity bias across different\nsocial group dimensions.", "published": "2025-01-04 06:51:49", "link": "http://arxiv.org/abs/2501.02211v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Validity Arguments For Constructed Response Scoring Using Generative\n  Artificial Intelligence Applications", "abstract": "The rapid advancements in large language models and generative artificial\nintelligence (AI) capabilities are making their broad application in the\nhigh-stakes testing context more likely. Use of generative AI in the scoring of\nconstructed responses is particularly appealing because it reduces the effort\nrequired for handcrafting features in traditional AI scoring and might even\noutperform those methods. The purpose of this paper is to highlight the\ndifferences in the feature-based and generative AI applications in constructed\nresponse scoring systems and propose a set of best practices for the collection\nof validity evidence to support the use and interpretation of constructed\nresponse scores from scoring systems using generative AI. We compare the\nvalidity evidence needed in scoring systems using human ratings, feature-based\nnatural language processing AI scoring engines, and generative AI. The evidence\nneeded in the generative AI context is more extensive than in the feature-based\nNLP scoring context because of the lack of transparency and other concerns\nunique to generative AI such as consistency. Constructed response score data\nfrom standardized tests demonstrate the collection of validity evidence for\ndifferent types of scoring systems and highlights the numerous complexities and\nconsiderations when making a validity argument for these scores. In addition,\nwe discuss how the evaluation of AI scores might include a consideration of how\na contributory scoring approach combining multiple AI scores (from different\nsources) will cover more of the construct in the absence of human ratings.", "published": "2025-01-04 16:59:29", "link": "http://arxiv.org/abs/2501.02334v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Prepending or Cross-Attention for Speech-to-Text? An Empirical\n  Comparison", "abstract": "Following the remarkable success of Large Language Models (LLMs) in NLP\ntasks, there is increasing interest in extending their capabilities to speech\n-- the most common form of communication. The most widespread approach to\nintegrating speech into LLMs is dense feature prepending (DFP), which prepends\nthe projected speech representations to the textual representations, allowing\nend-to-end training with a speech encoder. This raises questions about the need\nfor a sophisticated speech encoder for DFP and how its performance compares\nwith a standard encoder-decoder (i.e., cross-attention) architecture. We\ncompare DFP and cross-attention under a variety of configurations, such as CTC\ncompression, sequence-level knowledge distillation, on monolingual, bilingual,\nand multilingual models. To perform a controlled architectural comparison, we\ntrain all models from scratch rather than using large pretrained models and use\ncomparable data and parameter settings, testing speech-to-text recognition\n(ASR) and translation (ST) on MuST-C v1.0 and CoVoST2 datasets. Despite the\nwide adoption of DFP, our results do not indicate a clear advantage of DFP over\ncross-attention.", "published": "2025-01-04 20:14:16", "link": "http://arxiv.org/abs/2501.02370v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Survey of State of the Art Large Vision Language Models: Alignment,\n  Benchmark, Evaluations and Challenges", "abstract": "Multimodal Vision Language Models (VLMs) have emerged as a transformative\ntopic at the intersection of computer vision and natural language processing,\nenabling machines to perceive and reason about the world through both visual\nand textual modalities. For example, models such as CLIP, Claude, and GPT-4V\ndemonstrate strong reasoning and understanding abilities on visual and textual\ndata and beat classical single modality vision models on zero-shot\nclassification [93]. With their rapid advancements in research and growing\npopularity in various applications, we provide a comprehensive survey of VLMs.\nSpecifically, we provide a systematic overview of VLMs in the following\naspects: [1] model information of the major VLMs developed up to 2025; [2] the\ntransition of VLM architectures and the newest VLM alignment methods; [3]\nsummary and categorization of the popular benchmarks and evaluation metrics of\nVLMs; [4] the challenges and issues faced by current VLMs such as\nhallucination, alignment, fairness, and safety. Detailed collections including\npapers and model repository links are listed in\nhttps://github.com/zli12321/Vision-Language-Models-Overview.", "published": "2025-01-04 04:59:33", "link": "http://arxiv.org/abs/2501.02189v6", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Optimizing Small Language Models for In-Vehicle Function-Calling", "abstract": "We propose a holistic approach for deploying Small Language Models (SLMs) as\nfunction-calling agents within vehicles as edge devices, offering a more\nflexible and robust alternative to traditional rule-based systems. By\nleveraging SLMs, we simplify vehicle control mechanisms and enhance the user\nexperience. Given the in-vehicle hardware constraints, we apply\nstate-of-the-art model compression techniques, including structured pruning,\nhealing, and quantization, ensuring that the model fits within the resource\nlimitations while maintaining acceptable performance. Our work focuses on\noptimizing a representative SLM, Microsoft's Phi-3 mini, and outlines best\npractices for enabling embedded models, including compression, task-specific\nfine-tuning, and vehicle integration. We demonstrate that, despite significant\nreduction in model size which removes up to 2 billion parameters from the\noriginal model, our approach preserves the model's ability to handle complex\nin-vehicle tasks accurately and efficiently. Furthermore, by executing the\nmodel in a lightweight runtime environment, we achieve a generation speed of 11\ntokens per second, making real-time, on-device inference feasible without\nhardware acceleration. Our results demonstrate the potential of SLMs to\ntransform vehicle control systems, enabling more intuitive interactions between\nusers and their vehicles for an enhanced driving experience.", "published": "2025-01-04 17:32:56", "link": "http://arxiv.org/abs/2501.02342v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers", "abstract": "We present an approach to modifying Transformer architectures by integrating\ngraph-aware relational reasoning into the attention mechanism, merging concepts\nfrom graph neural networks and language modeling. Building on the inherent\nconnection between attention and graph theory, we reformulate the Transformer's\nattention mechanism as a graph operation and propose Graph-Aware Isomorphic\nAttention. This method leverages advanced graph modeling strategies, including\nGraph Isomorphism Networks (GIN) and Principal Neighborhood Aggregation (PNA),\nto enrich the representation of relational structures. Our approach captures\ncomplex dependencies and generalizes across tasks, as evidenced by a reduced\ngeneralization gap and improved learning performance. Additionally, we expand\nthe concept of graph-aware attention to introduce Sparse GIN-Attention, a\nfine-tuning approach that employs sparse GINs. By interpreting attention\nmatrices as sparse adjacency graphs, this technique enhances the adaptability\nof pre-trained foundational models with minimal computational overhead,\nendowing them with graph-aware capabilities. Sparse GIN-Attention fine-tuning\nachieves improved training dynamics and better generalization compared to\nalternative methods like low-rank adaption (LoRA). We discuss latent graph-like\nstructures within traditional attention mechanisms, offering a new lens through\nwhich Transformers can be understood. By evolving Transformers as hierarchical\nGIN models for relational reasoning. This perspective suggests profound\nimplications for foundational model development, enabling the design of\narchitectures that dynamically adapt to both local and global dependencies.\nApplications in bioinformatics, materials science, language modeling, and\nbeyond could benefit from this synthesis of relational and sequential data\nmodeling, setting the stage for interpretable and generalizable modeling\nstrategies.", "published": "2025-01-04 22:30:21", "link": "http://arxiv.org/abs/2501.02393v3", "categories": ["cs.LG", "cond-mat.mes-hall", "cond-mat.mtrl-sci", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLM Content Moderation and User Satisfaction: Evidence from Response\n  Refusals in Chatbot Arena", "abstract": "LLM safety and ethical alignment are widely discussed, but the impact of\ncontent moderation on user satisfaction remains underexplored. To address this,\nwe analyze nearly 50,000 Chatbot Arena response-pairs using a novel fine-tuned\nRoBERTa model, that we trained on hand-labeled data to disentangle refusals due\nto ethical concerns from other refusals due to technical disabilities or lack\nof information. Our findings reveal a significant refusal penalty on content\nmoderation, with users choosing ethical-based refusals roughly one-fourth as\noften as their preferred LLM response compared to standard responses. However,\nthe context and phrasing play critical roles: refusals on highly sensitive\nprompts, such as illegal content, achieve higher win rates than less sensitive\nethical concerns, and longer responses closely aligned with the prompt perform\nbetter. These results emphasize the need for nuanced moderation strategies that\nbalance ethical safeguards with user satisfaction. Moreover, we find that the\nrefusal penalty is notably lower in evaluations using the LLM-as-a-Judge\nmethod, highlighting discrepancies between user and automated assessments.", "published": "2025-01-04 06:36:44", "link": "http://arxiv.org/abs/2501.03266v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Statistical Tests for LLM-Generated Text Detection using\n  Finite Sample Concentration Inequalities", "abstract": "Verifying the provenance of content is crucial to the function of many\norganizations, e.g., educational institutions, social media platforms, firms,\netc. This problem is becoming increasingly difficult as text generated by Large\nLanguage Models (LLMs) becomes almost indistinguishable from human-generated\ncontent. In addition, many institutions utilize in-house LLMs and want to\nensure that external, non-sanctioned LLMs do not produce content within the\ninstitution. In this paper, we answer the following question: Given a piece of\ntext, can we identify whether it was produced by LLM $A$ or $B$ (where $B$ can\nbe a human)? We model LLM-generated text as a sequential stochastic process\nwith complete dependence on history and design zero-shot statistical tests to\ndistinguish between (i) the text generated by two different sets of LLMs $A$\n(in-house) and $B$ (non-sanctioned) and also (ii) LLM-generated and\nhuman-generated texts. We prove that the type I and type II errors for our\ntests decrease exponentially in the text length. In designing our tests, we\nderive concentration inequalities on the difference between log-perplexity and\nthe average entropy of the string under $A$. Specifically, for a given string,\nwe demonstrate that if the string is generated by $A$, the log-perplexity of\nthe string under $A$ converges to the average entropy of the string under $A$,\nexcept with an exponentially small probability in string length. We also show\nthat if $B$ generates the text, except with an exponentially small probability\nin string length, the log-perplexity of the string under $A$ converges to the\naverage cross-entropy of $B$ and $A$. Lastly, we present preliminary\nexperimental results to support our theoretical results. By enabling guaranteed\n(with high probability) finding of the origin of harmful LLM-generated text\nwith arbitrary size, we can help combat misinformation.", "published": "2025-01-04 23:51:43", "link": "http://arxiv.org/abs/2501.02406v2", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.IT", "cs.LG", "math.IT"], "primary_category": "stat.ML"}
{"title": "Optimizing Audio Compression Through Entropy-Controlled Dithering", "abstract": "This paper explores entropy-controlled dithering techniques in audio\ncompression, examining the application of standard and modified TPDFs, combined\nwith noise shaping and entropy-controlled parameters, across various audio\ncontexts, including pitch, loudness, rhythm, and instrumentation variations.\nPerceptual quality metrics such as VISQOL and STOI were used to evaluate\nperformance. The results demonstrate that TPDF-based dithering consistently\noutperforms RPDF, particularly under optimal alpha conditions, while\nhighlighting performance variability based on signal characteristics. These\nfindings suggest the situational appropriateness of using various TPDF\ndistributions. This work emphasizes the trade-off between entropy and\nperceptual fidelity, offering insights into the potential of entropy-controlled\ndithering as a foundation for enhanced audio compression algorithms. A\npractical implementation as a Digital Audio Workstation plugin introduces\ncustomizable dithering controls, laying the groundwork for future advancements\nin audio compression algorithms.", "published": "2025-01-04 14:03:56", "link": "http://arxiv.org/abs/2501.02293v2", "categories": ["eess.AS", "cs.SD", "94A29"], "primary_category": "eess.AS"}
