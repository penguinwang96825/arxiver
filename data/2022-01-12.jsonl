{"title": "PhysNLU: A Language Resource for Evaluating Natural Language\n  Understanding and Explanation Coherence in Physics", "abstract": "In order for language models to aid physics research, they must first encode\nrepresentations of mathematical and natural language discourse which lead to\ncoherent explanations, with correct ordering and relevance of statements. We\npresent a collection of datasets developed to evaluate the performance of\nlanguage models in this regard, which measure capabilities with respect to\nsentence ordering, position, section prediction, and discourse coherence.\nAnalysis of the data reveals equations and sub-disciplines which are most\ncommon in physics discourse, as well as the sentence-level frequency of\nequations and expressions. We present baselines that demonstrate how\ncontemporary language models are challenged by coherence related tasks in\nphysics, even when trained on mathematical natural language objectives.", "published": "2022-01-12 02:32:40", "link": "http://arxiv.org/abs/2201.04275v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PromptBERT: Improving BERT Sentence Embeddings with Prompts", "abstract": "We propose PromptBERT, a novel contrastive learning method for learning\nbetter sentence representation. We firstly analyze the drawback of current\nsentence embedding from original BERT and find that it is mainly due to the\nstatic token embedding bias and ineffective BERT layers. Then we propose the\nfirst prompt-based sentence embeddings method and discuss two prompt\nrepresenting methods and three prompt searching methods to make BERT achieve\nbetter sentence embeddings. Moreover, we propose a novel unsupervised training\nobjective by the technology of template denoising, which substantially shortens\nthe performance gap between the supervised and unsupervised settings. Extensive\nexperiments show the effectiveness of our method. Compared to SimCSE,\nPromptBert achieves 2.29 and 2.58 points of improvement based on BERT and\nRoBERTa in the unsupervised setting.", "published": "2022-01-12 06:54:21", "link": "http://arxiv.org/abs/2201.04337v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Computational analyses of the topics, sentiments, literariness,\n  creativity and beauty of texts in a large Corpus of English Literature", "abstract": "The Gutenberg Literary English Corpus (GLEC, Jacobs, 2018a) provides a rich\nsource of textual data for research in digital humanities, computational\nlinguistics or neurocognitive poetics. In this study we address differences\namong the different literature categories in GLEC, as well as differences\nbetween authors. We report the results of three studies providing i) topic and\nsentiment analyses for six text categories of GLEC (i.e., children and youth,\nessays, novels, plays, poems, stories) and its >100 authors, ii) novel measures\nof semantic complexity as indices of the literariness, creativity and book\nbeauty of the works in GLEC (e.g., Jane Austen's six novels), and iii) two\nexperiments on text classification and authorship recognition using novel\nfeatures of semantic complexity. The data on two novel measures estimating a\ntext's literariness, intratextual variance and stepwise distance (van\nCranenburgh et al., 2019) revealed that plays are the most literary texts in\nGLEC, followed by poems and novels. Computation of a novel index of text\ncreativity (Gray et al., 2016) revealed poems and plays as the most creative\ncategories with the most creative authors all being poets (Milton, Pope, Keats,\nByron, or Wordsworth). We also computed a novel index of perceived beauty of\nverbal art (Kintsch, 2012) for the works in GLEC and predict that Emma is the\ntheoretically most beautiful of Austen's novels. Finally, we demonstrate that\nthese novel measures of semantic complexity are important features for text\nclassification and authorship recognition with overall predictive accuracies in\nthe range of .75 to .97. Our data pave the way for future computational and\nempirical studies of literature or experiments in reading psychology and offer\nmultiple baselines and benchmarks for analysing and validating other book\ncorpora.", "published": "2022-01-12 08:16:52", "link": "http://arxiv.org/abs/2201.04356v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Differentiating Geographic Movement Described in Text Documents", "abstract": "Understanding movement described in text documents is important since text\ndescriptions of movement contain a wealth of geographic and contextual\ninformation about the movement of people, wildlife, goods, and much more. Our\nresearch makes several contributions to improve our understanding of movement\ndescriptions in text. First, we show how interpreting geographic movement\ndescribed in text is challenging because of general spatial terms, linguistic\nconstructions that make the thing(s) moving unclear, and many types of temporal\nreferences and groupings, among others. Next, as a step to overcome these\nchallenges, we report on an experiment with human subjects through which we\nidentify multiple important characteristics of movement descriptions (found in\ntext) that humans use to differentiate one movement description from another.\nBased on our empirical results, we provide recommendations for computational\nanalysis using movement described in text documents. Our findings contribute\ntowards an improved understanding of the important characteristics of the\nunderused information about geographic movement that is in the form of text\ndescriptions.", "published": "2022-01-12 11:49:13", "link": "http://arxiv.org/abs/2201.04427v1", "categories": ["cs.CL", "J.0; I.7.0; H.3.3"], "primary_category": "cs.CL"}
{"title": "Biaffine Discourse Dependency Parsing", "abstract": "We provide a study of using the biaffine model for neural discourse\ndependency parsing and achieve significant performance improvement compared\nwith the baseline parsers. We compare the Eisner algorithm and the\nChu-Liu-Edmonds algorithm in the task and find that using the Chu-Liu-Edmonds\nalgorithm generates deeper trees and achieves better performance. We also\nevaluate the structure of the output of the parser with average maximum path\nlength and average proportion of leaf nodes and find that the dependency trees\ngenerated by the parser are close to the gold trees. As the corpus allows\nnon-projective structures, we analyze the complexity of non-projectivity of the\ncorpus and find that the dependency structures in this corpus have gap degree\nat most one and edge degree at most one.", "published": "2022-01-12 12:56:13", "link": "http://arxiv.org/abs/2201.04450v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Does Data Corruption Affect Natural Language Understanding Models? A\n  Study on GLUE datasets", "abstract": "A central question in natural language understanding (NLU) research is\nwhether high performance demonstrates the models' strong reasoning\ncapabilities. We present an extensive series of controlled experiments where\npre-trained language models are exposed to data that have undergone specific\ncorruption transformations. These involve removing instances of specific word\nclasses and often lead to non-sensical sentences. Our results show that\nperformance remains high on most GLUE tasks when the models are fine-tuned or\ntested on corrupted data, suggesting that they leverage other cues for\nprediction even in non-sensical contexts. Our proposed data transformations can\nbe used to assess the extent to which a specific dataset constitutes a proper\ntestbed for evaluating models' language understanding capabilities.", "published": "2022-01-12 13:35:53", "link": "http://arxiv.org/abs/2201.04467v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring COVID-19 Related Stressors Using Topic Modeling", "abstract": "The COVID-19 pandemic has affected lives of people from different countries\nfor almost two years. The changes on lifestyles due to the pandemic may cause\npsychosocial stressors for individuals, and have a potential to lead to mental\nhealth problems. To provide high quality mental health supports, healthcare\norganization need to identify the COVID-19 specific stressors, and notice the\ntrends of prevalence of those stressors. This study aims to apply natural\nlanguage processing (NLP) on social media data to identify the psychosocial\nstressors during COVID-19 pandemic, and to analyze the trend on prevalence of\nstressors at different stages of the pandemic. We obtained dataset of 9266\nReddit posts from subreddit \\rCOVID19_support, from 14th Feb ,2020 to 19th July\n2021. We used Latent Dirichlet Allocation (LDA) topic model and lexicon methods\nto identify the topics that were mentioned on the subreddit. Our result\npresented a dashboard to visualize the trend of prevalence of topics about\ncovid-19 related stressors being discussed on social media platform. The result\ncould provide insights about the prevalence of pandemic related stressors\nduring different stages of COVID-19. The NLP techniques leveraged in this study\ncould also be applied to analyze event specific stressors in the future.", "published": "2022-01-12 20:22:43", "link": "http://arxiv.org/abs/2202.00476v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NeuraHealth: An Automated Screening Pipeline to Detect Undiagnosed\n  Cognitive Impairment in Electronic Health Records with Deep Learning and\n  Natural Language Processing", "abstract": "Dementia related cognitive impairment (CI) is a neurodegenerative disorder,\naffecting over 55 million people worldwide and growing rapidly at the rate of\none new case every 3 seconds. 75% cases go undiagnosed globally with up to 90%\nin low-and-middle-income countries, leading to an estimated annual worldwide\ncost of USD 1.3 trillion, forecasted to reach 2.8 trillion by 2030. With no\ncure, a recurring failure of clinical trials, and a lack of early diagnosis,\nthe mortality rate is 100%. Information in electronic health records (EHR) can\nprovide vital clues for early detection of CI, but a manual review by experts\nis tedious and error prone. Several computational methods have been proposed,\nhowever, they lack an enhanced understanding of the linguistic context in\ncomplex language structures of EHR. Therefore, I propose a novel and more\naccurate framework, NeuraHealth, to identify patients who had no earlier\ndiagnosis. In NeuraHealth, using patient EHR from Mass General Brigham BioBank,\nI fine-tuned a bi-directional attention-based deep learning natural language\nprocessing model to classify sequences. The sequence predictions were used to\ngenerate structured features as input for a patient level regularized logistic\nregression model. This two-step framework creates high dimensionality,\noutperforming all existing state-of-the-art computational methods as well as\nclinical methods. Further, I integrate the models into a real-world product, a\nweb app, to create an automated EHR screening pipeline for scalable and\nhigh-speed discovery of undetected CI in EHR, making early diagnosis viable in\nmedical facilities and in regions with scarce health services.", "published": "2022-01-12 06:19:14", "link": "http://arxiv.org/abs/2202.00478v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic Modeling on Podcast Short-Text Metadata", "abstract": "Podcasts have emerged as a massively consumed online content, notably due to\nwider accessibility of production means and scaled distribution through large\nstreaming platforms. Categorization systems and information access technologies\ntypically use topics as the primary way to organize or navigate podcast\ncollections. However, annotating podcasts with topics is still quite\nproblematic because the assigned editorial genres are broad, heterogeneous or\nmisleading, or because of data challenges (e.g. short metadata text, noisy\ntranscripts). Here, we assess the feasibility to discover relevant topics from\npodcast metadata, titles and descriptions, using topic modeling techniques for\nshort text. We also propose a new strategy to leverage named entities (NEs),\noften present in podcast metadata, in a Non-negative Matrix Factorization (NMF)\ntopic modeling framework. Our experiments on two existing datasets from Spotify\nand iTunes and Deezer, a new dataset from an online service providing a catalog\nof podcasts, show that our proposed document representation, NEiCE, leads to\nimproved topic coherence over the baselines. We release the code for\nexperimental reproducibility of the results.", "published": "2022-01-12 11:07:05", "link": "http://arxiv.org/abs/2201.04419v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Diagnosing BERT with Retrieval Heuristics", "abstract": "Word embeddings, made widely popular in 2013 with the release of word2vec,\nhave become a mainstay of NLP engineering pipelines. Recently, with the release\nof BERT, word embeddings have moved from the term-based embedding space to the\ncontextual embedding space -- each term is no longer represented by a single\nlow-dimensional vector but instead each term and \\emph{its context} determine\nthe vector weights. BERT's setup and architecture have been shown to be general\nenough to be applicable to many natural language tasks. Importantly for\nInformation Retrieval (IR), in contrast to prior deep learning solutions to IR\nproblems which required significant tuning of neural net architectures and\ntraining regimes, \"vanilla BERT\" has been shown to outperform existing\nretrieval algorithms by a wide margin, including on tasks and corpora that have\nlong resisted retrieval effectiveness gains over traditional IR baselines (such\nas Robust04). In this paper, we employ the recently proposed axiomatic dataset\nanalysis technique -- that is, we create diagnostic datasets that each fulfil a\nretrieval heuristic (both term matching and semantic-based) -- to explore what\nBERT is able to learn. In contrast to our expectations, we find BERT, when\napplied to a recently released large-scale web corpus with ad-hoc topics, to\n\\emph{not} adhere to any of the explored axioms. At the same time, BERT\noutperforms the traditional query likelihood retrieval model by 40\\%. This\nmeans that the axiomatic approach to IR (and its extension of diagnostic\ndatasets created for retrieval heuristics) may in its current form not be\napplicable to large-scale corpora. Additional -- different -- axioms are\nneeded.", "published": "2022-01-12 13:11:17", "link": "http://arxiv.org/abs/2201.04458v1", "categories": ["cs.IR", "cs.CL", "H.3.3"], "primary_category": "cs.IR"}
{"title": "Human Evaluation of Conversations is an Open Problem: comparing the\n  sensitivity of various methods for evaluating dialogue agents", "abstract": "At the heart of improving conversational AI is the open problem of how to\nevaluate conversations. Issues with automatic metrics are well known (Liu et\nal., 2016, arXiv:1603.08023), with human evaluations still considered the gold\nstandard. Unfortunately, how to perform human evaluations is also an open\nproblem: differing data collection methods have varying levels of human\nagreement and statistical sensitivity, resulting in differing amounts of human\nannotation hours and labor costs. In this work we compare five different\ncrowdworker-based human evaluation methods and find that different methods are\nbest depending on the types of models compared, with no clear winner across the\nboard. While this highlights the open problems in the area, our analysis leads\nto advice of when to use which one, and possible future directions.", "published": "2022-01-12 22:45:49", "link": "http://arxiv.org/abs/2201.04723v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Biochemical Space Language in Relation to Multiset Rewriting Systems", "abstract": "This technical report relates Biochemical Space Language (BCSL) to Multiset\nrewriting systems (MRS). For a BCSL model, the semantics are defined in terms\nof transition systems, while for an MRS, they are defined in terms of a set of\nruns. In this report, we relate BCSL to MRS by first showing how the transition\nsystem is related to a set of runs and consequently showing how for every BCSL\nmodel, an MRS can be constructed such that both represent the same set of runs.\nThe motivation of this step is to establish BCSL in the context of a more\ngeneral rewriting system and benefit from properties shown for them. Finally,\nwe show that regulations defined for MRS can be consequently used in the BCSL\nmodel.", "published": "2022-01-12 15:08:36", "link": "http://arxiv.org/abs/2201.08817v1", "categories": ["cs.LO", "cs.CL"], "primary_category": "cs.LO"}
{"title": "Detection of Increased Time Intervals of Anti-Vaccine Tweets for\n  COVID-19 Vaccine with BERT Model", "abstract": "The most effective of the solutions against Covid-19 is the various vaccines\ndeveloped. Distrust of vaccines can hinder the rapid and effective use of this\nremedy. One of the means of expressing the thoughts of society is social media.\nDetermining the time intervals during which anti-vaccination increases in\nsocial media can help institutions determine the strategy to be used in\ncombating anti-vaccination. Recording and tracking every tweet entered with\nhuman labor would be inefficient, so various automation solutions are needed.\nIn this study, The Bidirectional Encoder Representations from Transformers\n(BERT) model, which is a deep learning-based natural language processing (NLP)\nmodel, was used. In a dataset of 1506 tweets divided into four different\ncategories as news, irrelevant, anti-vaccine, and vaccine supporters, the model\nwas trained with a learning rate of 5e-6 for 25 epochs. To determine the\nintervals in which anti-vaccine tweets are concentrated, the categories to\nwhich 652840 tweets belong were determined by using the trained model. The\nchange of the determined categories overtime was visualized and the events that\ncould cause the change were determined. As a result of model training, in the\ntest dataset, the f-score of 0.81 and AUC values for different classes were\nobtained as 0.99,0.91, 0.92, 0.92, respectively. In this model, unlike the\nstudies in the literature, an auxiliary system is designed that provides data\nthat institutions can use when determining their strategy by measuring and\nvisualizing the frequency of anti-vaccine tweets in a time interval, different\nfrom detecting and censoring such tweets.", "published": "2022-01-12 18:30:23", "link": "http://arxiv.org/abs/2202.00477v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Can Graph Neural Networks Help Document Retrieval: A Case Study on\n  CORD19 with Concept Map Generation", "abstract": "Graph neural networks (GNNs), as a group of powerful tools for representation\nlearning on irregular data, have manifested superiority in various downstream\ntasks. With unstructured texts represented as concept maps, GNNs can be\nexploited for tasks like document retrieval. Intrigued by how can GNNs help\ndocument retrieval, we conduct an empirical study on a large-scale\nmulti-discipline dataset CORD-19. Results show that instead of the complex\nstructure-oriented GNNs such as GINs and GATs, our proposed semantics-oriented\ngraph functions achieve better and more stable performance based on the BM25\nretrieved candidates. Our insights in this case study can serve as a guideline\nfor future work to develop effective GNNs with appropriate semantics-oriented\ninductive biases for textual reasoning tasks like document retrieval and\nclassification. All code for this case study is available at\nhttps://github.com/HennyJie/GNN-DocRetrieval.", "published": "2022-01-12 19:52:29", "link": "http://arxiv.org/abs/2201.04672v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "68T50, 68T37, 68T01, 68P20", "H.3.3; I.7; I.2.7; I.2.6; I.2.4"], "primary_category": "cs.IR"}
{"title": "Sound-Dr: Reliable Sound Dataset and Baseline Artificial Intelligence\n  System for Respiratory Illnesses", "abstract": "As the burden of respiratory diseases continues to fall on society worldwide,\nthis paper proposes a high-quality and reliable dataset of human sounds for\nstudying respiratory illnesses, including pneumonia and COVID-19. It consists\nof coughing, mouth breathing, and nose breathing sounds together with metadata\non related clinical characteristics. We also develop a proof-of-concept system\nfor establishing baselines and benchmarking against multiple datasets, such as\nCoswara and COUGHVID. Our comprehensive experiments show that the Sound-Dr\ndataset has richer features, better performance, and is more robust to dataset\nshifts in various machine learning tasks. It is promising for a wide range of\nreal-time applications on mobile devices. The proposed dataset and system will\nserve as practical tools to support healthcare professionals in diagnosing\nrespiratory disorders. The dataset and code are publicly available here:\nhttps://github.com/ReML-AI/Sound-Dr/.", "published": "2022-01-12 17:15:17", "link": "http://arxiv.org/abs/2201.04581v3", "categories": ["cs.SD", "eess.AS", "68-11, 92-XX", "E.0; I.2.1"], "primary_category": "cs.SD"}
{"title": "VoxSRC 2021: The Third VoxCeleb Speaker Recognition Challenge", "abstract": "The third instalment of the VoxCeleb Speaker Recognition Challenge was held\nin conjunction with Interspeech 2021. The aim of this challenge was to assess\nhow well current speaker recognition technology is able to diarise and\nrecognise speakers in unconstrained or `in the wild' data. The challenge\nconsisted of: (i) the provision of publicly available speaker recognition and\ndiarisation data from YouTube videos together with ground truth annotation and\nstandardised evaluation software; and (ii) a virtual public challenge and\nworkshop held at Interspeech 2021. This paper outlines the challenge, and\ndescribes the baselines, methods and results. We conclude with a discussion on\nthe new multi-lingual focus of VoxSRC 2021, and on the progression of the\nchallenge since the previous two editions.", "published": "2022-01-12 17:19:45", "link": "http://arxiv.org/abs/2201.04583v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dynamical Audio-Visual Navigation: Catching Unheard Moving Sound Sources\n  in Unmapped 3D Environments", "abstract": "Recent work on audio-visual navigation targets a single static sound in\nnoise-free audio environments and struggles to generalize to unheard sounds. We\nintroduce the novel dynamic audio-visual navigation benchmark in which an\nembodied AI agent must catch a moving sound source in an unmapped environment\nin the presence of distractors and noisy sounds. We propose an end-to-end\nreinforcement learning approach that relies on a multi-modal architecture that\nfuses the spatial audio-visual information from a binaural audio signal and\nspatial occupancy maps to encode the features needed to learn a robust\nnavigation policy for our new complex task settings. We demonstrate that our\napproach outperforms the current state-of-the-art with better generalization to\nunheard sounds and better robustness to noisy scenarios on the two challenging\n3D scanned real-world datasets Replica and Matterport3D, for the static and\ndynamic audio-visual navigation benchmarks. Our novel benchmark will be made\navailable at http://dav-nav.cs.uni-freiburg.de.", "published": "2022-01-12 03:08:03", "link": "http://arxiv.org/abs/2201.04279v1", "categories": ["cs.CV", "cs.LG", "cs.RO", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
