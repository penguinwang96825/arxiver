{"title": "Acronym Identification and Disambiguation Shared Tasks for Scientific\n  Document Understanding", "abstract": "Acronyms are the short forms of longer phrases and they are frequently used\nin writing, especially scholarly writing, to save space and facilitate the\ncommunication of information. As such, every text understanding tool should be\ncapable of recognizing acronyms in text (i.e., acronym identification) and also\nfinding their correct meaning (i.e., acronym disambiguation). As most of the\nprior works on these tasks are restricted to the biomedical domain and use\nunsupervised methods or models trained on limited datasets, they fail to\nperform well for scientific document understanding. To push forward research in\nthis direction, we have organized two shared task for acronym identification\nand acronym disambiguation in scientific documents, named AI@SDU and AD@SDU,\nrespectively. The two shared tasks have attracted 52 and 43 participants,\nrespectively. While the submitted systems make substantial improvements\ncompared to the existing baselines, there are still far from the human-level\nperformance. This paper reviews the two shared tasks and the prominent\nparticipating systems for each of them.", "published": "2020-12-22 00:29:15", "link": "http://arxiv.org/abs/2012.11760v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Biomedical Word Embeddings in the Transformer Era", "abstract": "Biomedical word embeddings are usually pre-trained on free text corpora with\nneural methods that capture local and global distributional properties. They\nare leveraged in downstream tasks using various neural architectures that are\ndesigned to optimize task-specific objectives that might further tune such\nembeddings. Since 2018, however, there is a marked shift from these static\nembeddings to contextual embeddings motivated by language models (e.g., ELMo,\ntransformers such as BERT, and ULMFiT). These dynamic embeddings have the added\nbenefit of being able to distinguish homonyms and acronyms given their context.\nHowever, static embeddings are still relevant in low resource settings (e.g.,\nsmart devices, IoT elements) and to study lexical semantics from a\ncomputational linguistics perspective. In this paper, we jointly learn word and\nconcept embeddings by first using the skip-gram method and further fine-tuning\nthem with correlational information manifesting in co-occurring Medical Subject\nHeading (MeSH) concepts in biomedical citations. This fine-tuning is\naccomplished with the BERT transformer architecture in the two-sentence input\nmode with a classification objective that captures MeSH pair co-occurrence. In\nessence, we repurpose a transformer architecture (typically used to generate\ndynamic embeddings) to improve static embeddings using concept correlations. We\nconduct evaluations of these tuned static embeddings using multiple datasets\nfor word relatedness developed by previous efforts. Without selectively culling\nconcepts and terms (as was pursued by previous efforts), we believe we offer\nthe most exhaustive evaluation of static embeddings to date with clear\nperformance improvements across the board. We provide our code and embeddings\nfor public use for downstream applications and research endeavors:\nhttps://github.com/bionlproc/BERT-CRel-Embeddings", "published": "2020-12-22 03:03:50", "link": "http://arxiv.org/abs/2012.11808v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recognizing Emotion Cause in Conversations", "abstract": "We address the problem of recognizing emotion cause in conversations, define\ntwo novel sub-tasks of this problem, and provide a corresponding dialogue-level\ndataset, along with strong Transformer-based baselines. The dataset is\navailable at https://github.com/declare-lab/RECCON.\n  Introduction: Recognizing the cause behind emotions in text is a fundamental\nyet under-explored area of research in NLP. Advances in this area hold the\npotential to improve interpretability and performance in affect-based models.\nIdentifying emotion causes at the utterance level in conversations is\nparticularly challenging due to the intermingling dynamics among the\ninterlocutors.\n  Method: We introduce the task of Recognizing Emotion Cause in CONversations\nwith an accompanying dataset named RECCON, containing over 1,000 dialogues and\n10,000 utterance cause-effect pairs. Furthermore, we define different cause\ntypes based on the source of the causes, and establish strong Transformer-based\nbaselines to address two different sub-tasks on this dataset: causal span\nextraction and causal emotion entailment.\n  Result: Our Transformer-based baselines, which leverage contextual\npre-trained embeddings, such as RoBERTa, outperform the state-of-the-art\nemotion cause extraction approaches\n  Conclusion: We introduce a new task highly relevant for (explainable)\nemotion-aware artificial intelligence: recognizing emotion cause in\nconversations, provide a new highly challenging publicly available\ndialogue-level dataset for this task, and give strong baseline results on this\ndataset.", "published": "2020-12-22 03:51:35", "link": "http://arxiv.org/abs/2012.11820v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-Training a Language Model Without Human Language", "abstract": "In this paper, we study how the intrinsic nature of pre-training data\ncontributes to the fine-tuned downstream performance. To this end, we pre-train\ndifferent transformer-based masked language models on several corpora with\ncertain features, and we fine-tune those language models on GLUE benchmarks. We\nfind that models pre-trained on unstructured data beat those trained directly\nfrom scratch on downstream tasks. Our results also show that pre-training on\nstructured data does not always make the model acquire ability that can be\ntransferred to natural language downstream tasks. To our great astonishment, we\nuncover that pre-training on certain non-human language data gives GLUE\nperformance close to performance pre-trained on another non-English language.", "published": "2020-12-22 13:38:06", "link": "http://arxiv.org/abs/2012.11995v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Applying Wav2vec2.0 to Speech Recognition in Various Low-resource\n  Languages", "abstract": "There are several domains that own corresponding widely used feature\nextractors, such as ResNet, BERT, and GPT-x. These models are usually\npre-trained on large amounts of unlabeled data by self-supervision and can be\neffectively applied to downstream tasks. In the speech domain, wav2vec2.0\nstarts to show its powerful representation ability and feasibility of ultra-low\nresource speech recognition on the Librispeech corpus, which belongs to the\naudiobook domain. However, wav2vec2.0 has not been examined on real spoken\nscenarios and languages other than English. To verify its universality over\nlanguages, we apply pre-trained models to solve low-resource speech recognition\ntasks in various spoken languages. We achieve more than 20% relative\nimprovements in six languages compared with previous work. Among these\nlanguages, English achieves a gain of 52.4%. Moreover, using coarse-grained\nmodeling units, such as subword or character, achieves better results than\nfine-grained modeling units, such as phone or letter.", "published": "2020-12-22 15:59:44", "link": "http://arxiv.org/abs/2012.12121v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Head Self-Attention with Role-Guided Masks", "abstract": "The state of the art in learning meaningful semantic representations of words\nis the Transformer model and its attention mechanisms. Simply put, the\nattention mechanisms learn to attend to specific parts of the input dispensing\nrecurrence and convolutions. While some of the learned attention heads have\nbeen found to play linguistically interpretable roles, they can be redundant or\nprone to errors. We propose a method to guide the attention heads towards roles\nidentified in prior work as important. We do this by defining role-specific\nmasks to constrain the heads to attend to specific parts of the input, such\nthat different heads are designed to play different roles. Experiments on text\nclassification and machine translation using 7 different datasets show that our\nmethod outperforms competitive attention-based, CNN, and RNN baselines.", "published": "2020-12-22 21:34:02", "link": "http://arxiv.org/abs/2012.12366v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple-QE: Better Automatic Quality Estimation for Text Simplification", "abstract": "Text simplification systems generate versions of texts that are easier to\nunderstand for a broader audience. The quality of simplified texts is generally\nestimated using metrics that compare to human references, which can be\ndifficult to obtain. We propose Simple-QE, a BERT-based quality estimation (QE)\nmodel adapted from prior summarization QE work, and show that it correlates\nwell with human quality judgments. Simple-QE does not require human references,\nwhich makes the model useful in a practical setting where users would need to\nbe informed about the quality of generated simplifications. We also show that\nwe can adapt this approach to accurately predict the complexity of\nhuman-written texts.", "published": "2020-12-22 22:02:37", "link": "http://arxiv.org/abs/2012.12382v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Disentangled Framework for Transferable Named Entity\n  Recognition", "abstract": "Named entity recognition (NER) for identifying proper nouns in unstructured\ntext is one of the most important and fundamental tasks in natural language\nprocessing. However, despite the widespread use of NER models, they still\nrequire a large-scale labeled data set, which incurs a heavy burden due to\nmanual annotation. Domain adaptation is one of the most promising solutions to\nthis problem, where rich labeled data from the relevant source domain are\nutilized to strengthen the generalizability of a model based on the target\ndomain. However, the mainstream cross-domain NER models are still affected by\nthe following two challenges (1) Extracting domain-invariant information such\nas syntactic information for cross-domain transfer. (2) Integrating\ndomain-specific information such as semantic information into the model to\nimprove the performance of NER. In this study, we present a semi-supervised\nframework for transferable NER, which disentangles the domain-invariant latent\nvariables and domain-specific latent variables. In the proposed framework, the\ndomain-specific information is integrated with the domain-specific latent\nvariables by using a domain predictor. The domain-specific and domain-invariant\nlatent variables are disentangled using three mutual information regularization\nterms, i.e., maximizing the mutual information between the domain-specific\nlatent variables and the original embedding, maximizing the mutual information\nbetween the domain-invariant latent variables and the original embedding, and\nminimizing the mutual information between the domain-specific and\ndomain-invariant latent variables. Extensive experiments demonstrated that our\nmodel can obtain state-of-the-art performance with cross-domain and\ncross-lingual NER benchmark data sets.", "published": "2020-12-22 02:55:04", "link": "http://arxiv.org/abs/2012.11805v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Undivided Attention: Are Intermediate Layers Necessary for BERT?", "abstract": "In recent times, BERT-based models have been extremely successful in solving\na variety of natural language processing (NLP) tasks such as reading\ncomprehension, natural language inference, sentiment analysis, etc. All\nBERT-based architectures have a self-attention block followed by a block of\nintermediate layers as the basic building component. However, a strong\njustification for the inclusion of these intermediate layers remains missing in\nthe literature. In this work we investigate the importance of intermediate\nlayers on the overall network performance of downstream tasks. We show that\nreducing the number of intermediate layers and modifying the architecture for\nBERT-BASE results in minimal loss in fine-tuning accuracy for downstream tasks\nwhile decreasing the number of parameters and training time of the model.\nAdditionally, we use centered kernel alignment and probing linear classifiers\nto gain insight into our architectural modifications and justify that removal\nof intermediate layers has little impact on the fine-tuned accuracy.", "published": "2020-12-22 08:46:14", "link": "http://arxiv.org/abs/2012.11881v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Few-Shot Text Generation with Pattern-Exploiting Training", "abstract": "Providing pretrained language models with simple task descriptions in natural\nlanguage enables them to solve some tasks in a fully unsupervised fashion.\nMoreover, when combined with regular learning from examples, this idea yields\nimpressive few-shot results for a wide range of text classification tasks. It\nis also a promising direction to improve data efficiency in generative\nsettings, but there are several challenges to using a combination of task\ndescriptions and example-based learning for text generation. In particular, it\nis crucial to find task descriptions that are easy to understand for the\npretrained model and to ensure that it actually makes good use of them;\nfurthermore, effective measures against overfitting have to be implemented. In\nthis paper, we show how these challenges can be tackled: We introduce GenPET, a\nmethod for text generation that is based on pattern-exploiting training, a\nrecent approach for combining textual instructions with supervised learning\nthat only works for classification tasks. On several summarization and headline\ngeneration datasets, GenPET gives consistent improvements over strong baselines\nin few-shot settings.", "published": "2020-12-22 10:53:07", "link": "http://arxiv.org/abs/2012.11926v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Hierarchical Reasoning Graph Neural Network for The Automatic Scoring\n  of Answer Transcriptions in Video Job Interviews", "abstract": "We address the task of automatically scoring the competency of candidates\nbased on textual features, from the automatic speech recognition (ASR)\ntranscriptions in the asynchronous video job interview (AVI). The key challenge\nis how to construct the dependency relation between questions and answers, and\nconduct the semantic level interaction for each question-answer (QA) pair.\nHowever, most of the recent studies in AVI focus on how to represent questions\nand answers better, but ignore the dependency information and interaction\nbetween them, which is critical for QA evaluation. In this work, we propose a\nHierarchical Reasoning Graph Neural Network (HRGNN) for the automatic\nassessment of question-answer pairs. Specifically, we construct a\nsentence-level relational graph neural network to capture the dependency\ninformation of sentences in or between the question and the answer. Based on\nthese graphs, we employ a semantic-level reasoning graph attention network to\nmodel the interaction states of the current QA session. Finally, we propose a\ngated recurrent unit encoder to represent the temporal question-answer pairs\nfor the final prediction. Empirical results conducted on CHNAT (a real-world\ndataset) validate that our proposed model significantly outperforms\ntext-matching based benchmark models. Ablation studies and experimental results\nwith 10 random seeds also show the effectiveness and stability of our models.", "published": "2020-12-22 12:27:45", "link": "http://arxiv.org/abs/2012.11960v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Graph-Evolving Meta-Learning for Low-Resource Medical Dialogue\n  Generation", "abstract": "Human doctors with well-structured medical knowledge can diagnose a disease\nmerely via a few conversations with patients about symptoms. In contrast,\nexisting knowledge-grounded dialogue systems often require a large number of\ndialogue instances to learn as they fail to capture the correlations between\ndifferent diseases and neglect the diagnostic experience shared among them. To\naddress this issue, we propose a more natural and practical paradigm, i.e.,\nlow-resource medical dialogue generation, which can transfer the diagnostic\nexperience from source diseases to target ones with a handful of data for\nadaptation. It is capitalized on a commonsense knowledge graph to characterize\nthe prior disease-symptom relations. Besides, we develop a Graph-Evolving\nMeta-Learning (GEML) framework that learns to evolve the commonsense graph for\nreasoning disease-symptom correlations in a new disease, which effectively\nalleviates the needs of a large number of dialogues. More importantly, by\ndynamically evolving disease-symptom graphs, GEML also well addresses the\nreal-world challenges that the disease-symptom correlations of each disease may\nvary or evolve along with more diagnostic cases. Extensive experiment results\non the CMDD dataset and our newly-collected Chunyu dataset testify the\nsuperiority of our approach over state-of-the-art approaches. Besides, our GEML\ncan generate an enriched dialogue-sensitive knowledge graph in an online\nmanner, which could benefit other tasks grounded on knowledge graph.", "published": "2020-12-22 13:20:23", "link": "http://arxiv.org/abs/2012.11988v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Uncertainty and Surprisal Jointly Deliver the Punchline: Exploiting\n  Incongruity-Based Features for Humor Recognition", "abstract": "Humor recognition has been widely studied as a text classification problem\nusing data-driven approaches. However, most existing work does not examine the\nactual joke mechanism to understand humor. We break down any joke into two\ndistinct components: the set-up and the punchline, and further explore the\nspecial relationship between them. Inspired by the incongruity theory of humor,\nwe model the set-up as the part developing semantic uncertainty, and the\npunchline disrupting audience expectations. With increasingly powerful language\nmodels, we were able to feed the set-up along with the punchline into the GPT-2\nlanguage model, and calculate the uncertainty and surprisal values of the\njokes. By conducting experiments on the SemEval 2021 Task 7 dataset, we found\nthat these two features have better capabilities of telling jokes from\nnon-jokes, compared with existing baselines.", "published": "2020-12-22 13:48:09", "link": "http://arxiv.org/abs/2012.12007v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Event-Driven Query Expansion", "abstract": "A significant number of event-related queries are issued in Web search. In\nthis paper, we seek to improve retrieval performance by leveraging events and\nspecifically target the classic task of query expansion. We propose a method to\nexpand an event-related query by first detecting the events related to it.\nThen, we derive the candidates for expansion as terms semantically related to\nboth the query and the events. To identify the candidates, we utilize a novel\nmechanism to simultaneously embed words and events in the same vector space. We\nshow that our proposed method of leveraging events improves query expansion\nperformance significantly compared with state-of-the-art methods on various\nnewswire TREC datasets.", "published": "2020-12-22 14:56:54", "link": "http://arxiv.org/abs/2012.12065v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Domain Adaptation of NMT models for English-Hindi Machine Translation\n  Task at AdapMT ICON 2020", "abstract": "Recent advancements in Neural Machine Translation (NMT) models have proved to\nproduce a state of the art results on machine translation for low resource\nIndian languages. This paper describes the neural machine translation systems\nfor the English-Hindi language presented in AdapMT Shared Task ICON 2020. The\nshared task aims to build a translation system for Indian languages in specific\ndomains like Artificial Intelligence (AI) and Chemistry using a small in-domain\nparallel corpus. We evaluated the effectiveness of two popular NMT models i.e,\nLSTM, and Transformer architectures for the English-Hindi machine translation\ntask based on BLEU scores. We train these models primarily using the out of\ndomain data and employ simple domain adaptation techniques based on the\ncharacteristics of the in-domain dataset. The fine-tuning and mixed-domain data\napproaches are used for domain adaptation. Our team was ranked first in the\nchemistry and general domain En-Hi translation task and second in the AI domain\nEn-Hi translation task.", "published": "2020-12-22 15:46:40", "link": "http://arxiv.org/abs/2012.12112v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ActionBert: Leveraging User Actions for Semantic Understanding of User\n  Interfaces", "abstract": "As mobile devices are becoming ubiquitous, regularly interacting with a\nvariety of user interfaces (UIs) is a common aspect of daily life for many\npeople. To improve the accessibility of these devices and to enable their usage\nin a variety of settings, building models that can assist users and accomplish\ntasks through the UI is vitally important. However, there are several\nchallenges to achieve this. First, UI components of similar appearance can have\ndifferent functionalities, making understanding their function more important\nthan just analyzing their appearance. Second, domain-specific features like\nDocument Object Model (DOM) in web pages and View Hierarchy (VH) in mobile\napplications provide important signals about the semantics of UI elements, but\nthese features are not in a natural language format. Third, owing to a large\ndiversity in UIs and absence of standard DOM or VH representations, building a\nUI understanding model with high coverage requires large amounts of training\ndata.\n  Inspired by the success of pre-training based approaches in NLP for tackling\na variety of problems in a data-efficient way, we introduce a new pre-trained\nUI representation model called ActionBert. Our methodology is designed to\nleverage visual, linguistic and domain-specific features in user interaction\ntraces to pre-train generic feature representations of UIs and their\ncomponents. Our key intuition is that user actions, e.g., a sequence of clicks\non different UI components, reveals important information about their\nfunctionality. We evaluate the proposed model on a wide variety of downstream\ntasks, ranging from icon classification to UI component retrieval based on its\nnatural language description. Experiments show that the proposed ActionBert\nmodel outperforms multi-modal baselines across all downstream tasks by up to\n15.5%.", "published": "2020-12-22 20:49:52", "link": "http://arxiv.org/abs/2012.12350v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Seeing past words: Testing the cross-modal capabilities of pretrained\n  V&L models on counting tasks", "abstract": "We investigate the reasoning ability of pretrained vision and language (V&L)\nmodels in two tasks that require multimodal integration: (1) discriminating a\ncorrect image-sentence pair from an incorrect one, and (2) counting entities in\nan image. We evaluate three pretrained V&L models on these tasks: ViLBERT,\nViLBERT 12-in-1 and LXMERT, in zero-shot and finetuned settings. Our results\nshow that models solve task (1) very well, as expected, since all models are\npretrained on task (1). However, none of the pretrained V&L models is able to\nadequately solve task (2), our counting probe, and they cannot generalise to\nout-of-distribution quantities. We propose a number of explanations for these\nfindings: LXMERT (and to some extent ViLBERT 12-in-1) show some evidence of\ncatastrophic forgetting on task (1). Concerning our results on the counting\nprobe, we find evidence that all models are impacted by dataset bias, and also\nfail to individuate entities in the visual input. While a selling point of\npretrained V&L models is their ability to solve complex tasks, our findings\nsuggest that understanding their reasoning and grounding capabilities requires\nmore targeted investigations on specific phenomena.", "published": "2020-12-22 21:01:44", "link": "http://arxiv.org/abs/2012.12352v4", "categories": ["cs.CV", "cs.CL", "68Txx", "I.2.7; I.2.10"], "primary_category": "cs.CV"}
{"title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model\n  Fine-Tuning", "abstract": "Although pretrained language models can be fine-tuned to produce\nstate-of-the-art results for a very wide range of language understanding tasks,\nthe dynamics of this process are not well understood, especially in the low\ndata regime. Why can we use relatively vanilla gradient descent algorithms\n(e.g., without strong regularization) to tune a model with hundreds of millions\nof parameters on datasets with only hundreds or thousands of labeled examples?\nIn this paper, we argue that analyzing fine-tuning through the lens of\nintrinsic dimension provides us with empirical and theoretical intuitions to\nexplain this remarkable phenomenon. We empirically show that common pre-trained\nmodels have a very low intrinsic dimension; in other words, there exists a low\ndimension reparameterization that is as effective for fine-tuning as the full\nparameter space. For example, by optimizing only 200 trainable parameters\nrandomly projected back into the full space, we can tune a RoBERTa model to\nachieve 90\\% of the full parameter performance levels on MRPC. Furthermore, we\nempirically show that pre-training implicitly minimizes intrinsic dimension\nand, perhaps surprisingly, larger models tend to have lower intrinsic dimension\nafter a fixed number of pre-training updates, at least in part explaining their\nextreme effectiveness. Lastly, we connect intrinsic dimensionality with low\ndimensional task representations and compression based generalization bounds to\nprovide intrinsic-dimension-based generalization bounds that are independent of\nthe full parameter count.", "published": "2020-12-22 07:42:30", "link": "http://arxiv.org/abs/2012.13255v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Adversarial Meta Sampling for Multilingual Low-Resource Speech\n  Recognition", "abstract": "Low-resource automatic speech recognition (ASR) is challenging, as the\nlow-resource target language data cannot well train an ASR model. To solve this\nissue, meta-learning formulates ASR for each source language into many small\nASR tasks and meta-learns a model initialization on all tasks from different\nsource languages to access fast adaptation on unseen target languages. However,\nfor different source languages, the quantity and difficulty vary greatly\nbecause of their different data scales and diverse phonological systems, which\nleads to task-quantity and task-difficulty imbalance issues and thus a failure\nof multilingual meta-learning ASR (MML-ASR). In this work, we solve this\nproblem by developing a novel adversarial meta sampling (AMS) approach to\nimprove MML-ASR. When sampling tasks in MML-ASR, AMS adaptively determines the\ntask sampling probability for each source language. Specifically, for each\nsource language, if the query loss is large, it means that its tasks are not\nwell sampled to train ASR model in terms of its quantity and difficulty and\nthus should be sampled more frequently for extra learning. Inspired by this\nfact, we feed the historical task query loss of all source language domain into\na network to learn a task sampling policy for adversarially increasing the\ncurrent query loss of MML-ASR. Thus, the learnt task sampling policy can master\nthe learning situation of each language and thus predicts good task sampling\nprobability for each language for more effective learning. Finally, experiment\nresults on two multilingual datasets show significant performance improvement\nwhen applying our AMS on MML-ASR, and also demonstrate the applicability of AMS\nto other low-resource speech tasks and transfer learning ASR approaches.", "published": "2020-12-22 09:33:14", "link": "http://arxiv.org/abs/2012.11896v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning to Retrieve Entity-Aware Knowledge and Generate Responses with\n  Copy Mechanism for Task-Oriented Dialogue Systems", "abstract": "Task-oriented conversational modeling with unstructured knowledge access, as\ntrack 1 of the 9th Dialogue System Technology Challenges (DSTC 9), requests to\nbuild a system to generate response given dialogue history and knowledge\naccess. This challenge can be separated into three subtasks, (1)\nknowledge-seeking turn detection, (2) knowledge selection, and (3)\nknowledge-grounded response generation. We use pre-trained language models,\nELECTRA and RoBERTa, as our base encoder for different subtasks. For subtask 1\nand 2, the coarse-grained information like domain and entity are used to\nenhance knowledge usage. For subtask 3, we use a latent variable to encode\ndialog history and selected knowledge better and generate responses combined\nwith copy mechanism. Meanwhile, some useful post-processing strategies are\nperformed on the model's final output to make further knowledge usage in the\ngeneration task. As shown in released evaluation results, our proposed system\nranks second under objective metrics and ranks fourth under human metrics.", "published": "2020-12-22 11:36:37", "link": "http://arxiv.org/abs/2012.11937v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "g2tmn at Constraint@AAAI2021: Exploiting CT-BERT and Ensembling Learning\n  for COVID-19 Fake News Detection", "abstract": "The COVID-19 pandemic has had a huge impact on various areas of human life.\nHence, the coronavirus pandemic and its consequences are being actively\ndiscussed on social media. However, not all social media posts are truthful.\nMany of them spread fake news that cause panic among readers, misinform people\nand thus exacerbate the effect of the pandemic. In this paper, we present our\nresults at the Constraint@AAAI2021 Shared Task: COVID-19 Fake News Detection in\nEnglish. In particular, we propose our approach using the transformer-based\nensemble of COVID-Twitter-BERT (CT-BERT) models. We describe the models used,\nthe ways of text preprocessing and adding extra data. As a result, our best\nmodel achieved the weighted F1-score of 98.69 on the test set (the first place\nin the leaderboard) of this shared task that attracted 166 submitted teams in\ntotal.", "published": "2020-12-22 12:43:12", "link": "http://arxiv.org/abs/2012.11967v3", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "I.2.7; I.7.m; H.3.3"], "primary_category": "cs.CL"}
{"title": "Confronting Abusive Language Online: A Survey from the Ethical and Human\n  Rights Perspective", "abstract": "The pervasiveness of abusive content on the internet can lead to severe\npsychological and physical harm. Significant effort in Natural Language\nProcessing (NLP) research has been devoted to addressing this problem through\nabusive content detection and related sub-areas, such as the detection of hate\nspeech, toxicity, cyberbullying, etc. Although current technologies achieve\nhigh classification performance in research studies, it has been observed that\nthe real-life application of this technology can cause unintended harms, such\nas the silencing of under-represented groups. We review a large body of NLP\nresearch on automatic abuse detection with a new focus on ethical challenges,\norganized around eight established ethical principles: privacy, accountability,\nsafety and security, transparency and explainability, fairness and\nnon-discrimination, human control of technology, professional responsibility,\nand promotion of human values. In many cases, these principles relate not only\nto situational ethical codes, which may be context-dependent, but are in fact\nconnected to universal human rights, such as the right to privacy, freedom from\ndiscrimination, and freedom of expression. We highlight the need to examine the\nbroad social impacts of this technology, and to bring ethical and human rights\nconsiderations to every stage of the application life-cycle, from task\nformulation and dataset design, to model training and evaluation, to\napplication deployment. Guided by these principles, we identify several\nopportunities for rights-respecting, socio-technical solutions to detect and\nconfront online abuse, including `nudging', `quarantining', value sensitive\ndesign, counter-narratives, style transfer, and AI-driven public education\napplications.", "published": "2020-12-22 19:27:11", "link": "http://arxiv.org/abs/2012.12305v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Unboxing Engagement in YouTube Influencer Videos: An Attention-Based\n  Approach", "abstract": "Influencer marketing videos have surged in popularity, yet significant gaps\nremain in understanding the relationships between video features and\nengagement. This challenge is intensified by the complexities of interpreting\nunstructured data. While deep learning models effectively leverage raw\nunstructured data to predict engagement, they often function as black boxes\nwith limited interpretability, particularly when human validation is hindered\nby the absence of a known ground truth. To address this issue, we develop an\n'interpretable deep learning framework' that provides insights into the\nrelationships captured by the models. Inspired by visual attention in print\nadvertising, our interpretation approach uses measures of model attention to\nvideo features, eliminating spurious associations through a two-step process\nand identifying a subset of relationships for formal causal testing. This\napproach is versatile, as it applies across well-known attention mechanisms -\nadditive attention, scaled dot-product attention, and gradient-based attention\n- when analyzing text, audio, or video image data. We apply our framework to\nYouTube influencer videos, linking video features to measures of shallow and\ndeep engagement developed based on the dual-system framework of thinking. Our\nfindings guide influencers in prioritizing the design of video features\nassociated with deep engagement sentiment.", "published": "2020-12-22 19:32:52", "link": "http://arxiv.org/abs/2012.12311v5", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "On the effectiveness of signal decomposition, feature extraction and\n  selection on lung sound classification", "abstract": "Lung sounds refer to the sound generated by air moving through the\nrespiratory system. These sounds, as most biomedical signals, are non-linear\nand non-stationary. A vital part of using the lung sound for disease detection\nis discrimination between normal lung sound and abnormal lung sound. In this\npaper, several approaches for classifying between no-crackle and crackle lung\nsounds are explored. Decomposition methods such as Empirical Mode\nDecomposition, Ensemble Empirical Mode Decomposition, and Discrete Wavelet\nTransform are used along with several feature extraction techniques like\nPrincipal Component Analysis and Autoencoder, to explore how various\nclassifiers perform for the given task. An open-source dataset downloaded from\nKaggle, containing chest auscultation of varying quality is used to determine\nthe results of using the different decomposition and feature extraction\ncombinations. It is found that when higher-order statistical and spectral\nfeatures along with the Mel-frequency cepstral coefficients are fed to the\nclassier we get the best performance with the kNN classifier giving the best\naccuracy. Furthermore, it is also demonstrated that using a combination of\nfeature selection methods one can significantly reduce the number of input\nfeatures without adversely affecting the accuracy of the classifiers.", "published": "2020-12-22 00:14:48", "link": "http://arxiv.org/abs/2012.11759v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On Interfacing the Brain with Quantum Computers: An Approach to Listen\n  to the Logic of the Mind", "abstract": "This chapter presents a quantum computing-based approach to study and harness\nneuronal correlates of mental activity for the development of Brain-Computer\nInterface (BCI) systems. It introduces the notion of a logic of the mind, where\nneurophysiological data are encoded as logical expressions representing mental\nactivity. Effective logical expressions are likely to be extensive, involving\ndozens of variables. Large expressions require considerable computational power\nto be processed. This is problematic for BCI applications because they require\nfast reaction times to execute sequences of commands. Quantum computers hold\nmuch promise in terms of processing speed for some problems, including those\ninvolving logical expressions. Hence, we propose to use quantum computers to\nprocess the logic of the mind. The chapter begins with an introduction to BCI\nand the electroencephalogram, which is the neurophysiological signal that is\nnormally used in BCI. Then, it briefly discusses how the EEG corresponds to\nmental states, followed by an introduction to the logic of the mind. After\nthat, there is an overview of quantum computing, focusing on the basics deemed\nnecessary to understand how it processes logical expressions. An example of a\nBCI system is presented. In a nutshell, the system reads the EEG and builds\nlogical expressions, which are sent to a quantum computer to solve them. In\nturn, the system converts the results into sounds by means of a bespoke\nsynthesiser. Essentially, the BCI here is a musical instrument controlled by\nthe mind of the player. Our BCI is a proof-of-concept aimed at demonstrating\nhow quantum computing may support the development of sophisticated BCI systems.\nThe remaining of the chapter is devoted to technical and practical\nconsiderations on the limitations of current quantum computing hardware\ntechnology and scalability of the system.", "published": "2020-12-22 19:36:12", "link": "http://arxiv.org/abs/2101.03887v2", "categories": ["cs.ET", "cs.SD", "eess.AS", "quant-ph"], "primary_category": "cs.ET"}
{"title": "AudioViewer: Learning to Visualize Sounds", "abstract": "A long-standing goal in the field of sensory substitution is to enable sound\nperception for deaf and hard of hearing (DHH) people by visualizing audio\ncontent. Different from existing models that translate to hand sign language,\nbetween speech and text, or text and images, we target immediate and low-level\naudio to video translation that applies to generic environment sounds as well\nas human speech. Since such a substitution is artificial, without labels for\nsupervised learning, our core contribution is to build a mapping from audio to\nvideo that learns from unpaired examples via high-level constraints. For\nspeech, we additionally disentangle content from style, such as gender and\ndialect. Qualitative and quantitative results, including a human study,\ndemonstrate that our unpaired translation approach maintains important audio\nfeatures in the generated video and that videos of faces and numbers are well\nsuited for visualizing high-dimensional audio features that can be parsed by\nhumans to match and distinguish between sounds and words. Code and models are\navailable at https://chunjinsong.github.io/audioviewer", "published": "2020-12-22 21:52:45", "link": "http://arxiv.org/abs/2012.13341v5", "categories": ["cs.HC", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
