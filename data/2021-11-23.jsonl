{"title": "Deps-SAN: Neural Machine Translation with Dependency-Scaled\n  Self-Attention Network", "abstract": "Syntax knowledge contributes its powerful strength in Neural machine\ntranslation (NMT) tasks. Early NMT works supposed that syntax details can be\nautomatically learned from numerous texts via attention networks. However,\nsucceeding researches pointed out that limited by the uncontrolled nature of\nattention computation, the NMT model requires an external syntax to capture the\ndeep syntactic awareness. Although existing syntax-aware NMT methods have born\ngreat fruits in combining syntax, the additional workloads they introduced\nrender the model heavy and slow. Particularly, these efforts scarcely involve\nthe Transformer-based NMT and modify its core self-attention network (SAN). To\nthis end, we propose a parameter-free, Dependency-scaled Self-Attention Network\n(Deps-SAN) for syntax-aware Transformer-based NMT. A quantified matrix of\ndependency closeness between tokens is constructed to impose explicit syntactic\nconstraints into the SAN for learning syntactic details and dispelling the\ndispersion of attention distributions. Two knowledge sparsing techniques are\nfurther integrated to avoid the model overfitting the dependency noises\nintroduced by the external parser. Experiments and analyses on IWSLT14\nGerman-to-English and WMT16 German-to-English benchmark NMT tasks verify the\neffectiveness of our approach.", "published": "2021-11-23 08:01:21", "link": "http://arxiv.org/abs/2111.11707v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "S-SimCSE: Sampled Sub-networks for Contrastive Learning of Sentence\n  Embedding", "abstract": "Contrastive learning has been studied for improving the performance of\nlearning sentence embeddings. The current state-of-the-art method is the\nSimCSE, which takes dropout as the data augmentation method and feeds a\npre-trained transformer encoder the same input sentence twice. The\ncorresponding outputs, two sentence embeddings derived from the same sentence\nwith different dropout masks, can be used to build a positive pair. A network\nbeing applied with a dropout mask can be regarded as a sub-network of itsef,\nwhose expected scale is determined by the dropout rate. In this paper, we push\nsub-networks with different expected scales learn similar embedding for the\nsame sentence. SimCSE failed to do so because they fixed the dropout rate to a\ntuned hyperparameter. We achieve this by sampling dropout rate from a\ndistribution eatch forward process. As this method may make optimization\nharder, we also propose a simple sentence-wise mask strategy to sample more\nsub-networks. We evaluated the proposed S-SimCSE on several popular semantic\ntext similarity datasets. Experimental results show that S-SimCSE outperforms\nthe state-of-the-art SimCSE more than $1\\%$ on BERT$_{base}$", "published": "2021-11-23 09:52:45", "link": "http://arxiv.org/abs/2111.11750v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CL-NERIL: A Cross-Lingual Model for NER in Indian Languages", "abstract": "Developing Named Entity Recognition (NER) systems for Indian languages has\nbeen a long-standing challenge, mainly owing to the requirement of a large\namount of annotated clean training instances. This paper proposes an end-to-end\nframework for NER for Indian languages in a low-resource setting by exploiting\nparallel corpora of English and Indian languages and an English NER dataset.\nThe proposed framework includes an annotation projection method that combines\nword alignment score and NER tag prediction confidence score on source language\n(English) data to generate weakly labeled data in a target Indian language. We\nemploy a variant of the Teacher-Student model and optimize it jointly on the\npseudo labels of the Teacher model and predictions on the generated weakly\nlabeled data. We also present manually annotated test sets for three Indian\nlanguages: Hindi, Bengali, and Gujarati. We evaluate the performance of the\nproposed framework on the test sets of the three Indian languages. Empirical\nresults show a minimum 10% performance improvement compared to the zero-shot\ntransfer learning model on all languages. This indicates that weakly labeled\ndata generated using the proposed annotation projection method in target Indian\nlanguages can complement well-annotated source language data to enhance\nperformance. Our code is publicly available at\nhttps://github.com/aksh555/CL-NERIL", "published": "2021-11-23 12:09:15", "link": "http://arxiv.org/abs/2111.11815v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TWEETSUMM -- A Dialog Summarization Dataset for Customer Service", "abstract": "In a typical customer service chat scenario, customers contact a support\ncenter to ask for help or raise complaints, and human agents try to solve the\nissues. In most cases, at the end of the conversation, agents are asked to\nwrite a short summary emphasizing the problem and the proposed solution,\nusually for the benefit of other agents that may have to deal with the same\ncustomer or issue. The goal of the present article is advancing the automation\nof this task. We introduce the first large scale, high quality, customer care\ndialog summarization dataset with close to 6500 human annotated summaries. The\ndata is based on real-world customer support dialogs and includes both\nextractive and abstractive summaries. We also introduce a new unsupervised,\nextractive summarization method specific to dialogs.", "published": "2021-11-23 14:13:51", "link": "http://arxiv.org/abs/2111.11894v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Distributional Principles for the Semantic Study of Contextual\n  Language Models", "abstract": "Many studies were recently done for investigating the properties of\ncontextual language models but surprisingly, only a few of them consider the\nproperties of these models in terms of semantic similarity. In this article, we\nfirst focus on these properties for English by exploiting the distributional\nprinciple of substitution as a probing mechanism in the controlled context of\nSemCor and WordNet paradigmatic relations. Then, we propose to adapt the same\nmethod to a more open setting for characterizing the differences between static\nand contextual language models.", "published": "2021-11-23 22:21:16", "link": "http://arxiv.org/abs/2111.12174v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SimpleTRON: Simple Transformer with O(N) Complexity", "abstract": "In this paper, we propose that the dot product pairwise matching attention\nlayer, which is widely used in Transformer-based models, is redundant for the\nmodel performance. Attention, in its original formulation, has to be seen\nrather as a human-level tool to explore and/or visualize relevancy scores in\nsequential data. However, the way how it is constructed leads to significant\ncomputational complexity. Instead, we present SimpleTRON: Simple Transformer\nwith O(N) Complexity, a simple and fast alternative without any approximation\nthat, unlike other approximation models, does not have any architecture-related\noverhead and therefore can be seen as a purely linear Transformer-like model.\nThis architecture, to the best of our knowledge, outperforms existing\nsub-quadratic attention approximation models on several tasks from the\nLong-Range Arena benchmark. Moreover, we show, that SimpleTRON can benefit from\nweight transfer from pretrained large language models, as its parameters can be\nfully transferable.", "published": "2021-11-23 17:06:01", "link": "http://arxiv.org/abs/2111.15588v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Review of Web Infodemic Analysis and Detection Trends across\n  Multi-modalities using Deep Neural Networks", "abstract": "Fake news and misinformation are a matter of concern for people around the\nglobe. Users of the internet and social media sites encounter content with\nfalse information much frequently. Fake news detection is one of the most\nanalyzed and prominent areas of research. These detection techniques apply\npopular machine learning and deep learning algorithms. Previous work in this\ndomain covers fake news detection vastly among text circulating online.\nPlatforms that have extensively been observed and analyzed include news\nwebsites and Twitter. Facebook, Reddit, WhatsApp, YouTube, and other social\napplications are gradually gaining attention in this emerging field.\nResearchers are analyzing online data based on multiple modalities composed of\ntext, image, video, speech, and other contributing factors. The combination of\nvarious modalities has resulted in efficient fake news detection. At present,\nthere is an abundance of surveys consolidating textual fake news detection\nalgorithms. This review primarily deals with multi-modal fake news detection\ntechniques that include images, videos, and their combinations with text. We\nprovide a comprehensive literature survey of eighty articles presenting\nstate-of-the-art detection techniques, thereby identifying research gaps and\nbuilding a pathway for researchers to further advance this domain.", "published": "2021-11-23 16:02:28", "link": "http://arxiv.org/abs/2112.00803v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the application of NLP tools in mainstream participatory\n  budgeting processes in Scotland", "abstract": "In recent years participatory budgeting (PB) in Scotland has grown from a\nhandful of community-led processes to a movement supported by local and\nnational government. This is epitomized by an agreement between the Scottish\nGovernment and the Convention of Scottish Local Authorities (COSLA) that at\nleast 1% of local authority budgets will be subject to PB. This ongoing\nresearch paper explores the challenges that emerge from this 'scaling up' or\n'mainstreaming' across the 32 local authorities that make up Scotland. The main\nobjective is to evaluate local authority use of the digital platform Consul,\nwhich applies Natural Language Processing (NLP) to address these challenges.\nThis project adopts a qualitative longitudinal design with interviews,\nobservations of PB processes, and analysis of the digital platform data.\nThematic analysis is employed to capture the major issues and themes which\nemerge. Longitudinal analysis then explores how these evolve over time. The\npotential for 32 live study sites provides a unique opportunity to explore\ndiscrete political and social contexts which materialize and allow for a deeper\ndive into the challenges and issues that may exist, something a wider\ncross-sectional study would miss. Initial results show that issues and\nchallenges which come from scaling up may be tackled using NLP technology\nwhich, in a previous controlled use case-based evaluation, has shown to improve\nthe effectiveness of citizen participation.", "published": "2021-11-23 10:23:58", "link": "http://arxiv.org/abs/2111.11766v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Triple Classification for Scholarly Knowledge Graph Completion", "abstract": "Scholarly Knowledge Graphs (KGs) provide a rich source of structured\ninformation representing knowledge encoded in scientific publications. With the\nsheer volume of published scientific literature comprising a plethora of\ninhomogeneous entities and relations to describe scientific concepts, these KGs\nare inherently incomplete. We present exBERT, a method for leveraging\npre-trained transformer language models to perform scholarly knowledge graph\ncompletion. We model triples of a knowledge graph as text and perform triple\nclassification (i.e., belongs to KG or not). The evaluation shows that exBERT\noutperforms other baselines on three scholarly KG completion datasets in the\ntasks of triple classification, link prediction, and relation prediction.\nFurthermore, we present two scholarly datasets as resources for the research\ncommunity, collected from public KGs and online resources.", "published": "2021-11-23 13:16:31", "link": "http://arxiv.org/abs/2111.11845v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "A bifurcation threshold for contact-induced language change", "abstract": "One proposed mechanism of language change concerns the role played by\nsecond-language (L2) learners in situations of language contact. If\nsufficiently many L2 speakers are present in a speech community in relation to\nthe number of first-language (L1) speakers, then those features which present a\ndifficulty in L2 acquisition may be prone to disappearing from the language.\nThis paper presents a mathematical account of such contact situations based on\na stochastic model of learning and nonlinear population dynamics. The\nequilibria of a deterministic reduction of the model, describing a mixed\npopulation of L1 and L2 speakers, are fully characterized. Whether or not the\nlanguage changes in response to the introduction of L2 learners turns out to\ndepend on three factors: the overall proportion of L2 learners in the\npopulation, the strength of the difficulty speakers face in acquiring the\nlanguage as an L2, and the language-internal utilities of the competing\nlinguistic variants. These factors are related by a mathematical formula\ndescribing a phase transition from retention of the L2-difficult feature to its\nloss from both speaker populations. This supplies predictions that can be\ntested against empirical data. Here, the model is evaluated with the help of\ntwo case studies, morphological levelling in Afrikaans and the erosion of null\nsubjects in Afro-Peruvian Spanish; the model is found to be broadly in\nagreement with the historical development in both cases.", "published": "2021-11-23 18:21:12", "link": "http://arxiv.org/abs/2111.12061v2", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Variational Learning for Unsupervised Knowledge Grounded Dialogs", "abstract": "Recent methods for knowledge grounded dialogs generate responses by\nincorporating information from an external textual document. These methods do\nnot require the exact document to be known during training and rely on the use\nof a retrieval system to fetch relevant documents from a large index. The\ndocuments used to generate the responses are modeled as latent variables whose\nprior probabilities need to be estimated. Models such as RAG and REALM,\nmarginalize the document probabilities over the documents retrieved from the\nindex to define the log likelihood loss function which is optimized end-to-end.\n  In this paper, we develop a variational approach to the above technique\nwherein, we instead maximize the Evidence Lower bound (ELBO). Using a\ncollection of three publicly available open-conversation datasets, we\ndemonstrate how the posterior distribution, that has information from the\nground-truth response, allows for a better approximation of the objective\nfunction during training. To overcome the challenges associated with sampling\nover a large knowledge collection, we develop an efficient approach to\napproximate the ELBO. To the best of our knowledge we are the first to apply\nvariational training for open-scale unsupervised knowledge grounded dialog\nsystems.", "published": "2021-11-23 13:41:03", "link": "http://arxiv.org/abs/2112.00653v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SpeechMoE2: Mixture-of-Experts Model with Improved Routing", "abstract": "Mixture-of-experts based acoustic models with dynamic routing mechanisms have\nproved promising results for speech recognition. The design principle of router\narchitecture is important for the large model capacity and high computational\nefficiency. Our previous work SpeechMoE only uses local grapheme embedding to\nhelp routers to make route decisions. To further improve speech recognition\nperformance against varying domains and accents, we propose a new router\narchitecture which integrates additional global domain and accent embedding\ninto router input to promote adaptability. Experimental results show that the\nproposed SpeechMoE2 can achieve lower character error rate (CER) with\ncomparable parameters than SpeechMoE on both multi-domain and multi-accent\ntask. Primarily, the proposed method provides up to 1.6% - 4.8% relative CER\nimprovement for the multidomain task and 1.9% - 17.7% relative CER improvement\nfor the multi-accent task respectively. Besides, increasing the number of\nexperts also achieves consistent performance improvement and keeps the\ncomputational cost constant.", "published": "2021-11-23 12:53:16", "link": "http://arxiv.org/abs/2111.11831v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Romanian Speech Recognition Experiments from the ROBIN Project", "abstract": "One of the fundamental functionalities for accepting a socially assistive\nrobot is its communication capabilities with other agents in the environment.\nIn the context of the ROBIN project, situational dialogue through voice\ninteraction with a robot was investigated. This paper presents different speech\nrecognition experiments with deep neural networks focusing on producing fast\n(under 100ms latency from the network itself), while still reliable models.\nEven though one of the key desired characteristics is low latency, the final\ndeep neural network model achieves state of the art results for recognizing\nRomanian language, obtaining a 9.91% word error rate (WER), when combined with\na language model, thus improving over the previous results while offering at\nthe same time an improved runtime performance. Additionally, we explore two\nmodules for correcting the ASR output (hyphen and capitalization restoration\nand unknown words correction), targeting the ROBIN project's goals (dialogue in\nclosed micro-worlds). We design a modular architecture based on APIs allowing\nan integration engine (either in the robot or external) to chain together the\navailable modules as needed. Finally, we test the proposed design by\nintegrating it in the RELATE platform and making the ASR service available to\nweb users by either uploading a file or recording new speech.", "published": "2021-11-23 17:35:00", "link": "http://arxiv.org/abs/2111.12028v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Generating GPU Compiler Heuristics using Reinforcement Learning", "abstract": "GPU compilers are complex software programs with many optimizations specific\nto target hardware. These optimizations are often controlled by heuristics\nhand-designed by compiler experts using time- and resource-intensive processes.\nIn this paper, we developed a GPU compiler autotuning framework that uses\noff-policy deep reinforcement learning to generate heuristics that improve the\nframe rates of graphics applications. Furthermore, we demonstrate the\nresilience of these learned heuristics to frequent compiler updates by\nanalyzing their stability across a year of code check-ins without retraining.\nWe show that our machine learning-based compiler autotuning framework matches\nor surpasses the frame rates for 98% of graphics benchmarks with an average\nuplift of 1.6% up to 15.8%.", "published": "2021-11-23 18:15:34", "link": "http://arxiv.org/abs/2111.12055v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DABS: A Domain-Agnostic Benchmark for Self-Supervised Learning", "abstract": "Self-supervised learning algorithms, including BERT and SimCLR, have enabled\nsignificant strides in fields like natural language processing, computer\nvision, and speech processing. However, these algorithms are domain-specific,\nmeaning that new self-supervised learning algorithms must be developed for each\nnew setting, including myriad healthcare, scientific, and multimodal domains.\nTo catalyze progress toward domain-agnostic methods, we introduce DABS: a\nDomain-Agnostic Benchmark for Self-supervised learning. To perform well on\nDABS, an algorithm is evaluated on seven diverse domains: natural images,\nmultichannel sensor data, English text, speech recordings, multilingual text,\nchest x-rays, and images with text descriptions. Each domain contains an\nunlabeled dataset for pretraining; the model is then is scored based on its\ndownstream performance on a set of labeled tasks in the domain. We also present\ne-Mix and ShED: two baseline domain-agnostic algorithms; their relatively\nmodest performance demonstrates that significant progress is needed before\nself-supervised learning is an out-of-the-box solution for arbitrary domains.\nCode for benchmark datasets and baseline algorithms is available at\nhttps://github.com/alextamkin/dabs.", "published": "2021-11-23 18:22:14", "link": "http://arxiv.org/abs/2111.12062v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Effect of noise suppression losses on speech distortion and ASR\n  performance", "abstract": "Deep learning based speech enhancement has made rapid development towards\nimproving quality, while models are becoming more compact and usable for\nreal-time on-the-edge inference. However, the speech quality scales directly\nwith the model size, and small models are often still unable to achieve\nsufficient quality. Furthermore, the introduced speech distortion and artifacts\ngreatly harm speech quality and intelligibility, and often significantly\ndegrade automatic speech recognition (ASR) rates. In this work, we shed light\non the success of the spectral complex compressed mean squared error (MSE)\nloss, and how its magnitude and phase-aware terms are related to the speech\ndistortion vs. noise reduction trade off. We further investigate integrating\npre-trained reference-less predictors for mean opinion score (MOS) and word\nerror rate (WER), and pre-trained embeddings on ASR and sound event detection.\nOur analyses reveal that none of the pre-trained networks added significant\nperformance over the strong spectral loss.", "published": "2021-11-23 02:08:01", "link": "http://arxiv.org/abs/2111.11606v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ADTOF: A large dataset of non-synthetic music for automatic drum\n  transcription", "abstract": "The state-of-the-art methods for drum transcription in the presence of\nmelodic instruments (DTM) are machine learning models trained in a supervised\nmanner, which means that they rely on labeled datasets. The problem is that the\navailable public datasets are limited either in size or in realism, and are\nthus suboptimal for training purposes. Indeed, the best results are currently\nobtained via a rather convoluted multi-step training process that involves both\nreal and synthetic datasets. To address this issue, starting from the\nobservation that the communities of rhythm games players provide a large amount\nof annotated data, we curated a new dataset of crowdsourced drum\ntranscriptions. This dataset contains real-world music, is manually annotated,\nand is about two orders of magnitude larger than any other non-synthetic\ndataset, making it a prime candidate for training purposes. However, due to\ncrowdsourcing, the initial annotations contain mistakes. We discuss how the\nquality of the dataset can be improved by automatically correcting different\ntypes of mistakes. When used to train a popular DTM model, the dataset yields a\nperformance that matches that of the state-of-the-art for DTM, thus\ndemonstrating the quality of the annotations.", "published": "2021-11-23 09:16:17", "link": "http://arxiv.org/abs/2111.11737v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Learning Universal Audio Representations", "abstract": "The ability to learn universal audio representations that can solve diverse\nspeech, music, and environment tasks can spur many applications that require\ngeneral sound content understanding. In this work, we introduce a holistic\naudio representation evaluation suite (HARES) spanning 12 downstream tasks\nacross audio domains and provide a thorough empirical study of recent sound\nrepresentation learning systems on that benchmark. We discover that previous\nsound event classification or speech models do not generalize outside of their\ndomains. We observe that more robust audio representations can be learned with\nthe SimCLR objective; however, the model's transferability depends heavily on\nthe model architecture. We find the Slowfast architecture is good at learning\nrich representations required by different domains, but its performance is\naffected by the normalization scheme. Based on these findings, we propose a\nnovel normalizer-free Slowfast NFNet and achieve state-of-the-art performance\nacross all domains.", "published": "2021-11-23 19:47:02", "link": "http://arxiv.org/abs/2111.12124v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music Classification: Beyond Supervised Learning, Towards Real-world\n  Applications", "abstract": "Music classification is a music information retrieval (MIR) task to classify\nmusic items to labels such as genre, mood, and instruments. It is also closely\nrelated to other concepts such as music similarity and musical preference. In\nthis tutorial, we put our focus on two directions - the recent training schemes\nbeyond supervised learning and the successful application of music\nclassification models.\n  The target audience for this web book is researchers and practitioners who\nare interested in state-of-the-art music classification research and building\nreal-world applications. We assume the audience is familiar with the basic\nmachine learning concepts.\n  In this book, we present three lectures as follows: 1. Music classification\noverview: Task definition, applications, existing approaches, datasets, 2.\nBeyond supervised learning: Semi- and self-supervised learning for music\nclassification, 3. Towards real-world applications: Less-discussed, yet\nimportant research issues in practice.", "published": "2021-11-23 03:44:39", "link": "http://arxiv.org/abs/2111.11636v2", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Guided-TTS: A Diffusion Model for Text-to-Speech via Classifier Guidance", "abstract": "We propose Guided-TTS, a high-quality text-to-speech (TTS) model that does\nnot require any transcript of target speaker using classifier guidance.\nGuided-TTS combines an unconditional diffusion probabilistic model with a\nseparately trained phoneme classifier for classifier guidance. Our\nunconditional diffusion model learns to generate speech without any context\nfrom untranscribed speech data. For TTS synthesis, we guide the generative\nprocess of the diffusion model with a phoneme classifier trained on a\nlarge-scale speech recognition dataset. We present a norm-based scaling method\nthat reduces the pronunciation errors of classifier guidance in Guided-TTS. We\nshow that Guided-TTS achieves a performance comparable to that of the\nstate-of-the-art TTS model, Grad-TTS, without any transcript for LJSpeech. We\nfurther demonstrate that Guided-TTS performs well on diverse datasets including\na long-form untranscribed dataset.", "published": "2021-11-23 10:05:05", "link": "http://arxiv.org/abs/2111.11755v4", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Upsampling layers for music source separation", "abstract": "Upsampling artifacts are caused by problematic upsampling layers and due to\nspectral replicas that emerge while upsampling. Also, depending on the used\nupsampling layer, such artifacts can either be tonal artifacts (additive\nhigh-frequency noise) or filtering artifacts (substractive, attenuating some\nbands). In this work we investigate the practical implications of having\nupsampling artifacts in the resulting audio, by studying how different\nartifacts interact and assessing their impact on the models' performance. To\nthat end, we benchmark a large set of upsampling layers for music source\nseparation: different transposed and subpixel convolution setups, different\ninterpolation upsamplers (including two novel layers based on stretch and sinc\ninterpolation), and different wavelet-based upsamplers (including a novel\nlearnable wavelet layer). Our results show that filtering artifacts, associated\nwith interpolation upsamplers, are perceptually preferrable, even if they tend\nto achieve worse objective scores.", "published": "2021-11-23 10:36:28", "link": "http://arxiv.org/abs/2111.11773v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dataset of Spatial Room Impulse Responses in a Variable Acoustics Room\n  for Six Degrees-of-Freedom Rendering and Analysis", "abstract": "Room acoustics measurements are used in many areas of audio research, from\nphysical acoustics modelling and speech enhancement to virtual reality\napplications. This paper documents the technical specifications and choices\nmade in the measurement of a dataset of spatial room impulse responses (SRIRs)\nin a variable acoustics room. Two spherical microphone arrays are used: the mh\nAcoustics Eigenmike em32 and the Zylia ZM-1, capable of up to fourth- and\nthird-order Ambisonic capture, respectively. The dataset consists of three\nsource and seven receiver positions, repeated with five configurations of the\nroom's acoustics with varying levels of reverberation. Possible applications of\nthe dataset include six degrees-of-freedom (6DoF) analysis and rendering, SRIR\ninterpolation methods, and spatial dereverberation techniques.", "published": "2021-11-23 13:51:17", "link": "http://arxiv.org/abs/2111.11882v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A Contextual Latent Space Model: Subsequence Modulation in Melodic\n  Sequence", "abstract": "Some generative models for sequences such as music and text allow us to edit\nonly subsequences, given surrounding context sequences, which plays an\nimportant part in steering generation interactively. However, editing\nsubsequences mainly involves randomly resampling subsequences from a possible\ngeneration space. We propose a contextual latent space model (CLSM) in order\nfor users to be able to explore subsequence generation with a sense of\ndirection in the generation space, e.g., interpolation, as well as exploring\nvariations -- semantically similar possible subsequences. A context-informed\nprior and decoder constitute the generative model of CLSM, and a context\nposition-informed encoder is the inference model. In experiments, we use a\nmonophonic symbolic music dataset, demonstrating that our contextual latent\nspace is smoother in interpolation than baselines, and the quality of generated\nsamples is superior to baseline models. The generation examples are available\nonline.", "published": "2021-11-23 07:51:39", "link": "http://arxiv.org/abs/2111.11703v1", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
