{"title": "Question-Answer Selection in User to User Marketplace Conversations", "abstract": "Sellers in user to user marketplaces can be inundated with questions from\npotential buyers. Answers are often already available in the product\ndescription. We collected a dataset of around 590K such questions and answers\nfrom conversations in an online marketplace. We propose a question answering\nsystem that selects a sentence from the product description using a\nneural-network ranking model. We explore multiple encoding strategies, with\nrecurrent neural networks and feed-forward attention layers yielding good\nresults. This paper presents a demo to interactively pose buyer questions and\nvisualize the ranking scores of product description sentences from live online\nlistings.", "published": "2018-02-06 02:39:54", "link": "http://arxiv.org/abs/1802.01766v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Byte-Level Recursive Convolutional Auto-Encoder for Text", "abstract": "This article proposes to auto-encode text at byte-level using convolutional\nnetworks with a recursive architecture. The motivation is to explore whether it\nis possible to have scalable and homogeneous text generation at byte-level in a\nnon-sequential fashion through the simple task of auto-encoding. We show that\nnon-sequential text generation from a fixed-length representation is not only\npossible, but also achieved much better auto-encoding results than recurrent\nnetworks. The proposed model is a multi-stage deep convolutional\nencoder-decoder framework using residual connections, containing up to 160\nparameterized layers. Each encoder or decoder contains a shared group of\nmodules that consists of either pooling or upsampling layers, making the\nnetwork recursive in terms of abstraction levels in representation. Results for\n6 large-scale paragraph datasets are reported, in 3 languages including Arabic,\nChinese and English. Analyses are conducted to study several properties of the\nproposed model.", "published": "2018-02-06 06:47:09", "link": "http://arxiv.org/abs/1802.01817v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigations on Knowledge Base Embedding for Relation Prediction and\n  Extraction", "abstract": "We report an evaluation of the effectiveness of the existing knowledge base\nembedding models for relation prediction and for relation extraction on a wide\nrange of benchmarks. We also describe a new benchmark, which is much larger and\ncomplex than previous ones, which we introduce to help validate the\neffectiveness of both tasks. The results demonstrate that knowledge base\nembedding models are generally effective for relation prediction but unable to\ngive improvements for the state-of-art neural relation extraction model with\nthe existing strategies, while pointing limitations of existing methods.", "published": "2018-02-06 18:20:17", "link": "http://arxiv.org/abs/1802.02114v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-Projective Dependency Parsing via Latent Heads Representation (LHR)", "abstract": "In this paper, we introduce a novel approach based on a bidirectional\nrecurrent autoencoder to perform globally optimized non-projective dependency\nparsing via semi-supervised learning. The syntactic analysis is completed at\nthe end of the neural process that generates a Latent Heads Representation\n(LHR), without any algorithmic constraint and with a linear complexity. The\nresulting \"latent syntactic structure\" can be used directly in other semantic\ntasks. The LHR is transformed into the usual dependency tree computing a simple\nvectors similarity. We believe that our model has the potential to compete with\nmuch more complex state-of-the-art parsing architectures.", "published": "2018-02-06 18:28:45", "link": "http://arxiv.org/abs/1802.02116v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Neurobiologically Motivated Analysis of Distributional Semantic Models", "abstract": "The pervasive use of distributional semantic models or word embeddings in a\nvariety of research fields is due to their remarkable ability to represent the\nmeanings of words for both practical application and cognitive modeling.\nHowever, little has been known about what kind of information is encoded in\ntext-based word vectors. This lack of understanding is particularly problematic\nwhen word vectors are regarded as a model of semantic representation for\nabstract concepts. This paper attempts to reveal the internal information of\ndistributional word vectors by the analysis using Binder et al.'s (2016)\nbrain-based vectors, explicitly structured conceptual representations based on\nneurobiologically motivated attributes. In the analysis, the mapping from\ntext-based vectors to brain-based vectors is trained and prediction performance\nis evaluated by comparing the estimated and original brain-based vectors. The\nanalysis demonstrates that social and cognitive information is better encoded\nin text-based word vectors, but emotional information is not. This result is\ndiscussed in terms of embodied theories for abstract concepts.", "published": "2018-02-06 07:41:14", "link": "http://arxiv.org/abs/1802.01830v1", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Syst\u00e8me de traduction automatique statistique Anglais-Arabe", "abstract": "Machine translation (MT) is the process of translating text written in a\nsource language into text in a target language. In this article, we present our\nEnglish-Arabic statistical machine translation system. First, we present the\ngeneral process for setting up a statistical machine translation system, then\nwe describe the tools as well as the different corpora we used to build our MT\nsystem. Our system was evaluated in terms of the BLUE score (24.51%)", "published": "2018-02-06 16:36:44", "link": "http://arxiv.org/abs/1802.02053v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Decoding-History-Based Adaptive Control of Attention for Neural Machine\n  Translation", "abstract": "Attention-based sequence-to-sequence model has proved successful in Neural\nMachine Translation (NMT). However, the attention without consideration of\ndecoding history, which includes the past information in the decoder and the\nattention mechanism, often causes much repetition. To address this problem, we\npropose the decoding-history-based Adaptive Control of Attention (ACA) for the\nNMT model. ACA learns to control the attention by keeping track of the decoding\nhistory and the current information with a memory vector, so that the model can\ntake the translated contents and the current information into consideration.\nExperiments on Chinese-English translation and the English-Vietnamese\ntranslation have demonstrated that our model significantly outperforms the\nstrong baselines. The analysis shows that our model is capable of generating\ntranslation with less repetition and higher accuracy. The code will be\navailable at https://github.com/lancopku", "published": "2018-02-06 06:18:56", "link": "http://arxiv.org/abs/1802.01812v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Texygen: A Benchmarking Platform for Text Generation Models", "abstract": "We introduce Texygen, a benchmarking platform to support research on\nopen-domain text generation models. Texygen has not only implemented a majority\nof text generation models, but also covered a set of metrics that evaluate the\ndiversity, the quality and the consistency of the generated texts. The Texygen\nplatform could help standardize the research on text generation and facilitate\nthe sharing of fine-tuned open-source implementations among researchers for\ntheir work. As a consequence, this would help in improving the reproductivity\nand reliability of future research work in text generation.", "published": "2018-02-06 11:30:32", "link": "http://arxiv.org/abs/1802.01886v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Variational Encoder-Decoders in Dialogue Generation", "abstract": "Variational encoder-decoders (VEDs) have shown promising results in dialogue\ngeneration. However, the latent variable distributions are usually approximated\nby a much simpler model than the powerful RNN structure used for encoding and\ndecoding, yielding the KL-vanishing problem and inconsistent training\nobjective. In this paper, we separate the training step into two phases: The\nfirst phase learns to autoencode discrete texts into continuous embeddings,\nfrom which the second phase learns to generalize latent representations by\nreconstructing the encoded embedding. In this case, latent variables are\nsampled by transforming Gaussian noise through multi-layer perceptrons and are\ntrained with a separate VED model, which has the potential of realizing a much\nmore flexible distribution. We compare our model with current popular models\nand the experiment demonstrates substantial improvement in both metric-based\nand human evaluations.", "published": "2018-02-06 16:19:05", "link": "http://arxiv.org/abs/1802.02032v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How to Make Causal Inferences Using Texts", "abstract": "New text as data techniques offer a great promise: the ability to inductively\ndiscover measures that are useful for testing social science theories of\ninterest from large collections of text. We introduce a conceptual framework\nfor making causal inferences with discovered measures as a treatment or\noutcome. Our framework enables researchers to discover high-dimensional textual\ninterventions and estimate the ways that observed treatments affect text-based\noutcomes. We argue that nearly all text-based causal inferences depend upon a\nlatent representation of the text and we provide a framework to learn the\nlatent representation. But estimating this latent representation, we show,\ncreates new risks: we may introduce an identification problem or overfit. To\naddress these risks we describe a split-sample framework and apply it to\nestimate causal effects from an experiment on immigration attitudes and a study\non bureaucratic response. Our work provides a rigorous foundation for\ntext-based causal inferences.", "published": "2018-02-06 19:00:12", "link": "http://arxiv.org/abs/1802.02163v1", "categories": ["stat.ML", "cs.CL", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Efficient Large-Scale Multi-Modal Classification", "abstract": "While the incipient internet was largely text-based, the modern digital world\nis becoming increasingly multi-modal. Here, we examine multi-modal\nclassification where one modality is discrete, e.g. text, and the other is\ncontinuous, e.g. visual representations transferred from a convolutional neural\nnetwork. In particular, we focus on scenarios where we have to be able to\nclassify large quantities of data quickly. We investigate various methods for\nperforming multi-modal fusion and analyze their trade-offs in terms of\nclassification accuracy and computational efficiency. Our findings indicate\nthat the inclusion of continuous information improves performance over\ntext-only on a range of multi-modal classification tasks, even with simple\nfusion methods. In addition, we experiment with discretizing the continuous\nfeatures in order to speed up and simplify the fusion process even further. Our\nresults show that fusion with discretized features outperforms text-only\nclassification, at a fraction of the computational cost of full multi-modal\nfusion, with the additional benefit of improved interpretability.", "published": "2018-02-06 20:30:59", "link": "http://arxiv.org/abs/1802.02892v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Deep Inference of Personality Traits by Integrating Image and Word Use\n  in Social Networks", "abstract": "Social media, as a major platform for communication and information exchange,\nis a rich repository of the opinions and sentiments of 2.3 billion users about\na vast spectrum of topics. To sense the whys of certain social user's demands\nand cultural-driven interests, however, the knowledge embedded in the 1.8\nbillion pictures which are uploaded daily in public profiles has just started\nto be exploited since this process has been typically been text-based.\nFollowing this trend on visual-based social analysis, we present a novel\nmethodology based on Deep Learning to build a combined image-and-text based\npersonality trait model, trained with images posted together with words found\nhighly correlated to specific personality traits. So the key contribution here\nis to explore whether OCEAN personality trait modeling can be addressed based\non images, here called \\emph{Mind{P}ics}, appearing with certain tags with\npsychological insights. We found that there is a correlation between those\nposted images and their accompanying texts, which can be successfully modeled\nusing deep neural networks for personality estimation. The experimental results\nare consistent with previous cyber-psychology results based on texts or images.\nIn addition, classification results on some traits show that some patterns\nemerge in the set of images corresponding to a specific text, in essence to\nthose representing an abstract concept. These results open new avenues of\nresearch for further refining the proposed personality model under the\nsupervision of psychology experts.", "published": "2018-02-06 11:58:58", "link": "http://arxiv.org/abs/1802.06757v1", "categories": ["cs.CY", "cs.CL", "cs.CV"], "primary_category": "cs.CY"}
{"title": "Mining Public Opinion about Economic Issues: Twitter and the U.S.\n  Presidential Election", "abstract": "Opinion polls have been the bridge between public opinion and politicians in\nelections. However, developing surveys to disclose people's feedback with\nrespect to economic issues is limited, expensive, and time-consuming. In recent\nyears, social media such as Twitter has enabled people to share their opinions\nregarding elections. Social media has provided a platform for collecting a\nlarge amount of social media data. This paper proposes a computational public\nopinion mining approach to explore the discussion of economic issues in social\nmedia during an election. Current related studies use text mining methods\nindependently for election analysis and election prediction; this research\ncombines two text mining methods: sentiment analysis and topic modeling. The\nproposed approach has effectively been deployed on millions of tweets to\nanalyze economic concerns of people during the 2012 US presidential election.", "published": "2018-02-06 03:55:37", "link": "http://arxiv.org/abs/1802.01786v1", "categories": ["cs.SI", "cs.CL", "cs.IR", "stat.AP", "stat.ML"], "primary_category": "cs.SI"}
