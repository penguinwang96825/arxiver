{"title": "Adpositional Supersenses for Mandarin Chinese", "abstract": "This study adapts Semantic Network of Adposition and Case Supersenses (SNACS)\nannotation to Mandarin Chinese and demonstrates that the same supersense\ncategories are appropriate for Chinese adposition semantics. We annotated 15\nchapters of The Little Prince, with high interannotator agreement. The parallel\ncorpus gives insight into differences in construal between the two languages'\nadpositions, namely a number of construals that are frequent in Chinese but\nrare or unattested in the English corpus. The annotated corpus can further\nsupport automatic disambiguation of adpositions in Chinese, and the common\ninventory of supersenses between the two languages can potentially serve\ncross-linguistic tasks such as machine translation.", "published": "2018-12-06 02:54:49", "link": "http://arxiv.org/abs/1812.02317v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning with Multi-View Attention for Answer Selection and\n  Knowledge Base Question Answering", "abstract": "Answer selection and knowledge base question answering (KBQA) are two\nimportant tasks of question answering (QA) systems. Existing methods solve\nthese two tasks separately, which requires large number of repetitive work and\nneglects the rich correlation information between tasks. In this paper, we\ntackle answer selection and KBQA tasks simultaneously via multi-task learning\n(MTL), motivated by the following motivations. First, both answer selection and\nKBQA can be regarded as a ranking problem, with one at text-level while the\nother at knowledge-level. Second, these two tasks can benefit each other:\nanswer selection can incorporate the external knowledge from knowledge base\n(KB), while KBQA can be improved by learning contextual information from answer\nselection. To fulfill the goal of jointly learning these two tasks, we propose\na novel multi-task learning scheme that utilizes multi-view attention learned\nfrom various perspectives to enable these tasks to interact with each other as\nwell as learn more comprehensive sentence representations. The experiments\nconducted on several real-world datasets demonstrate the effectiveness of the\nproposed method, and the performance of answer selection and KBQA is improved.\nAlso, the multi-view attention scheme is proved to be effective in assembling\nattentive information from different representational perspectives.", "published": "2018-12-06 05:20:59", "link": "http://arxiv.org/abs/1812.02354v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the importance of context and embeddings in neural NER models\n  for task-oriented dialogue systems", "abstract": "Named Entity Recognition (NER), a classic sequence labelling task, is an\nessential component of natural language understanding (NLU) systems in\ntask-oriented dialog systems for slot filling. For well over a decade,\ndifferent methods from lookup using gazetteers and domain ontology, classifiers\nover handcrafted features to end-to-end systems involving neural network\narchitectures have been evaluated mostly in language-independent\nnon-conversational settings. In this paper, we evaluate a modified version of\nthe recent state of the art neural architecture in a conversational setting\nwhere messages are often short and noisy. We perform an array of experiments\nwith different combinations of including the previous utterance in the dialogue\nas a source of additional features and using word and character level\nembeddings trained on a larger external corpus. All methods are evaluated on a\ncombined dataset formed from two public English task-oriented conversational\ndatasets belonging to travel and restaurant domains respectively. For\nadditional evaluation, we also repeat some of our experiments after adding\nautomatically translated and transliterated (from translated) versions to the\nEnglish only dataset.", "published": "2018-12-06 06:53:37", "link": "http://arxiv.org/abs/1812.02370v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Streaming Keyword Spotting", "abstract": "We present a system for keyword spotting that, except for a frontend\ncomponent for feature generation, it is entirely contained in a deep neural\nnetwork (DNN) model trained \"end-to-end\" to predict the presence of the keyword\nin a stream of audio. The main contributions of this work are, first, an\nefficient memoized neural network topology that aims at making better use of\nthe parameters and associated computations in the DNN by holding a memory of\nprevious activations distributed over the depth of the DNN. The second\ncontribution is a method to train the DNN, end-to-end, to produce the keyword\nspotting score. This system significantly outperforms previous approaches both\nin terms of quality of detection as well as size and computation.", "published": "2018-12-06 21:00:58", "link": "http://arxiv.org/abs/1812.02802v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pathology Extraction from Chest X-Ray Radiology Reports: A Performance\n  Study", "abstract": "Extraction of relevant pathological terms from radiology reports is important\nfor correct image label generation and disease population studies. In this\nletter, we compare the performance of some known application program interface\n(APIs) for the task of thoracic abnormality extraction from radiology reports.\nWe explored several medical domain specific annotation tools like Medical Text\nIndexer(MTI) with Non-MEDLINE and Mesh On Demand(MOD) options and generic\nNatural Language Understanding (NLU) API provided by the IBM cloud. Our results\nshow that although MTI and MOD are intended for extracting medical terms, their\nperformance is worst compared to generic extraction API like IBM NLU. Finally,\nwe trained a DNN-based Named Entity Recognition (NER) model to extract the key\nconcept words from radiology reports. Our model outperforms the medical\nspecific and generic API performance by a large margin. Our results demonstrate\nthe inadequacy of generic APIs for pathology extraction task and establish the\nimportance of domain specific model training for improved results. We hope that\nthese results motivate the research community to release larger de-identified\nradiology reports corpus for building high accuracy machine learning models for\nthe important task of pathology extraction.", "published": "2018-12-06 06:56:16", "link": "http://arxiv.org/abs/1812.02305v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Evaluating Architectural Choices for Deep Learning Approaches for\n  Question Answering over Knowledge Bases", "abstract": "The task of answering natural language questions over knowledge bases has\nreceived wide attention in recent years. Various deep learning architectures\nhave been proposed for this task. However, architectural design choices are\ntypically not systematically compared nor evaluated under the same conditions.\nIn this paper, we contribute to a better understanding of the impact of\narchitectural design choices by evaluating four different architectures under\nthe same conditions. We address the task of answering simple questions,\nconsisting in predicting the subject and predicate of a triple given a\nquestion. In order to provide a fair comparison of different architectures, we\nevaluate them under the same strategy for inferring the subject, and compare\ndifferent architectures for inferring the predicate. The architecture for\ninferring the subject is based on a standard LSTM model trained to recognize\nthe span of the subject in the question and on a linking component that links\nthe subject span to an entity in the knowledge base. The architectures for\npredicate inference are based on i) a standard softmax classifier ranging over\nall predicates as output, iii) a model that predicts a low-dimensional encoding\nof the property given entity representation and question, iii) a model that\nlearns to score a pair of subject and predicate given the question as well as\niv) a model based on the well-known FastText model. The comparison of\narchitectures shows that FastText provides better results than other\narchitectures.", "published": "2018-12-06 14:11:25", "link": "http://arxiv.org/abs/1812.02536v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Relevant Word Order Vectorization for Improved Natural Language\n  Processing in Electronic Healthcare Records", "abstract": "Objective: Electronic health records (EHR) represent a rich resource for\nconducting observational studies, supporting clinical trials, and more.\nHowever, much of the relevant information is stored in an unstructured format\nthat makes it difficult to use. Natural language processing approaches that\nattempt to automatically classify the data depend on vectorization algorithms\nthat impose structure on the text, but these algorithms were not designed for\nthe unique characteristics of EHR. Here, we propose a new algorithm for\nstructuring so-called free-text that may help researchers make better use of\nEHR. We call this method Relevant Word Order Vectorization (RWOV).\n  Materials and Methods: As a proof-of-concept, we attempted to classify the\nhormone receptor status of breast cancer patients treated at the University of\nKansas Medical Center during a recent year, from the unstructured text of\npathology reports. Our approach attempts to account for the semi-structured way\nthat healthcare providers often enter information. We compared this approach to\nthe ngrams and word2vec methods.\n  Results: Our approach resulted in the most consistently high accuracy, as\nmeasured by F1 score and area under the receiver operating characteristic curve\n(AUC).\n  Discussion: Our results suggest that methods of structuring free text that\ntake into account its context may show better performance, and that our\napproach is promising.\n  Conclusion: By using a method that accounts for the fact that healthcare\nproviders tend to use certain key words repetitively and that the order of\nthese key words is important, we showed improved performance over methods that\ndo not.", "published": "2018-12-06 16:01:13", "link": "http://arxiv.org/abs/1812.02627v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Feature Analysis for Assessing the Quality of Wikipedia Articles through\n  Supervised Classification", "abstract": "Nowadays, thanks to Web 2.0 technologies, people have the possibility to\ngenerate and spread contents on different social media in a very easy way. In\nthis context, the evaluation of the quality of the information that is\navailable online is becoming more and more a crucial issue. In fact, a constant\nflow of contents is generated every day by often unknown sources, which are not\ncertified by traditional authoritative entities. This requires the development\nof appropriate methodologies that can evaluate in a systematic way these\ncontents, based on `objective' aspects connected with them. This would help\nindividuals, who nowadays tend to increasingly form their opinions based on\nwhat they read online and on social media, to come into contact with\ninformation that is actually useful and verified. Wikipedia is nowadays one of\nthe biggest online resources on which users rely as a source of information.\nThe amount of collaboratively generated content that is sent to the online\nencyclopedia every day can let to the possible creation of low-quality articles\n(and, consequently, misinformation) if not properly monitored and revised. For\nthis reason, in this paper, the problem of automatically assessing the quality\nof Wikipedia articles is considered. In particular, the focus is on the\nanalysis of hand-crafted features that can be employed by supervised machine\nlearning techniques to perform the classification of Wikipedia articles on\nqualitative bases. With respect to prior literature, a wider set of\ncharacteristics connected to Wikipedia articles are taken into account and\nillustrated in detail. Evaluations are performed by considering a labeled\ndataset provided in a prior work, and different supervised machine learning\nalgorithms, which produced encouraging results with respect to the considered\nfeatures.", "published": "2018-12-06 16:45:00", "link": "http://arxiv.org/abs/1812.02655v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generation of Synthetic Electronic Medical Record Text", "abstract": "Machine learning (ML) and Natural Language Processing (NLP) have achieved\nremarkable success in many fields and have brought new opportunities and high\nexpectation in the analyses of medical data. The most common type of medical\ndata is the massive free-text electronic medical records (EMR). It is widely\nregarded that mining such massive data can bring up important information for\nimproving medical practices as well as for possible new discoveries on complex\ndiseases. However, the free EMR texts are lacking consistent standards, rich of\nprivate information, and limited in availability. Also, as they are accumulated\nfrom everyday practices, it is often hard to have a balanced number of samples\nfor the types of diseases under study. These problems hinder the development of\nML and NLP methods for EMR data analysis. To tackle these problems, we\ndeveloped a model to generate synthetic text of EMRs called Medical Text\nGenerative Adversarial Network or mtGAN. It is based on the GAN framework and\nis trained by the REINFORCE algorithm. It takes disease features as inputs and\ngenerates synthetic texts as EMRs for the corresponding diseases. We evaluate\nthe model from micro-level, macro-level and application-level on a Chinese EMR\ntext dataset. The results show that the method has a good capacity to fit real\ndata and can generate realistic and diverse EMR samples. This provides a novel\nway to avoid potential leakage of patient privacy while still supply sufficient\nwell-controlled cohort data for developing downstream ML and NLP methods. It\ncan also be used as a data augmentation method to assist studies based on real\nEMR data.", "published": "2018-12-06 20:35:14", "link": "http://arxiv.org/abs/1812.02793v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The USTC-NEL Speech Translation system at IWSLT 2018", "abstract": "This paper describes the USTC-NEL system to the speech translation task of\nthe IWSLT Evaluation 2018. The system is a conventional pipeline system which\ncontains 3 modules: speech recognition, post-processing and machine\ntranslation. We train a group of hybrid-HMM models for our speech recognition,\nand for machine translation we train transformer based neural machine\ntranslation models with speech recognition output style text as input.\nExperiments conducted on the IWSLT 2018 task indicate that, compared to\nbaseline system from KIT, our system achieved 14.9 BLEU improvement.", "published": "2018-12-06 10:57:29", "link": "http://arxiv.org/abs/1812.02455v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Context is Key: New Approaches to Neural Coherence Modeling", "abstract": "We formulate coherence modeling as a regression task and propose two novel\nmethods to combine techniques from our setup with pairwise approaches. The\nfirst of our methods is a model that we call \"first-next,\" which operates\nsimilarly to selection sorting but conditions decision-making on information\nabout already-sorted sentences. The second consists of a technique for adding\ncontext to regression-based models by concatenating sentence-level\nrepresentations with an encoding of its corresponding out-of-order paragraph.\nThis latter model achieves Kendall-tau distance and positional accuracy scores\nthat match or exceed the current state-of-the-art on these metrics. Our results\nsuggest that many of the gains that come from more complex, machine-translation\ninspired approaches can be achieved with simpler, more efficient models.", "published": "2018-12-06 03:16:06", "link": "http://arxiv.org/abs/1812.04722v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Generative Adversarial Network based Speaker Adaptation for High\n  Fidelity WaveNet Vocoder", "abstract": "Although state-of-the-art parallel WaveNet has addressed the issue of\nreal-time waveform generation, there remains problems. Firstly, due to the\nnoisy input signal of the model, there is still a gap between the quality of\ngenerated and natural waveforms. Secondly, a parallel WaveNet is trained under\na distillation framework, which makes it tedious to adapt a well trained model\nto a new speaker. To address these two problems, in this paper we propose an\nend-to-end adaptation method based on the generative adversarial network (GAN),\nwhich can reduce the computational cost for the training of new speaker\nadaptation. Our subjective experiments shows that the proposed training method\ncan further reduce the quality gap between generated and natural waveforms.", "published": "2018-12-06 03:54:41", "link": "http://arxiv.org/abs/1812.02339v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Binaural Source Localization based on Modulation-Domain Features and\n  Decision Pooling", "abstract": "In this work we apply Amplitude Modulation Spectrum (AMS) features to the\nsource localization problem. Our approach computes 36 bilateral features for 2s\nlong signal segments and estimates the azimuthal directions of a sound source\nthrough a binaurally trained classifier. This directional information of a\nsound source could be e.g. used to steer the beamformer in a hearing aid to the\nsource of interest in order to increase the SNR. We evaluated our approach on\nthe development set of the IEEE-AASP Challenge on sound source localization and\ntracking (LOCATA) and achieved a 4.25{\\deg} smaller MAE than the baseline\napproach. Additionally, our approach is computationally less complex.", "published": "2018-12-06 08:30:56", "link": "http://arxiv.org/abs/1812.02399v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pitch-synchronous DCT features: A pilot study on speaker identification", "abstract": "We propose a new feature, namely, pitchsynchronous discrete cosine transform\n(PS-DCT), for the task of speaker identification. These features are obtained\ndirectly from the voiced segments of the speech signal, without any preemphasis\nor windowing. The feature vectors are vector quantized, to create one separate\ncodebook for each speaker during training. The performance of the PS-DCT\nfeatures is shown to be good, and hence it can be used to supplement other\nfeatures for the speaker identification task. Speaker identification is also\nperformed using Mel-frequency cepstral coefficient (MFCC) features and combined\nwith the proposed features to improve its performance. For this pilot study, 30\nspeakers (14 female and 16 male) have been picked up randomly from the TIMIT\ndatabase for the speaker identification task. On this data, both the proposed\nfeatures and MFCC give an identification accuracy of 90% and 96.7% for codebook\nsizes of 16 and 32, respectively, and the combined features achieve 100%\nperformance. Apart from the speaker identification task, this work also shows\nthe capability of DCT to capture discriminative information from the speech\nsignal with minimal pre-processing.", "published": "2018-12-06 10:35:47", "link": "http://arxiv.org/abs/1812.02447v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Frequency Tracking: LMS and RLS Applied to Speech Formant Estimation\n  (2000)", "abstract": "Introduction Several speech processing algorithms assume the signal is\nstationary during short intervals (approximately 20 to 30 ms). This assumption\nis valid for several applications, but it is too restrictive in some contexts.\nThis work investigates the application of adaptive signal processing to the\nproblem of estimating the formant frequencies of speech. Two algorithms were\nimplemented and tested. The first one is the conventional Least-Mean-Square\n(LMS) algorithm, and the second is the conventional Recursive Least-Squares\n(RLS) algorithm. The formant frequencies are the resonant frequencies of the\nvocal tract. The speech is the result of the convolution between the excitation\nand the vocal tract impulse response [Rabiner, 78], thus a kind of\n\"deconvolution\" is required to recover the formants. This is not an easy\nproblem because one does not have the excitation signal available. There are\nseveral algorithms for formant estimation [Rabiner, 78], [Snell, 93], [Laprie,\n94", "published": "2018-12-06 18:39:45", "link": "http://arxiv.org/abs/1812.02705v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
