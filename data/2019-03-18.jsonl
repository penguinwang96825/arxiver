{"title": "What You Say and How You Say it: Joint Modeling of Topics and Discourse\n  in Microblog Conversations", "abstract": "This paper presents an unsupervised framework for jointly modeling topic\ncontent and discourse behavior in microblog conversations. Concretely, we\npropose a neural model to discover word clusters indicating what a conversation\nconcerns (i.e., topics) and those reflecting how participants voice their\nopinions (i.e., discourse). Extensive experiments show that our model can yield\nboth coherent topics and meaningful discourse behavior. Further study shows\nthat our topic and discourse representations can benefit the classification of\nmicroblog messages, especially when they are jointly trained with the\nclassifier.", "published": "2019-03-18 09:21:30", "link": "http://arxiv.org/abs/1903.07319v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neutron: An Implementation of the Transformer Translation Model and its\n  Variants", "abstract": "The Transformer translation model is easier to parallelize and provides\nbetter performance compared to recurrent seq2seq models, which makes it popular\namong industry and research community. We implement the Neutron in this work,\nincluding the Transformer model and its several variants from most recent\nresearches. It is highly optimized, easy to modify and provides comparable\nperformance with interesting features while keeping readability.", "published": "2019-03-18 12:54:22", "link": "http://arxiv.org/abs/1903.07402v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The emergence of number and syntax units in LSTM language models", "abstract": "Recent work has shown that LSTMs trained on a generic language modeling\nobjective capture syntax-sensitive generalizations such as long-distance number\nagreement. We have however no mechanistic understanding of how they accomplish\nthis remarkable feat. Some have conjectured it depends on heuristics that do\nnot truly take hierarchical structure into account. We present here a detailed\nstudy of the inner mechanics of number tracking in LSTMs at the single neuron\nlevel. We discover that long-distance number information is largely managed by\ntwo `number units'. Importantly, the behaviour of these units is partially\ncontrolled by other units independently shown to track syntactic structure. We\nconclude that LSTMs are, to some extent, implementing genuinely syntactic\nprocessing mechanisms, paving the way to a more general understanding of\ngrammatical encoding in LSTMs.", "published": "2019-03-18 13:38:54", "link": "http://arxiv.org/abs/1903.07435v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multilingual Encoding Method for Text Classification and Dialect\n  Identification Using Convolutional Neural Network", "abstract": "This thesis presents a language-independent text classification model by\nintroduced two new encoding methods \"BUNOW\" and \"BUNOC\" used for feeding the\nraw text data into a new CNN spatial architecture with vertical and horizontal\nconvolutional process instead of commonly used methods like one hot vector or\nword representation (i.e. word2vec) with temporal CNN architecture. The\nproposed model can be classified as hybrid word-character model in its work\nmethodology because it consumes less memory space by using a fewer neural\nnetwork parameters as in character level representation, in addition to\nproviding much faster computations with fewer network layers depth, as in word\nlevel representation. A promising result achieved compared to state of art\nmodels in two different morphological benchmarked dataset one for Arabic\nlanguage and one for English language.", "published": "2019-03-18 17:31:14", "link": "http://arxiv.org/abs/1903.07588v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Updated Duet Model for Passage Re-ranking", "abstract": "We propose several small modifications to Duet---a deep neural ranking\nmodel---and evaluate the updated model on the MS MARCO passage ranking task. We\nreport significant improvements from the proposed changes based on an ablation\nstudy.", "published": "2019-03-18 18:44:07", "link": "http://arxiv.org/abs/1903.07666v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "CRAFT: A multifunction online platform for speech prosody visualisation", "abstract": "There are many research tools which are also used for teaching the acoustic\nphonetics of speech rhythm and speech melody. But they were not\npurpose-designed for teaching-learning situations, and some have a steep\nlearning curve. CRAFT (Creation and Recovery of Amplitude and Frequency Tracks)\nis custom-designed as a novel flexible online tool for visualisation and\ncritical comparison of functions and transforms, with implementations of the\nReaper, RAPT, PyRapt, YAAPT, YIN and PySWIPE F0 estimators, three Praat\nconfigurations, and two purpose-built estimators, PyAMDF, S0FT. Visualisations\nof amplitude and frequency envelope spectra, spectral edge detection of rhythm\nzones, and a parametrised spectrogram are included. A selection of audio clips\nfrom tone and intonation languages is provided for demonstration purposes. The\nmain advantages of online tools are consistency (users have the same version\nand the same data selection), interoperability over different platforms, and\nease of maintenance. The code is available on GitHub.", "published": "2019-03-18 11:35:10", "link": "http://arxiv.org/abs/1903.08718v1", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Effects of padding on LSTMs and CNNs", "abstract": "Long Short-Term Memory (LSTM) Networks and Convolutional Neural Networks\n(CNN) have become very common and are used in many fields as they were\neffective in solving many problems where the general neural networks were\ninefficient. They were applied to various problems mostly related to images and\nsequences. Since LSTMs and CNNs take inputs of the same length and dimension,\ninput images and sequences are padded to maximum length while testing and\ntraining. This padding can affect the way the networks function and can make a\ngreat deal when it comes to performance and accuracies. This paper studies this\nand suggests the best way to pad an input sequence. This paper uses a simple\nsentiment analysis task for this purpose. We use the same dataset on both the\nnetworks with various padding to show the difference. This paper also discusses\nsome preprocessing techniques done on the data to ensure effective analysis of\nthe data.", "published": "2019-03-18 07:52:59", "link": "http://arxiv.org/abs/1903.07288v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "An Effective Label Noise Model for DNN Text Classification", "abstract": "Because large, human-annotated datasets suffer from labeling errors, it is\ncrucial to be able to train deep neural networks in the presence of label\nnoise. While training image classification models with label noise have\nreceived much attention, training text classification models have not. In this\npaper, we propose an approach to training deep networks that is robust to label\nnoise. This approach introduces a non-linear processing layer (noise model)\nthat models the statistics of the label noise into a convolutional neural\nnetwork (CNN) architecture. The noise model and the CNN weights are learned\njointly from noisy training data, which prevents the model from overfitting to\nerroneous labels. Through extensive experiments on several text classification\ndatasets, we show that this approach enables the CNN to learn better sentence\nrepresentations and is robust even to extreme label noise. We find that proper\ninitialization and regularization of this noise model is critical. Further, by\ncontrast to results focusing on large batch sizes for mitigating label noise\nfor image classification, we find that altering the batch size does not have\nmuch effect on classification performance.", "published": "2019-03-18 15:27:50", "link": "http://arxiv.org/abs/1903.07507v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Lemotif: An Affective Visual Journal Using Deep Neural Networks", "abstract": "We present Lemotif, an integrated natural language processing and image\ngeneration system that uses machine learning to (1) parse a text-based input\njournal entry describing the user's day for salient themes and emotions and (2)\nvisualize the detected themes and emotions in creative and appealing image\nmotifs. Synthesizing approaches from artificial intelligence and psychology,\nLemotif acts as an affective visual journal, encouraging users to regularly\nwrite and reflect on their daily experiences through visual reinforcement. By\nmaking patterns in emotions and their sources more apparent, Lemotif aims to\nhelp users better understand their emotional lives, identify opportunities for\naction, and track the effectiveness of behavioral changes over time. We verify\nvia human studies that prospective users prefer motifs generated by Lemotif\nover corresponding baselines, find the motifs representative of their journal\nentries, and think they would be more likely to journal regularly using a\nLemotif-based app.", "published": "2019-03-18 23:35:31", "link": "http://arxiv.org/abs/1903.07766v3", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Sentiment Analysis on IMDB Movie Comments and Twitter Data by Machine\n  Learning and Vector Space Techniques", "abstract": "This study's goal is to create a model of sentiment analysis on a 2000 rows\nIMDB movie comments and 3200 Twitter data by using machine learning and vector\nspace techniques; positive or negative preliminary information about the text\nis to provide. In the study, a vector space was created in the KNIME Analytics\nplatform, and a classification study was performed on this vector space by\nDecision Trees, Na\\\"ive Bayes and Support Vector Machines classification\nalgorithms. The conclusions obtained were compared in terms of each algorithms.\nThe classification results for IMDB movie comments are obtained as 94,00%,\n73,20%, and 85,50% by Decision Tree, Naive Bayes and SVM algorithms. The\nclassification results for Twitter data set are presented as 82,76%, 75,44% and\n72,50% by Decision Tree, Naive Bayes SVM algorithms as well. It is seen that\nthe best classification results presented in both data sets are which\ncalculated by SVM algorithm.", "published": "2019-03-18 09:25:10", "link": "http://arxiv.org/abs/1903.11983v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "A Vocoder Based Method For Singing Voice Extraction", "abstract": "This paper presents a novel method for extracting the vocal track from a\nmusical mixture. The musical mixture consists of a singing voice and a backing\ntrack which may comprise of various instruments. We use a convolutional network\nwith skip and residual connections as well as dilated convolutions to estimate\nvocoder parameters, given the spectrogram of an input mixture. The estimated\nparameters are then used to synthesize the vocal track, without any\ninterference from the backing track. We evaluate our system, through objective\nmetrics pertinent to audio quality and interference from background sources,\nand via a comparative subjective evaluation. We use open-source source\nseparation systems based on Non-negative Matrix Factorization (NMFs) and Deep\nLearning methods as benchmarks for our system and discuss future applications\nfor this particular algorithm.", "published": "2019-03-18 16:46:48", "link": "http://arxiv.org/abs/1903.07554v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Counterpoint by Convolution", "abstract": "Machine learning models of music typically break up the task of composition\ninto a chronological process, composing a piece of music in a single pass from\nbeginning to end. On the contrary, human composers write music in a nonlinear\nfashion, scribbling motifs here and there, often revisiting choices previously\nmade. In order to better approximate this process, we train a convolutional\nneural network to complete partial musical scores, and explore the use of\nblocked Gibbs sampling as an analogue to rewriting. Neither the model nor the\ngenerative procedure are tied to a particular causal direction of composition.\nOur model is an instance of orderless NADE (Uria et al., 2014), which allows\nmore direct ancestral sampling. However, we find that Gibbs sampling greatly\nimproves sample quality, which we demonstrate to be due to some conditional\ndistributions being poorly modeled. Moreover, we show that even the cheap\napproximate blocked Gibbs procedure from Yao et al. (2014) yields better\nsamples than ancestral sampling, based on both log-likelihood and human\nevaluation.", "published": "2019-03-18 02:04:23", "link": "http://arxiv.org/abs/1903.07227v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML", "H.5.5; I.2"], "primary_category": "cs.LG"}
{"title": "Practical Hidden Voice Attacks against Speech and Speaker Recognition\n  Systems", "abstract": "Voice Processing Systems (VPSes), now widely deployed, have been made\nsignificantly more accurate through the application of recent advances in\nmachine learning. However, adversarial machine learning has similarly advanced\nand has been used to demonstrate that VPSes are vulnerable to the injection of\nhidden commands - audio obscured by noise that is correctly recognized by a VPS\nbut not by human beings. Such attacks, though, are often highly dependent on\nwhite-box knowledge of a specific machine learning model and limited to\nspecific microphones and speakers, making their use across different acoustic\nhardware platforms (and thus their practicality) limited. In this paper, we\nbreak these dependencies and make hidden command attacks more practical through\nmodel-agnostic (blackbox) attacks, which exploit knowledge of the signal\nprocessing algorithms commonly used by VPSes to generate the data fed into\nmachine learning systems. Specifically, we exploit the fact that multiple\nsource audio samples have similar feature vectors when transformed by acoustic\nfeature extraction algorithms (e.g., FFTs). We develop four classes of\nperturbations that create unintelligible audio and test them against 12 machine\nlearning models, including 7 proprietary models (e.g., Google Speech API, Bing\nSpeech API, IBM Speech API, Azure Speaker API, etc), and demonstrate successful\nattacks against all targets. Moreover, we successfully use our maliciously\ngenerated audio samples in multiple hardware configurations, demonstrating\neffectiveness across both models and real systems. In so doing, we demonstrate\nthat domain-specific knowledge of audio signal processing represents a\npractical means of generating successful hidden voice command attacks.", "published": "2019-03-18 20:10:13", "link": "http://arxiv.org/abs/1904.05734v1", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
