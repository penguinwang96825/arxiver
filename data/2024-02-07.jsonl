{"title": "Share What You Already Know: Cross-Language-Script Transfer and\n  Alignment for Sentiment Detection in Code-Mixed Data", "abstract": "Code-switching entails mixing multiple languages. It is an increasingly\noccurring phenomenon in social media texts. Usually, code-mixed texts are\nwritten in a single script, even though the languages involved have different\nscripts. Pre-trained multilingual models primarily utilize the data in the\nnative script of the language. In existing studies, the code-switched texts are\nutilized as they are. However, using the native script for each language can\ngenerate better representations of the text owing to the pre-trained knowledge.\nTherefore, a cross-language-script knowledge sharing architecture utilizing the\ncross attention and alignment of the representations of text in individual\nlanguage scripts was proposed in this study. Experimental results on two\ndifferent datasets containing Nepali-English and Hindi-English code-switched\ntexts, demonstrate the effectiveness of the proposed method. The interpretation\nof the model using model explainability technique illustrates the sharing of\nlanguage-specific knowledge between language-specific representations.", "published": "2024-02-07 02:59:18", "link": "http://arxiv.org/abs/2402.04542v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised\n  Fine-tuning Dataset", "abstract": "Open-source large language models (LLMs) have gained significant strength\nacross diverse fields. Nevertheless, the majority of studies primarily\nconcentrate on English, with only limited exploration into the realm of\nmultilingual abilities. In this work, we therefore construct an open-source\nmultilingual supervised fine-tuning dataset. Different from previous works that\nsimply translate English instructions, we consider both the language-specific\nand language-agnostic abilities of LLMs. Firstly, we introduce a\nknowledge-grounded data augmentation approach to elicit more language-specific\nknowledge of LLMs, improving their ability to serve users from different\ncountries. Moreover, we find modern LLMs possess strong cross-lingual transfer\ncapabilities, thus repeatedly learning identical content in various languages\nis not necessary. Consequently, we can substantially prune the\nlanguage-agnostic supervised fine-tuning (SFT) data without any performance\ndegradation, making multilingual SFT more efficient. The resulting UltraLink\ndataset comprises approximately 1 million samples across five languages (i.e.,\nEn, Zh, Ru, Fr, Es), and the proposed data construction method can be easily\nextended to other languages. UltraLink-LM, which is trained on UltraLink,\noutperforms several representative baselines across many tasks.", "published": "2024-02-07 05:05:53", "link": "http://arxiv.org/abs/2402.04588v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Cross-Domain Low-Resource Text Generation through LLM\n  Post-Editing: A Programmer-Interpreter Approach", "abstract": "Post-editing has proven effective in improving the quality of text generated\nby large language models (LLMs) such as GPT-3.5 or GPT-4, particularly when\ndirect updating of their parameters to enhance text quality is infeasible or\nexpensive. However, relying solely on smaller language models for post-editing\ncan limit the LLMs' ability to generalize across domains. Moreover, the editing\nstrategies in these methods are not optimally designed for text-generation\ntasks. To address these limitations, we propose a neural programmer-interpreter\napproach that preserves the domain generalization ability of LLMs when editing\ntheir output. The editing actions in this framework are specifically devised\nfor text generation. Extensive experiments demonstrate that the\nprogrammer-interpreter significantly enhances GPT-3.5's performance in logical\nform-to-text conversion and low-resource machine translation, surpassing other\nstate-of-the-art (SOTA) LLM post-editing methods in cross-domain settings.", "published": "2024-02-07 06:13:14", "link": "http://arxiv.org/abs/2402.04609v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations\n  from Large Language Models", "abstract": "Large Language Models (LLMs) are deployed as powerful tools for several\nnatural language processing (NLP) applications. Recent works show that modern\nLLMs can generate self-explanations (SEs), which elicit their intermediate\nreasoning steps for explaining their behavior. Self-explanations have seen\nwidespread adoption owing to their conversational and plausible nature.\nHowever, there is little to no understanding of their faithfulness. In this\nwork, we discuss the dichotomy between faithfulness and plausibility in SEs\ngenerated by LLMs. We argue that while LLMs are adept at generating plausible\nexplanations -- seemingly logical and coherent to human users -- these\nexplanations do not necessarily align with the reasoning processes of the LLMs,\nraising concerns about their faithfulness. We highlight that the current trend\ntowards increasing the plausibility of explanations, primarily driven by the\ndemand for user-friendly interfaces, may come at the cost of diminishing their\nfaithfulness. We assert that the faithfulness of explanations is critical in\nLLMs employed for high-stakes decision-making. Moreover, we emphasize the need\nfor a systematic characterization of faithfulness-plausibility requirements of\ndifferent real-world applications and ensure explanations meet those needs.\nWhile there are several approaches to improving plausibility, improving\nfaithfulness is an open challenge. We call upon the community to develop novel\nmethods to enhance the faithfulness of self explanations thereby enabling\ntransparent deployment of LLMs in diverse high-stakes settings.", "published": "2024-02-07 06:32:50", "link": "http://arxiv.org/abs/2402.04614v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MEMORYLLM: Towards Self-Updatable Large Language Models", "abstract": "Existing Large Language Models (LLMs) usually remain static after deployment,\nwhich might make it hard to inject new knowledge into the model. We aim to\nbuild models containing a considerable portion of self-updatable parameters,\nenabling the model to integrate new knowledge effectively and efficiently. To\nthis end, we introduce MEMORYLLM, a model that comprises a transformer and a\nfixed-size memory pool within the latent space of the transformer. MEMORYLLM\ncan self-update with text knowledge and memorize the knowledge injected\nearlier. Our evaluations demonstrate the ability of MEMORYLLM to effectively\nincorporate new knowledge, as evidenced by its performance on model editing\nbenchmarks. Meanwhile, the model exhibits long-term information retention\ncapacity, which is validated through our custom-designed evaluations and\nlong-context benchmarks. MEMORYLLM also shows operational integrity without any\nsign of performance degradation even after nearly a million memory updates. Our\ncode and model are open-sourced at https://github.com/wangyu-ustc/MemoryLLM.", "published": "2024-02-07 07:14:11", "link": "http://arxiv.org/abs/2402.04624v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents:\n  New Perspectives and Trends", "abstract": "Persuasion, as one of the crucial abilities in human communication, has\ngarnered extensive attention from researchers within the field of intelligent\ndialogue systems. We humans tend to persuade others to change their viewpoints,\nattitudes or behaviors through conversations in various scenarios (e.g.,\npersuasion for social good, arguing in online platforms). Developing dialogue\nagents that can persuade others to accept certain standpoints is essential to\nachieving truly intelligent and anthropomorphic dialogue system. Benefiting\nfrom the substantial progress of Large Language Models (LLMs), dialogue agents\nhave acquired an exceptional capability in context understanding and response\ngeneration. However, as a typical and complicated cognitive psychological\nsystem, persuasive dialogue agents also require knowledge from the domain of\ncognitive psychology to attain a level of human-like persuasion. Consequently,\nthe cognitive strategy-enhanced persuasive dialogue agent (defined as\nCogAgent), which incorporates cognitive strategies to achieve persuasive\ntargets through conversation, has become a predominant research paradigm. To\ndepict the research trends of CogAgent, in this paper, we first present several\nfundamental cognitive psychology theories and give the formalized definition of\nthree typical cognitive strategies, including the persuasion strategy, the\ntopic path planning strategy, and the argument structure prediction strategy.\nThen we propose a new system architecture by incorporating the formalized\ndefinition to lay the foundation of CogAgent. Representative works are detailed\nand investigated according to the combined cognitive strategy, followed by the\nsummary of authoritative benchmarks and evaluation metrics. Finally, we\nsummarize our insights on open issues and future directions of CogAgent for\nupcoming researchers.", "published": "2024-02-07 07:28:34", "link": "http://arxiv.org/abs/2402.04631v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TransLLaMa: LLM-based Simultaneous Translation System", "abstract": "Decoder-only large language models (LLMs) have recently demonstrated\nimpressive capabilities in text generation and reasoning. Nonetheless, they\nhave limited applications in simultaneous machine translation (SiMT), currently\ndominated by encoder-decoder transformers. This study demonstrates that, after\nfine-tuning on a small dataset comprising causally aligned source and target\nsentence pairs, a pre-trained open-source LLM can control input segmentation\ndirectly by generating a special \"wait\" token. This obviates the need for a\nseparate policy and enables the LLM to perform English-German and\nEnglish-Russian SiMT tasks with BLEU scores that are comparable to those of\nspecific state-of-the-art baselines. We also evaluated closed-source models\nsuch as GPT-4, which displayed encouraging results in performing the SiMT task\nwithout prior training (zero-shot), indicating a promising avenue for enhancing\nfuture SiMT systems.", "published": "2024-02-07 07:39:27", "link": "http://arxiv.org/abs/2402.04636v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Source Identification in Abstractive Summarization", "abstract": "Neural abstractive summarization models make summaries in an end-to-end\nmanner, and little is known about how the source information is actually\nconverted into summaries. In this paper, we define input sentences that contain\nessential information in the generated summary as $\\textit{source sentences}$\nand study how abstractive summaries are made by analyzing the source sentences.\nTo this end, we annotate source sentences for reference summaries and system\nsummaries generated by PEGASUS on document-summary pairs sampled from the\nCNN/DailyMail and XSum datasets. We also formulate automatic source sentence\ndetection and compare multiple methods to establish a strong baseline for the\ntask. Experimental results show that the perplexity-based method performs well\nin highly abstractive settings, while similarity-based methods perform robustly\nin relatively extractive settings. Our code and data are available at\nhttps://github.com/suhara/sourcesum.", "published": "2024-02-07 09:09:09", "link": "http://arxiv.org/abs/2402.04677v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Hypothesis-Driven Framework for the Analysis of Self-Rationalising\n  Models", "abstract": "The self-rationalising capabilities of LLMs are appealing because the\ngenerated explanations can give insights into the plausibility of the\npredictions. However, how faithful the explanations are to the predictions is\nquestionable, raising the need to explore the patterns behind them further. To\nthis end, we propose a hypothesis-driven statistical framework. We use a\nBayesian network to implement a hypothesis about how a task (in our example,\nnatural language inference) is solved, and its internal states are translated\ninto natural language with templates. Those explanations are then compared to\nLLM-generated free-text explanations using automatic and human evaluations.\nThis allows us to judge how similar the LLM's and the Bayesian network's\ndecision processes are. We demonstrate the usage of our framework with an\nexample hypothesis and two realisations in Bayesian networks. The resulting\nmodels do not exhibit a strong similarity to GPT-3.5. We discuss the\nimplications of this as well as the framework's potential to approximate LLM\ndecisions better in future work.", "published": "2024-02-07 12:26:12", "link": "http://arxiv.org/abs/2402.04787v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect-Based Sentiment Analysis for Open-Ended HR Survey Responses", "abstract": "Understanding preferences, opinions, and sentiment of the workforce is\nparamount for effective employee lifecycle management. Open-ended survey\nresponses serve as a valuable source of information. This paper proposes a\nmachine learning approach for aspect-based sentiment analysis (ABSA) of Dutch\nopen-ended responses in employee satisfaction surveys. Our approach aims to\novercome the inherent noise and variability in these responses, enabling a\ncomprehensive analysis of sentiments that can support employee lifecycle\nmanagement. Through response clustering we identify six key aspects (salary,\nschedule, contact, communication, personal attention, agreements), which we\nvalidate by domain experts. We compile a dataset of 1,458 Dutch survey\nresponses, revealing label imbalance in aspects and sentiments. We propose\nfew-shot approaches for ABSA based on Dutch BERT models, and compare them\nagainst bag-of-words and zero-shot baselines. Our work significantly\ncontributes to the field of ABSA by demonstrating the first successful\napplication of Dutch pre-trained language models to aspect-based sentiment\nanalysis in the domain of human resources (HR).", "published": "2024-02-07 13:01:43", "link": "http://arxiv.org/abs/2402.04812v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Communication Policies for Different Follower Behaviors in a\n  Collaborative Reference Game", "abstract": "Albrecht and Stone (2018) state that modeling of changing behaviors remains\nan open problem \"due to the essentially unconstrained nature of what other\nagents may do\". In this work we evaluate the adaptability of neural artificial\nagents towards assumed partner behaviors in a collaborative reference game. In\nthis game success is achieved when a knowledgeable Guide can verbally lead a\nFollower to the selection of a specific puzzle piece among several distractors.\nWe frame this language grounding and coordination task as a reinforcement\nlearning problem and measure to which extent a common reinforcement training\nalgorithm (PPO) is able to produce neural agents (the Guides) that perform well\nwith various heuristic Follower behaviors that vary along the dimensions of\nconfidence and autonomy. We experiment with a learning signal that in addition\nto the goal condition also respects an assumed communicative effort. Our\nresults indicate that this novel ingredient leads to communicative strategies\nthat are less verbose (staying silent in some of the steps) and that with\nrespect to that the Guide's strategies indeed adapt to the partner's level of\nconfidence and autonomy.", "published": "2024-02-07 13:22:17", "link": "http://arxiv.org/abs/2402.04824v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for\n  Instruction Fine-Tuning", "abstract": "There is a consensus that instruction fine-tuning of LLMs requires\nhigh-quality data, but what are they? LIMA (NeurIPS 2023) and AlpaGasus (ICLR\n2024) are state-of-the-art methods for selecting such high-quality examples,\neither via manual curation or using GPT-3.5-Turbo as a quality scorer. We show\nthat the extremely simple baseline of selecting the 1,000 instructions with\nlongest responses -- that intuitively contain more learnable information and\nare harder to overfit -- from standard datasets can consistently outperform\nthese sophisticated methods according to GPT-4 and PaLM-2 as judges, while\nremaining competitive on the Open LLM benchmarks that test factual knowledge.\nWe demonstrate this for several LLMs (Llama-2-7B, Llama-2-13B, Mistral-7B-v0.1)\nand datasets (Alpaca-52k, Evol-Instruct-70k). In addition, a lightweight\nrefinement of such long instructions can further improve the abilities of the\nfine-tuned LLMs, and allows us to obtain competitive results on MT-Bench and\nthe 2nd highest-ranked Llama-2-7B-based model on AlpacaEval 2.0, while training\non only 1,000 examples and no extra preference data. We also conduct a thorough\nanalysis of our models to ensure that their enhanced performance is not simply\ndue to GPT-4's preference for longer responses. Overall, our findings suggest\nthat fine-tuning on the longest responses should be the default baseline for\nany work on instruction fine-tuning. We provide our code at\nhttps://github.com/tml-epfl/long-is-more-for-alignment.", "published": "2024-02-07 13:32:11", "link": "http://arxiv.org/abs/2402.04833v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Personalized Text Generation with Fine-Grained Linguistic Control", "abstract": "As the text generation capabilities of large language models become\nincreasingly prominent, recent studies have focused on controlling particular\naspects of the generated text to make it more personalized. However, most\nresearch on controllable text generation focuses on controlling the content or\nmodeling specific high-level/coarse-grained attributes that reflect authors'\nwriting styles, such as formality, domain, or sentiment. In this paper, we\nfocus on controlling fine-grained attributes spanning multiple linguistic\ndimensions, such as lexical and syntactic attributes. We introduce a novel\nbenchmark to train generative models and evaluate their ability to generate\npersonalized text based on multiple fine-grained linguistic attributes. We\nsystematically investigate the performance of various large language models on\nour benchmark and draw insights from the factors that impact their performance.\nWe make our code, data, and pretrained models publicly available.", "published": "2024-02-07 14:41:08", "link": "http://arxiv.org/abs/2402.04914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reconfidencing LLMs from the Grouping Loss Perspective", "abstract": "Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to\ngenerating hallucinated answers in a confident tone. While efforts to elicit\nand calibrate confidence scores have proven useful, recent findings show that\ncontrolling uncertainty must go beyond calibration: predicted scores may\ndeviate significantly from the actual posterior probabilities due to the impact\nof grouping loss. In this work, we construct a new evaluation dataset derived\nfrom a knowledge base to assess confidence scores given to answers of Mistral\nand LLaMA. Experiments show that they tend to be overconfident. Further, we\nshow that they are more overconfident on some answers than others, \\emph{eg}\ndepending on the nationality of the person in the query. In\nuncertainty-quantification theory, this is grouping loss. To address this, we\npropose a solution to reconfidence LLMs, canceling not only calibration but\nalso grouping loss. The LLMs, after the reconfidencing process, indicate\nimproved confidence alignment with the accuracy of their responses.", "published": "2024-02-07 15:40:22", "link": "http://arxiv.org/abs/2402.04957v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pedagogical Alignment of Large Language Models", "abstract": "Large Language Models (LLMs), when used in educational settings without\npedagogical fine-tuning, often provide immediate answers rather than guiding\nstudents through the problem-solving process. This approach falls short of\npedagogically best practices and limits their effectiveness as educational\ntools. We term the objective of training LLMs to emulate effective teaching\nstrategies as `pedagogical alignment.' In this paper, we investigate Learning\nfrom Human Preferences (LHP) algorithms to achieve this alignment objective. A\nkey challenge in this process is the scarcity of high-quality preference\ndatasets to guide the alignment. To address this, we propose a novel approach\nfor constructing a large-scale dataset using synthetic data generation\ntechniques, eliminating the need for time-consuming and costly manual\nannotation. Leveraging this dataset, our experiments with Llama and Mistral\nmodels demonstrate that LHP methods outperform standard supervised fine-tuning\n(SFT), improving pedagogical alignment accuracy by 13.1% and 8.7% respectively.\nExisting evaluation methods also lack quantitative metrics to adequately\nmeasure the pedagogical alignment of LLMs. To address this gap, we propose\nnovel perplexity-based metrics that quantify LLMs' tendency to provide\nscaffolded guidance versus direct answers, offering a robust measure of\npedagogical alignment. Our analysis provides compelling evidence for the\nsuperiority of LHP methods over SFT in optimizing LLMs' behavior, underscoring\nthe potential of LHP methods in better aligning LLMs with educational\nobjectives and fostering effective learning experiences. Code and models are\navailable \\href{https://github.com/luffycodes/Tutorbot-Spock}{here}.", "published": "2024-02-07 16:15:59", "link": "http://arxiv.org/abs/2402.05000v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TreeForm: End-to-end Annotation and Evaluation for Form Document Parsing", "abstract": "Visually Rich Form Understanding (VRFU) poses a complex research problem due\nto the documents' highly structured nature and yet highly variable style and\ncontent. Current annotation schemes decompose form understanding and omit key\nhierarchical structure, making development and evaluation of end-to-end models\ndifficult. In this paper, we propose a novel F1 metric to evaluate form parsers\nand describe a new content-agnostic, tree-based annotation scheme for VRFU:\nTreeForm. We provide methods to convert previous annotation schemes into\nTreeForm structures and evaluate TreeForm predictions using a modified version\nof the normalized tree-edit distance. We present initial baselines for our\nend-to-end performance metric and the TreeForm edit distance, averaged over the\nFUNSD and XFUND datasets, of 61.5 and 26.4 respectively. We hope that TreeForm\nencourages deeper research in annotating, modeling, and evaluating the\ncomplexities of form-like documents.", "published": "2024-02-07 21:54:53", "link": "http://arxiv.org/abs/2402.05282v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation", "abstract": "This paper introduces the ColorSwap dataset, designed to assess and improve\nthe proficiency of multimodal models in matching objects with their colors. The\ndataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000\nexamples. Each example includes a caption-image pair, along with a\n``color-swapped'' pair. We follow the Winoground schema: the two captions in an\nexample have the same words, but the color words have been rearranged to modify\ndifferent objects. The dataset was created through a novel blend of automated\ncaption and image generation with humans in the loop. We evaluate image-text\nmatching (ITM) and visual language models (VLMs) and find that even the latest\nones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% on\nour main VLM metric, although they may improve with more advanced prompting\ntechniques. On the main ITM metric, contrastive models such as CLIP and SigLIP\nperform close to chance (at 12% and 30%, respectively), although the\nnon-contrastive BLIP ITM model is stronger (87%). We also find that finetuning\non fewer than 2,000 examples yields significant performance gains on this\nout-of-distribution word-order understanding task. The dataset is here:\nhttps://github.com/Top34051/colorswap and here:\nhttps://huggingface.co/datasets/stanfordnlp/colorswap.", "published": "2024-02-07 00:31:49", "link": "http://arxiv.org/abs/2402.04492v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Developments in Sheaf-Theoretic Models of Natural Language Ambiguities", "abstract": "Sheaves are mathematical objects consisting of a base which constitutes a\ntopological space and the data associated with each open set thereof, e.g.\ncontinuous functions defined on the open sets. Sheaves have originally been\nused in algebraic topology and logic. Recently, they have also modelled events\nsuch as physical experiments and natural language disambiguation processes. We\nextend the latter models from lexical ambiguities to discourse ambiguities\narising from anaphora. To begin, we calculated a new measure of contextuality\nfor a dataset of basic anaphoric discourses, resulting in a higher proportion\nof contextual models-82.9%-compared to previous work which only yielded 3.17%\ncontextual models. Then, we show how an extension of the natural language\nprocessing challenge, known as the Winograd Schema, which involves anaphoric\nambiguities can be modelled on the Bell-CHSH scenario with a contextual\nfraction of 0.096.", "published": "2024-02-07 01:18:55", "link": "http://arxiv.org/abs/2402.04505v2", "categories": ["cs.CL", "quant-ph"], "primary_category": "cs.CL"}
{"title": "Online Cascade Learning for Efficient Inference over Streams", "abstract": "Large Language Models (LLMs) have a natural role in answering complex queries\nabout data streams, but the high computational cost of LLM inference makes them\ninfeasible in many such tasks. We propose online cascade learning, the first\napproach to address this challenge. The objective here is to learn a \"cascade\"\nof models, starting with lower-capacity models (such as logistic regression)\nand ending with a powerful LLM, along with a deferral policy that determines\nthe model to be used on a given input. We formulate the task of learning\ncascades online as an imitation-learning problem, where smaller models are\nupdated over time imitating the collected LLM demonstrations, and give a\nno-regret algorithm for the problem. Experimental results across four\nbenchmarks show that our method parallels LLMs in accuracy while cutting down\ninference costs by as much as 90% with strong robustness against input\ndistribution shifts, underscoring its efficacy and adaptability in stream\nprocessing.", "published": "2024-02-07 01:46:50", "link": "http://arxiv.org/abs/2402.04513v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SumRec: A Framework for Recommendation using Open-Domain Dialogue", "abstract": "Chat dialogues contain considerable useful information about a speaker's\ninterests, preferences, and experiences.Thus, knowledge from open-domain chat\ndialogue can be used to personalize various systems and offer recommendations\nfor advanced information.This study proposed a novel framework SumRec for\nrecommending information from open-domain chat dialogue.The study also examined\nthe framework using ChatRec, a newly constructed dataset for training and\nevaluation. To extract the speaker and item characteristics, the SumRec\nframework employs a large language model (LLM) to generate a summary of the\nspeaker information from a dialogue and to recommend information about an item\naccording to the type of user.The speaker and item information are then input\ninto a score estimation model, generating a recommendation score.Experimental\nresults show that the SumRec framework provides better recommendations than the\nbaseline method of using dialogues and item descriptions in their original\nform. Our dataset and code is publicly available at\nhttps://github.com/Ryutaro-A/SumRec", "published": "2024-02-07 02:06:48", "link": "http://arxiv.org/abs/2402.04523v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector", "abstract": "Chinese grammatical error correction (CGEC) faces serious overcorrection\nchallenges when employing autoregressive generative models such as\nsequence-to-sequence (Seq2Seq) models and decoder-only large language models\n(LLMs). While previous methods aim to address overcorrection in Seq2Seq models,\nthey are difficult to adapt to decoder-only LLMs. In this paper, we propose an\nalignment-enhanced corrector for the overcorrection problem that applies to\nboth Seq2Seq models and decoder-only LLMs. Our method first trains a correction\nmodel to generate an initial correction of the source sentence. Then, we\ncombine the source sentence with the initial correction and feed it through an\nalignment model for another round of correction, aiming to enforce the\nalignment model to focus on potential overcorrection. Moreover, to enhance the\nmodel's ability to identify nuances, we further explore the reverse alignment\nof the source sentence and the initial correction. Finally, we transfer the\nalignment knowledge from two alignment models to the correction model,\ninstructing it on how to avoid overcorrection. Experimental results on three\nCGEC datasets demonstrate the effectiveness of our approach in alleviating\novercorrection and improving overall performance. Our code has been made\npublicly available.", "published": "2024-02-07 05:56:54", "link": "http://arxiv.org/abs/2402.04601v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "StableMask: Refining Causal Masking in Decoder-only Transformer", "abstract": "The decoder-only Transformer architecture with causal masking and relative\nposition encoding (RPE) has become the de facto choice in language modeling.\nDespite its exceptional performance across various tasks, we have identified\ntwo limitations: First, it requires all attention scores to be non-zero and sum\nup to 1, even if the current embedding has sufficient self-contained\ninformation. This compels the model to assign disproportional excessive\nattention to specific tokens. Second, RPE-based Transformers are not universal\napproximators due to their limited capacity at encoding absolute positional\ninformation, which limits their application in position-critical tasks. In this\nwork, we propose StableMask: a parameter-free method to address both\nlimitations by refining the causal mask. It introduces pseudo-attention values\nto balance attention distributions and encodes absolute positional information\nvia a progressively decreasing mask ratio. StableMask's effectiveness is\nvalidated both theoretically and empirically, showing significant enhancements\nin language models with parameter sizes ranging from 71M to 1.4B across diverse\ndatasets and encoding methods. We further show that it naturally supports (1)\nefficient extrapolation without special tricks such as StreamingLLM and (2)\neasy integration with existing attention optimization techniques.", "published": "2024-02-07 12:01:02", "link": "http://arxiv.org/abs/2402.04779v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity\n  Recognition", "abstract": "In this study, we aim to reduce generation latency for Named Entity\nRecognition (NER) with Large Language Models (LLMs). The main cause of high\nlatency in LLMs is the sequential decoding process, which autoregressively\ngenerates all labels and mentions for NER, significantly increase the sequence\nlength. To this end, we introduce Parallel Decoding in LLM for NE}\n(PaDeLLM-NER), a approach that integrates seamlessly into existing generative\nmodel frameworks without necessitating additional modules or architectural\nmodifications. PaDeLLM-NER allows for the simultaneous decoding of all\nmentions, thereby reducing generation latency. Experiments reveal that\nPaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times\nfaster than the autoregressive approach for both English and Chinese.\nSimultaneously it maintains the quality of predictions as evidenced by the\nperformance that is on par with the state-of-the-art across various datasets.", "published": "2024-02-07 13:39:38", "link": "http://arxiv.org/abs/2402.04838v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detecting Generated Native Ads in Conversational Search", "abstract": "Conversational search engines such as YouChat and Microsoft Copilot use large\nlanguage models (LLMs) to generate responses to queries. It is only a small\nstep to also let the same technology insert ads within the generated responses\n- instead of separately placing ads next to a response. Inserted ads would be\nreminiscent of native advertising and product placement, both of which are very\neffective forms of subtle and manipulative advertising. Considering the high\ncomputational costs associated with LLMs, for which providers need to develop\nsustainable business models, users of conversational search engines may very\nwell be confronted with generated native ads in the near future. In this paper,\nwe thus take a first step to investigate whether LLMs can also be used as a\ncountermeasure, i.e., to block generated native ads. We compile the Webis\nGenerated Native Ads 2024 dataset of queries and generated responses with\nautomatically inserted ads, and evaluate whether LLMs or fine-tuned sentence\ntransformers can detect the ads. In our experiments, the investigated LLMs\nstruggle with the task but sentence transformers achieve precision and recall\nvalues above 0.9.", "published": "2024-02-07 14:22:51", "link": "http://arxiv.org/abs/2402.04889v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large\n  Language Models", "abstract": "Due to the high memory and computational costs associated with large language\nmodels (LLMs), model compression techniques such as quantization, which reduces\ninference costs, and parameter-efficient fine-tuning (PEFT) methods like\nLow-Rank Adaptation (LoRA), which reduce training costs, have gained\nsignificant popularity. This trend has spurred active research into\nquantization-aware PEFT techniques, aimed at maintaining model accuracy while\nminimizing memory overhead during both inference and training. Previous\nquantization-aware PEFT methods typically apply post-training quantization\n(PTQ) to pre-trained LLMs, followed by PEFT to recover accuracy loss.\nMeanwhile, this approach has limitations in recovering the accuracy loss. In\nthis paper, we propose L4Q, a method that integrates Quantization-Aware\nTraining (QAT) with LoRA. By employing a memory-optimized layer design, L4Q\nsignificantly reduces QAT's memory overhead, making its training cost\ncomparable to LoRA, while preserving the advantage of QAT in producing fully\nquantized LLMs with high accuracy. Our experiments demonstrate that this\ncombined approach to quantization and fine-tuning achieves superior accuracy\ncompared to decoupled fine-tuning schemes, particularly in 4-bit and 3-bit\nquantization, positioning L4Q as an efficient QAT solution. Using the LLaMA and\nMistral models with instructional datasets, we showcase L4Q's capabilities in\nlanguage tasks and few-shot learning.", "published": "2024-02-07 14:35:05", "link": "http://arxiv.org/abs/2402.04902v5", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Prompting Implicit Discourse Relation Annotation", "abstract": "Pre-trained large language models, such as ChatGPT, archive outstanding\nperformance in various reasoning tasks without supervised training and were\nfound to have outperformed crowdsourcing workers. Nonetheless, ChatGPT's\nperformance in the task of implicit discourse relation classification, prompted\nby a standard multiple-choice question, is still far from satisfactory and\nconsiderably inferior to state-of-the-art supervised approaches. This work\ninvestigates several proven prompting techniques to improve ChatGPT's\nrecognition of discourse relations. In particular, we experimented with\nbreaking down the classification task that involves numerous abstract labels\ninto smaller subtasks. Nonetheless, experiment results show that the inference\naccuracy hardly changes even with sophisticated prompt engineering, suggesting\nthat implicit discourse relation classification is not yet resolvable under\nzero-shot or few-shot settings.", "published": "2024-02-07 14:44:42", "link": "http://arxiv.org/abs/2402.04918v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge\n  Graph-Integrated Collaboration", "abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in a\nmultitude of Natural Language Processing (NLP) tasks, they encounter challenges\nin practical applications, including issues with hallucinations, inadequate\nknowledge updating, and limited transparency in the reasoning process. To\novercome these limitations, this study innovatively proposes a collaborative\ntraining-free reasoning scheme involving tight cooperation between Knowledge\nGraph (KG) and LLMs. This scheme first involves using LLMs to iteratively\nexplore KG, selectively retrieving a task-relevant knowledge subgraph to\nsupport reasoning. The LLMs are then guided to further combine inherent\nimplicit knowledge to reason on the subgraph while explicitly elucidating the\nreasoning process. Through such a cooperative approach, our scheme achieves\nmore reliable knowledge-based reasoning and facilitates the tracing of the\nreasoning results. Experimental results show that our scheme significantly\nprogressed across multiple datasets, notably achieving over a 10% improvement\non the QALD10 dataset compared to the best baseline and the fine-tuned\nstate-of-the-art (SOTA) work. Building on this success, this study hopes to\noffer a valuable reference for future research in the fusion of KG and LLMs,\nthereby enhancing LLMs' proficiency in solving complex issues.", "published": "2024-02-07 15:56:17", "link": "http://arxiv.org/abs/2402.04978v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How BERT Speaks Shakespearean English? Evaluating Historical Bias in\n  Contextual Language Models", "abstract": "In this paper, we explore the idea of analysing the historical bias of\ncontextual language models based on BERT by measuring their adequacy with\nrespect to Early Modern (EME) and Modern (ME) English. In our preliminary\nexperiments, we perform fill-in-the-blank tests with 60 masked sentences (20\nEME-specific, 20 ME-specific and 20 generic) and three different models (i.e.,\nBERT Base, MacBERTh, English HLM). We then rate the model predictions according\nto a 5-point bipolar scale between the two language varieties and derive a\nweighted score to measure the adequacy of each model to EME and ME varieties of\nEnglish.", "published": "2024-02-07 17:07:53", "link": "http://arxiv.org/abs/2402.05034v1", "categories": ["cs.CL", "cs.CY", "I.2.7; J.5"], "primary_category": "cs.CL"}
{"title": "Edu-ConvoKit: An Open-Source Library for Education Conversation Data", "abstract": "We introduce Edu-ConvoKit, an open-source library designed to handle\npre-processing, annotation and analysis of conversation data in education.\nResources for analyzing education conversation data are scarce, making the\nresearch challenging to perform and therefore hard to access. We address these\nchallenges with Edu-ConvoKit. Edu-ConvoKit is open-source\n(https://github.com/stanfordnlp/edu-convokit ), pip-installable\n(https://pypi.org/project/edu-convokit/ ), with comprehensive documentation\n(https://edu-convokit.readthedocs.io/en/latest/ ). Our demo video is available\nat: https://youtu.be/zdcI839vAko?si=h9qlnl76ucSuXb8- . We include additional\nresources, such as Colab applications of Edu-ConvoKit to three diverse\neducation datasets and a repository of Edu-ConvoKit related papers, that can be\nfound in our GitHub repository.", "published": "2024-02-07 18:59:31", "link": "http://arxiv.org/abs/2402.05111v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ApiQ: Finetuning of 2-Bit Quantized Large Language Model", "abstract": "Memory-efficient finetuning of large language models (LLMs) has recently\nattracted huge attention with the increasing size of LLMs, primarily due to the\nconstraints posed by GPU memory limitations and the effectiveness of these\nmethods compared to full finetuning. Despite the advancements, current\nstrategies for memory-efficient finetuning, such as QLoRA, exhibit inconsistent\nperformance across diverse bit-width quantizations and multifaceted tasks. This\ninconsistency largely stems from the detrimental impact of the quantization\nprocess on preserved knowledge, leading to catastrophic forgetting and\nundermining the utilization of pretrained models for finetuning purposes. In\nthis work, we introduce a novel quantization framework, ApiQ, designed to\nrestore the lost information from quantization by concurrently initializing the\nLoRA components and quantizing the weights of LLMs. This approach ensures the\nmaintenance of the original LLM's activation precision while mitigating the\nerror propagation from shallower into deeper layers. Through comprehensive\nevaluations conducted on a spectrum of language tasks with various LLMs, ApiQ\ndemonstrably minimizes activation error during quantization. Consequently, it\nconsistently achieves superior finetuning results across various bit-widths.", "published": "2024-02-07 09:36:54", "link": "http://arxiv.org/abs/2402.05147v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "$\u03bb$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion\n  Models by Leveraging CLIP Latent Space", "abstract": "Despite the recent advances in personalized text-to-image (P-T2I) generative\nmodels, it remains challenging to perform finetuning-free multi-subject-driven\nT2I in a resource-efficient manner. Predominantly, contemporary approaches,\ninvolving the training of Hypernetworks and Multimodal Large Language Models\n(MLLMs), require heavy computing resources that range from 600 to 12300 GPU\nhours of training. These subject-driven T2I methods hinge on Latent Diffusion\nModels (LDMs), which facilitate T2I mapping through cross-attention layers.\nWhile LDMs offer distinct advantages, P-T2I methods' reliance on the latent\nspace of these diffusion models significantly escalates resource demands,\nleading to inconsistent results and necessitating numerous iterations for a\nsingle desired image. In this paper, we present $\\lambda$-ECLIPSE, an\nalternative prior-training strategy that works in the latent space of a\npre-trained CLIP model without relying on the diffusion UNet models.\n$\\lambda$-ECLIPSE leverages the image-text interleaved pre-training for fast\nand effective multi-subject-driven P-T2I. Through extensive experiments, we\nestablish that $\\lambda$-ECLIPSE surpasses existing baselines in composition\nalignment while preserving concept alignment performance, even with\nsignificantly lower resource utilization. $\\lambda$-ECLIPSE performs\nmulti-subject driven P-T2I with just 34M parameters and is trained on a mere 74\nGPU hours. Additionally, $\\lambda$-ECLIPSE demonstrates the unique ability to\nperform multi-concept interpolations.", "published": "2024-02-07 19:07:10", "link": "http://arxiv.org/abs/2402.05195v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "The Effect of Sampling Temperature on Problem Solving in Large Language\n  Models", "abstract": "In this research study, we empirically investigate the effect of sampling\ntemperature on the performance of Large Language Models (LLMs) on various\nproblem-solving tasks. We created a multiple-choice question-and-answer (MCQA)\nexam by randomly sampling problems from standard LLM benchmarks. Then, we used\nnine popular LLMs with five prompt-engineering techniques to solve the MCQA\nproblems while increasing the sampling temperature from 0.0 to 1.6. Despite\nanecdotal reports to the contrary, our empirical results indicate that changes\nin temperature from 0.0 to 1.0 do not have a statistically significant impact\non LLM performance for problem-solving tasks. In addition, these results appear\nto generalize across LLMs, prompt-engineering techniques, and problem domains.\nAll code, data, and supplemental materials are available on GitHub at:\nhttps://github.com/matthewrenze/jhu-llm-temperature", "published": "2024-02-07 19:11:23", "link": "http://arxiv.org/abs/2402.05201v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "News Source Credibility Assessment: A Reddit Case Study", "abstract": "In the era of social media platforms, identifying the credibility of online\ncontent is crucial to combat misinformation. We present the CREDiBERT\n(CREDibility assessment using Bi-directional Encoder Representations from\nTransformers), a source credibility assessment model fine-tuned for Reddit\nsubmissions focusing on political discourse as the main contribution. We adopt\na semi-supervised training approach for CREDiBERT, leveraging Reddit's\ncommunity-based structure. By encoding submission content using CREDiBERT and\nintegrating it into a Siamese neural network, we significantly improve the\nbinary classification of submission credibility, achieving a 9% increase in F1\nscore compared to existing methods. Additionally, we introduce a new version of\nthe post-to-post network in Reddit that efficiently encodes user interactions\nto enhance the binary classification task by nearly 8% in F1 score. Finally, we\nemploy CREDiBERT to evaluate the susceptibility of subreddits with respect to\ndifferent topics.", "published": "2024-02-07 13:39:24", "link": "http://arxiv.org/abs/2402.10938v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation", "abstract": "To ensure that text generated by large language models (LLMs) is in an\nexpected format, constrained decoding proposes to enforce strict formal\nlanguage constraints during generation. However, as we show in this work, not\nonly do such methods incur performance overhead during generation, but many of\nthem also significantly impair task accuracy, if they do not correctly align\nthe underlying LLM sub-word vocabularies with external constraints. To address\nthis, we present a novel decoding algorithm, DOMINO, that can enforce\nconstraints in a fully subword-aligned fashion, while leveraging\npre-computation and speculative decoding to achieve virtually no overhead and\nin some cases even almost 2$\\times$ speedup over unconstrained decoding --\nthereby outperforming existing approaches by a wide margin.", "published": "2024-02-07 13:36:02", "link": "http://arxiv.org/abs/2403.06988v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Unmasking the Shadows of AI: Investigating Deceptive Capabilities in\n  Large Language Models", "abstract": "This research critically navigates the intricate landscape of AI deception,\nconcentrating on deceptive behaviours of Large Language Models (LLMs). My\nobjective is to elucidate this issue, examine the discourse surrounding it, and\nsubsequently delve into its categorization and ramifications. The essay\ninitiates with an evaluation of the AI Safety Summit 2023 (ASS) and\nintroduction of LLMs, emphasising multidimensional biases that underlie their\ndeceptive behaviours.The literature review covers four types of deception\ncategorised: Strategic deception, Imitation, Sycophancy, and Unfaithful\nReasoning, along with the social implications and risks they entail. Lastly, I\ntake an evaluative stance on various aspects related to navigating the\npersistent challenges of the deceptive AI. This encompasses considerations of\ninternational collaborative governance, the reconfigured engagement of\nindividuals with AI, proposal of practical adjustments, and specific elements\nof digital education.", "published": "2024-02-07 00:21:46", "link": "http://arxiv.org/abs/2403.09676v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CapsF: Capsule Fusion for Extracting psychiatric stressors for suicide\n  from twitter", "abstract": "Along with factors such as cancer, blood pressure, street accidents and\nstroke, suicide has been one of Iran main causes of death. One of the main\nreasons for suicide is psychological stressors. Identifying psychological\nstressors in an at risk population can help in the early prevention of suicidal\nand suicidal behaviours. In recent years, the widespread popularity and flow of\nreal time information sharing of social media have allowed for potential early\nintervention in large scale and even small scale populations. However, some\nautomated approaches to extract psychiatric stressors from Twitter have been\npresented, but most of this research has been for non Persian languages. This\nstudy aims to investigate the techniques of detecting psychological stress\nrelated to suicide from Persian tweets using learning based methods. The\nproposed capsule based approach achieved a binary classification accuracy of\n0.83.", "published": "2024-02-07 13:41:22", "link": "http://arxiv.org/abs/2403.15391v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "The Fine-Grained Complexity of Gradient Computation for Training Large\n  Language Models", "abstract": "Large language models (LLMs) have made fundamental contributions over the\nlast a few years. To train an LLM, one needs to alternatingly run `forward'\ncomputations and `backward' computations. The forward computation can be viewed\nas attention function evaluation, and the backward computation can be viewed as\na gradient computation. In previous work by [Alman and Song, NeurIPS 2023], it\nwas proved that the forward step can be performed in almost-linear time in\ncertain parameter regimes, but that there is no truly sub-quadratic time\nalgorithm in the remaining parameter regimes unless the popular hypothesis SETH\nis false. In this work, we show nearly identical results for the harder-seeming\nproblem of computing the gradient of loss function of one layer attention\nnetwork, and thus for the entire process of LLM training. This completely\ncharacterizes the fine-grained complexity of every step of LLM training.", "published": "2024-02-07 00:45:31", "link": "http://arxiv.org/abs/2402.04497v1", "categories": ["cs.LG", "cs.CC", "cs.CL", "cs.DS"], "primary_category": "cs.LG"}
{"title": "Can Large Language Model Agents Simulate Human Trust Behavior?", "abstract": "Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one critical and elemental\nbehavior in human interactions, trust, and investigate whether LLM agents can\nsimulate human trust behavior. We first find that LLM agents generally exhibit\ntrust behavior, referred to as agent trust, under the framework of Trust Games,\nwhich are widely recognized in behavioral economics. Then, we discover that\nGPT-4 agents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount.", "published": "2024-02-07 03:37:19", "link": "http://arxiv.org/abs/2402.04559v4", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Beyond Answers: Transferring Reasoning Capabilities to Smaller LLMs\n  Using Multi-Teacher Knowledge Distillation", "abstract": "Transferring the reasoning capability from stronger large language models\n(LLMs) to smaller ones has been quite appealing, as smaller LLMs are more\nflexible to deploy with less expense. Among the existing solutions, knowledge\ndistillation stands out due to its outstanding efficiency and generalization.\nHowever, existing methods suffer from several drawbacks, including limited\nknowledge diversity and the lack of rich contextual information. To solve the\nproblems and facilitate the learning of compact language models, we propose\nTinyLLM, a new knowledge distillation paradigm to learn a small student LLM\nfrom multiple large teacher LLMs. In particular, we encourage the student LLM\nto not only generate the correct answers but also understand the rationales\nbehind these answers. Given that different LLMs possess diverse reasoning\nskills, we guide the student model to assimilate knowledge from various teacher\nLLMs. We further introduce an in-context example generator and a\nteacher-forcing Chain-of-Thought strategy to ensure that the rationales are\naccurate and grounded in contextually appropriate scenarios. Extensive\nexperiments on six datasets across two reasoning tasks demonstrate the\nsuperiority of our method. Results show that TinyLLM can outperform large\nteacher LLMs significantly, despite a considerably smaller model size. The\nsource code is available at: https://github.com/YikunHan42/TinyLLM.", "published": "2024-02-07 06:48:24", "link": "http://arxiv.org/abs/2402.04616v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "InfLLM: Training-Free Long-Context Extrapolation for LLMs with an\n  Efficient Context Memory", "abstract": "Large language models (LLMs) have emerged as a cornerstone in real-world\napplications with lengthy streaming inputs (e.g., LLM-driven agents). However,\nexisting LLMs, pre-trained on sequences with a restricted maximum length,\ncannot process longer sequences due to the out-of-domain and distraction\nissues. Common solutions often involve continual pre-training on longer\nsequences, which will introduce expensive computational overhead and\nuncontrollable change in model capabilities. In this paper, we unveil the\nintrinsic capacity of LLMs for understanding extremely long sequences without\nany fine-tuning. To this end, we introduce a training-free memory-based method,\nInfLLM. Specifically, InfLLM stores distant contexts into additional memory\nunits and employs an efficient mechanism to lookup token-relevant units for\nattention computation. Thereby, InfLLM allows LLMs to efficiently process long\nsequences with a limited context window and well capture long-distance\ndependencies. Without any training, InfLLM enables LLMs that are pre-trained on\nsequences consisting of a few thousand tokens to achieve comparable performance\nwith competitive baselines that continually train these LLMs on long sequences.\nEven when the sequence length is scaled to $1,024$K, InfLLM still effectively\ncaptures long-distance dependencies. Our code can be found in\n\\url{https://github.com/thunlp/InfLLM}.", "published": "2024-02-07 06:50:42", "link": "http://arxiv.org/abs/2402.04617v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question\n  Answering over a Life Science Knowledge Graph", "abstract": "The recent success of Large Language Models (LLM) in a wide range of Natural\nLanguage Processing applications opens the path towards novel Question\nAnswering Systems over Knowledge Graphs leveraging LLMs. However, one of the\nmain obstacles preventing their implementation is the scarcity of training data\nfor the task of translating questions into corresponding SPARQL queries,\nparticularly in the case of domain-specific KGs. To overcome this challenge, in\nthis study, we evaluate several strategies for fine-tuning the OpenLlama LLM\nfor question answering over life science knowledge graphs. In particular, we\npropose an end-to-end data augmentation approach for extending a set of\nexisting queries over a given knowledge graph towards a larger dataset of\nsemantically enriched question-to-SPARQL query pairs, enabling fine-tuning even\nfor datasets where these pairs are scarce. In this context, we also investigate\nthe role of semantic \"clues\" in the queries, such as meaningful variable names\nand inline comments. Finally, we evaluate our approach over the real-world Bgee\ngene expression knowledge graph and we show that semantic clues can improve\nmodel performance by up to 33% compared to a baseline with random variable\nnames and no comments included.", "published": "2024-02-07 07:24:01", "link": "http://arxiv.org/abs/2402.04627v1", "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.IR"], "primary_category": "cs.AI"}
{"title": "FaithLM: Towards Faithful Explanations for Large Language Models", "abstract": "Large Language Models (LLMs) have become proficient in addressing complex\ntasks by leveraging their extensive internal knowledge and reasoning\ncapabilities. However, the black-box nature of these models complicates the\ntask of explaining their decision-making processes. While recent advancements\ndemonstrate the potential of leveraging LLMs to self-explain their predictions\nthrough natural language (NL) explanations, their explanations may not\naccurately reflect the LLMs' decision-making process due to a lack of fidelity\noptimization on the derived explanations. Measuring the fidelity of NL\nexplanations is a challenging issue, as it is difficult to manipulate the input\ncontext to mask the semantics of these explanations. To this end, we introduce\nFaithLM to explain the decision of LLMs with NL explanations. Specifically,\nFaithLM designs a method for evaluating the fidelity of NL explanations by\nincorporating the contrary explanations to the query process. Moreover, FaithLM\nconducts an iterative process to improve the fidelity of derived explanations.\nExperiment results on three datasets from multiple domains demonstrate that\nFaithLM can significantly improve the fidelity of derived explanations, which\nalso provides a better alignment with the ground-truth explanations.", "published": "2024-02-07 09:09:14", "link": "http://arxiv.org/abs/2402.04678v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with\n  Vision-Language Benchmark", "abstract": "Multimodal Large Language Models (MLLMs) have gained significant attention\nrecently, showing remarkable potential in artificial general intelligence.\nHowever, assessing the utility of MLLMs presents considerable challenges,\nprimarily due to the absence of multimodal benchmarks that align with human\npreferences. Drawing inspiration from the concept of LLM-as-a-Judge within\nLLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to\nassess the ability of MLLMs in assisting judges across diverse modalities,\nencompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and\nBatch Ranking. Our study reveals that, while MLLMs demonstrate remarkable\nhuman-like discernment in Pair Comparison, there is a significant divergence\nfrom human preferences in Scoring Evaluation and Batch Ranking. Furthermore, a\ncloser examination reveals persistent challenges in the judgment capacities of\nLLMs, including diverse biases, hallucinatory responses, and inconsistencies in\njudgment, even in advanced models such as GPT-4V. These findings emphasize the\npressing need for enhancements and further research efforts to be undertaken\nbefore regarding MLLMs as fully reliable evaluators. In light of this, we\nadvocate for additional efforts dedicated to supporting the continuous\ndevelopment within the domain of MLLM functioning as judges. The code and\ndataset are publicly available at our project homepage:\n\\url{https://mllm-judge.github.io/}.", "published": "2024-02-07 12:28:32", "link": "http://arxiv.org/abs/2402.04788v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Direct Language Model Alignment from Online AI Feedback", "abstract": "Direct alignment from preferences (DAP) methods, such as DPO, have recently\nemerged as efficient alternatives to reinforcement learning from human feedback\n(RLHF), that do not require a separate reward model. However, the preference\ndatasets used in DAP methods are usually collected ahead of training and never\nupdated, thus the feedback is purely offline. Moreover, responses in these\ndatasets are often sampled from a language model distinct from the one being\naligned, and since the model evolves over training, the alignment phase is\ninevitably off-policy. In this study, we posit that online feedback is key and\nimproves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as\nannotator: on each training iteration, we sample two responses from the current\nmodel and prompt the LLM annotator to choose which one is preferred, thus\nproviding online feedback. Despite its simplicity, we demonstrate via human\nevaluation in several tasks that OAIF outperforms both offline DAP and RLHF\nmethods. We further show that the feedback leveraged in OAIF is easily\ncontrollable, via instruction prompts to the LLM annotator.", "published": "2024-02-07 12:31:13", "link": "http://arxiv.org/abs/2402.04792v2", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey", "abstract": "Research surveys have always posed a challenge for beginner researchers who\nlack of research training. These researchers struggle to understand the\ndirections within their research topic, and the discovery of new research\nfindings within a short time. One way to provide intuitive assistance to\nbeginner researchers is by offering relevant knowledge graphs(KG) and\nrecommending related academic papers. However, existing navigation knowledge\ngraphs primarily rely on keywords in the research field and often fail to\npresent the logical hierarchy among multiple related papers clearly. Moreover,\nmost recommendation systems for academic papers simply rely on high text\nsimilarity, which can leave researchers confused as to why a particular article\nis being recommended. They may lack of grasp important information about the\ninsight connection between \"Issue resolved\" and \"Issue finding\" that they hope\nto obtain. To address these issues, this study aims to support research insight\nsurveys for beginner researchers by establishing a hierarchical tree-structured\nknowledge graph that reflects the inheritance insight of research topics and\nthe relevance insight among the academic papers.", "published": "2024-02-07 13:54:06", "link": "http://arxiv.org/abs/2402.04854v7", "categories": ["cs.DL", "cs.CL", "cs.LG"], "primary_category": "cs.DL"}
{"title": "CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay", "abstract": "Large language models are increasingly solving tasks that are commonly\nbelieved to require human-level reasoning ability. However, these models still\nperform very poorly on benchmarks of general intelligence such as the\nAbstraction and Reasoning Corpus (ARC). In this paper, we approach ARC as a\nprogramming-by-examples problem, and introduce a novel and scalable method for\nlanguage model self-improvement called Code Iteration (CodeIt). Our method\niterates between 1) program sampling and hindsight relabeling, and 2) learning\nfrom prioritized experience replay. By relabeling the goal of an episode (i.e.,\nthe target program output given input) to the realized output produced by the\nsampled program, our method effectively deals with the extreme sparsity of\nrewards in program synthesis. Applying CodeIt to the ARC dataset, we\ndemonstrate that prioritized hindsight replay, along with pre-training and\ndata-augmentation, leads to successful inter-task generalization. CodeIt is the\nfirst neuro-symbolic approach that scales to the full ARC evaluation dataset.\nOur method solves 15% of ARC evaluation tasks, achieving state-of-the-art\nperformance and outperforming existing neural and symbolic baselines. Our code\nis available at https://github.com/Qualcomm-AI-research/codeit .", "published": "2024-02-07 13:55:27", "link": "http://arxiv.org/abs/2402.04858v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "On Provable Length and Compositional Generalization", "abstract": "Out-of-distribution generalization capabilities of sequence-to-sequence\nmodels can be studied from the lens of two crucial forms of generalization:\nlength generalization -- the ability to generalize to longer sequences than\nones seen during training, and compositional generalization: the ability to\ngeneralize to token combinations not seen during training. In this work, we\nprovide first provable guarantees on length and compositional generalization\nfor common sequence-to-sequence models -- deep sets, transformers, state space\nmodels, and recurrent neural nets -- trained to minimize the prediction error.\nWe show that limited capacity versions of these different architectures achieve\nboth length and compositional generalization provided the training distribution\nis sufficiently diverse. In the first part, we study structured limited\ncapacity variants of different architectures and arrive at the generalization\nguarantees with limited diversity requirements on the training distribution. In\nthe second part, we study limited capacity variants with less structural\nassumptions and arrive at generalization guarantees but with more diversity\nrequirements on the training distribution.", "published": "2024-02-07 14:16:28", "link": "http://arxiv.org/abs/2402.04875v5", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Text or Image? What is More Important in Cross-Domain Generalization\n  Capabilities of Hate Meme Detection Models?", "abstract": "This paper delves into the formidable challenge of cross-domain\ngeneralization in multimodal hate meme detection, presenting compelling\nfindings. We provide enough pieces of evidence supporting the hypothesis that\nonly the textual component of hateful memes enables the existing multimodal\nclassifier to generalize across different domains, while the image component\nproves highly sensitive to a specific training dataset. The evidence includes\ndemonstrations showing that hate-text classifiers perform similarly to\nhate-meme classifiers in a zero-shot setting. Simultaneously, the introduction\nof captions generated from images of memes to the hate-meme classifier worsens\nperformance by an average F1 of 0.02. Through blackbox explanations, we\nidentify a substantial contribution of the text modality (average of 83%),\nwhich diminishes with the introduction of meme's image captions (52%).\nAdditionally, our evaluation on a newly created confounder dataset reveals\nhigher performance on text confounders as compared to image confounders with an\naverage $\\Delta$F1 of 0.18.", "published": "2024-02-07 15:44:55", "link": "http://arxiv.org/abs/2402.04967v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large\n  Language Models", "abstract": "In the rapidly evolving landscape of Large Language Models (LLMs), ensuring\nrobust safety measures is paramount. To meet this crucial need, we propose\n\\emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating\nLLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench\ntranscends conventional benchmarks through its large scale, rich diversity,\nintricate taxonomy spanning three levels, and versatile\nfunctionalities.SALAD-Bench is crafted with a meticulous array of questions,\nfrom standard queries to complex ones enriched with attack, defense\nmodifications and multiple-choice. To effectively manage the inherent\ncomplexity, we introduce an innovative evaluators: the LLM-based MD-Judge for\nQA pairs with a particular focus on attack-enhanced queries, ensuring a\nseamless, and reliable evaluation. Above components extend SALAD-Bench from\nstandard LLM safety evaluation to both LLM attack and defense methods\nevaluation, ensuring the joint-purpose utility. Our extensive experiments shed\nlight on the resilience of LLMs against emerging threats and the efficacy of\ncontemporary defense tactics. Data and evaluator are released under\nhttps://github.com/OpenSafetyLab/SALAD-BENCH.", "published": "2024-02-07 17:33:54", "link": "http://arxiv.org/abs/2402.05044v4", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Roadmap to Pluralistic Alignment", "abstract": "With increased power and prevalence of AI systems, it is ever more critical\nthat AI systems are designed to serve all, i.e., people with diverse values and\nperspectives. However, aligning models to serve pluralistic human values\nremains an open research question. In this piece, we propose a roadmap to\npluralistic alignment, specifically using language models as a test bed. We\nidentify and formalize three possible ways to define and operationalize\npluralism in AI systems: 1) Overton pluralistic models that present a spectrum\nof reasonable responses; 2) Steerably pluralistic models that can steer to\nreflect certain perspectives; and 3) Distributionally pluralistic models that\nare well-calibrated to a given population in distribution. We also formalize\nand discuss three possible classes of pluralistic benchmarks: 1)\nMulti-objective benchmarks, 2) Trade-off steerable benchmarks, which\nincentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic\nbenchmarks which explicitly model diverse human ratings. We use this framework\nto argue that current alignment techniques may be fundamentally limited for\npluralistic AI; indeed, we highlight empirical evidence, both from our own\nexperiments and from other work, that standard alignment procedures might\nreduce distributional pluralism in models, motivating the need for further\nresearch on pluralistic alignment.", "published": "2024-02-07 18:21:17", "link": "http://arxiv.org/abs/2402.05070v3", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Image captioning for Brazilian Portuguese using GRIT model", "abstract": "This work presents the early development of a model of image captioning for\nthe Brazilian Portuguese language. We used the GRIT (Grid - and Region-based\nImage captioning Transformer) model to accomplish this work. GRIT is a\nTransformer-only neural architecture that effectively utilizes two visual\nfeatures to generate better captions. The GRIT method emerged as a proposal to\nbe a more efficient way to generate image captioning. In this work, we adapt\nthe GRIT model to be trained in a Brazilian Portuguese dataset to have an image\ncaptioning method for the Brazilian Portuguese Language.", "published": "2024-02-07 18:57:37", "link": "http://arxiv.org/abs/2402.05106v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank\n  Modifications", "abstract": "Large language models (LLMs) show inherent brittleness in their safety\nmechanisms, as evidenced by their susceptibility to jailbreaking and even\nnon-malicious fine-tuning. This study explores this brittleness of safety\nalignment by leveraging pruning and low-rank modifications. We develop methods\nto identify critical regions that are vital for safety guardrails, and that are\ndisentangled from utility-relevant regions at both the neuron and rank levels.\nSurprisingly, the isolated regions we find are sparse, comprising about $3\\%$\nat the parameter level and $2.5\\%$ at the rank level. Removing these regions\ncompromises safety without significantly impacting utility, corroborating the\ninherent brittleness of the model's safety mechanisms. Moreover, we show that\nLLMs remain vulnerable to low-cost fine-tuning attacks even when modifications\nto the safety-critical regions are restricted. These findings underscore the\nurgent need for more robust safety strategies in LLMs.", "published": "2024-02-07 18:34:38", "link": "http://arxiv.org/abs/2402.05162v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "InCoRo: In-Context Learning for Robotics Control with Feedback Loops", "abstract": "One of the challenges in robotics is to enable robotic units with the\nreasoning capability that would be robust enough to execute complex tasks in\ndynamic environments. Recent advances in LLMs have positioned them as go-to\ntools for simple reasoning tasks, motivating the pioneering work of Liang et\nal. [35] that uses an LLM to translate natural language commands into low-level\nstatic execution plans for robotic units. Using LLMs inside robotics systems\nbrings their generalization to a new level, enabling zero-shot generalization\nto new tasks. This paper extends this prior work to dynamic environments. We\npropose InCoRo, a system that uses a classical robotic feedback loop composed\nof an LLM controller, a scene understanding unit, and a robot. Our system\ncontinuously analyzes the state of the environment and provides adapted\nexecution commands, enabling the robot to adjust to changing environmental\nconditions and correcting for controller errors. Our system does not require\nany iterative optimization to learn to accomplish a task as it leverages\nin-context learning with an off-the-shelf LLM model. Through an extensive\nvalidation process involving two standardized industrial robotic units -- SCARA\nand DELTA types -- we contribute knowledge about these robots, not popular in\nthe community, thereby enriching it. We highlight the generalization\ncapabilities of our system and show that (1) in-context learning in combination\nwith the current state-of-the-art LLMs is an effective way to implement a\nrobotic controller; (2) in static environments, InCoRo surpasses the prior art\nin terms of the success rate; (3) in dynamic environments, we establish new\nstate-of-the-art for the SCARA and DELTA units, respectively. This research\npaves the way towards building reliable, efficient, intelligent autonomous\nsystems that adapt to dynamic environments.", "published": "2024-02-07 19:01:11", "link": "http://arxiv.org/abs/2402.05188v1", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Are LLMs Ready for Real-World Materials Discovery?", "abstract": "Large Language Models (LLMs) create exciting possibilities for powerful\nlanguage processing tools to accelerate research in materials science. While\nLLMs have great potential to accelerate materials understanding and discovery,\nthey currently fall short in being practical materials science tools. In this\nposition paper, we show relevant failure cases of LLMs in materials science\nthat reveal current limitations of LLMs related to comprehending and reasoning\nover complex, interconnected materials science knowledge. Given those\nshortcomings, we outline a framework for developing Materials Science LLMs\n(MatSci-LLMs) that are grounded in materials science knowledge and hypothesis\ngeneration followed by hypothesis testing. The path to attaining performant\nMatSci-LLMs rests in large part on building high-quality, multi-modal datasets\nsourced from scientific literature where various information extraction\nchallenges persist. As such, we describe key materials science information\nextraction challenges which need to be overcome in order to build large-scale,\nmulti-modal datasets that capture valuable materials science knowledge.\nFinally, we outline a roadmap for applying future MatSci-LLMs for real-world\nmaterials discovery via: 1. Automated Knowledge Base Generation; 2. Automated\nIn-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials\nLaboratories.", "published": "2024-02-07 19:10:36", "link": "http://arxiv.org/abs/2402.05200v2", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cond-mat.mtrl-sci"}
{"title": "VerAs: Verify then Assess STEM Lab Reports", "abstract": "With an increasing focus in STEM education on critical thinking skills,\nscience writing plays an ever more important role in curricula that stress\ninquiry skills. A recently published dataset of two sets of college level lab\nreports from an inquiry-based physics curriculum relies on analytic assessment\nrubrics that utilize multiple dimensions, specifying subject matter knowledge\nand general components of good explanations. Each analytic dimension is\nassessed on a 6-point scale, to provide detailed feedback to students that can\nhelp them improve their science writing skills. Manual assessment can be slow,\nand difficult to calibrate for consistency across all students in large\nclasses. While much work exists on automated assessment of open-ended questions\nin STEM subjects, there has been far less work on long-form writing such as lab\nreports. We present an end-to-end neural architecture that has separate\nverifier and assessment modules, inspired by approaches to Open Domain Question\nAnswering (OpenQA). VerAs first verifies whether a report contains any content\nrelevant to a given rubric dimension, and if so, assesses the relevant\nsentences. On the lab reports, VerAs outperforms multiple baselines based on\nOpenQA systems or Automated Essay Scoring (AES). VerAs also performs well on an\nanalytic rubric for middle school physics essays.", "published": "2024-02-07 20:02:09", "link": "http://arxiv.org/abs/2402.05224v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Examining Modality Incongruity in Multimodal Federated Learning for\n  Medical Vision and Language-based Disease Detection", "abstract": "Multimodal Federated Learning (MMFL) utilizes multiple modalities in each\nclient to build a more powerful Federated Learning (FL) model than its unimodal\ncounterpart. However, the impact of missing modality in different clients, also\ncalled modality incongruity, has been greatly overlooked. This paper, for the\nfirst time, analyses the impact of modality incongruity and reveals its\nconnection with data heterogeneity across participating clients. We\nparticularly inspect whether incongruent MMFL with unimodal and multimodal\nclients is more beneficial than unimodal FL. Furthermore, we examine three\npotential routes of addressing this issue. Firstly, we study the effectiveness\nof various self-attention mechanisms towards incongruity-agnostic information\nfusion in MMFL. Secondly, we introduce a modality imputation network (MIN)\npre-trained in a multimodal client for modality translation in unimodal clients\nand investigate its potential towards mitigating the missing modality problem.\nThirdly, we assess the capability of client-level and server-level\nregularization techniques towards mitigating modality incongruity effects.\nExperiments are conducted under several MMFL settings on two publicly available\nreal-world datasets, MIMIC-CXR and Open-I, with Chest X-Ray and radiology\nreports.", "published": "2024-02-07 22:16:53", "link": "http://arxiv.org/abs/2402.05294v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs", "abstract": "Information retrieval is a rapidly evolving field of information retrieval,\nwhich is characterized by a continuous refinement of techniques and\ntechnologies, from basic hyperlink-based navigation to sophisticated\nalgorithm-driven search engines. This paper aims to provide a comprehensive\noverview of the evolution of Information Retrieval Technology, with a\nparticular focus on the role of Large Language Models (LLMs) in bridging the\ngap between traditional search methods and the emerging paradigm of answer\nretrieval. The integration of LLMs in the realms of response retrieval and\nindexing signifies a paradigm shift in how users interact with information\nsystems. This paradigm shift is driven by the integration of large language\nmodels (LLMs) like GPT-4, which are capable of understanding and generating\nhuman-like text, thus enabling them to provide more direct and contextually\nrelevant answers to user queries. Through this exploration, we seek to\nilluminate the technological milestones that have shaped this journey and the\npotential future directions in this rapidly changing field.", "published": "2024-02-07 23:39:40", "link": "http://arxiv.org/abs/2402.05318v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Large Language User Interfaces: Voice Interactive User Interfaces\n  powered by LLMs", "abstract": "The evolution of Large Language Models (LLMs) has showcased remarkable\ncapacities for logical reasoning and natural language comprehension. These\ncapabilities can be leveraged in solutions that semantically and textually\nmodel complex problems. In this paper, we present our efforts toward\nconstructing a framework that can serve as an intermediary between a user and\ntheir user interface (UI), enabling dynamic and real-time interactions. We\nemploy a system that stands upon textual semantic mappings of UI components, in\nthe form of annotations. These mappings are stored, parsed, and scaled in a\ncustom data structure, supplementary to an agent-based prompting backend\nengine. Employing textual semantic mappings allows each component to not only\nexplain its role to the engine but also provide expectations. By comprehending\nthe needs of both the user and the components, our LLM engine can classify the\nmost appropriate application, extract relevant parameters, and subsequently\nexecute precise predictions of the user's expected actions. Such an integration\nevolves static user interfaces into highly dynamic and adaptable solutions,\nintroducing a new frontier of intelligent and responsive user experiences.", "published": "2024-02-07 21:08:49", "link": "http://arxiv.org/abs/2402.07938v2", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.1"], "primary_category": "cs.HC"}
{"title": "Neural machine translation of clinical procedure codes for medical\n  diagnosis and uncertainty quantification", "abstract": "A Clinical Decision Support System (CDSS) is designed to enhance clinician\ndecision-making by combining system-generated recommendations with medical\nexpertise. Given the high costs, intensive labor, and time-sensitive nature of\nmedical treatments, there is a pressing need for efficient decision support,\nespecially in complex emergency scenarios. In these scenarios, where\ninformation can be limited, an advanced CDSS framework that leverages AI\n(artificial intelligence) models to effectively reduce diagnostic uncertainty\nhas utility. Such an AI-enabled CDSS framework with quantified uncertainty\npromises to be practical and beneficial in the demanding context of real-world\nmedical care. In this study, we introduce the concept of Medical Entropy,\nquantifying uncertainties in patient outcomes predicted by neural machine\ntranslation based on the ICD-9 code of procedures. Our experimental results not\nonly show strong correlations between procedure and diagnosis sequences based\non the simple ICD-9 code but also demonstrate the promising capacity to model\ntrends of uncertainties during hospitalizations through a data-driven approach.", "published": "2024-02-07 20:11:56", "link": "http://arxiv.org/abs/2402.10940v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling Up LLM Reviews for Google Ads Content Moderation", "abstract": "Large language models (LLMs) are powerful tools for content moderation, but\ntheir inference costs and latency make them prohibitive for casual use on large\ndatasets, such as the Google Ads repository. This study proposes a method for\nscaling up LLM reviews for content moderation in Google Ads. First, we use\nheuristics to select candidates via filtering and duplicate removal, and create\nclusters of ads for which we select one representative ad per cluster. We then\nuse LLMs to review only the representative ads. Finally, we propagate the LLM\ndecisions for the representative ads back to their clusters. This method\nreduces the number of reviews by more than 3 orders of magnitude while\nachieving a 2x recall compared to a baseline non-LLM model. The success of this\napproach is a strong function of the representations used in clustering and\nlabel propagation; we found that cross-modal similarity representations yield\nbetter results than uni-modal representations.", "published": "2024-02-07 23:47:02", "link": "http://arxiv.org/abs/2402.14590v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Progressive unsupervised domain adaptation for ASR using ensemble models\n  and multi-stage training", "abstract": "In Automatic Speech Recognition (ASR), teacher-student (T/S) training has\nshown to perform well for domain adaptation with small amount of training data.\nHowever, adaption without ground-truth labels is still challenging. A previous\nstudy has shown the effectiveness of using ensemble teacher models in T/S\ntraining for unsupervised domain adaptation (UDA) but its performance still\nlags behind compared to the model trained on in-domain data. This paper\nproposes a method to yield better UDA by training multi-stage students with\nensemble teacher models. Initially, multiple teacher models are trained on\nlabelled data from read and meeting domains. These teachers are used to train a\nstudent model on unlabelled out-of-domain telephone speech data. To improve the\nadaptation, subsequent student models are trained sequentially considering\npreviously trained model as their teacher. Experiments are conducted with three\nteachers trained on AMI, WSJ and LibriSpeech and three stages of students on\nSwitchBoard data. Results shown on eval00 test set show significant WER\nimprovement with multi-stage training with an absolute gain of 9.8%, 7.7% and\n3.3% at each stage.", "published": "2024-02-07 12:52:22", "link": "http://arxiv.org/abs/2402.04805v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Review of Cetacean's click detection algorithms", "abstract": "The detection of echolocation clicks is key in understanding the intricate\nbehaviors of cetaceans and monitoring their populations. Cetacean species\nrelying on clicks for navigation, foraging and even communications are sperm\nwhales (Physeter macrocephalus) and a variety of dolphin groups. Echolocation\nclicks are wideband signals of short duration that are often emitted in\nsequences of varying inter-click-intervals. While datasets and models for\nclicks exist, the detection and classification of clicks present a significant\nchallenge, mostly due to the diversity of clicks' structures, overlapping\nsignals from simultaneously emitting animals, and the abundance of noise\ntransients from, for example, snapping shrimps and shipping cavitation noise.\nThis paper provides a survey of the many detection and classification\nmethodologies of clicks, ranging from 2002 to 2023. We divide the surveyed\ntechniques into categories by their methodology. Specifically, feature analysis\n(e.g., phase, ICI and duration), frequency content, energy based detection,\nsupervised and unsupervised machine learning, template matching and adaptive\ndetection approaches. Also surveyed are open access platforms for click\ndetections, and databases openly available for testing. Details of the method\napplied for each paper are given along with advantages and limitations, and for\neach category we analyze the remaining challenges. The paper also includes a\nperformance comparison for several schemes over a shared database. Finally, we\nprovide tables summarizing the existing detection schemes in terms of\nchallenges address, methods, detection and classification tools applied,\nfeatures used and applications.", "published": "2024-02-07 10:41:14", "link": "http://arxiv.org/abs/2402.04735v1", "categories": ["cs.SD", "eess.AS", "q-bio.QM", "53-02", "J.7; A.1"], "primary_category": "cs.SD"}
{"title": "Fast Timing-Conditioned Latent Audio Diffusion", "abstract": "Generating long-form 44.1kHz stereo audio from text prompts can be\ncomputationally demanding. Further, most previous works do not tackle that\nmusic and sound effects naturally vary in their duration. Our research focuses\non the efficient generation of long-form, variable-length stereo music and\nsounds at 44.1kHz using text prompts with a generative model. Stable Audio is\nbased on latent diffusion, with its latent defined by a fully-convolutional\nvariational autoencoder. It is conditioned on text prompts as well as timing\nembeddings, allowing for fine control over both the content and length of the\ngenerated music and sounds. Stable Audio is capable of rendering stereo signals\nof up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its compute\nefficiency and fast inference, it is one of the best in two public\ntext-to-music and -audio benchmarks and, differently from state-of-the-art\nmodels, can generate music with structure and stereo sounds.", "published": "2024-02-07 13:23:25", "link": "http://arxiv.org/abs/2402.04825v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tuning In: Analysis of Audio Classifier Performance in Clinical Settings\n  with Limited Data", "abstract": "This study assesses deep learning models for audio classification in a\nclinical setting with the constraint of small datasets reflecting real-world\nprospective data collection. We analyze CNNs, including DenseNet and ConvNeXt,\nalongside transformer models like ViT, SWIN, and AST, and compare them against\npre-trained audio models such as YAMNet and VGGish. Our method highlights the\nbenefits of pre-training on large datasets before fine-tuning on specific\nclinical data. We prospectively collected two first-of-their-kind patient audio\ndatasets from stroke patients. We investigated various preprocessing\ntechniques, finding that RGB and grayscale spectrogram transformations affect\nmodel performance differently based on the priors they learn from pre-training.\nOur findings indicate CNNs can match or exceed transformer models in small\ndataset contexts, with DenseNet-Contrastive and AST models showing notable\nperformance. This study highlights the significance of incremental marginal\ngains through model selection, pre-training, and preprocessing in sound\nclassification; this offers valuable insights for clinical diagnostics that\nrely on audio classification.", "published": "2024-02-07 16:41:11", "link": "http://arxiv.org/abs/2402.10100v3", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
