{"title": "You Shall Know the Most Frequent Sense by the Company it Keeps", "abstract": "Identification of the most frequent sense of a polysemous word is an\nimportant semantic task. We introduce two concepts that can benefit MFS\ndetection: companions, which are the most frequently co-occurring words, and\nthe most frequent translation in a bitext. We present two novel methods that\nincorporate these new concepts, and show that they advance the state of the art\non MFS detection.", "published": "2018-08-21 01:18:37", "link": "http://arxiv.org/abs/1808.06729v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Relation Extraction via Inner-Sentence Noise Reduction and\n  Transfer Learning", "abstract": "Extracting relations is critical for knowledge base completion and\nconstruction in which distant supervised methods are widely used to extract\nrelational facts automatically with the existing knowledge bases. However, the\nautomatically constructed datasets comprise amounts of low-quality sentences\ncontaining noisy words, which is neglected by current distant supervised\nmethods resulting in unacceptable precisions. To mitigate this problem, we\npropose a novel word-level distant supervised approach for relation extraction.\nWe first build Sub-Tree Parse(STP) to remove noisy words that are irrelevant to\nrelations. Then we construct a neural network inputting the sub-tree while\napplying the entity-wise attention to identify the important semantic features\nof relational words in each instance. To make our model more robust against\nnoisy words, we initialize our network with a priori knowledge learned from the\nrelevant task of entity classification by transfer learning. We conduct\nextensive experiments using the corpora of New York Times(NYT) and Freebase.\nExperiments show that our approach is effective and improves the area of\nPrecision/Recall(PR) from 0.35 to 0.39 over the state-of-the-art work.", "published": "2018-08-21 02:15:05", "link": "http://arxiv.org/abs/1808.06738v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lessons from Natural Language Inference in the Clinical Domain", "abstract": "State of the art models using deep neural networks have become very good in\nlearning an accurate mapping from inputs to outputs. However, they still lack\ngeneralization capabilities in conditions that differ from the ones encountered\nduring training. This is even more challenging in specialized, and knowledge\nintensive domains, where training data is limited. To address this gap, we\nintroduce MedNLI - a dataset annotated by doctors, performing a natural\nlanguage inference task (NLI), grounded in the medical history of patients. We\npresent strategies to: 1) leverage transfer learning using datasets from the\nopen domain, (e.g. SNLI) and 2) incorporate domain knowledge from external data\nand lexical sources (e.g. medical terminologies). Our results demonstrate\nperformance gains using both strategies.", "published": "2018-08-21 04:00:23", "link": "http://arxiv.org/abs/1808.06752v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Learning for Neural Keyphrase Generation", "abstract": "We study the problem of generating keyphrases that summarize the key points\nfor a given document. While sequence-to-sequence (seq2seq) models have achieved\nremarkable performance on this task (Meng et al., 2017), model training often\nrelies on large amounts of labeled data, which is only applicable to\nresource-rich domains. In this paper, we propose semi-supervised keyphrase\ngeneration methods by leveraging both labeled data and large-scale unlabeled\nsamples for learning. Two strategies are proposed. First, unlabeled documents\nare first tagged with synthetic keyphrases obtained from unsupervised keyphrase\nextraction methods or a selflearning algorithm, and then combined with labeled\nsamples for training. Furthermore, we investigate a multi-task learning\nframework to jointly learn to generate keyphrases as well as the titles of the\narticles. Experimental results show that our semi-supervised learning-based\nmethods outperform a state-of-the-art model trained with labeled data only.", "published": "2018-08-21 05:38:32", "link": "http://arxiv.org/abs/1808.06773v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Influence of Down-Sampling Strategies on SVD Word Embedding\n  Stability", "abstract": "The stability of word embedding algorithms, i.e., the consistency of the word\nrepresentations they reveal when trained repeatedly on the same data set, has\nrecently raised concerns. We here compare word embedding algorithms on three\ncorpora of different sizes, and evaluate both their stability and accuracy. We\nfind strong evidence that down-sampling strategies (used as part of their\ntraining procedures) are particularly influential for the stability of\nSVDPPMI-type embeddings. This finding seems to explain diverging reports on\ntheir stability and lead us to a simple modification which provides superior\nstability as well as accuracy on par with skip-gram embeddings.", "published": "2018-08-21 09:11:11", "link": "http://arxiv.org/abs/1808.06810v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Semantic Abstraction of Multilingual NMT with Paraphrase\n  Recognition and Generation Tasks", "abstract": "In this paper, we investigate whether multilingual neural translation models\nlearn stronger semantic abstractions of sentences than bilingual ones. We test\nthis hypotheses by measuring the perplexity of such models when applied to\nparaphrases of the source language. The intuition is that an encoder produces\nbetter representations if a decoder is capable of recognizing synonymous\nsentences in the same language even though the model is never trained for that\ntask. In our setup, we add 16 different auxiliary languages to a bidirectional\nbilingual baseline model (English-French) and test it with in-domain and\nout-of-domain paraphrases in English. The results show that the perplexity is\nsignificantly reduced in each of the cases, indicating that meaning can be\ngrounded in translation. This is further supported by a study on paraphrase\ngeneration that we also include at the end of the paper.", "published": "2018-08-21 10:07:18", "link": "http://arxiv.org/abs/1808.06826v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis of Speeches in Indian Parliamentary Debates", "abstract": "With the increasing usage of the internet, more and more data is being\ndigitized including parliamentary debates but they are in an unstructured\nformat. There is a need to convert them into a structured format for linguistic\nanalysis. Much work has been done on parliamentary data such as Hansard,\nAmerican congressional floor-debate data on various aspects but less on\npragmatics. In this paper, we provide a dataset for the synopsis of Indian\nparliamentary debates and perform stance classification of speeches i.e\nidentifying if the speaker is supporting the bill/issue or against it. We also\nanalyze the intention of the speeches beyond mere sentences i.e pragmatics in\nthe parliament. Based on thorough manual analysis of the debates, we developed\nan annotation scheme of 4 mutually exclusive categories to analyze the purpose\nof the speeches: to find out ISSUES, to BLAME, to APPRECIATE and for CALL FOR\nACTION. We have annotated the dataset provided, with these 4 categories and\nconducted preliminary experiments for automatic detection of the categories.\nOur automated classification approach gave us promising results.", "published": "2018-08-21 10:25:30", "link": "http://arxiv.org/abs/1808.06834v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Demonstrating PAR4SEM - A Semantic Writing Aid with Adaptive\n  Paraphrasing", "abstract": "In this paper, we present Par4Sem, a semantic writing aid tool based on\nadaptive paraphrasing. Unlike many annotation tools that are primarily used to\ncollect training examples, Par4Sem is integrated into a real word application,\nin this case a writing aid tool, in order to collect training examples from\nusage data. Par4Sem is a tool, which supports an adaptive, iterative, and\ninteractive process where the underlying machine learning models are updated\nfor each iteration using new training examples from usage data. After\nmotivating the use of ever-learning tools in NLP applications, we evaluate\nPar4Sem by adopting it to a text simplification task through mere usage.", "published": "2018-08-21 11:37:57", "link": "http://arxiv.org/abs/1808.06853v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial training for multi-context joint entity and relation\n  extraction", "abstract": "Adversarial training (AT) is a regularization method that can be used to\nimprove the robustness of neural network methods by adding small perturbations\nin the training data. We show how to use AT for the tasks of entity recognition\nand relation extraction. In particular, we demonstrate that applying AT to a\ngeneral purpose baseline model for jointly extracting entities and relations,\nallows improving the state-of-the-art effectiveness on several datasets in\ndifferent contexts (i.e., news, biomedical, and real estate data) and for\ndifferent languages (English and Dutch).", "published": "2018-08-21 12:52:03", "link": "http://arxiv.org/abs/1808.06876v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Skeleton-Based Model for Promoting Coherence Among Sentences in\n  Narrative Story Generation", "abstract": "Narrative story generation is a challenging problem because it demands the\ngenerated sentences with tight semantic connections, which has not been well\nstudied by most existing generative models. To address this problem, we propose\na skeleton-based model to promote the coherence of generated stories. Different\nfrom traditional models that generate a complete sentence at a stroke, the\nproposed model first generates the most critical phrases, called skeleton, and\nthen expands the skeleton to a complete and fluent sentence. The skeleton is\nnot manually defined, but learned by a reinforcement learning method. Compared\nto the state-of-the-art models, our skeleton-based model can generate\nsignificantly more coherent text according to human evaluation and automatic\nevaluation. The G-score is improved by 20.1% in the human evaluation. The code\nis available at https://github.com/lancopku/Skeleton-Based-Generation-Model", "published": "2018-08-21 14:55:34", "link": "http://arxiv.org/abs/1808.06945v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gaussian Word Embedding with a Wasserstein Distance Loss", "abstract": "Compared with word embedding based on point representation,\ndistribution-based word embedding shows more flexibility in expressing\nuncertainty and therefore embeds richer semantic information when representing\nwords. The Wasserstein distance provides a natural notion of dissimilarity with\nprobability measures and has a closed-form solution when measuring the distance\nbetween two Gaussian distributions. Therefore, with the aim of representing\nwords in a highly efficient way, we propose to operate a Gaussian word\nembedding model with a loss function based on the Wasserstein distance. Also,\nexternal information from ConceptNet will be used to semi-supervise the results\nof the Gaussian word embedding. Thirteen datasets from the word similarity\ntask, together with one from the word entailment task, and six datasets from\nthe downstream document classification task will be evaluated in this paper to\ntest our hypothesis.", "published": "2018-08-21 16:59:39", "link": "http://arxiv.org/abs/1808.07016v7", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ISNA-Set: A novel English Corpus of Iran NEWS", "abstract": "News agencies publish news on their websites all over the world. Moreover,\ncreating novel corpuses is necessary to bring natural processing to new\ndomains. Textual processing of online news is challenging in terms of the\nstrategy of collecting data, the complex structure of news websites, and\nselecting or designing suitable algorithms for processing these types of data.\nDespite the previous works which focus on creating corpuses for Iran news in\nPersian, in this paper, we introduce a new corpus for English news of a\nnational news agency. ISNA-Set is a new dataset of English news of Iranian\nStudents News Agency (ISNA), as one of the most famous news agencies in Iran.\nWe statistically analyze the data and the sentiments of news, and also extract\nentities and part-of-speech tagging.", "published": "2018-08-21 17:58:05", "link": "http://arxiv.org/abs/1808.07046v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Has Machine Translation Achieved Human Parity? A Case for Document-level\n  Evaluation", "abstract": "Recent research suggests that neural machine translation achieves parity with\nprofessional human translation on the WMT Chinese--English news translation\ntask. We empirically test this claim with alternative evaluation protocols,\ncontrasting the evaluation of single sentences and entire documents. In a\npairwise ranking experiment, human raters assessing adequacy and fluency show a\nstronger preference for human over machine translation when evaluating\ndocuments as compared to isolated sentences. Our findings emphasise the need to\nshift towards document-level evaluation as machine translation improves to the\ndegree that errors which are hard or impossible to spot at the sentence-level\nbecome decisive in discriminating quality of different translation outputs.", "published": "2018-08-21 17:58:21", "link": "http://arxiv.org/abs/1808.07048v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Identification in Code-Mixed Data using Multichannel Neural\n  Networks and Context Capture", "abstract": "An accurate language identification tool is an absolute necessity for\nbuilding complex NLP systems to be used on code-mixed data. Lot of work has\nbeen recently done on the same, but there's still room for improvement.\nInspired from the recent advancements in neural network architectures for\ncomputer vision tasks, we have implemented multichannel neural networks\ncombining CNN and LSTM for word level language identification of code-mixed\ndata. Combining this with a Bi-LSTM-CRF context capture module, accuracies of\n93.28% and 93.32% is achieved on our two testing sets.", "published": "2018-08-21 20:22:09", "link": "http://arxiv.org/abs/1808.07118v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interactive Semantic Parsing for If-Then Recipes via Hierarchical\n  Reinforcement Learning", "abstract": "Given a text description, most existing semantic parsers synthesize a program\nin one shot. However, it is quite challenging to produce a correct program\nsolely based on the description, which in reality is often ambiguous or\nincomplete. In this paper, we investigate interactive semantic parsing, where\nthe agent can ask the user clarification questions to resolve ambiguities via a\nmulti-turn dialogue, on an important type of programs called \"If-Then recipes.\"\nWe develop a hierarchical reinforcement learning (HRL) based agent that\nsignificantly improves the parsing performance with minimal questions to the\nuser. Results under both simulation and human evaluation show that our agent\nsubstantially outperforms non-interactive semantic parsers and rule-based\nagents.", "published": "2018-08-21 02:39:08", "link": "http://arxiv.org/abs/1808.06740v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Generation of Text Descriptive Comments for Code Blocks", "abstract": "We propose a framework to automatically generate descriptive comments for\nsource code blocks. While this problem has been studied by many researchers\npreviously, their methods are mostly based on fixed template and achieves poor\nresults. Our framework does not rely on any template, but makes use of a new\nrecursive neural network called Code-RNN to extract features from the source\ncode and embed them into one vector. When this vector representation is input\nto a new recurrent neural network (Code-GRU), the overall framework generates\ntext descriptions of the code with accuracy (Rouge-2 value) significantly\nhigher than other learning-based approaches such as sequence-to-sequence model.\nThe Code-RNN model can also be used in other scenario where the representation\nof code is required.", "published": "2018-08-21 12:53:52", "link": "http://arxiv.org/abs/1808.06880v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Aiming to Know You Better Perhaps Makes Me a More Engaging Dialogue\n  Partner", "abstract": "There have been several attempts to define a plausible motivation for a\nchit-chat dialogue agent that can lead to engaging conversations. In this work,\nwe explore a new direction where the agent specifically focuses on discovering\ninformation about its interlocutor. We formalize this approach by defining a\nquantitative metric. We propose an algorithm for the agent to maximize it. We\nvalidate the idea with human evaluation where our system outperforms various\nbaselines. We demonstrate that the metric indeed correlates with the human\njudgments of engagingness.", "published": "2018-08-21 19:52:08", "link": "http://arxiv.org/abs/1808.07104v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Multi-Source Pointer Network for Product Title Summarization", "abstract": "In this paper, we study the product title summarization problem in E-commerce\napplications for display on mobile devices. Comparing with conventional\nsentence summarization, product title summarization has some extra and\nessential constraints. For example, factual errors or loss of the key\ninformation are intolerable for E-commerce applications. Therefore, we abstract\ntwo more constraints for product title summarization: (i) do not introduce\nirrelevant information; (ii) retain the key information (e.g., brand name and\ncommodity name). To address these issues, we propose a novel multi-source\npointer network by adding a new knowledge encoder for pointer network. The\nfirst constraint is handled by pointer mechanism. For the second constraint, we\nrestore the key information by copying words from the knowledge encoder with\nthe help of the soft gating mechanism. For evaluation, we build a large\ncollection of real-world product titles along with human-written short titles.\nExperimental results demonstrate that our model significantly outperforms the\nother baselines. Finally, online deployment of our proposed model has yielded a\nsignificant business impact, as measured by the click-through rate.", "published": "2018-08-21 13:07:53", "link": "http://arxiv.org/abs/1808.06885v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "QuAC : Question Answering in Context", "abstract": "We present QuAC, a dataset for Question Answering in Context that contains\n14K information-seeking QA dialogs (100K questions in total). The dialogs\ninvolve two crowd workers: (1) a student who poses a sequence of freeform\nquestions to learn as much as possible about a hidden Wikipedia text, and (2) a\nteacher who answers the questions by providing short excerpts from the text.\nQuAC introduces challenges not found in existing machine comprehension\ndatasets: its questions are often more open-ended, unanswerable, or only\nmeaningful within the dialog context, as we show in a detailed qualitative\nevaluation. We also report results for a number of reference models, including\na recently state-of-the-art reading comprehension architecture extended to\nmodel dialog context. Our best model underperforms humans by 20 F1, suggesting\nthat there is significant room for future work on this data. Dataset, baseline,\nand leaderboard available at http://quac.ai.", "published": "2018-08-21 17:46:12", "link": "http://arxiv.org/abs/1808.07036v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CoQA: A Conversational Question Answering Challenge", "abstract": "Humans gather information by engaging in conversations involving a series of\ninterconnected questions and answers. For machines to assist in information\ngathering, it is therefore essential to enable them to answer conversational\nquestions. We introduce CoQA, a novel dataset for building Conversational\nQuestion Answering systems. Our dataset contains 127k questions with answers,\nobtained from 8k conversations about text passages from seven diverse domains.\nThe questions are conversational, and the answers are free-form text with their\ncorresponding evidence highlighted in the passage. We analyze CoQA in depth and\nshow that conversational questions have challenging phenomena not present in\nexisting reading comprehension datasets, e.g., coreference and pragmatic\nreasoning. We evaluate strong conversational and reading comprehension models\non CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points\nbehind human performance (88.8%), indicating there is ample room for\nimprovement. We launch CoQA as a challenge to the community at\nhttp://stanfordnlp.github.io/coqa/", "published": "2018-08-21 17:52:02", "link": "http://arxiv.org/abs/1808.07042v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring a Unified Attention-Based Pooling Framework for Speaker\n  Verification", "abstract": "The pooling layer is an essential component in the neural network based\nspeaker verification. Most of the current networks in speaker verification use\naverage pooling to derive the utterance-level speaker representations. Average\npooling takes every frame as equally important, which is suboptimal since the\nspeaker-discriminant power is different between speech segments. In this paper,\nwe present a unified attention-based pooling framework and combine it with the\nmulti-head attention. Experiments on the Fisher and NIST SRE 2010 dataset show\nthat involving outputs from lower layers to compute the attention weights can\noutperform average pooling and achieve better results than vanilla attention\nmethod. The multi-head attention further improves the performance.", "published": "2018-08-21 20:28:46", "link": "http://arxiv.org/abs/1808.07120v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
