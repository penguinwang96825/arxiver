{"title": "Why Does Zero-Shot Cross-Lingual Generation Fail? An Explanation and a\n  Solution", "abstract": "Zero-shot cross-lingual transfer is when a multilingual model is trained to\nperform a task in one language and then is applied to another language.\nAlthough the zero-shot cross-lingual transfer approach has achieved success in\nvarious classification tasks, its performance on natural language generation\ntasks falls short in quality and sometimes outputs an incorrect language. In\nour study, we show that the fine-tuning process learns language invariant\nrepresentations, which is beneficial for classification tasks but harmful for\ngeneration tasks. Motivated by this, we propose a simple method to regularize\nthe model from learning language invariant representations and a method to\nselect model checkpoints without a development set in the target language, both\nresulting in better generation quality. Experiments on three semantically\ndiverse generation tasks show that our method reduces the accidental\ntranslation problem by 68% and improves the ROUGE-L score by 1.5 on average.", "published": "2023-05-27 02:04:19", "link": "http://arxiv.org/abs/2305.17325v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CGELBank Annotation Manual v1.1", "abstract": "CGELBank is a treebank and associated tools based on a syntactic formalism\nfor English derived from the Cambridge Grammar of the English Language. This\ndocument lays out the particularities of the CGELBank annotation scheme.", "published": "2023-05-27 03:01:53", "link": "http://arxiv.org/abs/2305.17347v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Good is Automatic Segmentation as a Multimodal Discourse Annotation\n  Aid?", "abstract": "Collaborative problem solving (CPS) in teams is tightly coupled with the\ncreation of shared meaning between participants in a situated, collaborative\ntask. In this work, we assess the quality of different utterance segmentation\ntechniques as an aid in annotating CPS. We (1) manually transcribe utterances\nin a dataset of triads collaboratively solving a problem involving dialogue and\nphysical object manipulation, (2) annotate collaborative moves according to\nthese gold-standard transcripts, and then (3) apply these annotations to\nutterances that have been automatically segmented using toolkits from Google\nand OpenAI's Whisper. We show that the oracle utterances have minimal\ncorrespondence to automatically segmented speech, and that automatically\nsegmented speech using different segmentation methods is also inconsistent. We\nalso show that annotating automatically segmented speech has distinct\nimplications compared with annotating oracle utterances--since most annotation\nschemes are designed for oracle cases, when annotating automatically-segmented\nutterances, annotators must invoke other information to make arbitrary\njudgments which other annotators may not replicate. We conclude with a\ndiscussion of how future annotation specs can account for these needs.", "published": "2023-05-27 03:06:15", "link": "http://arxiv.org/abs/2305.17350v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disambiguated Lexically Constrained Neural Machine Translation", "abstract": "Lexically constrained neural machine translation (LCNMT), which controls the\ntranslation generation with pre-specified constraints, is important in many\npractical applications. Current approaches to LCNMT typically assume that the\npre-specified lexical constraints are contextually appropriate. This assumption\nlimits their application to real-world scenarios where a source lexicon may\nhave multiple target constraints, and disambiguation is needed to select the\nmost suitable one. In this paper, we propose disambiguated LCNMT (D-LCNMT) to\nsolve the problem. D-LCNMT is a robust and effective two-stage framework that\ndisambiguates the constraints based on contexts at first, then integrates the\ndisambiguated constraints into LCNMT. Experimental results show that our\napproach outperforms strong baselines including existing data augmentation\nbased approaches on benchmark datasets, and comprehensive experiments in\nscenarios where a source lexicon corresponds to multiple target constraints\ndemonstrate the constraint disambiguation superiority of our approach.", "published": "2023-05-27 03:15:10", "link": "http://arxiv.org/abs/2305.17351v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Complementary and Integrative Health Lexicon (CIHLex) and Entity\n  Recognition in the Literature", "abstract": "Objective: Our study aimed to construct an exhaustive Complementary and\nIntegrative Health (CIH) Lexicon (CIHLex) to better represent the often\nunderrepresented physical and psychological CIH approaches in standard\nterminologies. We also intended to apply advanced Natural Language Processing\n(NLP) models such as Bidirectional Encoder Representations from Transformers\n(BERT) and GPT-3.5 Turbo for CIH named entity recognition, evaluating their\nperformance against established models like MetaMap and CLAMP. Materials and\nMethods: We constructed the CIHLex by integrating various resources, compiling\nand integrating data from biomedical literature and relevant knowledge bases.\nThe Lexicon encompasses 198 unique concepts with 1090 corresponding unique\nterms. We matched these concepts to the Unified Medical Language System (UMLS).\nAdditionally, we developed and utilized BERT models and compared their\nefficiency in CIH named entity recognition to that of other models such as\nMetaMap, CLAMP, and GPT3.5-turbo. Results: From the 198 unique concepts in\nCIHLex, 62.1% could be matched to at least one term in the UMLS. Moreover,\n75.7% of the mapped UMLS Concept Unique Identifiers (CUIs) were categorized as\n\"Therapeutic or Preventive Procedure.\" Among the models applied to CIH named\nentity recognition, BLUEBERT delivered the highest macro average F1-score of\n0.90, surpassing other models. Conclusion: Our CIHLex significantly augments\nrepresentation of CIH approaches in biomedical literature. Demonstrating the\nutility of advanced NLP models, BERT notably excelled in CIH entity\nrecognition. These results highlight promising strategies for enhancing\nstandardization and recognition of CIH terminology in biomedical contexts.", "published": "2023-05-27 03:21:36", "link": "http://arxiv.org/abs/2305.17353v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging the Granularity Gap for Acoustic Modeling", "abstract": "While Transformer has become the de-facto standard for speech, modeling upon\nthe fine-grained frame-level features remains an open challenge of capturing\nlong-distance dependencies and distributing the attention weights. We propose\n\\textit{Progressive Down-Sampling} (PDS) which gradually compresses the\nacoustic features into coarser-grained units containing more complete semantic\ninformation, like text-level representation. In addition, we develop a\nrepresentation fusion method to alleviate information loss that occurs\ninevitably during high compression. In this way, we compress the acoustic\nfeatures into 1/32 of the initial length while achieving better or comparable\nperformances on the speech recognition task. And as a bonus, it yields\ninference speedups ranging from 1.20$\\times$ to 1.47$\\times$. By reducing the\nmodeling burden, we also achieve competitive results when training on the more\nchallenging speech translation task.", "published": "2023-05-27 03:52:52", "link": "http://arxiv.org/abs/2305.17356v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CTC-based Non-autoregressive Speech Translation", "abstract": "Combining end-to-end speech translation (ST) and non-autoregressive (NAR)\ngeneration is promising in language and speech processing for their advantages\nof less error propagation and low latency. In this paper, we investigate the\npotential of connectionist temporal classification (CTC) for non-autoregressive\nspeech translation (NAST). In particular, we develop a model consisting of two\nencoders that are guided by CTC to predict the source and target texts,\nrespectively. Introducing CTC into NAST on both language sides has obvious\nchallenges: 1) the conditional independent generation somewhat breaks the\ninterdependency among tokens, and 2) the monotonic alignment assumption in\nstandard CTC does not hold in translation tasks. In response, we develop a\nprediction-aware encoding approach and a cross-layer attention approach to\naddress these issues. We also use curriculum learning to improve convergence of\ntraining. Experiments on the MuST-C ST benchmarks show that our NAST model\nachieves an average BLEU score of 29.5 with a speed-up of 5.67$\\times$, which\nis comparable to the autoregressive counterpart and even outperforms the\nprevious best result of 0.9 BLEU points.", "published": "2023-05-27 03:54:09", "link": "http://arxiv.org/abs/2305.17358v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmenting Large Language Model Translators via Translation Memories", "abstract": "Using translation memories (TMs) as prompts is a promising approach to\nin-context learning of machine translation models. In this work, we take a step\ntowards prompting large language models (LLMs) with TMs and making them better\ntranslators. We find that the ability of LLMs to ``understand'' prompts is\nindeed helpful for making better use of TMs. Experiments show that the results\nof a pre-trained LLM translator can be greatly improved by using high-quality\nTM-based prompts. These results are even comparable to those of the\nstate-of-the-art NMT systems which have access to large-scale in-domain\nbilingual data and are well tuned on the downstream tasks.", "published": "2023-05-27 04:47:09", "link": "http://arxiv.org/abs/2305.17367v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Better Entity Linking with Multi-View Enhanced Distillation", "abstract": "Dense retrieval is widely used for entity linking to retrieve entities from\nlarge-scale knowledge bases. Mainstream techniques are based on a dual-encoder\nframework, which encodes mentions and entities independently and calculates\ntheir relevances via rough interaction metrics, resulting in difficulty in\nexplicitly modeling multiple mention-relevant parts within entities to match\ndivergent mentions. Aiming at learning entity representations that can match\ndivergent mentions, this paper proposes a Multi-View Enhanced Distillation\n(MVD) framework, which can effectively transfer knowledge of multiple\nfine-grained and mention-relevant parts within entities from cross-encoders to\ndual-encoders. Each entity is split into multiple views to avoid irrelevant\ninformation being over-squashed into the mention-relevant view. We further\ndesign cross-alignment and self-alignment mechanisms for this framework to\nfacilitate fine-grained knowledge distillation from the teacher model to the\nstudent model. Meanwhile, we reserve a global-view that embeds the entity as a\nwhole to prevent dispersal of uniform information. Experiments show our method\nachieves state-of-the-art performance on several entity linking benchmarks.", "published": "2023-05-27 05:15:28", "link": "http://arxiv.org/abs/2305.17371v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parallel Corpus for Indigenous Language Translation: Spanish-Mazatec and\n  Spanish-Mixtec", "abstract": "In this paper, we present a parallel Spanish-Mazatec and Spanish-Mixtec\ncorpus for machine translation (MT) tasks, where Mazatec and Mixtec are two\nindigenous Mexican languages. We evaluated the usability of the collected\ncorpus using three different approaches: transformer, transfer learning, and\nfine-tuning pre-trained multilingual MT models. Fine-tuning the Facebook\nM2M100-48 model outperformed the other approaches, with BLEU scores of 12.09\nand 22.25 for Mazatec-Spanish and Spanish-Mazatec translations, respectively,\nand 16.75 and 22.15 for Mixtec-Spanish and Spanish-Mixtec translations,\nrespectively. The findings show that the dataset size (9,799 sentences in\nMazatec and 13,235 sentences in Mixtec) affects translation performance and\nthat indigenous languages work better when used as target languages. The\nfindings emphasize the importance of creating parallel corpora for indigenous\nlanguages and fine-tuning models for low-resource translation tasks. Future\nresearch will investigate zero-shot and few-shot learning approaches to further\nimprove translation performance in low-resource settings. The dataset and\nscripts are available at\n\\url{https://github.com/atnafuatx/Machine-Translation-Resources}", "published": "2023-05-27 08:03:44", "link": "http://arxiv.org/abs/2305.17404v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Translation for Indigenous Languages: Experiments with\n  Multilingual Models", "abstract": "This paper describes CIC NLP's submission to the AmericasNLP 2023 Shared Task\non machine translation systems for indigenous languages of the Americas. We\npresent the system descriptions for three methods. We used two multilingual\nmodels, namely M2M-100 and mBART50, and one bilingual (one-to-one) -- Helsinki\nNLP Spanish-English translation model, and experimented with different transfer\nlearning setups. We experimented with 11 languages from America and report the\nsetups we used as well as the results we achieved. Overall, the mBART setup was\nable to improve upon the baseline for three out of the eleven languages.", "published": "2023-05-27 08:10:40", "link": "http://arxiv.org/abs/2305.17406v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Practical Toolkit for Multilingual Question and Answer Generation", "abstract": "Generating questions along with associated answers from a text has\napplications in several domains, such as creating reading comprehension tests\nfor students, or improving document search by providing auxiliary questions and\nanswers based on the query. Training models for question and answer generation\n(QAG) is not straightforward due to the expected structured output (i.e. a list\nof question and answer pairs), as it requires more than generating a single\nsentence. This results in a small number of publicly accessible QAG models. In\nthis paper, we introduce AutoQG, an online service for multilingual QAG, along\nwith lmqg, an all-in-one Python package for model fine-tuning, generation, and\nevaluation. We also release QAG models in eight languages fine-tuned on a few\nvariants of pre-trained encoder-decoder language models, which can be used\nonline via AutoQG or locally via lmqg. With these resources, practitioners of\nany level can benefit from a toolkit that includes a web interface for end\nusers, and easy-to-use code for developers who require custom models or\nfine-grained controls for generation.", "published": "2023-05-27 08:42:37", "link": "http://arxiv.org/abs/2305.17416v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Adversarial Attack on Pre-trained Language Models as Sequential\n  Decision Making", "abstract": "Pre-trained language models (PLMs) have been widely used to underpin various\ndownstream tasks. However, the adversarial attack task has found that PLMs are\nvulnerable to small perturbations. Mainstream methods adopt a detached\ntwo-stage framework to attack without considering the subsequent influence of\nsubstitution at each step. In this paper, we formally model the adversarial\nattack task on PLMs as a sequential decision-making problem, where the whole\nattack process is sequential with two decision-making problems, i.e., word\nfinder and word substitution. Considering the attack process can only receive\nthe final state without any direct intermediate signals, we propose to use\nreinforcement learning to find an appropriate sequential attack path to\ngenerate adversaries, named SDM-Attack. Extensive experimental results show\nthat SDM-Attack achieves the highest attack success rate with a comparable\nmodification rate and semantic similarity to attack fine-tuned BERT.\nFurthermore, our analyses demonstrate the generalization and transferability of\nSDM-Attack. The code is available at https://github.com/fduxuan/SDM-Attack.", "published": "2023-05-27 10:33:53", "link": "http://arxiv.org/abs/2305.17440v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weaker Than You Think: A Critical Look at Weakly Supervised Learning", "abstract": "Weakly supervised learning is a popular approach for training machine\nlearning models in low-resource settings. Instead of requesting high-quality\nyet costly human annotations, it allows training models with noisy annotations\nobtained from various weak sources. Recently, many sophisticated approaches\nhave been proposed for robust training under label noise, reporting impressive\nresults. In this paper, we revisit the setup of these approaches and find that\nthe benefits brought by these approaches are significantly overestimated.\nSpecifically, we find that the success of existing weakly supervised learning\napproaches heavily relies on the availability of clean validation samples\nwhich, as we show, can be leveraged much more efficiently by simply training on\nthem. After using these clean labels in training, the advantages of using these\nsophisticated approaches are mostly wiped out. This remains true even when\nreducing the size of the available clean data to just five samples per class,\nmaking these approaches impractical. To understand the true value of weakly\nsupervised learning, we thoroughly analyze diverse NLP datasets and tasks to\nascertain when and why weakly supervised approaches work. Based on our\nfindings, we provide recommendations for future research.", "published": "2023-05-27 10:46:50", "link": "http://arxiv.org/abs/2305.17442v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Your ASTE Models in The Wild: A Diversified Multi-domain\n  Dataset For Aspect Sentiment Triplet Extraction", "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is widely used in various\napplications. However, existing ASTE datasets are limited in their ability to\nrepresent real-world scenarios, hindering the advancement of research in this\narea. In this paper, we introduce a new dataset, named DMASTE, which is\nmanually annotated to better fit real-world scenarios by providing more diverse\nand realistic reviews for the task. The dataset includes various lengths,\ndiverse expressions, more aspect types, and more domains than existing\ndatasets. We conduct extensive experiments on DMASTE in multiple settings to\nevaluate previous ASTE approaches. Empirical results demonstrate that DMASTE is\na more challenging ASTE dataset. Further analyses of in-domain and cross-domain\nsettings provide promising directions for future research. Our code and dataset\nare available at https://github.com/NJUNLP/DMASTE.", "published": "2023-05-27 11:21:32", "link": "http://arxiv.org/abs/2305.17448v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Diffusion Model for Event Skeleton Generation", "abstract": "Event skeleton generation, aiming to induce an event schema skeleton graph\nwith abstracted event nodes and their temporal relations from a set of event\ninstance graphs, is a critical step in the temporal complex event schema\ninduction task. Existing methods effectively address this task from a graph\ngeneration perspective but suffer from noise-sensitive and error accumulation,\ne.g., the inability to correct errors while generating schema. We, therefore,\npropose a novel Diffusion Event Graph Model~(DEGM) to address these issues. Our\nDEGM is the first workable diffusion model for event skeleton generation, where\nthe embedding and rounding techniques with a custom edge-based loss are\nintroduced to transform a discrete event graph into learnable latent\nrepresentation. Furthermore, we propose a denoising training process to\nmaintain the model's robustness. Consequently, DEGM derives the final schema,\nwhere error correction is guaranteed by iteratively refining the latent\nrepresentation during the schema generation process. Experimental results on\nthree IED bombing datasets demonstrate that our DEGM achieves better results\nthan other state-of-the-art baselines. Our code and data are available at\nhttps://github.com/zhufq00/EventSkeletonGeneration.", "published": "2023-05-27 12:19:21", "link": "http://arxiv.org/abs/2305.17458v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FERMAT: An Alternative to Accuracy for Numerical Reasoning", "abstract": "While pre-trained language models achieve impressive performance on various\nNLP benchmarks, they still struggle with tasks that require numerical\nreasoning. Recent advances in improving numerical reasoning are mostly achieved\nusing very large language models that contain billions of parameters and are\nnot accessible to everyone. In addition, numerical reasoning is measured using\na single score on existing datasets. As a result, we do not have a clear\nunderstanding of the strengths and shortcomings of existing models on different\nnumerical reasoning aspects and therefore, potential ways to improve them apart\nfrom scaling them up. Inspired by CheckList (Ribeiro et al., 2020), we\nintroduce a multi-view evaluation set for numerical reasoning in English,\ncalled FERMAT. Instead of reporting a single score on a whole dataset, FERMAT\nevaluates models on various key numerical reasoning aspects such as number\nunderstanding, mathematical operations, and training dependency. Apart from\nproviding a comprehensive evaluation of models on different numerical reasoning\naspects, FERMAT enables a systematic and automated generation of an arbitrarily\nlarge training or evaluation set for each aspect.The datasets and codes are\npublicly available to generate further multi-view data for ulterior tasks and\nlanguages.", "published": "2023-05-27 15:00:45", "link": "http://arxiv.org/abs/2305.17491v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FACTUAL: A Benchmark for Faithful and Consistent Textual Scene Graph\n  Parsing", "abstract": "Textual scene graph parsing has become increasingly important in various\nvision-language applications, including image caption evaluation and image\nretrieval. However, existing scene graph parsers that convert image captions\ninto scene graphs often suffer from two types of errors. First, the generated\nscene graphs fail to capture the true semantics of the captions or the\ncorresponding images, resulting in a lack of faithfulness. Second, the\ngenerated scene graphs have high inconsistency, with the same semantics\nrepresented by different annotations.\n  To address these challenges, we propose a novel dataset, which involves\nre-annotating the captions in Visual Genome (VG) using a new intermediate\nrepresentation called FACTUAL-MR. FACTUAL-MR can be directly converted into\nfaithful and consistent scene graph annotations. Our experimental results\nclearly demonstrate that the parser trained on our dataset outperforms existing\napproaches in terms of faithfulness and consistency. This improvement leads to\na significant performance boost in both image caption evaluation and zero-shot\nimage retrieval tasks. Furthermore, we introduce a novel metric for measuring\nscene graph similarity, which, when combined with the improved scene graph\nparser, achieves state-of-the-art (SOTA) results on multiple benchmark datasets\nfor the aforementioned tasks. The code and dataset are available at\nhttps://github.com/zhuang-li/FACTUAL .", "published": "2023-05-27 15:38:31", "link": "http://arxiv.org/abs/2305.17497v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MeetingBank: A Benchmark Dataset for Meeting Summarization", "abstract": "As the number of recorded meetings increases, it becomes increasingly\nimportant to utilize summarization technology to create useful summaries of\nthese recordings. However, there is a crucial lack of annotated meeting corpora\nfor developing this technology, as it can be hard to collect meetings,\nespecially when the topics discussed are confidential. Furthermore, meeting\nsummaries written by experienced writers are scarce, making it hard for\nabstractive summarizers to produce sensible output without a reliable\nreference. This lack of annotated corpora has hindered the development of\nmeeting summarization technology. In this paper, we present MeetingBank, a new\nbenchmark dataset of city council meetings over the past decade. MeetingBank is\nunique among other meeting corpora due to its divide-and-conquer approach,\nwhich involves dividing professionally written meeting minutes into shorter\npassages and aligning them with specific segments of the meeting. This breaks\ndown the process of summarizing a lengthy meeting into smaller, more manageable\ntasks. The dataset provides a new testbed of various meeting summarization\nsystems and also allows the public to gain insight into how council decisions\nare made. We make the collection, including meeting video links, transcripts,\nreference summaries, agenda, and other metadata, publicly available to\nfacilitate the development of better meeting summarization techniques. Our\ndataset can be accessed at: https://meetingbank.github.io", "published": "2023-05-27 17:09:25", "link": "http://arxiv.org/abs/2305.17529v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grounding Characters and Places in Narrative Texts", "abstract": "Tracking characters and locations throughout a story can help improve the\nunderstanding of its plot structure. Prior research has analyzed characters and\nlocations from text independently without grounding characters to their\nlocations in narrative time. Here, we address this gap by proposing a new\nspatial relationship categorization task. The objective of the task is to\nassign a spatial relationship category for every character and location\nco-mention within a window of text, taking into consideration linguistic\ncontext, narrative tense, and temporal scope. To this end, we annotate spatial\nrelationships in approximately 2500 book excerpts and train a model using\ncontextual embeddings as features to predict these relationships. When applied\nto a set of books, this model allows us to test several hypotheses on mobility\nand domestic space, revealing that protagonists are more mobile than\nnon-central characters and that women as characters tend to occupy more\ninterior space than men. Overall, our work is the first step towards joint\nmodeling and analysis of characters and places in narrative text.", "published": "2023-05-27 19:31:41", "link": "http://arxiv.org/abs/2305.17561v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ArPanEmo: An Open-Source Dataset for Fine-Grained Emotion Recognition in\n  Arabic Online Content during COVID-19 Pandemic", "abstract": "Emotion recognition is a crucial task in Natural Language Processing (NLP)\nthat enables machines to comprehend the feelings conveyed in the text. The\napplications of emotion recognition are diverse, including mental health\ndiagnosis, student support, and the detection of online suspicious behavior.\nDespite the substantial amount of literature available on emotion recognition\nin various languages, Arabic emotion recognition has received relatively little\nattention, leading to a scarcity of emotion-annotated corpora. This paper\npresents the ArPanEmo dataset, a novel dataset for fine-grained emotion\nrecognition of online posts in Arabic. The dataset comprises 11,128 online\nposts manually labeled for ten emotion categories or neutral, with Fleiss'\nkappa of 0.71. It targets a specific Arabic dialect and addresses topics\nrelated to the COVID-19 pandemic, making it the first and largest of its kind.\nPython's packages were utilized to collect online posts related to the COVID-19\npandemic from three sources: Twitter, YouTube, and online newspaper comments\nbetween March 2020 and March 2022. Upon collection of the online posts, each\none underwent a semi-automatic classification process using a lexicon of\nemotion-related terms to determine whether it belonged to the neutral or\nemotional category. Subsequently, manual labeling was conducted to further\ncategorize the emotional data into fine-grained emotion categories.", "published": "2023-05-27 21:04:26", "link": "http://arxiv.org/abs/2305.17580v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Two-Stage Decoder for Efficient ICD Coding", "abstract": "Clinical notes in healthcare facilities are tagged with the International\nClassification of Diseases (ICD) code; a list of classification codes for\nmedical diagnoses and procedures. ICD coding is a challenging multilabel text\nclassification problem due to noisy clinical document inputs and long-tailed\nlabel distribution. Recent automated ICD coding efforts improve performance by\nencoding medical notes and codes with additional data and knowledge bases.\nHowever, most of them do not reflect how human coders generate the code: first,\nthe coders select general code categories and then look for specific\nsubcategories that are relevant to a patient's condition. Inspired by this, we\npropose a two-stage decoding mechanism to predict ICD codes. Our model uses the\nhierarchical properties of the codes to split the prediction into two steps: At\nfirst, we predict the parent code and then predict the child code based on the\nprevious prediction. Experiments on the public MIMIC-III data set show that our\nmodel performs well in single-model settings without external data or\nknowledge.", "published": "2023-05-27 17:25:13", "link": "http://arxiv.org/abs/2306.00005v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmentation-Adapted Retriever Improves Generalization of Language\n  Models as Generic Plug-In", "abstract": "Retrieval augmentation can aid language models (LMs) in knowledge-intensive\ntasks by supplying them with external information. Prior works on retrieval\naugmentation usually jointly fine-tune the retriever and the LM, making them\nclosely coupled. In this paper, we explore the scheme of generic retrieval\nplug-in: the retriever is to assist target LMs that may not be known beforehand\nor are unable to be fine-tuned together. To retrieve useful documents for\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\nlearns LM's preferences obtained from a known source LM. Experiments on the\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\nis able to significantly improve the zero-shot generalization of larger target\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\nthat the preferences of different LMs overlap, enabling AAR trained with a\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.", "published": "2023-05-27 02:26:52", "link": "http://arxiv.org/abs/2305.17331v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning Language Models with Just Forward Passes", "abstract": "Fine-tuning language models (LMs) has yielded success on diverse downstream\ntasks, but as LMs grow in size, backpropagation requires a prohibitively large\namount of memory. Zeroth-order (ZO) methods can in principle estimate gradients\nusing only two forward passes but are theorized to be catastrophically slow for\noptimizing large models. In this work, we propose a memory-efficient\nzerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate\nin-place, thereby fine-tuning LMs with the same memory footprint as inference.\nFor example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter\nmodel, whereas fine-tuning with backpropagation can train only a 2.7B LM with\nthe same budget. We conduct comprehensive experiments across model types\n(masked and autoregressive LMs), model scales (up to 66B), and downstream tasks\n(classification, multiple-choice, and generation). Our results demonstrate that\n(1) MeZO significantly outperforms in-context learning and linear probing; (2)\nMeZO achieves comparable performance to fine-tuning with backpropagation across\nmultiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reduction\nin our implementation; (3) MeZO is compatible with both full-parameter and\nparameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO\ncan effectively optimize non-differentiable objectives (e.g., maximizing\naccuracy or F1). We support our empirical findings with theoretical insights,\nhighlighting how adequate pre-training and task prompts enable MeZO to\nfine-tune huge models, despite classical ZO analyses suggesting otherwise.", "published": "2023-05-27 02:28:10", "link": "http://arxiv.org/abs/2305.17333v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Benchmarking Diverse-Modal Entity Linking with Generative Models", "abstract": "Entities can be expressed in diverse formats, such as texts, images, or\ncolumn names and cell values in tables. While existing entity linking (EL)\nmodels work well on per modality configuration, such as text-only EL, visual\ngrounding, or schema linking, it is more challenging to design a unified model\nfor diverse modality configurations. To bring various modality configurations\ntogether, we constructed a benchmark for diverse-modal EL (DMEL) from existing\nEL datasets, covering all three modalities including text, image, and table. To\napproach the DMEL task, we proposed a generative diverse-modal model (GDMM)\nfollowing a multimodal-encoder-decoder paradigm. Pre-training \\Model with rich\ncorpora builds a solid foundation for DMEL without storing the entire KB for\ninference. Fine-tuning GDMM builds a stronger DMEL baseline, outperforming\nstate-of-the-art task-specific EL models by 8.51 F1 score on average.\nAdditionally, extensive error analyses are conducted to highlight the\nchallenges of DMEL, facilitating future research on this task.", "published": "2023-05-27 02:38:46", "link": "http://arxiv.org/abs/2305.17337v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of\n  GPT-Generated Text", "abstract": "Large language models (LLMs) have notably enhanced the fluency and diversity\nof machine-generated text. However, this progress also presents a significant\nchallenge in detecting the origin of a given text, and current research on\ndetection methods lags behind the rapid evolution of LLMs. Conventional\ntraining-based methods have limitations in flexibility, particularly when\nadapting to new domains, and they often lack explanatory power. To address this\ngap, we propose a novel training-free detection strategy called Divergent\nN-Gram Analysis (DNA-GPT). Given a text, we first truncate it in the middle and\nthen use only the preceding portion as input to the LLMs to regenerate the new\nremaining parts. By analyzing the differences between the original and new\nremaining parts through N-gram analysis in black-box or probability divergence\nin white-box, we unveil significant discrepancies between the distribution of\nmachine-generated text and the distribution of human-written text. We conducted\nextensive experiments on the most advanced LLMs from OpenAI, including\ntext-davinci-003, GPT-3.5-turbo, and GPT-4, as well as open-source models such\nas GPT-NeoX-20B and LLaMa-13B. Results show that our zero-shot approach\nexhibits state-of-the-art performance in distinguishing between human and\nGPT-generated text on four English and one German dataset, outperforming\nOpenAI's own classifier, which is trained on millions of text. Additionally,\nour methods provide reasonable explanations and evidence to support our claim,\nwhich is a unique feature of explainable detection. Our method is also robust\nunder the revised text attack and can additionally solve model sourcing. Codes\nare available at https://github.com/Xianjun-Yang/DNA-GPT.", "published": "2023-05-27 03:58:29", "link": "http://arxiv.org/abs/2305.17359v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero- and Few-Shot Event Detection via Prompt-Based Meta Learning", "abstract": "With emerging online topics as a source for numerous new events, detecting\nunseen / rare event types presents an elusive challenge for existing event\ndetection methods, where only limited data access is provided for training. To\naddress the data scarcity problem in event detection, we propose MetaEvent, a\nmeta learning-based framework for zero- and few-shot event detection.\nSpecifically, we sample training tasks from existing event types and perform\nmeta training to search for optimal parameters that quickly adapt to unseen\ntasks. In our framework, we propose to use the cloze-based prompt and a\ntrigger-aware soft verbalizer to efficiently project output to unseen event\ntypes. Moreover, we design a contrastive meta objective based on maximum mean\ndiscrepancy (MMD) to learn class-separating features. As such, the proposed\nMetaEvent can perform zero-shot event detection by mapping features to event\ntypes without any prior knowledge. In our experiments, we demonstrate the\neffectiveness of MetaEvent in both zero-shot and few-shot scenarios, where the\nproposed method achieves state-of-the-art performance in extensive experiments\non benchmark datasets FewEvent and MAVEN.", "published": "2023-05-27 05:36:46", "link": "http://arxiv.org/abs/2305.17373v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Generalization in Language Model-Based Text-to-SQL Semantic\n  Parsing: Two Simple Semantic Boundary-Based Techniques", "abstract": "Compositional and domain generalization present significant challenges in\nsemantic parsing, even for state-of-the-art semantic parsers based on\npre-trained language models (LMs). In this study, we empirically investigate\nimproving an LM's generalization in semantic parsing with two simple\ntechniques: at the token level, we introduce a token preprocessing method to\npreserve the semantic boundaries of tokens produced by LM tokenizers; at the\nsequence level, we propose to use special tokens to mark the boundaries of\ncomponents aligned between input and output. Our experimental results on two\ntext-to-SQL semantic parsing datasets show that our token preprocessing,\nalthough simple, can substantially improve the LM performance on both types of\ngeneralization, and our component boundary marking method is particularly\nhelpful for compositional generalization.", "published": "2023-05-27 06:09:03", "link": "http://arxiv.org/abs/2305.17378v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "MPCHAT: Towards Multimodal Persona-Grounded Conversation", "abstract": "In order to build self-consistent personalized dialogue agents, previous\nresearch has mostly focused on textual persona that delivers personal facts or\npersonalities. However, to fully describe the multi-faceted nature of persona,\nimage modality can help better reveal the speaker's personal characteristics\nand experiences in episodic memory (Rubin et al., 2003; Conway, 2009). In this\nwork, we extend persona-based dialogue to the multimodal domain and make two\nmain contributions. First, we present the first multimodal persona-based\ndialogue dataset named MPCHAT, which extends persona with both text and images\nto contain episodic memories. Second, we empirically show that incorporating\nmultimodal persona, as measured by three proposed multimodal persona-grounded\ndialogue tasks (i.e., next response prediction, grounding persona prediction,\nand speaker identification), leads to statistically significant performance\nimprovements across all tasks. Thus, our work highlights that multimodal\npersona is crucial for improving multimodal dialogue comprehension, and our\nMPCHAT serves as a high-quality resource for this research.", "published": "2023-05-27 06:46:42", "link": "http://arxiv.org/abs/2305.17388v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Answering Unanswered Questions through Semantic Reformulations in Spoken\n  QA", "abstract": "Spoken Question Answering (QA) is a key feature of voice assistants, usually\nbacked by multiple QA systems. Users ask questions via spontaneous speech which\ncan contain disfluencies, errors, and informal syntax or phrasing. This is a\nmajor challenge in QA, causing unanswered questions or irrelevant answers, and\nleading to bad user experiences. We analyze failed QA requests to identify core\nchallenges: lexical gaps, proposition types, complex syntactic structure, and\nhigh specificity. We propose a Semantic Question Reformulation (SURF) model\noffering three linguistically-grounded operations (repair, syntactic reshaping,\ngeneralization) to rewrite questions to facilitate answering. Offline\nevaluation on 1M unanswered questions from a leading voice assistant shows that\nSURF significantly improves answer rates: up to 24% of previously unanswered\nquestions obtain relevant answers (75%). Live deployment shows positive impact\nfor millions of customers with unanswered questions; explicit relevance\nfeedback shows high user satisfaction.", "published": "2023-05-27 07:19:27", "link": "http://arxiv.org/abs/2305.17393v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Better Text Image Translation with Multimodal Codebook", "abstract": "Text image translation (TIT) aims to translate the source texts embedded in\nthe image to target translations, which has a wide range of applications and\nthus has important research value. However, current studies on TIT are\nconfronted with two main bottlenecks: 1) this task lacks a publicly available\nTIT dataset, 2) dominant models are constructed in a cascaded manner, which\ntends to suffer from the error propagation of optical character recognition\n(OCR). In this work, we first annotate a Chinese-English TIT dataset named\nOCRMT30K, providing convenience for subsequent studies. Then, we propose a TIT\nmodel with a multimodal codebook, which is able to associate the image with\nrelevant texts, providing useful supplementary information for translation.\nMoreover, we present a multi-stage training framework involving text machine\ntranslation, image-text alignment, and TIT tasks, which fully exploits\nadditional bilingual texts, OCR dataset and our OCRMT30K dataset to train our\nmodel. Extensive experiments and in-depth analyses strongly demonstrate the\neffectiveness of our proposed model and training framework.", "published": "2023-05-27 08:41:18", "link": "http://arxiv.org/abs/2305.17415v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding Emotion Valence is a Joint Deep Learning Task", "abstract": "The valence analysis of speakers' utterances or written posts helps to\nunderstand the activation and variations of the emotional state throughout the\nconversation. More recently, the concept of Emotion Carriers (EC) has been\nintroduced to explain the emotion felt by the speaker and its manifestations.\nIn this work, we investigate the natural inter-dependency of valence and ECs\nvia a multi-task learning approach. We experiment with Pre-trained Language\nModels (PLM) for single-task, two-step, and joint settings for the valence and\nEC prediction tasks. We compare and evaluate the performance of generative\n(GPT-2) and discriminative (BERT) architectures in each setting. We observed\nthat providing the ground truth label of one task improves the prediction\nperformance of the models in the other task. We further observed that the\ndiscriminative model achieves the best trade-off of valence and EC prediction\ntasks in the joint prediction setting. As a result, we attain a single model\nthat performs both tasks, thus, saving computation resources at training and\ninference times.", "published": "2023-05-27 09:07:18", "link": "http://arxiv.org/abs/2305.17422v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Unified Framework for Slot based Response Generation in a Multimodal\n  Dialogue System", "abstract": "Natural Language Understanding (NLU) and Natural Language Generation (NLG)\nare the two critical components of every conversational system that handles the\ntask of understanding the user by capturing the necessary information in the\nform of slots and generating an appropriate response in accordance with the\nextracted information. Recently, dialogue systems integrated with complementary\ninformation such as images, audio, or video have gained immense popularity. In\nthis work, we propose an end-to-end framework with the capability to extract\nnecessary slot values from the utterance and generate a coherent response,\nthereby assisting the user to achieve their desired goals in a multimodal\ndialogue system having both textual and visual information. The task of\nextracting the necessary information is dependent not only on the text but also\non the visual cues present in the dialogue. Similarly, for the generation, the\nprevious dialog context comprising multimodal information is significant for\nproviding coherent and informative responses. We employ a multimodal\nhierarchical encoder using pre-trained DialoGPT and also exploit the knowledge\nbase (Kb) to provide a stronger context for both the tasks. Finally, we design\na slot attention mechanism to focus on the necessary information in a given\nutterance. Lastly, a decoder generates the corresponding response for the given\ndialogue context and the extracted slot values. Experimental results on the\nMultimodal Dialogue Dataset (MMD) show that the proposed framework outperforms\nthe baselines approaches in both the tasks. The code is available at\nhttps://github.com/avinashsai/slot-gpt.", "published": "2023-05-27 10:06:03", "link": "http://arxiv.org/abs/2305.17433v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CrossGET: Cross-Guided Ensemble of Tokens for Accelerating\n  Vision-Language Transformers", "abstract": "Recent vision-language models have achieved tremendous advances. However,\ntheir computational costs are also escalating dramatically, making model\nacceleration exceedingly critical. To pursue more efficient vision-language\nTransformers, this paper introduces Cross-Guided Ensemble of Tokens (CrossGET),\na general acceleration framework for vision-language Transformers. This\nframework adaptively combines tokens in real-time during inference,\nsignificantly reducing computational costs while maintaining high performance.\nCrossGET features two primary innovations: 1) Cross-Guided Matching and\nEnsemble. CrossGET leverages cross-modal guided token matching and ensemble to\neffectively utilize cross-modal information, achieving wider applicability\nacross both modality-independent models, e.g., CLIP, and modality-dependent\nones, e.g., BLIP2. 2) Complete-Graph Soft Matching. CrossGET introduces an\nalgorithm for the token-matching mechanism, ensuring reliable matching results\nwhile facilitating parallelizability and high efficiency. Extensive experiments\nhave been conducted on various vision-language tasks, such as image-text\nretrieval, visual reasoning, image captioning, and visual question answering.\nThe performance on both classic multimodal architectures and emerging\nmultimodal LLMs demonstrates the framework's effectiveness and versatility. The\ncode is available at https://github.com/sdc17/CrossGET.", "published": "2023-05-27 12:07:21", "link": "http://arxiv.org/abs/2305.17455v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Match Made in Heaven: A Multi-task Framework for Hyperbole and\n  Metaphor Detection", "abstract": "Hyperbole and metaphor are common in day-to-day communication (e.g., \"I am in\ndeep trouble\": how does trouble have depth?), which makes their detection\nimportant, especially in a conversational AI setting. Existing approaches to\nautomatically detect metaphor and hyperbole have studied these language\nphenomena independently, but their relationship has hardly, if ever, been\nexplored computationally. In this paper, we propose a multi-task deep learning\nframework to detect hyperbole and metaphor simultaneously. We hypothesize that\nmetaphors help in hyperbole detection, and vice-versa. To test this hypothesis,\nwe annotate two hyperbole datasets- HYPO and HYPO-L- with metaphor labels.\nSimultaneously, we annotate two metaphor datasets- TroFi and LCC- with\nhyperbole labels. Experiments using these datasets give an improvement of the\nstate of the art of hyperbole detection by 12%. Additionally, our multi-task\nlearning (MTL) approach shows an improvement of up to 17% over single-task\nlearning (STL) for both hyperbole and metaphor detection, supporting our\nhypothesis. To the best of our knowledge, ours is the first demonstration of\ncomputational leveraging of linguistic intimacy between metaphor and hyperbole,\nleading to showing the superiority of MTL over STL for hyperbole and metaphor\ndetection.", "published": "2023-05-27 14:17:59", "link": "http://arxiv.org/abs/2305.17480v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning from Children: Improving Image-Caption Pretraining via\n  Curriculum", "abstract": "Image-caption pretraining has been quite successfully used for downstream\nvision tasks like zero-shot image classification and object detection. However,\nimage-caption pretraining is still a hard problem -- it requires multiple\nconcepts (nouns) from captions to be aligned to several objects in images. To\ntackle this problem, we go to the roots -- the best learner, children. We take\ninspiration from cognitive science studies dealing with children's language\nlearning to propose a curriculum learning framework. The learning begins with\neasy-to-align image caption pairs containing one concept per caption. The\ndifficulty is progressively increased with each new phase by adding one more\nconcept per caption. Correspondingly, the knowledge acquired in each learning\nphase is utilized in subsequent phases to effectively constrain the learning\nproblem to aligning one new concept-object pair in each phase. We show that\nthis learning strategy improves over vanilla image-caption training in various\nsettings -- pretraining from scratch, using a pretrained image or/and\npretrained text encoder, low data regime etc.", "published": "2023-05-27 17:59:54", "link": "http://arxiv.org/abs/2305.17540v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Non-Sequential Graph Script Induction via Multimedia Grounding", "abstract": "Online resources such as WikiHow compile a wide range of scripts for\nperforming everyday tasks, which can assist models in learning to reason about\nprocedures. However, the scripts are always presented in a linear manner, which\ndoes not reflect the flexibility displayed by people executing tasks in real\nlife. For example, in the CrossTask Dataset, 64.5% of consecutive step pairs\nare also observed in the reverse order, suggesting their ordering is not fixed.\nIn addition, each step has an average of 2.56 frequent next steps,\ndemonstrating \"branching\". In this paper, we propose the new challenging task\nof non-sequential graph script induction, aiming to capture optional and\ninterchangeable steps in procedural planning. To automate the induction of such\ngraph scripts for given tasks, we propose to take advantage of loosely aligned\nvideos of people performing the tasks. In particular, we design a multimodal\nframework to ground procedural videos to WikiHow textual steps and thus\ntransform each video into an observed step path on the latent ground truth\ngraph script. This key transformation enables us to train a script knowledge\nmodel capable of both generating explicit graph scripts for learnt tasks and\npredicting future steps given a partial step sequence. Our best model\noutperforms the strongest pure text/vision baselines by 17.52% absolute gains\non F1@3 for next step prediction and 13.8% absolute gains on Acc@1 for partial\nsequence completion. Human evaluation shows our model outperforming the WikiHow\nlinear baseline by 48.76% absolute gains in capturing sequential and\nnon-sequential step relationships.", "published": "2023-05-27 18:13:17", "link": "http://arxiv.org/abs/2305.17542v1", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "What can Large Language Models do in chemistry? A comprehensive\n  benchmark on eight tasks", "abstract": "Large Language Models (LLMs) with strong abilities in natural language\nprocessing tasks have emerged and have been applied in various kinds of areas\nsuch as science, finance and software engineering. However, the capability of\nLLMs to advance the field of chemistry remains unclear. In this paper, rather\nthan pursuing state-of-the-art performance, we aim to evaluate capabilities of\nLLMs in a wide range of tasks across the chemistry domain. We identify three\nkey chemistry-related capabilities including understanding, reasoning and\nexplaining to explore in LLMs and establish a benchmark containing eight\nchemistry tasks. Our analysis draws on widely recognized datasets facilitating\na broad exploration of the capacities of LLMs within the context of practical\nchemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are\nevaluated for each chemistry task in zero-shot and few-shot in-context learning\nsettings with carefully selected demonstration examples and specially crafted\nprompts. Our investigation found that GPT-4 outperformed other models and LLMs\nexhibit different competitive levels in eight chemistry tasks. In addition to\nthe key findings from the comprehensive benchmark analysis, our work provides\ninsights into the limitation of current LLMs and the impact of in-context\nlearning settings on LLMs' performance across various chemistry tasks. The code\nand datasets used in this study are available at\nhttps://github.com/ChemFoundationModels/ChemLLMBench.", "published": "2023-05-27 14:17:33", "link": "http://arxiv.org/abs/2305.18365v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Positive Scaling: How Negation Impacts Scaling Trends of Language\n  Models", "abstract": "Language models have been shown to exhibit positive scaling, where\nperformance improves as models are scaled up in terms of size, compute, or\ndata. In this work, we introduce NeQA, a dataset consisting of questions with\nnegation in which language models do not exhibit straightforward positive\nscaling. We show that this task can exhibit inverse scaling, U-shaped scaling,\nor positive scaling, and the three scaling trends shift in this order as we use\nmore powerful prompting methods or model families. We hypothesize that solving\nNeQA depends on two subtasks: question answering (task 1) and negation\nunderstanding (task 2). We find that task 1 has linear scaling, while task 2\nhas sigmoid-shaped scaling with an emergent transition point, and composing\nthese two scaling trends yields the final scaling trend of NeQA. Our work\nreveals and provides a way to analyze the complex scaling trends of language\nmodels.", "published": "2023-05-27 00:07:17", "link": "http://arxiv.org/abs/2305.17311v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Investigation of Evaluation Metrics for Automated Medical Note\n  Generation", "abstract": "Recent studies on automatic note generation have shown that doctors can save\nsignificant amounts of time when using automatic clinical note generation\n(Knoll et al., 2022). Summarization models have been used for this task to\ngenerate clinical notes as summaries of doctor-patient conversations (Krishna\net al., 2021; Cai et al., 2022). However, assessing which model would best\nserve clinicians in their daily practice is still a challenging task due to the\nlarge set of possible correct summaries, and the potential limitations of\nautomatic evaluation metrics. In this paper, we study evaluation methods and\nmetrics for the automatic generation of clinical notes from medical\nconversations. In particular, we propose new task-specific metrics and we\ncompare them to SOTA evaluation metrics in text summarization and generation,\nincluding: (i) knowledge-graph embedding-based metrics, (ii) customized\nmodel-based metrics, (iii) domain-adapted/fine-tuned metrics, and (iv) ensemble\nmetrics. To study the correlation between the automatic metrics and manual\njudgments, we evaluate automatic notes/summaries by comparing the system and\nreference facts and computing the factual correctness, and the hallucination\nand omission rates for critical medical facts. This study relied on seven\ndatasets manually annotated by domain experts. Our experiments show that\nautomatic evaluation metrics can have substantially different behaviors on\ndifferent types of clinical notes datasets. However, the results highlight one\nstable subset of metrics as the most correlated with human judgments with a\nrelevant aggregation of different evaluation criteria.", "published": "2023-05-27 04:34:58", "link": "http://arxiv.org/abs/2305.17364v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Framework For Refining Text Classification and Object Recognition from\n  Academic Articles", "abstract": "With the widespread use of the internet, it has become increasingly crucial\nto extract specific information from vast amounts of academic articles\nefficiently. Data mining techniques are generally employed to solve this issue.\nHowever, data mining for academic articles is challenging since it requires\nautomatically extracting specific patterns in complex and unstructured layout\ndocuments. Current data mining methods for academic articles employ\nrule-based(RB) or machine learning(ML) approaches. However, using rule-based\nmethods incurs a high coding cost for complex typesetting articles. On the\nother hand, simply using machine learning methods requires annotation work for\ncomplex content types within the paper, which can be costly. Furthermore, only\nusing machine learning can lead to cases where patterns easily recognized by\nrule-based methods are mistakenly extracted. To overcome these issues, from the\nperspective of analyzing the standard layout and typesetting used in the\nspecified publication, we emphasize implementing specific methods for specific\ncharacteristics in academic articles. We have developed a novel Text Block\nRefinement Framework (TBRF), a machine learning and rule-based scheme hybrid.\nWe used the well-known ACL proceeding articles as experimental data for the\nvalidation experiment. The experiment shows that our approach achieved over 95%\nclassification accuracy and 90% detection accuracy for tables and figures.", "published": "2023-05-27 07:59:49", "link": "http://arxiv.org/abs/2305.17401v4", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Query-Efficient Black-Box Red Teaming via Bayesian Optimization", "abstract": "The deployment of large-scale generative models is often restricted by their\npotential risk of causing harm to users in unpredictable ways. We focus on the\nproblem of black-box red teaming, where a red team generates test cases and\ninteracts with the victim model to discover a diverse set of failures with\nlimited query access. Existing red teaming methods construct test cases based\non human supervision or language model (LM) and query all test cases in a\nbrute-force manner without incorporating any information from past evaluations,\nresulting in a prohibitively large number of queries. To this end, we propose\nBayesian red teaming (BRT), novel query-efficient black-box red teaming methods\nbased on Bayesian optimization, which iteratively identify diverse positive\ntest cases leading to model failures by utilizing the pre-defined user input\npool and the past evaluations. Experimental results on various user input pools\ndemonstrate that our method consistently finds a significantly larger number of\ndiverse positive test cases under the limited query budget than the baseline\nmethods. The source code is available at\nhttps://github.com/snu-mllab/Bayesian-Red-Teaming.", "published": "2023-05-27 11:00:15", "link": "http://arxiv.org/abs/2305.17444v1", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific\n  Subspaces of Pre-trained Language Models", "abstract": "Pre-trained language models (PLMs) are known to be overly parameterized and\nhave significant redundancy, indicating a small degree of freedom of the PLMs.\nMotivated by the observation, in this paper, we study the problem of\nre-parameterizing and fine-tuning PLMs from a new perspective: Discovery of\nintrinsic task-specific subspace. Specifically, by exploiting the dynamics of\nthe fine-tuning process for a given task, the parameter optimization trajectory\nis learned to uncover its intrinsic task-specific subspace. A key finding is\nthat PLMs can be effectively fine-tuned in the subspace with a small number of\nfree parameters. Beyond, we observe some outlier dimensions emerging during\nfine-tuning in the subspace. Disabling these dimensions degrades the model\nperformance significantly. This suggests that these dimensions are crucial to\ninduce task-specific knowledge to downstream tasks.", "published": "2023-05-27 11:16:26", "link": "http://arxiv.org/abs/2305.17446v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Financial misstatement detection: a realistic evaluation", "abstract": "In this work, we examine the evaluation process for the task of detecting\nfinancial reports with a high risk of containing a misstatement. This task is\noften referred to, in the literature, as ``misstatement detection in financial\nreports''. We provide an extensive review of the related literature. We propose\na new, realistic evaluation framework for the task which, unlike a large part\nof the previous work: (a) focuses on the misstatement class and its rarity, (b)\nconsiders the dimension of time when splitting data into training and test and\n(c) considers the fact that misstatements can take a long time to detect. Most\nimportantly, we show that the evaluation process significantly affects system\nperformance, and we analyze the performance of different models and feature\ntypes in the new realistic framework.", "published": "2023-05-27 12:19:13", "link": "http://arxiv.org/abs/2305.17457v1", "categories": ["cs.CL", "cs.LG", "q-fin.CP"], "primary_category": "cs.CL"}
{"title": "CIF-PT: Bridging Speech and Text Representations for Spoken Language\n  Understanding via Continuous Integrate-and-Fire Pre-Training", "abstract": "Speech or text representation generated by pre-trained models contains\nmodal-specific information that could be combined for benefiting spoken\nlanguage understanding (SLU) tasks. In this work, we propose a novel\npre-training paradigm termed Continuous Integrate-and-Fire Pre-Training\n(CIF-PT). It relies on a simple but effective frame-to-token alignment:\ncontinuous integrate-and-fire (CIF) to bridge the representations between\nspeech and text. It jointly performs speech-to-text training and language model\ndistillation through CIF as the pre-training (PT). Evaluated on SLU benchmark\nSLURP dataset, CIF-PT outperforms the state-of-the-art model by 1.94% of\naccuracy and 2.71% of SLU-F1 on the tasks of intent classification and slot\nfilling, respectively. We also observe the cross-modal representation extracted\nby CIF-PT obtains better performance than other neural interfaces for the tasks\nof SLU, including the dominant speech representation learned from\nself-supervised pre-training.", "published": "2023-05-27 15:39:13", "link": "http://arxiv.org/abs/2305.17499v1", "categories": ["cs.CL", "cs.MM", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Backdooring Neural Code Search", "abstract": "Reusing off-the-shelf code snippets from online repositories is a common\npractice, which significantly enhances the productivity of software developers.\nTo find desired code snippets, developers resort to code search engines through\nnatural language queries. Neural code search models are hence behind many such\nengines. These models are based on deep learning and gain substantial attention\ndue to their impressive performance. However, the security aspect of these\nmodels is rarely studied. Particularly, an adversary can inject a backdoor in\nneural code search models, which return buggy or even vulnerable code with\nsecurity/privacy issues. This may impact the downstream software (e.g., stock\ntrading systems and autonomous driving) and cause financial loss and/or\nlife-threatening incidents. In this paper, we demonstrate such attacks are\nfeasible and can be quite stealthy. By simply modifying one variable/function\nname, the attacker can make buggy/vulnerable code rank in the top 11%. Our\nattack BADCODE features a special trigger generation and injection procedure,\nmaking the attack more effective and stealthy. The evaluation is conducted on\ntwo neural code search models and the results show our attack outperforms\nbaselines by 60%. Our user study demonstrates that our attack is more stealthy\nthan the baseline by two times based on the F1 score.", "published": "2023-05-27 16:00:50", "link": "http://arxiv.org/abs/2305.17506v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "68T01", "I.2.2; D.2.13"], "primary_category": "cs.SE"}
{"title": "PuMer: Pruning and Merging Tokens for Efficient Vision Language Models", "abstract": "Large-scale vision language (VL) models use Transformers to perform\ncross-modal interactions between the input text and image. These cross-modal\ninteractions are computationally expensive and memory-intensive due to the\nquadratic complexity of processing the input image and text. We present PuMer:\na token reduction framework that uses text-informed Pruning and modality-aware\nMerging strategies to progressively reduce the tokens of input image and text,\nimproving model inference speed and reducing memory footprint. PuMer learns to\nkeep salient image tokens related to the input text and merges similar textual\nand visual tokens by adding lightweight token reducer modules at several\ncross-modal layers in the VL model. Training PuMer is mostly the same as\nfinetuning the original VL model but faster. Our evaluation for two vision\nlanguage models on four downstream VL tasks shows PuMer increases inference\nthroughput by up to 2x and reduces memory footprint by over 50% while incurring\nless than a 1% accuracy drop.", "published": "2023-05-27 17:16:27", "link": "http://arxiv.org/abs/2305.17530v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Unsupervised Selective Rationalization with Noise Injection", "abstract": "A major issue with using deep learning models in sensitive applications is\nthat they provide no explanation for their output. To address this problem,\nunsupervised selective rationalization produces rationales alongside\npredictions by chaining two jointly-trained components, a rationale generator\nand a predictor. Although this architecture guarantees that the prediction\nrelies solely on the rationale, it does not ensure that the rationale contains\na plausible explanation for the prediction. We introduce a novel training\ntechnique that effectively limits generation of implausible rationales by\ninjecting noise between the generator and the predictor. Furthermore, we\npropose a new benchmark for evaluating unsupervised selective rationalization\nmodels using movie reviews from existing datasets. We achieve sizeable\nimprovements in rationale plausibility and task accuracy over the\nstate-of-the-art across a variety of tasks, including our new benchmark, while\nmaintaining or improving model faithfulness.", "published": "2023-05-27 17:34:36", "link": "http://arxiv.org/abs/2305.17534v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Translatotron 3: Speech to Speech Translation with Monolingual Data", "abstract": "This paper presents Translatotron 3, a novel approach to unsupervised direct\nspeech-to-speech translation from monolingual speech-text datasets by combining\nmasked autoencoder, unsupervised embedding mapping, and back-translation.\nExperimental results in speech-to-speech translation tasks between Spanish and\nEnglish show that Translatotron 3 outperforms a baseline cascade system,\nreporting $18.14$ BLEU points improvement on the synthesized\nUnpaired-Conversational dataset. In contrast to supervised approaches that\nnecessitate real paired data, or specialized modeling to replicate\npara-/non-linguistic information such as pauses, speaking rates, and speaker\nidentity, Translatotron 3 showcases its capability to retain it. Audio samples\ncan be found at http://google-research.github.io/lingvo-lab/translatotron3", "published": "2023-05-27 18:30:54", "link": "http://arxiv.org/abs/2305.17547v3", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Detecting Edit Failures In Large Language Models: An Improved\n  Specificity Benchmark", "abstract": "Recent model editing techniques promise to mitigate the problem of memorizing\nfalse or outdated associations during LLM training. However, we show that these\ntechniques can introduce large unwanted side effects which are not detected by\nexisting specificity benchmarks. We extend the existing CounterFact benchmark\nto include a dynamic component and dub our benchmark CounterFact+.\nAdditionally, we extend the metrics used for measuring specificity by a\nprincipled KL divergence-based metric. We use this improved benchmark to\nevaluate recent model editing techniques and find that they suffer from low\nspecificity. Our findings highlight the need for improved specificity\nbenchmarks that identify and prevent unwanted side effects.", "published": "2023-05-27 19:08:04", "link": "http://arxiv.org/abs/2305.17553v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Diagnosing Transformers: Illuminating Feature Spaces for Clinical\n  Decision-Making", "abstract": "Pre-trained transformers are often fine-tuned to aid clinical decision-making\nusing limited clinical notes. Model interpretability is crucial, especially in\nhigh-stakes domains like medicine, to establish trust and ensure safety, which\nrequires human engagement. We introduce SUFO, a systematic framework that\nenhances interpretability of fine-tuned transformer feature spaces. SUFO\nutilizes a range of analytic and visualization techniques, including Supervised\nprobing, Unsupervised similarity analysis, Feature dynamics, and Outlier\nanalysis to address key questions about model trust and interpretability. We\nconduct a case study investigating the impact of pre-training data where we\nfocus on real-world pathology classification tasks, and validate our findings\non MedNLI. We evaluate five 110M-sized pre-trained transformer models,\ncategorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical\nBioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal\nthat: (1) while PubMedBERT, the domain-specific model, contains valuable\ninformation for fine-tuning, it can overfit to minority classes when class\nimbalances exist. In contrast, mixed-domain models exhibit greater resistance\nto overfitting, suggesting potential improvements in domain-specific model\nrobustness; (2) in-domain pre-training accelerates feature disambiguation\nduring fine-tuning; and (3) feature spaces undergo significant sparsification\nduring this process, enabling clinicians to identify common outlier modes among\nfine-tuned models as demonstrated in this paper. These findings showcase the\nutility of SUFO in enhancing trust and safety when using transformers in\nmedicine, and we believe SUFO can aid practitioners in evaluating fine-tuned\nlanguage models for other applications in medicine and in more critical\ndomains.", "published": "2023-05-27 22:15:48", "link": "http://arxiv.org/abs/2305.17588v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex\n  Interactive Tasks", "abstract": "We introduce SwiftSage, a novel agent framework inspired by the dual-process\ntheory of human cognition, designed to excel in action planning for complex\ninteractive reasoning tasks. SwiftSage integrates the strengths of behavior\ncloning and prompting large language models (LLMs) to enhance task completion\nperformance. The framework comprises two primary modules: the Swift module,\nrepresenting fast and intuitive thinking, and the Sage module, emulating\ndeliberate thought processes. The Swift module is a small encoder-decoder LM\nfine-tuned on the oracle agent's action trajectories, while the Sage module\nemploys LLMs such as GPT-4 for subgoal planning and grounding. We develop a\nheuristic method to harmoniously integrate the two modules, resulting in a more\nefficient and robust problem-solving process. In 30 tasks from the ScienceWorld\nbenchmark, SwiftSage significantly outperforms other methods such as SayCan,\nReAct, and Reflexion, demonstrating its effectiveness in solving complex\ninteractive tasks.", "published": "2023-05-27 07:04:15", "link": "http://arxiv.org/abs/2305.17390v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA", "cs.RO"], "primary_category": "cs.CL"}
{"title": "The Curse of Recursion: Training on Generated Data Makes Models Forget", "abstract": "Stable Diffusion revolutionised image creation from descriptive text. GPT-2,\nGPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of\nlanguage tasks. ChatGPT introduced such language models to the general public.\nIt is now clear that large language models (LLMs) are here to stay, and will\nbring about drastic change in the whole ecosystem of online text and images. In\nthis paper we consider what the future might hold. What will happen to GPT-{n}\nonce LLMs contribute much of the language found online? We find that use of\nmodel-generated content in training causes irreversible defects in the\nresulting models, where tails of the original content distribution disappear.\nWe refer to this effect as Model Collapse and show that it can occur in\nVariational Autoencoders, Gaussian Mixture Models and LLMs. We build\ntheoretical intuition behind the phenomenon and portray its ubiquity amongst\nall learned generative models. We demonstrate that it has to be taken seriously\nif we are to sustain the benefits of training from large-scale data scraped\nfrom the web. Indeed, the value of data collected about genuine human\ninteractions with systems will be increasingly valuable in the presence of\ncontent generated by LLMs in data crawled from the Internet.", "published": "2023-05-27 15:10:41", "link": "http://arxiv.org/abs/2305.17493v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Creating Personalized Synthetic Voices from Post-Glossectomy Speech with\n  Guided Diffusion Models", "abstract": "This paper is about developing personalized speech synthesis systems with\nrecordings of mildly impaired speech. In particular, we consider consonant and\nvowel alterations resulted from partial glossectomy, the surgical removal of\npart of the tongue. The aim is to restore articulation in the synthesized\nspeech and maximally preserve the target speaker's individuality. We propose to\ntackle the problem with guided diffusion models. Specifically, a\ndiffusion-based speech synthesis model is trained on original recordings, to\ncapture and preserve the target speaker's original articulation style. When\nusing the model for inference, a separately trained phone classifier will guide\nthe synthesis process towards proper articulation. Objective and subjective\nevaluation results show that the proposed method substantially improves\narticulation in the synthesized speech over original recordings, and preserves\nmore of the target speaker's individuality than a voice conversion baseline.", "published": "2023-05-27 10:17:42", "link": "http://arxiv.org/abs/2305.17436v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "One-Step Knowledge Distillation and Fine-Tuning in Using Large\n  Pre-Trained Self-Supervised Learning Models for Speaker Verification", "abstract": "The application of speech self-supervised learning (SSL) models has achieved\nremarkable performance in speaker verification (SV). However, there is a\ncomputational cost hurdle in employing them, which makes development and\ndeployment difficult. Several studies have simply compressed SSL models through\nknowledge distillation (KD) without considering the target task. Consequently,\nthese methods could not extract SV-tailored features. This paper suggests\nOne-Step Knowledge Distillation and Fine-Tuning (OS-KDFT), which incorporates\nKD and fine-tuning (FT). We optimize a student model for SV during KD training\nto avert the distillation of inappropriate information for the SV. OS-KDFT\ncould downsize Wav2Vec 2.0 based ECAPA-TDNN size by approximately 76.2%, and\nreduce the SSL model's inference time by 79% while presenting an EER of 0.98%.\nThe proposed OS-KDFT is validated across VoxCeleb1 and VoxCeleb2 datasets and\nW2V2 and HuBERT SSL models. Experiments are available on our GitHub.", "published": "2023-05-27 07:20:54", "link": "http://arxiv.org/abs/2305.17394v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Modality-Independent Teachers Meet Weakly-Supervised Audio-Visual Event\n  Parser", "abstract": "Audio-visual learning has been a major pillar of multi-modal machine\nlearning, where the community mostly focused on its modality-aligned setting,\ni.e., the audio and visual modality are both assumed to signal the prediction\ntarget. With the Look, Listen, and Parse dataset (LLP), we investigate the\nunder-explored unaligned setting, where the goal is to recognize audio and\nvisual events in a video with only weak labels observed. Such weak video-level\nlabels only tell what events happen without knowing the modality they are\nperceived (audio, visual, or both). To enhance learning in this challenging\nsetting, we incorporate large-scale contrastively pre-trained models as the\nmodality teachers. A simple, effective, and generic method, termed Visual-Audio\nLabel Elaboration (VALOR), is innovated to harvest modality labels for the\ntraining events. Empirical studies show that the harvested labels significantly\nimprove an attentional baseline by 8.0 in average F-score (Type@AV).\nSurprisingly, we found that modality-independent teachers outperform their\nmodality-fused counterparts since they are noise-proof from the other\npotentially unaligned modality. Moreover, our best model achieves the new\nstate-of-the-art on all metrics of LLP by a substantial margin (+5.4 F-score\nfor Type@AV). VALOR is further generalized to Audio-Visual Event Localization\nand achieves the new state-of-the-art as well. Code is available at:\nhttps://github.com/Franklin905/VALOR.", "published": "2023-05-27 02:57:39", "link": "http://arxiv.org/abs/2305.17343v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
