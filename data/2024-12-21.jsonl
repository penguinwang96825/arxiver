{"title": "Direct Inversion for the Squared Bessel Process and Applications", "abstract": "In this paper we derive a new direct inversion method to simulate squared\nBessel processes. Since the transition probability of these processes can be\nrepresented by a non-central chi-square distribution, we construct an efficient\nand accurate algorithm to simulate non-central chi-square variables. In this\nmethod, the dimension of the squared Bessel process, equivalently the degrees\nof freedom of the chi-square distribution, is treated as a variable. We\ntherefore use a two-dimensional Chebyshev expansion to approximate the inverse\nfunction of the central chi-square distribution with one variable being the\ndegrees of freedom. The method is accurate and efficient for any value of\ndegrees of freedom including the computationally challenging case of small\nvalues. One advantage of the method is that noncentral chi-square samples can\nbe generated for a whole range of values of degrees of freedom using the same\nChebyshev coefficients. The squared Bessel process is a building block for the\nwell-known Cox-Ingersoll-Ross (CIR) processes, which can be generated from\nsquared Bessel processes through time change and linear transformation. Our\ndirect inversion method thus allows the efficient and accurate simulation of\nthese processes, which are used as models in a wide variety of applications.", "published": "2024-12-21 14:55:34", "link": "http://arxiv.org/abs/2412.16655v1", "categories": ["stat.CO", "math.PR", "q-fin.CP", "stat.ME"], "primary_category": "stat.CO"}
{"title": "Path-dependent Fractional Volterra Equations and the Microstructure of Rough Volatility Models driven by Poisson Random Measures", "abstract": "We consider a microstructure foundation for rough volatility models driven by\nPoisson random measures. In our model the volatility is driven by self-exciting\narrivals of market orders as well as self-exciting arrivals of limit orders and\ncancellations. The impact of market order on future order arrivals is captured\nby a Hawkes kernel with power law decay, and is hence persistent. The impact of\nlimit orders on future order arrivals is temporary, yet possibly long-lived.\nAfter suitable scaling the volatility process converges to a fractional Heston\nmodel driven by an additional Poisson random measure. The random measure\ngenerates occasional spikes and clusters of spikes in the volatility process.\nOur results are based on novel existence and uniqueness of solutions results\nfor stochastic path-dependent Volterra equations driven by Poisson random\nmeasures.", "published": "2024-12-21 01:53:22", "link": "http://arxiv.org/abs/2412.16436v1", "categories": ["math.PR", "q-fin.MF", "Primary 60G55, 60F05, secondary 60G22"], "primary_category": "math.PR"}
{"title": "Application of Multimodal Large Language Models in Autonomous Driving", "abstract": "In this era of technological advancements, several cutting-edge techniques\nare being implemented to enhance Autonomous Driving (AD) systems, focusing on\nimproving safety, efficiency, and adaptability in complex driving environments.\nHowever, AD still faces some problems including performance limitations. To\naddress this problem, we conducted an in-depth study on implementing the\nMulti-modal Large Language Model. We constructed a Virtual Question Answering\n(VQA) dataset to fine-tune the model and address problems with the poor\nperformance of MLLM on AD. We then break down the AD decision-making process by\nscene understanding, prediction, and decision-making. Chain of Thought has been\nused to make the decision more perfectly. Our experiments and detailed analysis\nof Autonomous Driving give an idea of how important MLLM is for AD.", "published": "2024-12-21 00:09:52", "link": "http://arxiv.org/abs/2412.16410v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InfoTech Assistant : A Multimodal Conversational Agent for\n  InfoTechnology Web Portal Queries", "abstract": "This pilot study presents the development of the InfoTech Assistant, a\ndomain-specific, multimodal chatbot engineered to address queries in bridge\nevaluation and infrastructure technology. By integrating web data scraping,\nlarge language models (LLMs), and Retrieval-Augmented Generation (RAG), the\nInfoTech Assistant provides accurate and contextually relevant responses. Data,\nincluding textual descriptions and images, are sourced from publicly available\ndocuments on the InfoTechnology website and organized in JSON format to\nfacilitate efficient querying. The architecture of the system includes an\nHTML-based interface and a Flask back end connected to the Llama 3.1 model via\nLLM Studio. Evaluation results show approximately 95 percent accuracy on\ndomain-specific tasks, with high similarity scores confirming the quality of\nresponse matching. This RAG-enhanced setup enables the InfoTech Assistant to\nhandle complex, multimodal queries, offering both textual and visual\ninformation in its responses. The InfoTech Assistant demonstrates strong\npotential as a dependable tool for infrastructure professionals, delivering\nhigh accuracy and relevance in its domain-specific outputs.", "published": "2024-12-21 00:34:52", "link": "http://arxiv.org/abs/2412.16412v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chained Tuning Leads to Biased Forgetting", "abstract": "Large language models (LLMs) are often fine-tuned for use on downstream\ntasks, though this can degrade capabilities learned during previous training.\nThis phenomenon, often referred to as catastrophic forgetting, has important\npotential implications for the safety of deployed models. In this work, we\nfirst show that models trained on downstream tasks forget their safety tuning\nto a greater extent than models trained in the opposite order. Second, we show\nthat forgetting disproportionately impacts safety information about certain\ngroups. To quantify this phenomenon, we define a new metric we term biased\nforgetting. We conduct a systematic evaluation of the effects of task ordering\non forgetting and apply mitigations that can help the model recover from the\nforgetting observed. We hope our findings can better inform methods for\nchaining the finetuning of LLMs in continual learning settings to enable\ntraining of safer and less toxic models.", "published": "2024-12-21 03:51:58", "link": "http://arxiv.org/abs/2412.16469v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention Entropy is a Key Factor: An Analysis of Parallel Context\n  Encoding with Full-attention-based Pre-trained Language Models", "abstract": "Large language models have shown remarkable performance across a wide range\nof language tasks, owing to their exceptional capabilities in context modeling.\nThe most commonly used method of context modeling is full self-attention, as\nseen in standard decoder-only Transformers. Although powerful, this method can\nbe inefficient for long sequences and may overlook inherent input structures.\nTo address these problems, an alternative approach is parallel context\nencoding, which splits the context into sub-pieces and encodes them parallelly.\nBecause parallel patterns are not encountered during training, naively applying\nparallel encoding leads to performance degradation. However, the underlying\nreasons and potential mitigations are unclear. In this work, we provide a\ndetailed analysis of this issue and identify that unusually high attention\nentropy can be a key factor. Furthermore, we adopt two straightforward methods\nto reduce attention entropy by incorporating attention sinks and selective\nmechanisms. Experiments on various tasks reveal that these methods effectively\nlower irregular attention entropy and narrow performance gaps. We hope this\nstudy can illuminate ways to enhance context modeling mechanisms.", "published": "2024-12-21 09:04:51", "link": "http://arxiv.org/abs/2412.16545v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language\n  Models", "abstract": "Large language models (LLMs) are widely applied in various fields of society\ndue to their powerful reasoning, understanding, and generation capabilities.\nHowever, the security issues associated with these models are becoming\nincreasingly severe. Jailbreaking attacks, as an important method for detecting\nvulnerabilities in LLMs, have been explored by researchers who attempt to\ninduce these models to generate harmful content through various attack methods.\nNevertheless, existing jailbreaking methods face numerous limitations, such as\nexcessive query counts, limited coverage of jailbreak modalities, low attack\nsuccess rates, and simplistic evaluation methods. To overcome these\nconstraints, this paper proposes a multimodal jailbreaking method: JMLLM. This\nmethod integrates multiple strategies to perform comprehensive jailbreak\nattacks across text, visual, and auditory modalities. Additionally, we\ncontribute a new and comprehensive dataset for multimodal jailbreaking\nresearch: TriJail, which includes jailbreak prompts for all three modalities.\nExperiments on the TriJail dataset and the benchmark dataset AdvBench,\nconducted on 13 popular LLMs, demonstrate advanced attack success rates and\nsignificant reduction in time overhead.", "published": "2024-12-21 09:43:51", "link": "http://arxiv.org/abs/2412.16555v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Acquisition of Recursive Possessives and Recursive Locatives in Mandarin", "abstract": "As recursion has been underlying any linguistic work for the last 60 years,\nthe acquisition of recursive structures by children during language learning\nhas become a focal point of inquiry. This study delves into the developmental\ntrajectory of Mandarin-speaking children's acquisition of recursive possessives\nand locatives, assessing the impact of structural diversity on language\nacquisition. The research contrasts the comprehension of two-level recursive\nstructures among children aged 3 to 7 years, employing answering question while\nseeing a picture task to elicit responses. The findings indicate that children\ndo not attain adult-like proficiency in two-level recursion until the age of 6,\nand there exists a notable asymmetry in the acquisition of recursive\npossessives versus locatives. These results underscore the primacy of\nstructural complexity and cognitive factors in the acquisition process,\nenhancing our comprehension of the cognitive foundations of language\ndevelopment and the pivotal role of recursion in child language acquisition.", "published": "2024-12-21 09:44:21", "link": "http://arxiv.org/abs/2412.16556v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NILE: Internal Consistency Alignment in Large Language Models", "abstract": "As a crucial step to enhance LLMs alignment with human intentions,\nInstruction Fine-Tuning (IFT) has a high demand on dataset quality. However,\nexisting IFT datasets often contain knowledge that is inconsistent with LLMs'\ninternal knowledge learned from the pre-training phase, which can greatly\naffect the efficacy of IFT. To address this issue, we introduce NILE (iNternal\nconsIstency aLignmEnt) framework, aimed at optimizing IFT datasets to unlock\nLLMs' capability further. NILE operates by eliciting target pre-trained LLM's\ninternal knowledge corresponding to instruction data. The internal knowledge is\nleveraged to revise the answer in IFT datasets. Additionally, we propose a\nnovel Internal Consistency Filtering (ICF) method to filter training samples,\nensuring its high consistency with LLM's internal knowledge. Our experiments\ndemonstrate that NILE-aligned IFT datasets sharply boost LLM performance across\nmultiple LLM ability evaluation datasets, achieving up to 66.6% gain on\nArena-Hard and 68.5% on Alpaca-Eval V2. Further analysis confirms that each\ncomponent of the NILE}framework contributes to these substantial performance\nimprovements, and provides compelling evidence that dataset consistency with\npre-trained internal knowledge is pivotal for maximizing LLM potential.", "published": "2024-12-21 16:25:16", "link": "http://arxiv.org/abs/2412.16686v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SubData: A Python Library to Collect and Combine Datasets for Evaluating\n  LLM Alignment on Downstream Tasks", "abstract": "With the release of ever more capable large language models (LLMs),\nresearchers in NLP and related disciplines have started to explore the\nusability of LLMs for a wide variety of different annotation tasks. Very\nrecently, a lot of this attention has shifted to tasks that are subjective in\nnature. Given that the latest generations of LLMs have digested and encoded\nextensive knowledge about different human subpopulations and individuals, the\nhope is that these models can be trained, tuned or prompted to align with a\nwide range of different human perspectives. While researchers already evaluate\nthe success of this alignment via surveys and tests, there is a lack of\nresources to evaluate the alignment on what oftentimes matters the most in NLP;\nthe actual downstream tasks. To fill this gap we present SubData, a Python\nlibrary that offers researchers working on topics related to subjectivity in\nannotation tasks a convenient way of collecting, combining and using a range of\nsuitable datasets.", "published": "2024-12-21 21:40:31", "link": "http://arxiv.org/abs/2412.16783v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Underutilization of Syntactic Processing by Chinese Learners of English\n  in Comprehending English Sentences, Evidenced from Adapted Garden-Path\n  Ambiguity Experiment", "abstract": "Many studies have revealed that sentence comprehension relies more on\nsemantic processing than on syntactic processing. However, previous studies\nhave predominantly emphasized the preference for semantic processing, focusing\non the semantic perspective. In contrast, this current study highlights the\nunder-utilization of syntactic processing, from a syntactic perspective. Based\non the traditional garden-path experiment, which involves locally ambiguous but\nglobally unambiguous sentences, this study's empirical experiment innovatively\ncrafted an adapted version featuring semantically ambiguous but syntactically\nunambiguous sentences to meet its specific research objective. This experiment,\ninvolving 140 subjects, demonstrates through descriptive and inferential\nstatistical analyses using SPSS, Graph Pad Prism, and Cursor that Chinese\nlearners of English tend to under-utilize syntactic processing when\ncomprehending English sentences. The study identifies two types of parsing\nunder-utilization: partial and complete. Further exploration reveals that trial\nand error in syntactic processing contributes to both. Consequently, this study\nlays a foundation for the development of a novel parsing method designed to\nfully integrate syntactic processing into sentence comprehension, thereby\nenhancing the level of English sentence comprehension for Chinese learners of\nEnglish.", "published": "2024-12-21 01:32:10", "link": "http://arxiv.org/abs/2501.00030v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distilling Large Language Models for Efficient Clinical Information\n  Extraction", "abstract": "Large language models (LLMs) excel at clinical information extraction but\ntheir computational demands limit practical deployment. Knowledge\ndistillation--the process of transferring knowledge from larger to smaller\nmodels--offers a potential solution. We evaluate the performance of distilled\nBERT models, which are approximately 1,000 times smaller than modern LLMs, for\nclinical named entity recognition (NER) tasks. We leveraged state-of-the-art\nLLMs (Gemini and OpenAI models) and medical ontologies (RxNorm and SNOMED) as\nteacher labelers for medication, disease, and symptom extraction. We applied\nour approach to over 3,300 clinical notes spanning five publicly available\ndatasets, comparing distilled BERT models against both their teacher labelers\nand BERT models fine-tuned on human labels. External validation was conducted\nusing clinical notes from the MedAlign dataset. For disease extraction, F1\nscores were 0.82 (teacher model), 0.89 (BioBERT trained on human labels), and\n0.84 (BioBERT-distilled). For medication, F1 scores were 0.84 (teacher model),\n0.91 (BioBERT-human), and 0.87 (BioBERT-distilled). For symptoms: F1 score of\n0.73 (teacher model) and 0.68 (BioBERT-distilled). Distilled BERT models had\nfaster inference (12x, 4x, 8x faster than GPT-4o, o1-mini, and Gemini Flash\nrespectively) and lower costs (85x, 101x, 2x cheaper than GPT-4o, o1-mini, and\nGemini Flash respectively). On the external validation dataset, the distilled\nBERT model achieved F1 scores of 0.883 (medication), 0.726 (disease), and 0.699\n(symptom). Distilled BERT models were up to 101x cheaper and 12x faster than\nstate-of-the-art LLMs while achieving similar performance on NER tasks.\nDistillation offers a computationally efficient and scalable alternative to\nlarge LLMs for clinical information extraction.", "published": "2024-12-21 02:15:29", "link": "http://arxiv.org/abs/2501.00031v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Beyond End-to-End VLMs: Leveraging Intermediate Text Representations for\n  Superior Flowchart Understanding", "abstract": "Flowcharts are typically presented as images, driving the trend of using\nvision-language models (VLMs) for end-to-end flowchart understanding. However,\ntwo key challenges arise: (i) Limited controllability--users have minimal\ninfluence over the downstream task, as they can only modify input images, while\nthe training of VLMs is often out of reach for most researchers. (ii) Lack of\nexplainability--it is difficult to trace VLM errors to specific causes, such as\nfailures in visual encoding or reasoning. We propose TextFlow, addressing\naforementioned issues with two stages: (i) Vision Textualizer--which generates\ntextual representations from flowchart images; and (ii) Textual Reasoner--which\nperforms question-answering based on the text representations. TextFlow offers\nthree key advantages: (i) users can select the type of text representations\n(e.g., Graphviz, Mermaid, PlantUML), or further convert them into executable\ngraph object to call tools, enhancing performance and controllability; (ii) it\nimproves explainability by helping to attribute errors more clearly to visual\nor textual processing components; and (iii) it promotes the modularization of\nthe solution, such as allowing advanced LLMs to be used in the Reasoner stage\nwhen VLMs underperform in end-to-end fashion. Experiments on the FlowVQA and\nFlowLearn benchmarks demonstrate TextFlow's state-of-the-art performance as\nwell as its robustness. All code is publicly available.", "published": "2024-12-21 00:52:41", "link": "http://arxiv.org/abs/2412.16420v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Effective Context Modeling Framework for Emotion Recognition in\n  Conversations", "abstract": "Emotion Recognition in Conversations (ERC) facilitates a deeper understanding\nof the emotions conveyed by speakers in each utterance within a conversation.\nRecently, Graph Neural Networks (GNNs) have demonstrated their strengths in\ncapturing data relationships, particularly in contextual information modeling\nand multimodal fusion. However, existing methods often struggle to fully\ncapture the complex interactions between multiple modalities and conversational\ncontext, limiting their expressiveness. To overcome these limitations, we\npropose ConxGNN, a novel GNN-based framework designed to capture contextual\ninformation in conversations. ConxGNN features two key parallel modules: a\nmulti-scale heterogeneous graph that captures the diverse effects of utterances\non emotional changes, and a hypergraph that models the multivariate\nrelationships among modalities and utterances. The outputs from these modules\nare integrated into a fusion layer, where a cross-modal attention mechanism is\napplied to produce a contextually enriched representation. Additionally,\nConxGNN tackles the challenge of recognizing minority or semantically similar\nemotion classes by incorporating a re-weighting scheme into the loss functions.\nExperimental results on the IEMOCAP and MELD benchmark datasets demonstrate the\neffectiveness of our method, achieving state-of-the-art performance compared to\nprevious baselines.", "published": "2024-12-21 02:22:06", "link": "http://arxiv.org/abs/2412.16444v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Research on Violent Text Detection System Based on BERT-fasttext Model", "abstract": "In the digital age of today, the internet has become an indispensable\nplatform for people's lives, work, and information exchange. However, the\nproblem of violent text proliferation in the network environment has arisen,\nwhich has brought about many negative effects. In view of this situation, it is\nparticularly important to build an effective system for cutting off violent\ntext. The study of violent text cutting off based on the BERT-fasttext model\nhas significant meaning. BERT is a pre-trained language model with strong\nnatural language understanding ability, which can deeply mine and analyze text\nsemantic information; Fasttext itself is an efficient text classification tool\nwith low complexity and good effect, which can quickly provide basic judgments\nfor text processing. By combining the two and applying them to the system for\ncutting off violent text, on the one hand, it can accurately identify violent\ntext, and on the other hand, it can efficiently and reasonably cut off the\ncontent, preventing harmful information from spreading freely on the network.\nCompared with the single BERT model and fasttext, the accuracy was improved by\n0.7% and 0.8%, respectively. Through this model, it is helpful to purify the\nnetwork environment, maintain the health of network information, and create a\npositive, civilized, and harmonious online communication space for netizens,\ndriving the development of social networking, information dissemination, and\nother aspects in a more benign direction.", "published": "2024-12-21 03:02:18", "link": "http://arxiv.org/abs/2412.16455v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Transducer-Llama: Integrating LLMs into Streamable Transducer-based\n  Speech Recognition", "abstract": "While large language models (LLMs) have been applied to automatic speech\nrecognition (ASR), the task of making the model streamable remains a challenge.\nThis paper proposes a novel model architecture, Transducer-Llama, that\nintegrates LLMs into a Factorized Transducer (FT) model, naturally enabling\nstreaming capabilities. Furthermore, given that the large vocabulary of LLMs\ncan cause data sparsity issue and increased training costs for spoken language\nsystems, this paper introduces an efficient vocabulary adaptation technique to\nalign LLMs with speech system vocabularies. The results show that directly\noptimizing the FT model with a strong pre-trained LLM-based predictor using the\nRNN-T loss yields some but limited improvements over a smaller pre-trained LM\npredictor. Therefore, this paper proposes a weak-to-strong LM swap strategy,\nusing a weak LM predictor during RNN-T loss training and then replacing it with\na strong LLM. After LM replacement, the minimum word error rate (MWER) loss is\nemployed to finetune the integration of the LLM predictor with the\nTransducer-Llama model. Experiments on the LibriSpeech and large-scale\nmulti-lingual LibriSpeech corpora show that the proposed streaming\nTransducer-Llama approach gave a 17% relative WER reduction (WERR) over a\nstrong FT baseline and a 32% WERR over an RNN-T baseline.", "published": "2024-12-21 03:35:49", "link": "http://arxiv.org/abs/2412.16464v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Enhancing Multilingual ASR for Unseen Languages via Language Embedding\n  Modeling", "abstract": "Multilingual Automatic Speech Recognition (ASR) aims to recognize and\ntranscribe speech from multiple languages within a single system. Whisper, one\nof the most advanced ASR models, excels in this domain by handling 99 languages\neffectively, leveraging a vast amount of data and incorporating language tags\nas prefixes to guide the recognition process. However, despite its success,\nWhisper struggles with unseen languages, those not included in its\npre-training. Motivated by the observation that many languages share linguistic\ncharacteristics, we propose methods that exploit these relationships to enhance\nASR performance on unseen languages. Specifically, we introduce a weighted sum\nmethod, which computes a weighted sum of the embeddings of language tags, using\nWhisper's predicted language probabilities. In addition, we develop a\npredictor-based approach that refines the weighted sum embedding to more\nclosely approximate the true embedding for unseen languages. Experimental\nresults demonstrate substantial improvements in ASR performance, both in\nzero-shot and fine-tuning settings. Our proposed methods outperform baseline\napproaches, providing an effective solution for addressing unseen languages in\nmultilingual ASR.", "published": "2024-12-21 04:05:43", "link": "http://arxiv.org/abs/2412.16474v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "HammerBench: Fine-Grained Function-Calling Evaluation in Real Mobile\n  Device Scenarios", "abstract": "Evaluating the performance of LLMs in multi-turn human-agent interactions\npresents significant challenges, particularly due to the complexity and\nvariability of user behavior. In this paper, we introduce HammerBench, a novel\nbenchmark framework for assessing LLMs' function-calling capabilities in\nreal-world, multi-turn dialogues. HammerBench simulates diverse mobile\nassistant use cases, incorporating imperfect instructions, dynamic\nquestion-answer trajectories, intent and argument shifts, and the indirect use\nof external information through pronouns. To construct this benchmark, we\ncurate a comprehensive dataset derived from popular mobile app functionalities\nand anonymized user logs, complemented by a cost-effective data generation\npipeline leveraging open-source models. HammerBench is further augmented with\nfine-grained interaction snapshots and metrics, enabling detailed evaluation of\nfunction-calling performance across individual conversational turns. We\ndemonstrate the effectiveness of HammerBench by evaluating several leading LLMs\nand uncovering key performance trends. Our experiments reveal that different\ntypes of parameter name errors are a significant source of failure across\ndifferent interaction scenarios, highlighting critical areas for further\nimprovement in LLM robustness for mobile assistant applications.", "published": "2024-12-21 07:33:55", "link": "http://arxiv.org/abs/2412.16516v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "DragonVerseQA: Open-Domain Long-Form Context-Aware Question-Answering", "abstract": "This paper proposes a novel approach to develop an open-domain and long-form\nOver-The-Top (OTT) Question-Answering (QA) dataset, DragonVerseQA, specifically\noriented to the fantasy universe of \"House of the Dragon\" and \"Game Of Thrones\"\nTV series. Most existing QA datasets focus on short, fact-based answers sourced\nalmost solely from Wikipedia articles, devoid of depth and contextual richness\nfor sophisticated narrative understanding. We curate a dataset that combines\nfull episode summaries sourced from HBO and fandom wiki websites, user reviews\nfrom sources like IMDb and Rotten Tomatoes, and high-quality, open-domain,\nlegally admissible sources, and structured data from repositories like WikiData\ninto one dataset. The dataset provides a multi-dimensional context, reflecting\ncomplex character dynamics and plot developments from these varied sources.\nThat means, on equal footing, only after heavy data preprocessing and filtering\nmethods will meaningful, non-spam unbiased reviews be available in this\nenriched dataset. The comprehensive insights are given through the long-form\nanswers generated from this enriched context. This is what makes this valuable\ndataset for improving conversational AI, narrative analysis, sentiment\nanalysis, summarization techniques, and relation extraction.\n  A comparative analysis with state-of-the-art QA datasets such as SQuAD 2.0,\nTriviaQA, and Natural Questions brings to light the unique advantages of our\ndataset in terms of contextual complexity and answer length. Detailed reviews\nadd layers to audience sentiment and narrative interpretation, raising the bar\nfor domain-specific QA with a new quality benchmark. Our work also allows a\ndeeper understanding of entertainment-industry content and opens the door to\nmore knowledgeable and creative AI-driven interactions within digital media\nenvironments.", "published": "2024-12-21 16:36:52", "link": "http://arxiv.org/abs/2412.16694v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "AlzheimerRAG: Multimodal Retrieval Augmented Generation for PubMed\n  articles", "abstract": "Recent advancements in generative AI have flourished the development of\nhighly adept Large Language Models (LLMs) that integrate diverse data types to\nempower decision-making. Among these, Multimodal Retrieval-Augmented Generation\n(RAG) applications are promising for their capability to combine the strengths\nof information retrieval and generative models, enhancing their utility across\nvarious domains, including biomedical research. This paper introduces\nAlzheimerRAG, a Multimodal RAG pipeline tool for biomedical research use cases,\nprimarily focusing on Alzheimer's disease from PubMed articles. Our pipeline\nincorporates multimodal fusion techniques to integrate textual and visual data\nprocessing by efficiently indexing and accessing vast amounts of biomedical\nliterature. Preliminary experimental results against benchmarks, such as BioASQ\nand PubMedQA, have returned improved results in information retrieval and\nsynthesis of domain-specific information. We also demonstrate a case study with\nour RAG pipeline across different Alzheimer's clinical scenarios. We infer that\nAlzheimerRAG can generate responses with accuracy non-inferior to humans and\nwith low rates of hallucination. Overall, a reduction in cognitive task load is\nobserved, which allows researchers to gain multimodal insights, improving\nunderstanding and treatment of Alzheimer's disease.", "published": "2024-12-21 16:59:00", "link": "http://arxiv.org/abs/2412.16701v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Quantum-Like Contextuality in Large Language Models", "abstract": "Contextuality is a distinguishing feature of quantum mechanics and there is\ngrowing evidence that it is a necessary condition for quantum advantage. In\norder to make use of it, researchers have been asking whether similar phenomena\narise in other domains. The answer has been yes, e.g. in behavioural sciences.\nHowever, one has to move to frameworks that take some degree of signalling into\naccount. Two such frameworks exist: (1) a signalling-corrected sheaf theoretic\nmodel, and (2) the Contextuality-by-Default (CbD) framework. This paper\nprovides the first large scale experimental evidence for a yes answer in\nnatural language. We construct a linguistic schema modelled over a contextual\nquantum scenario, instantiate it in the Simple English Wikipedia and extract\nprobability distributions for the instances using the large language model\nBERT. This led to the discovery of 77,118 sheaf-contextual and 36,938,948 CbD\ncontextual instances. We proved that the contextual instances came from\nsemantically similar words, by deriving an equation between degrees of\ncontextuality and Euclidean distances of BERT's embedding vectors. A regression\nmodel further reveals that Euclidean distance is indeed the best statistical\npredictor of contextuality. Our linguistic schema is a variant of the\nco-reference resolution challenge. These results are an indication that quantum\nmethods may be advantageous in language tasks.", "published": "2024-12-21 23:46:55", "link": "http://arxiv.org/abs/2412.16806v1", "categories": ["cs.CL", "quant-ph"], "primary_category": "cs.CL"}
{"title": "Evaluating and Enhancing LLMs for Multi-turn Text-to-SQL with Multiple\n  Question Types", "abstract": "Recent advancements in large language models (LLMs) have significantly\nadvanced text-to-SQL systems. However, most LLM-based methods often narrowly\nfocus on SQL generation, neglecting the complexities of real-world\nconversational queries. This oversight can lead to unreliable responses,\nparticularly for ambiguous questions that cannot be directly addressed with\nSQL. To bridge this gap, we propose MMSQL, a comprehensive test suite designed\nto evaluate the question classification and SQL generation capabilities of LLMs\nby simulating real-world scenarios with diverse question types and multi-turn\nQ&A interactions. Using MMSQL, we assessed the performance of popular LLMs,\nincluding both open-source and closed-source models, and identified key factors\nimpacting their performance in such scenarios. Moreover, we introduce an\nLLM-based multi-agent framework that employs specialized agents to identify\nquestion types and determine appropriate answering strategies. Our experiments\ndemonstrate that this approach significantly enhances the model's ability to\nnavigate the complexities of conversational dynamics, effectively handling the\ndiverse and complex nature of user queries. Our dataset and code are publicly\navailable at https://mcxiaoxiao.github.io/MMSQL.", "published": "2024-12-21 10:13:45", "link": "http://arxiv.org/abs/2412.17867v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MERaLiON-TextLLM: Cross-Lingual Understanding of Large Language Models\n  in Chinese, Indonesian, Malay, and Singlish", "abstract": "Multilingual large language models (MLLMs) have shown impressive capabilities\nacross a variety of languages. However, efficacy can differ greatly between\ndifferent language families, especially for those with limited linguistic\nresources. This report presents MERaLiON-TextLLM, a series of open-source\nlanguage models specifically tailored to improve understanding and generation\nin Chinese, Indonesian, Malay, and Singlish. The initial released model is\nbuilt on Llama-3-8B-Base and refined through a meticulously crafted process of\ncontinued pre-training and weight merging. Our approach achieves performance\nimprovements across benchmarks in these languages, exceeding the capabilities\nof the official Llama-3 models. We provide the model checkpoints as a resource\nto support further research and development in cross-lingual language\nunderstanding.", "published": "2024-12-21 05:50:48", "link": "http://arxiv.org/abs/2501.08335v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identifying Cyberbullying Roles in Social Media", "abstract": "Social media has revolutionized communication, allowing people worldwide to\nconnect and interact instantly. However, it has also led to increases in\ncyberbullying, which poses a significant threat to children and adolescents\nglobally, affecting their mental health and well-being. It is critical to\naccurately detect the roles of individuals involved in cyberbullying incidents\nto effectively address the issue on a large scale. This study explores the use\nof machine learning models to detect the roles involved in cyberbullying\ninteractions. After examining the AMiCA dataset and addressing class imbalance\nissues, we evaluate the performance of various models built with four\nunderlying LLMs (i.e., BERT, RoBERTa, T5, and GPT-2) for role detection. Our\nanalysis shows that oversampling techniques help improve model performance. The\nbest model, a fine-tuned RoBERTa using oversampled data, achieved an overall F1\nscore of 83.5%, increasing to 89.3% after applying a prediction threshold. The\ntop-2 F1 score without thresholding was 95.7%. Our method outperforms\npreviously proposed models. After investigating the per-class model performance\nand confidence scores, we show that the models perform well in classes with\nmore samples and less contextual confusion (e.g., Bystander Other), but\nstruggle with classes with fewer samples (e.g., Bystander Assistant) and more\ncontextual ambiguity (e.g., Harasser and Victim). This work highlights current\nstrengths and limitations in the development of accurate models with limited\ndata and complex scenarios.", "published": "2024-12-21 00:46:48", "link": "http://arxiv.org/abs/2412.16417v1", "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Technical Report: Small Language Model for Japanese Clinical and\n  Medicine", "abstract": "This report presents a small language model (SLM) for Japanese clinical and\nmedicine, named NCVC-slm-1. This 1B parameters model was trained using Japanese\ntext classified to be of high-quality. Moreover, NCVC-slm-1 was augmented with\nrespect to clinical and medicine content that includes the variety of diseases,\ndrugs, and examinations. Using a carefully designed pre-processing, a\nspecialized morphological analyzer and tokenizer, this small and light-weight\nmodel performed not only to generate text but also indicated the feasibility of\nunderstanding clinical and medicine text. In comparison to other large language\nmodels, a fine-tuning NCVC-slm-1 demonstrated the highest scores on 6 tasks of\ntotal 8 on JMED-LLM. According to this result, SLM indicated the feasibility of\nperforming several downstream tasks in the field of clinical and medicine.\nHopefully, NCVC-slm-1 will be contributed to develop and accelerate the field\nof clinical and medicine for a bright future.", "published": "2024-12-21 01:12:48", "link": "http://arxiv.org/abs/2412.16423v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Correcting Large Language Model Behavior via Influence Function", "abstract": "Recent advancements in AI alignment techniques have significantly improved\nthe alignment of large language models (LLMs) with static human preferences.\nHowever, the dynamic nature of human preferences can render some prior training\ndata outdated or even erroneous, ultimately causing LLMs to deviate from\ncontemporary human preferences and societal norms. Existing methodologies,\nwhether they involve the curation of new data for continual alignment or the\nmanual correction of outdated data for re-alignment, demand costly human\nresources. To address this challenge, we propose a novel approach, Large\nLanguage Model Behavior Correction with Influence Function Recall and\nPost-Training (LANCET), which requires no human involvement. LANCET consists of\ntwo phases: (1) using influence functions to identify the training data that\nsignificantly impact undesirable model outputs, and (2) applying an Influence\nfunction-driven Bregman Optimization (IBO) technique to adjust the model's\nbehavior based on these influence distributions. Our experiments demonstrate\nthat LANCET effectively and efficiently correct inappropriate behaviors of\nLLMs. Furthermore, LANCET can outperform methods that rely on collecting human\npreferences, and it enhances the interpretability of learning human preferences\nwithin LLMs.", "published": "2024-12-21 02:50:08", "link": "http://arxiv.org/abs/2412.16451v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Automated CVE Analysis: Harnessing Machine Learning In Designing\n  Question-Answering Models For Cybersecurity Information Extraction", "abstract": "The vast majority of cybersecurity information is unstructured text,\nincluding critical data within databases such as CVE, NVD, CWE, CAPEC, and the\nMITRE ATT&CK Framework. These databases are invaluable for analyzing attack\npatterns and understanding attacker behaviors. Creating a knowledge graph by\nintegrating this information could unlock significant insights. However,\nprocessing this large amount of data requires advanced deep-learning\ntechniques. A crucial step towards building such a knowledge graph is\ndeveloping a robust mechanism for automating the extraction of answers to\nspecific questions from the unstructured text. Question Answering (QA) systems\nplay a pivotal role in this process by pinpointing and extracting precise\ninformation, facilitating the mapping of relationships between various data\npoints. In the cybersecurity context, QA systems encounter unique challenges\ndue to the need to interpret and answer questions based on a wide array of\ndomain-specific information. To tackle these challenges, it is necessary to\ndevelop a cybersecurity-specific dataset and train a machine learning model on\nit, aimed at enhancing the understanding and retrieval of domain-specific\ninformation. This paper presents a novel dataset and describes a machine\nlearning model trained on this dataset for the QA task. It also discusses the\nmodel's performance and key findings in a manner that maintains a balance\nbetween formality and accessibility.", "published": "2024-12-21 04:50:45", "link": "http://arxiv.org/abs/2412.16484v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Evaluating the Performance of Large Language Models in Scientific Claim\n  Detection and Classification", "abstract": "The pervasive influence of social media during the COVID-19 pandemic has been\na double-edged sword, enhancing communication while simultaneously propagating\nmisinformation. This \\textit{Digital Infodemic} has highlighted the urgent need\nfor automated tools capable of discerning and disseminating factual content.\nThis study evaluates the efficacy of Large Language Models (LLMs) as innovative\nsolutions for mitigating misinformation on platforms like Twitter. LLMs, such\nas OpenAI's GPT and Meta's LLaMA, offer a pre-trained, adaptable approach that\nbypasses the extensive training and overfitting issues associated with\ntraditional machine learning models. We assess the performance of LLMs in\ndetecting and classifying COVID-19-related scientific claims, thus facilitating\ninformed decision-making. Our findings indicate that LLMs have significant\npotential as automated fact-checking tools, though research in this domain is\nnascent and further exploration is required. We present a comparative analysis\nof LLMs' performance using a specialized dataset and propose a framework for\ntheir application in public health communication.", "published": "2024-12-21 05:02:26", "link": "http://arxiv.org/abs/2412.16486v1", "categories": ["cs.CL", "cs.CY", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Real-time Bangla Sign Language Translator", "abstract": "The human body communicates through various meaningful gestures, with sign\nlanguage using hands being a prominent example. Bangla Sign Language\nTranslation (BSLT) aims to bridge communication gaps for the deaf and mute\ncommunity. Our approach involves using Mediapipe Holistic to gather key points,\nLSTM architecture for data training, and Computer Vision for realtime sign\nlanguage detection with an accuracy of 94%. Keywords=Recurrent Neural Network,\nLSTM, Computer Vision, Bangla font.", "published": "2024-12-21 05:56:32", "link": "http://arxiv.org/abs/2412.16497v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speech Retrieval-Augmented Generation without Automatic Speech\n  Recognition", "abstract": "One common approach for question answering over speech data is to first\ntranscribe speech using automatic speech recognition (ASR) and then employ\ntext-based retrieval-augmented generation (RAG) on the transcriptions. While\nthis cascaded pipeline has proven effective in many practical settings, ASR\nerrors can propagate to the retrieval and generation steps. To overcome this\nlimitation, we introduce SpeechRAG, a novel framework designed for\nopen-question answering over spoken data. Our proposed approach fine-tunes a\npre-trained speech encoder into a speech adapter fed into a frozen large\nlanguage model (LLM)--based retrieval model. By aligning the embedding spaces\nof text and speech, our speech retriever directly retrieves audio passages from\ntext-based queries, leveraging the retrieval capacity of the frozen text\nretriever. Our retrieval experiments on spoken question answering datasets show\nthat direct speech retrieval does not degrade over the text-based baseline, and\noutperforms the cascaded systems using ASR. For generation, we use a speech\nlanguage model (SLM) as a generator, conditioned on audio passages rather than\ntranscripts. Without fine-tuning of the SLM, this approach outperforms cascaded\ntext-based models when there is high WER in the transcripts.", "published": "2024-12-21 06:16:04", "link": "http://arxiv.org/abs/2412.16500v3", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Adapting Whisper for Code-Switching through Encoding Refining and\n  Language-Aware Decoding", "abstract": "Code-switching (CS) automatic speech recognition (ASR) faces challenges due\nto the language confusion resulting from accents, auditory similarity, and\nseamless language switches. Adaptation on the pre-trained multi-lingual model\nhas shown promising performance for CS-ASR. In this paper, we adapt Whisper,\nwhich is a large-scale multilingual pre-trained speech recognition model, to CS\nfrom both encoder and decoder parts. First, we propose an encoder refiner to\nenhance the encoder's capacity of intra-sentence swithching. Second, we propose\nusing two sets of language-aware adapters with different language prompt\nembeddings to achieve language-specific decoding information in each decoder\nlayer. Then, a fusion module is added to fuse the language-aware decoding. The\nexperimental results using the SEAME dataset show that, compared with the\nbaseline model, the proposed approach achieves a relative MER reduction of 4.1%\nand 7.2% on the dev_man and dev_sge test sets, respectively, surpassing\nstate-of-the-art methods. Through experiments, we found that the proposed\nmethod significantly improves the performance on non-native language in CS\nspeech, indicating that our approach enables Whisper to better distinguish\nbetween the two languages.", "published": "2024-12-21 07:06:44", "link": "http://arxiv.org/abs/2412.16507v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Text2midi: Generating Symbolic Music from Captions", "abstract": "This paper introduces text2midi, an end-to-end model to generate MIDI files\nfrom textual descriptions. Leveraging the growing popularity of multimodal\ngenerative approaches, text2midi capitalizes on the extensive availability of\ntextual data and the success of large language models (LLMs). Our end-to-end\nsystem harnesses the power of LLMs to generate symbolic music in the form of\nMIDI files. Specifically, we utilize a pretrained LLM encoder to process\ncaptions, which then condition an autoregressive transformer decoder to produce\nMIDI sequences that accurately reflect the provided descriptions. This\nintuitive and user-friendly method significantly streamlines the music creation\nprocess by allowing users to generate music pieces using text prompts. We\nconduct comprehensive empirical evaluations, incorporating both automated and\nhuman studies, that show our model generates MIDI files of high quality that\nare indeed controllable by text captions that may include music theory terms\nsuch as chords, keys, and tempo. We release the code and music samples on our\ndemo page (https://github.com/AMAAI-Lab/Text2midi) for users to interact with\ntext2midi.", "published": "2024-12-21 08:09:12", "link": "http://arxiv.org/abs/2412.16526v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-guided Knowledgeable Network of Thoughts: Amplifying Reasoning with\n  Large Language Models", "abstract": "We introduce Knowledgeable Network of Thoughts (kNoT): a prompt scheme that\nadvances the capabilities of large language models (LLMs) beyond existing\nparadigms like Chain-of-Thought (CoT), Tree of Thoughts (ToT), and Graph of\nThoughts (GoT). The key innovation of kNoT is the LLM Workflow Template (LWT),\nwhich allows for an executable plan to be specified by LLMs for LLMs. LWT\nallows these plans to be arbitrary networks, where single-step LLM operations\nare nodes, and edges correspond to message passing between these steps.\nFurthermore, LWT supports selection of individual elements through indexing,\nfacilitating kNoT to produce intricate plans where each LLM operation can be\nlimited to elementary operations, greatly enhancing reliability over extended\ntask sequences. We demonstrate that kNoT significantly outperforms the state of\nthe art on six use cases, while reducing the need for extensive prompt\nengineering. For instance, kNoT finds 92% accuracy for sorting 32 numbers over\n12% and 31% for ToT and GoT, while utilizing up to 84.4% and 87.3% less\ntask-specific prompts, respectively.", "published": "2024-12-21 08:19:42", "link": "http://arxiv.org/abs/2412.16533v1", "categories": ["cs.MA", "cs.CL", "cs.LG"], "primary_category": "cs.MA"}
{"title": "Open-Vocabulary Mobile Manipulation Based on Double Relaxed Contrastive\n  Learning with Dense Labeling", "abstract": "Growing labor shortages are increasing the demand for domestic service robots\n(DSRs) to assist in various settings. In this study, we develop a DSR that\ntransports everyday objects to specified pieces of furniture based on\nopen-vocabulary instructions. Our approach focuses on retrieving images of\ntarget objects and receptacles from pre-collected images of indoor\nenvironments. For example, given an instruction \"Please get the right red towel\nhanging on the metal towel rack and put it in the white washing machine on the\nleft,\" the DSR is expected to carry the red towel to the washing machine based\non the retrieved images. This is challenging because the correct images should\nbe retrieved from thousands of collected images, which may include many images\nof similar towels and appliances. To address this, we propose RelaX-Former,\nwhich learns diverse and robust representations from among positive, unlabeled\npositive, and negative samples. We evaluated RelaX-Former on a dataset\ncontaining real-world indoor images and human annotated instructions including\ncomplex referring expressions. The experimental results demonstrate that\nRelaX-Former outperformed existing baseline models across standard image\nretrieval metrics. Moreover, we performed physical experiments using a DSR to\nevaluate the performance of our approach in a zero-shot transfer setting. The\nexperiments involved the DSR to carry objects to specific receptacles based on\nopen-vocabulary instructions, achieving an overall success rate of 75%.", "published": "2024-12-21 10:40:56", "link": "http://arxiv.org/abs/2412.16576v2", "categories": ["cs.RO", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Large Language Model Can Be a Foundation for Hidden Rationale-Based\n  Retrieval", "abstract": "Despite the recent advancement in Retrieval-Augmented Generation (RAG)\nsystems, most retrieval methodologies are often developed for factual\nretrieval, which assumes query and positive documents are semantically similar.\nIn this paper, we instead propose and study a more challenging type of\nretrieval task, called hidden rationale retrieval, in which query and document\nare not similar but can be inferred by reasoning chains, logic relationships,\nor empirical experiences. To address such problems, an instruction-tuned Large\nlanguage model (LLM) with a cross-encoder architecture could be a reasonable\nchoice. To further strengthen pioneering LLM-based retrievers, we design a\nspecial instruction that transforms the retrieval task into a generative task\nby prompting LLM to answer a binary-choice question. The model can be\nfine-tuned with direct preference optimization (DPO). The framework is also\noptimized for computational efficiency with no performance degradation. We name\nthis retrieval framework by RaHoRe and verify its zero-shot and fine-tuned\nperformance superiority on Emotional Support Conversation (ESC), compared with\nprevious retrieval works. Our study suggests the potential to employ LLM as a\nfoundation for a wider scope of retrieval tasks. Our codes, models, and\ndatasets are available on https://github.com/flyfree5/LaHoRe.", "published": "2024-12-21 13:19:15", "link": "http://arxiv.org/abs/2412.16615v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "The Task Shield: Enforcing Task Alignment to Defend Against Indirect\n  Prompt Injection in LLM Agents", "abstract": "Large Language Model (LLM) agents are increasingly being deployed as\nconversational assistants capable of performing complex real-world tasks\nthrough tool integration. This enhanced ability to interact with external\nsystems and process various data sources, while powerful, introduces\nsignificant security vulnerabilities. In particular, indirect prompt injection\nattacks pose a critical threat, where malicious instructions embedded within\nexternal data sources can manipulate agents to deviate from user intentions.\nWhile existing defenses based on rule constraints, source spotlighting, and\nauthentication protocols show promise, they struggle to maintain robust\nsecurity while preserving task functionality. We propose a novel and orthogonal\nperspective that reframes agent security from preventing harmful actions to\nensuring task alignment, requiring every agent action to serve user objectives.\nBased on this insight, we develop Task Shield, a test-time defense mechanism\nthat systematically verifies whether each instruction and tool call contributes\nto user-specified goals. Through experiments on the AgentDojo benchmark, we\ndemonstrate that Task Shield reduces attack success rates (2.07\\%) while\nmaintaining high task utility (69.79\\%) on GPT-4o.", "published": "2024-12-21 16:17:48", "link": "http://arxiv.org/abs/2412.16682v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Improving Lip-synchrony in Direct Audio-Visual Speech-to-Speech\n  Translation", "abstract": "Audio-Visual Speech-to-Speech Translation typically prioritizes improving\ntranslation quality and naturalness. However, an equally critical aspect in\naudio-visual content is lip-synchrony-ensuring that the movements of the lips\nmatch the spoken content-essential for maintaining realism in dubbed videos.\nDespite its importance, the inclusion of lip-synchrony constraints in AVS2S\nmodels has been largely overlooked. This study addresses this gap by\nintegrating a lip-synchrony loss into the training process of AVS2S models. Our\nproposed method significantly enhances lip-synchrony in direct audio-visual\nspeech-to-speech translation, achieving an average LSE-D score of 10.67,\nrepresenting a 9.2% reduction in LSE-D over a strong baseline across four\nlanguage pairs. Additionally, it maintains the naturalness and high quality of\nthe translated speech when overlaid onto the original video, without any\ndegradation in translation quality.", "published": "2024-12-21 08:15:52", "link": "http://arxiv.org/abs/2412.16530v1", "categories": ["cs.SD", "cs.CL", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "L3TC: Leveraging RWKV for Learned Lossless Low-Complexity Text\n  Compression", "abstract": "Learning-based probabilistic models can be combined with an entropy coder for\ndata compression. However, due to the high complexity of learning-based models,\ntheir practical application as text compressors has been largely overlooked. To\naddress this issue, our work focuses on a low-complexity design while\nmaintaining compression performance. We introduce a novel Learned Lossless\nLow-complexity Text Compression method (L3TC). Specifically, we conduct\nextensive experiments demonstrating that RWKV models achieve the fastest\ndecoding speed with a moderate compression ratio, making it the most suitable\nbackbone for our method. Second, we propose an outlier-aware tokenizer that\nuses a limited vocabulary to cover frequent tokens while allowing outliers to\nbypass the prediction and encoding. Third, we propose a novel high-rank\nreparameterization strategy that enhances the learning capability during\ntraining without increasing complexity during inference. Experimental results\nvalidate that our method achieves 48% bit saving compared to gzip compressor.\nBesides, L3TC offers compression performance comparable to other learned\ncompressors, with a 50x reduction in model parameters. More importantly, L3TC\nis the fastest among all learned compressors, providing real-time decoding\nspeeds up to megabytes per second. Our code is available at\nhttps://github.com/alipay/L3TC-leveraging-rwkv-for-learned-lossless-low-complexity-text-compression.git.", "published": "2024-12-21 14:24:32", "link": "http://arxiv.org/abs/2412.16642v2", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.MM", "math.IT"], "primary_category": "cs.CL"}
{"title": "Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement", "abstract": "In recent speech enhancement (SE) research, transformer and its variants have\nemerged as the predominant methodologies. However, the quadratic complexity of\nthe self-attention mechanism imposes certain limitations on practical\ndeployment. Mamba, as a novel state-space model (SSM), has gained widespread\napplication in natural language processing and computer vision due to its\nstrong capabilities in modeling long sequences and relatively low computational\ncomplexity. In this work, we introduce Mamba-SEUNet, an innovative architecture\nthat integrates Mamba with U-Net for SE tasks. By leveraging bidirectional\nMamba to model forward and backward dependencies of speech signals at different\nresolutions, and incorporating skip connections to capture multi-scale\ninformation, our approach achieves state-of-the-art (SOTA) performance.\nExperimental results on the VCTK+DEMAND dataset indicate that Mamba-SEUNet\nattains a PESQ score of 3.59, while maintaining low computational complexity.\nWhen combined with the Perceptual Contrast Stretching technique, Mamba-SEUNet\nfurther improves the PESQ score to 3.73.", "published": "2024-12-21 13:43:51", "link": "http://arxiv.org/abs/2412.16626v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
