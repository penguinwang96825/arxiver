{"title": "Investigating Societal Biases in a Poetry Composition System", "abstract": "There is a growing collection of work analyzing and mitigating societal\nbiases in language understanding, generation, and retrieval tasks, though\nexamining biases in creative tasks remains underexplored. Creative language\napplications are meant for direct interaction with users, so it is important to\nquantify and mitigate societal biases in these applications. We introduce a\nnovel study on a pipeline to mitigate societal biases when retrieving next\nverse suggestions in a poetry composition system. Our results suggest that data\naugmentation through sentiment style transfer has potential for mitigating\nsocietal biases.", "published": "2020-11-05 07:07:40", "link": "http://arxiv.org/abs/2011.02686v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Aware Answer Extraction in Question Answering", "abstract": "Extractive QA models have shown very promising performance in predicting the\ncorrect answer to a question for a given passage. However, they sometimes\nresult in predicting the correct answer text but in a context irrelevant to the\ngiven question. This discrepancy becomes especially important as the number of\noccurrences of the answer text in a passage increases. To resolve this issue,\nwe propose \\textbf{BLANC} (\\textbf{BL}ock \\textbf{A}ttentio\\textbf{N} for\n\\textbf{C}ontext prediction) based on two main ideas: context prediction as an\nauxiliary task in multi-task learning manner, and a block attention method that\nlearns the context prediction task. With experiments on reading comprehension,\nwe show that BLANC outperforms the state-of-the-art QA models, and the\nperformance gap increases as the number of answer text occurrences increases.\nWe also conduct an experiment of training the models using SQuAD and predicting\nthe supporting facts on HotpotQA and show that BLANC outperforms all baseline\nmodels in this zero-shot setting.", "published": "2020-11-05 07:10:08", "link": "http://arxiv.org/abs/2011.02687v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NUAA-QMUL at SemEval-2020 Task 8: Utilizing BERT and DenseNet for\n  Internet Meme Emotion Analysis", "abstract": "This paper describes our contribution to SemEval 2020 Task 8: Memotion\nAnalysis. Our system learns multi-modal embeddings from text and images in\norder to classify Internet memes by sentiment. Our model learns text embeddings\nusing BERT and extracts features from images with DenseNet, subsequently\ncombining both features through concatenation. We also compare our results with\nthose produced by DenseNet, ResNet, BERT, and BERT-ResNet. Our results show\nthat image classification models have the potential to help classifying memes,\nwith DenseNet outperforming ResNet. Adding text features is however not always\nhelpful for Memotion Analysis.", "published": "2020-11-05 12:39:30", "link": "http://arxiv.org/abs/2011.02788v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Augmentation and Terminology Integration for Domain-Specific\n  Sinhala-English-Tamil Statistical Machine Translation", "abstract": "Out of vocabulary (OOV) is a problem in the context of Machine Translation\n(MT) in low-resourced languages. When source and/or target languages are\nmorphologically rich, it becomes even worse. Bilingual list integration is an\napproach to address the OOV problem. This allows more words to be translated\nthan are in the training data. However, since bilingual lists contain words in\nthe base form, it will not translate inflected forms for morphologically rich\nlanguages such as Sinhala and Tamil. This paper focuses on data augmentation\ntechniques where bilingual lexicon terms are expanded based on case-markers\nwith the objective of generating new words, to be used in Statistical machine\nTranslation (SMT). This data augmentation technique for dictionary terms shows\nimproved BLEU scores for Sinhala-English SMT.", "published": "2020-11-05 13:58:32", "link": "http://arxiv.org/abs/2011.02821v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QMUL-SDS @ DIACR-Ita: Evaluating Unsupervised Diachronic Lexical\n  Semantics Classification in Italian", "abstract": "In this paper, we present the results and main findings of our system for the\nDIACR-ITA 2020 Task. Our system focuses on using variations of training sets\nand different semantic detection methods. The task involves training, aligning\nand predicting a word's vector change from two diachronic Italian corpora. We\ndemonstrate that using Temporal Word Embeddings with a Compass C-BOW model is\nmore effective compared to different approaches including Logistic Regression\nand a Feed Forward Neural Network using accuracy. Our model ranked 3rd with an\naccuracy of 83.3%.", "published": "2020-11-05 16:00:35", "link": "http://arxiv.org/abs/2011.02935v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Efficient Task-Specific Meta-Embeddings with Word Prisms", "abstract": "Word embeddings are trained to predict word cooccurrence statistics, which\nleads them to possess different lexical properties (syntactic, semantic, etc.)\ndepending on the notion of context defined at training time. These properties\nmanifest when querying the embedding space for the most similar vectors, and\nwhen used at the input layer of deep neural networks trained to solve\ndownstream NLP problems. Meta-embeddings combine multiple sets of differently\ntrained word embeddings, and have been shown to successfully improve intrinsic\nand extrinsic performance over equivalent models which use just one set of\nsource embeddings. We introduce word prisms: a simple and efficient\nmeta-embedding method that learns to combine source embeddings according to the\ntask at hand. Word prisms learn orthogonal transformations to linearly combine\nthe input source embeddings, which allows them to be very efficient at\ninference time. We evaluate word prisms in comparison to other meta-embedding\nmethods on six extrinsic evaluations and observe that word prisms offer\nimprovements in performance on all tasks.", "published": "2020-11-05 16:08:50", "link": "http://arxiv.org/abs/2011.02944v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CODER: Knowledge infused cross-lingual medical term embedding for term\n  normalization", "abstract": "This paper proposes CODER: contrastive learning on knowledge graphs for\ncross-lingual medical term representation. CODER is designed for medical term\nnormalization by providing close vector representations for different terms\nthat represent the same or similar medical concepts with cross-lingual support.\nWe train CODER via contrastive learning on a medical knowledge graph (KG) named\nthe Unified Medical Language System, where similarities are calculated\nutilizing both terms and relation triplets from KG. Training with relations\ninjects medical knowledge into embeddings and aims to provide potentially\nbetter machine learning features. We evaluate CODER in zero-shot term\nnormalization, semantic similarity, and relation classification benchmarks,\nwhich show that CODERoutperforms various state-of-the-art biomedical word\nembedding, concept embeddings, and contextual embeddings. Our codes and models\nare available at https://github.com/GanjinZero/CODER.", "published": "2020-11-05 16:16:49", "link": "http://arxiv.org/abs/2011.02947v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Competence-Level Prediction and Resume & Job Description Matching Using\n  Context-Aware Transformer Models", "abstract": "This paper presents a comprehensive study on resume classification to reduce\nthe time and labor needed to screen an overwhelming number of applications\nsignificantly, while improving the selection of suitable candidates. A total of\n6,492 resumes are extracted from 24,933 job applications for 252 positions\ndesignated into four levels of experience for Clinical Research Coordinators\n(CRC). Each resume is manually annotated to its most appropriate CRC position\nby experts through several rounds of triple annotation to establish guidelines.\nAs a result, a high Kappa score of 61% is achieved for inter-annotator\nagreement. Given this dataset, novel transformer-based classification models\nare developed for two tasks: the first task takes a resume and classifies it to\na CRC level (T1), and the second task takes both a resume and a job description\nto apply and predicts if the application is suited to the job T2. Our best\nmodels using section encoding and multi-head attention decoding give results of\n73.3% to T1 and 79.2% to T2. Our analysis shows that the prediction errors are\nmostly made among adjacent CRC levels, which are hard for even experts to\ndistinguish, implying the practical value of our models in real HR platforms.", "published": "2020-11-05 17:47:03", "link": "http://arxiv.org/abs/2011.02998v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "MEGA RST Discourse Treebanks with Structure and Nuclearity from Scalable\n  Distant Sentiment Supervision", "abstract": "The lack of large and diverse discourse treebanks hinders the application of\ndata-driven approaches, such as deep-learning, to RST-style discourse parsing.\nIn this work, we present a novel scalable methodology to automatically generate\ndiscourse treebanks using distant supervision from sentiment-annotated\ndatasets, creating and publishing MEGA-DT, a new large-scale\ndiscourse-annotated corpus. Our approach generates discourse trees\nincorporating structure and nuclearity for documents of arbitrary length by\nrelying on an efficient heuristic beam-search strategy, extended with a\nstochastic component. Experiments on multiple datasets indicate that a\ndiscourse parser trained on our MEGA-DT treebank delivers promising\ninter-domain performance gains when compared to parsers trained on\nhuman-annotated discourse corpora.", "published": "2020-11-05 18:22:38", "link": "http://arxiv.org/abs/2011.03017v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Sentiment Annotations to Sentiment Prediction through Discourse\n  Augmentation", "abstract": "Sentiment analysis, especially for long documents, plausibly requires methods\ncapturing complex linguistics structures. To accommodate this, we propose a\nnovel framework to exploit task-related discourse for the task of sentiment\nanalysis. More specifically, we are combining the large-scale,\nsentiment-dependent MEGA-DT treebank with a novel neural architecture for\nsentiment prediction, based on a hybrid TreeLSTM hierarchical attention model.\nExperiments show that our framework using sentiment-related discourse\naugmentations for sentiment prediction enhances the overall performance for\nlong documents, even beyond previous approaches using well-established\ndiscourse parsers trained on human annotated data. We show that a simple\nensemble approach can further enhance performance by selectively using\ndiscourse, depending on the document length.", "published": "2020-11-05 18:28:13", "link": "http://arxiv.org/abs/2011.03021v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Hallucinated Content in Conditional Neural Sequence Generation", "abstract": "Neural sequence models can generate highly fluent sentences, but recent\nstudies have also shown that they are also prone to hallucinate additional\ncontent not supported by the input. These variety of fluent but wrong outputs\nare particularly problematic, as it will not be possible for users to tell they\nare being presented incorrect content. To detect these errors, we propose a\ntask to predict whether each token in the output sequence is hallucinated (not\ncontained in the input) and collect new manually annotated evaluation sets for\nthis task. We also introduce a method for learning to detect hallucinations\nusing pretrained language models fine tuned on synthetic data that includes\nautomatically inserted hallucinations Experiments on machine translation (MT)\nand abstractive summarization demonstrate that our proposed approach\nconsistently outperforms strong baselines on all benchmark datasets. We further\ndemonstrate how to use the token-level hallucination labels to define a\nfine-grained loss over the target sequence in low-resource MT and achieve\nsignificant improvements over strong baseline methods. We also apply our method\nto word-level quality estimation for MT and show its effectiveness in both\nsupervised and unsupervised settings. Codes and data available at\nhttps://github.com/violet-zct/fairseq-detect-hallucination.", "published": "2020-11-05 00:18:53", "link": "http://arxiv.org/abs/2011.02593v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Event Duration Prediction via Time-aware Pre-training", "abstract": "End-to-end models in NLP rarely encode external world knowledge about length\nof time. We introduce two effective models for duration prediction, which\nincorporate external knowledge by reading temporal-related news sentences\n(time-aware pre-training). Specifically, one model predicts the range/unit\nwhere the duration value falls in (R-pred); and the other predicts the exact\nduration value E-pred. Our best model -- E-pred, substantially outperforms\nprevious work, and captures duration information more accurately than R-pred.\nWe also demonstrate our models are capable of duration prediction in the\nunsupervised setting, outperforming the baselines.", "published": "2020-11-05 01:52:11", "link": "http://arxiv.org/abs/2011.02610v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Entity Linking in 100 Languages", "abstract": "We propose a new formulation for multilingual entity linking, where\nlanguage-specific mentions resolve to a language-agnostic Knowledge Base. We\ntrain a dual encoder in this new setting, building on prior work with improved\nfeature representation, negative mining, and an auxiliary entity-pairing task,\nto obtain a single entity retrieval model that covers 100+ languages and 20\nmillion entities. The model outperforms state-of-the-art results from a far\nmore limited cross-lingual linking task. Rare entities and low-resource\nlanguages pose challenges at this large-scale, so we advocate for an increased\nfocus on zero- and few-shot evaluation. To this end, we provide Mewsli-9, a\nlarge new multilingual dataset (http://goo.gle/mewsli-dataset) matched to our\nsetting, and show how frequency-based analysis provided key insights for our\nmodel and training enhancements.", "published": "2020-11-05 07:28:35", "link": "http://arxiv.org/abs/2011.02690v1", "categories": ["cs.CL", "cs.IR", "I.2.7; H.3.3"], "primary_category": "cs.CL"}
{"title": "Improving Commonsense Question Answering by Graph-based Iterative\n  Retrieval over Multiple Knowledge Sources", "abstract": "In order to facilitate natural language understanding, the key is to engage\ncommonsense or background knowledge. However, how to engage commonsense\neffectively in question answering systems is still under exploration in both\nresearch academia and industry. In this paper, we propose a novel\nquestion-answering method by integrating multiple knowledge sources, i.e.\nConceptNet, Wikipedia, and the Cambridge Dictionary, to boost the performance.\nMore concretely, we first introduce a novel graph-based iterative knowledge\nretrieval module, which iteratively retrieves concepts and entities related to\nthe given question and its choices from multiple knowledge sources. Afterward,\nwe use a pre-trained language model to encode the question, retrieved knowledge\nand choices, and propose an answer choice-aware attention mechanism to fuse all\nhidden representations of the previous modules. Finally, the linear classifier\nfor specific tasks is used to predict the answer. Experimental results on the\nCommonsenseQA dataset show that our method significantly outperforms other\ncompetitive methods and achieves the new state-of-the-art. In addition, further\nablation studies demonstrate the effectiveness of our graph-based iterative\nknowledge retrieval module and the answer choice-aware attention module in\nretrieving and synthesizing background knowledge from multiple knowledge\nsources.", "published": "2020-11-05 08:50:43", "link": "http://arxiv.org/abs/2011.02705v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Model is All You Need: Natural Language Understanding as\n  Question Answering", "abstract": "Different flavors of transfer learning have shown tremendous impact in\nadvancing research and applications of machine learning. In this work we study\nthe use of a specific family of transfer learning, where the target domain is\nmapped to the source domain. Specifically we map Natural Language Understanding\n(NLU) problems to QuestionAnswering (QA) problems and we show that in low data\nregimes this approach offers significant improvements compared to other\napproaches to NLU. Moreover we show that these gains could be increased through\nsequential transfer learning across NLU problems from different domains. We\nshow that our approach could reduce the amount of required data for the same\nperformance by up to a factor of 10.", "published": "2020-11-05 18:31:22", "link": "http://arxiv.org/abs/2011.03023v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HoVer: A Dataset for Many-Hop Fact Extraction And Claim Verification", "abstract": "We introduce HoVer (HOppy VERification), a dataset for many-hop evidence\nextraction and fact verification. It challenges models to extract facts from\nseveral Wikipedia articles that are relevant to a claim and classify whether\nthe claim is Supported or Not-Supported by the facts. In HoVer, the claims\nrequire evidence to be extracted from as many as four English Wikipedia\narticles and embody reasoning graphs of diverse shapes. Moreover, most of the\n3/4-hop claims are written in multiple sentences, which adds to the complexity\nof understanding long-range dependency relations such as coreference. We show\nthat the performance of an existing state-of-the-art semantic-matching model\ndegrades significantly on our dataset as the number of reasoning hops\nincreases, hence demonstrating the necessity of many-hop reasoning to achieve\nstrong results. We hope that the introduction of this challenging dataset and\nthe accompanying evaluation task will encourage research in many-hop fact\nretrieval and information verification. We make the HoVer dataset publicly\navailable at https://hover-nlp.github.io", "published": "2020-11-05 20:33:11", "link": "http://arxiv.org/abs/2011.03088v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Machine Generation and Detection of Arabic Manipulated and Fake News", "abstract": "Fake news and deceptive machine-generated text are serious problems\nthreatening modern societies, including in the Arab world. This motivates work\non detecting false and manipulated stories online. However, a bottleneck for\nthis research is lack of sufficient data to train detection models. We present\na novel method for automatically generating Arabic manipulated (and potentially\nfake) news stories. Our method is simple and only depends on availability of\ntrue stories, which are abundant online, and a part of speech tagger (POS). To\nfacilitate future work, we dispense with both of these requirements altogether\nby providing AraNews, a novel and large POS-tagged news dataset that can be\nused off-the-shelf. Using stories generated based on AraNews, we carry out a\nhuman annotation study that casts light on the effects of machine manipulation\non text veracity. The study also measures human ability to detect Arabic\nmachine manipulated text generated by our method. Finally, we develop the first\nmodels for detecting manipulated Arabic news and achieve state-of-the-art\nresults on Arabic fake news detection (macro F1=70.06). Our models and data are\npublicly available.", "published": "2020-11-05 20:50:22", "link": "http://arxiv.org/abs/2011.03092v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explain by Evidence: An Explainable Memory-based Neural Network for\n  Question Answering", "abstract": "Interpretability and explainability of deep neural networks are challenging\ndue to their scale, complexity, and the agreeable notions on which the\nexplaining process rests. Previous work, in particular, has focused on\nrepresenting internal components of neural networks through human-friendly\nvisuals and concepts. On the other hand, in real life, when making a decision,\nhuman tends to rely on similar situations and/or associations in the past.\nHence arguably, a promising approach to make the model transparent is to design\nit in a way such that the model explicitly connects the current sample with the\nseen ones, and bases its decision on these samples. Grounded on that principle,\nwe propose in this paper an explainable, evidence-based memory network\narchitecture, which learns to summarize the dataset and extract supporting\nevidences to make its decision. Our model achieves state-of-the-art performance\non two popular question answering datasets (i.e. TrecQA and WikiQA). Via\nfurther analysis, we show that this model can reliably trace the errors it has\nmade in the validation step to the training instances that might have caused\nthese errors. We believe that this error-tracing capability provides\nsignificant benefit in improving dataset quality in many applications.", "published": "2020-11-05 21:18:21", "link": "http://arxiv.org/abs/2011.03096v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semi-supervised URL Segmentation with Recurrent Neural Networks\n  Pre-trained on Knowledge Graph Entities", "abstract": "Breaking domain names such as openresearch into component words open and\nresearch is important for applications like Text-to-Speech synthesis and web\nsearch. We link this problem to the classic problem of Chinese word\nsegmentation and show the effectiveness of a tagging model based on Recurrent\nNeural Networks (RNNs) using characters as input. To compensate for the lack of\ntraining data, we propose a pre-training method on concatenated entity names in\na large knowledge database. Pre-training improves the model by 33% and brings\nthe sequence accuracy to 85%.", "published": "2020-11-05 23:31:00", "link": "http://arxiv.org/abs/2011.03138v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adversarial Context Aware Network Embeddings for Textual Networks", "abstract": "Representation learning of textual networks poses a significant challenge as\nit involves capturing amalgamated information from two modalities: (i)\nunderlying network structure, and (ii) node textual attributes. For this, most\nexisting approaches learn embeddings of text and network structure by enforcing\nembeddings of connected nodes to be similar. Then for achieving a modality\nfusion they use the similarities between text embedding of a node with the\nstructure embedding of its connected node and vice versa. This implies that\nthese approaches require edge information for learning embeddings and they\ncannot learn embeddings of unseen nodes. In this paper we propose an approach\nthat achieves both modality fusion and the capability to learn embeddings of\nunseen nodes. The main feature of our model is that it uses an adversarial\nmechanism between text embedding based discriminator, and structure embedding\nbased generator to learn efficient representations. Then for learning\nembeddings of unseen nodes, we use the supervision provided by the text\nembedding based discriminator. In addition this, we propose a novel\narchitecture for learning text embedding that can combine both mutual attention\nand topological attention mechanism, which give more flexible text embeddings.\nThrough extensive experiments on real-world datasets, we demonstrate that our\nmodel makes substantial gains over several state-of-the-art benchmarks. In\ncomparison with previous state-of-the-art, it gives up to 7% improvement in\nperformance in predicting links among nodes seen in the training and up to 12%\nimprovement in performance in predicting links involving nodes not seen in\ntraining. Further, in the node classification task, it gives up to 2%\nimprovement in performance.", "published": "2020-11-05 05:20:01", "link": "http://arxiv.org/abs/2011.02665v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "BW-EDA-EEND: Streaming End-to-End Neural Speaker Diarization for a\n  Variable Number of Speakers", "abstract": "We present a novel online end-to-end neural diarization system, BW-EDA-EEND,\nthat processes data incrementally for a variable number of speakers. The system\nis based on the Encoder-Decoder-Attractor (EDA) architecture of Horiguchi et\nal., but utilizes the incremental Transformer encoder, attending only to its\nleft contexts and using block-level recurrence in the hidden states to carry\ninformation from block to block, making the algorithm complexity linear in\ntime. We propose two variants: For unlimited-latency BW-EDA-EEND, which\nprocesses inputs in linear time, we show only moderate degradation for up to\ntwo speakers using a context size of 10 seconds compared to offline EDA-EEND.\nWith more than two speakers, the accuracy gap between online and offline grows,\nbut the algorithm still outperforms a baseline offline clustering diarization\nsystem for one to four speakers with unlimited context size, and shows\ncomparable accuracy with context size of 10 seconds. For limited-latency\nBW-EDA-EEND, which produces diarization outputs block-by-block as audio\narrives, we show accuracy comparable to the offline clustering-based system.", "published": "2020-11-05 06:42:31", "link": "http://arxiv.org/abs/2011.02678v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Accent Adaptation based on Gate Mechanism", "abstract": "When only a limited amount of accented speech data is available, to promote\nmulti-accent speech recognition performance, the conventional approach is\naccent-specific adaptation, which adapts the baseline model to multiple target\naccents independently. To simplify the adaptation procedure, we explore\nadapting the baseline model to multiple target accents simultaneously with\nmulti-accent mixed data. Thus, we propose using accent-specific top layer with\ngate mechanism (AST-G) to realize multi-accent adaptation. Compared with the\nbaseline model and accent-specific adaptation, AST-G achieves 9.8% and 1.9%\naverage relative WER reduction respectively. However, in real-world\napplications, we can't obtain the accent category label for inference in\nadvance. Therefore, we apply using an accent classifier to predict the accent\nlabel. To jointly train the acoustic model and the accent classifier, we\npropose the multi-task learning with gate mechanism (MTL-G). As the accent\nlabel prediction could be inaccurate, it performs worse than the\naccent-specific adaptation. Yet, in comparison with the baseline model, MTL-G\nachieves 5.1% average relative WER reduction.", "published": "2020-11-05 11:58:36", "link": "http://arxiv.org/abs/2011.02774v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Domain Adaptation Using Class Similarity for Robust Speech Recognition", "abstract": "When only limited target domain data is available, domain adaptation could be\nused to promote performance of deep neural network (DNN) acoustic model by\nleveraging well-trained source model and target domain data. However, suffering\nfrom domain mismatch and data sparsity, domain adaptation is very challenging.\nThis paper proposes a novel adaptation method for DNN acoustic model using\nclass similarity. Since the output distribution of DNN model contains the\nknowledge of similarity among classes, which is applicable to both source and\ntarget domain, it could be transferred from source to target model for the\nperformance improvement. In our approach, we first compute the frame level\nposterior probabilities of source samples using source model. Then, for each\nclass, probabilities of this class are used to compute a mean vector, which we\nrefer to as mean soft labels. During adaptation, these mean soft labels are\nused in a regularization term to train the target model. Experiments showed\nthat our approach outperforms fine-tuning using one-hot labels on both accent\nand noise adaptation task, especially when source and target domain are highly\nmismatched.", "published": "2020-11-05 12:26:43", "link": "http://arxiv.org/abs/2011.02782v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Imagining Grounded Conceptual Representations from Perceptual\n  Information in Situated Guessing Games", "abstract": "In visual guessing games, a Guesser has to identify a target object in a\nscene by asking questions to an Oracle. An effective strategy for the players\nis to learn conceptual representations of objects that are both discriminative\nand expressive enough to ask questions and guess correctly. However, as shown\nby Suglia et al. (2020), existing models fail to learn truly multi-modal\nrepresentations, relying instead on gold category labels for objects in the\nscene both at training and inference time. This provides an unnatural\nperformance advantage when categories at inference time match those at training\ntime, and it causes models to fail in more realistic \"zero-shot\" scenarios\nwhere out-of-domain object categories are involved. To overcome this issue, we\nintroduce a novel \"imagination\" module based on Regularized Auto-Encoders, that\nlearns context-aware and category-aware latent embeddings without relying on\ncategory labels at inference time. Our imagination module outperforms\nstate-of-the-art competitors by 8.26% gameplay accuracy in the CompGuessWhat?!\nzero-shot scenario (Suglia et al., 2020), and it improves the Oracle and\nGuesser accuracy by 2.08% and 12.86% in the GuessWhat?! benchmark, when no gold\ncategories are available at inference time. The imagination module also boosts\nreasoning about object properties and attributes.", "published": "2020-11-05 15:42:29", "link": "http://arxiv.org/abs/2011.02917v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Dark Jargon Interpretation in Underground Forums", "abstract": "Dark jargons are benign-looking words that have hidden, sinister meanings and\nare used by participants of underground forums for illicit behavior. For\nexample, the dark term \"rat\" is often used in lieu of \"Remote Access Trojan\".\nIn this work we present a novel method towards automatically identifying and\ninterpreting dark jargons. We formalize the problem as a mapping from dark\nwords to \"clean\" words with no hidden meaning. Our method makes use of\ninterpretable representations of dark and clean words in the form of\nprobability distributions over a shared vocabulary. In our experiments we show\nour method to be effective in terms of dark jargon identification, as it\noutperforms another related method on simulated data. Using manual evaluation,\nwe show that our method is able to detect dark jargons in a real-world\nunderground forum dataset.", "published": "2020-11-05 18:08:32", "link": "http://arxiv.org/abs/2011.03011v2", "categories": ["cs.CR", "cs.CL", "cs.SI"], "primary_category": "cs.CR"}
{"title": "Quantifying Intimacy in Language", "abstract": "Intimacy is a fundamental aspect of how we relate to others in social\nsettings. Language encodes the social information of intimacy through both\ntopics and other more subtle cues (such as linguistic hedging and swearing).\nHere, we introduce a new computational framework for studying expressions of\nthe intimacy in language with an accompanying dataset and deep learning model\nfor accurately predicting the intimacy level of questions (Pearson's r=0.87).\nThrough analyzing a dataset of 80.5M questions across social media, books, and\nfilms, we show that individuals employ interpersonal pragmatic moves in their\nlanguage to align their intimacy with social settings. Then, in three studies,\nwe further demonstrate how individuals modulate their intimacy to match social\nnorms around gender, social distance, and audience, each validating key\nfindings from studies in social psychology. Our work demonstrates that intimacy\nis a pervasive and impactful social dimension of language.", "published": "2020-11-05 18:27:20", "link": "http://arxiv.org/abs/2011.03020v1", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Alignment Restricted Streaming Recurrent Neural Network Transducer", "abstract": "There is a growing interest in the speech community in developing Recurrent\nNeural Network Transducer (RNN-T) models for automatic speech recognition (ASR)\napplications. RNN-T is trained with a loss function that does not enforce\ntemporal alignment of the training transcripts and audio. As a result, RNN-T\nmodels built with uni-directional long short term memory (LSTM) encoders tend\nto wait for longer spans of input audio, before streaming already decoded ASR\ntokens. In this work, we propose a modification to the RNN-T loss function and\ndevelop Alignment Restricted RNN-T (Ar-RNN-T) models, which utilize audio-text\nalignment information to guide the loss computation. We compare the proposed\nmethod with existing works, such as monotonic RNN-T, on LibriSpeech and\nin-house datasets. We show that the Ar-RNN-T loss provides a refined control to\nnavigate the trade-offs between the token emission delays and the Word Error\nRate (WER). The Ar-RNN-T models also improve downstream applications such as\nthe ASR End-pointing by guaranteeing token emissions within any given range of\nlatency. Moreover, the Ar-RNN-T loss allows for bigger batch sizes and 4 times\nhigher throughput for our LSTM model architecture, enabling faster training and\nconvergence on GPUs.", "published": "2020-11-05 19:38:54", "link": "http://arxiv.org/abs/2011.03072v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "EXAMS: A Multi-Subject High School Examinations Dataset for\n  Cross-Lingual and Multilingual Question Answering", "abstract": "We propose EXAMS -- a new benchmark dataset for cross-lingual and\nmultilingual question answering for high school examinations. We collected more\nthan 24,000 high-quality high school exam questions in 16 languages, covering 8\nlanguage families and 24 school subjects from Natural Sciences and Social\nSciences, among others.\n  EXAMS offers a fine-grained evaluation framework across multiple languages\nand subjects, which allows precise analysis and comparison of various models.\nWe perform various experiments with existing top-performing multilingual\npre-trained models and we show that EXAMS offers multiple challenges that\nrequire multilingual knowledge and reasoning in multiple domains. We hope that\nEXAMS will enable researchers to explore challenging reasoning and knowledge\ntransfer methods and pre-trained models for school question answering in\nvarious languages which was not possible before. The data, code, pre-trained\nmodels, and evaluation are available at https://github.com/mhardalov/exams-qa.", "published": "2020-11-05 20:06:50", "link": "http://arxiv.org/abs/2011.03080v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving RNN Transducer Based ASR with Auxiliary Tasks", "abstract": "End-to-end automatic speech recognition (ASR) models with a single neural\nnetwork have recently demonstrated state-of-the-art results compared to\nconventional hybrid speech recognizers. Specifically, recurrent neural network\ntransducer (RNN-T) has shown competitive ASR performance on various benchmarks.\nIn this work, we examine ways in which RNN-T can achieve better ASR accuracy\nvia performing auxiliary tasks. We propose (i) using the same auxiliary task as\nprimary RNN-T ASR task, and (ii) performing context-dependent graphemic state\nprediction as in conventional hybrid modeling. In transcribing social media\nvideos with varying training data size, we first evaluate the streaming ASR\nperformance on three languages: Romanian, Turkish and German. We find that both\nproposed methods provide consistent improvements. Next, we observe that both\nauxiliary tasks demonstrate efficacy in learning deep transformer encoders for\nRNN-T criterion, thus achieving competitive results - 2.0%/4.2% WER on\nLibriSpeech test-clean/other - as compared to prior top performing models.", "published": "2020-11-05 21:46:32", "link": "http://arxiv.org/abs/2011.03109v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "PubSqueezer: A Text-Mining Web Tool to Transform Unstructured Documents\n  into Structured Data", "abstract": "The amount of scientific papers published every day is daunting and\nconstantly increasing. Keeping up with literature represents a challenge. If\none wants to start exploring new topics it is hard to have a big picture\nwithout reading lots of articles. Furthermore, as one reads through literature,\nmaking mental connections is crucial to ask new questions which might lead to\ndiscoveries. In this work, I present a web tool which uses a Text Mining\nstrategy to transform large collections of unstructured biomedical articles\ninto structured data. Generated results give a quick overview on complex topics\nwhich can possibly suggest not explicitly reported information. In particular,\nI show two Data Science analyses. First, I present a literature based rare\ndiseases network build using this tool in the hope that it will help clarify\nsome aspects of these less popular pathologies. Secondly, I show how a\nliterature based analysis conducted with PubSqueezer results allows to describe\nknown facts about SARS-CoV-2. In one sentence, data generated with PubSqueezer\nmake it easy to use scientific literate in any computational analysis such as\nmachine learning, natural language processing etc.\n  Availability: http://www.pubsqueezer.com", "published": "2020-11-05 22:23:18", "link": "http://arxiv.org/abs/2011.03123v2", "categories": ["cs.IR", "cs.CL", "q-bio.QM", "H.3; I.2.7; E.0; J.3"], "primary_category": "cs.IR"}
{"title": "Learning to Respond with Your Favorite Stickers: A Framework of Unifying\n  Multi-Modality and User Preference in Multi-Turn Dialog", "abstract": "Stickers with vivid and engaging expressions are becoming increasingly\npopular in online messaging apps, and some works are dedicated to automatically\nselect sticker response by matching the stickers image with previous\nutterances. However, existing methods usually focus on measuring the matching\ndegree between the dialog context and sticker image, which ignores the user\npreference of using stickers. Hence, in this paper, we propose to recommend an\nappropriate sticker to user based on multi-turn dialog context and sticker\nusing history of user. Two main challenges are confronted in this task. One is\nto model the sticker preference of user based on the previous sticker selection\nhistory. Another challenge is to jointly fuse the user preference and the\nmatching between dialog context and candidate sticker into final prediction\nmaking. To tackle these challenges, we propose a \\emph{Preference Enhanced\nSticker Response Selector} (PESRS) model. Specifically, PESRS first employs a\nconvolutional based sticker image encoder and a self-attention based multi-turn\ndialog encoder to obtain the representation of stickers and utterances. Next,\ndeep interaction network is proposed to conduct deep matching between the\nsticker and each utterance. Then, we model the user preference by using the\nrecently selected stickers as input, and use a key-value memory network to\nstore the preference representation. PESRS then learns the short-term and\nlong-term dependency between all interaction results by a fusion network, and\ndynamically fuse the user preference representation into the final sticker\nselection prediction. Extensive experiments conducted on a large-scale\nreal-world dialog dataset show that our model achieves the state-of-the-art\nperformance for all commonly-used metrics. Experiments also verify the\neffectiveness of each component of PESRS.", "published": "2020-11-05 03:31:17", "link": "http://arxiv.org/abs/2011.03322v1", "categories": ["cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Don't look back: an online beat tracking method using RNN and enhanced\n  particle filtering", "abstract": "Online beat tracking (OBT) has always been a challenging task. Due to the\ninaccessibility of future data and the need to make inference in real-time. We\npropose Do not Look back! (DLB), a novel approach optimized for efficiency when\nperforming OBT. DLB feeds the activations of a unidirectional RNN into an\nenhanced Monte-Carlo localization model to infer beat positions. Most\npreexisting OBT methods either apply some offline approaches to a moving window\ncontaining past data to make predictions about future beat positions or must be\nprimed with past data at startup to initialize. Meanwhile, our proposed method\nonly uses activation of the current time frame to infer beat positions. As\nsuch, without waiting at the beginning to receive a chunk, it provides an\nimmediate beat tracking response, which is critical for many OBT applications.\nDLB significantly improves beat tracking accuracy over state-of-the-art OBT\nmethods, yielding a similar performance to offline methods.", "published": "2020-11-05 02:19:52", "link": "http://arxiv.org/abs/2011.02619v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Comparison Study on Infant-Parent Voice Diarization", "abstract": "We design a framework for studying prelinguistic child voicefrom 3 to 24\nmonths based on state-of-the-art algorithms in di-arization. Our system\nconsists of a time-invariant feature ex-tractor, a context-dependent embedding\ngenerator, and a clas-sifier. We study the effect of swapping out different\ncompo-nents of the system, as well as changing loss function, to findthe best\nperformance. We also present a multiple-instancelearning technique that allows\nus to pre-train our parame-ters on larger datasets with coarser segment\nboundary labels.We found that our best system achieved 43.8% DER on\ntestdataset, compared to 55.4% DER achieved by LENA soft-ware. We also found\nthat using convolutional feature extrac-tor instead of logmel features\nsignificantly increases the per-formance of neural diarization.", "published": "2020-11-05 08:21:05", "link": "http://arxiv.org/abs/2011.02698v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-class Spectral Clustering with Overlaps for Speaker Diarization", "abstract": "This paper describes a method for overlap-aware speaker diarization. Given an\noverlap detector and a speaker embedding extractor, our method performs\nspectral clustering of segments informed by the output of the overlap detector.\nThis is achieved by transforming the discrete clustering problem into a convex\noptimization problem which is solved by eigen-decomposition. Thereafter, we\ndiscretize the solution by alternatively using singular value decomposition and\na modified version of non-maximal suppression which is constrained by the\noutput of the overlap detector. Furthermore, we detail an HMM-DNN based overlap\ndetector which performs frame-level classification and enforces duration\nconstraints through HMM state transitions. Our method achieves a test\ndiarization error rate (DER) of 24.0% on the mixed-headset setting of the AMI\nmeeting corpus, which is a relative improvement of 15.2% over a strong\nagglomerative hierarchical clustering baseline, and compares favorably with\nother overlap-aware diarization methods. Further analysis on the LibriCSS data\ndemonstrates the effectiveness of the proposed method in high overlap\nconditions.", "published": "2020-11-05 15:21:28", "link": "http://arxiv.org/abs/2011.02900v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring End-to-End Multi-channel ASR with Bias Information for Meeting\n  Transcription", "abstract": "Joint optimization of multi-channel front-end and automatic speech\nrecognition (ASR) has attracted much interest. While promising results have\nbeen reported for various tasks, past studies on its meeting transcription\napplication were limited to small scale experiments. It is still unclear\nwhether such a joint framework can be beneficial for a more practical setup\nwhere a massive amount of single channel training data can be leveraged for\nbuilding a strong ASR back-end. In this work, we present our investigation on\nthe joint modeling of a mask-based beamformer and\nAttention-Encoder-Decoder-based ASR in the setting where we have 75k hours of\nsingle-channel data and a relatively small amount of real multi-channel data\nfor model training. We explore effective training procedures, including a\ncomparison of simulated and real multi-channel training data. To guide the\nrecognition towards a target speaker and deal with overlapped speech, we also\nexplore various combinations of bias information, such as direction of arrivals\nand speaker profiles. We propose an effective location bias integration method\ncalled deep concatenation for the beamformer network. In our evaluation on\nvarious meeting recordings, we show that the proposed framework achieves a\nsubstantial word error rate reduction.", "published": "2020-11-05 21:47:29", "link": "http://arxiv.org/abs/2011.03110v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Semi-supervised Learning for Singing Synthesis Timbre", "abstract": "We propose a semi-supervised singing synthesizer, which is able to learn new\nvoices from audio data only, without any annotations such as phonetic\nsegmentation. Our system is an encoder-decoder model with two encoders,\nlinguistic and acoustic, and one (acoustic) decoder. In a first step, the\nsystem is trained in a supervised manner, using a labelled multi-singer\ndataset. Here, we ensure that the embeddings produced by both encoders are\nsimilar, so that we can later use the model with either acoustic or linguistic\ninput features. To learn a new voice in an unsupervised manner, the pretrained\nacoustic encoder is used to train a decoder for the target singer. Finally, at\ninference, the pretrained linguistic encoder is used together with the decoder\nof the new voice, to produce acoustic features from linguistic input. We\nevaluate our system with a listening test and show that the results are\ncomparable to those obtained with an equivalent supervised approach.", "published": "2020-11-05 13:33:34", "link": "http://arxiv.org/abs/2011.02809v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Anomalous Sound Detection as a Simple Binary Classification Problem with\n  Careful Selection of Proxy Outlier Examples", "abstract": "Unsupervised anomalous sound detection is concerned with identifying sounds\nthat deviate from what is defined as 'normal', without explicitly specifying\nthe types of anomalies. A significant obstacle is the diversity and rareness of\noutliers, which typically prevent us from collecting a representative set of\nanomalous sounds. As a consequence, most anomaly detection methods use\nunsupervised rather than supervised machine learning methods. Nevertheless, we\nwill show that anomalous sound detection can be effectively framed as a\nsupervised classification problem if the set of anomalous samples is carefully\nsubstituted with what we call proxy outliers. Candidates for proxy outliers are\navailable in abundance as they potentially include all recordings that are\nneither normal nor abnormal sounds. We experiment with the machine condition\nmonitoring data set of the 2020's DCASE Challenge and find proxy outliers with\nmatching recording conditions and high similarity to the target sounds\nparticularly informative. If no data with similar sounds and matching recording\nconditions is available, data sets with a larger diversity in these two\ndimensions are preferable. Our models based on supervised training with proxy\noutliers achieved rank three in Task 2 of the DCASE2020 Challenge.", "published": "2020-11-05 16:22:46", "link": "http://arxiv.org/abs/2011.02949v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "From Note-Level to Chord-Level Neural Network Models for Voice\n  Separation in Symbolic Music", "abstract": "Music is often experienced as a progression of concurrent streams of notes,\nor voices. The degree to which this happens depends on the position along a\nvoice-leading continuum, ranging from monophonic, to homophonic, to polyphonic,\nwhich complicates the design of automatic voice separation models. We address\nthis continuum by defining voice separation as the task of decomposing music\ninto streams that exhibit both a high degree of external perceptual separation\nfrom the other streams and a high degree of internal perceptual consistency.\nThe proposed voice separation task allows for a voice to diverge to multiple\nvoices and also for multiple voices to converge to the same voice. Equipped\nwith this flexible task definition, we manually annotated a corpus of popular\nmusic and used it to train neural networks that assign notes to voices either\nseparately for each note in a chord (note-level), or jointly to all notes in a\nchord (chord-level). The trained neural models greedily assign notes to voices\nin a left to right traversal of the input chord sequence, using a diverse set\nof perceptually informed input features. When evaluated on the extraction of\nconsecutive within voice note pairs, both models surpass a strong baseline\nbased on an iterative application of an envelope extraction function, with the\nchord-level model consistently edging out the note-level model. The two models\nare also shown to outperform previous approaches on separating the voices in\nBach music.", "published": "2020-11-05 18:39:42", "link": "http://arxiv.org/abs/2011.03028v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "I.2"], "primary_category": "cs.SD"}
