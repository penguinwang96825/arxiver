{"title": "RMBR: A Regularized Minimum Bayes Risk Reranking Framework for Machine\n  Translation", "abstract": "Beam search is the most widely used decoding method for neural machine\ntranslation (NMT). In practice, the top-1 candidate with the highest\nlog-probability among the n candidates is selected as the preferred one.\nHowever, this top-1 candidate may not be the best overall translation among the\nn-best list. Recently, Minimum Bayes Risk (MBR) decoding has been proposed to\nimprove the quality for NMT, which seeks for a consensus translation that is\nclosest on average to other candidates from the n-best list. We argue that MBR\nstill suffers from the following problems: The utility function only considers\nthe lexical-level similarity between candidates; The expected utility considers\nthe entire n-best list which is time-consuming and inadequate candidates in the\ntail list may hurt the performance; Only the relationship between candidates is\nconsidered. To solve these issues, we design a regularized MBR reranking\nframework (RMBR), which considers semantic-based similarity and computes the\nexpected utility for each candidate by truncating the list. We expect the\nproposed framework to further consider the translation quality and model\nuncertainty of each candidate. Thus the proposed quality regularizer and\nuncertainty regularizer are incorporated into the framework. Extensive\nexperiments on multiple translation tasks demonstrate the effectiveness of our\nmethod.", "published": "2022-03-01 03:12:17", "link": "http://arxiv.org/abs/2203.00201v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Word Aware Multimodal Refinement for Multimodal Sentiment\n  Analysis with ASR Errors", "abstract": "Multimodal sentiment analysis has attracted increasing attention and lots of\nmodels have been proposed. However, the performance of the state-of-the-art\nmodels decreases sharply when they are deployed in the real world. We find that\nthe main reason is that real-world applications can only access the text\noutputs by the automatic speech recognition (ASR) models, which may be with\nerrors because of the limitation of model capacity. Through further analysis of\nthe ASR outputs, we find that in some cases the sentiment words, the key\nsentiment elements in the textual modality, are recognized as other words,\nwhich makes the sentiment of the text change and hurts the performance of\nmultimodal sentiment models directly. To address this problem, we propose the\nsentiment word aware multimodal refinement model (SWRM), which can dynamically\nrefine the erroneous sentiment words by leveraging multimodal sentiment clues.\nSpecifically, we first use the sentiment word position detection module to\nobtain the most possible position of the sentiment word in the text and then\nutilize the multimodal sentiment word refinement module to dynamically refine\nthe sentiment word embeddings. The refined embeddings are taken as the textual\ninputs of the multimodal feature fusion module to predict the sentiment labels.\nWe conduct extensive experiments on the real-world datasets including\nMOSI-Speechbrain, MOSI-IBM, and MOSI-iFlytek and the results demonstrate the\neffectiveness of our model, which surpasses the current state-of-the-art models\non three datasets. Furthermore, our approach can be adapted for other\nmultimodal feature fusion models easily. Data and code are available at\nhttps://github.com/albertwy/SWRM.", "published": "2022-03-01 06:33:19", "link": "http://arxiv.org/abs/2203.00257v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TableFormer: Robust Transformer Modeling for Table-Text Encoding", "abstract": "Understanding tables is an important aspect of natural language\nunderstanding. Existing models for table understanding require linearization of\nthe table structure, where row or column order is encoded as an unwanted bias.\nSuch spurious biases make the model vulnerable to row and column order\nperturbations. Additionally, prior work has not thoroughly modeled the table\nstructures or table-text alignments, hindering the table-text understanding\nability. In this work, we propose a robust and structurally aware table-text\nencoding architecture TableFormer, where tabular structural biases are\nincorporated completely through learnable attention biases. TableFormer is (1)\nstrictly invariant to row and column orders, and, (2) could understand tables\nbetter due to its tabular inductive biases. Our evaluations showed that\nTableFormer outperforms strong baselines in all settings on SQA, WTQ and\nTabFact table reasoning datasets, and achieves state-of-the-art performance on\nSQA, especially when facing answer-invariant row and column order perturbations\n(6% improvement over the best baseline), because previous SOTA models'\nperformance drops by 4% - 6% when facing such perturbations while TableFormer\nis not affected.", "published": "2022-03-01 07:23:06", "link": "http://arxiv.org/abs/2203.00274v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for\n  Grammar Induction and Text Representation", "abstract": "Recently CKY-based models show great potential in unsupervised grammar\ninduction thanks to their human-like encoding paradigm, which runs recursively\nand hierarchically, but requires $O(n^3)$ time-complexity. Recursive\nTransformer based on Differentiable Trees (R2D2) makes it possible to scale to\nlarge language model pre-training even with complex tree encoder by introducing\na heuristic pruning method. However, the rule-based pruning approach suffers\nfrom local optimum and slow inference issues. In this paper, we fix those\nissues in a unified method. We propose to use a top-down parser as a\nmodel-based pruning method, which also enables parallel encoding during\ninference. Typically, our parser casts parsing as a split point scoring task,\nwhich first scores all split points for a given sentence, and then recursively\nsplits a span into two by picking a split point with the highest score in the\ncurrent span. The reverse order of the splits is considered as the order of\npruning in R2D2 encoder. Beside the bi-directional language model loss, we also\noptimize the parser by minimizing the KL distance between tree probabilities\nfrom parser and R2D2. Our experiments show that our Fast-R2D2 improves\nperformance significantly in grammar induction and achieves competitive results\nin downstream classification tasks.", "published": "2022-03-01 07:54:44", "link": "http://arxiv.org/abs/2203.00281v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Is Whole Word Masking Always Better for Chinese BERT?\": Probing on\n  Chinese Grammatical Error Correction", "abstract": "Whole word masking (WWM), which masks all subwords corresponding to a word at\nonce, makes a better English BERT model. For the Chinese language, however,\nthere is no subword because each token is an atomic character. The meaning of a\nword in Chinese is different in that a word is a compositional unit consisting\nof multiple characters. Such difference motivates us to investigate whether WWM\nleads to better context understanding ability for Chinese BERT. To achieve\nthis, we introduce two probing tasks related to grammatical error correction\nand ask pretrained models to revise or insert tokens in a masked language\nmodeling manner. We construct a dataset including labels for 19,075 tokens in\n10,448 sentences. We train three Chinese BERT models with standard\ncharacter-level masking (CLM), WWM, and a combination of CLM and WWM,\nrespectively. Our major findings are as follows: First, when one character\nneeds to be inserted or replaced, the model trained with CLM performs the best.\nSecond, when more than one character needs to be handled, WWM is the key to\nbetter performance. Finally, when being fine-tuned on sentence-level downstream\ntasks, models trained with different masking strategies perform comparably.", "published": "2022-03-01 08:24:56", "link": "http://arxiv.org/abs/2203.00286v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VScript: Controllable Script Generation with Visual Presentation", "abstract": "In order to offer a customized script tool and inspire professional\nscriptwriters, we present VScript. It is a controllable pipeline that generates\ncomplete scripts, including dialogues and scene descriptions, as well as\npresents visually using video retrieval. With an interactive interface, our\nsystem allows users to select genres and input starting words that control the\ntheme and development of the generated script. We adopt a hierarchical\nstructure, which first generates the plot, then the script and its visual\npresentation. A novel approach is also introduced to plot-guided dialogue\ngeneration by treating it as an inverse dialogue summarization. The experiment\nresults show that our approach outperforms the baselines on both automatic and\nhuman evaluations, especially in genre control.", "published": "2022-03-01 09:43:02", "link": "http://arxiv.org/abs/2203.00314v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Performance of Automated Essay Scoring by using\n  back-translation essays and adjusted scores", "abstract": "Automated essay scoring plays an important role in judging students' language\nabilities in education. Traditional approaches use handcrafted features to\nscore and are time-consuming and complicated. Recently, neural network\napproaches have improved performance without any feature engineering. Unlike\nother natural language processing tasks, only a small number of datasets are\npublicly available for automated essay scoring, and the size of the dataset is\nnot sufficiently large. Considering that the performance of a neural network is\nclosely related to the size of the dataset, the lack of data limits the\nperformance improvement of the automated essay scoring model. In this paper, we\nproposed a method to increase the number of essay-score pairs using\nback-translation and score adjustment and applied it to the Automated Student\nAssessment Prize dataset for augmentation. We evaluated the effectiveness of\nthe augmented data using models from prior work. In addition, performance was\nevaluated in a model using long short-term memory, which is widely used for\nautomated essay scoring. The performance of the models was improved by using\naugmented data to train the models.", "published": "2022-03-01 11:05:43", "link": "http://arxiv.org/abs/2203.00354v2", "categories": ["cs.CL", "I.2.7; I.2.2; K.3.1"], "primary_category": "cs.CL"}
{"title": "Transformer Grammars: Augmenting Transformer Language Models with\n  Syntactic Inductive Biases at Scale", "abstract": "We introduce Transformer Grammars (TGs), a novel class of Transformer\nlanguage models that combine (i) the expressive power, scalability, and strong\nperformance of Transformers and (ii) recursive syntactic compositions, which\nhere are implemented through a special attention mask and deterministic\ntransformation of the linearized tree. We find that TGs outperform various\nstrong baselines on sentence-level language modeling perplexity, as well as on\nmultiple syntax-sensitive language modeling evaluation metrics. Additionally,\nwe find that the recursive syntactic composition bottleneck which represents\neach sentence as a single vector harms perplexity on document-level language\nmodeling, providing evidence that a different kind of memory mechanism -- one\nthat is independent of composed syntactic representations -- plays an important\nrole in current successful models of long text.", "published": "2022-03-01 17:22:31", "link": "http://arxiv.org/abs/2203.00633v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attend, Memorize and Generate: Towards Faithful Table-to-Text Generation\n  in Few Shots", "abstract": "Few-shot table-to-text generation is a task of composing fluent and faithful\nsentences to convey table content using limited data. Despite many efforts\nhaving been made towards generating impressive fluent sentences by fine-tuning\npowerful pre-trained language models, the faithfulness of generated content\nstill needs to be improved. To this end, this paper proposes a novel approach\nAttend, Memorize and Generate (called AMG), inspired by the text generation\nprocess of humans. In particular, AMG (1) attends over the multi-granularity of\ncontext using a novel strategy based on table slot level and traditional\ntoken-by-token level attention to exploit both the table structure and natural\nlinguistic information; (2) dynamically memorizes the table slot allocation\nstates; and (3) generates faithful sentences according to both the context and\nmemory allocation states. Comprehensive experiments with human evaluation on\nthree domains (i.e., humans, songs, and books) of the Wiki dataset show that\nour model can generate higher qualified texts when compared with several\nstate-of-the-art baselines, in both fluency and faithfulness.", "published": "2022-03-01 20:37:20", "link": "http://arxiv.org/abs/2203.00732v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Sentence Knowledge Selection in Open-Domain Dialogue", "abstract": "Incorporating external knowledge sources effectively in conversations is a\nlongstanding problem in open-domain dialogue research. The existing literature\non open-domain knowledge selection is limited and makes certain brittle\nassumptions on knowledge sources to simplify the overall task (Dinan et al.,\n2019), such as the existence of a single relevant knowledge sentence per\ncontext. In this work, we evaluate the existing state of open-domain\nconversation knowledge selection, showing where the existing methodologies\nregarding data and evaluation are flawed. We then improve on them by proposing\na new framework for collecting relevant knowledge, and create an augmented\ndataset based on the Wizard of Wikipedia (WOW) corpus, which we call WOW++.\nWOW++ averages 8 relevant knowledge sentences per dialogue context, embracing\nthe inherent ambiguity of open-domain dialogue knowledge selection. We then\nbenchmark various knowledge ranking algorithms on this augmented dataset with\nboth intrinsic evaluation and extrinsic measures of response quality, showing\nthat neural rerankers that use WOW++ can outperform rankers trained on standard\ndatasets.", "published": "2022-03-01 22:07:05", "link": "http://arxiv.org/abs/2203.00763v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Two-Level Supervised Contrastive Learning for Response Selection in\n  Multi-Turn Dialogue", "abstract": "Selecting an appropriate response from many candidates given the utterances\nin a multi-turn dialogue is the key problem for a retrieval-based dialogue\nsystem. Existing work formalizes the task as matching between the utterances\nand a candidate and uses the cross-entropy loss in learning of the model. This\npaper applies contrastive learning to the problem by using the supervised\ncontrastive loss. In this way, the learned representations of positive examples\nand representations of negative examples can be more distantly separated in the\nembedding space, and the performance of matching can be enhanced. We further\ndevelop a new method for supervised contrastive learning, referred to as\ntwo-level supervised contrastive learning, and employ the method in response\nselection in multi-turn dialogue. Our method exploits two techniques: sentence\ntoken shuffling (STS) and sentence re-ordering (SR) for supervised contrastive\nlearning. Experimental results on three benchmark datasets demonstrate that the\nproposed method significantly outperforms the contrastive learning baseline and\nthe state-of-the-art methods for the task.", "published": "2022-03-01 23:43:36", "link": "http://arxiv.org/abs/2203.00793v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EPPAC: Entity Pre-typing Relation Classification with Prompt\n  AnswerCentralizing", "abstract": "Relation classification (RC) aims to predict the relationship between a pair\nof subject and object in a given context. Recently, prompt tuning approaches\nhave achieved high performance in RC. However, existing prompt tuning\napproaches have the following issues: (1) numerous categories decrease RC\nperformance; (2) manually designed prompts require intensive labor. To address\nthese issues, a novel paradigm, Entity Pre-typing Relation Classification with\nPrompt Answer Centralizing(EPPAC) is proposed in this paper. The entity\npre-tying in EPPAC is presented to address the first issue using a double-level\nframework that pre-types entities before RC and prompt answer centralizing is\nproposed to address the second issue. Extensive experiments show that our\nproposed EPPAC outperformed state-of-the-art approaches on TACRED and TACREV by\n14.4% and 11.1%, respectively. The code is provided in the Supplementary\nMaterials.", "published": "2022-03-01 02:49:06", "link": "http://arxiv.org/abs/2203.00193v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring and Adapting Chinese GPT to Pinyin Input Method", "abstract": "While GPT has become the de-facto method for text generation tasks, its\napplication to pinyin input method remains unexplored. In this work, we make\nthe first exploration to leverage Chinese GPT for pinyin input method. We find\nthat a frozen GPT achieves state-of-the-art performance on perfect pinyin.\nHowever, the performance drops dramatically when the input includes abbreviated\npinyin. A reason is that an abbreviated pinyin can be mapped to many perfect\npinyin, which links to even larger number of Chinese characters. We mitigate\nthis issue with two strategies, including enriching the context with pinyin and\noptimizing the training process to help distinguish homophones. To further\nfacilitate the evaluation of pinyin input method, we create a dataset\nconsisting of 270K instances from 15 domains. Results show that our approach\nimproves performance on abbreviated pinyin across all domains. Model analysis\ndemonstrates that both strategies contribute to the performance boost.", "published": "2022-03-01 06:05:07", "link": "http://arxiv.org/abs/2203.00249v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Time Sensitivity for Question Answering over Temporal\n  Knowledge Graphs", "abstract": "Question answering over temporal knowledge graphs (KGs) efficiently uses\nfacts contained in a temporal KG, which records entity relations and when they\noccur in time, to answer natural language questions (e.g., \"Who was the\npresident of the US before Obama?\"). These questions often involve three\ntime-related challenges that previous work fail to adequately address: 1)\nquestions often do not specify exact timestamps of interest (e.g., \"Obama\"\ninstead of 2000); 2) subtle lexical differences in time relations (e.g.,\n\"before\" vs \"after\"); 3) off-the-shelf temporal KG embeddings that previous\nwork builds on ignore the temporal order of timestamps, which is crucial for\nanswering temporal-order related questions. In this paper, we propose a\ntime-sensitive question answering (TSQA) framework to tackle these problems.\nTSQA features a timestamp estimation module to infer the unwritten timestamp\nfrom the question. We also employ a time-sensitive KG encoder to inject\nordering information into the temporal KG embeddings that TSQA is based on.\nWith the help of techniques to reduce the search space for potential answers,\nTSQA significantly outperforms the previous state of the art on a new benchmark\nfor question answering over temporal KGs, especially achieving a 32% (absolute)\nerror reduction on complex questions that require multiple steps of reasoning\nover facts in the temporal KG.", "published": "2022-03-01 06:21:14", "link": "http://arxiv.org/abs/2203.00255v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Read before Generate! Faithful Long Form Question Answering with Machine\n  Reading", "abstract": "Long-form question answering (LFQA) aims to generate a paragraph-length\nanswer for a given question. While current work on LFQA using large pre-trained\nmodel for generation are effective at producing fluent and somewhat relevant\ncontent, one primary challenge lies in how to generate a faithful answer that\nhas less hallucinated content. We propose a new end-to-end framework that\njointly models answer generation and machine reading. The key idea is to\naugment the generation model with fine-grained, answer-related salient\ninformation which can be viewed as an emphasis on faithful facts.\nState-of-the-art results on two LFQA datasets, ELI5 and MS MARCO, demonstrate\nthe effectiveness of our method, in comparison with strong baselines on\nautomatic and human evaluation metrics. A detailed analysis further proves the\ncompetency of our methods in generating fluent, relevant, and more faithful\nanswers.", "published": "2022-03-01 10:41:17", "link": "http://arxiv.org/abs/2203.00343v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning", "abstract": "Logical reasoning is of vital importance to natural language understanding.\nPrevious studies either employ graph-based models to incorporate prior\nknowledge about logical relations, or introduce symbolic logic into neural\nmodels through data augmentation. These methods, however, heavily depend on\nannotated training data, and thus suffer from over-fitting and poor\ngeneralization problems due to the dataset sparsity. To address these two\nproblems, in this paper, we propose MERIt, a MEta-path guided contrastive\nlearning method for logical ReasonIng of text, to perform self-supervised\npre-training on abundant unlabeled text data. Two novel strategies serve as\nindispensable components of our method. In particular, a strategy based on\nmeta-path is devised to discover the logical structure in natural texts,\nfollowed by a counterfactual data augmentation strategy to eliminate the\ninformation shortcut induced by pre-training. The experimental results on two\nchallenging logical reasoning benchmarks, i.e., ReClor and LogiQA, demonstrate\nthat our method outperforms the SOTA baselines with significant improvements.", "published": "2022-03-01 11:13:00", "link": "http://arxiv.org/abs/2203.00357v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DAMO-NLP at SemEval-2022 Task 11: A Knowledge-based System for\n  Multilingual Named Entity Recognition", "abstract": "The MultiCoNER shared task aims at detecting semantically ambiguous and\ncomplex named entities in short and low-context settings for multiple\nlanguages. The lack of contexts makes the recognition of ambiguous named\nentities challenging. To alleviate this issue, our team DAMO-NLP proposes a\nknowledge-based system, where we build a multilingual knowledge base based on\nWikipedia to provide related context information to the named entity\nrecognition (NER) model. Given an input sentence, our system effectively\nretrieves related contexts from the knowledge base. The original input\nsentences are then augmented with such context information, allowing\nsignificantly better contextualized token representations to be captured. Our\nsystem wins 10 out of 13 tracks in the MultiCoNER shared task.", "published": "2022-03-01 15:29:35", "link": "http://arxiv.org/abs/2203.00545v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DeepNet: Scaling Transformers to 1,000 Layers", "abstract": "In this paper, we propose a simple yet effective method to stabilize\nextremely deep Transformers. Specifically, we introduce a new normalization\nfunction (DeepNorm) to modify the residual connection in Transformer,\naccompanying with theoretically derived initialization. In-depth theoretical\nanalysis shows that model updates can be bounded in a stable way. The proposed\nmethod combines the best of two worlds, i.e., good performance of Post-LN and\nstable training of Pre-LN, making DeepNorm a preferred alternative. We\nsuccessfully scale Transformers up to 1,000 layers (i.e., 2,500 attention and\nfeed-forward network sublayers) without difficulty, which is one order of\nmagnitude deeper than previous deep Transformers. Remarkably, on a multilingual\nbenchmark with 7,482 translation directions, our 200-layer model with 3.2B\nparameters significantly outperforms the 48-layer state-of-the-art model with\n12B parameters by 5 BLEU points, which indicates a promising scaling direction.", "published": "2022-03-01 15:36:38", "link": "http://arxiv.org/abs/2203.00555v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Topological Data Analysis for Word Sense Disambiguation", "abstract": "We develop and test a novel unsupervised algorithm for word sense induction\nand disambiguation which uses topological data analysis. Typical approaches to\nthe problem involve clustering, based on simple low level features of distance\nin word embeddings. Our approach relies on advanced mathematical concepts in\nthe field of topology which provides a richer conceptualization of clusters for\nthe word sense induction tasks. We use a persistent homology barcode algorithm\non the SemCor dataset and demonstrate that our approach gives low relative\nerror on word sense induction. This shows the promise of topological algorithms\nfor natural language processing and we advocate for future work in this\npromising area.", "published": "2022-03-01 15:41:54", "link": "http://arxiv.org/abs/2203.00565v1", "categories": ["cs.CL", "math.AT"], "primary_category": "cs.CL"}
{"title": "Structural invariants and semantic fingerprints in the \"ego network\" of\n  words", "abstract": "Well-established cognitive models coming from anthropology have shown that,\ndue to the cognitive constraints that limit our \"bandwidth\" for social\ninteractions, humans organize their social relations according to a regular\nstructure. In this work, we postulate that similar regularities can be found in\nother cognitive processes, such as those involving language production. In\norder to investigate this claim, we analyse a dataset containing tweets of a\nheterogeneous group of Twitter users (regular users and professional writers).\nLeveraging a methodology similar to the one used to uncover the\nwell-established social cognitive constraints, we find regularities at both the\nstructural and semantic level. At the former, we find that a concentric layered\nstructure (which we call ego network of words, in analogy to the ego network of\nsocial relationships) very well captures how individuals organise the words\nthey use. The size of the layers in this structure regularly grows\n(approximately 2-3 times with respect to the previous one) when moving\noutwards, and the two penultimate external layers consistently account for\napproximately 60% and 30% of the used words, irrespective of the number of the\ntotal number of layers of the user. For the semantic analysis, each ring of\neach ego network is described by a semantic profile, which captures the topics\nassociated with the words in the ring. We find that ring #1 has a special role\nin the model. It is semantically the most dissimilar and the most diverse among\nthe rings. We also show that the topics that are important in the innermost\nring also have the characteristic of being predominant in each of the other\nrings, as well as in the entire ego network. In this respect, ring #1 can be\nseen as the semantic fingerprint of the ego network of words.", "published": "2022-03-01 16:19:14", "link": "http://arxiv.org/abs/2203.00588v2", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Advancing an Interdisciplinary Science of Conversation: Insights from a\n  Large Multimodal Corpus of Human Speech", "abstract": "People spend a substantial portion of their lives engaged in conversation,\nand yet our scientific understanding of conversation is still in its infancy.\nIn this report we advance an interdisciplinary science of conversation, with\nfindings from a large, novel, multimodal corpus of 1,656 recorded conversations\nin spoken English. This 7+ million word, 850 hour corpus totals over 1TB of\naudio, video, and transcripts, with moment-to-moment measures of vocal, facial,\nand semantic expression, along with an extensive survey of speaker post\nconversation reflections. We leverage the considerable scope of the corpus to\n(1) extend key findings from the literature, such as the cooperativeness of\nhuman turn-taking; (2) define novel algorithmic procedures for the segmentation\nof speech into conversational turns; (3) apply machine learning insights across\nvarious textual, auditory, and visual features to analyze what makes\nconversations succeed or fail; and (4) explore how conversations are related to\nwell-being across the lifespan. We also report (5) a comprehensive mixed-method\nreport, based on quantitative analysis and qualitative review of each\nrecording, that showcases how individuals from diverse backgrounds alter their\ncommunication patterns and find ways to connect. We conclude with a discussion\nof how this large-scale public dataset may offer new directions for future\nresearch, especially across disciplinary boundaries, as scholars from a variety\nof fields appear increasingly interested in the study of conversation.", "published": "2022-03-01 18:50:33", "link": "http://arxiv.org/abs/2203.00674v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "E-LANG: Energy-Based Joint Inferencing of Super and Swift Language\n  Models", "abstract": "Building huge and highly capable language models has been a trend in the past\nyears. Despite their great performance, they incur high computational cost. A\ncommon solution is to apply model compression or choose light-weight\narchitectures, which often need a separate fixed-size model for each desirable\ncomputational budget, and may lose performance in case of heavy compression.\nThis paper proposes an effective dynamic inference approach, called E-LANG,\nwhich distributes the inference between large accurate Super-models and\nlight-weight Swift models. To this end, a decision making module routes the\ninputs to Super or Swift models based on the energy characteristics of the\nrepresentations in the latent space. This method is easily adoptable and\narchitecture agnostic. As such, it can be applied to black-box pre-trained\nmodels without a need for architectural manipulations, reassembling of modules,\nor re-training. Unlike existing methods that are only applicable to\nencoder-only backbones and classification tasks, our method also works for\nencoder-decoder structures and sequence-to-sequence tasks such as translation.\nThe E-LANG performance is verified through a set of experiments with T5 and\nBERT backbones on GLUE, SuperGLUE, and WMT. In particular, we outperform T5-11B\nwith an average computations speed-up of 3.3$\\times$ on GLUE and 2.9$\\times$ on\nSuperGLUE. We also achieve BERT-based SOTA on GLUE with 3.2$\\times$ less\ncomputations. Code and demo are available in the supplementary materials.", "published": "2022-03-01 21:21:27", "link": "http://arxiv.org/abs/2203.00748v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HyperPrompt: Prompt-based Task-Conditioning of Transformers", "abstract": "Prompt-Tuning is a new paradigm for finetuning pre-trained language models in\na parameter-efficient way. Here, we explore the use of HyperNetworks to\ngenerate hyper-prompts: we propose HyperPrompt, a novel architecture for\nprompt-based task-conditioning of self-attention in Transformers. The\nhyper-prompts are end-to-end learnable via generation by a HyperNetwork.\nHyperPrompt allows the network to learn task-specific feature maps where the\nhyper-prompts serve as task global memories for the queries to attend to, at\nthe same time enabling flexible information sharing among tasks. We show that\nHyperPrompt is competitive against strong multi-task learning baselines with as\nfew as $0.14\\%$ of additional task-conditioning parameters, achieving great\nparameter and computational efficiency. Through extensive empirical\nexperiments, we demonstrate that HyperPrompt can achieve superior performances\nover strong T5 multi-task learning baselines and parameter-efficient adapter\nvariants including Prompt-Tuning and HyperFormer++ on Natural Language\nUnderstanding benchmarks of GLUE and SuperGLUE across many model sizes.", "published": "2022-03-01 21:57:34", "link": "http://arxiv.org/abs/2203.00759v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic Sentence Composition Reasoning for Multi-Hop Question Answering", "abstract": "Due to the lack of insufficient data, existing multi-hop open domain question\nanswering systems require to effectively find out relevant supporting facts\naccording to each question. To alleviate the challenges of semantic factual\nsentences retrieval and multi-hop context expansion, we present a semantic\nsentence composition reasoning approach for a multi-hop question answering\ntask, which consists of two key modules: a multi-stage semantic matching module\n(MSSM) and a factual sentence composition module (FSC). With the combination of\nfactual sentences and multi-stage semantic retrieval, our approach can provide\nmore comprehensive contextual information for model training and reasoning.\nExperimental results demonstrate our model is able to incorporate existing\npre-trained language models and outperform the existing SOTA method on the QASC\ntask with an improvement of about 9%.", "published": "2022-03-01 00:35:51", "link": "http://arxiv.org/abs/2203.00160v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Investigating Selective Prediction Approaches Across Several Tasks in\n  IID, OOD, and Adversarial Settings", "abstract": "In order to equip NLP systems with selective prediction capability, several\ntask-specific approaches have been proposed. However, which approaches work\nbest across tasks or even if they consistently outperform the simplest baseline\n'MaxProb' remains to be explored. To this end, we systematically study\n'selective prediction' in a large-scale setup of 17 datasets across several NLP\ntasks. Through comprehensive experiments under in-domain (IID), out-of-domain\n(OOD), and adversarial (ADV) settings, we show that despite leveraging\nadditional resources (held-out data/computation), none of the existing\napproaches consistently and considerably outperforms MaxProb in all three\nsettings. Furthermore, their performance does not translate well across tasks.\nFor instance, Monte-Carlo Dropout outperforms all other approaches on Duplicate\nDetection datasets but does not fare well on NLI datasets, especially in the\nOOD setting. Thus, we recommend that future selective prediction approaches\nshould be evaluated across tasks and settings for reliable estimation of their\ncapabilities.", "published": "2022-03-01 03:35:37", "link": "http://arxiv.org/abs/2203.00211v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extended Graph Temporal Classification for Multi-Speaker End-to-End ASR", "abstract": "Graph-based temporal classification (GTC), a generalized form of the\nconnectionist temporal classification loss, was recently proposed to improve\nautomatic speech recognition (ASR) systems using graph-based supervision. For\nexample, GTC was first used to encode an N-best list of pseudo-label sequences\ninto a graph for semi-supervised learning. In this paper, we propose an\nextension of GTC to model the posteriors of both labels and label transitions\nby a neural network, which can be applied to a wider range of tasks. As an\nexample application, we use the extended GTC (GTC-e) for the multi-speaker\nspeech recognition task. The transcriptions and speaker information of\nmulti-speaker speech are represented by a graph, where the speaker information\nis associated with the transitions and ASR outputs with the nodes. Using GTC-e,\nmulti-speaker ASR modelling becomes very similar to single-speaker ASR\nmodeling, in that tokens by multiple speakers are recognized as a single merged\nsequence in chronological order. For evaluation, we perform experiments on a\nsimulated multi-speaker speech dataset derived from LibriSpeech, obtaining\npromising results with performance close to classical benchmarks for the task.", "published": "2022-03-01 05:02:02", "link": "http://arxiv.org/abs/2203.00232v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TRILLsson: Distilled Universal Paralinguistic Speech Representations", "abstract": "Recent advances in self-supervision have dramatically improved the quality of\nspeech representations. However, deployment of state-of-the-art embedding\nmodels on devices has been restricted due to their limited public availability\nand large resource footprint. Our work addresses these issues by publicly\nreleasing a collection of paralinguistic speech models that are small and near\nstate-of-the-art performance. Our approach is based on knowledge distillation,\nand our models are distilled on public data only. We explore different\narchitectures and thoroughly evaluate our models on the Non-Semantic Speech\n(NOSS) benchmark. Our largest distilled model is less than 15% the size of the\noriginal model (314MB vs 2.2GB), achieves over 96% the accuracy on 6 of 7\ntasks, and is trained on 6.5% the data. The smallest model is 1% in size (22MB)\nand achieves over 90% the accuracy on 6 of 7 tasks. Our models outperform the\nopen source Wav2Vec 2.0 model on 6 of 7 tasks, and our smallest model\noutperforms the open source Wav2Vec 2.0 on both emotion recognition tasks\ndespite being 7% the size.", "published": "2022-03-01 05:22:57", "link": "http://arxiv.org/abs/2203.00236v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Vision-and-Language Pre-training via Retrieval-based\n  Multi-Granular Alignment", "abstract": "Vision-and-Language (V+L) pre-training models have achieved tremendous\nsuccess in recent years on various multi-modal benchmarks. However, the\nmajority of existing models require pre-training on a large set of parallel\nimage-text data, which is costly to collect, compared to image-only or\ntext-only data. In this paper, we explore unsupervised Vision-and-Language\npre-training (UVLP) to learn the cross-modal representation from non-parallel\nimage and text datasets. We found two key factors that lead to good\nunsupervised V+L pre-training without parallel data: (i) joint image-and-text\ninput (ii) overall image-text alignment (even for non-parallel data).\nAccordingly, we propose a novel unsupervised V+L pre-training curriculum for\nnon-parallel texts and images. We first construct a weakly aligned image-text\ncorpus via a retrieval-based approach, then apply a set of multi-granular\nalignment pre-training tasks, including region-to-tag, region-to-phrase, and\nimage-to-sentence alignment, to bridge the gap between the two modalities. A\ncomprehensive ablation study shows each granularity is helpful to learn a\nstronger pre-trained model. We adapt our pre-trained model to a set of V+L\ndownstream tasks, including VQA, NLVR2, Visual Entailment, and RefCOCO+. Our\nmodel achieves the state-of-art performance in all these tasks under the\nunsupervised setting.", "published": "2022-03-01 05:34:01", "link": "http://arxiv.org/abs/2203.00242v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "ArabGend: Gender Analysis and Inference on Arabic Twitter", "abstract": "Gender analysis of Twitter can reveal important socio-cultural differences\nbetween male and female users. There has been a significant effort to analyze\nand automatically infer gender in the past for most widely spoken languages'\ncontent, however, to our knowledge very limited work has been done for Arabic.\nIn this paper, we perform an extensive analysis of differences between male and\nfemale users on the Arabic Twitter-sphere. We study differences in user\nengagement, topics of interest, and the gender gap in professions. Along with\ngender analysis, we also propose a method to infer gender by utilizing\nusernames, profile pictures, tweets, and networks of friends. In order to do\nso, we manually annotated gender and locations for ~166K Twitter accounts\nassociated with ~92K user location, which we plan to make publicly available at\nhttp://anonymous.com. Our proposed gender inference method achieve an F1 score\nof 82.1%, which is 47.3% higher than majority baseline. In addition, we also\ndeveloped a demo and made it publicly available.", "published": "2022-03-01 07:13:09", "link": "http://arxiv.org/abs/2203.00271v1", "categories": ["cs.CL", "cs.CY", "cs.SI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "BERT-LID: Leveraging BERT to Improve Spoken Language Identification", "abstract": "Language identification is the task of automatically determining the identity\nof a language conveyed by a spoken segment. It has a profound impact on the\nmultilingual interoperability of an intelligent speech system. Despite language\nidentification attaining high accuracy on medium or long utterances(>3s), the\nperformance on short utterances (<=1s) is still far from satisfactory. We\npropose a BERT-based language identification system (BERT-LID) to improve\nlanguage identification performance, especially on short-duration speech\nsegments. We extend the original BERT model by taking the phonetic\nposteriorgrams (PPG) derived from the front-end phone recognizer as input. Then\nwe deployed the optimal deep classifier followed by it for language\nidentification. Our BERT-LID model can improve the baseline accuracy by about\n6.5% on long-segment identification and 19.9% on short-segment identification,\ndemonstrating our BERT-LID's effectiveness to language identification.", "published": "2022-03-01 10:01:25", "link": "http://arxiv.org/abs/2203.00328v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards a Common Speech Analysis Engine", "abstract": "Recent innovations in self-supervised representation learning have led to\nremarkable advances in natural language processing. That said, in the speech\nprocessing domain, self-supervised representation learning-based systems are\nnot yet considered state-of-the-art. We propose leveraging recent advances in\nself-supervised-based speech processing to create a common speech analysis\nengine. Such an engine should be able to handle multiple speech processing\ntasks, using a single architecture, to obtain state-of-the-art accuracy. The\nengine must also enable support for new tasks with small training datasets.\nBeyond that, a common engine should be capable of supporting distributed\ntraining with client in-house private data. We present the architecture for a\ncommon speech analysis engine based on the HuBERT self-supervised speech\nrepresentation. Based on experiments, we report our results for language\nidentification and emotion recognition on the standard evaluations NIST-LRE 07\nand IEMOCAP. Our results surpass the state-of-the-art performance reported so\nfar on these tasks. We also analyzed our engine on the emotion recognition task\nusing reduced amounts of training data and show how to achieve improved\nresults.", "published": "2022-03-01 16:55:16", "link": "http://arxiv.org/abs/2203.00613v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Measuring the Impact of Individual Domain Factors in Self-Supervised\n  Pre-Training", "abstract": "Human speech data comprises a rich set of domain factors such as accent,\nsyntactic and semantic variety, or acoustic environment. Previous work explores\nthe effect of domain mismatch in automatic speech recognition between\npre-training and fine-tuning as a whole but does not dissect the contribution\nof individual factors. In this paper, we present a controlled study to better\nunderstand the effect of such factors on the performance of pre-trained\nrepresentations on automatic speech recognition. To do so, we pre-train models\neither on modified natural speech or synthesized audio, with a single domain\nfactor modified, and then measure performance after fine-tuning. Results show\nthat phonetic domain factors play an important role during pre-training while\ngrammatical and syntactic factors are far less important. To our knowledge,\nthis is the first study to better understand the domain characteristics of\npre-trained sets in self-supervised pre-training for speech.", "published": "2022-03-01 17:40:51", "link": "http://arxiv.org/abs/2203.00648v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Conformer Based Acoustic Model for Robust Automatic Speech Recognition", "abstract": "This study addresses robust automatic speech recognition (ASR) by introducing\na Conformer-based acoustic model. The proposed model builds on the wide\nresidual bi-directional long short-term memory network (WRBN) with\nutterance-wise dropout and iterative speaker adaptation, but employs a\nConformer encoder instead of the recurrent network. The Conformer encoder uses\na convolution-augmented attention mechanism for acoustic modeling. The proposed\nsystem is evaluated on the monaural ASR task of the CHiME-4 corpus. Coupled\nwith utterance-wise normalization and speaker adaptation, our model achieves\n$6.25\\%$ word error rate, which outperforms WRBN by $8.4\\%$ relatively. In\naddition, the proposed Conformer-based model is $18.3\\%$ smaller in model size\nand reduces total training time by $79.6\\%$.", "published": "2022-03-01 20:17:31", "link": "http://arxiv.org/abs/2203.00725v3", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Topic Analysis for Text with Side Data", "abstract": "Although latent factor models (e.g., matrix factorization) obtain good\nperformance in predictions, they suffer from several problems including\ncold-start, non-transparency, and suboptimal recommendations. In this paper, we\nemploy text with side data to tackle these limitations. We introduce a hybrid\ngenerative probabilistic model that combines a neural network with a latent\ntopic model, which is a four-level hierarchical Bayesian model. In the model,\neach document is modeled as a finite mixture over an underlying set of topics\nand each topic is modeled as an infinite mixture over an underlying set of\ntopic probabilities. Furthermore, each topic probability is modeled as a finite\nmixture over side data. In the context of text, the neural network provides an\noverview distribution about side data for the corresponding text, which is the\nprior distribution in LDA to help perform topic grouping. The approach is\nevaluated on several different datasets, where the model is shown to outperform\nstandard LDA and Dirichlet-multinomial regression (DMR) in terms of topic\ngrouping, model perplexity, classification and comment generation.", "published": "2022-03-01 22:06:30", "link": "http://arxiv.org/abs/2203.00762v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Compliance Checking with NLI: Privacy Policies vs. Regulations", "abstract": "A privacy policy is a document that states how a company intends to handle\nand manage their customers' personal data. One of the problems that arises with\nthese privacy policies is that their content might violate data privacy\nregulations. Because of the enormous number of privacy policies that exist, the\nonly realistic way to check for legal inconsistencies in all of them is through\nan automated method. In this work, we use Natural Language Inference (NLI)\ntechniques to compare privacy regulations against sections of privacy policies\nfrom a selection of large companies. Our NLI model uses pre-trained embeddings,\nalong with BiLSTM in its attention mechanism. We tried two versions of our\nmodel: one that was trained on the Stanford Natural Language Inference (SNLI)\nand the second on the Multi-Genre Natural Language Inference (MNLI) dataset. We\nfound that our test accuracy was higher on our model trained on the SNLI, but\nwhen actually doing NLI tasks on real world privacy policies, the model trained\non MNLI generalized and performed much better.", "published": "2022-03-01 17:27:16", "link": "http://arxiv.org/abs/2204.01845v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual Abusiveness Identification on Code-Mixed Social Media Text", "abstract": "Social Media platforms have been seeing adoption and growth in their usage\nover time. This growth has been further accelerated with the lockdown in the\npast year when people's interaction, conversation, and expression were limited\nphysically. It is becoming increasingly important to keep the platform safe\nfrom abusive content for better user experience. Much work has been done on\nEnglish social media content but text analysis on non-English social media is\nrelatively underexplored. Non-English social media content have the additional\nchallenges of code-mixing, transliteration and using different scripture in\nsame sentence. In this work, we propose an approach for abusiveness\nidentification on the multilingual Moj dataset which comprises of Indic\nlanguages. Our approach tackles the common challenges of non-English social\nmedia content and can be extended to other languages as well.", "published": "2022-03-01 12:23:25", "link": "http://arxiv.org/abs/2204.01848v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Exploring Descriptions of Movement Through Geovisual Analytics", "abstract": "Sensemaking using automatically extracted information from text is a\nchallenging problem. In this paper, we address a specific type of information\nextraction, namely extracting information related to descriptions of movement.\nAggregating and understanding information related to descriptions of movement\nand lack of movement specified in text can lead to an improved understanding\nand sensemaking of movement phenomena of various types, e.g., migration of\npeople and animals, impediments to travel due to COVID-19, etc. We present\nGeoMovement, a system that is based on combining machine learning and\nrule-based extraction of movement-related information with state-of-the-art\nvisualization techniques. Along with the depiction of movement, our tool can\nextract and present a lack of movement. Very little prior work exists on\nautomatically extracting descriptions of movement, especially negation and\nmovement. Apart from addressing these, GeoMovement also provides a novel\nintegrated framework for combining these extraction modules with visualization.\nWe include two systematic case studies of GeoMovement that show how humans can\nderive meaningful geographic movement information. GeoMovement can complement\nprecise movement data, e.g., obtained using sensors, or be used by itself when\nprecise data is unavailable.", "published": "2022-03-01 18:23:02", "link": "http://arxiv.org/abs/2204.09588v1", "categories": ["cs.HC", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "DMF-Net: A decoupling-style multi-band fusion model for full-band speech\n  enhancement", "abstract": "For the difficulty and large computational complexity of modeling more\nfrequency bands, full-band speech enhancement based on deep neural networks is\nstill challenging. Previous studies usually adopt compressed full-band speech\nfeatures in Bark and ERB scale with relatively low frequency resolution,\nleading to degraded performance, especially in the high-frequency region. In\nthis paper, we propose a decoupling-style multi-band fusion model to perform\nfull-band speech denoising and dereverberation. Instead of optimizing the\nfull-band speech by a single network structure, we decompose the full-band\ntarget into multi sub-band speech features and then employ a multi-stage chain\noptimization strategy to estimate clean spectrum stage by stage. Specifically,\nthe low- (0-8 kHz), middle- (8-16 kHz), and high-frequency (16-24 kHz) regions\nare mapped by three separate sub-networks and are then fused to obtain the\nfull-band clean target STFT spectrum. Comprehensive experiments on two public\ndatasets demonstrate that the proposed method outperforms previous advanced\nsystems and yields promising performance in terms of speech quality and\nintelligibility in real complex scenarios.", "published": "2022-03-01 14:07:19", "link": "http://arxiv.org/abs/2203.00472v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Real time spectrogram inversion on mobile phone", "abstract": "We present two methods of real time magnitude spectrogram inversion:\nstreaming Griffin Lim(GL) and streaming MelGAN. We demonstrate the impact of\nlooking ahead on perceptual quality of MelGAN. As little as one hop size\n(12.5ms) of lookahead is able to significantly improve perceptual quality in\ncomparison to its causal version. We compare streaming GL with the streaming\nMelGAN and show different trade-offs in terms of perceptual quality, on-device\nlatency, algorithmic delay, memory footprint and noise sensitivity. For fair\nquality assessment of the GL approach, we use input log magnitude spectrogram\nwithout mel transformation. We evaluate presented real time spectrogram\ninversion approaches on clean, noisy and atypical speech. We specified\nconditions when streaming GL has comparable quality with MelGAN: noisy audio\nand no mel transformation. Streaming GL is 2.4x faster than real time on the\nARM CPU of a Pixel4 and it uses 4.5x times less memory than MelGAN.", "published": "2022-03-01 21:50:01", "link": "http://arxiv.org/abs/2203.00756v6", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A multi-task learning for cavitation detection and cavitation intensity\n  recognition of valve acoustic signals", "abstract": "With the rapid development of smart manufacturing, data-driven machinery\nhealth management has received a growing attention. As one of the most popular\nmethods in machinery health management, deep learning (DL) has achieved\nremarkable successes. However, due to the issues of limited samples and poor\nseparability of different cavitation states of acoustic signals, which greatly\nhinder the eventual performance of DL modes for cavitation intensity\nrecognition and cavitation detection. In this work, a novel multi-task learning\nframework for simultaneous cavitation detection and cavitation intensity\nrecognition framework using 1-D double hierarchical residual networks (1-D\nDHRN) is proposed for analyzing valves acoustic signals. Firstly, a data\naugmentation method based on sliding window with fast Fourier transform\n(Swin-FFT) is developed to alleviate the small-sample issue confronted in this\nstudy. Secondly, a 1-D double hierarchical residual block (1-D DHRB) is\nconstructed to capture sensitive features from the frequency domain acoustic\nsignals of valve. Then, a new structure of 1-D DHRN is proposed. Finally, the\ndevised 1-D DHRN is evaluated on two datasets of valve acoustic signals without\nnoise (Dataset 1 and Dataset 2) and one dataset of valve acoustic signals with\nrealistic surrounding noise (Dataset 3) provided by SAMSON AG (Frankfurt). Our\nmethod has achieved state-of-the-art results. The prediction accurcies of 1-D\nDHRN for cavitation intensitys recognition are as high as 93.75%, 94.31% and\n100%, which indicates that 1-D DHRN outperforms other DL models and\nconventional methods. At the same time, the testing accuracies of 1-D DHRN for\ncavitation detection are as high as 97.02%, 97.64% and 100%. In addition, 1-D\nDHRN has also been tested for different frequencies of samples and shows\nexcellent results for frequency of samples that mobile phones can accommodate.", "published": "2022-03-01 14:59:35", "link": "http://arxiv.org/abs/2203.01118v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SMTNet: Hierarchical cavitation intensity recognition based on sub-main\n  transfer network", "abstract": "With the rapid development of smart manufacturing, data-driven machinery\nhealth management has been of growing attention. In situations where some\nclasses are more difficult to be distinguished compared to others and where\nclasses might be organised in a hierarchy of categories, current DL methods can\nnot work well. In this study, a novel hierarchical cavitation intensity\nrecognition framework using Sub-Main Transfer Network, termed SMTNet, is\nproposed to classify acoustic signals of valve cavitation. SMTNet model outputs\nmultiple predictions ordered from coarse to fine along a network corresponding\nto a hierarchy of target cavitation states. Firstly, a data augmentation method\nbased on Sliding Window with Fast Fourier Transform (Swin-FFT) is developed to\nsolve few-shot problem. Secondly, a 1-D double hierarchical residual block (1-D\nDHRB) is presented to capture sensitive features of the frequency domain valve\nacoustic signals. Thirdly, hierarchical multi-label tree is proposed to assist\nthe embedding of the semantic structure of target cavitation states into\nSMTNet. Fourthly, experience filtering mechanism is proposed to fully learn a\nprior knowledge of cavitation detection model. Finally, SMTNet has been\nevaluated on two cavitation datasets without noise (Dataset 1 and Dataset 2),\nand one cavitation dataset with real noise (Dataset 3) provided by SAMSON AG\n(Frankfurt). The prediction accurcies of SMTNet for cavitation intensity\nrecognition are as high as 95.32%, 97.16% and 100%, respectively. At the same\ntime, the testing accuracies of SMTNet for cavitation detection are as high as\n97.02%, 97.64% and 100%. In addition, SMTNet has also been tested for different\nfrequencies of samples and has achieved excellent results of the highest\nfrequency of samples of mobile phones.", "published": "2022-03-01 15:56:13", "link": "http://arxiv.org/abs/2203.01429v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Non-native Word-level Pronunciation Scoring with Phone-level\n  Mixup Data Augmentation and Multi-source Information", "abstract": "Deep learning-based pronunciation scoring models highly rely on the\navailability of the annotated non-native data, which is costly and has\nscalability issues. To deal with the data scarcity problem, data augmentation\nis commonly used for model pretraining. In this paper, we propose a phone-level\nmixup, a simple yet effective data augmentation method, to improve the\nperformance of word-level pronunciation scoring. Specifically, given a phoneme\nsequence from lexicon, the artificial augmented word sample can be generated by\nrandomly sampling from the corresponding phone-level features in training data,\nwhile the word score is the average of their GOP scores. Benefit from the\narbitrary phone-level combination, the mixup is able to generate any word with\nvarious pronunciation scores. Moreover, we utilize multi-source information\n(e.g., MFCC and deep features) to further improve the scoring system\nperformance. The experiments conducted on the Speechocean762 show that the\nproposed system outperforms the baseline by adding the mixup data for\npretraining, with Pearson correlation coefficients (PCC) increasing from 0.567\nto 0.61. The results also indicate that proposed method achieves similar\nperformance by using 1/10 unlabeled data of baseline. In addition, the\nexperimental results also demonstrate the efficiency of our proposed\nmulti-source approach.", "published": "2022-03-01 05:29:57", "link": "http://arxiv.org/abs/2203.01826v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "A Brief Overview of Unsupervised Neural Speech Representation Learning", "abstract": "Unsupervised representation learning for speech processing has matured\ngreatly in the last few years. Work in computer vision and natural language\nprocessing has paved the way, but speech data offers unique challenges. As a\nresult, methods from other domains rarely translate directly. We review the\ndevelopment of unsupervised representation learning for speech over the last\ndecade. We identify two primary model categories: self-supervised methods and\nprobabilistic latent variable models. We describe the models and develop a\ncomprehensive taxonomy. Finally, we discuss and compare models from the two\ncategories.", "published": "2022-03-01 11:15:35", "link": "http://arxiv.org/abs/2203.01829v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
