{"title": "Coreferential Reasoning Learning for Language Representation", "abstract": "Language representation models such as BERT could effectively capture\ncontextual semantic information from plain text, and have been proved to\nachieve promising results in lots of downstream NLP tasks with appropriate\nfine-tuning. However, most existing language representation models cannot\nexplicitly handle coreference, which is essential to the coherent understanding\nof the whole discourse. To address this issue, we present CorefBERT, a novel\nlanguage representation model that can capture the coreferential relations in\ncontext. The experimental results show that, compared with existing baseline\nmodels, CorefBERT can achieve significant improvements consistently on various\ndownstream NLP tasks that require coreferential reasoning, while maintaining\ncomparable performance to previous models on other common NLP tasks. The source\ncode and experiment details of this paper can be obtained from\nhttps://github.com/thunlp/CorefBERT.", "published": "2020-04-15 03:57:45", "link": "http://arxiv.org/abs/2004.06870v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented\n  Dialogue", "abstract": "The underlying difference of linguistic patterns between general text and\ntask-oriented dialogue makes existing pre-trained language models less useful\nin practice. In this work, we unify nine human-human and multi-turn\ntask-oriented dialogue datasets for language modeling. To better model dialogue\nbehavior during pre-training, we incorporate user and system tokens into the\nmasked language modeling. We propose a contrastive objective function to\nsimulate the response selection task. Our pre-trained task-oriented dialogue\nBERT (TOD-BERT) outperforms strong baselines like BERT on four downstream\ntask-oriented dialogue applications, including intention recognition, dialogue\nstate tracking, dialogue act prediction, and response selection. We also show\nthat TOD-BERT has a stronger few-shot ability that can mitigate the data\nscarcity problem for task-oriented dialogue.", "published": "2020-04-15 04:09:05", "link": "http://arxiv.org/abs/2004.06871v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Probabilistic Soft Logic as a framework for integrating\n  top-down and bottom-up processing of language in a task context", "abstract": "This technical report describes a new prototype architecture designed to\nintegrate top-down and bottom-up analysis of non-standard linguistic input,\nwhere a semantic model of the context of an utterance is used to guide the\nanalysis of the non-standard surface forms, including their automated\nnormalization in context. While the architecture is generally applicable, as a\nconcrete use case of the architecture we target the generation of\nsemantically-informed target hypotheses for answers written by German learners\nin response to reading comprehension questions, where the reading context and\npossible target answers are given.\n  The architecture integrates existing NLP components to produce candidate\nanalyses on eight levels of linguistic modeling, all of which are broken down\ninto atomic statements and connected into a large graphical model using\nProbabilistic Soft Logic (PSL) as a framework. Maximum a posteriori inference\non the resulting graphical model then assigns a belief distribution to\ncandidate target hypotheses. The current version of the architecture builds on\nUniversal Dependencies (UD) as its representation formalism on the form level\nand on Abstract Meaning Representations (AMRs) to represent semantic analyses\nof learner answers and the context information provided by the target answers.\nThese general choices will make it comparatively straightforward to apply the\narchitecture to other tasks and other languages.", "published": "2020-04-15 11:00:07", "link": "http://arxiv.org/abs/2004.07000v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPECTER: Document-level Representation Learning using Citation-informed\n  Transformers", "abstract": "Representation learning is a critical ingredient for natural language\nprocessing systems. Recent Transformer language models like BERT learn powerful\ntextual representations, but these models are targeted towards token- and\nsentence-level training objectives and do not leverage information on\ninter-document relatedness, which limits their document-level representation\npower. For applications on scientific documents, such as classification and\nrecommendation, the embeddings power strong performance on end tasks. We\npropose SPECTER, a new method to generate document-level embedding of\nscientific documents based on pretraining a Transformer language model on a\npowerful signal of document-level relatedness: the citation graph. Unlike\nexisting pretrained language models, SPECTER can be easily applied to\ndownstream applications without task-specific fine-tuning. Additionally, to\nencourage further research on document-level models, we introduce SciDocs, a\nnew evaluation benchmark consisting of seven document-level tasks ranging from\ncitation prediction, to document classification and recommendation. We show\nthat SPECTER outperforms a variety of competitive baselines on the benchmark.", "published": "2020-04-15 16:05:51", "link": "http://arxiv.org/abs/2004.07180v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Structured Embeddings of Knowledge Graphs with Adversarial\n  Learning Framework", "abstract": "Many large-scale knowledge graphs are now available and ready to provide\nsemantically structured information that is regarded as an important resource\nfor question answering and decision support tasks. However, they are built on\nrigid symbolic frameworks which makes them hard to be used in other intelligent\nsystems. We present a learning method using generative adversarial architecture\ndesigned to embed the entities and relations of the knowledge graphs into a\ncontinuous vector space. A generative network (GN) takes two elements of a\n(subject, predicate, object) triple as input and generates the vector\nrepresentation of the missing element. A discriminative network (DN) scores a\ntriple to distinguish a positive triple from those generated by GN. The\ntraining goal for GN is to deceive DN to make wrong classification. When\narriving at a convergence, GN recovers the training data and can be used for\nknowledge graph completion, while DN is trained to be a good triple classifier.\nUnlike few previous studies based on generative adversarial architectures, our\nGN is able to generate unseen instances while they just use GN to better choose\nnegative samples (already existed) for DN. Experiments demonstrate our method\ncan improve classical relational learning models (e.g.TransE) with a\nsignificant margin on both the link prediction and triple classification tasks.", "published": "2020-04-15 18:01:36", "link": "http://arxiv.org/abs/2004.07265v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building a Multi-domain Neural Machine Translation Model using Knowledge\n  Distillation", "abstract": "Lack of specialized data makes building a multi-domain neural machine\ntranslation tool challenging. Although emerging literature dealing with low\nresource languages starts to show promising results, most state-of-the-art\nmodels used millions of sentences. Today, the majority of multi-domain\nadaptation techniques are based on complex and sophisticated architectures that\nare not adapted for real-world applications. So far, no scalable method is\nperforming better than the simple yet effective mixed-finetuning, i.e\nfinetuning a generic model with a mix of all specialized data and generic data.\nIn this paper, we propose a new training pipeline where knowledge distillation\nand multiple specialized teachers allow us to efficiently finetune a model\nwithout adding new costs at inference time. Our experiments demonstrated that\nour training pipeline allows improving the performance of multi-domain\ntranslation over finetuning in configurations with 2, 3, and 4 domains by up to\n2 points in BLEU.", "published": "2020-04-15 20:21:19", "link": "http://arxiv.org/abs/2004.07324v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Linguistic Capacity of Real-Time Counter Automata", "abstract": "Counter machines have achieved a newfound relevance to the field of natural\nlanguage processing (NLP): recent work suggests some strong-performing\nrecurrent neural networks utilize their memory as counters. Thus, one potential\nway to understand the success of these networks is to revisit the theory of\ncounter computation. Therefore, we study the abilities of real-time counter\nmachines as formal grammars, focusing on formal properties that are relevant\nfor NLP models. We first show that several variants of the counter machine\nconverge to express the same class of formal languages. We also prove that\ncounter languages are closed under complement, union, intersection, and many\nother common set operations. Next, we show that counter machines cannot\nevaluate boolean expressions, even though they can weakly validate their\nsyntax. This has implications for the interpretability and evaluation of neural\nnetwork systems: successfully matching syntactic patterns does not guarantee\nthat counter memory accurately encodes compositional semantics. Finally, we\nconsider whether counter languages are semilinear. This work makes general\ncontributions to the theory of formal languages that are of potential interest\nfor understanding recurrent neural networks.", "published": "2020-04-15 03:37:47", "link": "http://arxiv.org/abs/2004.06866v2", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "Framing COVID-19: How we conceptualize and discuss the pandemic on\n  Twitter", "abstract": "Doctors and nurses in these weeks are busy in the trenches, fighting against\na new invisible enemy: Covid-19. Cities are locked down and civilians are\nbesieged in their own homes, to prevent the spreading of the virus. War-related\nterminology is commonly used to frame the discourse around epidemics and\ndiseases. Arguably the discourse around the current epidemic will make use of\nwar-related metaphors too,not only in public discourse and the media, but also\nin the tweets written by non-experts of mass communication. We hereby present\nan analysis of the discourse around #Covid-19, based on a corpus of 200k tweets\nposted on Twitter during March and April 2020. Using topic modelling we first\nanalyze the topics around which the discourse can be classified. Then, we show\nthat the WAR framing is used to talk about specific topics, such as the virus\ntreatment, but not others, such as the effects of social distancing on the\npopulation. We then measure and compare the popularity of the WAR frame to\nthree alternative figurative frames (MONSTER, STORM and TSUNAMI) and a literal\nframe used as control (FAMILY). The results show that while the FAMILY literal\nframe covers a wider portion of the corpus, among the figurative framings WAR\nis the most frequently used, and thus arguably the most conventional one.\nHowever, we conclude, this frame is not apt to elaborate the discourse around\nmany aspects involved in the current situation. Therefore, we conclude, in line\nwith previous suggestions, a plethora of framing options, or a metaphor menu,\nmay facilitate the communication of various aspects involved in the\nCovid-19-related discourse on the social media, and thus support civilians in\nthe expression of their feelings, opinions and ideas during the current\npandemic.", "published": "2020-04-15 10:14:15", "link": "http://arxiv.org/abs/2004.06986v2", "categories": ["cs.CL", "cs.SI", "J.5; I.7.0"], "primary_category": "cs.CL"}
{"title": "Entities as Experts: Sparse Memory Access with Entity Supervision", "abstract": "We focus on the problem of capturing declarative knowledge about entities in\nthe learned parameters of a language model. We introduce a new model - Entities\nas Experts (EAE) - that can access distinct memories of the entities mentioned\nin a piece of text. Unlike previous efforts to integrate entity knowledge into\nsequence models, EAE's entity representations are learned directly from text.\nWe show that EAE's learned representations capture sufficient knowledge to\nanswer TriviaQA questions such as \"Which Dr. Who villain has been played by\nRoger Delgado, Anthony Ainley, Eric Roberts?\", outperforming an\nencoder-generator Transformer model with 10x the parameters. According to the\nLAMA knowledge probes, EAE contains more factual knowledge than a similarly\nsized BERT, as well as previous approaches that integrate external sources of\nentity knowledge. Because EAE associates parameters with specific entities, it\nonly needs to access a fraction of its parameters at inference time, and we\nshow that the correct identification and representation of entities is\nessential to EAE's performance.", "published": "2020-04-15 17:00:05", "link": "http://arxiv.org/abs/2004.07202v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and\n  Textual Data", "abstract": "Existing question answering datasets focus on dealing with homogeneous\ninformation, based either only on text or KB/Table information alone. However,\nas human knowledge is distributed over heterogeneous forms, using homogeneous\ninformation alone might lead to severe coverage problems. To fill in the gap,\nwe present HybridQA https://github.com/wenhuchen/HybridQA, a new large-scale\nquestion-answering dataset that requires reasoning on heterogeneous\ninformation. Each question is aligned with a Wikipedia table and multiple\nfree-form corpora linked with the entities in the table. The questions are\ndesigned to aggregate both tabular information and text information, i.e., lack\nof either form would render the question unanswerable. We test with three\ndifferent models: 1) a table-only model. 2) text-only model. 3) a hybrid model\nthat combines heterogeneous information to find the answer. The experimental\nresults show that the EM scores obtained by two baselines are below 20\\%, while\nthe hybrid model can achieve an EM over 40\\%. This gap suggests the necessity\nto aggregate heterogeneous information in HybridQA. However, the hybrid model's\nscore is still far behind human performance. Hence, HybridQA can serve as a\nchallenging benchmark to study question answering with heterogeneous\ninformation.", "published": "2020-04-15 21:18:15", "link": "http://arxiv.org/abs/2004.07347v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analyzing analytical methods: The case of phonology in neural models of\n  spoken language", "abstract": "Given the fast development of analysis techniques for NLP and speech\nprocessing systems, few systematic studies have been conducted to compare the\nstrengths and weaknesses of each method. As a step in this direction we study\nthe case of representations of phonology in neural network models of spoken\nlanguage. We use two commonly applied analytical techniques, diagnostic\nclassifiers and representational similarity analysis, to quantify to what\nextent neural activation patterns encode phonemes and phoneme sequences. We\nmanipulate two factors that can affect the outcome of analysis. First, we\ninvestigate the role of learning by comparing neural activations extracted from\ntrained versus randomly-initialized models. Second, we examine the temporal\nscope of the activations by probing both local activations corresponding to a\nfew milliseconds of the speech signal, and global activations pooled over the\nwhole utterance. We conclude that reporting analysis results with randomly\ninitialized models is crucial, and that global-scope methods tend to yield more\nconsistent results and we recommend their use as a complement to local-scope\ndiagnostic methods.", "published": "2020-04-15 13:04:15", "link": "http://arxiv.org/abs/2004.07070v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "lamBERT: Language and Action Learning Using Multimodal BERT", "abstract": "Recently, the bidirectional encoder representations from transformers (BERT)\nmodel has attracted much attention in the field of natural language processing,\nowing to its high performance in language understanding-related tasks. The BERT\nmodel learns language representation that can be adapted to various tasks via\npre-training using a large corpus in an unsupervised manner. This study\nproposes the language and action learning using multimodal BERT (lamBERT) model\nthat enables the learning of language and actions by 1) extending the BERT\nmodel to multimodal representation and 2) integrating it with reinforcement\nlearning. To verify the proposed model, an experiment is conducted in a grid\nenvironment that requires language understanding for the agent to act properly.\nAs a result, the lamBERT model obtained higher rewards in multitask settings\nand transfer settings when compared to other models, such as the convolutional\nneural network-based model and the lamBERT model without pre-training.", "published": "2020-04-15 13:54:55", "link": "http://arxiv.org/abs/2004.07093v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Trakhtenbrot's Theorem in Coq, A Constructive Approach to Finite Model\n  Theory", "abstract": "We study finite first-order satisfiability (FSAT) in the constructive setting\nof dependent type theory. Employing synthetic accounts of enumerability and\ndecidability, we give a full classification of FSAT depending on the\nfirst-order signature of non-logical symbols. On the one hand, our development\nfocuses on Trakhtenbrot's theorem, stating that FSAT is undecidable as soon as\nthe signature contains an at least binary relation symbol. Our proof proceeds\nby a many-one reduction chain starting from the Post correspondence problem. On\nthe other hand, we establish the decidability of FSAT for monadic first-order\nlogic, i.e. where the signature only contains at most unary function and\nrelation symbols, as well as the enumerability of FSAT for arbitrary enumerable\nsignatures. All our results are mechanised in the framework of a growing Coq\nlibrary of synthetic undecidability proofs.", "published": "2020-04-15 23:26:04", "link": "http://arxiv.org/abs/2004.07390v1", "categories": ["cs.LO", "cs.CL", "math.LO"], "primary_category": "cs.LO"}
{"title": "Speaker Recognition in Bengali Language from Nonlinear Features", "abstract": "At present Automatic Speaker Recognition system is a very important issue due\nto its diverse applications. Hence, it becomes absolutely necessary to obtain\nmodels that take into consideration the speaking style of a person, vocal tract\ninformation, timbral qualities of his voice and other congenital information\nregarding his voice. The study of Bengali speech recognition and speaker\nidentification is scarce in the literature. Hence the need arises for involving\nBengali subjects in modelling our speaker identification engine. In this work,\nwe have extracted some acoustic features of speech using non linear\nmultifractal analysis. The Multifractal Detrended Fluctuation Analysis reveals\nessentially the complexity associated with the speech signals taken. The source\ncharacteristics have been quantified with the help of different techniques like\nCorrelation Matrix, skewness of MFDFA spectrum etc. The Results obtained from\nthis study gives a good recognition rate for Bengali Speakers.", "published": "2020-04-15 22:38:54", "link": "http://arxiv.org/abs/2004.07820v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sentiment Analysis of Yelp Reviews: A Comparison of Techniques and\n  Models", "abstract": "We use over 350,000 Yelp reviews on 5,000 restaurants to perform an ablation\nstudy on text preprocessing techniques. We also compare the effectiveness of\nseveral machine learning and deep learning models on predicting user sentiment\n(negative, neutral, or positive). For machine learning models, we find that\nusing binary bag-of-word representation, adding bi-grams, imposing minimum\nfrequency constraints and normalizing texts have positive effects on model\nperformance. For deep learning models, we find that using pre-trained word\nembeddings and capping maximum length often boost model performance. Finally,\nusing macro F1 score as our comparison metric, we find simpler models such as\nLogistic Regression and Support Vector Machine to be more effective at\npredicting sentiments than more complex models such as Gradient Boosting, LSTM\nand BERT.", "published": "2020-04-15 18:50:49", "link": "http://arxiv.org/abs/2004.13851v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TXtract: Taxonomy-Aware Knowledge Extraction for Thousands of Product\n  Categories", "abstract": "Extracting structured knowledge from product profiles is crucial for various\napplications in e-Commerce. State-of-the-art approaches for knowledge\nextraction were each designed for a single category of product, and thus do not\napply to real-life e-Commerce scenarios, which often contain thousands of\ndiverse categories. This paper proposes TXtract, a taxonomy-aware knowledge\nextraction model that applies to thousands of product categories organized in a\nhierarchical taxonomy. Through category conditional self-attention and\nmulti-task learning, our approach is both scalable, as it trains a single model\nfor thousands of categories, and effective, as it extracts category-specific\nattribute values. Experiments on products from a taxonomy with 4,000 categories\nshow that TXtract outperforms state-of-the-art approaches by up to 10% in F1\nand 15% in coverage across all categories.", "published": "2020-04-15 03:02:09", "link": "http://arxiv.org/abs/2004.13852v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Musical Features for Automatic Music Transcription Evaluation", "abstract": "This technical report gives a detailed, formal description of the features\nintroduced in the paper: Adrien Ycart, Lele Liu, Emmanouil Benetos and Marcus\nT. Pearce. \"Investigating the Perceptual Validity of Evaluation Metrics for\nAutomatic Piano Music Transcription\", Transactions of the International Society\nfor Music Information Retrieval (TISMIR), Accepted, 2020.", "published": "2020-04-15 15:56:45", "link": "http://arxiv.org/abs/2004.07171v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ESResNet: Environmental Sound Classification Based on Visual Domain\n  Models", "abstract": "Environmental Sound Classification (ESC) is an active research area in the\naudio domain and has seen a lot of progress in the past years. However, many of\nthe existing approaches achieve high accuracy by relying on domain-specific\nfeatures and architectures, making it harder to benefit from advances in other\nfields (e.g., the image domain). Additionally, some of the past successes have\nbeen attributed to a discrepancy of how results are evaluated (i.e., on\nunofficial splits of the UrbanSound8K (US8K) dataset), distorting the overall\nprogression of the field.\n  The contribution of this paper is twofold. First, we present a model that is\ninherently compatible with mono and stereo sound inputs. Our model is based on\nsimple log-power Short-Time Fourier Transform (STFT) spectrograms and combines\nthem with several well-known approaches from the image domain (i.e., ResNet,\nSiamese-like networks and attention). We investigate the influence of\ncross-domain pre-training, architectural changes, and evaluate our model on\nstandard datasets. We find that our model out-performs all previously known\napproaches in a fair comparison by achieving accuracies of 97.0 % (ESC-10),\n91.5 % (ESC-50) and 84.2 % / 85.4 % (US8K mono / stereo).\n  Second, we provide a comprehensive overview of the actual state of the field,\nby differentiating several previously reported results on the US8K dataset\nbetween official or unofficial splits. For better reproducibility, our code\n(including any re-implementations) is made available.", "published": "2020-04-15 19:07:55", "link": "http://arxiv.org/abs/2004.07301v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "F0-consistent many-to-many non-parallel voice conversion via conditional\n  autoencoder", "abstract": "Non-parallel many-to-many voice conversion remains an interesting but\nchallenging speech processing task. Many style-transfer-inspired methods such\nas generative adversarial networks (GANs) and variational autoencoders (VAEs)\nhave been proposed. Recently, AutoVC, a conditional autoencoders (CAEs) based\nmethod achieved state-of-the-art results by disentangling the speaker identity\nand speech content using information-constraining bottlenecks, and it achieves\nzero-shot conversion by swapping in a different speaker's identity embedding to\nsynthesize a new voice. However, we found that while speaker identity is\ndisentangled from speech content, a significant amount of prosodic information,\nsuch as source F0, leaks through the bottleneck, causing target F0 to fluctuate\nunnaturally. Furthermore, AutoVC has no control of the converted F0 and thus\nunsuitable for many applications. In the paper, we modified and improved\nautoencoder-based voice conversion to disentangle content, F0, and speaker\nidentity at the same time. Therefore, we can control the F0 contour, generate\nspeech with F0 consistent with the target speaker, and significantly improve\nquality and similarity. We support our improvement through quantitative and\nqualitative analysis.", "published": "2020-04-15 22:00:06", "link": "http://arxiv.org/abs/2004.07370v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Sound of Guns: Digital Forensics of Gun Audio Samples meets Artificial\n  Intelligence", "abstract": "Classifying a weapon based on its muzzle blast is a challenging task that has\nsignificant applications in various security and military fields. Most of the\nexisting works rely on ad-hoc deployment of spatially diverse microphone\nsensors to capture multiple replicas of the same gunshot, which enables\naccurate detection and identification of the acoustic source. However,\ncarefully controlled setups are difficult to obtain in scenarios such as crime\nscene forensics, making the aforementioned techniques inapplicable and\nimpractical. We introduce a novel technique that requires zero knowledge about\nthe recording setup and is completely agnostic to the relative positions of\nboth the microphone and shooter. Our solution can identify the category,\ncaliber, and model of the gun, reaching over 90% accuracy on a dataset composed\nof 3655 samples that are extracted from YouTube videos. Our results demonstrate\nthe effectiveness and efficiency of applying Convolutional Neural Network (CNN)\nin gunshot classification eliminating the need for an ad-hoc setup while\nsignificantly improving the classification performance.", "published": "2020-04-15 09:12:45", "link": "http://arxiv.org/abs/2004.07948v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Acoustical classification of different speech acts using nonlinear\n  methods", "abstract": "A recitation is a way of combining the words together so that they have a\nsense of rhythm and thus an emotional content is imbibed within. In this study\nwe envisaged to answer these questions in a scientific manner taking into\nconsideration 5 (five) well known Bengali recitations of different poets\nconveying a variety of moods ranging from joy to sorrow. The clips were recited\nas well as read (in the form of flat speech without any rhythm) by the same\nperson to avoid any perceptual difference arising out of timbre variation.\nNext, the emotional content from the 5 recitations were standardized with the\nhelp of listening test conducted on a pool of 50 participants. The recitations\nas well as the speech were analyzed with the help of a latest non linear\ntechnique called Detrended Fluctuation Analysis (DFA) that gives a scaling\nexponent {\\alpha}, which is essentially the measure of long range correlations\npresent in the signal. Similar pieces (the parts which have the exact lyrical\ncontent in speech as well as in the recital) were extracted from the complete\nsignal and analyzed with the help of DFA technique. Our analysis shows that the\nscaling exponent for all parts of recitation were much higher in general as\ncompared to their counterparts in speech. We have also established a critical\nvalue from our analysis, above which a mere speech may become a recitation. The\ncase may be similar to the conventional phase transition, wherein the\nmeasurement of external condition at which the transformation occurs (generally\ntemperature) is called phase transition. Further, we have also categorized the\n5 recitations on the basis of their emotional content with the help of the same\nDFA technique. Analysis with a greater variety of recitations is being carried\nout to yield more interesting results.", "published": "2020-04-15 22:33:16", "link": "http://arxiv.org/abs/2004.08248v2", "categories": ["eess.AS", "cs.SD", "nlin.CD", "q-bio.NC"], "primary_category": "eess.AS"}
