{"title": "Controlled Experiments for Word Embeddings", "abstract": "An experimental approach to studying the properties of word embeddings is\nproposed. Controlled experiments, achieved through modifications of the\ntraining corpus, permit the demonstration of direct relations between word\nproperties and word vector direction and length. The approach is demonstrated\nusing the word2vec CBOW model with experiments that independently vary word\nfrequency and word co-occurrence noise. The experiments reveal that word vector\nlength depends more or less linearly on both word frequency and the level of\nnoise in the co-occurrence distribution of the word. The coefficients of\nlinearity depend upon the word. The special point in feature space, defined by\nthe (artificial) word with pure noise in its co-occurrence distribution, is\nfound to be small but non-zero.", "published": "2015-10-09 14:03:33", "link": "http://arxiv.org/abs/1510.02675v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Human languages order information efficiently", "abstract": "Most languages use the relative order between words to encode meaning\nrelations. Languages differ, however, in what orders they use and how these\norders are mapped onto different meanings. We test the hypothesis that, despite\nthese differences, human languages might constitute different `solutions' to\ncommon pressures of language use. Using Monte Carlo simulations over data from\nfive languages, we find that their word orders are efficient for processing in\nterms of both dependency length and local lexical probability. This suggests\nthat biases originating in how the brain understands language strongly\nconstrain how human languages change over generations.", "published": "2015-10-09 21:05:02", "link": "http://arxiv.org/abs/1510.02823v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Feedforward Sequential Memory Neural Networks without Recurrent Feedback", "abstract": "We introduce a new structure for memory neural networks, called feedforward\nsequential memory networks (FSMN), which can learn long-term dependency without\nusing recurrent feedback. The proposed FSMN is a standard feedforward neural\nnetworks equipped with learnable sequential memory blocks in the hidden layers.\nIn this work, we have applied FSMN to several language modeling (LM) tasks.\nExperimental results have shown that the memory blocks in FSMN can learn\neffective representations of long history. Experiments have shown that FSMN\nbased language models can significantly outperform not only feedforward neural\nnetwork (FNN) based LMs but also the popular recurrent neural network (RNN)\nLMs.", "published": "2015-10-09 15:04:11", "link": "http://arxiv.org/abs/1510.02693v1", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
