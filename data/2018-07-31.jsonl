{"title": "An Enhanced Latent Semantic Analysis Approach for Arabic Document\n  Summarization", "abstract": "The fast-growing amount of information on the Internet makes the research in\nautomatic document summarization very urgent. It is an effective solution for\ninformation overload. Many approaches have been proposed based on different\nstrategies, such as latent semantic analysis (LSA). However, LSA, when applied\nto document summarization, has some limitations which diminish its performance.\nIn this work, we try to overcome these limitations by applying statistic and\nlinear algebraic approaches combined with syntactic and semantic processing of\ntext. First, the part of speech tagger is utilized to reduce the dimension of\nLSA. Then, the weight of the term in four adjacent sentences is added to the\nweighting schemes while calculating the input matrix to take into account the\nword order and the syntactic relations. In addition, a new LSA-based sentence\nselection algorithm is proposed, in which the term description is combined with\nsentence description for each topic which in turn makes the generated summary\nmore informative and diverse. To ensure the effectiveness of the proposed\nLSA-based sentence selection algorithm, extensive experiment on Arabic and\nEnglish are done. Four datasets are used to evaluate the new model, Linguistic\nData Consortium (LDC) Arabic Newswire-a corpus, Essex Arabic Summaries Corpus\n(EASC), DUC2002, and Multilingual MSS 2015 dataset. Experimental results on the\nfour datasets show the effectiveness of the proposed model on Arabic and\nEnglish datasets. It performs comprehensively better compared to the\nstate-of-the-art methods.", "published": "2018-07-31 00:50:15", "link": "http://arxiv.org/abs/1807.11618v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RiTUAL-UH at TRAC 2018 Shared Task: Aggression Identification", "abstract": "This paper presents our system for \"TRAC 2018 Shared Task on Aggression\nIdentification\". Our best systems for the English dataset use a combination of\nlexical and semantic features. However, for Hindi data using only lexical\nfeatures gave us the best results. We obtained weighted F1- measures of 0.5921\nfor the English Facebook task (ranked 12th), 0.5663 for the English Social\nMedia task (ranked 6th), 0.6292 for the Hindi Facebook task (ranked 1st), and\n0.4853 for the Hindi Social Media task (ranked 2nd).", "published": "2018-07-31 09:21:22", "link": "http://arxiv.org/abs/1807.11712v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gender Bias in Neural Natural Language Processing", "abstract": "We examine whether neural natural language processing (NLP) systems reflect\nhistorical biases in training data. We define a general benchmark to quantify\ngender bias in a variety of neural NLP tasks. Our empirical evaluation with\nstate-of-the-art neural coreference resolution and textbook RNN-based language\nmodels trained on benchmark datasets finds significant gender bias in how\nmodels view occupations. We then mitigate bias with CDA: a generic methodology\nfor corpus augmentation via causal interventions that breaks associations\nbetween gendered and gender-neutral words. We empirically show that CDA\neffectively decreases gender bias while preserving accuracy. We also explore\nthe space of mitigation strategies with CDA, a prior approach to word embedding\ndebiasing (WED), and their compositions. We show that CDA outperforms WED,\ndrastically so when word embeddings are trained. For pre-trained embeddings,\nthe two methods can be effectively composed. We also find that as training\nproceeds on the original data set with gradient descent the gender bias grows\nas the loss reduces, indicating that the optimization encourages bias; CDA\nmitigates this behavior.", "published": "2018-07-31 09:27:27", "link": "http://arxiv.org/abs/1807.11714v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Parallel Corpus Mining using Bilingual Sentence Embeddings", "abstract": "This paper presents an effective approach for parallel corpus mining using\nbilingual sentence embeddings. Our embedding models are trained to produce\nsimilar representations exclusively for bilingual sentence pairs that are\ntranslations of each other. This is achieved using a novel training method that\nintroduces hard negatives consisting of sentences that are not translations but\nthat have some degree of semantic similarity. The quality of the resulting\nembeddings are evaluated on parallel corpus reconstruction and by assessing\nmachine translation systems trained on gold vs. mined sentence pairs. We find\nthat the sentence embeddings can be used to reconstruct the United Nations\nParallel Corpus at the sentence level with a precision of 48.9% for en-fr and\n54.9% for en-es. When adapted to document level matching, we achieve a parallel\ndocument matching accuracy that is comparable to the significantly more\ncomputationally intensive approach of [Jakob 2010]. Using reconstructed\nparallel data, we are able to train NMT models that perform nearly as well as\nmodels trained on the original data (within 1-2 BLEU).", "published": "2018-07-31 16:32:50", "link": "http://arxiv.org/abs/1807.11906v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Task Effects in Human Reading with Neural Network-based\n  Attention", "abstract": "Research on human reading has long documented that reading behavior shows\ntask-specific effects, but it has been challenging to build general models\npredicting what reading behavior humans will show in a given task. We introduce\nNEAT, a computational model of the allocation of attention in human reading,\nbased on the hypothesis that human reading optimizes a tradeoff between economy\nof attention and success at a task. Our model is implemented using contemporary\nneural network modeling techniques, and makes explicit and testable predictions\nabout how the allocation of attention varies across different tasks. We test\nthis in an eyetracking study comparing two versions of a reading comprehension\ntask, finding that our model successfully accounts for reading behavior across\nthe tasks. Our work thus provides evidence that task effects can be modeled as\noptimal adaptation to task demands.", "published": "2018-07-31 20:02:08", "link": "http://arxiv.org/abs/1808.00054v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A First Experiment on Including Text Literals in KGloVe", "abstract": "Graph embedding models produce embedding vectors for entities and relations\nin Knowledge Graphs, often without taking literal properties into account. We\nshow an initial idea based on the combination of global graph structure with\nadditional information provided by textual information in properties. Our\ninitial experiment shows that this approach might be useful, but does not\nclearly outperform earlier approaches when evaluated on machine learning tasks.", "published": "2018-07-31 11:18:18", "link": "http://arxiv.org/abs/1807.11761v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "An Ontology-Based Recommender System with an Application to the Star\n  Trek Television Franchise", "abstract": "Collaborative filtering based recommender systems have proven to be extremely\nsuccessful in settings where user preference data on items is abundant.\nHowever, collaborative filtering algorithms are hindered by their weakness\nagainst the item cold-start problem and general lack of interpretability.\nOntology-based recommender systems exploit hierarchical organizations of users\nand items to enhance browsing, recommendation, and profile construction. While\nontology-based approaches address the shortcomings of their collaborative\nfiltering counterparts, ontological organizations of items can be difficult to\nobtain for items that mostly belong to the same category (e.g., television\nseries episodes). In this paper, we present an ontology-based recommender\nsystem that integrates the knowledge represented in a large ontology of\nliterary themes to produce fiction content recommendations. The main novelty of\nthis work is an ontology-based method for computing similarities between items\nand its integration with the classical Item-KNN (K-nearest neighbors)\nalgorithm. As a study case, we evaluated the proposed method against other\napproaches by performing the classical rating prediction task on a collection\nof Star Trek television series episodes in an item cold-start scenario. This\ntransverse evaluation provides insights into the utility of different\ninformation resources and methods for the initial stages of recommender system\ndevelopment. We found our proposed method to be a convenient alternative to\ncollaborative filtering approaches for collections of mostly similar items,\nparticularly when other content-based approaches are not applicable or\notherwise unavailable. Aside from the new methods, this paper contributes a\ntestbed for future research and an online framework to collaboratively extend\nthe ontology of literary themes to cover other narrative content.", "published": "2018-07-31 22:53:30", "link": "http://arxiv.org/abs/1808.00103v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Wasserstein GAN and Waveform Loss-based Acoustic Model Training for\n  Multi-speaker Text-to-Speech Synthesis Systems Using a WaveNet Vocoder", "abstract": "Recent neural networks such as WaveNet and sampleRNN that learn directly from\nspeech waveform samples have achieved very high-quality synthetic speech in\nterms of both naturalness and speaker similarity even in multi-speaker\ntext-to-speech synthesis systems. Such neural networks are being used as an\nalternative to vocoders and hence they are often called neural vocoders. The\nneural vocoder uses acoustic features as local condition parameters, and these\nparameters need to be accurately predicted by another acoustic model. However,\nit is not yet clear how to train this acoustic model, which is problematic\nbecause the final quality of synthetic speech is significantly affected by the\nperformance of the acoustic model. Significant degradation happens, especially\nwhen predicted acoustic features have mismatched characteristics compared to\nnatural ones. In order to reduce the mismatched characteristics between natural\nand generated acoustic features, we propose frameworks that incorporate either\na conditional generative adversarial network (GAN) or its variant, Wasserstein\nGAN with gradient penalty (WGAN-GP), into multi-speaker speech synthesis that\nuses the WaveNet vocoder. We also extend the GAN frameworks and use the\ndiscretized mixture logistic loss of a well-trained WaveNet in addition to mean\nsquared error and adversarial losses as parts of objective functions.\nExperimental results show that acoustic models trained using the WGAN-GP\nframework using back-propagated discretized-mixture-of-logistics (DML) loss\nachieves the highest subjective evaluation scores in terms of both quality and\nspeaker similarity.", "published": "2018-07-31 06:38:54", "link": "http://arxiv.org/abs/1807.11679v1", "categories": ["eess.AS", "cs.CL", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Neural Article Pair Modeling for Wikipedia Sub-article Matching", "abstract": "Nowadays, editors tend to separate different subtopics of a long Wiki-pedia\narticle into multiple sub-articles. This separation seeks to improve human\nreadability. However, it also has a deleterious effect on many Wikipedia-based\ntasks that rely on the article-as-concept assumption, which requires each\nentity (or concept) to be described solely by one article. This underlying\nassumption significantly simplifies knowledge representation and extraction,\nand it is vital to many existing technologies such as automated knowledge base\nconstruction, cross-lingual knowledge alignment, semantic search and data\nlineage of Wikipedia entities. In this paper we provide an approach to match\nthe scattered sub-articles back to their corresponding main-articles, with the\nintent of facilitating automated Wikipedia curation and processing. The\nproposed model adopts a hierarchical learning structure that combines multiple\nvariants of neural document pair encoders with a comprehensive set of explicit\nfeatures. A large crowdsourced dataset is created to support the evaluation and\nfeature extraction for the task. Based on the large dataset, the proposed model\nachieves promising results of cross-validation and significantly outperforms\nprevious approaches. Large-scale serving on the entire English Wikipedia also\nproves the practicability and scalability of the proposed model by effectively\nextracting a vast collection of newly paired main and sub-articles.", "published": "2018-07-31 07:19:36", "link": "http://arxiv.org/abs/1807.11689v2", "categories": ["cs.IR", "cs.CL", "cs.HC"], "primary_category": "cs.IR"}
{"title": "Extensible Grounding of Speech for Robot Instruction", "abstract": "Spoken language is a convenient interface for commanding a mobile robot. Yet\nfor this to work a number of base terms must be grounded in perceptual and\nmotor skills. We detail the language processing used on our robot ELI and\nexplain how this grounding is performed, how it interacts with user gestures,\nand how it handles phenomena such as anaphora. More importantly, however, there\nare certain concepts which the robot cannot be preprogrammed with, such as the\nnames of various objects in a household or the nature of specific tasks it may\nbe requested to perform. In these cases it is vital that there exist a method\nfor extending the grounding, essentially \"learning by being told\". We describe\nhow this was successfully implemented for learning new nouns and verbs in a\ntabletop setting. Creating this language learning kernel may be the last\nexplicit programming the robot ever needs - the core mechanism could eventually\nbe used for imparting a vast amount of knowledge, much as a child learns from\nits parents and teachers.", "published": "2018-07-31 14:31:17", "link": "http://arxiv.org/abs/1807.11838v1", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Manual Post-editing of Automatically Transcribed Speeches from the\n  Icelandic Parliament - Althingi", "abstract": "The design objectives for an automatic transcription system are to produce\ntext readable by humans and to minimize the impact on manual post-editing. This\nstudy reports on a recognition system used for transcribing speeches in the\nIcelandic parliament - Althingi. It evaluates the system performance and its\neffect on manual post-editing. The results are compared against the original\nmanual transcription process. 239 total speeches, consisting of 11 hours and 33\nminutes, were processed, both manually and automatically, and the editing\nprocess was analysed. The dependence of word edit distance on edit time and the\nediting real-time factor has been estimated and compared to user evaluations of\nthe transcription system. The main findings show that the word edit distance is\npositively correlated with edit time and a system achieving a 12.6% edit\ndistance would match the performance of manual transcribers. Producing perfect\ntranscriptions would result in a real-time factor of 2.56. The study also shows\nthat 99% of low error rate speeches received a medium or good grade in\nsubjective evaluations. On the contrary, 21% of high error rate speeches\nreceived a bad grade.", "published": "2018-07-31 16:11:13", "link": "http://arxiv.org/abs/1807.11893v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Delay-Performance Tradeoffs in Causal Microphone Array Processing", "abstract": "In real-time listening enhancement applications, such as hearing aid signal\nprocessing, sounds must be processed with no more than a few milliseconds of\ndelay to sound natural to the listener. Listening devices can achieve better\nperformance with lower delay by using microphone arrays to filter acoustic\nsignals in both space and time. Here, we analyze the tradeoff between delay and\nsquared-error performance of causal multichannel Wiener filters for microphone\narray noise reduction. We compute exact expressions for the delay-error curves\nin two special cases and present experimental results from real-world\nmicrophone array recordings. We find that delay-performance characteristics are\ndetermined by both the spatial and temporal correlation structures of the\nsignals.", "published": "2018-07-31 22:04:22", "link": "http://arxiv.org/abs/1808.00082v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Separation Using Partially Asynchronous Microphone Arrays Without\n  Resampling", "abstract": "We consider the problem of separating speech sources captured by multiple\nspatially separated devices, each of which has multiple microphones and samples\nits signals at a slightly different rate. Most asynchronous array processing\nmethods rely on sample rate offset estimation and resampling, but these offsets\ncan be difficult to estimate if the sources or microphones are moving. We\npropose a source separation method that does not require offset estimation or\nsignal resampling. Instead, we divide the distributed array into several\nsynchronous subarrays. All arrays are used jointly to estimate the time-varying\nsignal statistics, and those statistics are used to design separate\ntime-varying spatial filters in each array. We demonstrate the method for\nspeech mixtures recorded on both stationary and moving microphone arrays.", "published": "2018-07-31 22:34:52", "link": "http://arxiv.org/abs/1808.00096v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Scaling and bias codes for modeling speaker-adaptive DNN-based speech\n  synthesis systems", "abstract": "Most neural-network based speaker-adaptive acoustic models for speech\nsynthesis can be categorized into either layer-based or input-code approaches.\nAlthough both approaches have their own pros and cons, most existing works on\nspeaker adaptation focus on improving one or the other. In this paper, after we\nfirst systematically overview the common principles of neural-network based\nspeaker-adaptive models, we show that these approaches can be represented in a\nunified framework and can be generalized further. More specifically, we\nintroduce the use of scaling and bias codes as generalized means for\nspeaker-adaptive transformation. By utilizing these codes, we can create a more\nefficient factorized speaker-adaptive model and capture advantages of both\napproaches while reducing their disadvantages. The experiments show that the\nproposed method can improve the performance of speaker adaptation compared with\nspeaker adaptation based on the conventional input code.", "published": "2018-07-31 02:29:41", "link": "http://arxiv.org/abs/1807.11632v2", "categories": ["eess.AS", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Multi-Speaker DOA Estimation Using Deep Convolutional Networks Trained\n  with Noise Signals", "abstract": "Supervised learning based methods for source localization, being data driven,\ncan be adapted to different acoustic conditions via training and have been\nshown to be robust to adverse acoustic environments. In this paper, a\nconvolutional neural network (CNN) based supervised learning method for\nestimating the direction-of-arrival (DOA) of multiple speakers is proposed.\nMulti-speaker DOA estimation is formulated as a multi-class multi-label\nclassification problem, where the assignment of each DOA label to the input\nfeature is treated as a separate binary classification problem. The phase\ncomponent of the short-time Fourier transform (STFT) coefficients of the\nreceived microphone signals are directly fed into the CNN, and the features for\nDOA estimation are learnt during training. Utilizing the assumption of disjoint\nspeaker activity in the STFT domain, a novel method is proposed to train the\nCNN with synthesized noise signals. Through experimental evaluation with both\nsimulated and measured acoustic impulse responses, the ability of the proposed\nDOA estimation approach to adapt to unseen acoustic conditions and its\nrobustness to unseen noise type is demonstrated. Through additional empirical\ninvestigation, it is also shown that with an array of M microphones our\nproposed framework yields the best localization performance with M-1\nconvolution layers. The ability of the proposed method to accurately localize\nspeakers in a dynamic acoustic scenario with varying number of sources is also\nshown.", "published": "2018-07-31 09:40:39", "link": "http://arxiv.org/abs/1807.11722v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Lip-Reading Driven Deep Learning Approach for Speech Enhancement", "abstract": "This paper proposes a novel lip-reading driven deep learning framework for\nspeech enhancement. The proposed approach leverages the complementary strengths\nof both deep learning and analytical acoustic modelling (filtering based\napproach) as compared to recently published, comparatively simpler benchmark\napproaches that rely only on deep learning. The proposed audio-visual (AV)\nspeech enhancement framework operates at two levels. In the first level, a\nnovel deep learning-based lip-reading regression model is employed. In the\nsecond level, lip-reading approximated clean-audio features are exploited,\nusing an enhanced, visually-derived Wiener filter (EVWF), for the clean audio\npower spectrum estimation. Specifically, a stacked long-short-term memory\n(LSTM) based lip-reading regression model is designed for clean audio features\nestimation using only temporal visual features considering different number of\nprior visual frames. For clean speech spectrum estimation, a new\nfilterbank-domain EVWF is formulated, which exploits estimated speech features.\nThe proposed EVWF is compared with conventional Spectral Subtraction and\nLog-Minimum Mean-Square Error methods using both ideal AV mapping and LSTM\ndriven AV mapping. The potential of the proposed speech enhancement framework\nis evaluated under different dynamic real-world commercially-motivated\nscenarios (e.g. cafe, public transport, pedestrian area) at different SNR\nlevels (ranging from low to high SNRs) using benchmark Grid and ChiME3 corpora.\nFor objective testing, perceptual evaluation of speech quality is used to\nevaluate the quality of restored speech. For subjective testing, the standard\nmean-opinion-score method is used with inferential statistics. Comparative\nsimulation results demonstrate significant lip-reading and speech enhancement\nimprovement in terms of both speech quality and speech intelligibility.", "published": "2018-07-31 19:50:13", "link": "http://arxiv.org/abs/1808.00046v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS", "I.4; I.5; I.2"], "primary_category": "cs.CV"}
{"title": "DNN driven Speaker Independent Audio-Visual Mask Estimation for Speech\n  Separation", "abstract": "Human auditory cortex excels at selectively suppressing background noise to\nfocus on a target speaker. The process of selective attention in the brain is\nknown to contextually exploit the available audio and visual cues to better\nfocus on target speaker while filtering out other noises. In this study, we\npropose a novel deep neural network (DNN) based audiovisual (AV) mask\nestimation model. The proposed AV mask estimation model contextually integrates\nthe temporal dynamics of both audio and noise-immune visual features for\nimproved mask estimation and speech separation. For optimal AV features\nextraction and ideal binary mask (IBM) estimation, a hybrid DNN architecture is\nexploited to leverages the complementary strengths of a stacked long short term\nmemory (LSTM) and convolution LSTM network. The comparative simulation results\nin terms of speech quality and intelligibility demonstrate significant\nperformance improvement of our proposed AV mask estimation model as compared to\naudio-only and visual-only mask estimation approaches for both speaker\ndependent and independent scenarios.", "published": "2018-07-31 20:12:15", "link": "http://arxiv.org/abs/1808.00060v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS", "I.5; I.4; I.2"], "primary_category": "cs.SD"}
{"title": "Prosodic-Enhanced Siamese Convolutional Neural Networks for Cross-Device\n  Text-Independent Speaker Verification", "abstract": "In this paper a novel cross-device text-independent speaker verification\narchitecture is proposed. Majority of the state-of-the-art deep architectures\nthat are used for speaker verification tasks consider Mel-frequency cepstral\ncoefficients. In contrast, our proposed Siamese convolutional neural network\narchitecture uses Mel-frequency spectrogram coefficients to benefit from the\ndependency of the adjacent spectro-temporal features. Moreover, although\nspectro-temporal features have proved to be highly reliable in speaker\nverification models, they only represent some aspects of short-term acoustic\nlevel traits of the speaker's voice. However, the human voice consists of\nseveral linguistic levels such as acoustic, lexicon, prosody, and phonetics,\nthat can be utilized in speaker verification models. To compensate for these\ninherited shortcomings in spectro-temporal features, we propose to enhance the\nproposed Siamese convolutional neural network architecture by deploying a\nmultilayer perceptron network to incorporate the prosodic, jitter, and shimmer\nfeatures. The proposed end-to-end verification architecture performs feature\nextraction and verification simultaneously. This proposed architecture displays\nsignificant improvement over classical signal processing approaches and deep\nalgorithms for forensic cross-device speaker verification.", "published": "2018-07-31 19:21:59", "link": "http://arxiv.org/abs/1808.01026v1", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
