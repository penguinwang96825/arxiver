{"title": "Neutral evolution and turnover over centuries of English word popularity", "abstract": "Here we test Neutral models against the evolution of English word frequency\nand vocabulary at the population scale, as recorded in annual word frequencies\nfrom three centuries of English language books. Against these data, we test\nboth static and dynamic predictions of two neutral models, including the\nrelation between corpus size and vocabulary size, frequency distributions, and\nturnover within those frequency distributions. Although a commonly used Neutral\nmodel fails to replicate all these emergent properties at once, we find that\nmodified two-stage Neutral model does replicate the static and dynamic\nproperties of the corpus data. This two-stage model is meant to represent a\nrelatively small corpus (population) of English books, analogous to a `canon',\nsampled by an exponentially increasing corpus of books in the wider population\nof authors. More broadly, this mode -- a smaller neutral model within a larger\nneutral model -- could represent more broadly those situations where mass\nattention is focused on a small subset of the cultural variants.", "published": "2017-03-30 21:57:37", "link": "http://arxiv.org/abs/1703.10698v1", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Finding News Citations for Wikipedia", "abstract": "An important editing policy in Wikipedia is to provide citations for added\nstatements in Wikipedia pages, where statements can be arbitrary pieces of\ntext, ranging from a sentence to a paragraph. In many cases citations are\neither outdated or missing altogether.\n  In this work we address the problem of finding and updating news citations\nfor statements in entity pages. We propose a two-stage supervised approach for\nthis problem. In the first step, we construct a classifier to find out whether\nstatements need a news citation or other kinds of citations (web, book,\njournal, etc.). In the second step, we develop a news citation algorithm for\nWikipedia statements, which recommends appropriate citations from a given news\ncollection. Apart from IR techniques that use the statement to query the news\ncollection, we also formalize three properties of an appropriate citation,\nnamely: (i) the citation should entail the Wikipedia statement, (ii) the\nstatement should be central to the citation, and (iii) the citation should be\nfrom an authoritative source.\n  We perform an extensive evaluation of both steps, using 20 million articles\nfrom a real-world news collection. Our results are quite promising, and show\nthat we can perform this task with high precision and at scale.", "published": "2017-03-30 07:48:31", "link": "http://arxiv.org/abs/1703.10339v2", "categories": ["cs.IR", "cs.CL", "cs.SI"], "primary_category": "cs.IR"}
{"title": "Automated News Suggestions for Populating Wikipedia Entity Pages", "abstract": "Wikipedia entity pages are a valuable source of information for direct\nconsumption and for knowledge-base construction, update and maintenance. Facts\nin these entity pages are typically supported by references. Recent studies\nshow that as much as 20\\% of the references are from online news sources.\nHowever, many entity pages are incomplete even if relevant information is\nalready available in existing news articles. Even for the already present\nreferences, there is often a delay between the news article publication time\nand the reference time. In this work, we therefore look at Wikipedia through\nthe lens of news and propose a novel news-article suggestion task to improve\nnews coverage in Wikipedia, and reduce the lag of newsworthy references. Our\nwork finds direct application, as a precursor, to Wikipedia page generation and\nknowledge-base acceleration tasks that rely on relevant and high quality input\nsources.\n  We propose a two-stage supervised approach for suggesting news articles to\nentity pages for a given state of Wikipedia. First, we suggest news articles to\nWikipedia entities (article-entity placement) relying on a rich set of features\nwhich take into account the \\emph{salience} and \\emph{relative authority} of\nentities, and the \\emph{novelty} of news articles to entity pages. Second, we\ndetermine the exact section in the entity page for the input article\n(article-section placement) guided by class-based section templates. We perform\nan extensive evaluation of our approach based on ground-truth data that is\nextracted from external references in Wikipedia. We achieve a high precision\nvalue of up to 93\\% in the \\emph{article-entity} suggestion stage and upto 84\\%\nfor the \\emph{article-section placement}. Finally, we compare our approach\nagainst competitive baselines and show significant improvements.", "published": "2017-03-30 07:56:42", "link": "http://arxiv.org/abs/1703.10344v1", "categories": ["cs.IR", "cs.CL", "cs.SI"], "primary_category": "cs.IR"}
{"title": "Simplified End-to-End MMI Training and Voting for ASR", "abstract": "A simplified speech recognition system that uses the maximum mutual\ninformation (MMI) criterion is considered. End-to-end training using gradient\ndescent is suggested, similarly to the training of connectionist temporal\nclassification (CTC). We use an MMI criterion with a simple language model in\nthe training stage, and a standard HMM decoder. Our method compares favorably\nto CTC in terms of performance, robustness, decoding time, disk footprint and\nquality of alignments. The good alignments enable the use of a straightforward\nensemble method, obtained by simply averaging the predictions of several neural\nnetwork models, that were trained separately end-to-end. The ensemble method\nyields a considerable reduction in the word error rate.", "published": "2017-03-30 08:40:19", "link": "http://arxiv.org/abs/1703.10356v2", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Speaking the Same Language: Matching Machine to Human Captions by\n  Adversarial Training", "abstract": "While strong progress has been made in image captioning over the last years,\nmachine and human captions are still quite distinct. A closer look reveals that\nthis is due to the deficiencies in the generated word distribution, vocabulary\nsize, and strong bias in the generators towards frequent captions. Furthermore,\nhumans -- rightfully so -- generate multiple, diverse captions, due to the\ninherent ambiguity in the captioning task which is not considered in today's\nsystems.\n  To address these challenges, we change the training objective of the caption\ngenerator from reproducing groundtruth captions to generating a set of captions\nthat is indistinguishable from human generated captions. Instead of\nhandcrafting such a learning target, we employ adversarial training in\ncombination with an approximate Gumbel sampler to implicitly match the\ngenerated distribution to the human one. While our method achieves comparable\nperformance to the state-of-the-art in terms of the correctness of the\ncaptions, we generate a set of diverse captions, that are significantly less\nbiased and match the word statistics better in several aspects.", "published": "2017-03-30 13:54:51", "link": "http://arxiv.org/abs/1703.10476v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
