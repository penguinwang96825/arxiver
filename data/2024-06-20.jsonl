{"title": "MacroHFT: Memory Augmented Context-aware Reinforcement Learning On High Frequency Trading", "abstract": "High-frequency trading (HFT) that executes algorithmic trading in short time\nscales, has recently occupied the majority of cryptocurrency market. Besides\ntraditional quantitative trading methods, reinforcement learning (RL) has\nbecome another appealing approach for HFT due to its terrific ability of\nhandling high-dimensional financial data and solving sophisticated sequential\ndecision-making problems, \\emph{e.g.,} hierarchical reinforcement learning\n(HRL) has shown its promising performance on second-level HFT by training a\nrouter to select only one sub-agent from the agent pool to execute the current\ntransaction. However, existing RL methods for HFT still have some defects: 1)\nstandard RL-based trading agents suffer from the overfitting issue, preventing\nthem from making effective policy adjustments based on financial context; 2)\ndue to the rapid changes in market conditions, investment decisions made by an\nindividual agent are usually one-sided and highly biased, which might lead to\nsignificant loss in extreme markets. To tackle these problems, we propose a\nnovel Memory Augmented Context-aware Reinforcement learning method On HFT,\n\\emph{a.k.a.} MacroHFT, which consists of two training phases: 1) we first\ntrain multiple types of sub-agents with the market data decomposed according to\nvarious financial indicators, specifically market trend and volatility, where\neach agent owns a conditional adapter to adjust its trading policy according to\nmarket conditions; 2) then we train a hyper-agent to mix the decisions from\nthese sub-agents and output a consistently profitable meta-policy to handle\nrapid market fluctuations, equipped with a memory mechanism to enhance the\ncapability of decision-making. Extensive experiments on various cryptocurrency\nmarkets demonstrate that MacroHFT can achieve state-of-the-art performance on\nminute-level trading tasks.", "published": "2024-06-20 17:48:24", "link": "http://arxiv.org/abs/2406.14537v1", "categories": ["cs.LG", "q-fin.TR"], "primary_category": "cs.LG"}
{"title": "Persuasiveness of Generated Free-Text Rationales in Subjective\n  Decisions: A Case Study on Pairwise Argument Ranking", "abstract": "Generating free-text rationales is among the emergent capabilities of Large\nLanguage Models (LLMs). These rationales have been found to enhance LLM\nperformance across various NLP tasks. Recently, there has been growing interest\nin using these rationales to provide insights for various important downstream\ntasks. In this paper, we analyze generated free-text rationales in tasks with\nsubjective answers, emphasizing the importance of rationalization in such\nscenarios. We focus on pairwise argument ranking, a highly subjective task with\nsignificant potential for real-world applications, such as debate assistance.\nWe evaluate the persuasiveness of rationales generated by nine LLMs to support\ntheir subjective choices. Our findings suggest that open-source LLMs,\nparticularly Llama2-70B-chat, are capable of providing highly persuasive\nrationalizations, surpassing even GPT models. Additionally, our experiments\nshow that rationale persuasiveness can be improved by controlling its\nparameters through prompting or through self-refinement.", "published": "2024-06-20 00:28:33", "link": "http://arxiv.org/abs/2406.13905v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AutoCAP: Towards Automatic Cross-lingual Alignment Planning for\n  Zero-shot Chain-of-Thought", "abstract": "Cross-lingual chain-of-thought can effectively complete reasoning tasks\nacross languages, which gains increasing attention. Recently, dominant\napproaches in the literature improve cross-lingual alignment capabilities by\nintegrating reasoning knowledge from different languages. Despite achieving\nexcellent performance, current methods still have two main challenges: (1)\nManual language specification: They still highly rely on manually selecting the\nlanguages to integrate, severely affecting their generalizability; (2) Static\nweight allocation: Current methods simply integrate all languages equally. In\nfact, different language reasoning paths should have different weights to\nachieve better complementation and integration. Motivated by this, we introduce\nan Automatic Cross-lingual Alignment Planning (AutoCAP) for zero-shot\nchain-of-thought to address the above challenges. The core of AutoCAP consists\nof two components: (1) Automatic Language Selection Prompting to guide LLMs to\nselect appropriate languages and (2) Automatic Weight Allocation Prompting to\nautomatically allocate alignment weight scores to each reasoning path.\nExtensive experiments on several benchmarks reveal that AutoCAP achieves\nstate-of-the-art performance, surpassing previous methods that required manual\neffort.", "published": "2024-06-20 02:19:33", "link": "http://arxiv.org/abs/2406.13940v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large\n  Language Model Evaluation", "abstract": "The training process of large language models (LLMs) often involves varying\ndegrees of test data contamination. Although current LLMs are achieving\nincreasingly better performance on various benchmarks, their performance in\npractical applications does not always match their benchmark results. Leakage\nof benchmarks can prevent the accurate assessment of LLMs' true performance.\nHowever, constructing new benchmarks is costly, labor-intensive and still\ncarries the risk of leakage. Therefore, in this paper, we ask the question, Can\nwe reuse these leaked benchmarks for LLM evaluation? We propose Inference-Time\nDecontamination (ITD) to address this issue by detecting and rewriting leaked\nsamples without altering their difficulties. ITD can mitigate performance\ninflation caused by memorizing leaked benchmarks. Our proof-of-concept\nexperiments demonstrate that ITD reduces inflated accuracy by 22.9% on GSM8K\nand 19.0% on MMLU. On MMLU, using Inference-time Decontamination can lead to a\ndecrease in the results of Phi3 and Mistral by 6.7% and 3.6% respectively. We\nhope that ITD can provide more truthful evaluation results for large language\nmodels.", "published": "2024-06-20 04:35:59", "link": "http://arxiv.org/abs/2406.13990v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM Critics Help Catch Bugs in Mathematics: Towards a Better\n  Mathematical Verifier with Natural Language Feedback", "abstract": "In recent progress, mathematical verifiers have achieved success in\nmathematical reasoning tasks by validating the correctness of solutions\ngenerated by policy models. However, existing verifiers are trained with binary\nclassification labels, which are not informative enough for the model to\naccurately assess the solutions. To mitigate the aforementioned insufficiency\nof binary labels, we introduce step-wise natural language feedback as rationale\nlabels, that is, the correctness of each step and the detailed explanations. In\nthis paper, we propose Math-Minos, a natural language feedback-enhanced\nverifier by constructing automatically generated training data and a two-stage\ntraining paradigm for effective training and efficient inference. Our\nexperiments reveal that a small set of natural language feedback can\nsignificantly boost the performance of the verifier in both verification and\nreinforcement learning. We have released the code and data for further\nexploration.", "published": "2024-06-20 06:42:27", "link": "http://arxiv.org/abs/2406.14024v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt Injection Attacks in Defended Systems", "abstract": "Large language models play a crucial role in modern natural language\nprocessing technologies. However, their extensive use also introduces potential\nsecurity risks, such as the possibility of black-box attacks. These attacks can\nembed hidden malicious features into the model, leading to adverse consequences\nduring its deployment.\n  This paper investigates methods for black-box attacks on large language\nmodels with a three-tiered defense mechanism. It analyzes the challenges and\nsignificance of these attacks, highlighting their potential implications for\nlanguage processing system security. Existing attack and defense methods are\nexamined, evaluating their effectiveness and applicability across various\nscenarios.\n  Special attention is given to the detection algorithm for black-box attacks,\nidentifying hazardous vulnerabilities in language models and retrieving\nsensitive information. This research presents a methodology for vulnerability\ndetection and the development of defensive strategies against black-box attacks\non large language models.", "published": "2024-06-20 07:13:25", "link": "http://arxiv.org/abs/2406.14048v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EXCEEDS: Extracting Complex Events as Connecting the Dots to Graphs in\n  Scientific Domain", "abstract": "It is crucial to utilize events to understand a specific domain. There are\nlots of research on event extraction in many domains such as news, finance and\nbiology domain. However, scientific domain still lacks event extraction\nresearch, including comprehensive datasets and corresponding methods. Compared\nto other domains, scientific domain presents two characteristics: denser\nnuggets and more complex events. To solve the above problem, considering these\ntwo characteristics, we first construct SciEvents, a large-scale multi-event\ndocument-level dataset with a schema tailored for scientific domain. It has\n2,508 documents and 24,381 events under refined annotation and quality control.\nThen, we propose EXCEEDS, a novel end-to-end scientific event extraction\nframework by storing dense nuggets in a grid matrix and simplifying complex\nevent extraction into a dot construction and connection task. Experimental\nresults demonstrate state-of-the-art performances of EXCEEDS on SciEvents.\nAdditionally, we release SciEvents and EXCEEDS on GitHub.", "published": "2024-06-20 07:50:37", "link": "http://arxiv.org/abs/2406.14075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Protecting Privacy Through Approximating Optimal Parameters for Sequence\n  Unlearning in Language Models", "abstract": "Although language models (LMs) demonstrate exceptional capabilities on\nvarious tasks, they are potentially vulnerable to extraction attacks, which\nrepresent a significant privacy risk. To mitigate the privacy concerns of LMs,\nmachine unlearning has emerged as an important research area, which is utilized\nto induce the LM to selectively forget about some of its training data. While\ncompletely retraining the model will guarantee successful unlearning and\nprivacy assurance, it is impractical for LMs, as it would be time-consuming and\nresource-intensive. Prior works efficiently unlearn the target token sequences,\nbut upon subsequent iterations, the LM displays significant degradation in\nperformance. In this work, we propose Privacy Protection via Optimal Parameters\n(POP), a novel unlearning method that effectively forgets the target token\nsequences from the pretrained LM by applying optimal gradient updates to the\nparameters. Inspired by the gradient derivation of complete retraining, we\napproximate the optimal training objective that successfully unlearns the\ntarget sequence while retaining the knowledge from the rest of the training\ndata. Experimental results demonstrate that POP exhibits remarkable retention\nperformance post-unlearning across 9 classification and 4 dialogue benchmarks,\noutperforming the state-of-the-art by a large margin. Furthermore, we introduce\nRemnant Memorization Accuracy that quantifies privacy risks based on token\nlikelihood and validate its effectiveness through both qualitative and\nquantitative analyses.", "published": "2024-06-20 08:12:49", "link": "http://arxiv.org/abs/2406.14091v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Let Guidelines Guide You: A Prescriptive Guideline-Centered Data\n  Annotation Methodology", "abstract": "We introduce the Guideline-Centered Annotation Methodology (GCAM), a novel\ndata annotation methodology designed to report the annotation guidelines\nassociated with each data sample. Our approach addresses three key limitations\nof the standard prescriptive annotation methodology by reducing the information\nloss during annotation and ensuring adherence to guidelines. Furthermore, GCAM\nenables the efficient reuse of annotated data across multiple tasks. We\nevaluate GCAM in two ways: (i) through a human annotation study and (ii) an\nexperimental evaluation with several machine learning models. Our results\nhighlight the advantages of GCAM from multiple perspectives, demonstrating its\npotential to improve annotation quality and error analysis.", "published": "2024-06-20 08:24:57", "link": "http://arxiv.org/abs/2406.14099v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Take the essence and discard the dross: A Rethinking on Data Selection\n  for Fine-Tuning Large Language Models", "abstract": "Data selection for fine-tuning large language models (LLMs) aims to choose a\nhigh-quality subset from existing datasets, allowing the trained model to\noutperform baselines trained on the full dataset. However, the expanding body\nof research lacks a clear, unified framework, and the variability in\nexperimental settings complicates systematic comparisons. While existing\nsurveys comprehensively overview the stages and methods of data selection, they\noften overlook an in-depth exploration of the fine-tuning phase. In this paper,\nwe conduct a focused review of recent data selection techniques for fine-tuning\nLLMs, analyzing a dozen key studies. We introduce a novel three-stage scheme -\ncomprising feature extraction, criteria design, and selector evaluation - to\nsystematically categorize and evaluate these methods. Additionally, we propose\na unified comparison approach that incorporates ratio-based efficiency and\nranking-based feasibility metrics to address inconsistencies across\nexperiments. Our findings reveal that methods emphasizing more targeted quality\nmeasurement achieve higher efficiency but at the cost of feasibility. Finally,\nwe discuss trends and highlight four key challenges in fine-tuning data\nselection, offering potential directions for future research.", "published": "2024-06-20 08:58:58", "link": "http://arxiv.org/abs/2406.14115v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MACAROON: Training Vision-Language Models To Be Your Engaged Partners", "abstract": "Large vision-language models (LVLMs), while proficient in following\ninstructions and responding to diverse questions, invariably generate detailed\nresponses even when questions are ambiguous or unanswerable, leading to\nhallucinations and bias issues. Thus, it is essential for LVLMs to proactively\nengage with humans to ask for clarifications or additional information for\nbetter responses. In this study, we aim to shift LVLMs from passive answer\nproviders to proactive engaged partners. We begin by establishing a\nthree-tiered hierarchy for questions of invalid, ambiguous, and personalizable\nnature to measure the proactive engagement capabilities of LVLMs. Utilizing\nthis hierarchy, we create PIE, (ProactIve Engagement Evaluation) through GPT-4o\nand human annotators, consisting of 853 questions across six distinct,\nfine-grained question types that are verified by human annotators and\naccompanied with well-defined metrics. Our evaluations on \\benchmark indicate\npoor performance of existing LVLMs, with the best-performing open-weights model\nonly achieving an Aggregate Align Rate (AAR) of 0.28. In response, we introduce\nMACAROON, self-iMaginAtion for ContrAstive pReference OptimizatiON, which\ninstructs LVLMs to autonomously generate contrastive response pairs for\nunlabeled questions given the task description and human-crafted criteria.\nThen, the self-imagined data is formatted for conditional reinforcement\nlearning. Experimental results show MACAROON effectively improves LVLMs'\ncapabilities to be proactively engaged (0.84 AAR) while maintaining comparable\nperformance on general tasks.", "published": "2024-06-20 09:27:33", "link": "http://arxiv.org/abs/2406.14137v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aligning Large Language Models with Diverse Political Viewpoints", "abstract": "Large language models such as ChatGPT exhibit striking political biases. If\nusers query them about political information, they often take a normative\nstance. To overcome this, we align LLMs with diverse political viewpoints from\n100,000 comments written by candidates running for national parliament in\nSwitzerland. Models aligned with this data can generate more accurate political\nviewpoints from Swiss parties, compared to commercial models such as ChatGPT.\nWe also propose a procedure to generate balanced overviews summarizing multiple\nviewpoints using such models. The replication package contains all code and\ndata.", "published": "2024-06-20 09:53:23", "link": "http://arxiv.org/abs/2406.14155v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Definition generation for lexical semantic change detection", "abstract": "We use contextualized word definitions generated by large language models as\nsemantic representations in the task of diachronic lexical semantic change\ndetection (LSCD). In short, generated definitions are used as `senses', and the\nchange score of a target word is retrieved by comparing their distributions in\ntwo time periods under comparison. On the material of five datasets and three\nlanguages, we show that generated definitions are indeed specific and general\nenough to convey a signal sufficient to rank sets of words by the degree of\ntheir semantic change over time. Our approach is on par with or outperforms\nprior non-supervised sense-based LSCD methods. At the same time, it preserves\ninterpretability and allows to inspect the reasons behind a specific shift in\nterms of discrete definitions-as-senses. This is another step in the direction\nof explainable semantic change modeling.", "published": "2024-06-20 10:13:08", "link": "http://arxiv.org/abs/2406.14167v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In Tree Structure Should Sentence Be Generated", "abstract": "Generative models reliant on sequential autoregression have been at the\nforefront of language generation for an extensive period, particularly\nfollowing the introduction of widely acclaimed transformers. Despite its\nexcellent performance, there are always some issues that we face today. For\nexample, problems such as hallucinations and getting trapped in a logic loop\nmay occur. To enhance the performance of existing systems, this paper\nintroduces a new method for generating sequences in natural language, which\ninvolves generating the targeted sentence in a tree-traversing order. The paper\nincludes an illustration of the theoretical basis and validity of the approach,\nas well as a comparison of its fundamentals with the diffusion model in graphic\ngeneration. Finally, a module called SenTree is introduced for generating an\napproximating binary tree. It is already available at\nhttps://github.com/arklyg/sentree. Additionally, a joint training framework\nbased on this approach is proposed, incorporating the intrinsics of generative\nadversarial networks.", "published": "2024-06-20 10:49:08", "link": "http://arxiv.org/abs/2406.14189v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Complexity of Symbolic Representation in Working Memory of Transformer\n  Correlates with the Complexity of a Task", "abstract": "Even though Transformers are extensively used for Natural Language Processing\ntasks, especially for machine translation, they lack an explicit memory to\nstore key concepts of processed texts. This paper explores the properties of\nthe content of symbolic working memory added to the Transformer model decoder.\nSuch working memory enhances the quality of model predictions in machine\ntranslation task and works as a neural-symbolic representation of information\nthat is important for the model to make correct translations. The study of\nmemory content revealed that translated text keywords are stored in the working\nmemory, pointing to the relevance of memory content to the processed text.\nAlso, the diversity of tokens and parts of speech stored in memory correlates\nwith the complexity of the corpora for machine translation task.", "published": "2024-06-20 11:27:29", "link": "http://arxiv.org/abs/2406.14213v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced\n  Clinical Diagnosis on EMRs", "abstract": "Electronic Medical Records (EMRs), while integral to modern healthcare,\npresent challenges for clinical reasoning and diagnosis due to their complexity\nand information redundancy. To address this, we proposed medIKAL (Integrating\nKnowledge Graphs as Assistants of LLMs), a framework that combines Large\nLanguage Models (LLMs) with knowledge graphs (KGs) to enhance diagnostic\ncapabilities. medIKAL assigns weighted importance to entities in medical\nrecords based on their type, enabling precise localization of candidate\ndiseases within KGs. It innovatively employs a residual network-like approach,\nallowing initial diagnosis by the LLM to be merged into KG search results.\nThrough a path-based reranking algorithm and a fill-in-the-blank style prompt\ntemplate, it further refined the diagnostic process. We validated medIKAL's\neffectiveness through extensive experiments on a newly introduced open-sourced\nChinese EMR dataset, demonstrating its potential to improve clinical diagnosis\nin real-world settings.", "published": "2024-06-20 13:56:52", "link": "http://arxiv.org/abs/2406.14326v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SEC-QA: A Systematic Evaluation Corpus for Financial QA", "abstract": "The financial domain frequently deals with large numbers of long documents\nthat are essential for daily operations. Significant effort is put towards\nautomating financial data analysis. However, a persistent challenge, not\nlimited to the finance domain, is the scarcity of datasets that accurately\nreflect real-world tasks for model evaluation. Existing datasets are often\nconstrained by size, context, or relevance to practical applications. Moreover,\nLLMs are currently trained on trillions of tokens of text, limiting access to\nnovel data or documents that models have not encountered during training for\nunbiased evaluation. We propose SEC-QA, a continuous dataset generation\nframework with two key features: 1) the semi-automatic generation of\nQuestion-Answer (QA) pairs spanning multiple long context financial documents,\nwhich better represent real-world financial scenarios; 2) the ability to\ncontinually refresh the dataset using the most recent public document\ncollections, not yet ingested by LLMs. Our experiments show that current\nretrieval augmented generation methods systematically fail to answer these\nchallenging multi-document questions. In response, we introduce a QA system\nbased on program-of-thought that improves the ability to perform complex\ninformation retrieval and quantitative reasoning pipelines, thereby increasing\nQA accuracy.", "published": "2024-06-20 15:12:41", "link": "http://arxiv.org/abs/2406.14394v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Selected Languages are All You Need for Cross-lingual Truthfulness\n  Transfer", "abstract": "Truthfulness stands out as an essential challenge for Large Language Models\n(LLMs). Although many works have developed various ways for truthfulness\nenhancement, they seldom focus on truthfulness in multilingual scenarios.\nMeanwhile, contemporary multilingual aligning technologies struggle to balance\nnumerous languages and often exhibit serious truthfulness gaps across different\nlanguages, especially those that differ greatly from English. In our work, we\nextend truthfulness evaluation to multilingual contexts and propose a practical\nmethod for cross-lingual truthfulness transfer called Fact-aware Multilingual\nSelective Synergy (FaMSS). FaMSS is able to select an optimal subset of all\ntested languages by language bias and transfer contributions, and then employ\ntranslation instruction tuning for cross-lingual truthfulness transfer.\nExperimental results demonstrate that our approach can effectively reduce the\nmultilingual representation disparity and boost cross-lingual truthfulness\ntransfer of LLMs.", "published": "2024-06-20 15:59:07", "link": "http://arxiv.org/abs/2406.14434v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Healing Powers of BERT: How Task-Specific Fine-Tuning Recovers Corrupted\n  Language Models", "abstract": "Language models like BERT excel at sentence classification tasks due to\nextensive pre-training on general data, but their robustness to parameter\ncorruption is unexplored. To understand this better, we look at what happens if\na language model is \"broken\", in the sense that some of its parameters are\ncorrupted and then recovered by fine-tuning. Strategically corrupting BERT\nvariants at different levels, we find corrupted models struggle to fully\nrecover their original performance, with higher corruption causing more severe\ndegradation. Notably, bottom-layer corruption affecting fundamental linguistic\nfeatures is more detrimental than top-layer corruption. Our insights contribute\nto understanding language model robustness and adaptability under adverse\nconditions, informing strategies for developing resilient NLP systems against\nparameter perturbations.", "published": "2024-06-20 16:18:04", "link": "http://arxiv.org/abs/2406.14459v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Human Subjectivity in LLMs Using Explicit and Implicit Human\n  Factors in Personas", "abstract": "Large language models (LLMs) are increasingly being used in human-centered\nsocial scientific tasks, such as data annotation, synthetic data creation, and\nengaging in dialog. However, these tasks are highly subjective and dependent on\nhuman factors, such as one's environment, attitudes, beliefs, and lived\nexperiences. Thus, it may be the case that employing LLMs (which do not have\nsuch human factors) in these tasks results in a lack of variation in data,\nfailing to reflect the diversity of human experiences. In this paper, we\nexamine the role of prompting LLMs with human-like personas and asking the\nmodels to answer as if they were a specific human. This is done explicitly,\nwith exact demographics, political beliefs, and lived experiences, or\nimplicitly via names prevalent in specific populations. The LLM personas are\nthen evaluated via (1) subjective annotation task (e.g., detecting toxicity)\nand (2) a belief generation task, where both tasks are known to vary across\nhuman factors. We examine the impact of explicit vs. implicit personas and\ninvestigate which human factors LLMs recognize and respond to. Results show\nthat explicit LLM personas show mixed results when reproducing known human\nbiases, but generally fail to demonstrate implicit biases. We conclude that\nLLMs may capture the statistical patterns of how people speak, but are\ngenerally unable to model the complex interactions and subtleties of human\nperceptions, potentially limiting their effectiveness in social science\napplications.", "published": "2024-06-20 16:24:07", "link": "http://arxiv.org/abs/2406.14462v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Instruction Pre-Training: Language Models are Supervised Multitask\n  Learners", "abstract": "Unsupervised multitask pre-training has been the critical method behind the\nrecent success of language models (LMs). However, supervised multitask learning\nstill holds significant promise, as scaling it in the post-training stage\ntrends towards better generalization. In this paper, we explore supervised\nmultitask pre-training by proposing Instruction Pre-Training, a framework that\nscalably augments massive raw corpora with instruction-response pairs to\npre-train LMs. The instruction-response pairs are generated by an efficient\ninstruction synthesizer built on open-source models. In our experiments, we\nsynthesize 200M instruction-response pairs covering 40+ task categories to\nverify the effectiveness of Instruction Pre-Training. In pre-training from\nscratch, Instruction Pre-Training not only consistently enhances pre-trained\nbase models but also benefits more from further instruction tuning. In\ncontinual pre-training, Instruction Pre-Training enables Llama3-8B to be\ncomparable to or even outperform Llama3-70B. Our model, code, and data are\navailable at https://github.com/microsoft/LMOps.", "published": "2024-06-20 16:55:33", "link": "http://arxiv.org/abs/2406.14491v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLaSA: A Multimodal LLM for Human Activity Analysis Through Wearable and\n  Smartphone Sensors", "abstract": "Integrating inertial measurement units (IMUs) with large language models\n(LLMs) expands the potential of multimodal AI, enabling more nuanced human\nactivity analysis. In this paper, we introduce LLaSA (Large Language and Sensor\nAssistant), a multimodal large language model built on LIMU-BERT and Llama,\ndesigned to interpret and answer queries related to human activities and motion\nanalysis, leveraging sensor data and contextual reasoning. To develop LLaSA, we\nintroduce two key datasets: SensorCaps, a comprehensive collection of 35,960\nIMU-derived narratives with handcrafted features, and OpenSQA, an\ninstruction-following dataset containing 179,727 question-answer pairs aware of\nthe sensor and human activity context. These datasets provide diverse and rich\ninputs to train LLaSA for complex sensor-based queries. To optimize LLaSA's\nperformance, we apply a unique hyperparameter tuning method, which\nsignificantly enhances its effectiveness in contextual question-answering\ntasks. Extensive evaluations, including a human-led assessment of the\nquestion-answering, demonstrate that LLaSA achieves superior data\ninterpretation and context-aware responses compared to GPT-3.5-Turbo and\nVicuna-1.5-13b-16K. These contributions advance the frontier of sensor-aware\nLLMs and create new opportunities for impactful multimodal research in\nhealthcare, sports science, and human-computer interactions. Our code\nrepository and datasets can be found at https://github.com/BASHLab/LLaSA.", "published": "2024-06-20 17:00:34", "link": "http://arxiv.org/abs/2406.14498v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Expert Radiology Report Summarization by Prompting Large\n  Language Models with a Layperson Summary", "abstract": "Radiology report summarization (RRS) is crucial for patient care, requiring\nconcise \"Impressions\" from detailed \"Findings.\" This paper introduces a novel\nprompting strategy to enhance RRS by first generating a layperson summary. This\napproach normalizes key observations and simplifies complex information using\nnon-expert communication techniques inspired by doctor-patient interactions.\nCombined with few-shot in-context learning, this method improves the model's\nability to link general terms to specific findings. We evaluate this approach\non the MIMIC-CXR, CheXpert, and MIMIC-III datasets, benchmarking it against\n7B/8B parameter state-of-the-art open-source large language models (LLMs) like\nMeta-Llama-3-8B-Instruct. Our results demonstrate improvements in summarization\naccuracy and accessibility, particularly in out-of-domain tests, with\nimprovements as high as 5% for some metrics.", "published": "2024-06-20 17:01:55", "link": "http://arxiv.org/abs/2406.14500v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overview of the CAIL 2023 Argument Mining Track", "abstract": "We give a detailed overview of the CAIL 2023 Argument Mining Track, one of\nthe Chinese AI and Law Challenge (CAIL) 2023 tracks. The main goal of the track\nis to identify and extract interacting argument pairs in trial dialogs. It\nmainly uses summarized judgment documents but can also refer to trial\nrecordings. The track consists of two stages, and we introduce the tasks\ndesigned for each stage; we also extend the data from previous events into a\nnew dataset -- CAIL2023-ArgMine -- with annotated new cases from various causes\nof action. We outline several submissions that achieve the best results,\nincluding their methods for different stages. While all submissions rely on\nlanguage models, they have incorporated strategies that may benefit future work\nin this field.", "published": "2024-06-20 17:06:13", "link": "http://arxiv.org/abs/2406.14503v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Translating Across Cultures: LLMs for Intralingual Cultural Adaptation", "abstract": "LLMs are increasingly being deployed for multilingual applications and have\ndemonstrated impressive translation capabilities between several low and\nhigh-resource languages. An aspect of translation that often gets overlooked is\nthat of cultural adaptation, or modifying source culture references to suit the\ntarget culture. While specialized translation models still outperform LLMs on\nthe machine translation task when viewed from the lens of correctness, they are\nnot sensitive to cultural differences often requiring manual correction. LLMs\non the other hand have a rich reservoir of cultural knowledge embedded within\nits parameters that can be potentially exploited for such applications. In this\npaper, we define the task of cultural adaptation and create an evaluation\nframework to evaluate the performance of modern LLMs for cultural adaptation\nand analyze their cross-cultural knowledge while connecting related concepts\nacross different cultures. We also analyze possible issues with automatic\nadaptation. We hope that this task will offer more insight into the cultural\nunderstanding of LLMs and their creativity in cross-cultural scenarios.", "published": "2024-06-20 17:06:58", "link": "http://arxiv.org/abs/2406.14504v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Mysteries of CoT-Augmented Distillation", "abstract": "Eliciting \"chain of thought\" (CoT) rationales -- sequences of token that\nconvey a \"reasoning\" process -- has been shown to consistently improve LLM\nperformance on tasks like question answering. More recent efforts have shown\nthat such rationales can also be used for model distillation: Including CoT\nsequences (elicited from a large \"teacher\" model) in addition to target labels\nwhen fine-tuning a small student model yields (often substantial) improvements.\nIn this work we ask: Why and how does this additional training signal help in\nmodel distillation? We perform ablations to interrogate this, and report some\npotentially surprising results. Specifically: (1) Placing CoT sequences after\nlabels (rather than before) realizes consistently better downstream performance\n-- this means that no student \"reasoning\" is necessary at test time to realize\ngains. (2) When rationales are appended in this way, they need not be coherent\nreasoning sequences to yield improvements; performance increases are robust to\npermutations of CoT tokens, for example. In fact, (3) a small number of key\ntokens are sufficient to achieve improvements equivalent to those observed when\nfull rationales are used in model distillation.", "published": "2024-06-20 17:15:46", "link": "http://arxiv.org/abs/2406.14511v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference\n  Attacks in Text-to-SQL Systems", "abstract": "Text-to-SQL systems empower users to interact with databases using natural\nlanguage, automatically translating queries into executable SQL code. However,\ntheir reliance on database schema information for SQL generation exposes them\nto significant security vulnerabilities, particularly schema inference attacks\nthat can lead to unauthorized data access or manipulation. In this paper, we\nintroduce a novel zero-knowledge framework for reconstructing the underlying\ndatabase schema of text-to-SQL models without any prior knowledge of the\ndatabase. Our approach systematically probes text-to-SQL models with specially\ncrafted questions and leverages a surrogate GPT-4 model to interpret the\noutputs, effectively uncovering hidden schema elements -- including tables,\ncolumns, and data types. We demonstrate that our method achieves high accuracy\nin reconstructing table names, with F1 scores of up to .99 for generative\nmodels and .78 for fine-tuned models, underscoring the severity of schema\nleakage risks. Furthermore, we propose a simple protection mechanism for\ngenerative models and empirically show its limitations in mitigating these\nattacks.", "published": "2024-06-20 17:54:33", "link": "http://arxiv.org/abs/2406.14545v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned\n  MT Evaluation Metrics", "abstract": "State-of-the-art trainable machine translation evaluation metrics like xCOMET\nachieve high correlation with human judgment but rely on large encoders (up to\n10.7B parameters), making them computationally expensive and inaccessible to\nresearchers with limited resources. To address this issue, we investigate\nwhether the knowledge stored in these large encoders can be compressed while\nmaintaining quality. We employ distillation, quantization, and pruning\ntechniques to create efficient xCOMET alternatives and introduce a novel data\ncollection pipeline for efficient black-box distillation. Our experiments show\nthat, using quantization, xCOMET can be compressed up to three times with no\nquality degradation. Additionally, through distillation, we create an\n278M-sized xCOMET-lite metric, which has only 2.6% of xCOMET-XXL parameters,\nbut retains 92.1% of its quality. Besides, it surpasses strong small-scale\nmetrics like COMET-22 and BLEURT-20 on the WMT22 metrics challenge dataset by\n6.4%, despite using 50% fewer parameters. All code, dataset, and models are\navailable online at https://github.com/NL2G/xCOMET-lite.", "published": "2024-06-20 17:58:34", "link": "http://arxiv.org/abs/2406.14553v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Compute the Probability of a Word", "abstract": "Language models (LMs) estimate a probability distribution over strings in a\nnatural language; these distributions are crucial for computing perplexity and\nsurprisal in linguistics research. While we are usually concerned with\nmeasuring these values for words, most LMs operate over subwords. Despite\nseemingly straightforward, accurately computing probabilities over one unit\ngiven probabilities over the other requires care. Indeed, we show here that\nmany recent linguistic studies have been incorrectly computing these values.\nThis paper derives the correct methods for computing word probabilities,\nhighlighting issues when relying on language models that use beginning-of-word\n(bow)-marking tokenisers, e.g., the GPT family. Empirically, we show that\ncorrecting the widespread bug in probability computations affects measured\noutcomes in sentence comprehension and lexical optimisation analyses.", "published": "2024-06-20 17:59:42", "link": "http://arxiv.org/abs/2406.14561v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling the Spectrum of Data Contamination in Language Models: A\n  Survey from Detection to Remediation", "abstract": "Data contamination has garnered increased attention in the era of large\nlanguage models (LLMs) due to the reliance on extensive internet-derived\ntraining corpora. The issue of training corpus overlap with evaluation\nbenchmarks--referred to as contamination--has been the focus of significant\nrecent research. This body of work aims to identify contamination, understand\nits impacts, and explore mitigation strategies from diverse perspectives.\nHowever, comprehensive studies that provide a clear pathway from foundational\nconcepts to advanced insights are lacking in this nascent field. Therefore, we\npresent a comprehensive survey in the field of data contamination, laying out\nthe key issues, methodologies, and findings to date, and highlighting areas in\nneed of further research and development. In particular, we begin by examining\nthe effects of data contamination across various stages and forms. We then\nprovide a detailed analysis of current contamination detection methods,\ncategorizing them to highlight their focus, assumptions, strengths, and\nlimitations. We also discuss mitigation strategies, offering a clear guide for\nfuture research. This survey serves as a succinct overview of the most recent\nadvancements in data contamination research, providing a straightforward guide\nfor the benefit of future research endeavors.", "published": "2024-06-20 18:07:26", "link": "http://arxiv.org/abs/2406.14644v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Co-training for Low Resource Scientific Natural Language Inference", "abstract": "Scientific Natural Language Inference (NLI) is the task of predicting the\nsemantic relation between a pair of sentences extracted from research articles.\nThe automatic annotation method based on distant supervision for the training\nset of SciNLI (Sadat and Caragea, 2022b), the first and most popular dataset\nfor this task, results in label noise which inevitably degenerates the\nperformance of classifiers. In this paper, we propose a novel co-training\nmethod that assigns weights based on the training dynamics of the classifiers\nto the distantly supervised labels, reflective of the manner they are used in\nthe subsequent training epochs. That is, unlike the existing semi-supervised\nlearning (SSL) approaches, we consider the historical behavior of the\nclassifiers to evaluate the quality of the automatically annotated labels.\nFurthermore, by assigning importance weights instead of filtering out examples\nbased on an arbitrary threshold on the predicted confidence, we maximize the\nusage of automatically labeled data, while ensuring that the noisy labels have\na minimal impact on model training. The proposed method obtains an improvement\nof 1.5% in Macro F1 over the distant supervision baseline, and substantial\nimprovements over several other strong SSL baselines. We make our code and data\navailable on Github.", "published": "2024-06-20 18:35:47", "link": "http://arxiv.org/abs/2406.14666v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Insights into LLM Long-Context Failures: When Transformers Know but\n  Don't Tell", "abstract": "Large Language Models (LLMs) exhibit positional bias, struggling to utilize\ninformation from the middle or end of long contexts. Our study explores LLMs'\nlong-context reasoning by probing their hidden representations. We find that\nwhile LLMs encode the position of target information, they often fail to\nleverage this in generating accurate responses. This reveals a disconnect\nbetween information retrieval and utilization, a \"know but don't tell\"\nphenomenon. We further analyze the relationship between extraction time and\nfinal accuracy, offering insights into the underlying mechanics of transformer\nmodels.", "published": "2024-06-20 18:50:44", "link": "http://arxiv.org/abs/2406.14673v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Contextualized Representations of (Spanish) Ambiguous Words:\n  A New Lexical Resource and Empirical Analysis", "abstract": "Lexical ambiguity -- where a single wordform takes on distinct,\ncontext-dependent meanings -- serves as a useful tool to compare across\ndifferent language models' (LMs') ability to form distinct, contextualized\nrepresentations of the same stimulus. Few studies have systematically compared\nLMs' contextualized word embeddings for languages beyond English. Here, we\nevaluate semantic representations of Spanish ambiguous nouns in context in a\nsuite of Spanish-language monolingual and multilingual BERT-based models. We\ndevelop a novel dataset of minimal-pair sentences evoking the same or different\nsense for a target ambiguous noun. In a pre-registered study, we collect\ncontextualized human relatedness judgments for each sentence pair. We find that\nvarious BERT-based LMs' contextualized semantic representations capture some\nvariance in human judgments but fall short of the human benchmark. In\nexploratory work, we find that performance scales with model size. We also\nidentify stereotyped trajectories of target noun disambiguation as a proportion\nof traversal through a given LM family's architecture, which we partially\nreplicate in English. We contribute (1) a dataset of controlled, Spanish\nsentence stimuli with human relatedness norms, and (2) to our evolving\nunderstanding of the impact that LM specification (architectures, training\nprotocols) exerts on contextualized embeddings.", "published": "2024-06-20 18:58:11", "link": "http://arxiv.org/abs/2406.14678v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dravidian language family through Universal Dependencies lens", "abstract": "The Universal Dependencies (UD) project aims to create a cross-linguistically\nconsistent dependency annotation for multiple languages, to facilitate\nmultilingual NLP. It currently supports 114 languages. Dravidian languages are\nspoken by over 200 million people across the word, and yet there are only two\nlanguages from this family in UD. This paper examines some of the morphological\nand syntactic features of Dravidian languages and explores how they can be\nannotated in the UD framework.", "published": "2024-06-20 18:59:46", "link": "http://arxiv.org/abs/2406.14680v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Factual Dialogue Summarization via Learning from Large Language Models", "abstract": "Factual consistency is an important quality in dialogue summarization. Large\nlanguage model (LLM)-based automatic text summarization models generate more\nfactually consistent summaries compared to those by smaller pretrained language\nmodels, but they face deployment challenges in real-world applications due to\nprivacy or resource constraints. In this paper, we investigate the use of\nsymbolic knowledge distillation to improve the factual consistency of smaller\npretrained models for dialogue summarization. We employ zero-shot learning to\nextract symbolic knowledge from LLMs, generating both factually consistent\n(positive) and inconsistent (negative) summaries. We then apply two contrastive\nlearning objectives on these summaries to enhance smaller summarization models.\nExperiments with BART, PEGASUS, and Flan-T5 indicate that our approach\nsurpasses strong baselines that rely on complex data augmentation strategies.\nOur approach achieves better factual consistency while maintaining coherence,\nfluency, and relevance, as confirmed by various automatic evaluation metrics.\nWe also provide access to the data and code to facilitate future research.", "published": "2024-06-20 20:03:37", "link": "http://arxiv.org/abs/2406.14709v1", "categories": ["cs.CL", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "1+1>2: Can Large Language Models Serve as Cross-Lingual Knowledge\n  Aggregators?", "abstract": "Large Language Models (LLMs) have garnered significant attention due to their\nremarkable ability to process information across various languages. Despite\ntheir capabilities, they exhibit inconsistencies in handling identical queries\nin different languages, presenting challenges for further advancement. This\npaper introduces a method to enhance the multilingual performance of LLMs by\naggregating knowledge from diverse languages. This approach incorporates a\nlow-resource knowledge detector specific to a language, a language selection\nprocess, and mechanisms for answer replacement and integration. Our experiments\ndemonstrate notable performance improvements, particularly in reducing language\nperformance disparity. An ablation study confirms that each component of our\nmethod significantly contributes to these enhancements. This research\nhighlights the inherent potential of LLMs to harmonize multilingual\ncapabilities and offers valuable insights for further exploration.", "published": "2024-06-20 20:32:53", "link": "http://arxiv.org/abs/2406.14721v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dissecting the Ullman Variations with a SCALPEL: Why do LLMs fail at\n  Trivial Alterations to the False Belief Task?", "abstract": "Recent empirical results have sparked a debate about whether or not Large\nLanguage Models (LLMs) are capable of Theory of Mind (ToM). While some have\nfound LLMs to be successful on ToM evaluations such as the False Belief task\n(Kosinski, 2023), others have argued that LLMs solve these tasks by exploiting\nspurious correlations -- not representing beliefs -- since they fail on trivial\nalterations to these tasks (Ullman, 2023). In this paper, we introduce SCALPEL:\na technique to generate targeted modifications for False Belief tasks to test\ndifferent specific hypotheses about why LLMs fail. We find that modifications\nwhich make explicit common inferences -- such as that looking at a transparent\nobject implies recognizing its contents -- preserve LLMs' performance. This\nsuggests that LLMs' failures on modified ToM tasks could result from a lack of\nmore general commonsense reasoning, rather than a failure to represent mental\nstates. We argue that SCALPEL could be helpful for explaining LLM successes and\nfailures in other cases.", "published": "2024-06-20 21:02:30", "link": "http://arxiv.org/abs/2406.14737v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Retrieve Iteratively for In-Context Learning", "abstract": "We introduce iterative retrieval, a novel framework that empowers retrievers\nto make iterative decisions through policy optimization. Finding an optimal\nportfolio of retrieved items is a combinatorial optimization problem, generally\nconsidered NP-hard. This approach provides a learned approximation to such a\nsolution, meeting specific task requirements under a given family of large\nlanguage models (LLMs). We propose a training procedure based on reinforcement\nlearning, incorporating feedback from LLMs. We instantiate an iterative\nretriever for composing in-context learning (ICL) exemplars and apply it to\nvarious semantic parsing tasks that demand synthesized programs as outputs. By\nadding only 4M additional parameters for state encoding, we convert an\noff-the-shelf dense retriever into a stateful iterative retriever,\noutperforming previous methods in selecting ICL exemplars on semantic parsing\ndatasets such as CalFlow, TreeDST, and MTOP. Additionally, the trained\niterative retriever generalizes across different inference LLMs beyond the one\nused during training.", "published": "2024-06-20 21:07:55", "link": "http://arxiv.org/abs/2406.14739v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Analysis of Multilingual FActScore", "abstract": "FActScore has gained popularity as a metric to estimate the factuality of\nlong-form texts generated by Large Language Models (LLMs) in English. However,\nthere has not been any work in studying the behavior of FActScore in other\nlanguages. This paper studies the limitations of each component in the\nfour-component pipeline of FActScore in the multilingual setting. We introduce\na new dataset for FActScore on texts generated by strong multilingual LLMs. Our\nevaluation shows that LLMs exhibit distinct behaviors in both fact extraction\nand fact scoring tasks. No LLM produces consistent and reliable FActScore\nacross languages with varying levels of resources. We also find that the\nknowledge source plays an important role in the quality of the estimated\nFActScore. Using Wikipedia as the knowledge source may hinder the true\nFActScore of long-form text due to its limited coverage in medium- and\nlow-resource languages. We also incorporate three mitigations to our knowledge\nsource that ultimately improve FActScore estimation across all languages.", "published": "2024-06-20 18:09:40", "link": "http://arxiv.org/abs/2406.19415v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large\n  Language Models", "abstract": "Large Language Models (LLMs) are prone to generating content that exhibits\ngender biases, raising significant ethical concerns. Alignment, the process of\nfine-tuning LLMs to better align with desired behaviors, is recognized as an\neffective approach to mitigate gender biases. Although proprietary LLMs have\nmade significant strides in mitigating gender bias, their alignment datasets\nare not publicly available. The commonly used and publicly available alignment\ndataset, HH-RLHF, still exhibits gender bias to some extent. There is a lack of\npublicly available alignment datasets specifically designed to address gender\nbias. Hence, we developed a new dataset named GenderAlign, aiming at mitigating\na comprehensive set of gender biases in LLMs. This dataset comprises 8k\nsingle-turn dialogues, each paired with a \"chosen\" and a \"rejected\" response.\nCompared to the \"rejected\" responses, the \"chosen\" responses demonstrate lower\nlevels of gender bias and higher quality. Furthermore, we categorized the\ngender biases in the \"rejected\" responses of GenderAlign into 4 principal\ncategories. The experimental results show the effectiveness of GenderAlign in\nreducing gender bias in LLMs.", "published": "2024-06-20 01:45:44", "link": "http://arxiv.org/abs/2406.13925v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reasoning Like a Doctor: Improving Medical Dialogue Systems via\n  Diagnostic Reasoning Process Alignment", "abstract": "Medical dialogue systems have attracted significant attention for their\npotential to act as medical assistants. Enabling these medical systems to\nemulate clinicians' diagnostic reasoning process has been the long-standing\nresearch focus. Previous studies rudimentarily realized the simulation of\nclinicians' diagnostic process by fine-tuning language models on high-quality\ndialogue datasets. Nonetheless, they overly focus on the outcomes of the\nclinician's reasoning process while ignoring their internal thought processes\nand alignment with clinician preferences. Our work aims to build a medical\ndialogue system that aligns with clinicians' diagnostic reasoning processes. We\npropose a novel framework, Emulation, designed to generate an appropriate\nresponse that relies on abductive and deductive diagnostic reasoning analyses\nand aligns with clinician preferences through thought process modeling.\nExperimental results on two datasets confirm the efficacy of Emulation.\nCrucially, our framework furnishes clear explanations for the generated\nresponses, enhancing its transparency in medical consultations.", "published": "2024-06-20 02:02:53", "link": "http://arxiv.org/abs/2406.13934v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AspirinSum: an Aspect-based utility-preserved de-identification\n  Summarization framework", "abstract": "Due to the rapid advancement of Large Language Model (LLM), the whole\ncommunity eagerly consumes any available text data in order to train the LLM.\nCurrently, large portion of the available text data are collected from\ninternet, which has been thought as a cheap source of the training data.\nHowever, when people try to extend the LLM's capability to the personal related\ndomain, such as healthcare or education, the lack of public dataset in these\ndomains make the adaption of the LLM in such domains much slower. The reason of\nlacking public available dataset in such domains is because they usually\ncontain personal sensitive information. In order to comply with privacy law,\nthe data in such domains need to be de-identified before any kind of\ndissemination. It had been much research tried to address this problem for the\nimage or tabular data. However, there was limited research on the efficient and\ngeneral de-identification method for text data. Most of the method based on\nhuman annotation or predefined category list. It usually can not be easily\nadapted to specific domains. The goal of this proposal is to develop a text\nde-identification framework, which can be easily adapted to the specific\ndomain, leverage the existing expert knowledge without further human\nannotation. We propose an aspect-based utility-preserved de-identification\nsummarization framework, AspirinSum, by learning to align expert's aspect from\nexisting comment data, it can efficiently summarize the personal sensitive\ndocument by extracting personal sensitive aspect related sub-sentence and\nde-identify it by substituting it with similar aspect sub-sentence. We envision\nthat the de-identified text can then be used in data publishing, eventually\npublishing our de-identified dataset for downstream task use.", "published": "2024-06-20 02:29:46", "link": "http://arxiv.org/abs/2406.13947v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "AutoPal: Autonomous Adaptation to Users for Personal AI Companionship", "abstract": "Previous research has demonstrated the potential of AI agents to act as\ncompanions that can provide constant emotional support for humans. In this\npaper, we emphasize the necessity of autonomous adaptation in personal AI\ncompanionship, an underexplored yet promising direction. Such adaptability is\ncrucial as it can facilitate more tailored interactions with users and allow\nthe agent to evolve in response to users' changing needs. However, imbuing\nagents with autonomous adaptability presents unique challenges, including\nidentifying optimal adaptations to meet users' expectations and ensuring a\nsmooth transition during the adaptation process. To address them, we devise a\nhierarchical framework, AutoPal, that enables controllable and authentic\nadjustments to the agent's persona based on user interactions. A\npersonamatching dataset is constructed to facilitate the learning of optimal\npersona adaptations. Extensive experiments demonstrate the effectiveness of\nAutoPal and highlight the importance of autonomous adaptability in AI\ncompanionship.", "published": "2024-06-20 03:02:38", "link": "http://arxiv.org/abs/2406.13960v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in\n  LLMs", "abstract": "Large language models (LLMs) have shown increasing capability in\nproblem-solving and decision-making, largely based on the step-by-step\nchain-of-thought reasoning processes. However, evaluating these reasoning\nabilities has become increasingly challenging. Existing outcome-based\nbenchmarks are beginning to saturate, becoming less effective in tracking\nmeaningful progress. To address this, we present a process-based benchmark\nMR-Ben that demands a meta-reasoning skill, where LMs are asked to locate and\nanalyse potential errors in automatically generated reasoning steps. Our\nmeta-reasoning paradigm is especially suited for system-2 slow thinking,\nmirroring the human cognitive process of carefully examining assumptions,\nconditions, calculations, and logic to identify mistakes.MR-Ben comprises 5,975\nquestions curated by human experts across a wide range of subjects, including\nphysics, chemistry, logic, coding, and more. Through our designed metrics for\nassessing meta-reasoning on this benchmark, we identify interesting limitations\nand weaknesses of current LLMs (open-source and closed-source models). For\nexample, with models like the o1 series from OpenAI demonstrating strong\nperformance by effectively scrutinizing the solution space, many other\nstate-of-the-art models fall significantly behind on MR-Ben, exposing potential\nshortcomings in their training strategies and inference methodologies.", "published": "2024-06-20 03:50:23", "link": "http://arxiv.org/abs/2406.13975v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Changes in Nation Perception with Nationality-Assigned\n  Personas in LLMs", "abstract": "Persona assignment has become a common strategy for customizing LLM use to\nparticular tasks and contexts. In this study, we explore how evaluation of\ndifferent nations change when LLMs are assigned specific nationality personas.\nWe assign 193 different nationality personas (e.g., an American person) to four\nLLMs and examine how the LLM evaluations (or ''perceptions'')of countries\nchange. We find that all LLM-persona combinations tend to favor Western\nEuropean nations, though nation-personas push LLM behaviors to focus more on\nand treat the nation-persona's own region more favorably. Eastern European,\nLatin American, and African nations are treated more negatively by different\nnationality personas. We additionally find that evaluations by nation-persona\nLLMs of other nations correlate with human survey responses but fail to match\nthe values closely. Our study provides insight into how biases and stereotypes\nare realized within LLMs when adopting different national personas. In line\nwith the ''Blueprint for an AI Bill of Rights'', our findings underscore the\ncritical need for developing mechanisms to ensure that LLM outputs promote\nfairness and avoid over-generalization.", "published": "2024-06-20 04:44:20", "link": "http://arxiv.org/abs/2406.13993v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "\"Global is Good, Local is Bad?\": Understanding Brand Bias in LLMs", "abstract": "Many recent studies have investigated social biases in LLMs but brand bias\nhas received little attention. This research examines the biases exhibited by\nLLMs towards different brands, a significant concern given the widespread use\nof LLMs in affected use cases such as product recommendation and market\nanalysis. Biased models may perpetuate societal inequalities, unfairly favoring\nestablished global brands while marginalizing local ones. Using a curated\ndataset across four brand categories, we probe the behavior of LLMs in this\nspace. We find a consistent pattern of bias in this space -- both in terms of\ndisproportionately associating global brands with positive attributes and\ndisproportionately recommending luxury gifts for individuals in high-income\ncountries. We also find LLMs are subject to country-of-origin effects which may\nboost local brand preference in LLM outputs in specific contexts.", "published": "2024-06-20 04:52:19", "link": "http://arxiv.org/abs/2406.13997v2", "categories": ["cs.CL", "cs.CE"], "primary_category": "cs.CL"}
{"title": "Seeing Through AI's Lens: Enhancing Human Skepticism Towards\n  LLM-Generated Fake News", "abstract": "LLMs offer valuable capabilities, yet they can be utilized by malicious users\nto disseminate deceptive information and generate fake news. The growing\nprevalence of LLMs poses difficulties in crafting detection approaches that\nremain effective across various text domains. Additionally, the absence of\nprecautionary measures for AI-generated news on online social platforms is\nconcerning. Therefore, there is an urgent need to improve people's ability to\ndifferentiate between news articles written by humans and those produced by\nLLMs. By providing cues in human-written and LLM-generated news, we can help\nindividuals increase their skepticism towards fake LLM-generated news. This\npaper aims to elucidate simple markers that help individuals distinguish\nbetween articles penned by humans and those created by LLMs. To achieve this,\nwe initially collected a dataset comprising 39k news articles authored by\nhumans or generated by four distinct LLMs with varying degrees of fake. We then\ndevise a metric named Entropy-Shift Authorship Signature (ESAS) based on the\ninformation theory and entropy principles. The proposed ESAS ranks terms or\nentities, like POS tagging, within news articles based on their relevance in\ndiscerning article authorship. We demonstrate the effectiveness of our metric\nby showing the high accuracy attained by a basic method, i.e., TF-IDF combined\nwith logistic regression classifier, using a small set of terms with the\nhighest ESAS score. Consequently, we introduce and scrutinize these top\nESAS-ranked terms to aid individuals in strengthening their skepticism towards\nLLM-generated fake news.", "published": "2024-06-20 06:02:04", "link": "http://arxiv.org/abs/2406.14012v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Investigating the Pre-Training Dynamics of In-Context Learning: Task\n  Recognition vs. Task Learning", "abstract": "The emergence of in-context learning (ICL) is potentially attributed to two\nmajor abilities: task recognition (TR) for recognizing the task from\ndemonstrations and utilizing pre-trained priors, and task learning (TL) for\nlearning from demonstrations. However, relationships between the two abilities\nand how such relationships affect the emergence of ICL is unclear. In this\npaper, we take the first step by examining the pre-training dynamics of the\nemergence of ICL. With carefully designed metrics, we find that these two\nabilities are, in fact, competitive during pre-training. Moreover, we observe a\nstrong negative correlation between the competition and ICL performance.\nFurther analysis of common pre-training factors (i.e., model size, dataset\nsize, and data curriculum) demonstrates possible ways to manage the\ncompetition. Based on these insights, we propose a simple yet effective method\nto better integrate these two abilities for ICL at inference time. Through\nadaptive ensemble learning, the performance of ICL can be significantly\nboosted, enabling two small models to outperform a larger one with more than\ntwice the parameters. The code is available at\nhttps://github.com/RUCAIBox/Competitive-ICL.", "published": "2024-06-20 06:37:47", "link": "http://arxiv.org/abs/2406.14022v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Evaluating Implicit Bias in Large Language Models by Attacking From a\n  Psychometric Perspective", "abstract": "As large language models (LLMs) become an important way of information\naccess, there have been increasing concerns that LLMs may intensify the spread\nof unethical content, including implicit bias that hurts certain populations\nwithout explicit harmful words. In this paper, we conduct a rigorous evaluation\nof LLMs' implicit bias towards certain demographics by attacking them from a\npsychometric perspective to elicit agreements to biased viewpoints. Inspired by\npsychometric principles in cognitive and social psychology, we propose three\nattack approaches, i.e., Disguise, Deception, and Teaching. Incorporating the\ncorresponding attack instructions, we built two benchmarks: (1) a bilingual\ndataset with biased statements covering four bias types (2.7K instances) for\nextensive comparative analysis, and (2) BUMBLE, a larger benchmark spanning\nnine common bias types (12.7K instances) for comprehensive evaluation.\nExtensive evaluation of popular commercial and open-source LLMs shows that our\nmethods can elicit LLMs' inner bias more effectively than competitive\nbaselines. Our attack methodology and benchmarks offer an effective means of\nassessing the ethical risks of LLMs, driving progress toward greater\naccountability in their development.", "published": "2024-06-20 06:42:08", "link": "http://arxiv.org/abs/2406.14023v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Game Play to Investigate Multimodal and Conversational Grounding\n  in Large Multimodal Models", "abstract": "While the situation has improved for text-only models, it again seems to be\nthe case currently that multimodal (text and image) models develop faster than\nways to evaluate them. In this paper, we bring a recently developed evaluation\nparadigm from text models to multimodal models, namely evaluation through the\ngoal-oriented game (self) play, complementing reference-based and\npreference-based evaluation. Specifically, we define games that challenge a\nmodel's capability to represent a situation from visual information and align\nsuch representations through dialogue. We find that the largest closed models\nperform rather well on the games that we define, while even the best\nopen-weight models struggle with them. On further analysis, we find that the\nexceptional deep captioning capabilities of the largest models drive some of\nthe performance. There is still room to grow for both kinds of models, ensuring\nthe continued relevance of the benchmark.", "published": "2024-06-20 06:56:19", "link": "http://arxiv.org/abs/2406.14035v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Taxonomy-Guided Zero-Shot Recommendations with LLMs", "abstract": "With the emergence of large language models (LLMs) and their ability to\nperform a variety of tasks, their application in recommender systems (RecSys)\nhas shown promise. However, we are facing significant challenges when deploying\nLLMs into RecSys, such as limited prompt length, unstructured item information,\nand un-constrained generation of recommendations, leading to sub-optimal\nperformance. To address these issues, we propose a novel method using a\ntaxonomy dictionary. This method provides a systematic framework for\ncategorizing and organizing items, improving the clarity and structure of item\ninformation. By incorporating the taxonomy dictionary into LLM prompts, we\nachieve efficient token utilization and controlled feature generation, leading\nto more accurate and contextually relevant recommendations. Our Taxonomy-guided\nRecommendation (TaxRec) approach features a two-step process: one-time taxonomy\ncategorization and LLM-based recommendation, enabling zero-shot recommendations\nwithout the need for domain-specific fine-tuning. Experimental results\ndemonstrate TaxRec significantly enhances recommendation quality compared to\ntraditional zero-shot approaches, showcasing its efficacy as personal\nrecommender with LLMs. Code is available at\nhttps://github.com/yueqingliang1/TaxRec.", "published": "2024-06-20 07:06:58", "link": "http://arxiv.org/abs/2406.14043v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "How Many Parameters Does it Take to Change a Light Bulb? Evaluating\n  Performance in Self-Play of Conversational Games as a Function of Model\n  Characteristics", "abstract": "What makes a good Large Language Model (LLM)? That it performs well on the\nrelevant benchmarks -- which hopefully measure, with some validity, the\npresence of capabilities that are also challenged in real application. But what\nmakes the model perform well? What gives a model its abilities? We take a\nrecently introduced type of benchmark that is meant to challenge capabilities\nin a goal-directed, agentive context through self-play of conversational games,\nand analyse how performance develops as a function of model characteristics\nlike number of parameters, or type of training. We find that while there is a\nclear relationship between number of parameters and performance, there is still\na wide spread of performance points within a given size bracket, which is to be\naccounted for by training parameters such as fine-tuning data quality and\nmethod. From a more practical angle, we also find a certain degree of\nunpredictability about performance across access methods, possible due to\nunexposed sampling parameters, and a, very welcome, performance stability\nagainst at least moderate weight quantisation during inference.", "published": "2024-06-20 07:17:09", "link": "http://arxiv.org/abs/2406.14051v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Seamless Language Expansion: Enhancing Multilingual Mastery in\n  Self-Supervised Models", "abstract": "Self-supervised (SSL) models have shown great performance in various\ndownstream tasks. However, they are typically developed for limited languages,\nand may encounter new languages in real-world. Developing a SSL model for each\nnew language is costly. Thus, it is vital to figure out how to efficiently\nadapt existed SSL models to a new language without impairing its original\nabilities. We propose adaptation methods which integrate LoRA to existed SSL\nmodels to extend new language. We also develop preservation strategies which\ninclude data combination and re-clustering to retain abilities on existed\nlanguages. Applied to mHuBERT, we investigate their effectiveness on speech\nre-synthesis task. Experiments show that our adaptation methods enable mHuBERT\nto be applied to a new language (Mandarin) with MOS value increased about 1.6\nand the relative value of WER reduced up to 61.72%. Also, our preservation\nstrategies ensure that the performance on both existed and new languages\nremains intact.", "published": "2024-06-20 08:13:30", "link": "http://arxiv.org/abs/2406.14092v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "EasyECR: A Library for Easy Implementation and Evaluation of Event\n  Coreference Resolution Models", "abstract": "Event Coreference Resolution (ECR) is the task of clustering event mentions\nthat refer to the same real-world event. Despite significant advancements, ECR\nresearch faces two main challenges: limited generalizability across domains due\nto narrow dataset evaluations, and difficulties in comparing models within\ndiverse ECR pipelines. To address these issues, we develop EasyECR, the first\nopen-source library designed to standardize data structures and abstract ECR\npipelines for easy implementation and fair evaluation. More specifically,\nEasyECR integrates seven representative pipelines and ten popular benchmark\ndatasets, enabling model evaluations on various datasets and promoting the\ndevelopment of robust ECR pipelines. By conducting extensive evaluation via our\nEasyECR, we find that, \\lowercase\\expandafter{\\romannumeral1}) the\nrepresentative ECR pipelines cannot generalize across multiple datasets, hence\nevaluating ECR pipelines on multiple datasets is necessary,\n\\lowercase\\expandafter{\\romannumeral2}) all models in ECR pipelines have a\ngreat effect on pipeline performance, therefore, when one model in ECR\npipelines are compared, it is essential to ensure that the other models remain\nconsistent. Additionally, reproducing ECR results is not trivial, and the\ndeveloped library can help reduce this discrepancy. The experimental results\nprovide valuable baselines for future research.", "published": "2024-06-20 08:40:21", "link": "http://arxiv.org/abs/2406.14106v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "An Investigation of Prompt Variations for Zero-shot LLM-based Rankers", "abstract": "We provide a systematic understanding of the impact of specific components\nand wordings used in prompts on the effectiveness of rankers based on zero-shot\nLarge Language Models (LLMs). Several zero-shot ranking methods based on LLMs\nhave recently been proposed. Among many aspects, methods differ across (1) the\nranking algorithm they implement, e.g., pointwise vs. listwise, (2) the\nbackbone LLMs used, e.g., GPT3.5 vs. FLAN-T5, (3) the components and wording\nused in prompts, e.g., the use or not of role-definition (role-playing) and the\nactual words used to express this. It is currently unclear whether performance\ndifferences are due to the underlying ranking algorithm, or because of spurious\nfactors such as better choice of words used in prompts. This confusion risks to\nundermine future research. Through our large-scale experimentation and\nanalysis, we find that ranking algorithms do contribute to differences between\nmethods for zero-shot LLM ranking. However, so do the LLM backbones -- but even\nmore importantly, the choice of prompt components and wordings affect the\nranking. In fact, in our experiments, we find that, at times, these latter\nelements have more impact on the ranker's effectiveness than the actual ranking\nalgorithms, and that differences among ranking methods become more blurred when\nprompt variations are considered.", "published": "2024-06-20 09:03:18", "link": "http://arxiv.org/abs/2406.14117v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Data-Driven Guided Decoding Mechanism for Diagnostic Captioning", "abstract": "Diagnostic Captioning (DC) automatically generates a diagnostic text from one\nor more medical images (e.g., X-rays, MRIs) of a patient. Treated as a draft,\nthe generated text may assist clinicians, by providing an initial estimation of\nthe patient's condition, speeding up and helping safeguard the diagnostic\nprocess. The accuracy of a diagnostic text, however, strongly depends on how\nwell the key medical conditions depicted in the images are expressed. We\npropose a new data-driven guided decoding method that incorporates medical\ninformation, in the form of existing tags capturing key conditions of the\nimage(s), into the beam search of the diagnostic text generation process. We\nevaluate the proposed method on two medical datasets using four DC systems that\nrange from generic image-to-text systems with CNN encoders and RNN decoders to\npre-trained Large Language Models. The latter can also be used in few- and\nzero-shot learning scenarios. In most cases, the proposed mechanism improves\nperformance with respect to all evaluation measures. We provide an open-source\nimplementation of the proposed method at https://github.com/nlpaueb/dmmcs.", "published": "2024-06-20 10:08:17", "link": "http://arxiv.org/abs/2406.14164v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Ranking LLMs by compression", "abstract": "We conceptualize the process of understanding as information compression, and\npropose a method for ranking large language models (LLMs) based on lossless\ndata compression. We demonstrate the equivalence of compression length under\narithmetic coding with cumulative negative log probabilities when using a large\nlanguage model as a prior, that is, the pre-training phase of the model is\nessentially the process of learning the optimal coding length. At the same\ntime, the evaluation metric compression ratio can be obtained without actual\ncompression, which greatly saves overhead. In this paper, we use five large\nlanguage models as priors for compression, then compare their performance on\nchallenging natural language processing tasks, including sentence completion,\nquestion answering, and coreference resolution. Experimental results show that\ncompression ratio and model performance are positively correlated, so it can be\nused as a general metric to evaluate large language models.", "published": "2024-06-20 10:23:38", "link": "http://arxiv.org/abs/2406.14171v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Timo: Towards Better Temporal Reasoning for Language Models", "abstract": "Reasoning about time is essential for Large Language Models (LLMs) to\nunderstand the world. Previous works focus on solving specific tasks, primarily\non time-sensitive question answering. While these methods have proven\neffective, they cannot generalize to a wider spectrum of temporal reasoning\ntasks. Therefore, we propose a crucial question: Can we build a universal\nframework to handle a variety of temporal reasoning tasks? To that end, we\nsystematically study 38 temporal reasoning tasks. Based on the observation that\n19 tasks are directly related to mathematics, we first leverage the available\nmathematical dataset to set a solid foundation for temporal reasoning. However,\nthe in-depth study indicates that focusing solely on mathematical enhancement\nfalls short of addressing pure temporal reasoning tasks. To mitigate this\nlimitation, we propose a simple but effective self-critic temporal optimization\nmethod to enhance the model's temporal reasoning capabilities without\nsacrificing general task abilities. Finally, we develop Timo, a model designed\nto excel in temporal reasoning at the 7B and 13B scales. Notably, Timo\noutperforms the counterpart LLMs by 10.0 and 7.6 in average accuracy scores and\nachieves the new state-of-the-art (SOTA) performance of comparable size.\nExtensive experiments further validate our framework's effectiveness and its\ngeneralization across diverse temporal tasks. The code is available at\nhttps://github.com/zhaochen0110/Timo.", "published": "2024-06-20 10:52:14", "link": "http://arxiv.org/abs/2406.14192v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Representational Capacity of Neural Language Models with\n  Chain-of-Thought Reasoning", "abstract": "The performance of modern language models (LMs) has been improved by\nchain-of-thought (CoT) reasoning, i.e., the process of generating intermediate\nresults that guide the model towards a final answer. A possible explanation for\nthis improvement is that CoT reasoning extends an LM's computational power, as\nRNNs and transformers with additional scratch space are known to be Turing\ncomplete. Comparing LMs to Turing machines, however, introduces a category\nerror - Turing machines decide language membership, whereas LMs define\ndistributions over strings. To bridge this gap, we formalize CoT reasoning in a\nprobabilistic setting. We present several results on the representational\ncapacity of recurrent and transformer LMs with CoT reasoning, showing that they\ncan represent the same family of distributions over strings as probabilistic\nTuring machines.", "published": "2024-06-20 10:59:02", "link": "http://arxiv.org/abs/2406.14197v2", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "On the Evaluation Practices in Multilingual NLP: Can Machine Translation\n  Offer an Alternative to Human Translations?", "abstract": "While multilingual language models (MLMs) have been trained on 100+\nlanguages, they are typically only evaluated across a handful of them due to a\nlack of available test data in most languages. This is particularly problematic\nwhen assessing MLM's potential for low-resource and unseen languages. In this\npaper, we present an analysis of existing evaluation frameworks in multilingual\nNLP, discuss their limitations, and propose several directions for more robust\nand reliable evaluation practices. Furthermore, we empirically study to what\nextent machine translation offers a {reliable alternative to human translation}\nfor large-scale evaluation of MLMs across a wide set of languages. We use a\nSOTA translation model to translate test data from 4 tasks to 198 languages and\nuse them to evaluate three MLMs. We show that while the selected subsets of\nhigh-resource test languages are generally sufficiently representative of a\nwider range of high-resource languages, we tend to overestimate MLMs' ability\non low-resource languages. Finally, we show that simpler baselines can achieve\nrelatively strong performance without having benefited from large-scale\nmultilingual pretraining.", "published": "2024-06-20 12:46:12", "link": "http://arxiv.org/abs/2406.14267v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Step-Back Profiling: Distilling User History for Personalized Scientific\n  Writing", "abstract": "Large language models (LLM) excel at a variety of natural language processing\ntasks, yet they struggle to generate personalized content for individuals,\nparticularly in real-world scenarios like scientific writing. Addressing this\nchallenge, we introduce STEP-BACK PROFILING to personalize LLMs by distilling\nuser history into concise profiles, including essential traits and preferences\nof users. To conduct the experiments, we construct a Personalized Scientific\nWriting (PSW) dataset to study multi-user personalization. PSW requires the\nmodels to write scientific papers given specialized author groups with diverse\nacademic backgrounds. As for the results, we demonstrate the effectiveness of\ncapturing user characteristics via STEP-BACK PROFILING for collaborative\nwriting. Moreover, our approach outperforms the baselines by up to 3.6 points\non the general personalization benchmark (LaMP), including 7 personalization\nLLM tasks. Our ablation studies validate the contributions of different\ncomponents in our method and provide insights into our task definition. Our\ndataset and code are available at\n\\url{https://github.com/gersteinlab/step-back-profiling}.", "published": "2024-06-20 12:58:26", "link": "http://arxiv.org/abs/2406.14275v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "QPaug: Question and Passage Augmentation for Open-Domain Question\n  Answering of LLMs", "abstract": "Retrieval-augmented generation (RAG) has received much attention for\nOpen-domain question-answering (ODQA) tasks as a means to compensate for the\nparametric knowledge of large language models (LLMs). While previous approaches\nfocused on processing retrieved passages to remove irrelevant context, they\nstill rely heavily on the quality of retrieved passages which can degrade if\nthe question is ambiguous or complex. In this paper, we propose a simple yet\nefficient method called question and passage augmentation (QPaug) via LLMs for\nopen-domain QA. QPaug first decomposes the original questions into\nmultiple-step sub-questions. By augmenting the original question with detailed\nsub-questions and planning, we are able to make the query more specific on what\nneeds to be retrieved, improving the retrieval performance. In addition, to\ncompensate for the case where the retrieved passages contain distracting\ninformation or divided opinions, we augment the retrieved passages with\nself-generated passages by LLMs to guide the answer extraction. Experimental\nresults show that QPaug outperforms the previous state-of-the-art and achieves\nsignificant performance gain over existing RAG methods. The source code is\navailable at \\url{https://github.com/kmswin1/QPaug}.", "published": "2024-06-20 12:59:27", "link": "http://arxiv.org/abs/2406.14277v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Plan for Retrieval-Augmented Large Language Models from\n  Knowledge Graphs", "abstract": "Improving the performance of large language models (LLMs) in complex\nquestion-answering (QA) scenarios has always been a research focal point.\nRecent studies have attempted to enhance LLMs' performance by combining\nstep-wise planning with external retrieval. While effective for advanced models\nlike GPT-3.5, smaller LLMs face challenges in decomposing complex questions,\nnecessitating supervised fine-tuning. Previous work has relied on manual\nannotation and knowledge distillation from teacher LLMs, which are\ntime-consuming and not accurate enough. In this paper, we introduce a novel\nframework for enhancing LLMs' planning capabilities by using planning data\nderived from knowledge graphs (KGs). LLMs fine-tuned with this data have\nimproved planning capabilities, better equipping them to handle complex QA\ntasks that involve retrieval. Evaluations on multiple datasets, including our\nnewly proposed benchmark, highlight the effectiveness of our framework and the\nbenefits of KG-derived planning data.", "published": "2024-06-20 13:07:38", "link": "http://arxiv.org/abs/2406.14282v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VAIYAKARANA : A Benchmark for Automatic Grammar Correction in Bangla", "abstract": "Bangla (Bengali) is the fifth most spoken language globally and, yet, the\nproblem of automatic grammar correction in Bangla is still in its nascent\nstage. This is mostly due to the need for a large corpus of grammatically\nincorrect sentences, with their corresponding correct counterparts. The present\nstate-of-the-art techniques to curate a corpus for grammatically wrong\nsentences involve random swapping, insertion and deletion of words.\nHowever,these steps may not always generate grammatically wrong sentences in\nBangla. In this work, we propose a pragmatic approach to generate grammatically\nwrong sentences in Bangla. We first categorize the different kinds of errors in\nBangla into 5 broad classes and 12 finer classes. We then use these to generate\ngrammatically wrong sentences systematically from a correct sentence. This\napproach can generate a large number of wrong sentences and can, thus, mitigate\nthe challenge of lacking a large corpus for neural networks. We provide a\ndataset, Vaiyakarana, consisting of 92,830 grammatically incorrect sentences as\nwell as 18,426 correct sentences. We also collected 619 human-generated\nsentences from essays written by Bangla native speakers. This helped us to\nunderstand errors that are more frequent. We evaluated our corpus against\nneural models and LLMs and also benchmark it against human evaluators who are\nnative speakers of Bangla. Our analysis shows that native speakers are far more\naccurate than state-of-the-art models to detect whether the sentence is\ngrammatically correct. Our methodology of generating erroneous sentences can be\napplied for most other Indian languages as well.", "published": "2024-06-20 13:09:29", "link": "http://arxiv.org/abs/2406.14284v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Infusing clinical knowledge into tokenisers for language models", "abstract": "This study introduces a novel knowledge enhanced tokenisation mechanism,\nK-Tokeniser, for clinical text processing. Technically, at initialisation\nstage, K-Tokeniser populates global representations of tokens based on semantic\ntypes of domain concepts (such as drugs or diseases) from either a domain\nontology like Unified Medical Language System or the training data of the task\nrelated corpus. At training or inference stage, sentence level localised\ncontext will be utilised for choosing the optimal global token representation\nto realise the semantic-based tokenisation. To avoid pretraining using the new\ntokeniser, an embedding initialisation approach is proposed to generate\nrepresentations for new tokens. Using three transformer-based language models,\na comprehensive set of experiments are conducted on four real-world datasets\nfor evaluating K-Tokeniser in a wide range of clinical text analytics tasks\nincluding clinical concept and relation extraction, automated clinical coding,\nclinical phenotype identification, and clinical research article\nclassification. Overall, our models demonstrate consistent improvements over\ntheir counterparts in all tasks. In particular, substantial improvements are\nobserved in the automated clinical coding task with 13\\% increase on Micro\n$F_1$ score. Furthermore, K-Tokeniser also shows significant capacities in\nfacilitating quicker converge of language models. Specifically, using\nK-Tokeniser, the language models would only require 50\\% of the training data\nto achieve the best performance of the baseline tokeniser using all training\ndata in the concept extraction task and less than 20\\% of the data for the\nautomated coding task. It is worth mentioning that all these improvements\nrequire no pre-training process, making the approach generalisable.", "published": "2024-06-20 13:43:03", "link": "http://arxiv.org/abs/2406.14312v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with\n  Unanswerability", "abstract": "Real-world applications of KBQA require models to handle unanswerable\nquestions with a limited volume of in-domain labeled training data. We propose\nthe novel task of few-shot transfer for KBQA with unanswerable questions and\ncontribute two new datasets for performance evaluation. We present FUn-FuSIC -\na novel solution for our task that extends FuSIC KBQA, the state-of-the-art\nfew-shot transfer model for answerable-only KBQA. We first note that\nFuSIC-KBQA's iterative repair makes a strong assumption that all questions are\nunanswerable. As a remedy, we propose Feedback for Unanswerability (FUn), which\nuses iterative repair using feedback from a suite of strong and weak verifiers,\nand an adaptation of self consistency for unanswerabilty to better assess the\nanswerability of a question. Our experiments show that FUn-FuSIC significantly\noutperforms suitable adaptations of multiple LLM based and supervised SoTA\nmodels on our task, while establishing a new SoTA for answerable few-shot\ntransfer as well.", "published": "2024-06-20 13:43:38", "link": "http://arxiv.org/abs/2406.14313v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identifying User Goals from UI Trajectories", "abstract": "Identifying underlying user goals and intents has been recognized as valuable\nin various personalization-oriented settings, such as personalized agents,\nimproved search responses, advertising, user analytics, and more. In this\npaper, we propose a new task goal identification from observed UI trajectories\naiming to infer the user's detailed intentions when performing a task within UI\nenvironments. To support this task, we also introduce a novel evaluation\nmethodology designed to assess whether two intent descriptions can be\nconsidered paraphrases within a specific UI environment. Furthermore, we\ndemonstrate how this task can leverage datasets designed for the inverse\nproblem of UI automation, utilizing Android and web datasets for our\nexperiments. To benchmark this task, we compare the performance of humans and\nstate-of-the-art models, specifically GPT-4 and Gemini-1.5 Pro, using our\nproposed metric. The results reveal that both Gemini and GPT underperform\nrelative to human performance, underscoring the challenge of the proposed task\nand the significant room for improvement. This work highlights the importance\nof goal identification within UI trajectories, providing a foundation for\nfurther exploration and advancement in this area.", "published": "2024-06-20 13:46:10", "link": "http://arxiv.org/abs/2406.14314v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LiveMind: Low-latency Large Language Models with Simultaneous Inference", "abstract": "In this paper, we introduce LiveMind, a novel low-latency inference framework\nfor large language model (LLM) inference which enables LLMs to perform\ninferences with incomplete user input. By reallocating computational processes\nto the input phase, a substantial reduction in latency is achieved, thereby\nsignificantly enhancing the interactive experience for users of LLMs. The\nframework adeptly manages the visibility of the streaming input to the model,\nallowing it to infer from incomplete user input or await additional content.\nCompared with traditional inference methods on complete user input, our\napproach demonstrates an average reduction in response latency of 84.0% on the\nMMLU dataset and 71.6% on the MMLU-Pro dataset, while maintaining comparable\naccuracy. Additionally, our framework facilitates collaborative inference and\noutput across different models. By employing an large LLM for inference and a\nsmall LLM for output, we achieve an average 37% reduction in response latency,\nalongside a 4.30% improvement in accuracy on the MMLU-Pro dataset compared with\nthe baseline. The proposed LiveMind framework advances the field of human-AI\ninteraction by enabling more responsive and efficient communication between\nusers and AI systems.", "published": "2024-06-20 13:52:30", "link": "http://arxiv.org/abs/2406.14319v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Self-supervised Interpretable Concept-based Models for Text\n  Classification", "abstract": "Despite their success, Large-Language Models (LLMs) still face criticism as\ntheir lack of interpretability limits their controllability and reliability.\nTraditional post-hoc interpretation methods, based on attention and\ngradient-based analysis, offer limited insight into the model's decision-making\nprocesses. In the image field, Concept-based models have emerged as\nexplainable-by-design architectures, employing human-interpretable features as\nintermediate representations. However, these methods have not been yet adapted\nto textual data, mainly because they require expensive concept annotations,\nwhich are impractical for real-world text data. This paper addresses this\nchallenge by proposing a self-supervised Interpretable Concept Embedding Models\n(ICEMs). We leverage the generalization abilities of LLMs to predict the\nconcepts labels in a self-supervised way, while we deliver the final\npredictions with an interpretable function. The results of our experiments show\nthat ICEMs can be trained in a self-supervised way achieving similar\nperformance to fully supervised concept-based models and end-to-end black-box\nones. Additionally, we show that our models are (i) interpretable, offering\nmeaningful logical explanations for their predictions; (ii) interactable,\nallowing humans to modify intermediate predictions through concept\ninterventions; and (iii) controllable, guiding the LLMs' decoding process to\nfollow a required decision-making path.", "published": "2024-06-20 14:04:53", "link": "http://arxiv.org/abs/2406.14335v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Spatial Representations in the Historical Lake District Texts\n  with LLM-based Relation Extraction", "abstract": "Navigating historical narratives poses a challenge in unveiling the spatial\nintricacies of past landscapes. The proposed work addresses this challenge\nwithin the context of the English Lake District, employing the Corpus of the\nLake District Writing. The method utilizes a generative pre-trained transformer\nmodel to extract spatial relations from the textual descriptions in the corpus.\nThe study applies this large language model to understand the spatial\ndimensions inherent in historical narratives comprehensively. The outcomes are\npresented as semantic triples, capturing the nuanced connections between\nentities and locations, and visualized as a network, offering a graphical\nrepresentation of the spatial narrative. The study contributes to a deeper\ncomprehension of the English Lake District's spatial tapestry and provides an\napproach to uncovering spatial relations within diverse historical contexts.", "published": "2024-06-20 14:04:59", "link": "http://arxiv.org/abs/2406.14336v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Jailbreaking as a Reward Misspecification Problem", "abstract": "The widespread adoption of large language models (LLMs) has raised concerns\nabout their safety and reliability, particularly regarding their vulnerability\nto adversarial attacks. In this paper, we propose a novel perspective that\nattributes this vulnerability to reward misspecification during the alignment\nprocess. This misspecification occurs when the reward function fails to\naccurately capture the intended behavior, leading to misaligned model outputs.\nWe introduce a metric ReGap to quantify the extent of reward misspecification\nand demonstrate its effectiveness and robustness in detecting harmful backdoor\nprompts. Building upon these insights, we present ReMiss, a system for\nautomated red teaming that generates adversarial prompts in a\nreward-misspecified space. ReMiss achieves state-of-the-art attack success\nrates on the AdvBench benchmark against various target aligned LLMs while\npreserving the human readability of the generated prompts. Furthermore, these\nattacks on open-source models demonstrate high transferability to closed-source\nmodels like GPT-4o and out-of-distribution tasks from HarmBench. Detailed\nanalysis highlights the unique advantages of the proposed reward\nmisspecification objective compared to previous methods, offering new insights\nfor improving LLM safety and robustness.", "published": "2024-06-20 15:12:27", "link": "http://arxiv.org/abs/2406.14393v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Data-Centric AI in the Age of Large Language Models", "abstract": "This position paper proposes a data-centric viewpoint of AI research,\nfocusing on large language models (LLMs). We start by making the key\nobservation that data is instrumental in the developmental (e.g., pretraining\nand fine-tuning) and inferential stages (e.g., in-context learning) of LLMs,\nand yet it receives disproportionally low attention from the research\ncommunity. We identify four specific scenarios centered around data, covering\ndata-centric benchmarks and data curation, data attribution, knowledge\ntransfer, and inference contextualization. In each scenario, we underscore the\nimportance of data, highlight promising research directions, and articulate the\npotential impacts on the research community and, where applicable, the society\nas a whole. For instance, we advocate for a suite of data-centric benchmarks\ntailored to the scale and complexity of data for LLMs. These benchmarks can be\nused to develop new data curation methods and document research efforts and\nresults, which can help promote openness and transparency in AI and LLM\nresearch.", "published": "2024-06-20 16:34:07", "link": "http://arxiv.org/abs/2406.14473v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Does Object Grounding Really Reduce Hallucination of Large\n  Vision-Language Models?", "abstract": "Large vision-language models (LVLMs) have recently dramatically pushed the\nstate of the art in image captioning and many image understanding tasks (e.g.,\nvisual question answering). LVLMs, however, often \\textit{hallucinate} and\nproduce captions that mention concepts that cannot be found in the image. These\nhallucinations erode the trustworthiness of LVLMs and are arguably among the\nmain obstacles to their ubiquitous adoption. Recent work suggests that addition\nof grounding objectives -- those that explicitly align image regions or objects\nto text spans -- reduces the amount of LVLM hallucination. Although intuitive,\nthis claim is not empirically justified as the reduction effects have been\nestablished, we argue, with flawed evaluation protocols that (i) rely on data\n(i.e., MSCOCO) that has been extensively used in LVLM training and (ii) measure\nhallucination via question answering rather than open-ended caption generation.\nIn this work, in contrast, we offer the first systematic analysis of the effect\nof fine-grained object grounding on LVLM hallucination under an evaluation\nprotocol that more realistically captures LVLM hallucination in open\ngeneration. Our extensive experiments over three backbone LLMs reveal that\ngrounding objectives have little to no effect on object hallucination in open\ncaption generation.", "published": "2024-06-20 16:56:11", "link": "http://arxiv.org/abs/2406.14492v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "African or European Swallow? Benchmarking Large Vision-Language Models\n  for Fine-Grained Object Classification", "abstract": "Recent Large Vision-Language Models (LVLMs) demonstrate impressive abilities\non numerous image understanding and reasoning tasks. The task of fine-grained\nobject classification (e.g., distinction between \\textit{animal species}),\nhowever, has been probed insufficiently, despite its downstream importance. We\nfill this evaluation gap by creating \\texttt{FOCI} (\\textbf{F}ine-grained\n\\textbf{O}bject \\textbf{C}lass\\textbf{I}fication), a difficult multiple-choice\nbenchmark for fine-grained object classification, from existing object\nclassification datasets: (1) multiple-choice avoids ambiguous answers\nassociated with casting classification as open-ended QA task; (2) we retain\nclassification difficulty by mining negative labels with a CLIP model.\n\\texttt{FOCI}\\xspace complements five popular classification datasets with four\ndomain-specific subsets from ImageNet-21k. We benchmark 12 public LVLMs on\n\\texttt{FOCI} and show that it tests for a \\textit{complementary skill} to\nestablished image understanding and reasoning benchmarks. Crucially, CLIP\nmodels exhibit dramatically better performance than LVLMs. Since the image\nencoders of LVLMs come from these CLIP models, this points to inadequate\nalignment for fine-grained object distinction between the encoder and the LLM\nand warrants (pre)training data with more fine-grained annotation. We release\nour code at \\url{https://github.com/gregor-ge/FOCI-Benchmark}.", "published": "2024-06-20 16:59:39", "link": "http://arxiv.org/abs/2406.14496v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CodeRAG-Bench: Can Retrieval Augment Code Generation?", "abstract": "While language models (LMs) have proven remarkably adept at generating code,\nmany programs are challenging for LMs to generate using their parametric\nknowledge alone. Providing external contexts such as library documentation can\nfacilitate generating accurate and functional code. Despite the success of\nretrieval-augmented generation (RAG) in various text-oriented tasks, its\npotential for improving code generation remains under-explored. In this work,\nwe conduct a systematic, large-scale analysis by asking: in what scenarios can\nretrieval benefit code generation models? and what challenges remain? We first\ncurate a comprehensive evaluation benchmark, CodeRAG-Bench, encompassing three\ncategories of code generation tasks, including basic programming, open-domain,\nand repository-level problems. We aggregate documents from five sources for\nmodels to retrieve contexts: competition solutions, online tutorials, library\ndocumentation, StackOverflow posts, and GitHub repositories. We examine\ntop-performing models on CodeRAG-Bench by providing contexts retrieved from one\nor multiple sources. While notable gains are made in final code generation by\nretrieving high-quality contexts across various settings, our analysis reveals\nroom for improvement -- current retrievers still struggle to fetch useful\ncontexts especially with limited lexical overlap, and generators fail to\nimprove with limited context lengths or abilities to integrate additional\ncontexts. We hope CodeRAG-Bench serves as an effective testbed to encourage\nfurther development of advanced code-oriented RAG methods.", "published": "2024-06-20 16:59:52", "link": "http://arxiv.org/abs/2406.14497v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math\n  Reasoning by Eight-Fold", "abstract": "Training on model-generated synthetic data is a promising approach for\nfinetuning LLMs, but it remains unclear when it helps or hurts. In this paper,\nwe investigate this question for math reasoning via an empirical study,\nfollowed by building a conceptual understanding of our observations. First, we\nfind that while the typical approach of finetuning a model on synthetic correct\nor positive problem-solution pairs generated by capable models offers modest\nperformance gains, sampling more correct solutions from the finetuned learner\nitself followed by subsequent fine-tuning on this self-generated data\n$\\textbf{doubles}$ the efficiency of the same synthetic problems. At the same\ntime, training on model-generated positives can amplify various spurious\ncorrelations, resulting in flat or even inverse scaling trends as the amount of\ndata increases. Surprisingly, we find that several of these issues can be\naddressed if we also utilize negative responses, i.e., model-generated\nresponses that are deemed incorrect by a final answer verifier. Crucially,\nthese negatives must be constructed such that the training can appropriately\nrecover the utility or advantage of each intermediate step in the negative\nresponse. With this per-step scheme, we are able to attain consistent gains\nover only positive data, attaining performance similar to amplifying the amount\nof synthetic data by $\\mathbf{8 \\times}$. We show that training on per-step\nnegatives can help to unlearn spurious correlations in the positive data, and\nis equivalent to advantage-weighted reinforcement learning (RL), implying that\nit inherits robustness benefits of RL over imitating positive data alone.", "published": "2024-06-20 17:45:54", "link": "http://arxiv.org/abs/2406.14532v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs", "abstract": "Vision Language Models (VLMs) demonstrate remarkable proficiency in\naddressing a wide array of visual questions, which requires strong perception\nand reasoning faculties. Assessing these two competencies independently is\ncrucial for model refinement, despite the inherent difficulty due to the\nintertwined nature of seeing and reasoning in existing VLMs. To tackle this\nissue, we present Prism, an innovative framework designed to disentangle the\nperception and reasoning processes involved in visual question solving. Prism\ncomprises two distinct stages: a perception stage that utilizes a VLM to\nextract and articulate visual information in textual form, and a reasoning\nstage that formulates responses based on the extracted visual information using\na Large Language Model (LLM). This modular design enables the systematic\ncomparison and assessment of both proprietary and open-source VLM for their\nperception and reasoning strengths. Our analytical framework provides several\nvaluable insights, underscoring Prism's potential as a cost-effective solution\nfor vision-language tasks. By combining a streamlined VLM focused on perception\nwith a powerful LLM tailored for reasoning, Prism achieves superior results in\ngeneral vision-language tasks while substantially cutting down on training and\noperational expenses. Quantitative evaluations show that Prism, when configured\nwith a vanilla 2B LLaVA and freely accessible GPT-3.5, delivers performance on\npar with VLMs $10 \\times$ larger on the rigorous multimodal benchmark MMStar.\nThe project is released at: https://github.com/SparksJoe/Prism.", "published": "2024-06-20 17:54:03", "link": "http://arxiv.org/abs/2406.14544v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "GraphReader: Building Graph-based Agent to Enhance Long-Context\n  Abilities of Large Language Models", "abstract": "Long-context capabilities are essential for large language models (LLMs) to\ntackle complex and long-input tasks. Despite numerous efforts made to optimize\nLLMs for long contexts, challenges persist in robustly processing long inputs.\nIn this paper, we introduce GraphReader, a graph-based agent system designed to\nhandle long texts by structuring them into a graph and employing an agent to\nexplore this graph autonomously. Upon receiving a question, the agent first\nundertakes a step-by-step analysis and devises a rational plan. It then invokes\na set of predefined functions to read node content and neighbors, facilitating\na coarse-to-fine exploration of the graph. Throughout the exploration, the\nagent continuously records new insights and reflects on current circumstances\nto optimize the process until it has gathered sufficient information to\ngenerate an answer. Experimental results on the LV-Eval dataset reveal that\nGraphReader, using a 4k context window, consistently outperforms GPT-4-128k\nacross context lengths from 16k to 256k by a large margin. Additionally, our\napproach demonstrates superior performance on four challenging single-hop and\nmulti-hop benchmarks.", "published": "2024-06-20 17:57:51", "link": "http://arxiv.org/abs/2406.14550v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can LLMs Learn by Teaching for Better Reasoning? A Preliminary Study", "abstract": "Teaching to improve student models (e.g., knowledge distillation) is an\nextensively studied methodology in LLMs. However, for humans, teaching improves\nnot only students but also teachers, by fostering more rigorous and clear\nreasoning as well as knowledge building. We ask: Can LLMs also learn by\nteaching (LbT) for better reasoning? If the answer is yes, we can potentially\nunlock the possibility of continuously advancing the models without solely\nrelying on human-produced data or stronger models. In this paper, we provide a\npreliminary exploration on this question. We show that LbT ideas can be\nincorporated into existing LLM training/prompting pipelines and bring\nimprovements. Specifically, we design three methods, each mimicking one of the\nthree levels of LbT: observing students' feedback, learning from the feedback,\nand learning iteratively, with the goals of improving answer accuracy without\ntraining or improving models' inherent capability with fine-tuning. We reveal\nsome findings: (1) Teaching materials that make it easier for students to learn\nhave clearer and more accurate logic when using in-context learning as the\nstudent's \"learning\" method; (2) Weak-to-strong generalization: LbT might help\nimprove strong models by teaching weak models; (3) Diversity in students might\nhelp: teaching multiple students could be better than teaching one student or\nthe teacher itself. We hope that our exploration can inspire future research on\nLbT and more broadly adopting the advanced techniques in education to improve\nLLMs. The code and website are at https://github.com/imagination-research/lbt\nand https://sites.google.com/view/llm-learning-by-teaching.", "published": "2024-06-20 18:00:17", "link": "http://arxiv.org/abs/2406.14629v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TAGLAS: An atlas of text-attributed graph datasets in the era of large\n  graph and language models", "abstract": "In this report, we present TAGLAS, an atlas of text-attributed graph (TAG)\ndatasets and benchmarks. TAGs are graphs with node and edge features\nrepresented in text, which have recently gained wide applicability in training\ngraph-language or graph foundation models. In TAGLAS, we collect and integrate\nmore than 23 TAG datasets with domains ranging from citation graphs to molecule\ngraphs and tasks from node classification to graph question-answering. Unlike\nprevious graph datasets and benchmarks, all datasets in TAGLAS have a unified\nnode and edge text feature format, which allows a graph model to be\nsimultaneously trained and evaluated on multiple datasets from various domains.\nFurther, we provide a standardized, efficient, and simplified way to load all\ndatasets and tasks. We also provide useful utils like text-to-embedding\nconversion, and graph-to-text conversion, which can facilitate different\nevaluation scenarios. Finally, we also provide standard and easy-to-use\nevaluation utils. The project is open-sourced at\nhttps://github.com/JiaruiFeng/TAGLAS and is still under construction. Please\nexpect more datasets/features in the future.", "published": "2024-06-20 19:11:35", "link": "http://arxiv.org/abs/2406.14683v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Depth $F_1$: Improving Evaluation of Cross-Domain Text Classification by\n  Measuring Semantic Generalizability", "abstract": "Recent evaluations of cross-domain text classification models aim to measure\nthe ability of a model to obtain domain-invariant performance in a target\ndomain given labeled samples in a source domain. The primary strategy for this\nevaluation relies on assumed differences between source domain samples and\ntarget domain samples in benchmark datasets. This evaluation strategy fails to\naccount for the similarity between source and target domains, and may mask when\nmodels fail to transfer learning to specific target samples which are highly\ndissimilar from the source domain. We introduce Depth $F_1$, a novel\ncross-domain text classification performance metric. Designed to be\ncomplementary to existing classification metrics such as $F_1$, Depth $F_1$\nmeasures how well a model performs on target samples which are dissimilar from\nthe source domain. We motivate this metric using standard cross-domain text\nclassification datasets and benchmark several recent cross-domain text\nclassification models, with the goal of enabling in-depth evaluation of the\nsemantic generalizability of cross-domain text classification models.", "published": "2024-06-20 19:35:17", "link": "http://arxiv.org/abs/2406.14695v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality\n  Testset designed for LLMs with Psychometrics", "abstract": "Recent advancements in Large Language Models (LLMs) have led to their\nadaptation in various domains as conversational agents. We wonder: can\npersonality tests be applied to these agents to analyze their behavior, similar\nto humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice\nquestions designed to assess the personality of LLMs. TRAIT is built on two\npsychometrically validated small human questionnaires, Big Five Inventory (BFI)\nand Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a\nvariety of real-world scenarios. TRAIT also outperforms existing personality\ntests for LLMs in terms of reliability and validity, achieving the highest\nscores across four key metrics: Content Validity, Internal Validity, Refusal\nRate, and Reliability. Using TRAIT, we reveal two notable insights into\npersonalities of LLMs: 1) LLMs exhibit distinct and consistent personality,\nwhich is highly influenced by their training data (e.g., data used for\nalignment tuning), and 2) current prompting techniques have limited\neffectiveness in eliciting certain traits, such as high psychopathy or low\nconscientiousness, suggesting the need for further research in this direction.", "published": "2024-06-20 19:50:56", "link": "http://arxiv.org/abs/2406.14703v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TTQA-RS- A break-down prompting approach for Multi-hop Table-Text\n  Question Answering with Reasoning and Summarization", "abstract": "Question answering (QA) over tables and text has gained much popularity over\nthe years. Multi-hop table-text QA requires multiple hops between the table and\ntext, making it a challenging QA task. Although several works have attempted to\nsolve the table-text QA task, most involve training the models and requiring\nlabeled data. In this paper, we have proposed a Retrieval Augmented Generation\n(RAG) based model - TTQA-RS: A break-down prompting approach for Multi-hop\nTable-Text Question Answering with Reasoning and Summarization. Our model uses\nan enhanced retriever for table-text information retrieval and uses augmented\nknowledge, including table-text summary with decomposed sub-questions with\nanswers for a reasoning-based table-text QA. Using open-source language models,\nour model outperformed all existing prompting methods for table-text QA tasks\non existing table-text QA datasets, such as HybridQA and OTT-QA's development\nset. Our experiments demonstrate the potential of prompt-based approaches using\nopen-source LLMs. Additionally, by using LLaMA3-70B, our model achieved\nstate-of-the-art performance for prompting-based methods on multi-hop\ntable-text QA.", "published": "2024-06-20 20:55:38", "link": "http://arxiv.org/abs/2406.14732v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Relation Extraction with Fine-Tuned Large Language Models in Retrieval\n  Augmented Generation Frameworks", "abstract": "Information Extraction (IE) is crucial for converting unstructured data into\nstructured formats like Knowledge Graphs (KGs). A key task within IE is\nRelation Extraction (RE), which identifies relationships between entities in\ntext. Various RE methods exist, including supervised, unsupervised, weakly\nsupervised, and rule-based approaches. Recent studies leveraging pre-trained\nlanguage models (PLMs) have shown significant success in this area. In the\ncurrent era dominated by Large Language Models (LLMs), fine-tuning these models\ncan overcome limitations associated with zero-shot LLM prompting-based RE\nmethods, especially regarding domain adaptation challenges and identifying\nimplicit relations between entities in sentences. These implicit relations,\nwhich cannot be easily extracted from a sentence's dependency tree, require\nlogical inference for accurate identification. This work explores the\nperformance of fine-tuned LLMs and their integration into the Retrieval\nAugmented-based (RAG) RE approach to address the challenges of identifying\nimplicit relations at the sentence level, particularly when LLMs act as\ngenerators within the RAG framework. Empirical evaluations on the TACRED,\nTACRED-Revisited (TACREV), Re-TACRED, and SemEVAL datasets show significant\nperformance improvements with fine-tuned LLMs, including Llama2-7B, Mistral-7B,\nand T5 (Large). Notably, our approach achieves substantial gains on SemEVAL,\nwhere implicit relations are common, surpassing previous results on this\ndataset. Additionally, our method outperforms previous works on TACRED, TACREV,\nand Re-TACRED, demonstrating exceptional performance across diverse evaluation\nscenarios.", "published": "2024-06-20 21:27:57", "link": "http://arxiv.org/abs/2406.14745v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Adapter-Based Unified Model for Multiple Spoken Language Processing\n  Tasks", "abstract": "Self-supervised learning models have revolutionized the field of speech\nprocessing. However, the process of fine-tuning these models on downstream\ntasks requires substantial computational resources, particularly when dealing\nwith multiple speech-processing tasks. In this paper, we explore the potential\nof adapter-based fine-tuning in developing a unified model capable of\neffectively handling multiple spoken language processing tasks. The tasks we\ninvestigate are Automatic Speech Recognition, Phoneme Recognition, Intent\nClassification, Slot Filling, and Spoken Emotion Recognition. We validate our\napproach through a series of experiments on the SUPERB benchmark, and our\nresults indicate that adapter-based fine-tuning enables a single\nencoder-decoder model to perform multiple speech processing tasks with an\naverage improvement of 18.4% across the five target tasks while staying\nefficient in terms of parameter updates.", "published": "2024-06-20 21:39:04", "link": "http://arxiv.org/abs/2406.14747v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Learn-Then-Reason Model Towards Generalization in Knowledge Base\n  Question Answering", "abstract": "Large-scale knowledge bases (KBs) like Freebase and Wikidata house millions\nof structured knowledge. Knowledge Base Question Answering (KBQA) provides a\nuser-friendly way to access these valuable KBs via asking natural language\nquestions. In order to improve the generalization capabilities of KBQA models,\nextensive research has embraced a retrieve-then-reason framework to retrieve\nrelevant evidence for logical expression generation. These multi-stage efforts\nprioritize acquiring external sources but overlook the incorporation of new\nknowledge into their model parameters. In effect, even advanced language models\nand retrievers have knowledge boundaries, thereby limiting the generalization\ncapabilities of previous KBQA models. Therefore, this paper develops KBLLaMA,\nwhich follows a learn-then-reason framework to inject new KB knowledge into a\nlarge language model for flexible end-to-end KBQA. At the core of KBLLaMA, we\nstudy (1) how to organize new knowledge about KBQA and (2) how to facilitate\nthe learning of the organized knowledge. Extensive experiments on various KBQA\ngeneralization tasks showcase the state-of-the-art performance of KBLLaMA.\nEspecially on the general benchmark GrailQA and domain-specific benchmark\nBio-chemical, KBLLaMA respectively derives a performance gain of up to 3.8% and\n9.8% compared to the baselines.", "published": "2024-06-20 22:22:41", "link": "http://arxiv.org/abs/2406.14763v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating RAG-Fusion with RAGElo: an Automated Elo-based Framework", "abstract": "Challenges in the automated evaluation of Retrieval-Augmented Generation\n(RAG) Question-Answering (QA) systems include hallucination problems in\ndomain-specific knowledge and the lack of gold standard benchmarks for company\ninternal tasks. This results in difficulties in evaluating RAG variations, like\nRAG-Fusion (RAGF), in the context of a product QA task at Infineon\nTechnologies. To solve these problems, we propose a comprehensive evaluation\nframework, which leverages Large Language Models (LLMs) to generate large\ndatasets of synthetic queries based on real user queries and in-domain\ndocuments, uses LLM-as-a-judge to rate retrieved documents and answers,\nevaluates the quality of answers, and ranks different variants of\nRetrieval-Augmented Generation (RAG) agents with RAGElo's automated Elo-based\ncompetition. LLM-as-a-judge rating of a random sample of synthetic queries\nshows a moderate, positive correlation with domain expert scoring in relevance,\naccuracy, completeness, and precision. While RAGF outperformed RAG in Elo\nscore, a significance analysis against expert annotations also shows that RAGF\nsignificantly outperforms RAG in completeness, but underperforms in precision.\nIn addition, Infineon's RAGF assistant demonstrated slightly higher performance\nin document relevance based on MRR@5 scores. We find that RAGElo positively\naligns with the preferences of human annotators, though due caution is still\nrequired. Finally, RAGF's approach leads to more complete answers based on\nexpert annotations and better answers overall based on RAGElo's evaluation\ncriteria.", "published": "2024-06-20 23:20:34", "link": "http://arxiv.org/abs/2406.14783v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Understanding Finetuning for Factual Knowledge Extraction", "abstract": "In this work, we study the impact of QA fine-tuning data on downstream\nfactuality. We show that fine-tuning on lesser-known facts that are poorly\nstored during pretraining yields significantly worse factuality than\nfine-tuning on well-known facts, even when all facts are seen during\npretraining. We prove this phenomenon theoretically, showing that training on\nlesser-known facts can lead the model to ignore subject entity names and\ninstead output a generic plausible response even when the relevant factual\nknowledge is encoded in the model. On three question answering benchmarks\n(PopQA, Entity Questions, and MMLU) and two language models (Llama-2-7B and\nMistral-7B), we find that (i) finetuning on a completely factual but\nlesser-known subset of the data deteriorates downstream factuality (5-10%) and\n(ii) finetuning on a subset of better-known examples matches or outperforms\nfinetuning on the entire dataset. Ultimately, our results shed light on the\ninteraction between pretrained knowledge and finetuning data and demonstrate\nthe importance of taking into account how facts are stored in the pretrained\nmodel when fine-tuning for knowledge-intensive tasks.", "published": "2024-06-20 23:27:06", "link": "http://arxiv.org/abs/2406.14785v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PKU-SafeRLHF: Towards Multi-Level Safety Alignment for LLMs with Human\n  Preference", "abstract": "In this work, we introduce the PKU-SafeRLHF dataset, designed to promote\nresearch on safety alignment in large language models (LLMs). As a sibling\nproject to SafeRLHF and BeaverTails, we separate annotations of helpfulness and\nharmlessness for question-answering pairs, providing distinct perspectives on\nthese coupled attributes. Overall, we provide 44.6k refined prompts and 265k\nquestion-answer pairs with safety meta-labels for 19 harm categories and three\nseverity levels ranging from minor to severe, with answers generated by\nLlama-family models. Based on this, we collected 166.8k preference data,\nincluding dual-preference (helpfulness and harmlessness decoupled) and\nsingle-preference data (trade-off the helpfulness and harmlessness from\nscratch), respectively. Using the large-scale annotation data, we further train\nseverity-sensitive moderation for the risk control of LLMs and safety-centric\nRLHF algorithms for the safety alignment of LLMs. We believe this dataset will\nbe a valuable resource for the community, aiding in the safe deployment of\nLLMs.", "published": "2024-06-20 18:37:36", "link": "http://arxiv.org/abs/2406.15513v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Validation of a new, minimally-invasive, software smartphone device to\n  predict sleep apnea and its severity: transversal study", "abstract": "Obstructive sleep apnea (OSA) is frequent and responsible for cardiovascular\ncomplications and excessive daytime sleepiness. It is underdiagnosed due to the\ndifficulty to access the gold standard for diagnosis, polysomnography (PSG).\nAlternative methods using smartphone sensors could be useful to increase\ndiagnosis. The objective is to assess the performances of Apneal, an\napplication that records the sound using a smartphone's microphone and\nmovements thanks to a smartphone's accelerometer and gyroscope, to estimate\npatients' AHI. In this article, we perform a monocentric proof-of-concept study\nwith a first manual scoring step, and then an automatic detection of\nrespiratory events from the recorded signals using a sequential deep-learning\nmodel which was released internally at Apneal at the end of 2022 (version 0.1\nof Apneal automatic scoring of respiratory events), in adult patients during\nin-hospital polysomnography.46 patients (women 34 per cent, mean BMI 28.7 kg\nper m2) were included. For AHI superior to 15, sensitivity of manual scoring\nwas 0.91, and positive predictive value (PPV) 0.89. For AHI superior to 30,\nsensitivity was 0.85, PPV 0.94. We obtained an AUC-ROC of 0.85 and an AUC-PR of\n0.94 for the identification of AHI superior to 15, and AUC-ROC of 0.95 and\nAUC-PR of 0.93 for AHI superior to 30. Promising results are obtained for the\nautomatic annotations of events.This article shows that manual scoring of\nsmartphone-based signals is possible and accurate compared to PSG-based\nscorings. Automatic scoring method based on a deep learning model provides\npromising results. A larger multicentric validation study, involving subjects\nwith different SAHS severity is required to confirm these results.", "published": "2024-06-20 14:36:15", "link": "http://arxiv.org/abs/2406.16953v1", "categories": ["eess.SP", "cs.CL"], "primary_category": "eess.SP"}
{"title": "The Use of Multimodal Large Language Models to Detect Objects from\n  Thermal Images: Transportation Applications", "abstract": "The integration of thermal imaging data with Multimodal Large Language Models\n(MLLMs) constitutes an exciting opportunity for improving the safety and\nfunctionality of autonomous driving systems and many Intelligent Transportation\nSystems (ITS) applications. This study investigates whether MLLMs can\nunderstand complex images from RGB and thermal cameras and detect objects\ndirectly. Our goals were to 1) assess the ability of the MLLM to learn from\ninformation from various sets, 2) detect objects and identify elements in\nthermal cameras, 3) determine whether two independent modality images show the\nsame scene, and 4) learn all objects using different modalities. The findings\nshowed that both GPT-4 and Gemini were effective in detecting and classifying\nobjects in thermal images. Similarly, the Mean Absolute Percentage Error (MAPE)\nfor pedestrian classification was 70.39% and 81.48%, respectively. Moreover,\nthe MAPE for bike, car, and motorcycle detection were 78.4%, 55.81%, and\n96.15%, respectively. Gemini produced MAPE of 66.53%, 59.35% and 78.18%\nrespectively. This finding further demonstrates that MLLM can identify thermal\nimages and can be employed in advanced imaging automation technologies for ITS\napplications.", "published": "2024-06-20 00:05:10", "link": "http://arxiv.org/abs/2406.13898v1", "categories": ["cs.CV", "cs.CL", "cs.CY"], "primary_category": "cs.CV"}
{"title": "Generative AI for Enhancing Active Learning in Education: A Comparative\n  Study of GPT-3.5 and GPT-4 in Crafting Customized Test Questions", "abstract": "This study investigates how LLMs, specifically GPT-3.5 and GPT-4, can develop\ntailored questions for Grade 9 math, aligning with active learning principles.\nBy utilizing an iterative method, these models adjust questions based on\ndifficulty and content, responding to feedback from a simulated 'student'\nmodel. A novel aspect of the research involved using GPT-4 as a 'teacher' to\ncreate complex questions, with GPT-3.5 as the 'student' responding to these\nchallenges. This setup mirrors active learning, promoting deeper engagement.\nThe findings demonstrate GPT-4's superior ability to generate precise,\nchallenging questions and notable improvements in GPT-3.5's ability to handle\nmore complex problems after receiving instruction from GPT-4. These results\nunderscore the potential of LLMs to mimic and enhance active learning\nscenarios, offering a promising path for AI in customized education. This\nresearch contributes to understanding how AI can support personalized learning\nexperiences, highlighting the need for further exploration in various\neducational contexts", "published": "2024-06-20 00:25:43", "link": "http://arxiv.org/abs/2406.13903v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal\n  Documents", "abstract": "Recent advancements in Large Multimodal Models (LMMs) have leveraged\nextensive multimodal datasets to enhance capabilities in complex\nknowledge-driven tasks. However, persistent challenges in perceptual and\nreasoning errors limit their efficacy, particularly in interpreting intricate\nvisual data and deducing multimodal relationships. Addressing these issues, we\nintroduce a novel dataset format, PIN (Paired and INterleaved multimodal\ndocuments), designed to significantly improve both the depth and breadth of\nmultimodal training. The PIN format is built on three foundational principles:\nknowledge intensity, scalability, and support for diverse training modalities.\nThis innovative format combines markdown files and comprehensive images to\nenrich training data with a dense knowledge structure and versatile training\nstrategies. We present PIN-14M, an open-source dataset comprising 14 million\nsamples derived from a diverse range of Chinese and English sources, tailored\nto include complex web and scientific content. This dataset is constructed\nmeticulously to ensure data quality and ethical integrity, aiming to facilitate\nadvanced training strategies and improve model robustness against common\nmultimodal training pitfalls. Our initial results, forming the basis of this\ntechnical report, suggest significant potential for the PIN format in refining\nLMM performance, with plans for future expansions and detailed evaluations of\nits impact on model capabilities.", "published": "2024-06-20 01:43:08", "link": "http://arxiv.org/abs/2406.13923v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.AI"}
{"title": "Large Language Models are Skeptics: False Negative Problem of\n  Input-conflicting Hallucination", "abstract": "In this paper, we identify a new category of bias that induces\ninput-conflicting hallucinations, where large language models (LLMs) generate\nresponses inconsistent with the content of the input context. This issue we\nhave termed the false negative problem refers to the phenomenon where LLMs are\npredisposed to return negative judgments when assessing the correctness of a\nstatement given the context. In experiments involving pairs of statements that\ncontain the same information but have contradictory factual directions, we\nobserve that LLMs exhibit a bias toward false negatives. Specifically, the\nmodel presents greater overconfidence when responding with False. Furthermore,\nwe analyze the relationship between the false negative problem and context and\nquery rewriting and observe that both effectively tackle false negatives in\nLLMs.", "published": "2024-06-20 01:53:25", "link": "http://arxiv.org/abs/2406.13929v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CityBench: Evaluating the Capabilities of Large Language Models for\n  Urban Tasks", "abstract": "Recently, large language models (LLMs) with extensive general knowledge and\npowerful reasoning abilities have seen rapid development and widespread\napplication. A systematic and reliable evaluation of LLMs or vision-language\nmodel (VLMs) is a crucial step in applying and developing them for various\nfields. There have been some early explorations about the usability of LLMs for\nlimited urban tasks, but a systematic and scalable evaluation benchmark is\nstill lacking. The challenge in constructing a systematic evaluation benchmark\nfor urban research lies in the diversity of urban data, the complexity of\napplication scenarios and the highly dynamic nature of the urban environment.\nIn this paper, we design CityBench, an interactive simulator based evaluation\nplatform, as the first systematic benchmark for evaluating the capabilities of\nLLMs for diverse tasks in urban research. First, we build CityData to integrate\nthe diverse urban data and CitySimu to simulate fine-grained urban dynamics.\nBased on CityData and CitySimu, we design 8 representative urban tasks in 2\ncategories of perception-understanding and decision-making as the CityBench.\nWith extensive results from 30 well-known LLMs and VLMs in 13 cities around the\nworld, we find that advanced LLMs and VLMs can achieve competitive performance\nin diverse urban tasks requiring commonsense and semantic understanding\nabilities, e.g., understanding the human dynamics and semantic inference of\nurban images. Meanwhile, they fail to solve the challenging urban tasks\nrequiring professional knowledge and high-level reasoning abilities, e.g.,\ngeospatial prediction and traffic control task. These observations provide\nvaluable perspectives for utilizing and developing LLMs in the future. Codes\nare openly accessible via https://github.com/tsinghua-fib-lab/CityBench.", "published": "2024-06-20 02:25:07", "link": "http://arxiv.org/abs/2406.13945v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models", "abstract": "Large language models(LLMs) with powerful language generation and reasoning\ncapabilities have already achieved success in many domains, e.g., math and code\ngeneration. However, due to the lacking of physical world's corpus and\nknowledge during training, they usually fail to solve many real-life tasks in\nthe urban space. In this paper, we propose CityGPT, a systematic framework for\nenhancing the capability of LLMs on understanding urban space and solving the\nrelated urban tasks by building a city-scale world model in the model. First,\nwe construct a diverse instruction tuning dataset CityInstruction for injecting\nurban knowledge and enhancing spatial reasoning capability effectively. By\nusing a mixture of CityInstruction and general instruction data, we fine-tune\nvarious LLMs (e.g., ChatGLM3-6B, Qwen1.5 and LLama3 series) to enhance their\ncapability without sacrificing general abilities. To further validate the\neffectiveness of proposed methods, we construct a comprehensive benchmark\nCityEval to evaluate the capability of LLMs on diverse urban scenarios and\nproblems. Extensive evaluation results demonstrate that small LLMs trained with\nCityInstruction can achieve competitive performance with commercial LLMs in the\ncomprehensive evaluation of CityEval. The source codes are openly accessible to\nthe research community via https://github.com/tsinghua-fib-lab/CityGPT.", "published": "2024-06-20 02:32:16", "link": "http://arxiv.org/abs/2406.13948v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Information Guided Regularization for Fine-tuning Language Models", "abstract": "The pretraining-fine-tuning paradigm has been the de facto strategy for\ntransfer learning in modern language modeling. With the understanding that task\nadaptation in LMs is often a function of parameters shared across tasks, we\nargue that a more surgical approach to regularization needs to exist for\nsmoother transfer learning. Towards this end, we investigate how the\npretraining loss landscape is affected by these task-sensitive parameters\nthrough an information-theoretic lens. We then leverage the findings from our\ninvestigations to devise a novel approach to dropout for improved model\nregularization and better downstream generalization. This approach, named\nguided dropout, is both task & architecture agnostic and adds no computational\noverhead to the fine-tuning process. Through empirical evaluations, we showcase\nthat our approach to regularization yields consistently better performance,\neven in scenarios of data paucity, compared to standardized baselines.", "published": "2024-06-20 05:18:37", "link": "http://arxiv.org/abs/2406.14005v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HIGHT: Hierarchical Graph Tokenization for Graph-Language Alignment", "abstract": "Recently there has been a surge of interest in extending the success of large\nlanguage models (LLMs) to graph modality, such as social networks and\nmolecules. As LLMs are predominantly trained with 1D text data, most existing\napproaches adopt a graph neural network to represent a graph as a series of\nnode tokens and feed these tokens to LLMs for graph-language alignment. Despite\nachieving some successes, existing approaches have overlooked the hierarchical\nstructures that are inherent in graph data. Especially, in molecular graphs,\nthe high-order structural information contains rich semantics of molecular\nfunctional groups, which encode crucial biochemical functionalities of the\nmolecules. We establish a simple benchmark showing that neglecting the\nhierarchical information in graph tokenization will lead to subpar\ngraph-language alignment and severe hallucination in generated outputs. To\naddress this problem, we propose a novel strategy called HIerarchical GrapH\nTokenization (HIGHT). HIGHT employs a hierarchical graph tokenizer that\nextracts and encodes the hierarchy of node, motif, and graph levels of\ninformative tokens to improve the graph perception of LLMs. HIGHT also adopts\nan augmented graph-language supervised fine-tuning dataset, enriched with the\nhierarchical graph information, to further enhance the graph-language\nalignment. Extensive experiments on 7 molecule-centric benchmarks confirm the\neffectiveness of HIGHT in reducing hallucination by 40%, as well as significant\nimprovements in various molecule-language downstream tasks.", "published": "2024-06-20 06:37:35", "link": "http://arxiv.org/abs/2406.14021v1", "categories": ["cs.CL", "cs.LG", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "Demystifying Language Model Forgetting with Low-rank Example\n  Associations", "abstract": "Large Language models (LLMs) suffer from forgetting of upstream data when\nfine-tuned. Despite efforts on mitigating forgetting, few have investigated\nwhether, and how forgotten upstream examples are dependent on newly learned\ntasks. Insights on such dependencies enable efficient and targeted mitigation\nof forgetting. In this paper, we empirically analyze forgetting that occurs in\n$N$ upstream examples of language modeling or instruction-tuning after\nfine-tuning LLMs on one of $M$ new tasks, visualized in $M\\times N$ matrices.\nWe show that the matrices are often well-approximated with low-rank matrices,\nindicating the dominance of simple associations between the learned tasks and\nforgotten upstream examples. Leveraging the analysis, we predict forgetting of\nupstream examples when fine-tuning on unseen tasks with matrix completion over\nthe empirical associations. This enables fast identification of most forgotten\nexamples without expensive inference on the entire upstream data. The approach,\ndespite simplicity, outperforms prior approaches that learn semantic\nrelationships of learned tasks and upstream examples with LMs for predicting\nforgetting. We demonstrate the practical utility of our analysis by showing\nstatistically significantly reduced forgetting as we upweight predicted\nexamples for replay at fine-tuning. Project page:\nhttps://inklab.usc.edu/lm-forgetting-prediction/", "published": "2024-06-20 06:46:23", "link": "http://arxiv.org/abs/2406.14026v5", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Towards Infinite-Long Prefix in Transformer", "abstract": "Prompting and context-based fine-tuning methods, which we call Prefix\nLearning, have been proposed to enhance the performance of language models on\nvarious downstream tasks. They are empirically efficient and effective,\nmatching the performance of full parameter fine-tuning, but the theoretical\nunderstandings are limited. In this paper, we aim to address this limitation by\nstudying their ability from the perspective of prefix length. In particular, we\nprovide a convergence guarantee for training an ultra-long prefix in a stylized\nsetting using the Neural Tangent Kernel (NTK) framework. Based on this strong\ntheoretical guarantee, we design and implement an algorithm that only needs to\nintroduce and fine-tune a few extra trainable parameters instead of an\ninfinite-long prefix in each layer of a transformer, and can approximate the\nprefix attention to a guaranteed polynomial-small error. Preliminary\nexperimental results on vision, natural language, and math data show that our\nmethod achieves superior or competitive performance compared to existing\nmethods like full parameters fine-tuning, P-Tuning V2, and LoRA. This\ndemonstrates our method is promising for parameter-efficient fine-tuning. Our\ncode can be found at\n\\url{https://github.com/ChristianYang37/chiwun/tree/main/src/NTK-Attention}.", "published": "2024-06-20 06:56:35", "link": "http://arxiv.org/abs/2406.14036v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CryptoGPT: a 7B model rivaling GPT-4 in the task of analyzing and\n  classifying real-time financial news", "abstract": "CryptoGPT: a 7B model competing with GPT-4 in a specific task -- The Impact\nof Automatic Annotation and Strategic Fine-Tuning via QLoRAIn this article, we\npresent a method aimed at refining a dedicated LLM of reasonable quality with\nlimited resources in an industrial setting via CryptoGPT. It is an LLM designed\nfor financial news analysis for the cryptocurrency market in real-time. This\nproject was launched in an industrial context. This model allows not only for\nthe classification of financial information but also for providing\ncomprehensive analysis. We refined different LLMs of the same size such as\nMistral-7B and LLama-7B using semi-automatic annotation and compared them with\nvarious LLMs such as GPT-3.5 and GPT-4. Our goal is to find a balance among\nseveral needs: 1. Protecting data (by avoiding their transfer to external\nservers), 2. Limiting annotation cost and time, 3. Controlling the model's size\n(to manage deployment costs), and 4. Maintaining better analysis quality.", "published": "2024-06-20 06:59:46", "link": "http://arxiv.org/abs/2406.14039v1", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.NE"], "primary_category": "cs.AI"}
{"title": "ReaLHF: Optimized RLHF Training for Large Language Models through\n  Parameter Reallocation", "abstract": "Reinforcement Learning from Human Feedback (RLHF) stands as a pivotal\ntechnique in empowering large language model (LLM) applications. Since RLHF\ninvolves diverse computational workloads and intricate dependencies among\nmultiple LLMs, directly adopting parallelization techniques from supervised\ntraining can result in sub-optimal performance. To overcome this limitation, we\npropose a novel approach named parameter ReaLlocation, which dynamically\nredistributes LLM parameters in the cluster and adapts parallelization\nstrategies during training. Building upon this idea, we introduce ReaLHF, a\npioneering system capable of automatically discovering and running efficient\nexecution plans for RLHF training given the desired algorithmic and hardware\nconfigurations. ReaLHF formulates the execution plan for RLHF as an augmented\ndataflow graph. Based on this formulation, ReaLHF employs a tailored search\nalgorithm with a lightweight cost estimator to discover an efficient execution\nplan. Subsequently, the runtime engine deploys the selected plan by effectively\nparallelizing computations and redistributing parameters. We evaluate ReaLHF on\nthe LLaMA-2 models with up to $4\\times70$ billion parameters and 128 GPUs. The\nexperiment results showcase ReaLHF's substantial speedups of $2.0-10.6\\times$\ncompared to baselines. Furthermore, the execution plans generated by ReaLHF\nexhibit an average of $26\\%$ performance improvement over heuristic approaches\nbased on Megatron-LM. The source code of ReaLHF is publicly available at\nhttps://github.com/openpsi-project/ReaLHF .", "published": "2024-06-20 08:04:07", "link": "http://arxiv.org/abs/2406.14088v1", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.DC"}
{"title": "Towards Event-oriented Long Video Understanding", "abstract": "With the rapid development of video Multimodal Large Language Models (MLLMs),\nnumerous benchmarks have been proposed to assess their video understanding\ncapability. However, due to the lack of rich events in the videos, these\ndatasets may suffer from the short-cut bias that the answers can be deduced\nfrom a few frames, without the need to watch the entire video. To address this\nissue, we introduce Event-Bench, an event-oriented long video understanding\nbenchmark built on existing datasets and human annotations. Event-Bench\nincludes six event-related tasks and 2,190 test instances to comprehensively\nevaluate video event understanding ability. Additionally, we propose Video\nInstruction Merging~(VIM), a cost-effective method that enhances video MLLMs\nusing merged, event-intensive video instructions, addressing the scarcity of\nhuman-annotated, event-intensive data. Extensive experiments show that the\nbest-performing model, GPT-4o, achieves an overall accuracy of 53.33,\nsignificantly outperforming the best open-source model by 41.42%. Leveraging an\neffective instruction synthesis method and an adaptive model architecture, VIM\nsurpasses both state-of-the-art open-source models and GPT-4V on the\nEvent-Bench. All code, data, and models are publicly available at\nhttps://github.com/RUCAIBox/Event-Bench.", "published": "2024-06-20 09:14:19", "link": "http://arxiv.org/abs/2406.14129v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Finding Safety Neurons in Large Language Models", "abstract": "Large language models (LLMs) excel in various capabilities but also pose\nsafety risks such as generating harmful content and misinformation, even after\nsafety alignment. In this paper, we explore the inner mechanisms of safety\nalignment from the perspective of mechanistic interpretability, focusing on\nidentifying and analyzing safety neurons within LLMs that are responsible for\nsafety behaviors. We propose generation-time activation contrasting to locate\nthese neurons and dynamic activation patching to evaluate their causal effects.\nExperiments on multiple recent LLMs show that: (1) Safety neurons are sparse\nand effective. We can restore $90$% safety performance with intervention only\non about $5$% of all the neurons. (2) Safety neurons encode transferrable\nmechanisms. They exhibit consistent effectiveness on different red-teaming\ndatasets. The finding of safety neurons also interprets \"alignment tax\". We\nobserve that the identified key neurons for safety and helpfulness\nsignificantly overlap, but they require different activation patterns of the\nshared neurons. Furthermore, we demonstrate an application of safety neurons in\ndetecting unsafe outputs before generation. Our findings may promote further\nresearch on understanding LLM alignment. The source codes will be publicly\nreleased to facilitate future research.", "published": "2024-06-20 09:35:22", "link": "http://arxiv.org/abs/2406.14144v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Watching the Watchers: A Comparative Fairness Audit of Cloud-based\n  Content Moderation Services", "abstract": "Online platforms face the challenge of moderating an ever-increasing volume\nof content, including harmful hate speech. In the absence of clear legal\ndefinitions and a lack of transparency regarding the role of algorithms in\nshaping decisions on content moderation, there is a critical need for external\naccountability. Our study contributes to filling this gap by systematically\nevaluating four leading cloud-based content moderation services through a\nthird-party audit, highlighting issues such as biases against minorities and\nvulnerable groups that may arise through over-reliance on these services. Using\na black-box audit approach and four benchmark data sets, we measure performance\nin explicit and implicit hate speech detection as well as counterfactual\nfairness through perturbation sensitivity analysis and present disparities in\nperformance for certain target identity groups and data sets. Our analysis\nreveals that all services had difficulties detecting implicit hate speech,\nwhich relies on more subtle and codified messages. Moreover, our results point\nto the need to remove group-specific bias. It seems that biases towards some\ngroups, such as Women, have been mostly rectified, while biases towards other\ngroups, such as LGBTQ+ and PoC remain.", "published": "2024-06-20 09:52:10", "link": "http://arxiv.org/abs/2406.14154v1", "categories": ["cs.CY", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CY"}
{"title": "DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval\n  Augmented Generation", "abstract": "Retrieval Augmented Generation (RAG) is widely employed to ground responses\nto queries on domain-specific documents. But do RAG implementations leave out\nimportant information when answering queries that need an integrated analysis\nof information (e.g., Tell me good news in the stock market today.)? To address\nthese concerns, RAG developers need to annotate information retrieval (IR) data\nfor their domain of interest, which is challenging because (1) domain-specific\nqueries usually need nuanced definitions of relevance beyond shallow semantic\nrelevance; and (2) human or GPT-4 annotation is costly and cannot cover all\n(query, document) pairs (i.e., annotation selection bias), thus harming the\neffectiveness in evaluating IR recall. To address these challenges, we propose\nDIRAS (Domain-specific Information Retrieval Annotation with Scalability), a\nmanual-annotation-free schema that fine-tunes open-sourced LLMs to consider\nnuanced relevance definition and annotate (partial) relevance labels with\ncalibrated relevance scores. Extensive evaluation shows that DIRAS enables\nsmaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking\nunseen (query, document) pairs, and is helpful for real-world RAG development.\nAll code, LLM generations, and human annotations can be found in\n\\url{https://github.com/EdisonNi-hku/DIRAS}.", "published": "2024-06-20 10:04:09", "link": "http://arxiv.org/abs/2406.14162v4", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "SimulSeamless: FBK at IWSLT 2024 Simultaneous Speech Translation", "abstract": "This paper describes the FBK's participation in the Simultaneous Translation\nEvaluation Campaign at IWSLT 2024. For this year's submission in the\nspeech-to-text translation (ST) sub-track, we propose SimulSeamless, which is\nrealized by combining AlignAtt and SeamlessM4T in its medium configuration. The\nSeamlessM4T model is used \"off-the-shelf\" and its simultaneous inference is\nenabled through the adoption of AlignAtt, a SimulST policy based on\ncross-attention that can be applied without any retraining or adaptation of the\nunderlying model for the simultaneous task. We participated in all the Shared\nTask languages (English->{German, Japanese, Chinese}, and Czech->English),\nachieving acceptable or even better results compared to last year's\nsubmissions. SimulSeamless, covering more than 143 source languages and 200\ntarget languages, is released at: https://github.com/hlt-mt/FBK-fairseq/.", "published": "2024-06-20 10:34:46", "link": "http://arxiv.org/abs/2406.14177v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Temporal Knowledge Graph Question Answering: A Survey", "abstract": "Knowledge Base Question Answering (KBQA) has been a long-standing field to\nanswer questions based on knowledge bases. Recently, the evolving dynamics of\nknowledge have attracted a growing interest in Temporal Knowledge Graph\nQuestion Answering (TKGQA), an emerging task to answer temporal questions.\nHowever, this field grapples with ambiguities in defining temporal questions\nand lacks a systematic categorization of existing methods for TKGQA. In\nresponse, this paper provides a thorough survey from two perspectives: the\ntaxonomy of temporal questions and the methodological categorization for TKGQA.\nSpecifically, we first establish a detailed taxonomy of temporal questions\nengaged in prior studies. Subsequently, we provide a comprehensive review of\nTKGQA techniques of two categories: semantic parsing-based and TKG\nembedding-based. Building on this review, the paper outlines potential research\ndirections aimed at advancing the field of TKGQA. This work aims to serve as a\ncomprehensive reference for TKGQA and to stimulate further research.", "published": "2024-06-20 10:51:06", "link": "http://arxiv.org/abs/2406.14191v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Raising the Bar: Investigating the Values of Large Language Models via\n  Generative Evolving Testing", "abstract": "Warning: Contains harmful model outputs.\n  Despite significant advancements, the propensity of Large Language Models\n(LLMs) to generate harmful and unethical content poses critical challenges.\nMeasuring value alignment of LLMs becomes crucial for their regulation and\nresponsible deployment. Although numerous benchmarks have been constructed to\nassess social bias, toxicity, and ethical issues in LLMs, those static\nbenchmarks suffer from evaluation chronoeffect, in which, as models rapidly\nevolve, existing benchmarks may leak into training data or become saturated,\noverestimating ever-developing LLMs. To tackle this problem, we propose GETA, a\nnovel generative evolving testing approach based on adaptive testing methods in\nmeasurement theory. Unlike traditional adaptive testing methods that rely on a\nstatic test item pool, GETA probes the underlying moral boundaries of LLMs by\ndynamically generating test items tailored to model capability. GETA co-evolves\nwith LLMs by learning a joint distribution of item difficulty and model value\nconformity, thus effectively addressing evaluation chronoeffect. We evaluated\nvarious popular LLMs with GETA and demonstrated that 1) GETA can dynamically\ncreate difficulty-tailored test items and 2) GETA's evaluation results are more\nconsistent with models' performance on unseen OOD and i.i.d. items, laying the\ngroundwork for future evaluation paradigms.", "published": "2024-06-20 11:51:00", "link": "http://arxiv.org/abs/2406.14230v3", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "QuST-LLM: Integrating Large Language Models for Comprehensive Spatial\n  Transcriptomics Analysis", "abstract": "In this paper, we introduce QuST-LLM, an innovative extension of QuPath that\nutilizes the capabilities of large language models (LLMs) to analyze and\ninterpret spatial transcriptomics (ST) data. In addition to simplifying the\nintricate and high-dimensional nature of ST data by offering a comprehensive\nworkflow that includes data loading, region selection, gene expression\nanalysis, and functional annotation, QuST-LLM employs LLMs to transform complex\nST data into understandable and detailed biological narratives based on gene\nontology annotations, thereby significantly improving the interpretability of\nST data. Consequently, users can interact with their own ST data using natural\nlanguage. Hence, QuST-LLM provides researchers with a potent functionality to\nunravel the spatial and functional complexities of tissues, fostering novel\ninsights and advancements in biomedical research. QuST-LLM is a part of QuST\nproject. The source code is hosted on GitHub and documentation is available at\n(https://github.com/huangch/qust).", "published": "2024-06-20 13:37:10", "link": "http://arxiv.org/abs/2406.14307v2", "categories": ["q-bio.GN", "cs.CL", "cs.CV"], "primary_category": "q-bio.GN"}
{"title": "The Fire Thief Is Also the Keeper: Balancing Usability and Privacy in\n  Prompts", "abstract": "The rapid adoption of online chatbots represents a significant advancement in\nartificial intelligence. However, this convenience brings considerable privacy\nconcerns, as prompts can inadvertently contain sensitive information exposed to\nlarge language models (LLMs). Limited by high computational costs, reduced task\nusability, and excessive system modifications, previous works based on local\ndeployment, embedding perturbation, and homomorphic encryption are inapplicable\nto online prompt-based LLM applications.\n  To address these issues, this paper introduces Prompt Privacy Sanitizer\n(i.e., ProSan), an end-to-end prompt privacy protection framework that can\nproduce anonymized prompts with contextual privacy removed while maintaining\ntask usability and human readability. It can also be seamlessly integrated into\nthe online LLM service pipeline. To achieve high usability and dynamic\nanonymity, ProSan flexibly adjusts its protection targets and strength based on\nthe importance of the words and the privacy leakage risk of the prompts.\nAdditionally, ProSan is capable of adapting to diverse computational resource\nconditions, ensuring privacy protection even for mobile devices with limited\ncomputing power. Our experiments demonstrate that ProSan effectively removes\nprivate information across various tasks, including question answering, text\nsummarization, and code generation, with minimal reduction in task performance.", "published": "2024-06-20 13:52:25", "link": "http://arxiv.org/abs/2406.14318v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Mind the Privacy Unit! User-Level Differential Privacy for Language\n  Model Fine-Tuning", "abstract": "Large language models (LLMs) have emerged as powerful tools for tackling\ncomplex tasks across diverse domains, but they also raise privacy concerns when\nfine-tuned on sensitive data due to potential memorization. While differential\nprivacy (DP) offers a promising solution by ensuring models are 'almost\nindistinguishable' with or without any particular privacy unit, current\nevaluations on LLMs mostly treat each example (text record) as the privacy\nunit. This leads to uneven user privacy guarantees when contributions per user\nvary. We therefore study user-level DP motivated by applications where it\nnecessary to ensure uniform privacy protection across users. We present a\nsystematic evaluation of user-level DP for LLM fine-tuning on natural language\ngeneration tasks. Focusing on two mechanisms for achieving user-level DP\nguarantees, Group Privacy and User-wise DP-SGD, we investigate design choices\nlike data selection strategies and parameter tuning for the best\nprivacy-utility tradeoff.", "published": "2024-06-20 13:54:32", "link": "http://arxiv.org/abs/2406.14322v3", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The neural correlates of logical-mathematical symbol systems processing\n  resemble that of spatial cognition more than natural language processing", "abstract": "The ability to manipulate logical-mathematical symbols (LMS), encompassing\ntasks such as calculation, reasoning, and programming, is a cognitive skill\narguably unique to humans. Considering the relatively recent emergence of this\nability in human evolutionary history, it has been suggested that LMS\nprocessing may build upon more fundamental cognitive systems, possibly through\nneuronal recycling. Previous studies have pinpointed two primary candidates,\nnatural language processing and spatial cognition. Existing comparisons between\nthese domains largely relied on task-level comparison, which may be confounded\nby task idiosyncrasy. The present study instead compared the neural correlates\nat the domain level with both automated meta-analysis and synthesized maps\nbased on three representative LMS tasks, reasoning, calculation, and mental\nprogramming. Our results revealed a more substantial cortical overlap between\nLMS processing and spatial cognition, in contrast to language processing.\nFurthermore, in regions activated by both spatial and language processing, the\nmultivariate activation pattern for LMS processing exhibited greater\nmultivariate similarity to spatial cognition than to language processing. A\nhierarchical clustering analysis further indicated that typical LMS tasks were\nindistinguishable from spatial cognition tasks at the neural level, suggesting\nan inherent connection between these two cognitive processes. Taken together,\nour findings support the hypothesis that spatial cognition is likely the basis\nof LMS processing, which may shed light on the limitations of large language\nmodels in logical reasoning, particularly those trained exclusively on textual\ndata without explicit emphasis on spatial content.", "published": "2024-06-20 14:31:09", "link": "http://arxiv.org/abs/2406.14358v1", "categories": ["q-bio.NC", "cs.AI", "cs.CL"], "primary_category": "q-bio.NC"}
{"title": "FVEL: Interactive Formal Verification Environment with Large Language\n  Models via Theorem Proving", "abstract": "Formal verification (FV) has witnessed growing significance with current\nemerging program synthesis by the evolving large language models (LLMs).\nHowever, current formal verification mainly resorts to symbolic verifiers or\nhand-craft rules, resulting in limitations for extensive and flexible\nverification. On the other hand, formal languages for automated theorem\nproving, such as Isabelle, as another line of rigorous verification, are\nmaintained with comprehensive rules and theorems. In this paper, we propose\nFVEL, an interactive Formal Verification Environment with LLMs. Specifically,\nFVEL transforms a given code to be verified into Isabelle, and then conducts\nverification via neural automated theorem proving with an LLM. The joined\nparadigm leverages the rigorous yet abundant formulated and organized rules in\nIsabelle and is also convenient for introducing and adjusting cutting-edge\nLLMs. To achieve this goal, we extract a large-scale FVELER3. The FVELER\ndataset includes code dependencies and verification processes that are\nformulated in Isabelle, containing 758 theories, 29,125 lemmas, and 200,646\nproof steps in total with in-depth dependencies. We benchmark FVELER in the\nFVEL environment by first fine-tuning LLMs with FVELER and then evaluating them\non Code2Inv and SV-COMP. The results show that FVEL with FVELER fine-tuned\nLlama3- 8B solves 17.39% (69 -> 81) more problems, and Mistral-7B 12% (75 ->\n84) more problems in SV-COMP. And the proportion of proof errors is reduced.\nProject page: https://fveler.github.io/.", "published": "2024-06-20 15:31:05", "link": "http://arxiv.org/abs/2406.14408v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "SynDARin: Synthesising Datasets for Automated Reasoning in Low-Resource\n  Languages", "abstract": "Question Answering (QA) datasets have been instrumental in developing and\nevaluating Large Language Model (LLM) capabilities. However, such datasets are\nscarce for languages other than English due to the cost and difficulties of\ncollection and manual annotation. This means that producing novel models and\nmeasuring the performance of multilingual LLMs in low-resource languages is\nchallenging. To mitigate this, we propose $\\textbf{S}$yn$\\textbf{DAR}$in, a\nmethod for generating and validating QA datasets for low-resource languages. We\nutilize parallel content mining to obtain $\\textit{human-curated}$ paragraphs\nbetween English and the target language. We use the English data as context to\n$\\textit{generate}$ synthetic multiple-choice (MC) question-answer pairs, which\nare automatically translated and further validated for quality. Combining these\nwith their designated non-English $\\textit{human-curated}$ paragraphs form the\nfinal QA dataset. The method allows to maintain the content quality, reduces\nthe likelihood of factual errors, and circumvents the need for costly\nannotation. To test the method, we created a QA dataset with $1.2$K samples for\nthe Armenian language. The human evaluation shows that $98\\%$ of the generated\nEnglish data maintains quality and diversity in the question types and topics,\nwhile the translation validation pipeline can filter out $\\sim70\\%$ of data\nwith poor quality. We use the dataset to benchmark state-of-the-art LLMs,\nshowing their inability to achieve human accuracy with some model performances\ncloser to random chance. This shows that the generated dataset is non-trivial\nand can be used to evaluate reasoning capabilities in low-resource language.", "published": "2024-06-20 15:49:28", "link": "http://arxiv.org/abs/2406.14425v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Review of Common Online Speaker Diarization Methods", "abstract": "Speaker diarization provides the answer to the question \"who spoke when?\" for\nan audio file. This information can be used to complete audio transcripts for\nfurther processing steps. Most speaker diarization systems assume that the\naudio file is available as a whole. However, there are scenarios in which the\nspeaker labels are needed immediately after the arrival of an audio segment.\nSpeaker diarization with a correspondingly low latency is referred to as online\nspeaker diarization. This paper provides an overview. First the history of\nonline speaker diarization is briefly presented. Next a taxonomy and datasets\nfor training and evaluation are given. In the sections that follow, online\ndiarization methods and systems are discussed in detail. This paper concludes\nwith the presentation of challenges that still need to be solved by future\nresearch in the field of online speaker diarization.", "published": "2024-06-20 16:26:03", "link": "http://arxiv.org/abs/2406.14464v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tracing Representation Progression: Analyzing and Enhancing Layer-Wise\n  Similarity", "abstract": "Analyzing the similarity of internal representations has been an important\ntechnique for understanding the behavior of deep neural networks. Most existing\nmethods for analyzing the similarity between representations of high\ndimensions, such as those based on Centered Kernel Alignment (CKA), rely on\nstatistical properties of the representations for a set of data points. In this\npaper, we focus on transformer models and study the similarity of\nrepresentations between the hidden layers of individual transformers. In this\ncontext, we show that a simple sample-wise cosine similarity metric is capable\nof capturing the similarity and aligns with the complicated CKA. Our\nexperimental results on common transformers reveal that representations across\nlayers are positively correlated, with similarity increasing when layers get\ncloser. We provide a theoretical justification for this phenomenon under the\ngeodesic curve assumption for the learned transformer. We then show that an\nincrease in representation similarity implies an increase in predicted\nprobability when directly applying the last-layer classifier to any hidden\nlayer representation. We then propose an aligned training method to improve the\neffectiveness of shallow layer by enhancing the similarity between internal\nrepresentations, with trained models that enjoy the following properties: (1)\nmore early saturation events, (2) layer-wise accuracies monotonically increase\nand reveal the minimal depth needed for the given task, (3) when served as\nmulti-exit models, they achieve on-par performance with standard multi-exit\narchitectures which consist of additional classifiers designed for early\nexiting in shallow layers. To our knowledge, our work is the first to show that\none common classifier is sufficient for multi-exit models. We conduct\nexperiments on both vision and NLP tasks to demonstrate the performance of the\nproposed aligned training.", "published": "2024-06-20 16:41:09", "link": "http://arxiv.org/abs/2406.14479v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Evidence of a log scaling law for political persuasion with large\n  language models", "abstract": "Large language models can now generate political messages as persuasive as\nthose written by humans, raising concerns about how far this persuasiveness may\ncontinue to increase with model size. Here, we generate 720 persuasive messages\non 10 U.S. political issues from 24 language models spanning several orders of\nmagnitude in size. We then deploy these messages in a large-scale randomized\nsurvey experiment (N = 25,982) to estimate the persuasive capability of each\nmodel. Our findings are twofold. First, we find evidence of a log scaling law:\nmodel persuasiveness is characterized by sharply diminishing returns, such that\ncurrent frontier models are barely more persuasive than models smaller in size\nby an order of magnitude or more. Second, mere task completion (coherence,\nstaying on topic) appears to account for larger models' persuasive advantage.\nThese findings suggest that further scaling model size will not much increase\nthe persuasiveness of static LLM-generated messages.", "published": "2024-06-20 17:12:38", "link": "http://arxiv.org/abs/2406.14508v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "PostMark: A Robust Blackbox Watermark for Large Language Models", "abstract": "The most effective techniques to detect LLM-generated text rely on inserting\na detectable signature -- or watermark -- during the model's decoding process.\nMost existing watermarking methods require access to the underlying LLM's\nlogits, which LLM API providers are loath to share due to fears of model\ndistillation. As such, these watermarks must be implemented independently by\neach LLM provider. In this paper, we develop PostMark, a modular post-hoc\nwatermarking procedure in which an input-dependent set of words (determined via\na semantic embedding) is inserted into the text after the decoding process has\ncompleted. Critically, PostMark does not require logit access, which means it\ncan be implemented by a third party. We also show that PostMark is more robust\nto paraphrasing attacks than existing watermarking methods: our experiments\ncover eight baseline algorithms, five base LLMs, and three datasets. Finally,\nwe evaluate the impact of PostMark on text quality using both automated and\nhuman assessments, highlighting the trade-off between quality and robustness to\nparaphrasing. We release our code, outputs, and annotations at\nhttps://github.com/lilakk/PostMark.", "published": "2024-06-20 17:27:14", "link": "http://arxiv.org/abs/2406.14517v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from\n  Disparate Training Data", "abstract": "One way to address safety risks from large language models (LLMs) is to\ncensor dangerous knowledge from their training data. While this removes the\nexplicit information, implicit information can remain scattered across various\ntraining documents. Could an LLM infer the censored knowledge by piecing\ntogether these implicit hints? As a step towards answering this question, we\nstudy inductive out-of-context reasoning (OOCR), a type of generalization in\nwhich LLMs infer latent information from evidence distributed across training\ndocuments and apply it to downstream tasks without in-context learning. Using a\nsuite of five tasks, we demonstrate that frontier LLMs can perform inductive\nOOCR. In one experiment we finetune an LLM on a corpus consisting only of\ndistances between an unknown city and other known cities. Remarkably, without\nin-context examples or Chain of Thought, the LLM can verbalize that the unknown\ncity is Paris and use this fact to answer downstream questions. Further\nexperiments show that LLMs trained only on individual coin flip outcomes can\nverbalize whether the coin is biased, and those trained only on pairs\n$(x,f(x))$ can articulate a definition of $f$ and compute inverses. While OOCR\nsucceeds in a range of cases, we also show that it is unreliable, particularly\nfor smaller LLMs learning complex structures. Overall, the ability of LLMs to\n\"connect the dots\" without explicit in-context learning poses a potential\nobstacle to monitoring and controlling the knowledge acquired by LLMs.", "published": "2024-06-20 17:55:04", "link": "http://arxiv.org/abs/2406.14546v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities", "abstract": "When presented with questions involving visual thinking, humans naturally\nswitch reasoning modalities, often forming mental images or drawing visual\naids. Large language models have shown promising results in arithmetic and\nsymbolic reasoning by expressing intermediate reasoning in text as a chain of\nthought, yet struggle to extend this capability to answer text queries that are\neasily solved by visual reasoning, even with extensive multimodal pretraining.\nWe introduce a simple method, whiteboard-of-thought prompting, to unlock the\nvisual reasoning capabilities of multimodal large language models across\nmodalities. Whiteboard-of-thought prompting provides multimodal large language\nmodels with a metaphorical `whiteboard' to draw out reasoning steps as images,\nthen returns these images back to the model for further processing. We find\nthis can be accomplished with no demonstrations or specialized modules, instead\nleveraging models' existing ability to write code with libraries such as\nMatplotlib and Turtle. This simple approach shows state-of-the-art results on\nfour difficult natural language tasks that involve visual and spatial\nreasoning. We identify multiple settings where GPT-4o using chain-of-thought\nfails dramatically, including more than one where it achieves $0\\%$ accuracy,\nwhile whiteboard-of-thought enables up to $92\\%$ accuracy in these same\nsettings. We present a detailed exploration of where the technique succeeds as\nwell as its sources of error.", "published": "2024-06-20 17:59:45", "link": "http://arxiv.org/abs/2406.14562v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch", "abstract": "Merging Large Language Models (LLMs) is a cost-effective technique for\ncombining multiple expert LLMs into a single versatile model, retaining the\nexpertise of the original ones. However, current approaches often overlook the\nimportance of safety alignment during merging, leading to highly misaligned\nmodels. This work investigates the effects of model merging on alignment. We\nevaluate several popular model merging techniques, demonstrating that existing\nmethods do not only transfer domain expertise but also propagate misalignment.\nWe propose a simple two-step approach to address this problem: (i) generating\nsynthetic safety and domain-specific data, and (ii) incorporating these\ngenerated data into the optimization process of existing data-aware model\nmerging techniques. This allows us to treat alignment as a skill that can be\nmaximized in the resulting merged LLM. Our experiments illustrate the\neffectiveness of integrating alignment-related data during merging, resulting\nin models that excel in both domain expertise and alignment.", "published": "2024-06-20 17:59:58", "link": "http://arxiv.org/abs/2406.14563v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Holistic Evaluation for Interleaved Text-and-Image Generation", "abstract": "Interleaved text-and-image generation has been an intriguing research\ndirection, where the models are required to generate both images and text\npieces in an arbitrary order. Despite the emerging advancements in interleaved\ngeneration, the progress in its evaluation still significantly lags behind.\nExisting evaluation benchmarks do not support arbitrarily interleaved images\nand text for both inputs and outputs, and they only cover a limited number of\ndomains and use cases. Also, current works predominantly use similarity-based\nmetrics which fall short in assessing the quality in open-ended scenarios. To\nthis end, we introduce InterleavedBench, the first benchmark carefully curated\nfor the evaluation of interleaved text-and-image generation. InterleavedBench\nfeatures a rich array of tasks to cover diverse real-world use cases. In\naddition, we present InterleavedEval, a strong reference-free metric powered by\nGPT-4o to deliver accurate and explainable evaluation. We carefully define five\nessential evaluation aspects for InterleavedEval, including text quality,\nperceptual quality, image coherence, text-image coherence, and helpfulness, to\nensure a comprehensive and fine-grained assessment. Through extensive\nexperiments and rigorous human evaluation, we show that our benchmark and\nmetric can effectively evaluate the existing models with a strong correlation\nwith human judgments surpassing previous reference-based metrics. We also\nprovide substantial findings and insights to foster future research in\ninterleaved generation and its evaluation.", "published": "2024-06-20 18:07:19", "link": "http://arxiv.org/abs/2406.14643v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Major Entity Identification: A Generalizable Alternative to Coreference\n  Resolution", "abstract": "The limited generalization of coreference resolution (CR) models has been a\nmajor bottleneck in the task's broad application. Prior work has identified\nannotation differences, especially for mention detection, as one of the main\nreasons for the generalization gap and proposed using additional annotated\ntarget domain data. Rather than relying on this additional annotation, we\npropose an alternative referential task, Major Entity Identification (MEI),\nwhere we: (a) assume the target entities to be specified in the input, and (b)\nlimit the task to only the frequent entities. Through extensive experiments, we\ndemonstrate that MEI models generalize well across domains on multiple datasets\nwith supervised models and LLM-based few-shot prompting. Additionally, MEI fits\nthe classification framework, which enables the use of robust and intuitive\nclassification-based metrics. Finally, MEI is also of practical use as it\nallows a user to search for all mentions of a particular entity or a group of\nentities of interest.", "published": "2024-06-20 18:17:58", "link": "http://arxiv.org/abs/2406.14654v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "OpenDebateEvidence: A Massive-Scale Argument Mining and Summarization\n  Dataset", "abstract": "We introduce OpenDebateEvidence, a comprehensive dataset for argument mining\nand summarization sourced from the American Competitive Debate community. This\ndataset includes over 3.5 million documents with rich metadata, making it one\nof the most extensive collections of debate evidence. OpenDebateEvidence\ncaptures the complexity of arguments in high school and college debates,\nproviding valuable resources for training and evaluation. Our extensive\nexperiments demonstrate the efficacy of fine-tuning state-of-the-art large\nlanguage models for argumentative abstractive summarization across various\nmethods, models, and datasets. By providing this comprehensive resource, we aim\nto advance computational argumentation and support practical applications for\ndebaters, educators, and researchers. OpenDebateEvidence is publicly available\nto support further research and innovation in computational argumentation.\nAccess it here: https://huggingface.co/datasets/Yusuf5/OpenCaselist", "published": "2024-06-20 18:22:59", "link": "http://arxiv.org/abs/2406.14657v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Design Choices for Building Language-Specific LLMs", "abstract": "Despite rapid progress in large language models (LLMs), their performance on\na vast majority of languages remains unsatisfactory. In this paper, we study\nbuilding language-specific LLMs by adapting monolingual and multilingual LLMs.\nWe conduct systematic experiments on how design choices (base model selection,\nvocabulary extension, and continued pretraining) impact the adapted LLM, both\nin terms of efficiency (how many tokens are needed to encode the same amount of\ninformation) and end task performance. We find that (1) the initial performance\nof LLM does not always correlate with the final performance after the\nadaptation. Adapting an English-centric models can yield better results than\nadapting multilingual models despite their worse initial performance on\nlow-resource languages. (2) Efficiency can easily improved with simple\nvocabulary extension and continued pretraining in most LLMs we study, and (3)\nThe optimal adaptation method (choice of the base model, new vocabulary size,\ntraining data, initialization strategy) is highly language-dependent, and the\nsimplest embedding initialization works well across various experimental\nsettings. Together, our work lays foundations on efficiently building\nlanguage-specific LLMs by adapting existing LLMs.", "published": "2024-06-20 18:47:43", "link": "http://arxiv.org/abs/2406.14670v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Contrastive Learning Approach to Mitigate Bias in Speech Models", "abstract": "Speech models may be affected by performance imbalance in different\npopulation subgroups, raising concerns about fair treatment across these\ngroups. Prior attempts to mitigate unfairness either focus on user-defined\nsubgroups, potentially overlooking other affected subgroups, or do not\nexplicitly improve the internal representation at the subgroup level. This\npaper proposes the first adoption of contrastive learning to mitigate speech\nmodel bias in underperforming subgroups. We employ a three-level learning\ntechnique that guides the model in focusing on different scopes for the\ncontrastive loss, i.e., task, subgroup, and the errors within subgroups. The\nexperiments on two spoken language understanding datasets and two languages\ndemonstrate that our approach improves internal subgroup representations, thus\nreducing model bias and enhancing performance.", "published": "2024-06-20 19:20:00", "link": "http://arxiv.org/abs/2406.14686v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Speech Prefix-Tuning with RNNT Loss for Improving LLM Predictions", "abstract": "In this paper, we focus on addressing the constraints faced when applying\nLLMs to ASR. Recent works utilize prefixLM-type models, which directly apply\nspeech as a prefix to LLMs for ASR. We have found that optimizing speech\nprefixes leads to better ASR performance and propose applying RNNT loss to\nperform speech prefix-tuning. This is a simple approach and does not increase\nthe model complexity or alter the inference pipeline. We also propose\nlanguage-based soft prompting to further improve with frozen LLMs. Empirical\nanalysis on realtime testset from 10 Indic languages demonstrate that our\nproposed speech prefix-tuning yields improvements with both frozen and\nfine-tuned LLMs. Our recognition results on an average of 10 Indics show that\nthe proposed prefix-tuning with RNNT loss results in a 12\\% relative\nimprovement in WER over the baseline with a fine-tuned LLM. Our proposed\napproches with the frozen LLM leads to a 31\\% relative improvement over basic\nsoft-prompting prefixLM.", "published": "2024-06-20 19:50:49", "link": "http://arxiv.org/abs/2406.14701v1", "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in\n  Large Language Model Collaborations via Debate", "abstract": "Large Language Models (LLMs) have shown exceptional results on current\nbenchmarks when working individually. The advancement in their capabilities,\nalong with a reduction in parameter size and inference times, has facilitated\nthe use of these models as agents, enabling interactions among multiple models\nto execute complex tasks. Such collaborations offer several advantages,\nincluding the use of specialized models (e.g. coding), improved confidence\nthrough multiple computations, and enhanced divergent thinking, leading to more\ndiverse outputs. Thus, the collaborative use of language models is expected to\ngrow significantly in the coming years. In this work, we evaluate the behavior\nof a network of models collaborating through debate under the influence of an\nadversary. We introduce pertinent metrics to assess the adversary's\neffectiveness, focusing on system accuracy and model agreement. Our findings\nhighlight the importance of a model's persuasive ability in influencing others.\nAdditionally, we explore inference-time methods to generate more compelling\narguments and evaluate the potential of prompt-based mitigation as a defensive\nstrategy.", "published": "2024-06-20 20:09:37", "link": "http://arxiv.org/abs/2406.14711v2", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "An LLM Feature-based Framework for Dialogue Constructiveness Assessment", "abstract": "Research on dialogue constructiveness assessment focuses on (i) analysing\nconversational factors that influence individuals to take specific actions, win\ndebates, change their perspectives or broaden their open-mindedness and (ii)\npredicting constructiveness outcomes following dialogues for such use cases.\nThese objectives can be achieved by training either interpretable feature-based\nmodels (which often involve costly human annotations) or neural models such as\npre-trained language models (which have empirically shown higher task accuracy\nbut lack interpretability). In this paper we propose an LLM feature-based\nframework for dialogue constructiveness assessment that combines the strengths\nof feature-based and neural approaches, while mitigating their downsides. The\nframework first defines a set of dataset-independent and interpretable\nlinguistic features, which can be extracted by both prompting an LLM and simple\nheuristics. Such features are then used to train LLM feature-based models. We\napply this framework to three datasets of dialogue constructiveness and find\nthat our LLM feature-based models outperform or performs at least as well as\nstandard feature-based models and neural models. We also find that the LLM\nfeature-based model learns more robust prediction rules instead of relying on\nsuperficial shortcuts, which often trouble neural models.", "published": "2024-06-20 22:10:52", "link": "http://arxiv.org/abs/2406.14760v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RE-AdaptIR: Improving Information Retrieval through Reverse Engineered\n  Adaptation", "abstract": "Large language models (LLMs) fine-tuned for text-retrieval have demonstrated\nstate-of-the-art results across several information retrieval (IR) benchmarks.\nHowever, supervised training for improving these models requires numerous\nlabeled examples, which are generally unavailable or expensive to acquire. In\nthis work, we explore the effectiveness of extending reverse engineered\nadaptation to the context of information retrieval (RE-AdaptIR). We use\nRE-AdaptIR to improve LLM-based IR models using only unlabeled data. We\ndemonstrate improved performance both in training domains as well as zero-shot\nin domains where the models have seen no queries. We analyze performance\nchanges in various fine-tuning scenarios and offer findings of immediate use to\npractitioners.", "published": "2024-06-20 22:28:11", "link": "http://arxiv.org/abs/2406.14764v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Evaluating Numerical Reasoning in Text-to-Image Models", "abstract": "Text-to-image generative models are capable of producing high-quality images\nthat often faithfully depict concepts described using natural language. In this\nwork, we comprehensively evaluate a range of text-to-image models on numerical\nreasoning tasks of varying difficulty, and show that even the most advanced\nmodels have only rudimentary numerical skills. Specifically, their ability to\ncorrectly generate an exact number of objects in an image is limited to small\nnumbers, it is highly dependent on the context the number term appears in, and\nit deteriorates quickly with each successive number. We also demonstrate that\nmodels have poor understanding of linguistic quantifiers (such as \"a few\" or\n\"as many as\"), the concept of zero, and struggle with more advanced concepts\nsuch as partial quantities and fractional representations. We bundle prompts,\ngenerated images and human annotations into GeckoNum, a novel benchmark for\nevaluation of numerical reasoning.", "published": "2024-06-20 22:56:31", "link": "http://arxiv.org/abs/2406.14774v3", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "System Description for the Displace Speaker Diarization Challenge 2023", "abstract": "This paper describes our solution for the Diarization of Speaker and Language\nin Conversational Environments Challenge (Displace 2023). We used a combination\nof VAD for finding segfments with speech, Resnet architecture based CNN for\nfeature extraction from these segments, and spectral clustering for features\nclustering. Even though it was not trained with using Hindi, the described\nalgorithm achieves the following metrics: DER 27. 1% and DER 27. 4%, on the\ndevelopment and phase-1 evaluation parts of the dataset, respectively.", "published": "2024-06-20 21:40:02", "link": "http://arxiv.org/abs/2406.15516v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on\n  Path Planning", "abstract": "Path planning is a fundamental scientific problem in robotics and autonomous\nnavigation, requiring the derivation of efficient routes from starting to\ndestination points while avoiding obstacles. Traditional algorithms like A* and\nits variants are capable of ensuring path validity but suffer from significant\ncomputational and memory inefficiencies as the state space grows. Conversely,\nlarge language models (LLMs) excel in broader environmental analysis through\ncontextual understanding, providing global insights into environments. However,\nthey fall short in detailed spatial and temporal reasoning, often leading to\ninvalid or inefficient routes. In this work, we propose LLM-A*, an new LLM\nbased route planning method that synergistically combines the precise\npathfinding capabilities of A* with the global reasoning capability of LLMs.\nThis hybrid approach aims to enhance pathfinding efficiency in terms of time\nand space complexity while maintaining the integrity of path validity,\nespecially in large-scale scenarios. By integrating the strengths of both\nmethodologies, LLM-A* addresses the computational and memory limitations of\nconventional algorithms without compromising on the validity required for\neffective pathfinding.", "published": "2024-06-20 01:24:30", "link": "http://arxiv.org/abs/2407.02511v2", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Artificial Leviathan: Exploring Social Evolution of LLM Agents Through\n  the Lens of Hobbesian Social Contract Theory", "abstract": "The emergence of Large Language Models (LLMs) and advancements in Artificial\nIntelligence (AI) offer an opportunity for computational social science\nresearch at scale. Building upon prior explorations of LLM agent design, our\nwork introduces a simulated agent society where complex social relationships\ndynamically form and evolve over time. Agents are imbued with psychological\ndrives and placed in a sandbox survival environment. We conduct an evaluation\nof the agent society through the lens of Thomas Hobbes's seminal Social\nContract Theory (SCT). We analyze whether, as the theory postulates, agents\nseek to escape a brutish \"state of nature\" by surrendering rights to an\nabsolute sovereign in exchange for order and security. Our experiments unveil\nan alignment: Initially, agents engage in unrestrained conflict, mirroring\nHobbes's depiction of the state of nature. However, as the simulation\nprogresses, social contracts emerge, leading to the authorization of an\nabsolute sovereign and the establishment of a peaceful commonwealth founded on\nmutual cooperation. This congruence between our LLM agent society's\nevolutionary trajectory and Hobbes's theoretical account indicates LLMs'\ncapability to model intricate social dynamics and potentially replicate forces\nthat shape human societies. By enabling such insights into group behavior and\nemergent societal phenomena, LLM-driven multi-agent simulations, while unable\nto simulate all the nuances of human behavior, may hold potential for advancing\nour understanding of social structures, group dynamics, and complex human\nsystems.", "published": "2024-06-20 14:42:58", "link": "http://arxiv.org/abs/2406.14373v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.MA"], "primary_category": "cs.AI"}
{"title": "ChatGPT as Research Scientist: Probing GPT's Capabilities as a Research\n  Librarian, Research Ethicist, Data Generator and Data Predictor", "abstract": "How good a research scientist is ChatGPT? We systematically probed the\ncapabilities of GPT-3.5 and GPT-4 across four central components of the\nscientific process: as a Research Librarian, Research Ethicist, Data Generator,\nand Novel Data Predictor, using psychological science as a testing field. In\nStudy 1 (Research Librarian), unlike human researchers, GPT-3.5 and GPT-4\nhallucinated, authoritatively generating fictional references 36.0% and 5.4% of\nthe time, respectively, although GPT-4 exhibited an evolving capacity to\nacknowledge its fictions. In Study 2 (Research Ethicist), GPT-4 (though not\nGPT-3.5) proved capable of detecting violations like p-hacking in fictional\nresearch protocols, correcting 88.6% of blatantly presented issues, and 72.6%\nof subtly presented issues. In Study 3 (Data Generator), both models\nconsistently replicated patterns of cultural bias previously discovered in\nlarge language corpora, indicating that ChatGPT can simulate known results, an\nantecedent to usefulness for both data generation and skills like hypothesis\ngeneration. Contrastingly, in Study 4 (Novel Data Predictor), neither model was\nsuccessful at predicting new results absent in their training data, and neither\nappeared to leverage substantially new information when predicting more versus\nless novel outcomes. Together, these results suggest that GPT is a flawed but\nrapidly improving librarian, a decent research ethicist already, capable of\ndata generation in simple domains with known characteristics but poor at\npredicting novel patterns of empirical data to aid future experimentation.", "published": "2024-06-20 22:30:06", "link": "http://arxiv.org/abs/2406.14765v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.IR", "cs.LG", "I.2.7; K.4.0; K.4.1; K.4.2"], "primary_category": "cs.AI"}
{"title": "Decoding Vocal Articulations from Acoustic Latent Representations", "abstract": "We present a novel neural encoder system for acoustic-to-articulatory\ninversion. We leverage the Pink Trombone voice synthesizer that reveals\narticulatory parameters (e.g tongue position and vocal cord configuration). Our\nsystem is designed to identify the articulatory features responsible for\nproducing specific acoustic characteristics contained in a neural latent\nrepresentation. To generate the necessary latent embeddings, we employed two\nmain methodologies. The first was a self-supervised variational autoencoder\ntrained from scratch to reconstruct the input signal at the decoder stage. We\nconditioned its bottleneck layer with a subnetwork called the \"projector,\"\nwhich decodes the voice synthesizer's parameters.\n  The second methodology utilized two pretrained models: EnCodec and Wav2Vec.\nThey eliminate the need to train the encoding process from scratch, allowing us\nto focus on training the projector network. This approach aimed to explore the\npotential of these existing models in the context of acoustic-to-articulatory\ninversion. By reusing the pretrained models, we significantly simplified the\ndata processing pipeline, increasing efficiency and reducing computational\noverhead.\n  The primary goal of our project was to demonstrate that these neural\narchitectures can effectively encapsulate both acoustic and articulatory\nfeatures. This prediction-based approach is much faster than traditional\nmethods focused on acoustic feature-based parameter optimization. We validated\nour models by predicting six different parameters and evaluating them with\nobjective and ViSQOL subjective-equivalent metric using both synthesizer- and\nhuman-generated sounds. The results show that the predicted parameters can\ngenerate human-like vowel sounds when input into the synthesizer. We provide\nthe dataset, code, and detailed findings to support future research in this\nfield.", "published": "2024-06-20 14:52:42", "link": "http://arxiv.org/abs/2406.14379v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Improved Remixing Process for Domain Adaptation-Based Speech Enhancement\n  by Mitigating Data Imbalance in Signal-to-Noise Ratio", "abstract": "RemixIT and Remixed2Remixed are domain adaptation-based speech enhancement\n(DASE) methods that use a teacher model trained in full supervision to generate\npseudo-paired data by remixing the outputs of the teacher model. The student\nmodel for enhancing real-world recorded signals is trained using the\npseudo-paired data without ground truth. Since the noisy signals are recorded\nin natural environments, the dataset inevitably suffers data imbalance in some\nacoustic properties, leading to subpar performance for the underrepresented\ndata. The signal-to-noise ratio (SNR), inherently balanced in supervised\nlearning, is a prime example. In this paper, we provide empirical evidence that\nthe SNR of pseudo data has a significant impact on model performance using the\ndataset of the CHiME-7 UDASE task, highlighting the importance of balanced SNR\nin DASE. Furthermore, we propose adopting curriculum learning to encompass a\nbroad range of SNRs to boost performance for underrepresented data.", "published": "2024-06-20 04:19:19", "link": "http://arxiv.org/abs/2406.13982v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Disentangled Representation Learning for Environment-agnostic Speaker\n  Recognition", "abstract": "This work presents a framework based on feature disentanglement to learn\nspeaker embeddings that are robust to environmental variations. Our framework\nutilises an auto-encoder as a disentangler, dividing the input speaker\nembedding into components related to the speaker and other residual\ninformation. We employ a group of objective functions to ensure that the\nauto-encoder's code representation - used as the refined embedding - condenses\nonly the speaker characteristics. We show the versatility of our framework\nthrough its compatibility with any existing speaker embedding extractor,\nrequiring no structural modifications or adaptations for integration. We\nvalidate the effectiveness of our framework by incorporating it into two\npopularly used embedding extractors and conducting experiments across various\nbenchmarks. The results show a performance improvement of up to 16%. We release\nour code for this work to be available\nhttps://github.com/kaistmm/voxceleb-disentangler", "published": "2024-06-20 17:59:25", "link": "http://arxiv.org/abs/2406.14559v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Voice Disorder Analysis: a Transformer-based Approach", "abstract": "Voice disorders are pathologies significantly affecting patient quality of\nlife. However, non-invasive automated diagnosis of these pathologies is still\nunder-explored, due to both a shortage of pathological voice data, and\ndiversity of the recording types used for the diagnosis. This paper proposes a\nnovel solution that adopts transformers directly working on raw voice signals\nand addresses data shortage through synthetic data generation and data\naugmentation. Further, we consider many recording types at the same time, such\nas sentence reading and sustained vowel emission, by employing a Mixture of\nExpert ensemble to align the predictions on different data types. The\nexperimental results, obtained on both public and private datasets, show the\neffectiveness of our solution in the disorder detection and classification\ntasks and largely improve over existing approaches.", "published": "2024-06-20 19:29:04", "link": "http://arxiv.org/abs/2406.14693v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "CONMOD: Controllable Neural Frame-based Modulation Effects", "abstract": "Deep learning models have seen widespread use in modelling LFO-driven audio\neffects, such as phaser and flanger. Although existing neural architectures\nexhibit high-quality emulation of individual effects, they do not possess the\ncapability to manipulate the output via control parameters. To address this\nissue, we introduce Controllable Neural Frame-based Modulation Effects\n(CONMOD), a single black-box model which emulates various LFO-driven effects in\na frame-wise manner, offering control over LFO frequency and feedback\nparameters. Additionally, the model is capable of learning the continuous\nembedding space of two distinct phaser effects, enabling us to steer between\neffects and achieve creative outputs. Our model outperforms previous work while\npossessing both controllability and universality, presenting opportunities to\nenhance creativity in modern LFO-driven audio effects.", "published": "2024-06-20 02:02:54", "link": "http://arxiv.org/abs/2406.13935v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Multi-Stream Fusion Approach with One-Class Learning for Audio-Visual\n  Deepfake Detection", "abstract": "This paper addresses the challenge of developing a robust audio-visual\ndeepfake detection model. In practical use cases, new generation algorithms are\ncontinually emerging, and these algorithms are not encountered during the\ndevelopment of detection methods. This calls for the generalization ability of\nthe method. Additionally, to ensure the credibility of detection methods, it is\nbeneficial for the model to interpret which cues from the video indicate it is\nfake. Motivated by these considerations, we then propose a multi-stream fusion\napproach with one-class learning as a representation-level regularization\ntechnique. We study the generalization problem of audio-visual deepfake\ndetection by creating a new benchmark by extending and re-splitting the\nexisting FakeAVCeleb dataset. The benchmark contains four categories of fake\nvideos (Real Audio-Fake Visual, Fake Audio-Fake Visual, Fake Audio-Real Visual,\nand Unsynchronized videos). The experimental results demonstrate that our\napproach surpasses the previous models by a large margin. Furthermore, our\nproposed framework offers interpretability, indicating which modality the model\nidentifies as more likely to be fake. The source code is released at\nhttps://github.com/bok-bok/MSOC.", "published": "2024-06-20 10:33:15", "link": "http://arxiv.org/abs/2406.14176v3", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DASB - Discrete Audio and Speech Benchmark", "abstract": "Discrete audio tokens have recently gained considerable attention for their\npotential to connect audio and language processing, enabling the creation of\nmodern multimodal large language models. Ideal audio tokens must effectively\npreserve phonetic and semantic content along with paralinguistic information,\nspeaker identity, and other details. While several types of audio tokens have\nbeen recently proposed, identifying the optimal tokenizer for various tasks is\nchallenging due to the inconsistent evaluation settings in existing studies. To\naddress this gap, we release the Discrete Audio and Speech Benchmark (DASB), a\ncomprehensive leaderboard for benchmarking discrete audio tokens across a wide\nrange of discriminative tasks, including speech recognition, speaker\nidentification and verification, emotion recognition, keyword spotting, and\nintent classification, as well as generative tasks such as speech enhancement,\nseparation, and text-to-speech. Our results show that, on average, semantic\ntokens outperform compression tokens across most discriminative and generative\ntasks. However, the performance gap between semantic tokens and standard\ncontinuous representations remains substantial, highlighting the need for\nfurther research in this field.", "published": "2024-06-20 13:23:27", "link": "http://arxiv.org/abs/2406.14294v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LARP: Language Audio Relational Pre-training for Cold-Start Playlist\n  Continuation", "abstract": "As online music consumption increasingly shifts towards playlist-based\nlistening, the task of playlist continuation, in which an algorithm suggests\nsongs to extend a playlist in a personalized and musically cohesive manner, has\nbecome vital to the success of music streaming. Currently, many existing\nplaylist continuation approaches rely on collaborative filtering methods to\nperform recommendation. However, such methods will struggle to recommend songs\nthat lack interaction data, an issue known as the cold-start problem. Current\napproaches to this challenge design complex mechanisms for extracting\nrelational signals from sparse collaborative data and integrating them into\ncontent representations. However, these approaches leave content representation\nlearning out of scope and utilize frozen, pre-trained content models that may\nnot be aligned with the distribution or format of a specific musical setting.\nFurthermore, even the musical state-of-the-art content modules are either (1)\nincompatible with the cold-start setting or (2) unable to effectively integrate\ncross-modal and relational signals. In this paper, we introduce LARP, a\nmulti-modal cold-start playlist continuation model, to effectively overcome\nthese limitations. LARP is a three-stage contrastive learning framework that\nintegrates both multi-modal and relational signals into its learned\nrepresentations. Our framework uses increasing stages of task-specific\nabstraction: within-track (language-audio) contrastive loss, track-track\ncontrastive loss, and track-playlist contrastive loss. Experimental results on\ntwo publicly available datasets demonstrate the efficacy of LARP over uni-modal\nand multi-modal models for playlist continuation in a cold-start setting. Code\nand dataset are released at: https://github.com/Rsalganik1123/LARP.", "published": "2024-06-20 14:02:15", "link": "http://arxiv.org/abs/2406.14333v1", "categories": ["cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Machine Learning Techniques in Automatic Music Transcription: A\n  Systematic Survey", "abstract": "In the domain of Music Information Retrieval (MIR), Automatic Music\nTranscription (AMT) emerges as a central challenge, aiming to convert audio\nsignals into symbolic notations like musical notes or sheet music. This\nsystematic review accentuates the pivotal role of AMT in music signal analysis,\nemphasizing its importance due to the intricate and overlapping spectral\nstructure of musical harmonies. Through a thorough examination of existing\nmachine learning techniques utilized in AMT, we explore the progress and\nconstraints of current models and methodologies. Despite notable advancements,\nAMT systems have yet to match the accuracy of human experts, largely due to the\ncomplexities of musical harmonies and the need for nuanced interpretation. This\nreview critically evaluates both fully automatic and semi-automatic AMT\nsystems, emphasizing the importance of minimal user intervention and examining\nvarious methodologies proposed to date. By addressing the limitations of prior\ntechniques and suggesting avenues for improvement, our objective is to steer\nfuture research towards fully automated AMT systems capable of accurately and\nefficiently translating intricate audio signals into precise symbolic\nrepresentations. This study not only synthesizes the latest advancements but\nalso lays out a road-map for overcoming existing challenges in AMT, providing\nvaluable insights for researchers aiming to narrow the gap between current\nsystems and human-level transcription accuracy.", "published": "2024-06-20 03:48:15", "link": "http://arxiv.org/abs/2406.15249v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fish Tracking, Counting, and Behaviour Analysis in Digital Aquaculture:\n  A Comprehensive Survey", "abstract": "Digital aquaculture leverages advanced technologies and data-driven methods,\nproviding substantial benefits over traditional aquaculture practices. This\npaper presents a comprehensive review of three interconnected digital\naquaculture tasks, namely, fish tracking, counting, and behaviour analysis,\nusing a novel and unified approach. Unlike previous reviews which focused on\nsingle modalities or individual tasks, we analyse vision-based (i.e. image- and\nvideo-based), acoustic-based, and biosensor-based methods across all three\ntasks. We examine their advantages, limitations, and applications, highlighting\nrecent advancements and identifying critical cross-cutting research gaps. The\nreview also includes emerging ideas such as applying multi-task learning and\nlarge language models to address various aspects of fish monitoring, an\napproach not previously explored in aquaculture literature. We identify the\nmajor obstacles hindering research progress in this field, including the\nscarcity of comprehensive fish datasets and the lack of unified evaluation\nstandards. To overcome the current limitations, we explore the potential of\nusing emerging technologies such as multimodal data fusion and deep learning to\nimprove the accuracy, robustness, and efficiency of integrated fish monitoring\nsystems. In addition, we provide a summary of existing datasets available for\nfish tracking, counting, and behaviour analysis. This holistic perspective\noffers a roadmap for future research, emphasizing the need for comprehensive\ndatasets and evaluation standards to facilitate meaningful comparisons between\ntechnologies and to promote their practical implementations in real-world\nsettings.", "published": "2024-06-20 11:37:27", "link": "http://arxiv.org/abs/2406.17800v3", "categories": ["q-bio.QM", "cs.SD", "eess.AS"], "primary_category": "q-bio.QM"}
{"title": "Proceedings of The second international workshop on eXplainable AI for\n  the Arts (XAIxArts)", "abstract": "This second international workshop on explainable AI for the Arts (XAIxArts)\nbrought together a community of researchers in HCI, Interaction Design, AI,\nexplainable AI (XAI), and digital arts to explore the role of XAI for the Arts.\nWorkshop held at the 16th ACM Conference on Creativity and Cognition (C&C\n2024), Chicago, USA.", "published": "2024-06-20 16:48:14", "link": "http://arxiv.org/abs/2406.14485v8", "categories": ["cs.AI", "cs.HC", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
