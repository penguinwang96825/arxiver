{"title": "Rule-based Emotion Detection on Social Media: Putting Tweets on\n  Plutchik's Wheel", "abstract": "We study sentiment analysis beyond the typical granularity of polarity and\ninstead use Plutchik's wheel of emotions model. We introduce RBEM-Emo as an\nextension to the Rule-Based Emission Model algorithm to deduce such emotions\nfrom human-written messages. We evaluate our approach on two different datasets\nand compare its performance with the current state-of-the-art techniques for\nemotion detection, including a recursive auto-encoder. The results of the\nexperimental study suggest that RBEM-Emo is a promising approach advancing the\ncurrent state-of-the-art in emotion detection.", "published": "2014-12-15 17:20:47", "link": "http://arxiv.org/abs/1412.4682v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Broadcast News Corpus for Evaluation and Tuning of German LVCSR\n  Systems", "abstract": "Transcription of broadcast news is an interesting and challenging application\nfor large-vocabulary continuous speech recognition (LVCSR). We present in\ndetail the structure of a manually segmented and annotated corpus including\nover 160 hours of German broadcast news, and propose it as an evaluation\nframework of LVCSR systems. We show our own experimental results on the corpus,\nachieved with a state-of-the-art LVCSR decoder, measuring the effect of\ndifferent feature sets and decoding parameters, and thereby demonstrate that\nreal-time decoding of our test set is feasible on a desktop PC at 9.2% word\nerror rate.", "published": "2014-12-15 14:34:38", "link": "http://arxiv.org/abs/1412.4616v1", "categories": ["cs.CL", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Translating Videos to Natural Language Using Deep Recurrent Neural\n  Networks", "abstract": "Solving the visual symbol grounding problem has long been a goal of\nartificial intelligence. The field appears to be advancing closer to this goal\nwith recent breakthroughs in deep learning for natural language grounding in\nstatic images. In this paper, we propose to translate videos directly to\nsentences using a unified deep neural network with both convolutional and\nrecurrent structure. Described video datasets are scarce, and most existing\nmethods have been applied to toy domains with a small vocabulary of possible\nwords. By transferring knowledge from 1.2M+ images with category labels and\n100,000+ images with captions, our method is able to create sentence\ndescriptions of open-domain videos with large vocabularies. We compare our\napproach with recent work using language generation metrics, subject, verb, and\nobject prediction accuracy, and a human evaluation.", "published": "2014-12-15 19:21:50", "link": "http://arxiv.org/abs/1412.4729v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
