{"title": "Theoretical Perspectives on Deep Learning Methods in Inverse Problems", "abstract": "In recent years, there have been significant advances in the use of deep learning methods in inverse problems such as denoising, compressive sensing, inpainting, and super-resolution. While this line of works has predominantly been driven by practical algorithms and experiments, it has also given rise to a variety of intriguing theoretical problems. In this paper, we survey some of the prominent theoretical developments in this line of works, focusing in particular on generative priors, untrained neural network priors, and unfolding algorithms. In addition to summarizing existing results in these topics, we highlight several ongoing challenges and open problems.", "published": "2022-06-29 02:37:50", "link": "http://arxiv.org/abs/2206.14373v2", "categories": ["stat.ML", "cs.IT", "cs.LG", "eess.SP", "math.ST"], "primary_category": "stat.ML"}
{"title": "When Optimal Transport Meets Information Geometry", "abstract": "Information geometry and optimal transport are two distinct geometric frameworks for modeling families of probability measures. During the recent years, there has been a surge of research endeavors that cut across these two areas and explore their links and interactions. This paper is intended to provide an (incomplete) survey of these works, including entropy-regularized transport, divergence functions arising from $c$-duality, density manifolds and transport information geometry, the para-K\u00e4hler and K\u00e4hler geometries underlying optimal transport and the regularity theory for its solutions. Some outstanding questions that would be of interest to audience of both these two disciplines are posed. Our piece also serves as an introduction to the Special Issue on Optimal Transport of the journal Information Geometry.", "published": "2022-06-29 17:46:27", "link": "http://arxiv.org/abs/2206.14791v1", "categories": ["math.OC", "cs.IT", "math.DG"], "primary_category": "math.OC"}
{"title": "The Lepto-Variance of Stock Returns", "abstract": "The Regression Tree (RT) sorts the samples using a specific feature and finds the split point that produces the maximum variance reduction from a node to its children. Our key observation is that the best factor to use (in terms of MSE drop) is always the target itself, as this most clearly separates the target. Thus using the target as the splitting factor provides an upper bound on MSE drop (or lower bound on the residual children MSE). Based on this observation, we define the k-bit lepto-variance $\u03bbk^2$ of a target variable (or equivalently the lepto-variance at a specific depth k) as the variance that cannot be removed by any regression tree of a depth equal to k. As the upper bound performance for any feature, we believe $\u03bbk^2$ to be an interesting statistical concept related to the underlying structure of the sample as it quantifies the resolving power of the RT for the sample. The max variance that may be explained using RTs of depth up to k is called the sample k-bit macro-variance. At any depth, total sample variance is thus decomposed into lepto-variance $\u03bb^2$ and macro-variance $\u03bc^2$. We demonstrate the concept, by performing 1- and 2-bit RT based lepto-structure analysis for daily IBM stock returns.", "published": "2022-06-29 04:15:35", "link": "http://arxiv.org/abs/2207.04867v2", "categories": ["q-fin.ST", "cs.LG", "q-fin.CP", "q-fin.RM"], "primary_category": "q-fin.ST"}
