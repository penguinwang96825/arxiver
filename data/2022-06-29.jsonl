{"title": "Chinese Word Sense Embedding with SememeWSD and Synonym Set", "abstract": "Word embedding is a fundamental natural language processing task which can\nlearn feature of words. However, most word embedding methods assign only one\nvector to a word, even if polysemous words have multi-senses. To address this\nlimitation, we propose SememeWSD Synonym (SWSDS) model to assign a different\nvector to every sense of polysemous words with the help of word sense\ndisambiguation (WSD) and synonym set in OpenHowNet. We use the SememeWSD model,\nan unsupervised word sense disambiguation model based on OpenHowNet, to do word\nsense disambiguation and annotate the polysemous word with sense id. Then, we\nobtain top 10 synonyms of the word sense from OpenHowNet and calculate the\naverage vector of synonyms as the vector of the word sense. In experiments, We\nevaluate the SWSDS model on semantic similarity calculation with Gensim's\nwmdistance method. It achieves improvement of accuracy. We also examine the\nSememeWSD model on different BERT models to find the more effective model.", "published": "2022-06-29 03:42:03", "link": "http://arxiv.org/abs/2206.14388v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TweetNLP: Cutting-Edge Natural Language Processing for Social Media", "abstract": "In this paper we present TweetNLP, an integrated platform for Natural\nLanguage Processing (NLP) in social media. TweetNLP supports a diverse set of\nNLP tasks, including generic focus areas such as sentiment analysis and named\nentity recognition, as well as social media-specific tasks such as emoji\nprediction and offensive language identification. Task-specific systems are\npowered by reasonably-sized Transformer-based language models specialized on\nsocial media text (in particular, Twitter) which can be run without the need\nfor dedicated hardware or cloud services. The main contributions of TweetNLP\nare: (1) an integrated Python library for a modern toolkit supporting social\nmedia analysis using our various task-specific models adapted to the social\ndomain; (2) an interactive online demo for codeless experimentation using our\nmodels; and (3) a tutorial covering a wide variety of typical social media\napplications.", "published": "2022-06-29 17:16:58", "link": "http://arxiv.org/abs/2206.14774v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GPTs at Factify 2022: Prompt Aided Fact-Verification", "abstract": "One of the most pressing societal issues is the fight against false news. The\nfalse claims, as difficult as they are to expose, create a lot of damage. To\ntackle the problem, fact verification becomes crucial and thus has been a topic\nof interest among diverse research communities. Using only the textual form of\ndata we propose our solution to the problem and achieve competitive results\nwith other approaches. We present our solution based on two approaches - PLM\n(pre-trained language model) based method and Prompt based method. The\nPLM-based approach uses the traditional supervised learning, where the model is\ntrained to take 'x' as input and output prediction 'y' as P(y|x). Whereas,\nPrompt-based learning reflects the idea to design input to fit the model such\nthat the original objective may be re-framed as a problem of (masked) language\nmodeling. We may further stimulate the rich knowledge provided by PLMs to\nbetter serve downstream tasks by employing extra prompts to fine-tune PLMs. Our\nexperiments showed that the proposed method performs better than just\nfine-tuning PLMs. We achieved an F1 score of 0.6946 on the FACTIFY dataset and\na 7th position on the competition leader-board.", "published": "2022-06-29 21:07:39", "link": "http://arxiv.org/abs/2206.14913v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Distillation of Transformer-based Language Models Revisited", "abstract": "In the past few years, transformer-based pre-trained language models have\nachieved astounding success in both industry and academia. However, the large\nmodel size and high run-time latency are serious impediments to applying them\nin practice, especially on mobile phones and Internet of Things (IoT) devices.\nTo compress the model, considerable literature has grown up around the theme of\nknowledge distillation (KD) recently. Nevertheless, how KD works in\ntransformer-based models is still unclear. We tease apart the components of KD\nand propose a unified KD framework. Through the framework, systematic and\nextensive experiments that spent over 23,000 GPU hours render a comprehensive\nanalysis from the perspectives of knowledge types, matching strategies,\nwidth-depth trade-off, initialization, model size, etc. Our empirical results\nshed light on the distillation in the pre-train language model and with\nrelative significant improvement over previous state-of-the-arts(SOTA).\nFinally, we provide a best-practice guideline for the KD in transformer-based\nmodels.", "published": "2022-06-29 02:16:56", "link": "http://arxiv.org/abs/2206.14366v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language-specific Characteristic Assistance for Code-switching Speech\n  Recognition", "abstract": "Dual-encoder structure successfully utilizes two language-specific encoders\n(LSEs) for code-switching speech recognition. Because LSEs are initialized by\ntwo pre-trained language-specific models (LSMs), the dual-encoder structure can\nexploit sufficient monolingual data and capture the individual language\nattributes. However, most existing methods have no language constraints on LSEs\nand underutilize language-specific knowledge of LSMs. In this paper, we propose\na language-specific characteristic assistance (LSCA) method to mitigate the\nabove problems. Specifically, during training, we introduce two\nlanguage-specific losses as language constraints and generate corresponding\nlanguage-specific targets for them. During decoding, we take the decoding\nabilities of LSMs into account by combining the output probabilities of two\nLSMs and the mixture model to obtain the final predictions. Experiments show\nthat either the training or decoding method of LSCA can improve the model's\nperformance. Furthermore, the best result can obtain up to 15.4% relative error\nreduction on the code-switching test set by combining the training and decoding\nmethods of LSCA. Moreover, the system can process code-switching speech\nrecognition tasks well without extra shared parameters or even retraining based\non two pre-trained LSMs by using our method.", "published": "2022-06-29 13:39:51", "link": "http://arxiv.org/abs/2206.14580v3", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Simple and Effective Multi-sentence TTS with Expressive and Coherent\n  Prosody", "abstract": "Generating expressive and contextually appropriate prosody remains a\nchallenge for modern text-to-speech (TTS) systems. This is particularly evident\nfor long, multi-sentence inputs. In this paper, we examine simple extensions to\na Transformer-based FastSpeech-like system, with the goal of improving prosody\nfor multi-sentence TTS. We find that long context, powerful text features, and\ntraining on multi-speaker data all improve prosody. More interestingly, they\nresult in synergies. Long context disambiguates prosody, improves coherence,\nand plays to the strengths of Transformers. Fine-tuning word-level features\nfrom a powerful language model, such as BERT, appears to profit from more\ntraining data, readily available in a multi-speaker setting. We look into\nobjective metrics on pausing and pacing and perform thorough subjective\nevaluations for speech naturalness. Our main system, which incorporates all the\nextensions, achieves consistently strong results, including statistically\nsignificant improvements in speech naturalness over all its competitors.", "published": "2022-06-29 13:37:03", "link": "http://arxiv.org/abs/2206.14643v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Towards a Data-Driven Requirements Engineering Approach: Automatic\n  Analysis of User Reviews", "abstract": "We are concerned by Data Driven Requirements Engineering, and in particular\nthe consideration of user's reviews. These online reviews are a rich source of\ninformation for extracting new needs and improvement requests. In this work, we\nprovide an automated analysis using CamemBERT, which is a state-of-the-art\nlanguage model in French. We created a multi-label classification dataset of\n6000 user reviews from three applications in the Health & Fitness field. The\nresults are encouraging and suggest that it's possible to identify\nautomatically the reviews concerning requests for new features.\n  Dataset is available at:\nhttps://github.com/Jl-wei/APIA2022-French-user-reviews-classification-dataset.", "published": "2022-06-29 14:14:54", "link": "http://arxiv.org/abs/2206.14669v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Not Cheating on the Turing Test: Towards Grounded Language Learning in\n  Artificial Intelligence", "abstract": "Recent hype surrounding the increasing sophistication of language processing\nmodels has renewed optimism regarding machines achieving a human-like command\nof natural language. Research in the area of natural language understanding\n(NLU) in artificial intelligence claims to have been making great strides in\nthis area, however, the lack of conceptual clarity/consistency in how\n'understanding' is used in this and other disciplines makes it difficult to\ndiscern how close we actually are. In this interdisciplinary research thesis, I\nintegrate insights from cognitive science/psychology, philosophy of mind, and\ncognitive linguistics, and evaluate it against a critical review of current\napproaches in NLU to explore the basic requirements--and remaining\nchallenges--for developing artificially intelligent systems with human-like\ncapacities for language use and comprehension.", "published": "2022-06-29 14:19:48", "link": "http://arxiv.org/abs/2206.14672v4", "categories": ["cs.CL", "cs.AI", "68T01", "F.0; J.0; J.4; I.2"], "primary_category": "cs.CL"}
{"title": "Space-Efficient Representation of Entity-centric Query Language Models", "abstract": "Virtual assistants make use of automatic speech recognition (ASR) to help\nusers answer entity-centric queries. However, spoken entity recognition is a\ndifficult problem, due to the large number of frequently-changing named\nentities. In addition, resources available for recognition are constrained when\nASR is performed on-device.\n  In this work, we investigate the use of probabilistic grammars as language\nmodels within the finite-state transducer (FST) framework. We introduce a\ndeterministic approximation to probabilistic grammars that avoids the explicit\nexpansion of non-terminals at model creation time, integrates directly with the\nFST framework, and is complementary to n-gram models.\n  We obtain a 10% relative word error rate improvement on long tail entity\nqueries compared to when a similarly-sized n-gram model is used without our\nmethod.", "published": "2022-06-29 19:59:50", "link": "http://arxiv.org/abs/2206.14885v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Robustly Optimized Long Text to Math Models for Numerical Reasoning On\n  FinQA", "abstract": "Numerical reasoning is required when solving most problems in our life, but\nit has been neglected in previous artificial intelligence researches. FinQA\nchallenge has been organized to strengthen the study on numerical reasoning\nwhere the participants are asked to predict the numerical reasoning program to\nsolve financial question. The result of FinQA will be evaluated by both\nexecution accuracy and program accuracy. In this paper, we present our approach\nto tackle the task objective by developing models with different specialized\ncapabilities and fusing their strength. Overall, our approach achieves the 1st\nplace in FinQA challenge, with 71.93% execution accuracy and 67.03% program\naccuracy.", "published": "2022-06-29 12:10:18", "link": "http://arxiv.org/abs/2207.06490v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "OASYS: Domain-Agnostic Automated System for Constructing Knowledge Base\n  from Unstructured Text", "abstract": "In recent years, creating and managing knowledge bases have become crucial to\nthe retail product and enterprise domains. We present an automatic knowledge\nbase construction system that mines data from documents. This system can\ngenerate training data during the training process without human intervention.\nTherefore, it is domain-agnostic trainable using only the target domain text\ncorpus and a pre-defined knowledge base. This system is called OASYS and is the\nfirst system built with the Korean language in mind. In addition, we also have\nconstructed a new human-annotated benchmark dataset of the Korean Wikipedia\ncorpus paired with a Korean DBpedia to aid system evaluation. The system\nperformance results on human-annotated benchmark test dataset are meaningful\nand show that the generated knowledge base from OASYS trained on only\nauto-generated data is useful. We provide both a human-annotated test dataset\nand an auto-generated dataset.", "published": "2022-06-29 22:03:38", "link": "http://arxiv.org/abs/2207.07597v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "STOP: A dataset for Spoken Task Oriented Semantic Parsing", "abstract": "End-to-end spoken language understanding (SLU) predicts intent directly from\naudio using a single model. It promises to improve the performance of assistant\nsystems by leveraging acoustic information lost in the intermediate textual\nrepresentation and preventing cascading errors from Automatic Speech\nRecognition (ASR). Further, having one unified model has efficiency advantages\nwhen deploying assistant systems on-device. However, the limited number of\npublic audio datasets with semantic parse labels hinders the research progress\nin this area. In this paper, we release the Spoken Task-Oriented semantic\nParsing (STOP) dataset, the largest and most complex SLU dataset to be publicly\navailable. Additionally, we define low-resource splits to establish a benchmark\nfor improving SLU when limited labeled data is available. Furthermore, in\naddition to the human-recorded audio, we are releasing a TTS-generated version\nto benchmark the performance for low-resource domain adaptation of end-to-end\nSLU systems. Initial experimentation show end-to-end SLU models performing\nslightly worse than their cascaded counterparts, which we hope encourages\nfuture work in this direction.", "published": "2022-06-29 00:36:34", "link": "http://arxiv.org/abs/2207.10643v3", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Extreme compression of sentence-transformer ranker models: faster\n  inference, longer battery life, and less storage on edge devices", "abstract": "Modern search systems use several large ranker models with transformer\narchitectures. These models require large computational resources and are not\nsuitable for usage on devices with limited computational resources. Knowledge\ndistillation is a popular compression technique that can reduce the resource\nneeds of such models, where a large teacher model transfers knowledge to a\nsmall student model. To drastically reduce memory requirements and energy\nconsumption, we propose two extensions for a popular sentence-transformer\ndistillation procedure: generation of an optimal size vocabulary and\ndimensionality reduction of the embedding dimension of teachers prior to\ndistillation. We evaluate these extensions on two different types of ranker\nmodels. This results in extremely compressed student models whose analysis on a\ntest dataset shows the significance and utility of our proposed extensions.", "published": "2022-06-29 08:07:09", "link": "http://arxiv.org/abs/2207.12852v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "What Can Secondary Predictions Tell Us? An Exploration on\n  Question-Answering with SQuAD-v2.0", "abstract": "Performance in natural language processing, and specifically for the\nquestion-answer task, is typically measured by comparing a model\\'s most\nconfident (primary) prediction to golden answers (the ground truth). We are\nmaking the case that it is also useful to quantify how close a model came to\npredicting a correct answer even for examples that failed. We define the Golden\nRank (GR) of an example as the rank of its most confident prediction that\nexactly matches a ground truth, and show why such a match always exists. For\nthe 16 transformer models we analyzed, the majority of exactly matched golden\nanswers in secondary prediction space hover very close to the top rank. We\nrefer to secondary predictions as those ranking above 0 in descending\nconfidence probability order. We demonstrate how the GR can be used to classify\nquestions and visualize their spectrum of difficulty, from persistent near\nsuccesses to persistent extreme failures. We derive a new aggregate statistic\nover entire test sets, named the Golden Rank Interpolated Median (GRIM) that\nquantifies the proximity of failed predictions to the top choice made by the\nmodel. To develop some intuition and explore the applicability of these metrics\nwe use the Stanford Question Answering Dataset (SQuAD-2) and a few popular\ntransformer models from the Hugging Face hub. We first demonstrate that the\nGRIM is not directly correlated with the F1 and exact match (EM) scores. We\nthen calculate and visualize these scores for various transformer\narchitectures, probe their applicability in error analysis by clustering failed\npredictions, and compare how they relate to other training diagnostics such as\nthe EM and F1 scores. We finally suggest various research goals, such as\nbroadening data collection for these metrics and their possible use in\nadversarial training.", "published": "2022-06-29 01:17:47", "link": "http://arxiv.org/abs/2206.14348v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EBMs vs. CL: Exploring Self-Supervised Visual Pretraining for Visual\n  Question Answering", "abstract": "The availability of clean and diverse labeled data is a major roadblock for\ntraining models on complex tasks such as visual question answering (VQA). The\nextensive work on large vision-and-language models has shown that\nself-supervised learning is effective for pretraining multimodal interactions.\nIn this technical report, we focus on visual representations. We review and\nevaluate self-supervised methods to leverage unlabeled images and pretrain a\nmodel, which we then fine-tune on a custom VQA task that allows controlled\nevaluation and diagnosis. We compare energy-based models (EBMs) with\ncontrastive learning (CL). While EBMs are growing in popularity, they lack an\nevaluation on downstream tasks. We find that both EBMs and CL can learn\nrepresentations from unlabeled images that enable training a VQA model on very\nlittle annotated data. In a simple setting similar to CLEVR, we find that CL\nrepresentations also improve systematic generalization, and even match the\nperformance of representations from a larger, supervised, ImageNet-pretrained\nmodel. However, we find EBMs to be difficult to train because of instabilities\nand high variability in their results. Although EBMs prove useful for OOD\ndetection, other results on supervised energy-based training and uncertainty\ncalibration are largely negative. Overall, CL currently seems a preferable\noption over EBMs.", "published": "2022-06-29 01:44:23", "link": "http://arxiv.org/abs/2206.14355v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Using Twitter Data to Understand Public Perceptions of Approved versus\n  Off-label Use for COVID-19-related Medications", "abstract": "Understanding public discourse on emergency use of unproven therapeutics is\ncrucial for monitoring safe use and combating misinformation. We developed a\nnatural language processing-based pipeline to comprehend public perceptions of\nand stances on coronavirus disease 2019 (COVID-19)-related drugs on Twitter\nover time. This retrospective study included 609,189 US-based tweets from\nJanuary 29, 2020, to November 30, 2021, about four drugs that garnered\nsignificant public attention during the COVID-19 pandemic: (1)\nHydroxychloroquine and Ivermectin, therapies with anecdotal evidence; and (2)\nMolnupiravir and Remdesivir, FDA-approved treatments for eligible patients.\nTime-trend analysis was employed to understand popularity trends and related\nevents. Content and demographic analyses were conducted to explore potential\nrationales behind people's stances on each drug. Time-trend analysis indicated\nthat Hydroxychloroquine and Ivermectin were discussed more than Molnupiravir\nand Remdesivir, particularly during COVID-19 surges. Hydroxychloroquine and\nIvermectin discussions were highly politicized, related to conspiracy theories,\nhearsay, and celebrity influences. The distribution of stances between the two\nmajor US political parties was significantly different (P < .001); Republicans\nwere more likely to support Hydroxychloroquine (55%) and Ivermectin (30%) than\nDemocrats. People with healthcare backgrounds tended to oppose\nHydroxychloroquine (7%) more than the general population, while the general\npopulation was more likely to support Ivermectin (14%). Our study found that\nsocial media users have varying perceptions and stances on off-label versus\nFDA-authorized drug use at different stages of COVID-19. This indicates that\nhealth systems, regulatory agencies, and policymakers should design tailored\nstrategies to monitor and reduce misinformation to promote safe drug use.", "published": "2022-06-29 01:57:44", "link": "http://arxiv.org/abs/2206.14358v2", "categories": ["cs.CY", "cs.CL", "cs.LG", "stat.AP"], "primary_category": "cs.CY"}
{"title": "GERNERMED++: Transfer Learning in German Medical NLP", "abstract": "We present a statistical model for German medical natural language processing\ntrained for named entity recognition (NER) as an open, publicly available\nmodel. The work serves as a refined successor to our first GERNERMED model\nwhich is substantially outperformed by our work. We demonstrate the\neffectiveness of combining multiple techniques in order to achieve strong\nresults in entity recognition performance by the means of transfer-learning on\npretrained deep language models (LM), word-alignment and neural machine\ntranslation. Due to the sparse situation on open, public medical entity\nrecognition models for German texts, this work offers benefits to the German\nresearch community on medical NLP as a baseline model. Since our model is based\non public English data, its weights are provided without legal restrictions on\nusage and distribution. The sample code and the statistical model is available\nat: https://github.com/frankkramer-lab/GERNERMED-pp", "published": "2022-06-29 09:53:10", "link": "http://arxiv.org/abs/2206.14504v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Finstreder: Simple and fast Spoken Language Understanding with Finite\n  State Transducers using modern Speech-to-Text models", "abstract": "In Spoken Language Understanding (SLU) the task is to extract important\ninformation from audio commands, like the intent of what a user wants the\nsystem to do and special entities like locations or numbers. This paper\npresents a simple method for embedding intents and entities into Finite State\nTransducers, and, in combination with a pretrained general-purpose\nSpeech-to-Text model, allows building SLU-models without any additional\ntraining. Building those models is very fast and only takes a few seconds. It\nis also completely language independent. With a comparison on different\nbenchmarks it is shown that this method can outperform multiple other, more\nresource demanding SLU approaches.", "published": "2022-06-29 12:49:53", "link": "http://arxiv.org/abs/2206.14589v1", "categories": ["cs.CL", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Language-Based Audio Retrieval with Converging Tied Layers and\n  Contrastive Loss", "abstract": "In this paper, we tackle the new Language-Based Audio Retrieval task proposed\nin DCASE 2022. Firstly, we introduce a simple, scalable architecture which ties\nboth the audio and text encoder together. Secondly, we show that using this\narchitecture along with contrastive loss allows the model to significantly beat\nthe performance of the baseline model. Finally, in addition to having an\nextremely low training memory requirement, we are able to use pretrained models\nas it is without needing to finetune them. We test our methods and show that\nusing a combination of our methods beats the baseline scores significantly.", "published": "2022-06-29 13:59:19", "link": "http://arxiv.org/abs/2206.14659v1", "categories": ["cs.SD", "cs.CL", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The THUEE System Description for the IARPA OpenASR21 Challenge", "abstract": "This paper describes the THUEE team's speech recognition system for the IARPA\nOpen Automatic Speech Recognition Challenge (OpenASR21), with further\nexperiment explorations. We achieve outstanding results under both the\nConstrained and Constrained-plus training conditions. For the Constrained\ntraining condition, we construct our basic ASR system based on the standard\nhybrid architecture. To alleviate the Out-Of-Vocabulary (OOV) problem, we\nextend the pronunciation lexicon using Grapheme-to-Phoneme (G2P) techniques for\nboth OOV and potential new words. Standard acoustic model structures such as\nCNN-TDNN-F and CNN-TDNN-F-A are adopted. In addition, multiple data\naugmentation techniques are applied. For the Constrained-plus training\ncondition, we use the self-supervised learning framework wav2vec2.0. We\nexperiment with various fine-tuning techniques with the Connectionist Temporal\nClassification (CTC) criterion on top of the publicly available pre-trained\nmodel XLSR-53. We find that the frontend feature extractor plays an important\nrole when applying the wav2vec2.0 pre-trained model to the encoder-decoder\nbased CTC/Attention ASR architecture. Extra improvements can be achieved by\nusing the CTC model finetuned in the target language as the frontend feature\nextractor.", "published": "2022-06-29 14:03:05", "link": "http://arxiv.org/abs/2206.14660v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving Deliberation by Text-Only and Semi-Supervised Training", "abstract": "Text-only and semi-supervised training based on audio-only data has gained\npopularity recently due to the wide availability of unlabeled text and speech\ndata. In this work, we propose incorporating text-only and semi-supervised\ntraining into an attention-based deliberation model. By incorporating text-only\ndata in training a bidirectional encoder representation from transformer (BERT)\nfor the deliberation text encoder, and large-scale text-to-speech and\naudio-only utterances using joint acoustic and text decoder (JATD) and\nsemi-supervised training, we achieved 4%-12% WER reduction for various tasks\ncompared to the baseline deliberation. Compared to a state-of-the-art language\nmodel (LM) rescoring method, the deliberation model reduces the Google Voice\nSearch WER by 11% relative. We show that the deliberation model also achieves a\npositive human side-by-side evaluation compared to the state-of-the-art LM\nrescorer with reasonable endpointer latencies.", "published": "2022-06-29 15:30:44", "link": "http://arxiv.org/abs/2206.14716v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Trial2Vec: Zero-Shot Clinical Trial Document Similarity Search using\n  Self-Supervision", "abstract": "Clinical trials are essential for drug development but are extremely\nexpensive and time-consuming to conduct. It is beneficial to study similar\nhistorical trials when designing a clinical trial. However, lengthy trial\ndocuments and lack of labeled data make trial similarity search difficult. We\npropose a zero-shot clinical trial retrieval method, Trial2Vec, which learns\nthrough self-supervision without annotating similar clinical trials.\nSpecifically, the meta-structure of trial documents (e.g., title, eligibility\ncriteria, target disease) along with clinical knowledge (e.g., UMLS knowledge\nbase https://www.nlm.nih.gov/research/umls/index.html) are leveraged to\nautomatically generate contrastive samples. Besides, Trial2Vec encodes trial\ndocuments considering meta-structure thus producing compact embeddings\naggregating multi-aspect information from the whole document. We show that our\nmethod yields medically interpretable embeddings by visualization and it gets a\n15% average improvement over the best baselines on precision/recall for trial\nretrieval, which is evaluated on our labeled 1600 trial pairs. In addition, we\nprove the pre-trained embeddings benefit the downstream trial outcome\nprediction task over 240k trials. Software ias available at\nhttps://github.com/RyanWangZf/Trial2Vec.", "published": "2022-06-29 15:37:11", "link": "http://arxiv.org/abs/2206.14719v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "longhorns at DADC 2022: How many linguists does it take to fool a\n  Question Answering model? A systematic approach to adversarial attacks", "abstract": "Developing methods to adversarially challenge NLP systems is a promising\navenue for improving both model performance and interpretability. Here, we\ndescribe the approach of the team \"longhorns\" on Task 1 of the The First\nWorkshop on Dynamic Adversarial Data Collection (DADC), which asked teams to\nmanually fool a model on an Extractive Question Answering task. Our team\nfinished first, with a model error rate of 62%. We advocate for a systematic,\nlinguistically informed approach to formulating adversarial questions, and we\ndescribe the results of our pilot experiments, as well as our official\nsubmission.", "published": "2022-06-29 15:52:20", "link": "http://arxiv.org/abs/2206.14729v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "On the Robustness of Dialogue History Representation in Conversational\n  Question Answering: A Comprehensive Study and a New Prompt-based Method", "abstract": "Most works on modeling the conversation history in Conversational Question\nAnswering (CQA) report a single main result on a common CQA benchmark. While\nexisting models show impressive results on CQA leaderboards, it remains unclear\nwhether they are robust to shifts in setting (sometimes to more realistic\nones), training data size (e.g. from large to small sets) and domain. In this\nwork, we design and conduct the first large-scale robustness study of history\nmodeling approaches for CQA. We find that high benchmark scores do not\nnecessarily translate to strong robustness, and that various methods can\nperform extremely differently under different settings. Equipped with the\ninsights from our study, we design a novel prompt-based history modeling\napproach, and demonstrate its strong robustness across various settings. Our\napproach is inspired by existing methods that highlight historic answers in the\npassage. However, instead of highlighting by modifying the passage token\nembeddings, we add textual prompts directly in the passage text. Our approach\nis simple, easy-to-plug into practically any model, and highly effective, thus\nwe recommend it as a starting point for future model developers. We also hope\nthat our study and insights will raise awareness to the importance of\nrobustness-focused evaluation, in addition to obtaining high leaderboard\nscores, leading to better CQA systems.", "published": "2022-06-29 17:55:43", "link": "http://arxiv.org/abs/2206.14796v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Solving Quantitative Reasoning Problems with Language Models", "abstract": "Language models have achieved remarkable performance on a wide range of tasks\nthat require natural language understanding. Nevertheless, state-of-the-art\nmodels have generally struggled with tasks that require quantitative reasoning,\nsuch as solving mathematics, science, and engineering problems at the college\nlevel. To help close this gap, we introduce Minerva, a large language model\npretrained on general natural language data and further trained on technical\ncontent. The model achieves state-of-the-art performance on technical\nbenchmarks without the use of external tools. We also evaluate our model on\nover two hundred undergraduate-level problems in physics, biology, chemistry,\neconomics, and other sciences that require quantitative reasoning, and find\nthat the model can correctly answer nearly a third of them.", "published": "2022-06-29 18:54:49", "link": "http://arxiv.org/abs/2206.14858v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A light-weight full-band speech enhancement model", "abstract": "Deep neural network based full-band speech enhancement systems face\nchallenges of high demand of computational resources and imbalanced frequency\ndistribution. In this paper, a light-weight full-band model is proposed with\ntwo dedicated strategies, i.e., a learnable spectral compression mapping for\nmore effective high-band spectral information compression, and the utilization\nof the multi-head attention mechanism for more effective modeling of the global\nspectral pattern. Experiments validate the efficacy of the proposed strategies\nand show that the proposed model achieves competitive performance with only\n0.89M parameters.", "published": "2022-06-29 10:44:21", "link": "http://arxiv.org/abs/2206.14524v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On the Prediction Network Architecture in RNN-T for ASR", "abstract": "RNN-T models have gained popularity in the literature and in commercial\nsystems because of their competitiveness and capability of operating in online\nstreaming mode. In this work, we conduct an extensive study comparing several\nprediction network architectures for both monotonic and original RNN-T models.\nWe compare 4 types of prediction networks based on a common state-of-the-art\nConformer encoder and report results obtained on Librispeech and an internal\nmedical conversation data set. Our study covers both offline batch-mode and\nonline streaming scenarios. In contrast to some previous works, our results\nshow that Transformer does not always outperform LSTM when used as prediction\nnetwork along with Conformer encoder. Inspired by our scoreboard, we propose a\nnew simple prediction network architecture, N-Concat, that outperforms the\nothers in our on-line streaming benchmark. Transformer and n-gram reduced\narchitectures perform very similarly yet with some important distinct behaviour\nin terms of previous context. Overall we obtained up to 4.1 % relative WER\nimprovement compared to our LSTM baseline, while reducing prediction network\nparameters by nearly an order of magnitude (8.4 times).", "published": "2022-06-29 13:11:46", "link": "http://arxiv.org/abs/2206.14618v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Contextual Density Ratio for Language Model Biasing of Sequence to\n  Sequence ASR Systems", "abstract": "End-2-end (E2E) models have become increasingly popular in some ASR tasks\nbecause of their performance and advantages. These E2E models directly\napproximate the posterior distribution of tokens given the acoustic inputs.\nConsequently, the E2E systems implicitly define a language model (LM) over the\noutput tokens, which makes the exploitation of independently trained language\nmodels less straightforward than in conventional ASR systems. This makes it\ndifficult to dynamically adapt E2E ASR system to contextual profiles for better\nrecognizing special words such as named entities. In this work, we propose a\ncontextual density ratio approach for both training a contextual aware E2E\nmodel and adapting the language model to named entities. We apply the\naforementioned technique to an E2E ASR system, which transcribes doctor and\npatient conversations, for better adapting the E2E system to the names in the\nconversations. Our proposed technique achieves a relative improvement of up to\n46.5% on the names over an E2E baseline without degrading the overall\nrecognition accuracy of the whole test set. Moreover, it also surpasses a\ncontextual shallow fusion baseline by 22.1 % relative.", "published": "2022-06-29 13:12:46", "link": "http://arxiv.org/abs/2206.14623v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Nextformer: A ConvNeXt Augmented Conformer For End-To-End Speech\n  Recognition", "abstract": "Conformer models have achieved state-of-the-art(SOTA) results in end-to-end\nspeech recognition. However Conformer mainly focuses on temporal modeling while\npays less attention on time-frequency property of speech feature. In this paper\nwe augment Conformer with ConvNeXt and propose Nextformer structure. We use\nstacks of ConvNeXt block to replace the commonly used subsampling module in\nConformer for utilizing the information contained in time-frequency speech\nfeature. Besides, we insert an additional downsampling module in middle of\nConformer layers to make our model more efficient and accurate. We conduct\nexperiments on two opening datasets, AISHELL-1 and WenetSpeech. On AISHELL-1,\ncompared to Conformer baselines, Nextformer obtains 7.3% and 6.3% relative CER\nreduction in non-streaming and streaming mode respectively, and on a much\nlarger WenetSpeech dataset, Nextformer gives 5.0%~6.5% and 7.5%~14.6% relative\nCER reduction in non-streaming and streaming mode, while keep the computational\ncost FLOPs comparable to Conformer. To the best of our knowledge, the proposed\nNextformer model achieves SOTA results on AISHELL-1(CER 4.06%) and\nWenetSpeech(CER 7.56%/11.29%).", "published": "2022-06-29 16:20:54", "link": "http://arxiv.org/abs/2206.14747v2", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "iEmoTTS: Toward Robust Cross-Speaker Emotion Transfer and Control for\n  Speech Synthesis based on Disentanglement between Prosody and Timbre", "abstract": "The capability of generating speech with specific type of emotion is desired\nfor many applications of human-computer interaction. Cross-speaker emotion\ntransfer is a common approach to generating emotional speech when speech with\nemotion labels from target speakers is not available for model training. This\npaper presents a novel cross-speaker emotion transfer system, named iEmoTTS.\nThe system is composed of an emotion encoder, a prosody predictor, and a timbre\nencoder. The emotion encoder extracts the identity of emotion type as well as\nthe respective emotion intensity from the mel-spectrogram of input speech. The\nemotion intensity is measured by the posterior probability that the input\nutterance carries that emotion. The prosody predictor is used to provide\nprosodic features for emotion transfer. The timber encoder provides\ntimbre-related information for the system. Unlike many other studies which\nfocus on disentangling speaker and style factors of speech, the iEmoTTS is\ndesigned to achieve cross-speaker emotion transfer via disentanglement between\nprosody and timbre. Prosody is considered as the main carrier of\nemotion-related speech characteristics and timbre accounts for the essential\ncharacteristics for speaker identification. Zero-shot emotion transfer, meaning\nthat speech of target speakers are not seen in model training, is also realized\nwith iEmoTTS. Extensive experiments of subjective evaluation have been carried\nout. The results demonstrate the effectiveness of iEmoTTS as compared with\nother recently proposed systems of cross-speaker emotion transfer. It is shown\nthat iEmoTTS can produce speech with designated emotion type and controllable\nemotion intensity. With appropriate information bottleneck capacity, iEmoTTS is\nable to effectively transfer emotion information to a new speaker. Audio\nsamples are publicly available https://patrick-g-zhang.github.io/iemotts/", "published": "2022-06-29 19:11:42", "link": "http://arxiv.org/abs/2206.14866v2", "categories": ["eess.AS", "cs.HC"], "primary_category": "eess.AS"}
{"title": "Comparing Conventional Pitch Detection Algorithms with a Neural Network\n  Approach", "abstract": "Despite much research, traditional methods to pitch prediction are still not\nperfect. With the emergence of neural networks (NNs), researchers hope to\ncreate a NN-based pitch predictor that outperforms traditional methods. Three\npitch detection algorithms (PDAs), pYIN, YAAPT, and CREPE are compared in this\npaper. pYIN and YAAPT are conventional approaches considering time domain and\nfrequency domain processing. CREPE utilizes a data-trained deep convolutional\nneural network to estimate pitch. It involves 6 densely connected convolutional\nhidden layers and determines pitch probabilities for a given input signal. The\nperformance of CREPE representing neural network pitch predictors is compared\nto more classical approaches represented by pYIN and YAAPT. The figure of merit\n(FOM) will include the amount of unvoiced-to-voiced errors, voiced-to-voiced\nerrors, gross pitch errors, and fine pitch errors.", "published": "2022-06-29 01:53:07", "link": "http://arxiv.org/abs/2206.14357v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "DDKtor: Automatic Diadochokinetic Speech Analysis", "abstract": "Diadochokinetic speech tasks (DDK), in which participants repeatedly produce\nsyllables, are commonly used as part of the assessment of speech motor\nimpairments. These studies rely on manual analyses that are time-intensive,\nsubjective, and provide only a coarse-grained picture of speech. This paper\npresents two deep neural network models that automatically segment consonants\nand vowels from unannotated, untranscribed speech. Both models work on the raw\nwaveform and use convolutional layers for feature extraction. The first model\nis based on an LSTM classifier followed by fully connected layers, while the\nsecond model adds more convolutional layers followed by fully connected layers.\nThese segmentations predicted by the models are used to obtain measures of\nspeech rate and sound duration. Results on a young healthy individuals dataset\nshow that our LSTM model outperforms the current state-of-the-art systems and\nperforms comparably to trained human annotators. Moreover, the LSTM model also\npresents comparable results to trained human annotators when evaluated on\nunseen older individuals with Parkinson's Disease dataset.", "published": "2022-06-29 13:34:03", "link": "http://arxiv.org/abs/2206.14639v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DrumGAN VST: A Plugin for Drum Sound Analysis/Synthesis With\n  Autoencoding Generative Adversarial Networks", "abstract": "In contemporary popular music production, drum sound design is commonly\nperformed by cumbersome browsing and processing of pre-recorded samples in\nsound libraries. One can also use specialized synthesis hardware, typically\ncontrolled through low-level, musically meaningless parameters. Today, the\nfield of Deep Learning offers methods to control the synthesis process via\nlearned high-level features and allows generating a wide variety of sounds. In\nthis paper, we present DrumGAN VST, a plugin for synthesizing drum sounds using\na Generative Adversarial Network. DrumGAN VST operates on 44.1 kHz sample-rate\naudio, offers independent and continuous instrument class controls, and\nfeatures an encoding neural network that maps sounds into the GAN's latent\nspace, enabling resynthesis and manipulation of pre-existing drum sounds. We\nprovide numerous sound examples and a demo of the proposed VST plugin.", "published": "2022-06-29 15:44:19", "link": "http://arxiv.org/abs/2206.14723v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
