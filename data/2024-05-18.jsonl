{"title": "LG AI Research & KAIST at EHRSQL 2024: Self-Training Large Language\n  Models with Pseudo-Labeled Unanswerable Questions for a Reliable Text-to-SQL\n  System on EHRs", "abstract": "Text-to-SQL models are pivotal for making Electronic Health Records (EHRs)\naccessible to healthcare professionals without SQL knowledge. With the\nadvancements in large language models, these systems have become more adept at\ntranslating complex questions into SQL queries. Nonetheless, the critical need\nfor reliability in healthcare necessitates these models to accurately identify\nunanswerable questions or uncertain predictions, preventing misinformation. To\naddress this problem, we present a self-training strategy using pseudo-labeled\nunanswerable questions to enhance the reliability of text-to-SQL models for\nEHRs. This approach includes a two-stage training process followed by a\nfiltering method based on the token entropy and query execution. Our\nmethodology's effectiveness is validated by our top performance in the EHRSQL\n2024 shared task, showcasing the potential to improve healthcare\ndecision-making through more reliable text-to-SQL systems.", "published": "2024-05-18 03:25:44", "link": "http://arxiv.org/abs/2405.11162v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automating PTSD Diagnostics in Clinical Interviews: Leveraging Large\n  Language Models for Trauma Assessments", "abstract": "The shortage of clinical workforce presents significant challenges in mental\nhealthcare, limiting access to formal diagnostics and services. We aim to\ntackle this shortage by integrating a customized large language model (LLM)\ninto the workflow, thus promoting equity in mental healthcare for the general\npopulation. Although LLMs have showcased their capability in clinical\ndecision-making, their adaptation to severe conditions like Post-traumatic\nStress Disorder (PTSD) remains largely unexplored. Therefore, we collect 411\nclinician-administered diagnostic interviews and devise a novel approach to\nobtain high-quality data. Moreover, we build a comprehensive framework to\nautomate PTSD diagnostic assessments based on interview contents by leveraging\ntwo state-of-the-art LLMs, GPT-4 and Llama-2, with potential for broader\nclinical diagnoses. Our results illustrate strong promise for LLMs, tested on\nour dataset, to aid clinicians in diagnostic validation. To the best of our\nknowledge, this is the first AI system that fully automates assessments for\nmental illness based on clinician-administered interviews.", "published": "2024-05-18 05:04:18", "link": "http://arxiv.org/abs/2405.11178v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Designing NLP Systems That Adapt to Diverse Worldviews", "abstract": "Natural Language Inference (NLI) is foundational for evaluating language\nunderstanding in AI. However, progress has plateaued, with models failing on\nambiguous examples and exhibiting poor generalization. We argue that this stems\nfrom disregarding the subjective nature of meaning, which is intrinsically tied\nto an individual's \\textit{weltanschauung} (which roughly translates to\nworldview). Existing NLP datasets often obscure this by aggregating labels or\nfiltering out disagreement. We propose a perspectivist approach: building\ndatasets that capture annotator demographics, values, and justifications for\ntheir labels. Such datasets would explicitly model diverse worldviews. Our\ninitial experiments with a subset of the SBIC dataset demonstrate that even\nlimited annotator metadata can improve model performance.", "published": "2024-05-18 06:48:09", "link": "http://arxiv.org/abs/2405.11197v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LexGen: Domain-aware Multilingual Lexicon Generation", "abstract": "Lexicon or dictionary generation across domains is of significant societal\nimportance, as it can potentially enhance information accessibility for a\ndiverse user base while preserving language identity. Prior work in the field\nprimarily focuses on bilingual lexical induction, which deals with word\nalignments using mapping-based or corpora-based approaches. Though initiated by\nresearchers, the research associated with lexicon generation is limited, even\nmore so with domain-specific lexicons. This task becomes particularly important\nin atypical medical, engineering, and other technical domains, owing to the\nhighly infrequent usage of the terms and negligibly low data availability of\ntechnical terms in many low-resource languages. Owing to the research gap in\nlexicon generation, especially with a limited focus on the domain-specific\narea, we propose a new model to generate dictionary words for 6 Indian\nlanguages in the multi-domain setting. Our model consists of domain-specific\nand domain-generic layers that encode information, and these layers are invoked\nvia a learnable routing technique. Further, we propose an approach to\nexplicitly leverage the relatedness between these Indian languages toward\ncoherent translation. We also release a new benchmark dataset across 6 Indian\nlanguages that span 8 diverse domains that can propel further research in\ndomain-specific lexicon induction. We conduct both zero-shot and few-shot\nexperiments across multiple domains to show the efficacy of our proposed model\nin generalizing to unseen domains and unseen languages.", "published": "2024-05-18 07:02:43", "link": "http://arxiv.org/abs/2405.11200v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Text Identification Using CNN and Training Dynamics", "abstract": "We used Data Maps to model and characterize the AuTexTification dataset. This\nprovides insights about the behaviour of individual samples during training\nacross epochs (training dynamics). We characterized the samples across 3\ndimensions: confidence, variability and correctness. This shows the presence of\n3 regions: easy-to-learn, ambiguous and hard-to-learn examples. We used a\nclassic CNN architecture and found out that training the model only on a subset\nof ambiguous examples improves the model's out-of-distribution generalization.", "published": "2024-05-18 07:37:17", "link": "http://arxiv.org/abs/2405.11212v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer based neural networks for emotion recognition in\n  conversations", "abstract": "This paper outlines the approach of the ISDS-NLP team in the SemEval 2024\nTask 10: Emotion Discovery and Reasoning its Flip in Conversation (EDiReF). For\nSubtask 1 we obtained a weighted F1 score of 0.43 and placed 12 in the\nleaderboard. We investigate two distinct approaches: Masked Language Modeling\n(MLM) and Causal Language Modeling (CLM). For MLM, we employ pre-trained\nBERT-like models in a multilingual setting, fine-tuning them with a classifier\nto predict emotions. Experiments with varying input lengths, classifier\narchitectures, and fine-tuning strategies demonstrate the effectiveness of this\napproach. Additionally, we utilize Mistral 7B Instruct V0.2, a state-of-the-art\nmodel, applying zero-shot and few-shot prompting techniques. Our findings\nindicate that while Mistral shows promise, MLMs currently outperform them in\nsentence-level emotion classification.", "published": "2024-05-18 08:05:05", "link": "http://arxiv.org/abs/2405.11222v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MBIAS: Mitigating Bias in Large Language Models While Retaining Context", "abstract": "The deployment of Large Language Models (LLMs) in diverse applications\nnecessitates an assurance of safety without compromising the contextual\nintegrity of the generated content. Traditional approaches, including\nsafety-specific fine-tuning or adversarial testing, often yield safe outputs at\nthe expense of contextual meaning. This can result in a diminished capacity to\nhandle nuanced aspects of bias and toxicity, such as underrepresentation or\nnegative portrayals across various demographics. To address these challenges,\nwe introduce MBIAS, an LLM framework carefully instruction fine-tuned on a\ncustom dataset designed specifically for safety interventions. MBIAS is\ndesigned to significantly reduce biases and toxic elements in LLM outputs while\npreserving the main information. This work also details our further use of\nLLMs: as annotator under human supervision and as evaluator of generated\ncontent. Empirical analysis reveals that MBIAS achieves a reduction in bias and\ntoxicity by over 30\\% in standard evaluations, and by more than 90\\% in diverse\ndemographic tests, highlighting the robustness of our approach. We make the\ndataset and the fine-tuned model available to the research community for\nfurther investigation and ensure reproducibility. The code for this project can\nbe accessed here https://github.com/shainarazavi/MBIAS/tree/main.\n  Warning: This paper contains examples that may be offensive or upsetting.", "published": "2024-05-18 13:31:12", "link": "http://arxiv.org/abs/2405.11290v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling Key Aspects of Fine-Tuning in Sentence Embeddings: A\n  Representation Rank Analysis", "abstract": "The latest advancements in unsupervised learning of sentence embeddings\npredominantly involve employing contrastive learning-based (CL-based)\nfine-tuning over pre-trained language models. In this study, we analyze the\nlatest sentence embedding methods by adopting representation rank as the\nprimary tool of analysis. We first define Phase 1 and Phase 2 of fine-tuning\nbased on when representation rank peaks. Utilizing these phases, we conduct a\nthorough analysis and obtain essential findings across key aspects, including\nalignment and uniformity, linguistic abilities, and correlation between\nperformance and rank. For instance, we find that the dynamics of the key\naspects can undergo significant changes as fine-tuning transitions from Phase 1\nto Phase 2. Based on these findings, we experiment with a rank reduction (RR)\nstrategy that facilitates rapid and stable fine-tuning of the latest CL-based\nmethods. Through empirical investigations, we showcase the efficacy of RR in\nenhancing the performance and stability of five state-of-the-art sentence\nembedding methods.", "published": "2024-05-18 13:51:27", "link": "http://arxiv.org/abs/2405.11297v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Lack Understanding of Character Composition of\n  Words", "abstract": "Large language models (LLMs) have demonstrated remarkable performances on a\nwide range of natural language tasks. Yet, LLMs' successes have been largely\nrestricted to tasks concerning words, sentences, or documents, and it remains\nquestionable how much they understand the minimal units of text, namely\ncharacters. In this paper, we examine contemporary LLMs regarding their ability\nto understand character composition of words, and show that most of them fail\nto reliably carry out even the simple tasks that can be handled by humans with\nperfection. We analyze their behaviors with comparison to token level\nperformances, and discuss the potential directions for future research.", "published": "2024-05-18 18:08:58", "link": "http://arxiv.org/abs/2405.11357v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Modular LLMs by Building and Reusing a Library of LoRAs", "abstract": "The growing number of parameter-efficient adaptations of a base large\nlanguage model (LLM) calls for studying whether we can reuse such trained\nadapters to improve performance for new tasks. We study how to best build a\nlibrary of adapters given multi-task data and devise techniques for both\nzero-shot and supervised task generalization through routing in such library.\nWe benchmark existing approaches to build this library and introduce\nmodel-based clustering, MBC, a method that groups tasks based on the similarity\nof their adapter parameters, indirectly optimizing for transfer across the\nmulti-task dataset. To re-use the library, we present a novel zero-shot routing\nmechanism, Arrow, which enables dynamic selection of the most relevant adapters\nfor new inputs without the need for retraining. We experiment with several\nLLMs, such as Phi-2 and Mistral, on a wide array of held-out tasks, verifying\nthat MBC-based adapters and Arrow routing lead to superior generalization to\nnew tasks. We make steps towards creating modular, adaptable LLMs that can\nmatch or outperform traditional joint training.", "published": "2024-05-18 03:02:23", "link": "http://arxiv.org/abs/2405.11157v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Knowledge-Infused Automated Disease Diagnosis Assistant", "abstract": "With the advancement of internet communication and telemedicine, people are\nincreasingly turning to the web for various healthcare activities. With an\never-increasing number of diseases and symptoms, diagnosing patients becomes\nchallenging. In this work, we build a diagnosis assistant to assist doctors,\nwhich identifies diseases based on patient-doctor interaction. During\ndiagnosis, doctors utilize both symptomatology knowledge and diagnostic\nexperience to identify diseases accurately and efficiently. Inspired by this,\nwe investigate the role of medical knowledge in disease diagnosis through\ndoctor-patient interaction. We propose a two-channel, knowledge-infused,\ndiscourse-aware disease diagnosis model (KI-DDI), where the first channel\nencodes patient-doctor communication using a transformer-based encoder, while\nthe other creates an embedding of symptom-disease using a graph attention\nnetwork (GAT). In the next stage, the conversation and knowledge graph\nembeddings are infused together and fed to a deep neural network for disease\nidentification. Furthermore, we first develop an empathetic conversational\nmedical corpus comprising conversations between patients and doctors, annotated\nwith intent and symptoms information. The proposed model demonstrates a\nsignificant improvement over the existing state-of-the-art models, establishing\nthe crucial roles of (a) a doctor's effort for additional symptom extraction\n(in addition to patient self-report) and (b) infusing medical knowledge in\nidentifying diseases effectively. Many times, patients also show their medical\nconditions, which acts as crucial evidence in diagnosis. Therefore, integrating\nvisual sensory information would represent an effective avenue for enhancing\nthe capabilities of diagnostic assistants.", "published": "2024-05-18 05:18:50", "link": "http://arxiv.org/abs/2405.11181v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "BrainStorm @ iREL at #SMM4H 2024: Leveraging Translation and Topical\n  Embeddings for Annotation Detection in Tweets", "abstract": "The proliferation of LLMs in various NLP tasks has sparked debates regarding\ntheir reliability, particularly in annotation tasks where biases and\nhallucinations may arise. In this shared task, we address the challenge of\ndistinguishing annotations made by LLMs from those made by human domain experts\nin the context of COVID-19 symptom detection from tweets in Latin American\nSpanish. This paper presents BrainStorm @ iRELs approach to the SMM4H 2024\nShared Task, leveraging the inherent topical information in tweets, we propose\na novel approach to identify and classify annotations, aiming to enhance the\ntrustworthiness of annotated data.", "published": "2024-05-18 06:08:07", "link": "http://arxiv.org/abs/2405.11192v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "MemeMQA: Multimodal Question Answering for Memes via Rationale-Based\n  Inferencing", "abstract": "Memes have evolved as a prevalent medium for diverse communication, ranging\nfrom humour to propaganda. With the rising popularity of image-focused content,\nthere is a growing need to explore its potential harm from different aspects.\nPrevious studies have analyzed memes in closed settings - detecting harm,\napplying semantic labels, and offering natural language explanations. To extend\nthis research, we introduce MemeMQA, a multimodal question-answering framework\naiming to solicit accurate responses to structured questions while providing\ncoherent explanations. We curate MemeMQACorpus, a new dataset featuring 1,880\nquestions related to 1,122 memes with corresponding answer-explanation pairs.\nWe further propose ARSENAL, a novel two-stage multimodal framework that\nleverages the reasoning capabilities of LLMs to address MemeMQA. We benchmark\nMemeMQA using competitive baselines and demonstrate its superiority - ~18%\nenhanced answer prediction accuracy and distinct text generation lead across\nvarious metrics measuring lexical and semantic alignment over the best\nbaseline. We analyze ARSENAL's robustness through diversification of\nquestion-set, confounder-based evaluation regarding MemeMQA's generalizability,\nand modality-specific assessment, enhancing our understanding of meme\ninterpretation in the multimodal communication landscape.", "published": "2024-05-18 07:44:41", "link": "http://arxiv.org/abs/2405.11215v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Identifying and Aligning Medical Claims Made on Social Media with\n  Medical Evidence", "abstract": "Evidence-based medicine is the practice of making medical decisions that\nadhere to the latest, and best known evidence at that time. Currently, the best\nevidence is often found in the form of documents, such as randomized control\ntrials, meta-analyses and systematic reviews. This research focuses on aligning\nmedical claims made on social media platforms with this medical evidence. By\ndoing so, individuals without medical expertise can more effectively assess the\nveracity of such medical claims. We study three core tasks: identifying medical\nclaims, extracting medical vocabulary from these claims, and retrieving\nevidence relevant to those identified medical claims. We propose a novel system\nthat can generate synthetic medical claims to aid each of these core tasks. We\nadditionally introduce a novel dataset produced by our synthetic generator\nthat, when applied to these tasks, demonstrates not only a more flexible and\nholistic approach, but also an improvement in all comparable metrics. We make\nour dataset, the Expansive Medical Claim Corpus (EMCC), available at\nhttps://zenodo.org/records/8321460", "published": "2024-05-18 07:50:43", "link": "http://arxiv.org/abs/2405.11219v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "BadActs: A Universal Backdoor Defense in the Activation Space", "abstract": "Backdoor attacks pose an increasingly severe security threat to Deep Neural\nNetworks (DNNs) during their development stage. In response, backdoor sample\npurification has emerged as a promising defense mechanism, aiming to eliminate\nbackdoor triggers while preserving the integrity of the clean content in the\nsamples. However, existing approaches have been predominantly focused on the\nword space, which are ineffective against feature-space triggers and\nsignificantly impair performance on clean data. To address this, we introduce a\nuniversal backdoor defense that purifies backdoor samples in the activation\nspace by drawing abnormal activations towards optimized minimum clean\nactivation distribution intervals. The advantages of our approach are twofold:\n(1) By operating in the activation space, our method captures from\nsurface-level information like words to higher-level semantic concepts such as\nsyntax, thus counteracting diverse triggers; (2) the fine-grained continuous\nnature of the activation space allows for more precise preservation of clean\ncontent while removing triggers. Furthermore, we propose a detection module\nbased on statistical information of abnormal activations, to achieve a better\ntrade-off between clean accuracy and defending performance.", "published": "2024-05-18 08:32:37", "link": "http://arxiv.org/abs/2405.11227v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "WisPerMed at \"Discharge Me!\": Advancing Text Generation in Healthcare\n  with Large Language Models, Dynamic Expert Selection, and Priming Techniques\n  on MIMIC-IV", "abstract": "This study aims to leverage state of the art language models to automate\ngenerating the \"Brief Hospital Course\" and \"Discharge Instructions\" sections of\nDischarge Summaries from the MIMIC-IV dataset, reducing clinicians'\nadministrative workload. We investigate how automation can improve\ndocumentation accuracy, alleviate clinician burnout, and enhance operational\nefficacy in healthcare facilities. This research was conducted within our\nparticipation in the Shared Task Discharge Me! at BioNLP @ ACL 2024. Various\nstrategies were employed, including few-shot learning, instruction tuning, and\nDynamic Expert Selection (DES), to develop models capable of generating the\nrequired text sections. Notably, utilizing an additional clinical\ndomain-specific dataset demonstrated substantial potential to enhance clinical\nlanguage processing. The DES method, which optimizes the selection of text\noutputs from multiple predictions, proved to be especially effective. It\nachieved the highest overall score of 0.332 in the competition, surpassing\nsingle-model outputs. This finding suggests that advanced deep learning methods\nin combination with DES can effectively automate parts of electronic health\nrecord documentation. These advancements could enhance patient care by freeing\nclinician time for patient interactions. The integration of text selection\nstrategies represents a promising avenue for further research.", "published": "2024-05-18 10:56:45", "link": "http://arxiv.org/abs/2405.11255v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Language Assessment of Mathematical Capability of ChatGPT", "abstract": "This paper presents an evaluation of the mathematical capability of ChatGPT\nacross diverse languages like Hindi, Gujarati, and Marathi. ChatGPT, based on\nGPT-3.5 by OpenAI, has garnered significant attention for its natural language\nunderstanding and generation abilities. However, its performance in solving\nmathematical problems across multiple natural languages remains a comparatively\nunexplored area, especially in regional Indian languages. In this paper, we\nexplore those capabilities as well as using chain-of-thought prompting to\nfigure out if it increases the accuracy of responses as much as it does in the\nEnglish language and provide insights into the current limitations.", "published": "2024-05-18 11:29:19", "link": "http://arxiv.org/abs/2405.11264v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EnviroExam: Benchmarking Environmental Science Knowledge of Large\n  Language Models", "abstract": "In the field of environmental science, it is crucial to have robust\nevaluation metrics for large language models to ensure their efficacy and\naccuracy. We propose EnviroExam, a comprehensive evaluation method designed to\nassess the knowledge of large language models in the field of environmental\nscience. EnviroExam is based on the curricula of top international\nuniversities, covering undergraduate, master's, and doctoral courses, and\nincludes 936 questions across 42 core courses. By conducting 0-shot and 5-shot\ntests on 31 open-source large language models, EnviroExam reveals the\nperformance differences among these models in the domain of environmental\nscience and provides detailed evaluation standards. The results show that 61.3%\nof the models passed the 5-shot tests, while 48.39% passed the 0-shot tests. By\nintroducing the coefficient of variation as an indicator, we evaluate the\nperformance of mainstream open-source large language models in environmental\nscience from multiple perspectives, providing effective criteria for selecting\nand fine-tuning language models in this field. Future research will involve\nconstructing more domain-specific test sets using specialized environmental\nscience textbooks to further enhance the accuracy and specificity of the\nevaluation.", "published": "2024-05-18 11:31:03", "link": "http://arxiv.org/abs/2405.11265v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Estimating the Level of Dialectness Predicts Interannotator Agreement in\n  Multi-dialect Arabic Datasets", "abstract": "On annotating multi-dialect Arabic datasets, it is common to randomly assign\nthe samples across a pool of native Arabic speakers. Recent analyses\nrecommended routing dialectal samples to native speakers of their respective\ndialects to build higher-quality datasets. However, automatically identifying\nthe dialect of samples is hard. Moreover, the pool of annotators who are native\nspeakers of specific Arabic dialects might be scarce. Arabic Level of\nDialectness (ALDi) was recently introduced as a quantitative variable that\nmeasures how sentences diverge from Standard Arabic. On randomly assigning\nsamples to annotators, we hypothesize that samples of higher ALDi scores are\nharder to label especially if they are written in dialects that the annotators\ndo not speak. We test this by analyzing the relation between ALDi scores and\nthe annotators' agreement, on 15 public datasets having raw individual sample\nannotations for various sentence-classification tasks. We find strong evidence\nsupporting our hypothesis for 11 of them. Consequently, we recommend\nprioritizing routing samples of high ALDi scores to native speakers of each\nsample's dialect, for which the dialect could be automatically identified at\nhigher accuracies.", "published": "2024-05-18 12:58:02", "link": "http://arxiv.org/abs/2405.11282v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Fine-Grained Image Classifications via Cascaded Vision\n  Language Models", "abstract": "Fine-grained image classification, particularly in zero/few-shot scenarios,\npresents a significant challenge for vision-language models (VLMs), such as\nCLIP. These models often struggle with the nuanced task of distinguishing\nbetween semantically similar classes due to limitations in their pre-trained\nrecipe, which lacks supervision signals for fine-grained categorization. This\npaper introduces CascadeVLM, an innovative framework that overcomes the\nconstraints of previous CLIP-based methods by effectively leveraging the\ngranular knowledge encapsulated within large vision-language models (LVLMs).\nExperiments across various fine-grained image datasets demonstrate that\nCascadeVLM significantly outperforms existing models, specifically on the\nStanford Cars dataset, achieving an impressive 85.6% zero-shot accuracy.\nPerformance gain analysis validates that LVLMs produce more accurate\npredictions for challenging images that CLIPs are uncertain about, bringing the\noverall accuracy boost. Our framework sheds light on a holistic integration of\nVLMs and LVLMs for effective and efficient fine-grained image classification.", "published": "2024-05-18 14:12:04", "link": "http://arxiv.org/abs/2405.11301v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "MapCoder: Multi-Agent Code Generation for Competitive Problem Solving", "abstract": "Code synthesis, which requires a deep understanding of complex natural\nlanguage problem descriptions, generation of code instructions for complex\nalgorithms and data structures, and the successful execution of comprehensive\nunit tests, presents a significant challenge. While large language models\n(LLMs) demonstrate impressive proficiency in natural language processing, their\nperformance in code generation tasks remains limited. In this paper, we\nintroduce a new approach to code generation tasks leveraging multi-agent\nprompting that uniquely replicates the full cycle of program synthesis as\nobserved in human developers. Our framework, MapCoder, consists of four LLM\nagents specifically designed to emulate the stages of this cycle: recalling\nrelevant examples, planning, code generation, and debugging. After conducting\nthorough experiments, with multiple LLM ablations and analyses across eight\nchallenging competitive problem-solving and program synthesis benchmarks,\nMapCoder showcases remarkable code generation capabilities, achieving new\nstate-of-the-art results (pass@1) on HumanEval (93.9%), MBPP (83.1%), APPS\n(22.0%), CodeContests (28.5%), and xCodeEval (45.3%). Moreover, our method\nconsistently delivers superior performance across various programming languages\nand varying problem difficulties. We open-source our framework at\nhttps://github.com/Md-Ashraful-Pramanik/MapCoder.", "published": "2024-05-18 22:10:15", "link": "http://arxiv.org/abs/2405.11403v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Public LLMs be used for Self-Diagnosis of Medical Conditions ?", "abstract": "Advancements in deep learning have generated a large-scale interest in the\ndevelopment of foundational deep learning models. The development of Large\nLanguage Models (LLM) has evolved as a transformative paradigm in\nconversational tasks, which has led to its integration and extension even in\nthe critical domain of healthcare. With LLMs becoming widely popular and their\npublic access through open-source models and integration with other\napplications, there is a need to investigate their potential and limitations.\nOne such crucial task where LLMs are applied but require a deeper understanding\nis that of self-diagnosis of medical conditions based on bias-validating\nsymptoms in the interest of public health. The widespread integration of Gemini\nwith Google search and GPT-4.0 with Bing search has led to a shift in the trend\nof self-diagnosis using search engines to conversational LLM models. Owing to\nthe critical nature of the task, it is prudent to investigate and understand\nthe potential and limitations of public LLMs in the task of self-diagnosis. In\nthis study, we prepare a prompt engineered dataset of 10000 samples and test\nthe performance on the general task of self-diagnosis. We compared the\nperformance of both the state-of-the-art GPT-4.0 and the fee Gemini model on\nthe task of self-diagnosis and recorded contrasting accuracies of 63.07% and\n6.01%, respectively. We also discuss the challenges, limitations, and potential\nof both Gemini and GPT-4.0 for the task of self-diagnosis to facilitate future\nresearch and towards the broader impact of general public knowledge.\nFurthermore, we demonstrate the potential and improvement in performance for\nthe task of self-diagnosis using Retrieval Augmented Generation.", "published": "2024-05-18 22:43:44", "link": "http://arxiv.org/abs/2405.11407v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LeaPformer: Enabling Linear Transformers for Autoregressive and\n  Simultaneous Tasks via Learned Proportions", "abstract": "A promising approach to preserving model performance in linearized\ntransformers is to employ position-based re-weighting functions. However,\nstate-of-the-art re-weighting functions rely heavily on target sequence\nlengths, making it difficult or impossible to apply them to autoregressive and\nsimultaneous tasks, where the target and sometimes even the input sequence\nlength are unknown. To address this issue, we propose Learned Proportions\n(LeaP) and LeaPformers. Our contribution is built on two major components.\nFirst, we generalize the dependence on explicit positional representations and\nsequence lengths into dependence on sequence proportions for re-weighting.\nSecond, we replace static positional representations with dynamic proportions\nderived via a compact module, enabling more flexible attention concentration\npatterns. We evaluate LeaPformer against eight representative efficient\ntransformers on the Long-Range Arena benchmark, showing that LeaPformer\nachieves the best quality-throughput trade-off, as well as LeaPformer to\nWikitext-103 autoregressive language modeling and simultaneous speech-to-text\ntranslation for two language pairs, achieving competitive results.", "published": "2024-05-18 22:23:07", "link": "http://arxiv.org/abs/2405.13046v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EnterpriseEM: Fine-tuned Embeddings for Enterprise Semantic Search", "abstract": "Enterprises grapple with the significant challenge of managing proprietary\nunstructured data, hindering efficient information retrieval. This has led to\nthe emergence of AI-driven information retrieval solutions, designed to adeptly\nextract relevant insights to address employee inquiries. These solutions often\nleverage pre-trained embedding models and generative models as foundational\ncomponents. While pre-trained embeddings may exhibit proximity or disparity\nbased on their original training objectives, they might not fully align with\nthe unique characteristics of enterprise-specific data, leading to suboptimal\nalignment with the retrieval goals of enterprise environments. In this paper,\nwe propose a comprehensive methodology for contextualizing pre-trained\nembedding models to enterprise environments, covering the entire process from\ndata preparation to model fine-tuning and evaluation. By adapting the\nembeddings to better suit the retrieval tasks prevalent in enterprises, we aim\nto enhance the performance of information retrieval solutions. We discuss the\nprocess of fine-tuning, its effect on retrieval accuracy, and the potential\nbenefits for enterprise information management. Our findings demonstrate the\nefficacy of fine-tuned embedding models in improving the precision and\nrelevance of search results in enterprise settings.", "published": "2024-05-18 14:06:53", "link": "http://arxiv.org/abs/2406.00010v2", "categories": ["cs.IR", "cs.CL", "I.2.7"], "primary_category": "cs.IR"}
{"title": "Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts", "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) underscore\nthe significance of scalable models and data to boost performance, yet this\noften incurs substantial computational costs. Although the Mixture of Experts\n(MoE) architecture has been employed to efficiently scale large language and\nimage-text models, these efforts typically involve fewer experts and limited\nmodalities. To address this, our work presents the pioneering attempt to\ndevelop a unified MLLM with the MoE architecture, named Uni-MoE that can handle\na wide array of modalities. Specifically, it features modality-specific\nencoders with connectors for a unified multimodal representation. We also\nimplement a sparse MoE architecture within the LLMs to enable efficient\ntraining and inference through modality-level data parallelism and expert-level\nmodel parallelism. To enhance the multi-expert collaboration and\ngeneralization, we present a progressive training strategy: 1) Cross-modality\nalignment using various connectors with different cross-modality data, 2)\nTraining modality-specific experts with cross-modality instruction data to\nactivate experts' preferences, and 3) Tuning the Uni-MoE framework utilizing\nLow-Rank Adaptation (LoRA) on mixed multimodal instruction data. We evaluate\nthe instruction-tuned Uni-MoE on a comprehensive set of multimodal datasets.\nThe extensive experimental results demonstrate Uni-MoE's principal advantage of\nsignificantly reducing performance bias in handling mixed multimodal datasets,\nalongside improved multi-expert collaboration and generalization. Our findings\nhighlight the substantial potential of MoE frameworks in advancing MLLMs and\nthe code is available at\nhttps://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs.", "published": "2024-05-18 12:16:01", "link": "http://arxiv.org/abs/2405.11273v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.AI"}
{"title": "Action Controlled Paraphrasing", "abstract": "Recent studies have demonstrated the potential to control paraphrase\ngeneration, such as through syntax, which has broad applications in various\ndownstream tasks. However, these methods often require detailed parse trees or\nsyntactic exemplars, countering human-like paraphrasing behavior in language\nuse. Furthermore, an inference gap exists, as control specifications are only\navailable during training but not during inference. In this work, we propose a\nnew setup for controlled paraphrase generation. Specifically, we represent user\nintent as action tokens, embedding and concatenating them with text embeddings,\nthus flowing together into a self-attention encoder for representation fusion.\nTo address the inference gap, we introduce an optional action token as a\nplaceholder that encourages the model to determine the appropriate action\nindependently when users' intended actions are not provided. Experimental\nresults show that our method successfully enables precise action-controlled\nparaphrasing and preserves or even enhances performance compared to\nconventional uncontrolled methods when actions are not given. Our findings\npromote the concept of action-controlled paraphrasing for a more user-centered\ndesign.", "published": "2024-05-18 12:26:31", "link": "http://arxiv.org/abs/2405.11277v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Case-Based Reasoning Approach for Solving Financial Question Answering", "abstract": "Measuring a machine's understanding of human language often involves\nassessing its reasoning skills, i.e. logical process of deriving answers to\nquestions. While recent language models have shown remarkable proficiency in\ntext based tasks, their efficacy in complex reasoning problems involving\nheterogeneous information such as text, tables, and numbers remain uncertain.\nAddressing this gap, FinQA introduced a numerical reasoning dataset for\nfinancial documents and simultaneously proposed a program generation approach .\nOur investigation reveals that half of the errors (48%) stem from incorrect\noperations being generated. To address this issue, we propose a novel approach\nto tackle numerical reasoning problems using case based reasoning (CBR), an\nartificial intelligence paradigm that provides problem solving guidance by\noffering similar cases (i.e. similar questions and corresponding logical\nprograms). Our model retrieves relevant cases to address a given question, and\nthen generates an answer based on the retrieved cases and contextual\ninformation. Through experiments on the FinQA dataset, we demonstrate\ncompetitive performance of our approach and additionally show that by expanding\ncase repository, we can help solving complex multi step programs which FinQA\nshowed weakness of.", "published": "2024-05-18 10:06:55", "link": "http://arxiv.org/abs/2405.13044v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring speech style spaces with language models: Emotional TTS\n  without emotion labels", "abstract": "Many frameworks for emotional text-to-speech (E-TTS) rely on human-annotated\nemotion labels that are often inaccurate and difficult to obtain. Learning\nemotional prosody implicitly presents a tough challenge due to the subjective\nnature of emotions. In this study, we propose a novel approach that leverages\ntext awareness to acquire emotional styles without the need for explicit\nemotion labels or text prompts. We present TEMOTTS, a two-stage framework for\nE-TTS that is trained without emotion labels and is capable of inference\nwithout auxiliary inputs. Our proposed method performs knowledge transfer\nbetween the linguistic space learned by BERT and the emotional style space\nconstructed by global style tokens. Our experimental results demonstrate the\neffectiveness of our proposed framework, showcasing improvements in emotional\naccuracy and naturalness. This is one of the first studies to leverage the\nemotional correlation between spoken content and expressive delivery for\nemotional TTS.", "published": "2024-05-18 23:21:39", "link": "http://arxiv.org/abs/2405.11413v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
