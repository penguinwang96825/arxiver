{"title": "Improving Correlation with Human Judgments by Integrating Semantic\n  Similarity with Second--Order Vectors", "abstract": "Vector space methods that measure semantic similarity and relatedness often\nrely on distributional information such as co--occurrence frequencies or\nstatistical measures of association to weight the importance of particular\nco--occurrences. In this paper, we extend these methods by incorporating a\nmeasure of semantic similarity based on a human curated taxonomy into a\nsecond--order vector representation. This results in a measure of semantic\nrelatedness that combines both the contextual information available in a\ncorpus--based vector space representation with the semantic knowledge found in\na biomedical ontology. Our results show that incorporating semantic similarity\ninto a second order co--occurrence matrices improves correlation with human\njudgments for both similarity and relatedness, and that our method compares\nfavorably to various different word embedding methods that have recently been\nevaluated on the same reference standards we have used.", "published": "2016-09-02 11:44:17", "link": "http://arxiv.org/abs/1609.00559v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Skipping Word: A Character-Sequential Representation based Framework for\n  Question Answering", "abstract": "Recent works using artificial neural networks based on word distributed\nrepresentation greatly boost the performance of various natural language\nlearning tasks, especially question answering. Though, they also carry along\nwith some attendant problems, such as corpus selection for embedding learning,\ndictionary transformation for different learning tasks, etc. In this paper, we\npropose to straightforwardly model sentences by means of character sequences,\nand then utilize convolutional neural networks to integrate character embedding\nlearning together with point-wise answer selection training. Compared with deep\nmodels pre-trained on word embedding (WE) strategy, our character-sequential\nrepresentation (CSR) based method shows a much simpler procedure and more\nstable performance across different benchmarks. Extensive experiments on two\nbenchmark answer selection datasets exhibit the competitive performance\ncompared with the state-of-the-art methods.", "published": "2016-09-02 11:57:46", "link": "http://arxiv.org/abs/1609.00565v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Citation Classification for Behavioral Analysis of a Scientific Field", "abstract": "Citations are an important indicator of the state of a scientific field,\nreflecting how authors frame their work, and influencing uptake by future\nscholars. However, our understanding of citation behavior has been limited to\nsmall-scale manual citation analysis. We perform the largest behavioral study\nof citations to date, analyzing how citations are both framed and taken up by\nscholars in one entire field: natural language processing. We introduce a new\ndataset of nearly 2,000 citations annotated for function and centrality, and\nuse it to develop a state-of-the-art classifier and label the entire ACL\nReference Corpus. We then study how citations are framed by authors and use\nboth papers and online traces to track how citations are followed by readers.\nWe demonstrate that authors are sensitive to discourse structure and\npublication venue when citing, that online readers follow temporal links to\nprevious and future work rather than methodological links, and that how a paper\ncites related work is predictive of its citation count. Finally, we use changes\nin citation roles to show that the field of NLP is undergoing a significant\nincrease in consensus.", "published": "2016-09-02 00:40:15", "link": "http://arxiv.org/abs/1609.00435v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "SynsetRank: Degree-adjusted Random Walk for Relation Identification", "abstract": "In relation extraction, a key process is to obtain good detectors that find\nrelevant sentences describing the target relation. To minimize the necessity of\nlabeled data for refining detectors, previous work successfully made use of\nBabelNet, a semantic graph structure expressing relationships between synsets,\nas side information or prior knowledge. The goal of this paper is to enhance\nthe use of graph structure in the framework of random walk with a few\nadjustable parameters. Actually, a straightforward application of random walk\ndegrades the performance even after parameter optimization. With the insight\nfrom this unsuccessful trial, we propose SynsetRank, which adjusts the initial\nprobability so that high degree nodes influence the neighbors as strong as low\ndegree nodes. In our experiment on 13 relations in the FB15K-237 dataset,\nSynsetRank significantly outperforms baselines and the plain random walk\napproach.", "published": "2016-09-02 14:42:18", "link": "http://arxiv.org/abs/1609.00626v2", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "The Semantic Knowledge Graph: A compact, auto-generated model for\n  real-time traversal and ranking of any relationship within a domain", "abstract": "This paper describes a new kind of knowledge representation and mining system\nwhich we are calling the Semantic Knowledge Graph. At its heart, the Semantic\nKnowledge Graph leverages an inverted index, along with a complementary\nuninverted index, to represent nodes (terms) and edges (the documents within\nintersecting postings lists for multiple terms/nodes). This provides a layer of\nindirection between each pair of nodes and their corresponding edge, enabling\nedges to materialize dynamically from underlying corpus statistics. As a\nresult, any combination of nodes can have edges to any other nodes materialize\nand be scored to reveal latent relationships between the nodes. This provides\nnumerous benefits: the knowledge graph can be built automatically from a\nreal-world corpus of data, new nodes - along with their combined edges - can be\ninstantly materialized from any arbitrary combination of preexisting nodes\n(using set operations), and a full model of the semantic relationships between\nall entities within a domain can be represented and dynamically traversed using\na highly compact representation of the graph. Such a system has widespread\napplications in areas as diverse as knowledge modeling and reasoning, natural\nlanguage processing, anomaly detection, data cleansing, semantic search,\nanalytics, data classification, root cause analysis, and recommendations\nsystems. The main contribution of this paper is the introduction of a novel\nsystem - the Semantic Knowledge Graph - which is able to dynamically discover\nand score interesting relationships between any arbitrary combination of\nentities (words, phrases, or extracted concepts) through dynamically\nmaterializing nodes and edges from a compact graphical representation built\nautomatically from a corpus of data representative of a knowledge domain.", "published": "2016-09-02 04:26:54", "link": "http://arxiv.org/abs/1609.00464v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "On Horizontal and Vertical Separation in Hierarchical Text\n  Classification", "abstract": "Hierarchy is a common and effective way of organizing data and representing\ntheir relationships at different levels of abstraction. However, hierarchical\ndata dependencies cause difficulties in the estimation of \"separable\" models\nthat can distinguish between the entities in the hierarchy. Extracting\nseparable models of hierarchical entities requires us to take their relative\nposition into account and to consider the different types of dependencies in\nthe hierarchy. In this paper, we present an investigation of the effect of\nseparability in text-based entity classification and argue that in hierarchical\nclassification, a separation property should be established between entities\nnot only in the same layer, but also in different layers. Our main findings are\nthe followings. First, we analyse the importance of separability on the data\nrepresentation in the task of classification and based on that, we introduce a\n\"Strong Separation Principle\" for optimizing expected effectiveness of\nclassifiers decision based on separation property. Second, we present\nHierarchical Significant Words Language Models (HSWLM) which capture all, and\nonly, the essential features of hierarchical entities according to their\nrelative position in the hierarchy resulting in horizontally and vertically\nseparable models. Third, we validate our claims on real-world data and\ndemonstrate that how HSWLM improves the accuracy of classification and how it\nprovides transferable models over time. Although discussions in this paper\nfocus on the classification problem, the models are applicable to any\ninformation access tasks on data that has, or can be mapped to, a hierarchical\nstructure.", "published": "2016-09-02 09:21:33", "link": "http://arxiv.org/abs/1609.00514v1", "categories": ["cs.IR", "cs.CL", "cs.IT", "math.IT", "68P20"], "primary_category": "cs.IR"}
