{"title": "Enforcing Consistency in Weakly Supervised Semantic Parsing", "abstract": "The predominant challenge in weakly supervised semantic parsing is that of\nspurious programs that evaluate to correct answers for the wrong reasons. Prior\nwork uses elaborate search strategies to mitigate the prevalence of spurious\nprograms; however, they typically consider only one input at a time. In this\nwork we explore the use of consistency between the output programs for related\ninputs to reduce the impact of spurious programs. We bias the program search\n(and thus the model's training signal) towards programs that map the same\nphrase in related inputs to the same sub-parts in their respective programs.\nAdditionally, we study the importance of designing logical formalisms that\nfacilitate this kind of consAistency-based training. We find that a more\nconsistent formalism leads to improved model performance even without\nconsistency-based training. When combined together, these two insights lead to\na 10% absolute improvement over the best prior result on the Natural Language\nVisual Reasoning dataset.", "published": "2021-07-13 03:48:04", "link": "http://arxiv.org/abs/2107.05833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Gender Augmented Data for NLP", "abstract": "Gender bias is a frequent occurrence in NLP-based applications, especially\npronounced in gender-inflected languages. Bias can appear through associations\nof certain adjectives and animate nouns with the natural gender of referents,\nbut also due to unbalanced grammatical gender frequencies of inflected words.\nThis type of bias becomes more evident in generating conversational utterances\nwhere gender is not specified within the sentence, because most current NLP\napplications still work on a sentence-level context. As a step towards more\ninclusive NLP, this paper proposes an automatic and generalisable rewriting\napproach for short conversational sentences. The rewriting method can be\napplied to sentences that, without extra-sentential context, have multiple\nequivalent alternatives in terms of gender. The method can be applied both for\ncreating gender balanced outputs as well as for creating gender balanced\ntraining data. The proposed approach is based on a neural machine translation\n(NMT) system trained to 'translate' from one gender alternative to another.\nBoth the automatic and manual analysis of the approach show promising results\nfor automatic generation of gender alternatives for conversational sentences in\nSpanish.", "published": "2021-07-13 11:13:21", "link": "http://arxiv.org/abs/2107.05987v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot Speech Translation", "abstract": "Speech Translation (ST) is the task of translating speech in one language\ninto text in another language. Traditional cascaded approaches for ST, using\nAutomatic Speech Recognition (ASR) and Machine Translation (MT) systems, are\nprone to error propagation. End-to-end approaches use only one system to avoid\npropagating error, yet are difficult to employ due to data scarcity. We explore\nzero-shot translation, which enables translating a pair of languages that is\nunseen during training, thus avoid the use of end-to-end ST data. Zero-shot\ntranslation has been shown to work for multilingual machine translation, yet\nhas not been studied for speech translation. We attempt to build zero-shot ST\nmodels that are trained only on ASR and MT tasks but can do ST task during\ninference. The challenge is that the representation of text and audio is\nsignificantly different, thus the models learn ASR and MT tasks in different\nways, making it non-trivial to perform zero-shot. These models tend to output\nthe wrong language when performing zero-shot ST. We tackle the issues by\nincluding additional training data and an auxiliary loss function that\nminimizes the text-audio difference. Our experiment results and analysis show\nthat the methods are promising for zero-shot ST. Moreover, our methods are\nparticularly useful in the few-shot settings where a limited amount of ST data\nis available, with improvements of up to +11.8 BLEU points compared to direct\nend-to-end ST models and +3.9 BLEU points compared to ST models fine-tuned from\npre-trained ASR model.", "published": "2021-07-13 12:00:44", "link": "http://arxiv.org/abs/2107.06010v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Rating Facts under Coarse-to-fine Regimes", "abstract": "The rise of manipulating fake news as a political weapon has become a global\nconcern and highlighted the incapability of manually fact checking against\nrapidly produced fake news. Thus, statistical approaches are required if we are\nto address this problem efficiently. The shortage of publicly available\ndatasets is one major bottleneck of automated fact checking. To remedy this, we\ncollected 24K manually rated statements from PolitiFact. The class values\nexhibit a natural order with respect to truthfulness as shown in Table 1. Thus,\nour task represents a twist from standard classification, due to the various\ndegrees of similarity between classes. To investigate this, we defined\ncoarse-to-fine classification regimes, which presents new challenge for\nclassification. To address this, we propose BERT-based models. After training,\nclass similarity is sensible over the multi-class datasets, especially in the\nfine-grained one. Under all the regimes, BERT achieves state of the art, while\nthe additional layers provide insignificant improvement.", "published": "2021-07-13 13:05:11", "link": "http://arxiv.org/abs/2107.06051v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Difficulty of Translating Free-Order Case-Marking Languages", "abstract": "Identifying factors that make certain languages harder to model than others\nis essential to reach language equality in future Natural Language Processing\ntechnologies. Free-order case-marking languages, such as Russian, Latin or\nTamil, have proved more challenging than fixed-order languages for the tasks of\nsyntactic parsing and subject-verb agreement prediction. In this work, we\ninvestigate whether this class of languages is also more difficult to translate\nby state-of-the-art Neural Machine Translation models (NMT). Using a variety of\nsynthetic languages and a newly introduced translation challenge set, we find\nthat word order flexibility in the source language only leads to a very small\nloss of NMT quality, even though the core verb arguments become impossible to\ndisambiguate in sentences without semantic cues. The latter issue is indeed\nsolved by the addition of case marking. However, in medium- and low-resource\nsettings, the overall NMT quality of fixed-order languages remains unmatched.", "published": "2021-07-13 13:09:58", "link": "http://arxiv.org/abs/2107.06055v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Between Flexibility and Consistency: Joint Generation of Captions and\n  Subtitles", "abstract": "Speech translation (ST) has lately received growing interest for the\ngeneration of subtitles without the need for an intermediate source language\ntranscription and timing (i.e. captions). However, the joint generation of\nsource captions and target subtitles does not only bring potential output\nquality advantages when the two decoding processes inform each other, but it is\nalso often required in multilingual scenarios. In this work, we focus on ST\nmodels which generate consistent captions-subtitles in terms of structure and\nlexical content. We further introduce new metrics for evaluating subtitling\nconsistency. Our findings show that joint decoding leads to increased\nperformance and consistency between the generated captions and subtitles while\nstill allowing for sufficient flexibility to produce subtitles conforming to\nlanguage-specific needs and norms.", "published": "2021-07-13 17:06:04", "link": "http://arxiv.org/abs/2107.06246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What do writing features tell us about AI papers?", "abstract": "As the numbers of submissions to conferences grow quickly, the task of\nassessing the quality of academic papers automatically, convincingly, and with\nhigh accuracy attracts increasing attention. We argue that studying\ninterpretable dimensions of these submissions could lead to scalable solutions.\nWe extract a collection of writing features, and construct a suite of\nprediction tasks to assess the usefulness of these features in predicting\ncitation counts and the publication of AI-related papers. Depending on the\nvenues, the writing features can predict the conference vs. workshop appearance\nwith F1 scores up to 60-90, sometimes even outperforming the content-based\ntf-idf features and RoBERTa. We show that the features describe writing style\nmore than content. To further understand the results, we estimate the causal\nimpact of the most indicative features. Our analysis on writing features\nprovides a perspective to assessing and refining the writing of academic\narticles at scale.", "published": "2021-07-13 18:12:12", "link": "http://arxiv.org/abs/2107.06310v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human Attention during Goal-directed Reading Comprehension Relies on\n  Task Optimization", "abstract": "The computational principles underlying attention allocation in complex\ngoal-directed tasks remain elusive. Goal-directed reading, i.e., reading a\npassage to answer a question in mind, is a common real-world task that strongly\nengages attention. Here, we investigate what computational models can explain\nattention distribution in this complex task. We show that the reading time on\neach word is predicted by the attention weights in transformer-based deep\nneural networks (DNNs) optimized to perform the same reading task. Eye-tracking\nfurther reveals that readers separately attend to basic text features and\nquestion-relevant information during first-pass reading and rereading,\nrespectively. Similarly, text features and question relevance separately\nmodulate attention weights in shallow and deep DNN layers. Furthermore, when\nreaders scan a passage without a question in mind, their reading time is\npredicted by DNNs optimized for a word prediction task. Therefore, attention\nduring real-world reading can be interpreted as the consequence of task\noptimization.", "published": "2021-07-13 01:07:22", "link": "http://arxiv.org/abs/2107.05799v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Dialogue-based Information Extraction System for Medical Insurance\n  Assessment", "abstract": "In the Chinese medical insurance industry, the assessor's role is essential\nand requires significant efforts to converse with the claimant. This is a\nhighly professional job that involves many parts, such as identifying personal\ninformation, collecting related evidence, and making a final insurance report.\nDue to the coronavirus (COVID-19) pandemic, the previous offline insurance\nassessment has to be conducted online. However, for the junior assessor often\nlacking practical experience, it is not easy to quickly handle such a complex\nonline procedure, yet this is important as the insurance company needs to\ndecide how much compensation the claimant should receive based on the\nassessor's feedback. In order to promote assessors' work efficiency and speed\nup the overall procedure, in this paper, we propose a dialogue-based\ninformation extraction system that integrates advanced NLP technologies for\nmedical insurance assessment. With the assistance of our system, the average\ntime cost of the procedure is reduced from 55 minutes to 35 minutes, and the\ntotal human resources cost is saved 30% compared with the previous offline\nprocedure. Until now, the system has already served thousands of online claim\ncases.", "published": "2021-07-13 06:14:08", "link": "http://arxiv.org/abs/2107.05866v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Exploiting Network Structures to Improve Semantic Representation for the\n  Financial Domain", "abstract": "This paper presents the participation of the MiniTrue team in the FinSim-3\nshared task on learning semantic similarities for the financial domain in\nEnglish language. Our approach combines contextual embeddings learned by\ntransformer-based language models with network structures embeddings extracted\non external knowledge sources, to create more meaningful representations of\nfinancial domain entities and terms. For this, two BERT based language models\nand a knowledge graph embedding model are used. Besides, we propose a voting\nfunction to joint three basic models for the final inference. Experimental\nresults show that the model with the knowledge graph embeddings has achieved a\nsuperior result than these models with only contextual embeddings.\nNevertheless, we also observe that our voting function brings an extra benefit\nto the final system.", "published": "2021-07-13 07:32:18", "link": "http://arxiv.org/abs/2107.05885v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Indian Legal NLP Benchmarks : A Survey", "abstract": "Availability of challenging benchmarks is the key to advancement of AI in a\nspecific field.Since Legal Text is significantly different than normal English\ntext, there is a need to create separate Natural Language Processing benchmarks\nfor Indian Legal Text which are challenging and focus on tasks specific to\nLegal Systems. This will spur innovation in applications of Natural language\nProcessing for Indian Legal Text and will benefit AI community and Legal\nfraternity. We review the existing work in this area and propose ideas to\ncreate new benchmarks for Indian Legal Natural Language Processing.", "published": "2021-07-13 13:10:10", "link": "http://arxiv.org/abs/2107.06056v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TSCAN : Dialog Structure discovery using SCAN", "abstract": "Can we discover dialog structure by dividing utterances into labelled\nclusters. Can these labels be generated from the data. Typically for dialogs we\nneed an ontology and use that to discover structure, however by using\nunsupervised classification and self-labelling we are able to intuit this\nstructure without any labels or ontology. In this paper we apply SCAN (Semantic\nClustering using Nearest Neighbors) to dialog data. We used BERT for pretext\ntask and an adaptation of SCAN for clustering and self labeling. These clusters\nare used to identify transition probabilities and create the dialog structure.\nThe self-labelling method used for SCAN makes these structures interpretable as\nevery cluster has a label. As the approach is unsupervised, evaluation metrics\nis a challenge, we use statistical measures as proxies for structure quality", "published": "2021-07-13 22:55:07", "link": "http://arxiv.org/abs/2107.06426v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semiparametric Latent Topic Modeling on Consumer-Generated Corpora", "abstract": "Legacy procedures for topic modelling have generally suffered problems of\noverfitting and a weakness towards reconstructing sparse topic structures. With\nmotivation from a consumer-generated corpora, this paper proposes\nsemiparametric topic model, a two-step approach utilizing nonnegative matrix\nfactorization and semiparametric regression in topic modeling. The model\nenables the reconstruction of sparse topic structures in the corpus and\nprovides a generative model for predicting topics in new documents entering the\ncorpus. Assuming the presence of auxiliary information related to the topics,\nthis approach exhibits better performance in discovering underlying topic\nstructures in cases where the corpora are small and limited in vocabulary. In\nan actual consumer feedback corpus, the model also demonstrably provides\ninterpretable and useful topic definitions comparable with those produced by\nother methods.", "published": "2021-07-13 00:22:02", "link": "http://arxiv.org/abs/2107.10651v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FairyTailor: A Multimodal Generative Framework for Storytelling", "abstract": "Storytelling is an open-ended task that entails creative thinking and\nrequires a constant flow of ideas. Natural language generation (NLG) for\nstorytelling is especially challenging because it requires the generated text\nto follow an overall theme while remaining creative and diverse to engage the\nreader. In this work, we introduce a system and a web-based demo, FairyTailor,\nfor human-in-the-loop visual story co-creation. Users can create a cohesive\nchildren's fairytale by weaving generated texts and retrieved images with their\ninput. FairyTailor adds another modality and modifies the text generation\nprocess to produce a coherent and creative sequence of text and images. To our\nknowledge, this is the first dynamic tool for multimodal story generation that\nallows interactive co-formation of both texts and images. It allows users to\ngive feedback on co-created stories and share their results.", "published": "2021-07-13 02:45:08", "link": "http://arxiv.org/abs/2108.04324v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A Configurable Multilingual Model is All You Need to Recognize All\n  Languages", "abstract": "Multilingual automatic speech recognition (ASR) models have shown great\npromise in recent years because of the simplified model training and deployment\nprocess. Conventional methods either train a universal multilingual model\nwithout taking any language information or with a 1-hot language ID (LID)\nvector to guide the recognition of the target language. In practice, the user\ncan be prompted to pre-select several languages he/she can speak. The\nmultilingual model without LID cannot well utilize the language information set\nby the user while the multilingual model with LID can only handle one\npre-selected language. In this paper, we propose a novel configurable\nmultilingual model (CMM) which is trained only once but can be configured as\ndifferent models based on users' choices by extracting language-specific\nmodules together with a universal model from the trained CMM. Particularly, a\nsingle CMM can be deployed to any user scenario where the users can pre-select\nany combination of languages. Trained with 75K hours of transcribed anonymized\nMicrosoft multilingual data and evaluated with 10-language test sets, the\nproposed CMM improves from the universal multilingual model by 26.0%, 16.9%,\nand 10.4% relative word error reduction when the user selects 1, 2, or 3\nlanguages, respectively. CMM also performs significantly better on\ncode-switching test sets.", "published": "2021-07-13 06:52:41", "link": "http://arxiv.org/abs/2107.05876v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Conformer-based End-to-end Speech Recognition With Rotary Position\n  Embedding", "abstract": "Transformer-based end-to-end speech recognition models have received\nconsiderable attention in recent years due to their high training speed and\nability to model a long-range global context. Position embedding in the\ntransformer architecture is indispensable because it provides supervision for\ndependency modeling between elements at different positions in the input\nsequence. To make use of the time order of the input sequence, many works\ninject some information about the relative or absolute position of the element\ninto the input sequence. In this work, we investigate various position\nembedding methods in the convolution-augmented transformer (conformer) and\nadopt a novel implementation named rotary position embedding (RoPE). RoPE\nencodes absolute positional information into the input sequence by a rotation\nmatrix, and then naturally incorporates explicit relative position information\ninto a self-attention module. To evaluate the effectiveness of the RoPE method,\nwe conducted experiments on AISHELL-1 and LibriSpeech corpora. Results show\nthat the conformer enhanced with RoPE achieves superior performance in the\nspeech recognition task. Specifically, our model achieves a relative word error\nrate reduction of 8.70% and 7.27% over the conformer on test-clean and\ntest-other sets of the LibriSpeech corpus respectively.", "published": "2021-07-13 08:07:22", "link": "http://arxiv.org/abs/2107.05907v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The IWSLT 2021 BUT Speech Translation Systems", "abstract": "The paper describes BUT's English to German offline speech translation(ST)\nsystems developed for IWSLT2021. They are based on jointly trained Automatic\nSpeech Recognition-Machine Translation models. Their performances is evaluated\non MustC-Common test set. In this work, we study their efficiency from the\nperspective of having a large amount of separate ASR training data and MT\ntraining data, and a smaller amount of speech-translation training data. Large\namounts of ASR and MT training data are utilized for pre-training the ASR and\nMT models. Speech-translation data is used to jointly optimize ASR-MT models by\ndefining an end-to-end differentiable path from speech to translations. For\nthis purpose, we use the internal continuous representations from the\nASR-decoder as the input to MT module. We show that speech translation can be\nfurther improved by training the ASR-decoder jointly with the MT-module using\nlarge amount of text-only MT training data. We also show significant\nimprovements by training an ASR module capable of generating punctuated text,\nrather than leaving the punctuation task to the MT module.", "published": "2021-07-13 15:11:18", "link": "http://arxiv.org/abs/2107.06155v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Fairness-aware Summarization for Justified Decision-Making", "abstract": "In consequential domains such as recidivism prediction, facility inspection,\nand benefit assignment, it's important for individuals to know the\ndecision-relevant information for the model's prediction. In addition,\npredictions should be fair both in terms of the outcome and the justification\nof the outcome. In other words, decision-relevant features should provide\nsufficient information for the predicted outcome and should be independent of\nthe membership of individuals in protected groups such as race and gender. In\nthis work, we focus on the problem of (un)fairness in the justification of the\ntext-based neural models. We tie the explanatory power of the model to fairness\nin the outcome and propose a fairness-aware summarization mechanism to detect\nand counteract the bias in such models. Given a potentially biased natural\nlanguage explanation for a decision, we use a multi-task neural model and an\nattribution mechanism based on integrated gradients to extract high-utility and\nlow-bias justifications in form of a summary. The extracted summary is then\nused for training a model to make decisions for individuals. Results on several\nreal world datasets suggest that our method drastically limits the demographic\nleakage in the input (fairness in justification) while moderately enhancing the\nfairness in the outcome. Our model is also effective in detecting and\ncounteracting several types of data poisoning attacks that synthesize\nrace-coded reasoning or irrelevant justifications.", "published": "2021-07-13 17:04:10", "link": "http://arxiv.org/abs/2107.06243v2", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI"}
{"title": "How Much Can CLIP Benefit Vision-and-Language Tasks?", "abstract": "Most existing Vision-and-Language (V&L) models rely on pre-trained visual\nencoders, using a relatively small set of manually-annotated data (as compared\nto web-crawled data), to perceive the visual world. However, it has been\nobserved that large-scale pretraining usually can result in better\ngeneralization performance, e.g., CLIP (Contrastive Language-Image\nPre-training), trained on a massive amount of image-caption pairs, has shown a\nstrong zero-shot capability on various vision tasks. To further study the\nadvantage brought by CLIP, we propose to use CLIP as the visual encoder in\nvarious V&L models in two typical scenarios: 1) plugging CLIP into\ntask-specific fine-tuning; 2) combining CLIP with V&L pre-training and\ntransferring to downstream tasks. We show that CLIP significantly outperforms\nwidely-used visual encoders trained with in-domain annotated data, such as\nBottomUp-TopDown. We achieve competitive or better results on diverse V&L\ntasks, while establishing new state-of-the-art results on Visual Question\nAnswering, Visual Entailment, and V&L Navigation tasks. We release our code at\nhttps://github.com/clip-vil/CLIP-ViL.", "published": "2021-07-13 20:48:12", "link": "http://arxiv.org/abs/2107.06383v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Using BERT Encoding to Tackle the Mad-lib Attack in SMS Spam Detection", "abstract": "One of the stratagems used to deceive spam filters is to substitute vocables\nwith synonyms or similar words that turn the message unrecognisable by the\ndetection algorithms. In this paper we investigate whether the recent\ndevelopment of language models sensitive to the semantics and context of words,\nsuch as Google's BERT, may be useful to overcome this adversarial attack\n(called \"Mad-lib\" as per the word substitution game). Using a dataset of 5572\nSMS spam messages, we first established a baseline of detection performance\nusing widely known document representation models (BoW and TFIDF) and the novel\nBERT model, coupled with a variety of classification algorithms (Decision Tree,\nkNN, SVM, Logistic Regression, Naive Bayes, Multilayer Perceptron). Then, we\nbuilt a thesaurus of the vocabulary contained in these messages, and set up a\nMad-lib attack experiment in which we modified each message of a held out\nsubset of data (not used in the baseline experiment) with different rates of\nsubstitution of original words with synonyms from the thesaurus. Lastly, we\nevaluated the detection performance of the three representation models (BoW,\nTFIDF and BERT) coupled with the best classifier from the baseline experiment\n(SVM). We found that the classic models achieved a 94% Balanced Accuracy (BA)\nin the original dataset, whereas the BERT model obtained 96%. On the other\nhand, the Mad-lib attack experiment showed that BERT encodings manage to\nmaintain a similar BA performance of 96% with an average substitution rate of\n1.82 words per message, and 95% with 3.34 words substituted per message. In\ncontrast, the BA performance of the BoW and TFIDF encoders dropped to chance.\nThese results hint at the potential advantage of BERT models to combat these\ntype of ingenious attacks, offsetting to some extent for the inappropriate use\nof semantic relationships in language.", "published": "2021-07-13 21:17:57", "link": "http://arxiv.org/abs/2107.06400v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AUC Optimization for Robust Small-footprint Keyword Spotting with\n  Limited Training Data", "abstract": "Deep neural networks provide effective solutions to small-footprint keyword\nspotting (KWS). However, if training data is limited, it remains challenging to\nachieve robust and highly accurate KWS in real-world scenarios where unseen\nsounds that are out of the training data are frequently encountered. Most\nconventional methods aim to maximize the classification accuracy on the\ntraining set, without taking the unseen sounds into account. To enhance the\nrobustness of the deep neural networks based KWS, in this paper, we introduce a\nnew loss function, named the maximization of the area under the\nreceiver-operating-characteristic curve (AUC). The proposed method not only\nmaximizes the classification accuracy of keywords on the closed training set,\nbut also maximizes the AUC score for optimizing the performance of non-keyword\nsegments detection. Experimental results on the Google Speech Commands dataset\nv1 and v2 show that our method achieves new state-of-the-art performance in\nterms of most evaluation metrics.", "published": "2021-07-13 05:44:47", "link": "http://arxiv.org/abs/2107.05859v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Representation Learning Combining Conformer CPC with Deep Cluster\n  for the ZeroSpeech Challenge 2021", "abstract": "We present a system for the Zero Resource Speech Challenge 2021, which\ncombines a Contrastive Predictive Coding (CPC) with deep cluster. In deep\ncluster, we first prepare pseudo-labels obtained by clustering the outputs of a\nCPC network with k-means. Then, we train an additional autoregressive model to\nclassify the previously obtained pseudo-labels in a supervised manner. Phoneme\ndiscriminative representation is achieved by executing the second-round\nclustering with the outputs of the final layer of the autoregressive model. We\nshow that replacing a Transformer layer with a Conformer layer leads to a\nfurther gain in a lexical metric. Experimental results show that a relative\nimprovement of 35% in a phonetic metric, 1.5% in the lexical metric, and 2.3%\nin a syntactic metric are achieved compared to a baseline method of CPC-small\nwhich is trained on LibriSpeech 460h data. We achieve top results in this\nchallenge with the syntactic metric.", "published": "2021-07-13 07:53:01", "link": "http://arxiv.org/abs/2107.05899v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Automatic Instrumentation by Learning to Separate Parts in\n  Symbolic Multitrack Music", "abstract": "Modern keyboards allow a musician to play multiple instruments at the same\ntime by assigning zones -- fixed pitch ranges of the keyboard -- to different\ninstruments. In this paper, we aim to further extend this idea and examine the\nfeasibility of automatic instrumentation -- dynamically assigning instruments\nto notes in solo music during performance. In addition to the online,\nreal-time-capable setting for performative use cases, automatic instrumentation\ncan also find applications in assistive composing tools in an offline setting.\nDue to the lack of paired data of original solo music and their full\narrangements, we approach automatic instrumentation by learning to separate\nparts (e.g., voices, instruments and tracks) from their mixture in symbolic\nmultitrack music, assuming that the mixture is to be played on a keyboard. We\nframe the task of part separation as a sequential multi-class classification\nproblem and adopt machine learning to map sequences of notes into sequences of\npart labels. To examine the effectiveness of our proposed models, we conduct a\ncomprehensive empirical evaluation over four diverse datasets of different\ngenres and ensembles -- Bach chorales, string quartets, game music and pop\nmusic. Our experiments show that the proposed models outperform various\nbaselines. We also demonstrate the potential for our proposed models to produce\nalternative convincing instrumentations for an existing arrangement by\nseparating its mixture into parts. All source code and audio samples can be\nfound at https://salu133445.github.io/arranger/ .", "published": "2021-07-13 08:34:44", "link": "http://arxiv.org/abs/2107.05916v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Piano Inpainting Application", "abstract": "Autoregressive models are now capable of generating high-quality minute-long\nexpressive MIDI piano performances. Even though this progress suggests new\ntools to assist music composition, we observe that generative algorithms are\nstill not widely used by artists due to the limited control they offer,\nprohibitive inference times or the lack of integration within musicians'\nworkflows. In this work, we present the Piano Inpainting Application (PIA), a\ngenerative model focused on inpainting piano performances, as we believe that\nthis elementary operation (restoring missing parts of a piano performance)\nencourages human-machine interaction and opens up new ways to approach music\ncomposition. Our approach relies on an encoder-decoder Linear Transformer\narchitecture trained on a novel representation for MIDI piano performances\ntermed Structured MIDI Encoding. By uncovering an interesting synergy between\nLinear Transformers and our inpainting task, we are able to efficiently inpaint\ncontiguous regions of a piano performance, which makes our model suitable for\ninteractive and responsive A.I.-assisted composition. Finally, we introduce our\nfreely-available Ableton Live PIA plugin, which allows musicians to smoothly\ngenerate or modify any MIDI clip using PIA within a widely-used professional\nDigital Audio Workstation.", "published": "2021-07-13 09:33:11", "link": "http://arxiv.org/abs/2107.05944v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Timbre Classification of Musical Instruments with a Deep Learning\n  Multi-Head Attention-Based Model", "abstract": "The aim of this work is to define a model based on deep learning that is able\nto identify different instrument timbres with as few parameters as possible.\nFor this purpose, we have worked with classical orchestral instruments played\nwith different dynamics, which are part of a few instrument families and which\nplay notes in the same pitch range. It has been possible to assess the ability\nto classify instruments by timbre even if the instruments are playing the same\nnote with the same intensity. The network employed uses a multi-head attention\nmechanism, with 8 heads and a dense network at the output taking as input the\nlog-mel magnitude spectrograms of the sound samples. This network allows the\nidentification of 20 instrument classes of the classical orchestra, achieving\nan overall F$_1$ value of 0.62. An analysis of the weights of the attention\nlayer has been performed and the confusion matrix of the model is presented,\nallowing us to assess the ability of the proposed architecture to distinguish\ntimbre and to establish the aspects on which future work should focus.", "published": "2021-07-13 16:34:19", "link": "http://arxiv.org/abs/2107.06231v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dance2Music: Automatic Dance-driven Music Generation", "abstract": "Dance and music typically go hand in hand. The complexities in dance, music,\nand their synchronisation make them fascinating to study from a computational\ncreativity perspective. While several works have looked at generating dance for\na given music, automatically generating music for a given dance remains\nunder-explored. This capability could have several creative expression and\nentertainment applications. We present some early explorations in this\ndirection. We present a search-based offline approach that generates music\nafter processing the entire dance video and an online approach that uses a deep\nneural network to generate music on-the-fly as the video proceeds. We compare\nthese approaches to a strong heuristic baseline via human studies and present\nour findings. We have integrated our online approach in a live demo! A video of\nthe demo can be found here:\nhttps://sites.google.com/view/dance2music/live-demo.", "published": "2021-07-13 17:22:42", "link": "http://arxiv.org/abs/2107.06252v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
