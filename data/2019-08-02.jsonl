{"title": "Predicting Behavior in Cancer-Afflicted Patient and Spouse Interactions\n  using Speech and Language", "abstract": "Cancer impacts the quality of life of those diagnosed as well as their spouse\ncaregivers, in addition to potentially influencing their day-to-day behaviors.\nThere is evidence that effective communication between spouses can improve\nwell-being related to cancer but it is difficult to efficiently evaluate the\nquality of daily life interactions using manual annotation frameworks.\nAutomated recognition of behaviors based on the interaction cues of speakers\ncan help analyze interactions in such couples and identify behaviors which are\nbeneficial for effective communication. In this paper, we present and detail a\ndataset of dyadic interactions in 85 real-life cancer-afflicted couples and a\nset of observational behavior codes pertaining to interpersonal communication\nattributes. We describe and employ neural network-based systems for classifying\nthese behaviors based on turn-level acoustic and lexical speech patterns.\nFurthermore, we investigate the effect of controlling for factors such as\ngender, patient/caregiver role and conversation content on behavior\nclassification. Analysis of our preliminary results indicates the challenges in\nthis task due to the nature of the targeted behaviors and suggests that\ntechniques incorporating contextual processing might be better suited to tackle\nthis problem.", "published": "2019-08-02 15:30:06", "link": "http://arxiv.org/abs/1908.00908v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Speech Test Set of Practice Business Presentations with Additional\n  Relevant Texts", "abstract": "We present a test corpus of audio recordings and transcriptions of\npresentations of students' enterprises together with their slides and\nweb-pages. The corpus is intended for evaluation of automatic speech\nrecognition (ASR) systems, especially in conditions where the prior\navailability of in-domain vocabulary and named entities is benefitable. The\ncorpus consists of 39 presentations in English, each up to 90 seconds long. The\nspeakers are high school students from European countries with English as their\nsecond language. We benchmark three baseline ASR systems on the corpus and show\ntheir imperfection.", "published": "2019-08-02 15:39:31", "link": "http://arxiv.org/abs/1908.00916v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DELTA: A DEep learning based Language Technology plAtform", "abstract": "In this paper we present DELTA, a deep learning based language technology\nplatform. DELTA is an end-to-end platform designed to solve industry level\nnatural language and speech processing problems. It integrates most popular\nneural network models for training as well as comprehensive deployment tools\nfor production. DELTA aims to provide easy and fast experiences for using,\ndeploying, and developing natural language processing and speech models for\nboth academia and industry use cases. We demonstrate the reliable performance\nwith DELTA on several natural language processing and speech tasks, including\ntext classification, named entity recognition, natural language inference,\nspeech recognition, speaker verification, etc. DELTA has been used for\ndeveloping several state-of-the-art algorithms for publications and delivering\nreal production to serve millions of users.", "published": "2019-08-02 01:13:50", "link": "http://arxiv.org/abs/1908.01853v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep learning languages: a key fundamental shift from probabilities to\n  weights?", "abstract": "Recent successes in language modeling, notably with deep learning methods,\ncoincide with a shift from probabilistic to weighted representations. We raise\nhere the question of the importance of this evolution, in the light of the\npractical limitations of a classical and simple probabilistic modeling approach\nfor the classification of protein sequences and in relation to the need for\nprincipled methods to learn non-probabilistic models.", "published": "2019-08-02 10:09:51", "link": "http://arxiv.org/abs/1908.00785v1", "categories": ["q-bio.OT", "cs.CL", "cs.LG"], "primary_category": "q-bio.OT"}
{"title": "Multilingual Speech Recognition with Corpus Relatedness Sampling", "abstract": "Multilingual acoustic models have been successfully applied to low-resource\nspeech recognition. Most existing works have combined many small corpora\ntogether and pretrained a multilingual model by sampling from each corpus\nuniformly. The model is eventually fine-tuned on each target corpus. This\napproach, however, fails to exploit the relatedness and similarity among\ncorpora in the training set. For example, the target corpus might benefit more\nfrom a corpus in the same domain or a corpus from a close language. In this\nwork, we propose a simple but useful sampling strategy to take advantage of\nthis relatedness. We first compute the corpus-level embeddings and estimate the\nsimilarity between each corpus. Next, we start training the multilingual model\nwith uniform-sampling from each corpus at first, then we gradually increase the\nprobability to sample from related corpora based on its similarity with the\ntarget corpus. Finally, the model would be fine-tuned automatically on the\ntarget corpus. Our sampling strategy outperforms the baseline multilingual\nmodel on 16 low-resource tasks. Additionally, we demonstrate that our corpus\nembeddings capture the language and domain information of each corpus.", "published": "2019-08-02 21:08:13", "link": "http://arxiv.org/abs/1908.01060v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SANTLR: Speech Annotation Toolkit for Low Resource Languages", "abstract": "While low resource speech recognition has attracted a lot of attention from\nthe speech community, there are a few tools available to facilitate low\nresource speech collection. In this work, we present SANTLR: Speech Annotation\nToolkit for Low Resource Languages. It is a web-based toolkit which allows\nresearchers to easily collect and annotate a corpus of speech in a low resource\nlanguage. Annotators may use this toolkit for two purposes: transcription or\nrecording. In transcription, annotators would transcribe audio files provided\nby the researchers; in recording, annotators would record their voice by\nreading provided texts. We highlight two properties of this toolkit. First,\nSANTLR has a very user-friendly User Interface (UI). Both researchers and\nannotators may use this simple web interface to interact. There is no\nrequirement for the annotators to have any expertise in audio or text\nprocessing. The toolkit would handle all preprocessing and postprocessing\nsteps. Second, we employ a multi-step ranking mechanism facilitate the\nannotation process. In particular, the toolkit would give higher priority to\nutterances which are easier to annotate and are more beneficial to achieving\nthe goal of the annotation, e.g. quickly training an acoustic model.", "published": "2019-08-02 21:13:27", "link": "http://arxiv.org/abs/1908.01067v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Dialogue Act Classification in Group Chats with DAG-LSTMs", "abstract": "Dialogue act (DA) classification has been studied for the past two decades\nand has several key applications such as workflow automation and conversation\nanalytics. Researchers have used, to address this problem, various traditional\nmachine learning models, and more recently deep neural network models such as\nhierarchical convolutional neural networks (CNNs) and long short-term memory\n(LSTM) networks. In this paper, we introduce a new model architecture,\ndirected-acyclic-graph LSTM (DAG-LSTM) for DA classification. A DAG-LSTM\nexploits the turn-taking structure naturally present in a multi-party\nconversation, and encodes this relation in its model structure. Using the STAC\ncorpus, we show that the proposed method performs roughly 0.8% better in\naccuracy and 1.2% better in macro-F1 score when compared to existing methods.\nThe proposed method is generic and not limited to conversation applications.", "published": "2019-08-02 17:12:38", "link": "http://arxiv.org/abs/1908.01821v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Self-Knowledge Distillation in Natural Language Processing", "abstract": "Since deep learning became a key player in natural language processing (NLP),\nmany deep learning models have been showing remarkable performances in a\nvariety of NLP tasks, and in some cases, they are even outperforming humans.\nSuch high performance can be explained by efficient knowledge representation of\ndeep learning models. While many methods have been proposed to learn more\nefficient representation, knowledge distillation from pretrained deep networks\nsuggest that we can use more information from the soft target probability to\ntrain other neural networks. In this paper, we propose a new knowledge\ndistillation method self-knowledge distillation, based on the soft target\nprobabilities of the training model itself, where multimode information is\ndistilled from the word embedding space right below the softmax layer. Due to\nthe time complexity, our method approximates the soft target probabilities. In\nexperiments, we applied the proposed method to two different and fundamental\nNLP tasks: language model and neural machine translation. The experiment\nresults show that our proposed method improves performance on the tasks.", "published": "2019-08-02 15:17:27", "link": "http://arxiv.org/abs/1908.01851v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Sound source detection, localization and classification using\n  consecutive ensemble of CRNN models", "abstract": "In this paper, we describe our method for DCASE2019 task3: Sound Event\nLocalization and Detection (SELD). We use four CRNN SELDnet-like single output\nmodels which run in a consecutive manner to recover all possible information of\noccurring events. We decompose the SELD task into estimating number of active\nsources, estimating direction of arrival of a single source, estimating\ndirection of arrival of the second source where the direction of the first one\nis known and a multi-label classification task. We use custom consecutive\nensemble to predict events' onset, offset, direction of arrival and class. The\nproposed approach is evaluated on the TAU Spatial Sound Events 2019 - Ambisonic\nand it is compared with other participants' submissions.", "published": "2019-08-02 09:10:08", "link": "http://arxiv.org/abs/1908.00766v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "High-Level Control of Drum Track Generation Using Learned Patterns of\n  Rhythmic Interaction", "abstract": "Spurred by the potential of deep learning, computational music generation has\ngained renewed academic interest. A crucial issue in music generation is that\nof user control, especially in scenarios where the music generation process is\nconditioned on existing musical material. Here we propose a model for\nconditional kick drum track generation that takes existing musical material as\ninput, in addition to a low-dimensional code that encodes the desired relation\nbetween the existing material and the new material to be generated. These\nrelational codes are learned in an unsupervised manner from a music dataset. We\nshow that codes can be sampled to create a variety of musically plausible kick\ndrum tracks and that the model can be used to transfer kick drum patterns from\none song to another. Lastly, we demonstrate that the learned codes are largely\ninvariant to tempo and time-shift.", "published": "2019-08-02 16:39:02", "link": "http://arxiv.org/abs/1908.00948v1", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LSTM Based Music Generation System", "abstract": "Traditionally, music was treated as an analogue signal and was generated\nmanually. In recent years, music is conspicuous to technology which can\ngenerate a suite of music automatically without any human intervention. To\naccomplish this task, we need to overcome some technical challenges which are\ndiscussed descriptively in this paper. A brief introduction about music and its\ncomponents is provided in the paper along with the citation and analysis of\nrelated work accomplished by different authors in this domain. Main objective\nof this paper is to propose an algorithm which can be used to generate musical\nnotes using Recurrent Neural Networks (RNN), principally Long Short-Term Memory\n(LSTM) networks. A model is designed to execute this algorithm where data is\nrepresented with the help of musical instrument digital interface (MIDI) file\nformat for easier access and better understanding. Preprocessing of data before\nfeeding it into the model, revealing methods to read, process and prepare MIDI\nfiles for input are also discussed. The model used in this paper is used to\nlearn the sequences of polyphonic musical notes over a single-layered LSTM\nnetwork. The model must have the potential to recall past details of a musical\nsequence and its structure for better learning. Description of layered\narchitecture used in LSTM model and its intertwining connections to develop a\nneural network is presented in this work. This paper imparts a peek view of\ndistributions of weights and biases in every layer of the model along with a\nprecise representation of losses and accuracy at each step and batches. When\nthe model was thoroughly analyzed, it produced stellar results in composing new\nmelodies.", "published": "2019-08-02 22:10:19", "link": "http://arxiv.org/abs/1908.01080v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
