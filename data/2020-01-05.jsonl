{"title": "Computationally Efficient NER Taggers with Combined Embeddings and\n  Constrained Decoding", "abstract": "Current State-of-the-Art models in Named Entity Recognition (NER) are neural\nmodels with a Conditional Random Field (CRF) as the final network layer, and\npre-trained \"contextual embeddings\". The CRF layer is used to facilitate global\ncoherence between labels, and the contextual embeddings provide a better\nrepresentation of words in context. However, both of these improvements come at\na high computational cost. In this work, we explore two simple techniques that\nsubstantially improve NER performance over a strong baseline with negligible\ncost. First, we use multiple pre-trained embeddings as word representations via\nconcatenation. Second, we constrain the tagger, trained using a cross-entropy\nloss, during decoding to eliminate illegal transitions. While training a tagger\non CoNLL 2003 we find a $786$\\% speed-up over a contextual embeddings-based\ntagger without sacrificing strong performance. We also show that the\nconcatenation technique works across multiple tasks and datasets. We analyze\naspects of similarity and coverage between pre-trained embeddings and the\ndynamics of tag co-occurrence to explain why these techniques work. We provide\nan open source implementation of our tagger using these techniques in three\npopular deep learning frameworks --- TensorFlow, Pytorch, and DyNet.", "published": "2020-01-05 04:50:38", "link": "http://arxiv.org/abs/2001.01167v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Word and Document Embeddings for Sentiment Analysis", "abstract": "Sentiments of words differ from one corpus to another. Inducing general\nsentiment lexicons for languages and using them cannot, in general, produce\nmeaningful results for different domains. In this paper, we combine contextual\nand supervised information with the general semantic representations of words\noccurring in the dictionary. Contexts of words help us capture the\ndomain-specific information and supervised scores of words are indicative of\nthe polarities of those words. When we combine supervised features of words\nwith the features extracted from their dictionary definitions, we observe an\nincrease in the success rates. We try out the combinations of contextual,\nsupervised, and dictionary-based approaches, and generate original vectors. We\nalso combine the word2vec approach with hand-crafted features. We induce\ndomain-specific sentimental vectors for two corpora, which are the movie domain\nand the Twitter datasets in Turkish. When we thereafter generate document\nvectors and employ the support vector machines method utilising those vectors,\nour approaches perform better than the baseline studies for Turkish with a\nsignificant margin. We evaluated our models on two English corpora as well and\nthese also outperformed the word2vec approach. It shows that our approaches are\ncross-domain and portable to other languages.", "published": "2020-01-05 16:34:32", "link": "http://arxiv.org/abs/2001.01269v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Business Process Structure Discovery using Ordered Neurons\n  LSTM: A Preliminary Study", "abstract": "Automatic process discovery from textual process documentations is highly\ndesirable to reduce time and cost of Business Process Management (BPM)\nimplementation in organizations. However, existing automatic process discovery\napproaches mainly focus on identifying activities out of the documentations.\nDeriving the structural relationships between activities, which is important in\nthe whole process discovery scope, is still a challenge. In fact, a business\nprocess has latent semantic hierarchical structure which defines different\nlevels of detail to reflect the complex business logic. Recent findings in\nneural machine learning area show that the meaningful linguistic structure can\nbe induced by joint language modeling and structure learning. Inspired by these\nfindings, we propose to retrieve the latent hierarchical structure present in\nthe textual business process documents by building a neural network that\nleverages a novel recurrent architecture, Ordered Neurons LSTM (ON-LSTM), with\nprocess-level language model objective. We tested the proposed approach on data\nset of Process Description Documents (PDD) from our practical Robotic Process\nAutomation (RPA) projects. Preliminary experiments showed promising results.", "published": "2020-01-05 14:19:11", "link": "http://arxiv.org/abs/2001.01243v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Identifying Hashtags in Disaster Twitter Data", "abstract": "Tweet hashtags have the potential to improve the search for information\nduring disaster events. However, there is a large number of disaster-related\ntweets that do not have any user-provided hashtags. Moreover, only a small\nnumber of tweets that contain actionable hashtags are useful for disaster\nresponse. To facilitate progress on automatic identification (or extraction) of\ndisaster hashtags for Twitter data, we construct a unique dataset of\ndisaster-related tweets annotated with hashtags useful for filtering actionable\ninformation. Using this dataset, we further investigate Long Short Term\nMemory-based models within a Multi-Task Learning framework. The best performing\nmodel achieves an F1-score as high as 92.22%. The dataset, code, and other\nresources are available on Github.", "published": "2020-01-05 22:37:17", "link": "http://arxiv.org/abs/2001.01323v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
