{"title": "SentiWords: Deriving a High Precision and High Coverage Lexicon for\n  Sentiment Analysis", "abstract": "Deriving prior polarity lexica for sentiment analysis - where positive or\nnegative scores are associated with words out of context - is a challenging\ntask. Usually, a trade-off between precision and coverage is hard to find, and\nit depends on the methodology used to build the lexicon. Manually annotated\nlexica provide a high precision but lack in coverage, whereas automatic\nderivation from pre-existing knowledge guarantees high coverage at the cost of\na lower precision. Since the automatic derivation of prior polarities is less\ntime consuming than manual annotation, there has been a great bloom of these\napproaches, in particular based on the SentiWordNet resource. In this paper, we\ncompare the most frequently used techniques based on SentiWordNet with newer\nones and blend them in a learning framework (a so called 'ensemble method'). By\ntaking advantage of manually built prior polarity lexica, our ensemble method\nis better able to predict the prior value of unseen words and to outperform all\nthe other SentiWordNet approaches. Using this technique we have built\nSentiWords, a prior polarity lexicon of approximately 155,000 words, that has\nboth a high precision and a high coverage. We finally show that in sentiment\nanalysis tasks, using our lexicon allows us to outperform both the single\nmetrics derived from SentiWordNet and popular manually annotated sentiment\nlexica.", "published": "2015-10-30 13:19:47", "link": "http://arxiv.org/abs/1510.09079v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prediction-Adaptation-Correction Recurrent Neural Networks for\n  Low-Resource Language Speech Recognition", "abstract": "In this paper, we investigate the use of prediction-adaptation-correction\nrecurrent neural networks (PAC-RNNs) for low-resource speech recognition. A\nPAC-RNN is comprised of a pair of neural networks in which a {\\it correction}\nnetwork uses auxiliary information given by a {\\it prediction} network to help\nestimate the state probability. The information from the correction network is\nalso used by the prediction network in a recurrent loop. Our model outperforms\nother state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks.\nMoreover, transfer learning from a language that is similar to the target\nlanguage can help improve performance further.", "published": "2015-10-30 06:42:03", "link": "http://arxiv.org/abs/1510.08985v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Generating Text with Deep Reinforcement Learning", "abstract": "We introduce a novel schema for sequence to sequence learning with a Deep\nQ-Network (DQN), which decodes the output sequence iteratively. The aim here is\nto enable the decoder to first tackle easier portions of the sequences, and\nthen turn to cope with difficult parts. Specifically, in each iteration, an\nencoder-decoder Long Short-Term Memory (LSTM) network is employed to, from the\ninput sequence, automatically create features to represent the internal states\nof and formulate a list of potential actions for the DQN. Take rephrasing a\nnatural sentence as an example. This list can contain ranked potential words.\nNext, the DQN learns to make decision on which action (e.g., word) will be\nselected from the list to modify the current decoded sequence. The newly\nmodified output sequence is subsequently used as the input to the DQN for the\nnext decoding iteration. In each iteration, we also bias the reinforcement\nlearning's attention to explore sequence portions which are previously\ndifficult to be decoded. For evaluation, the proposed strategy was trained to\ndecode ten thousands natural sentences. Our experiments indicate that, when\ncompared to a left-to-right greedy beam search LSTM decoder, the proposed\nmethod performed competitively well when decoding sentences from the training\nset, but significantly outperformed the baseline when decoding unseen\nsentences, in terms of BLEU score obtained.", "published": "2015-10-30 19:02:53", "link": "http://arxiv.org/abs/1510.09202v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Quantifying the Cognitive Extent of Science", "abstract": "While the modern science is characterized by an exponential growth in\nscientific literature, the increase in publication volume clearly does not\nreflect the expansion of the cognitive boundaries of science. Nevertheless,\nmost of the metrics for assessing the vitality of science or for making funding\nand policy decisions are based on productivity. Similarly, the increasing level\nof knowledge production by large science teams, whose results often enjoy\ngreater visibility, does not necessarily mean that \"big science\" leads to\ncognitive expansion. Here we present a novel, big-data method to quantify the\nextents of cognitive domains of different bodies of scientific literature\nindependently from publication volume, and apply it to 20 million articles\npublished over 60-130 years in physics, astronomy, and biomedicine. The method\nis based on the lexical diversity of titles of fixed quotas of research\narticles. Owing to large size of quotas, the method overcomes the inherent\nstochasticity of article titles to achieve <1% precision. We show that the\nperiods of cognitive growth do not necessarily coincide with the trends in\npublication volume. Furthermore, we show that the articles produced by larger\nteams cover significantly smaller cognitive territory than (the same quota of)\narticles from smaller teams. Our findings provide a new perspective on the role\nof small teams and individual researchers in expanding the cognitive boundaries\nof science. The proposed method of quantifying the extent of the cognitive\nterritory can also be applied to study many other aspects of \"science of\nscience.\"", "published": "2015-10-30 22:21:45", "link": "http://arxiv.org/abs/1511.00040v2", "categories": ["cs.DL", "astro-ph.IM", "cs.CL", "physics.soc-ph"], "primary_category": "cs.DL"}
{"title": "Highway Long Short-Term Memory RNNs for Distant Speech Recognition", "abstract": "In this paper, we extend the deep long short-term memory (DLSTM) recurrent\nneural networks by introducing gated direct connections between memory cells in\nadjacent layers. These direct links, called highway connections, enable\nunimpeded information flow across different layers and thus alleviate the\ngradient vanishing problem when building deeper LSTMs. We further introduce the\nlatency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole\nhistory while keeping the latency under control. Efficient algorithms are\nproposed to train these novel networks using both frame and sequence\ndiscriminative criteria. Experiments on the AMI distant speech recognition\n(DSR) task indicate that we can train deeper LSTMs and achieve better\nimprovement from sequence training with highway LSTMs (HLSTMs). Our novel model\nobtains $43.9/47.7\\%$ WER on AMI (SDM) dev and eval sets, outperforming all\nprevious works. It beats the strong DNN and DLSTM baselines with $15.7\\%$ and\n$5.3\\%$ relative improvement respectively.", "published": "2015-10-30 06:40:14", "link": "http://arxiv.org/abs/1510.08983v2", "categories": ["cs.NE", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.NE"}
