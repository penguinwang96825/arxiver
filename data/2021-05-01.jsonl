{"title": "It's not what you said, it's how you said it: discriminative perception\n  of speech as a multichannel communication system", "abstract": "People convey information extremely effectively through spoken interaction\nusing multiple channels of information transmission: the lexical channel of\nwhat is said, and the non-lexical channel of how it is said. We propose\nstudying human perception of spoken communication as a means to better\nunderstand how information is encoded across these channels, focusing on the\nquestion 'What characteristics of communicative context affect listener's\nexpectations of speech?'. To investigate this, we present a novel behavioural\ntask testing whether listeners can discriminate between the true utterance in a\ndialogue and utterances sampled from other contexts with the same lexical\ncontent. We characterize how perception - and subsequent discriminative\ncapability - is affected by different degrees of additional contextual\ninformation across both the lexical and non-lexical channel of speech. Results\ndemonstrate that people can effectively discriminate between different prosodic\nrealisations, that non-lexical context is informative, and that this channel\nprovides more salient information than the lexical channel, highlighting the\nimportance of the non-lexical channel in spoken interaction.", "published": "2021-05-01 14:30:30", "link": "http://arxiv.org/abs/2105.00260v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When to Fold'em: How to answer Unanswerable questions", "abstract": "We present 3 different question-answering models trained on the SQuAD2.0\ndataset -- BIDAF, DocumentQA and ALBERT Retro-Reader -- demonstrating the\nimprovement of language models in the past three years. Through our research in\nfine-tuning pre-trained models for question-answering, we developed a novel\napproach capable of achieving a 2% point improvement in SQuAD2.0 F1 in reduced\ntraining time. Our method of re-initializing select layers of a\nparameter-shared language model is simple yet empirically powerful.", "published": "2021-05-01 19:08:40", "link": "http://arxiv.org/abs/2105.00328v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hidden Backdoors in Human-Centric Language Models", "abstract": "Natural language processing (NLP) systems have been proven to be vulnerable\nto backdoor attacks, whereby hidden features (backdoors) are trained into a\nlanguage model and may only be activated by specific inputs (called triggers),\nto trick the model into producing unexpected behaviors. In this paper, we\ncreate covert and natural triggers for textual backdoor attacks, \\textit{hidden\nbackdoors}, where triggers can fool both modern language models and human\ninspection. We deploy our hidden backdoors through two state-of-the-art trigger\nembedding methods. The first approach via homograph replacement, embeds the\ntrigger into deep neural networks through the visual spoofing of lookalike\ncharacter replacement. The second approach uses subtle differences between text\ngenerated by language models and real natural text to produce trigger sentences\nwith correct grammar and high fluency. We demonstrate that the proposed hidden\nbackdoors can be effective across three downstream security-critical NLP tasks,\nrepresentative of modern human-centric NLP systems, including toxic comment\ndetection, neural machine translation (NMT), and question answering (QA). Our\ntwo hidden backdoor attacks can achieve an Attack Success Rate (ASR) of at\nleast $97\\%$ with an injection rate of only $3\\%$ in toxic comment detection,\n$95.1\\%$ ASR in NMT with less than $0.5\\%$ injected data, and finally $91.12\\%$\nASR against QA updated with only 27 poisoning data samples on a model\npreviously trained with 92,024 samples (0.029\\%). We are able to demonstrate\nthe adversary's high success rate of attacks, while maintaining functionality\nfor regular users, with triggers inconspicuous by the human administrators.", "published": "2021-05-01 04:41:00", "link": "http://arxiv.org/abs/2105.00164v3", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "MRCBert: A Machine Reading ComprehensionApproach for Unsupervised\n  Summarization", "abstract": "When making an online purchase, it becomes important for the customer to read\nthe product reviews carefully and make a decision based on that. However,\nreviews can be lengthy, may contain repeated, or sometimes irrelevant\ninformation that does not help in decision making. In this paper, we introduce\nMRCBert, a novel unsupervised method to generate summaries from product\nreviews. We leverage Machine Reading Comprehension, i.e. MRC, approach to\nextract relevant opinions and generate both rating-wise and aspect-wise\nsummaries from reviews. Through MRCBert we show that we can obtain reasonable\nperformance using existing models and transfer learning, which can be useful\nfor learning under limited or low resource scenarios. We demonstrated our\nresults on reviews of a product from the Electronics category in the Amazon\nReviews dataset. Our approach is unsupervised as it does not require any\ndomain-specific dataset, such as the product review dataset, for training or\nfine-tuning. Instead, we have used SQuAD v1.1 dataset only to fine-tune BERT\nfor the MRC task. Since MRCBert does not require a task-specific dataset, it\ncan be easily adapted and used in other domains.", "published": "2021-05-01 12:57:08", "link": "http://arxiv.org/abs/2105.00239v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PREDICT: Persian Reverse Dictionary", "abstract": "Finding the appropriate words to convey concepts (i.e., lexical access) is\nessential for effective communication. Reverse dictionaries fulfill this need\nby helping individuals to find the word(s) which could relate to a specific\nconcept or idea. To the best of our knowledge, this resource has not been\navailable for the Persian language. In this paper, we compare four different\narchitectures for implementing a Persian reverse dictionary (PREDICT).\n  We evaluate our models using (phrase,word) tuples extracted from the only\nPersian dictionaries available online, namely Amid, Moein, and Dehkhoda where\nthe phrase describes the word. Given the phrase, a model suggests the most\nrelevant word(s) in terms of the ability to convey the concept. The model is\nconsidered to perform well if the correct word is one of its top suggestions.\n  Our experiments show that a model consisting of Long Short-Term Memory (LSTM)\nunits enhanced by an additive attention mechanism is enough to produce\nsuggestions comparable to (or in some cases better than) the word in the\noriginal dictionary. The study also reveals that the model sometimes produces\nthe synonyms of the word as its output which led us to introduce a new metric\nfor the evaluation of reverse dictionaries called Synonym Accuracy accounting\nfor the percentage of times the event of producing the word or a synonym of it\noccurs. The assessment of the best model using this new metric also indicates\nthat at least 62% of the times, it produces an accurate result within the top\n100 suggestions.", "published": "2021-05-01 17:37:01", "link": "http://arxiv.org/abs/2105.00309v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Capturing Logical Structure of Visually Structured Documents with\n  Multimodal Transition Parser", "abstract": "While many NLP pipelines assume raw, clean texts, many texts we encounter in\nthe wild, including a vast majority of legal documents, are not so clean, with\nmany of them being visually structured documents (VSDs) such as PDFs.\nConventional preprocessing tools for VSDs mainly focused on word segmentation\nand coarse layout analysis, whereas fine-grained logical structure analysis\n(such as identifying paragraph boundaries and their hierarchies) of VSDs is\nunderexplored. To that end, we proposed to formulate the task as prediction of\n\"transition labels\" between text fragments that maps the fragments to a tree,\nand developed a feature-based machine learning system that fuses visual,\ntextual and semantic cues.Our system is easily customizable to different types\nof VSDs and it significantly outperformed baselines in identifying different\nstructures in VSDs. For example, our system obtained a paragraph boundary\ndetection F1 score of 0.953 which is significantly better than a popular\nPDF-to-text tool with an F1 score of 0.739.", "published": "2021-05-01 02:33:50", "link": "http://arxiv.org/abs/2105.00150v2", "categories": ["cs.CL", "cs.CV", "cs.IR"], "primary_category": "cs.CL"}
{"title": "AlloST: Low-resource Speech Translation without Source Transcription", "abstract": "The end-to-end architecture has made promising progress in speech translation\n(ST). However, the ST task is still challenging under low-resource conditions.\nMost ST models have shown unsatisfactory results, especially in the absence of\nword information from the source speech utterance. In this study, we survey\nmethods to improve ST performance without using source transcription, and\npropose a learning framework that utilizes a language-independent universal\nphone recognizer. The framework is based on an attention-based\nsequence-to-sequence model, where the encoder generates the phonetic embeddings\nand phone-aware acoustic representations, and the decoder controls the fusion\nof the two embedding streams to produce the target token sequence. In addition\nto investigating different fusion strategies, we explore the specific usage of\nbyte pair encoding (BPE), which compresses a phone sequence into a\nsyllable-like segmented sequence. Due to the conversion of symbols, a segmented\nsequence represents not only pronunciation but also language-dependent\ninformation lacking in phones. Experiments conducted on the Fisher\nSpanish-English and Taigi-Mandarin drama corpora show that our method\noutperforms the conformer-based baseline, and the performance is close to that\nof the existing best method using source transcription.", "published": "2021-05-01 05:30:18", "link": "http://arxiv.org/abs/2105.00171v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.CL"}
{"title": "One-shot learning for acoustic identification of bird species in\n  non-stationary environments", "abstract": "This work introduces the one-shot learning paradigm in the computational\nbioacoustics domain. Even though, most of the related literature assumes\navailability of data characterizing the entire class dictionary of the problem\nat hand, that is rarely true as a habitat's species composition is only known\nup to a certain extent. Thus, the problem needs to be addressed by\nmethodologies able to cope with non-stationarity. To this end, we propose a\nframework able to detect changes in the class dictionary and incorporate new\nclasses on the fly. We design an one-shot learning architecture composed of a\nSiamese Neural Network operating in the logMel spectrogram space. We\nextensively examine the proposed approach on two datasets of various bird\nspecies using suitable figures of merit. Interestingly, such a learning scheme\nexhibits state of the art performance, while taking into account extreme\nnon-stationarity cases.", "published": "2021-05-01 09:43:20", "link": "http://arxiv.org/abs/2105.00202v1", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Audio Transformers:Transformer Architectures For Large Scale Audio\n  Understanding. Adieu Convolutions", "abstract": "Over the past two decades, CNN architectures have produced compelling models\nof sound perception and cognition, learning hierarchical organizations of\nfeatures. Analogous to successes in computer vision, audio feature\nclassification can be optimized for a particular task of interest, over a wide\nvariety of datasets and labels. In fact similar architectures designed for\nimage understanding have proven effective for acoustic scene analysis. Here we\npropose applying Transformer based architectures without convolutional layers\nto raw audio signals. On a standard dataset of Free Sound 50K,comprising of 200\ncategories, our model outperforms convolutional models to produce state of the\nart results. This is significant as unlike in natural language processing and\ncomputer vision, we do not perform unsupervised pre-training for outperforming\nconvolutional architectures. On the same training set, with respect mean\naver-age precision benchmarks, we show a significant improvement. We further\nimprove the performance of Transformer architectures by using techniques such\nas pooling inspired from convolutional net-work designed in the past few years.\nIn addition, we also show how multi-rate signal processing ideas inspired from\nwavelets, can be applied to the Transformer embeddings to improve the results.\nWe also show how our models learns a non-linear non constant band-width\nfilter-bank, which shows an adaptable time frequency front end representation\nfor the task of audio understanding, different from other tasks e.g. pitch\nestimation.", "published": "2021-05-01 19:38:30", "link": "http://arxiv.org/abs/2105.00335v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Emotion Recognition of the Singing Voice: Toward a Real-Time Analysis\n  Tool for Singers", "abstract": "Current computational-emotion research has focused on applying acoustic\nproperties to analyze how emotions are perceived mathematically or used in\nnatural language processing machine learning models. While recent interest has\nfocused on analyzing emotions from the spoken voice, little experimentation has\nbeen performed to discover how emotions are recognized in the singing voice --\nboth in noiseless and noisy data (i.e., data that is either inaccurate,\ndifficult to interpret, has corrupted/distorted/nonsense information like\nactual noise sounds in this case, or has a low ratio of usable/unusable\ninformation). Not only does this ignore the challenges of training machine\nlearning models on more subjective data and testing them with much noisier\ndata, but there is also a clear disconnect in progress between advancing the\ndevelopment of convolutional neural networks and the goal of emotionally\ncognizant artificial intelligence. By training a new model to include this type\nof information with a rich comprehension of psycho-acoustic properties, not\nonly can models be trained to recognize information within extremely noisy\ndata, but advancement can be made toward more complex biofeedback applications\n-- including creating a model which could recognize emotions given any human\ninformation (language, breath, voice, body, posture) and be used in any\nperformance medium (music, speech, acting) or psychological assistance for\npatients with disorders such as BPD, alexithymia, autism, among others. This\npaper seeks to reflect and expand upon the findings of related research and\npresent a stepping-stone toward this end goal.", "published": "2021-05-01 05:47:15", "link": "http://arxiv.org/abs/2105.00173v2", "categories": ["cs.SD", "cs.AI", "cs.CY", "cs.LG", "cs.NE", "eess.AS", "I.2; I.5; J.3; J.5; J.7; K.3"], "primary_category": "cs.SD"}
