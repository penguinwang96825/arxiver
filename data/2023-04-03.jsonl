{"title": "MiniRBT: A Two-stage Distilled Small Chinese Pre-trained Model", "abstract": "In natural language processing, pre-trained language models have become\nessential infrastructures. However, these models often suffer from issues such\nas large size, long inference time, and challenging deployment. Moreover, most\nmainstream pre-trained models focus on English, and there are insufficient\nstudies on small Chinese pre-trained models. In this paper, we introduce\nMiniRBT, a small Chinese pre-trained model that aims to advance research in\nChinese natural language processing. MiniRBT employs a narrow and deep student\nmodel and incorporates whole word masking and two-stage distillation during\npre-training to make it well-suited for most downstream tasks. Our experiments\non machine reading comprehension and text classification tasks reveal that\nMiniRBT achieves 94% performance relative to RoBERTa, while providing a 6.8x\nspeedup, demonstrating its effectiveness and efficiency.", "published": "2023-04-03 04:45:57", "link": "http://arxiv.org/abs/2304.00717v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Use of Large Language Models for Reference-Free Text\n  Quality Evaluation: An Empirical Study", "abstract": "Evaluating the quality of generated text is a challenging task in NLP, due to\nthe inherent complexity and diversity of text. Recently, large language models\n(LLMs) have garnered significant attention due to their impressive performance\nin various tasks. Therefore, we present this paper to investigate the\neffectiveness of LLMs, especially ChatGPT, and explore ways to optimize their\nuse in assessing text quality. We compared three kinds of reference-free\nevaluation methods. The experimental results prove that ChatGPT is capable of\nevaluating text quality effectively from various perspectives without reference\nand demonstrates superior performance than most existing automatic metrics. In\nparticular, the Explicit Score, which utilizes ChatGPT to generate a numeric\nscore measuring text quality, is the most effective and reliable method among\nthe three exploited approaches. However, directly comparing the quality of two\ntexts may lead to suboptimal results. We believe this paper will provide\nvaluable insights for evaluating text quality with LLMs and have released the\nused data.", "published": "2023-04-03 05:29:58", "link": "http://arxiv.org/abs/2304.00723v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inspecting and Editing Knowledge Representations in Language Models", "abstract": "Neural language models (LMs) represent facts about the world described by\ntext. Sometimes these facts derive from training data (in most LMs, a\nrepresentation of the word \"banana\" encodes the fact that bananas are fruits).\nSometimes facts derive from input text itself (a representation of the sentence\n\"I poured out the bottle\" encodes the fact that the bottle became empty). We\ndescribe REMEDI, a method for learning to map statements in natural language to\nfact encodings in an LM's internal representation system. REMEDI encodings can\nbe used as knowledge editors: when added to LM hidden representations, they\nmodify downstream generation to be consistent with new facts. REMEDI encodings\nmay also be used as probes: when compared to LM representations, they reveal\nwhich properties LMs already attribute to mentioned entities, in some cases\nmaking it possible to predict when LMs will generate outputs that conflict with\nbackground knowledge or input text. REMEDI thus links work on probing,\nprompting, and LM editing, and offers steps toward general tools for\nfine-grained inspection and control of knowledge in LMs.", "published": "2023-04-03 06:24:10", "link": "http://arxiv.org/abs/2304.00740v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Design Choices for Crowdsourcing Implicit Discourse Relations: Revealing\n  the Biases Introduced by Task Design", "abstract": "Disagreement in natural language annotation has mostly been studied from a\nperspective of biases introduced by the annotators and the annotation\nframeworks. Here, we propose to analyze another source of bias: task design\nbias, which has a particularly strong impact on crowdsourced linguistic\nannotations where natural language is used to elicit the interpretation of\nlaymen annotators. For this purpose we look at implicit discourse relation\nannotation, a task that has repeatedly been shown to be difficult due to the\nrelations' ambiguity. We compare the annotations of 1,200 discourse relations\nobtained using two distinct annotation tasks and quantify the biases of both\nmethods across four different domains. Both methods are natural language\nannotation tasks designed for crowdsourcing. We show that the task design can\npush annotators towards certain relations and that some discourse relations\nsenses can be better elicited with one or the other annotation approach. We\nalso conclude that this type of bias should be taken into account when training\nand testing models.", "published": "2023-04-03 09:04:18", "link": "http://arxiv.org/abs/2304.00815v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Integration of Discriminability and Robustness for\n  Document-Level Relation Extraction", "abstract": "Document-level relation extraction (DocRE) predicts relations for entity\npairs that rely on long-range context-dependent reasoning in a document. As a\ntypical multi-label classification problem, DocRE faces the challenge of\neffectively distinguishing a small set of positive relations from the majority\nof negative ones. This challenge becomes even more difficult to overcome when\nthere exists a significant number of annotation errors in the dataset. In this\nwork, we aim to achieve better integration of both the discriminability and\nrobustness for the DocRE problem. Specifically, we first design an effective\nloss function to endow high discriminability to both probabilistic outputs and\ninternal representations. We innovatively customize entropy minimization and\nsupervised contrastive learning for the challenging multi-label and long-tailed\nlearning problems. To ameliorate the impact of label errors, we equipped our\nmethod with a novel negative label sampling strategy to strengthen the model\nrobustness. In addition, we introduce two new data regimes to mimic more\nrealistic scenarios with annotation errors and evaluate our sampling strategy.\nExperimental results verify the effectiveness of each component and show that\nour method achieves new state-of-the-art results on the DocRED dataset, its\nrecently cleaned version, Re-DocRED, and the proposed data regimes.", "published": "2023-04-03 09:11:18", "link": "http://arxiv.org/abs/2304.00824v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GreekBART: The First Pretrained Greek Sequence-to-Sequence Model", "abstract": "The era of transfer learning has revolutionized the fields of Computer Vision\nand Natural Language Processing, bringing powerful pretrained models with\nexceptional performance across a variety of tasks. Specifically, Natural\nLanguage Processing tasks have been dominated by transformer-based language\nmodels. In Natural Language Inference and Natural Language Generation tasks,\nthe BERT model and its variants, as well as the GPT model and its successors,\ndemonstrated exemplary performance. However, the majority of these models are\npretrained and assessed primarily for the English language or on a multilingual\ncorpus. In this paper, we introduce GreekBART, the first Seq2Seq model based on\nBART-base architecture and pretrained on a large-scale Greek corpus. We\nevaluate and compare GreekBART against BART-random, Greek-BERT, and XLM-R on a\nvariety of discriminative tasks. In addition, we examine its performance on two\nNLG tasks from GreekSUM, a newly introduced summarization dataset for the Greek\nlanguage. The model, the code, and the new summarization dataset will be\npublicly available.", "published": "2023-04-03 10:48:51", "link": "http://arxiv.org/abs/2304.00869v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialog-to-Actions: Building Task-Oriented Dialogue System via\n  Action-Level Generation", "abstract": "End-to-end generation-based approaches have been investigated and applied in\ntask-oriented dialogue systems. However, in industrial scenarios, existing\nmethods face the bottlenecks of controllability (e.g., domain-inconsistent\nresponses, repetition problem, etc) and efficiency (e.g., long computation\ntime, etc). In this paper, we propose a task-oriented dialogue system via\naction-level generation. Specifically, we first construct dialogue actions from\nlarge-scale dialogues and represent each natural language (NL) response as a\nsequence of dialogue actions. Further, we train a Sequence-to-Sequence model\nwhich takes the dialogue history as input and outputs sequence of dialogue\nactions. The generated dialogue actions are transformed into verbal responses.\nExperimental results show that our light-weighted method achieves competitive\nperformance, and has the advantage of controllability and efficiency.", "published": "2023-04-03 11:09:20", "link": "http://arxiv.org/abs/2304.00884v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical\n  domains", "abstract": "In recent years, pre-trained language models (PLMs) achieve the best\nperformance on a wide range of natural language processing (NLP) tasks. While\nthe first models were trained on general domain data, specialized ones have\nemerged to more effectively treat specific domains. In this paper, we propose\nan original study of PLMs in the medical domain on French language. We compare,\nfor the first time, the performance of PLMs trained on both public data from\nthe web and private data from healthcare establishments. We also evaluate\ndifferent learning strategies on a set of biomedical tasks. In particular, we\nshow that we can take advantage of already existing biomedical PLMs in a\nforeign language by further pre-train it on our targeted data. Finally, we\nrelease the first specialized PLMs for the biomedical field in French, called\nDrBERT, as well as the largest corpus of medical data under free license on\nwhich these models are trained.", "published": "2023-04-03 13:25:53", "link": "http://arxiv.org/abs/2304.00958v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RPTQ: Reorder-based Post-training Quantization for Large Language Models", "abstract": "Large-scale language models (LLMs) have demonstrated impressive performance,\nbut their deployment presents challenges due to their significant memory usage.\nThis issue can be alleviated through quantization. In this paper, we identify\nthat the challenge in quantizing activations in LLMs arises from varying ranges\nacross channels, rather than solely the presence of outliers. To address this\nchallenge, we introduce a quantization method called RPTQ, which utilizes a\nreorder-based approach. By rearranging the channels and quantizing them in\nclusters, RPTQ effectively mitigates the impact of range differences between\nchannels. To minimize the overhead of the reorder operation, we fuse it into\nthe layer norm operation and weights in linear layers. In our experiments, RPTQ\nachieved a significant breakthrough by utilizing 3-bit activation in LLMs for\nthe first time, resulting in a substantial reduction in memory usage. For\ninstance, quantizing OPT-175b can lead to a memory consumption reduction of up\nto 80%.", "published": "2023-04-03 15:46:15", "link": "http://arxiv.org/abs/2304.01089v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task", "abstract": "The recent progress of large language models (LLMs), including ChatGPT and\nGPT-4, in comprehending and responding to human instructions has been\nremarkable. Nevertheless, these models typically perform better in English and\nhave not been explicitly trained for the medical domain, resulting in\nsuboptimal precision in diagnoses, drug recommendations, and other medical\nadvice. Additionally, training and deploying a dialogue model is still believed\nto be impossible for hospitals, hindering the promotion of LLMs. To tackle\nthese challenges, we have collected databases of medical dialogues in Chinese\nwith ChatGPT's help and adopted several techniques to train an easy-deploy LLM.\nRemarkably, we were able to fine-tune the ChatGLM-6B on a single A100 80G in 13\nhours, which means having a healthcare-purpose LLM can be very affordable.\nDoctorGLM is currently an early-stage engineering attempt and contain various\nmistakes. We are sharing it with the broader community to invite feedback and\nsuggestions to improve its healthcare-focused capabilities:\nhttps://github.com/xionghonglin/DoctorGLM.", "published": "2023-04-03 15:57:51", "link": "http://arxiv.org/abs/2304.01097v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PEACH: Pre-Training Sequence-to-Sequence Multilingual Models for\n  Translation with Semi-Supervised Pseudo-Parallel Document Generation", "abstract": "Multilingual pre-training significantly improves many multilingual NLP tasks,\nincluding machine translation. Most existing methods are based on some variants\nof masked language modeling and text-denoising objectives on monolingual data.\nMultilingual pre-training on monolingual data ignores the availability of\nparallel data in many language pairs. Also, some other works integrate the\navailable human-generated parallel translation data in their pre-training. This\nkind of parallel data is definitely helpful, but it is limited even in\nhigh-resource language pairs. This paper introduces a novel semi-supervised\nmethod, SPDG, that generates high-quality pseudo-parallel data for multilingual\npre-training. First, a denoising model is pre-trained on monolingual data to\nreorder, add, remove, and substitute words, enhancing the pre-training\ndocuments' quality. Then, we generate different pseudo-translations for each\npre-training document using dictionaries for word-by-word translation and\napplying the pre-trained denoising model. The resulting pseudo-parallel data is\nthen used to pre-train our multilingual sequence-to-sequence model, PEACH. Our\nexperiments show that PEACH outperforms existing approaches used in training\nmT5 and mBART on various translation tasks, including supervised, zero- and\nfew-shot scenarios. Moreover, PEACH's ability to transfer knowledge between\nsimilar languages makes it particularly useful for low-resource languages. Our\nresults demonstrate that with high-quality dictionaries for generating accurate\npseudo-parallel, PEACH can be valuable for low-resource languages.", "published": "2023-04-03 18:19:26", "link": "http://arxiv.org/abs/2304.01282v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Approaches to Corpus Creation for Low-Resource Language Technology: the\n  Case of Southern Kurdish and Laki", "abstract": "One of the major challenges that under-represented and endangered language\ncommunities face in language technology is the lack or paucity of language\ndata. This is also the case of the Southern varieties of the Kurdish and Laki\nlanguages for which very limited resources are available with insubstantial\nprogress in tools. To tackle this, we provide a few approaches that rely on the\ncontent of local news websites, a local radio station that broadcasts content\nin Southern Kurdish and fieldwork for Laki. In this paper, we describe some of\nthe challenges of such under-represented languages, particularly in writing and\nstandardization, and also, in retrieving sources of data and retro-digitizing\nhandwritten content to create a corpus for Southern Kurdish and Laki. In\naddition, we study the task of language identification in light of the other\nvariants of Kurdish and Zaza-Gorani languages.", "published": "2023-04-03 19:36:32", "link": "http://arxiv.org/abs/2304.01319v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PALI: A Language Identification Benchmark for Perso-Arabic Scripts", "abstract": "The Perso-Arabic scripts are a family of scripts that are widely adopted and\nused by various linguistic communities around the globe. Identifying various\nlanguages using such scripts is crucial to language technologies and\nchallenging in low-resource setups. As such, this paper sheds light on the\nchallenges of detecting languages using Perso-Arabic scripts, especially in\nbilingual communities where ``unconventional'' writing is practiced. To address\nthis, we use a set of supervised techniques to classify sentences into their\nlanguages. Building on these, we also propose a hierarchical model that targets\nclusters of languages that are more often confused by the classifiers. Our\nexperiment results indicate the effectiveness of our solutions.", "published": "2023-04-03 19:40:14", "link": "http://arxiv.org/abs/2304.01322v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Creating Custom Event Data Without Dictionaries: A Bag-of-Tricks", "abstract": "Event data, or structured records of ``who did what to whom'' that are\nautomatically extracted from text, is an important source of data for scholars\nof international politics. The high cost of developing new event datasets,\nespecially using automated systems that rely on hand-built dictionaries, means\nthat most researchers draw on large, pre-existing datasets such as ICEWS rather\nthan developing tailor-made event datasets optimized for their specific\nresearch question. This paper describes a ``bag of tricks'' for efficient,\ncustom event data production, drawing on recent advances in natural language\nprocessing (NLP) that allow researchers to rapidly produce customized event\ndatasets. The paper introduces techniques for training an event category\nclassifier with active learning, identifying actors and the recipients of\nactions in text using large language models and standard machine learning\nclassifiers and pretrained ``question-answering'' models from NLP, and\nresolving mentions of actors to their Wikipedia article to categorize them. We\ndescribe how these techniques produced the new POLECAT global event dataset\nthat is intended to replace ICEWS, along with examples of how scholars can\nquickly produce smaller, custom event datasets. We publish example code and\nmodels to implement our new techniques.", "published": "2023-04-03 19:51:00", "link": "http://arxiv.org/abs/2304.01331v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Models for Chemical-Protein Interaction Extraction: Better\n  Tokenization and Span-Based Pipeline Strategies", "abstract": "End-to-end relation extraction (E2ERE) is an important task in information\nextraction, more so for biomedicine as scientific literature continues to grow\nexponentially. E2ERE typically involves identifying entities (or named entity\nrecognition (NER)) and associated relations, while most RE tasks simply assume\nthat the entities are provided upfront and end up performing relation\nclassification. E2ERE is inherently more difficult than RE alone given the\npotential snowball effect of errors from NER leading to more errors in RE. A\ncomplex dataset in biomedical E2ERE is the ChemProt dataset (BioCreative VI,\n2017) that identifies relations between chemical compounds and genes/proteins\nin scientific literature. ChemProt is included in all recent biomedical natural\nlanguage processing benchmarks including BLUE, BLURB, and BigBio. However, its\ntreatment in these benchmarks and in other separate efforts is typically not\nend-to-end, with few exceptions. In this effort, we employ a span-based\npipeline approach to produce a new state-of-the-art E2ERE performance on the\nChemProt dataset, resulting in $> 4\\%$ improvement in F1-score over the prior\nbest effort. Our results indicate that a straightforward fine-grained\ntokenization scheme helps span-based approaches excel in E2ERE, especially with\nregards to handling complex named entities. Our error analysis also identifies\na few key failure modes in E2ERE for ChemProt.", "published": "2023-04-03 20:20:22", "link": "http://arxiv.org/abs/2304.01344v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pythia: A Suite for Analyzing Large Language Models Across Training and\n  Scaling", "abstract": "How do large language models (LLMs) develop and evolve over the course of\ntraining? How do these patterns change as models scale? To answer these\nquestions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on\npublic data seen in the exact same order and ranging in size from 70M to 12B\nparameters. We provide public access to 154 checkpoints for each one of the 16\nmodels, alongside tools to download and reconstruct their exact training\ndataloaders for further study. We intend \\textit{Pythia} to facilitate research\nin many areas, and we present several case studies including novel results in\nmemorization, term frequency effects on few-shot performance, and reducing\ngender bias. We demonstrate that this highly controlled setup can be used to\nyield novel insights toward LLMs and their training dynamics. Trained models,\nanalysis code, training code, and training data can be found at\n\\url{https://github.com/EleutherAI/pythia}.", "published": "2023-04-03 20:58:15", "link": "http://arxiv.org/abs/2304.01373v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The StatCan Dialogue Dataset: Retrieving Data Tables through\n  Conversations with Genuine Intents", "abstract": "We introduce the StatCan Dialogue Dataset consisting of 19,379 conversation\nturns between agents working at Statistics Canada and online users looking for\npublished data tables. The conversations stem from genuine intents, are held in\nEnglish or French, and lead to agents retrieving one of over 5000 complex data\ntables. Based on this dataset, we propose two tasks: (1) automatic retrieval of\nrelevant tables based on a on-going conversation, and (2) automatic generation\nof appropriate agent responses at each turn. We investigate the difficulty of\neach task by establishing strong baselines. Our experiments on a temporal data\nsplit reveal that all models struggle to generalize to future conversations, as\nwe observe a significant drop in performance across both tasks when we move\nfrom the validation to the test set. In addition, we find that response\ngeneration models struggle to decide when to return a table. Considering that\nthe tasks pose significant challenges to existing models, we encourage the\ncommunity to develop models for our task, which can be directly used to help\nknowledge workers find relevant tables for live chat users.", "published": "2023-04-03 23:18:30", "link": "http://arxiv.org/abs/2304.01412v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ScandEval: A Benchmark for Scandinavian Natural Language Processing", "abstract": "This paper introduces a Scandinavian benchmarking platform, ScandEval, which\ncan benchmark any pretrained model on four different tasks in the Scandinavian\nlanguages. The datasets used in two of the tasks, linguistic acceptability and\nquestion answering, are new. We develop and release a Python package and\ncommand-line interface, scandeval, which can benchmark any model that has been\nuploaded to the Hugging Face Hub, with reproducible results. Using this\npackage, we benchmark more than 100 Scandinavian or multilingual models and\npresent the results of these in an interactive online leaderboard, as well as\nprovide an analysis of the results. The analysis shows that there is\nsubstantial cross-lingual transfer among the Mainland Scandinavian languages\n(Danish, Swedish and Norwegian), with limited cross-lingual transfer between\nthe group of Mainland Scandinavian languages and the group of Insular\nScandinavian languages (Icelandic and Faroese). The benchmarking results also\nshow that the investment in language technology in Norway, Sweden and Denmark\nhas led to language models that outperform massively multilingual models such\nas XLM-RoBERTa and mDeBERTaV3. We release the source code for both the package\nand leaderboard.", "published": "2023-04-03 11:51:46", "link": "http://arxiv.org/abs/2304.00906v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LAHM : Large Annotated Dataset for Multi-Domain and Multilingual Hate\n  Speech Identification", "abstract": "Current research on hate speech analysis is typically oriented towards\nmonolingual and single classification tasks. In this paper, we present a new\nmultilingual hate speech analysis dataset for English, Hindi, Arabic, French,\nGerman and Spanish languages for multiple domains across hate speech - Abuse,\nRacism, Sexism, Religious Hate and Extremism. To the best of our knowledge,\nthis paper is the first to address the problem of identifying various types of\nhate speech in these five wide domains in these six languages. In this work, we\ndescribe how we created the dataset, created annotations at high level and low\nlevel for different domains and how we use it to test the current\nstate-of-the-art multilingual and multitask learning approaches. We evaluate\nour dataset in various monolingual, cross-lingual and machine translation\nclassification settings and compare it against open source English datasets\nthat we aggregated and merged for this task. Then we discuss how this approach\ncan be used to create large scale hate-speech datasets and how to leverage our\nannotations in order to improve hate speech detection and classification in\ngeneral.", "published": "2023-04-03 12:03:45", "link": "http://arxiv.org/abs/2304.00913v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Robust Text-driven Image Editing Method that Adaptively Explores\n  Directions in Latent Spaces of StyleGAN and CLIP", "abstract": "Automatic image editing has great demands because of its numerous\napplications, and the use of natural language instructions is essential to\nachieving flexible and intuitive editing as the user imagines. A pioneering\nwork in text-driven image editing, StyleCLIP, finds an edit direction in the\nCLIP space and then edits the image by mapping the direction to the StyleGAN\nspace. At the same time, it is difficult to tune appropriate inputs other than\nthe original image and text instructions for image editing. In this study, we\npropose a method to construct the edit direction adaptively in the StyleGAN and\nCLIP spaces with SVM. Our model represents the edit direction as a normal\nvector in the CLIP space obtained by training a SVM to classify positive and\nnegative images. The images are retrieved from a large-scale image corpus,\noriginally used for pre-training StyleGAN, according to the CLIP similarity\nbetween the images and the text instruction. We confirmed that our model\nperformed as well as the StyleCLIP baseline, whereas it allows simple inputs\nwithout increasing the computational time.", "published": "2023-04-03 13:30:48", "link": "http://arxiv.org/abs/2304.00964v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Simple Yet Effective Neural Ranking and Reranking Baselines for\n  Cross-Lingual Information Retrieval", "abstract": "The advent of multilingual language models has generated a resurgence of\ninterest in cross-lingual information retrieval (CLIR), which is the task of\nsearching documents in one language with queries from another. However, the\nrapid pace of progress has led to a confusing panoply of methods and\nreproducibility has lagged behind the state of the art. In this context, our\nwork makes two important contributions: First, we provide a conceptual\nframework for organizing different approaches to cross-lingual retrieval using\nmulti-stage architectures for mono-lingual retrieval as a scaffold. Second, we\nimplement simple yet effective reproducible baselines in the Anserini and\nPyserini IR toolkits for test collections from the TREC 2022 NeuCLIR Track, in\nPersian, Russian, and Chinese. Our efforts are built on a collaboration of the\ntwo teams that submitted the most effective runs to the TREC evaluation. These\ncontributions provide a firm foundation for future advances.", "published": "2023-04-03 14:17:00", "link": "http://arxiv.org/abs/2304.01019v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Hate Speech Targets Detection in Parler using BERT", "abstract": "Online social networks have become a fundamental component of our everyday\nlife. Unfortunately, these platforms are also a stage for hate speech. Popular\nsocial networks have regularized rules against hate speech. Consequently,\nsocial networks like Parler and Gab advocating and claiming to be free speech\nplatforms have evolved. These platforms have become a district for hate speech\nagainst diverse targets. We present in our paper a pipeline for detecting hate\nspeech and its targets and use it for creating Parler hate targets'\ndistribution. The pipeline consists of two models; one for hate speech\ndetection and the second for target classification, both based on BERT with\nBack-Translation and data pre-processing for improved results. The source code\nused in this work, as well as other relevant sources, are available at:\nhttps://github.com/NadavSc/HateRecognition.git", "published": "2023-04-03 17:49:04", "link": "http://arxiv.org/abs/2304.01179v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on\n  Self-Chat Data", "abstract": "Chat models, such as ChatGPT, have shown impressive capabilities and have\nbeen rapidly adopted across numerous domains. However, these models are only\naccessible through a restricted API, creating barriers for new research and\nprogress in the field. We propose a pipeline that can automatically generate a\nhigh-quality multi-turn chat corpus by leveraging ChatGPT to engage in a\nconversation with itself. Subsequently, we employ parameter-efficient tuning to\nenhance LLaMA, an open-source large language model. The resulting model, named\nBaize, demonstrates good performance in multi-turn dialogues with guardrails\nthat minimize potential risks. Furthermore, we propose a new technique called\nSelf-Distill with Feedback, to further improve the performance of the Baize\nmodels with feedback from ChatGPT. The Baize models and data are released for\nresearch purposes only at https://github.com/project-baize/baize-chatbot. An\nonline demo is also available at\nhttps://huggingface.co/spaces/project-baize/chat-with-baize.", "published": "2023-04-03 17:59:09", "link": "http://arxiv.org/abs/2304.01196v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Modal Perceiver Language Model for Outcome Prediction in Emergency\n  Department", "abstract": "Language modeling have shown impressive progress in generating compelling\ntext with good accuracy and high semantic coherence. An interesting research\ndirection is to augment these powerful models for specific applications using\ncontextual information. In this work, we explore multi-modal language modeling\nfor healthcare applications. We are interested in outcome prediction and\npatient triage in hospital emergency department based on text information in\nchief complaints and vital signs recorded at triage. We adapt Perceiver - a\nmodality-agnostic transformer-based model that has shown promising results in\nseveral applications. Since vital-sign modality is represented in tabular\nformat, we modified Perceiver position encoding to ensure permutation\ninvariance. We evaluated the multi-modal language model for the task of\ndiagnosis code prediction using MIMIC-IV ED dataset on 120K visits. In the\nexperimental analysis, we show that mutli-modality improves the prediction\nperformance compared with models trained solely on text or vital signs. We\nidentified disease categories for which multi-modality leads to performance\nimprovement and show that for these categories, vital signs have added\npredictive power. By analyzing the cross-attention layer, we show how\nmulti-modality contributes to model predictions. This work gives interesting\ninsights on the development of multi-modal language models for healthcare\napplications.", "published": "2023-04-03 06:32:00", "link": "http://arxiv.org/abs/2304.01233v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam\n  Detection", "abstract": "This paper investigates the effectiveness of large language models (LLMs) in\nemail spam detection by comparing prominent models from three distinct\nfamilies: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we\nexamine well-established machine learning techniques for spam detection, such\nas Na\\\"ive Bayes and LightGBM, as baseline methods. We assess the performance\nof these models across four public datasets, utilizing different numbers of\ntraining samples (full training set and few-shot settings). Our findings reveal\nthat, in the majority of cases, LLMs surpass the performance of the popular\nbaseline techniques, particularly in few-shot scenarios. This adaptability\nrenders LLMs uniquely suited to spam detection tasks, where labeled samples are\nlimited in number and models require frequent updates. Additionally, we\nintroduce Spam-T5, a Flan-T5 model that has been specifically adapted and\nfine-tuned for the purpose of detecting email spam. Our results demonstrate\nthat Spam-T5 surpasses baseline models and other LLMs in the majority of\nscenarios, particularly when there are a limited number of training samples\navailable. Our code is publicly available at\nhttps://github.com/jpmorganchase/emailspamdetection.", "published": "2023-04-03 10:27:53", "link": "http://arxiv.org/abs/2304.01238v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identifying Mentions of Pain in Mental Health Records Text: A Natural\n  Language Processing Approach", "abstract": "Pain is a common reason for accessing healthcare resources and is a growing\narea of research, especially in its overlap with mental health. Mental health\nelectronic health records are a good data source to study this overlap.\nHowever, much information on pain is held in the free text of these records,\nwhere mentions of pain present a unique natural language processing problem due\nto its ambiguous nature. This project uses data from an anonymised mental\nhealth electronic health records database. The data are used to train a machine\nlearning based classification algorithm to classify sentences as discussing\npatient pain or not. This will facilitate the extraction of relevant pain\ninformation from large databases, and the use of such outputs for further\nstudies on pain and mental health. 1,985 documents were manually\ntriple-annotated for creation of gold standard training data, which was used to\ntrain three commonly used classification algorithms. The best performing model\nachieved an F1-score of 0.98 (95% CI 0.98-0.99).", "published": "2023-04-03 11:56:11", "link": "http://arxiv.org/abs/2304.01240v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Detection of Homophobia & Transphobia in Dravidian Languages: Exploring\n  Deep Learning Methods", "abstract": "The increase in abusive content on online social media platforms is impacting\nthe social life of online users. Use of offensive and hate speech has been\nmaking so-cial media toxic. Homophobia and transphobia constitute offensive\ncomments against LGBT+ community. It becomes imperative to detect and handle\nthese comments, to timely flag or issue a warning to users indulging in such\nbehaviour. However, automated detection of such content is a challenging task,\nmore so in Dravidian languages which are identified as low resource languages.\nMotivated by this, the paper attempts to explore applicability of different\ndeep learning mod-els for classification of the social media comments in\nMalayalam and Tamil lan-guages as homophobic, transphobic and\nnon-anti-LGBT+content. The popularly used deep learning models- Convolutional\nNeural Network (CNN), Long Short Term Memory (LSTM) using GloVe embedding and\ntransformer-based learning models (Multilingual BERT and IndicBERT) are applied\nto the classification problem. Results obtained show that IndicBERT outperforms\nthe other imple-mented models, with obtained weighted average F1-score of 0.86\nand 0.77 for Malayalam and Tamil, respectively. Therefore, the present work\nconfirms higher performance of IndicBERT on the given task in selected\nDravidian languages.", "published": "2023-04-03 12:15:27", "link": "http://arxiv.org/abs/2304.01241v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Clinical Evidence Recommendation with Multi-Channel\n  Heterogeneous Learning on Evidence Graphs", "abstract": "Clinical evidence encompasses the associations and impacts between patients,\ninterventions (such as drugs or physiotherapy), problems, and outcomes. The\ngoal of recommending clinical evidence is to provide medical practitioners with\nrelevant information to support their decision-making processes and to generate\nnew evidence. Our specific task focuses on recommending evidence based on\nclinical problems. However, the direct connections between certain clinical\nproblems and related evidence are often sparse, creating a challenge of link\nsparsity. Additionally, to recommend appropriate evidence, it is essential to\njointly exploit both topological relationships among evidence and textual\ninformation describing them. To address these challenges, we define two\nknowledge graphs: an Evidence Co-reference Graph and an Evidence Text Graph, to\nrepresent the topological and linguistic relations among evidential elements,\nrespectively. We also introduce a multi-channel heterogeneous learning model\nand a fusional attention mechanism to handle the co-reference-text\nheterogeneity in evidence recommendation. Our experiments demonstrate that our\nmodel outperforms state-of-the-art methods on open data.", "published": "2023-04-03 12:15:53", "link": "http://arxiv.org/abs/2304.01242v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficiently Aligned Cross-Lingual Transfer Learning for Conversational\n  Tasks using Prompt-Tuning", "abstract": "Cross-lingual transfer of language models trained on high-resource languages\nlike English has been widely studied for many NLP tasks, but focus on\nconversational tasks has been rather limited. This is partly due to the high\ncost of obtaining non-English conversational data, which results in limited\ncoverage. In this work, we introduce XSGD for cross-lingual alignment\npretraining, a parallel and large-scale multilingual conversation dataset that\nwe created by translating the English-only Schema-Guided Dialogue (SGD) dataset\n(Rastogi et al., 2020) into 105 other languages. XSGD contains approximately\n330k utterances per language. To facilitate aligned cross-lingual\nrepresentations, we develop an efficient prompt-tuning-based method for\nlearning alignment prompts. We also investigate two different classifiers:\nNLI-based and vanilla classifiers, and test cross-lingual capability enabled by\nthe aligned prompts. We evaluate our model's cross-lingual generalization\ncapabilities on two conversation tasks: slot-filling and intent classification.\nOur results demonstrate the strong and efficient modeling ability of NLI-based\nclassifiers and the large cross-lingual transfer improvements achieved by our\naligned prompts, particularly in few-shot settings. In addition, we highlight\nthe nice results of our approach compared to LLMs such as text-davinci-003 and\nChatGPT in both zero-shot and few-shot settings. While LLMs exhibit impressive\nperformance in English, their cross-lingual capabilities in other languages,\nparticularly low-resource languages, are limited.", "published": "2023-04-03 18:46:01", "link": "http://arxiv.org/abs/2304.01295v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Grand Challenge On Detecting Cheapfakes", "abstract": "Cheapfake is a recently coined term that encompasses non-AI (\"cheap\")\nmanipulations of multimedia content. Cheapfakes are known to be more prevalent\nthan deepfakes. Cheapfake media can be created using editing software for\nimage/video manipulations, or even without using any software, by simply\naltering the context of an image/video by sharing the media alongside\nmisleading claims. This alteration of context is referred to as out-of-context\n(OOC) misuse of media. OOC media is much harder to detect than fake media,\nsince the images and videos are not tampered. In this challenge, we focus on\ndetecting OOC images, and more specifically the misuse of real photographs with\nconflicting image captions in news items. The aim of this challenge is to\ndevelop and benchmark models that can be used to detect whether given samples\n(news image and associated captions) are OOC, based on the recently compiled\nCOSMOS dataset.", "published": "2023-04-03 19:50:26", "link": "http://arxiv.org/abs/2304.01328v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Comparison of Document Similarity Algorithms", "abstract": "Document similarity is an important part of Natural Language Processing and\nis most commonly used for plagiarism-detection and text summarization. Thus,\nfinding the overall most effective document similarity algorithm could have a\nmajor positive impact on the field of Natural Language Processing. This report\nsets out to examine the numerous document similarity algorithms, and determine\nwhich ones are the most useful. It addresses the most effective document\nsimilarity algorithm by categorizing them into 3 types of document similarity\nalgorithms: statistical algorithms, neural networks, and corpus/knowledge-based\nalgorithms. The most effective algorithms in each category are also compared in\nour work using a series of benchmark datasets and evaluations that test every\npossible area that each algorithm could be used in.", "published": "2023-04-03 19:50:55", "link": "http://arxiv.org/abs/2304.01330v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Simple and Effective Method of Cross-Lingual Plagiarism Detection", "abstract": "We present a simple cross-lingual plagiarism detection method applicable to a\nlarge number of languages. The presented approach leverages open multilingual\nthesauri for candidate retrieval task and pre-trained multilingual BERT-based\nlanguage models for detailed analysis. The method does not rely on machine\ntranslation and word sense disambiguation when in use, and therefore is\nsuitable for a large number of languages, including under-resourced languages.\nThe effectiveness of the proposed approach is demonstrated for several existing\nand new benchmarks, achieving state-of-the-art results for French, Russian, and\nArmenian languages.", "published": "2023-04-03 20:27:10", "link": "http://arxiv.org/abs/2304.01352v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Does Human Collaboration Enhance the Accuracy of Identifying\n  LLM-Generated Deepfake Texts?", "abstract": "Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved the\ngeneration of coherent sentences resembling human writing on a large scale,\nresulting in the creation of so-called deepfake texts. However, this progress\nposes security and privacy concerns, necessitating effective solutions for\ndistinguishing deepfake texts from human-written ones. Although prior works\nstudied humans' ability to detect deepfake texts, none has examined whether\n\"collaboration\" among humans improves the detection of deepfake texts. In this\nstudy, to address this gap of understanding on deepfake texts, we conducted\nexperiments with two groups: (1) nonexpert individuals from the AMT platform\nand (2) writing experts from the Upwork platform. The results demonstrate that\ncollaboration among humans can potentially improve the detection of deepfake\ntexts for both groups, increasing detection accuracies by 6.36% for non-experts\nand 12.76% for experts, respectively, compared to individuals' detection\naccuracies. We further analyze the explanations that humans used for detecting\na piece of text as deepfake text, and find that the strongest indicator of\ndeepfake texts is their lack of coherence and consistency. Our study provides\nuseful insights for future tools and framework designs to facilitate the\ncollaborative human detection of deepfake texts. The experiment datasets and\nAMT implementations are available at:\nhttps://github.com/huashen218/llm-deepfake-human-study.git", "published": "2023-04-03 14:06:47", "link": "http://arxiv.org/abs/2304.01002v3", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Deep Manifold Learning for Reading Comprehension and Logical Reasoning\n  Tasks with Polytuplet Loss", "abstract": "The current trend in developing machine learning models for reading\ncomprehension and logical reasoning tasks is focused on improving the models'\nabilities to understand and utilize logical rules. This work focuses on\nproviding a novel loss function and accompanying model architecture that has\nmore interpretable components than some other models by representing a common\nstrategy employed by humans when given reading comprehension and logical\nreasoning tasks. Our strategy involves emphasizing relative accuracy over\nabsolute accuracy and can theoretically produce the correct answer with\nincomplete knowledge. We examine the effectiveness of this strategy to solve\nreading comprehension and logical reasoning questions. The models were\nevaluated on the ReClor dataset, a challenging reading comprehension and\nlogical reasoning benchmark. We propose the polytuplet loss function, which\nforces prioritization of learning the relative correctness of answer choices\nover learning the true accuracy of each choice. Our results indicate that\nmodels employing polytuplet loss outperform existing baseline models, though\nfurther research is required to quantify the benefits it may present.", "published": "2023-04-03 14:48:34", "link": "http://arxiv.org/abs/2304.01046v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Can the Inference Logic of Large Language Models be Disentangled into\n  Symbolic Concepts?", "abstract": "In this paper, we explain the inference logic of large language models (LLMs)\nas a set of symbolic concepts. Many recent studies have discovered that\ntraditional DNNs usually encode sparse symbolic concepts. However, because an\nLLM has much more parameters than traditional DNNs, whether the LLM also\nencodes sparse symbolic concepts is still an open problem. Therefore, in this\npaper, we propose to disentangle the inference score of LLMs for dialogue tasks\ninto a small number of symbolic concepts. We verify that we can use those\nsparse concepts to well estimate all inference scores of the LLM on all\narbitrarily masking states of the input sentence. We also evaluate the\ntransferability of concepts encoded by an LLM and verify that symbolic concepts\nusually exhibit high transferability across similar input sentences. More\ncrucially, those symbolic concepts can be used to explain the exact reasons\naccountable for the LLM's prediction errors.", "published": "2023-04-03 15:39:35", "link": "http://arxiv.org/abs/2304.01083v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Crossword: A Semantic Approach to Data Compression via Masking", "abstract": "The traditional methods for data compression are typically based on the\nsymbol-level statistics, with the information source modeled as a long sequence\nof i.i.d. random variables or a stochastic process, thus establishing the\nfundamental limit as entropy for lossless compression and as mutual information\nfor lossy compression. However, the source (including text, music, and speech)\nin the real world is often statistically ill-defined because of its close\nconnection to human perception, and thus the model-driven approach can be quite\nsuboptimal. This study places careful emphasis on English text and exploits its\nsemantic aspect to enhance the compression efficiency further. The main idea\nstems from the puzzle crossword, observing that the hidden words can still be\nprecisely reconstructed so long as some key letters are provided. The proposed\nmasking-based strategy resembles the above game. In a nutshell, the encoder\nevaluates the semantic importance of each word according to the semantic loss\nand then masks the minor ones, while the decoder aims to recover the masked\nwords from the semantic context by means of the Transformer. Our experiments\nshow that the proposed semantic approach can achieve much higher compression\nefficiency than the traditional methods such as Huffman code and UTF-8 code,\nwhile preserving the meaning in the target text to a great extent.", "published": "2023-04-03 16:04:06", "link": "http://arxiv.org/abs/2304.01106v1", "categories": ["cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Safety Analysis in the Era of Large Language Models: A Case Study of\n  STPA using ChatGPT", "abstract": "Can safety analysis make use of Large Language Models (LLMs)? A case study\nexplores Systems Theoretic Process Analysis (STPA) applied to Automatic\nEmergency Brake (AEB) and Electricity Demand Side Management (DSM) systems\nusing ChatGPT. We investigate how collaboration schemes, input semantic\ncomplexity, and prompt guidelines influence STPA results. Comparative results\nshow that using ChatGPT without human intervention may be inadequate due to\nreliability related issues, but with careful design, it may outperform human\nexperts. No statistically significant differences are found when varying the\ninput semantic complexity or using common prompt guidelines, which suggests the\nnecessity for developing domain-specific prompt engineering. We also highlight\nfuture challenges, including concerns about LLM trustworthiness and the\nnecessity for standardisation and regulation in this domain.", "published": "2023-04-03 16:46:49", "link": "http://arxiv.org/abs/2304.01246v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Dual-Attention Neural Transducers for Efficient Wake Word Spotting in\n  Speech Recognition", "abstract": "We present dual-attention neural biasing, an architecture designed to boost\nWake Words (WW) recognition and improve inference time latency on speech\nrecognition tasks. This architecture enables a dynamic switch for its runtime\ncompute paths by exploiting WW spotting to select which branch of its attention\nnetworks to execute for an input audio frame. With this approach, we\neffectively improve WW spotting accuracy while saving runtime compute cost as\ndefined by floating point operations (FLOPs). Using an in-house de-identified\ndataset, we demonstrate that the proposed dual-attention network can reduce the\ncompute cost by $90\\%$ for WW audio frames, with only $1\\%$ increase in the\nnumber of parameters. This architecture improves WW F1 score by $16\\%$ relative\nand improves generic rare word error rate by $3\\%$ relative compared to the\nbaselines.", "published": "2023-04-03 01:19:39", "link": "http://arxiv.org/abs/2304.01905v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Bibliometric Review of Large Language Models Research from 2017 to\n  2023", "abstract": "Large language models (LLMs) are a class of language models that have\ndemonstrated outstanding performance across a range of natural language\nprocessing (NLP) tasks and have become a highly sought-after research area,\nbecause of their ability to generate human-like language and their potential to\nrevolutionize science and technology. In this study, we conduct bibliometric\nand discourse analyses of scholarly literature on LLMs. Synthesizing over 5,000\npublications, this paper serves as a roadmap for researchers, practitioners,\nand policymakers to navigate the current landscape of LLMs research. We present\nthe research trends from 2017 to early 2023, identifying patterns in research\nparadigms and collaborations. We start with analyzing the core algorithm\ndevelopments and NLP tasks that are fundamental in LLMs research. We then\ninvestigate the applications of LLMs in various fields and domains including\nmedicine, engineering, social science, and humanities. Our review also reveals\nthe dynamic, fast-paced evolution of LLMs research. Overall, this paper offers\nvaluable insights into the current state, impact, and potential of LLMs\nresearch and its applications.", "published": "2023-04-03 21:46:41", "link": "http://arxiv.org/abs/2304.02020v1", "categories": ["cs.DL", "cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.DL"}
{"title": "Benchmarking Faithfulness: Towards Accurate Natural Language\n  Explanations in Vision-Language Tasks", "abstract": "With deep neural models increasingly permeating our daily lives comes a need\nfor transparent and comprehensible explanations of their decision-making.\nHowever, most explanation methods that have been developed so far are not\nintuitively understandable for lay users. In contrast, natural language\nexplanations (NLEs) promise to enable the communication of a model's\ndecision-making in an easily intelligible way. While current models\nsuccessfully generate convincing explanations, it is an open question how well\nthe NLEs actually represent the reasoning process of the models - a property\ncalled faithfulness. Although the development of metrics to measure\nfaithfulness is crucial to designing more faithful models, current metrics are\neither not applicable to NLEs or are not designed to compare different model\narchitectures across multiple modalities.\n  Building on prior research on faithfulness measures and based on a detailed\nrationale, we address this issue by proposing three faithfulness metrics:\nAttribution-Similarity, NLE-Sufficiency, and NLE-Comprehensiveness. The\nefficacy of the metrics is evaluated on the VQA-X and e-SNLI-VE datasets of the\ne-ViL benchmark for vision-language NLE generation by systematically applying\nmodifications to the performant e-UG model for which we expect changes in the\nmeasured explanation faithfulness. We show on the e-SNLI-VE dataset that the\nremoval of redundant inputs to the explanation-generation module of e-UG\nsuccessively increases the model's faithfulness on the linguistic modality as\nmeasured by Attribution-Similarity. Further, our analysis demonstrates that\nNLE-Sufficiency and -Comprehensiveness are not necessarily correlated to\nAttribution-Similarity, and we discuss how the two metrics can be utilized to\ngain further insights into the explanation generation process.", "published": "2023-04-03 08:24:10", "link": "http://arxiv.org/abs/2304.08174v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "AUDIT: Audio Editing by Following Instructions with Latent Diffusion\n  Models", "abstract": "Audio editing is applicable for various purposes, such as adding background\nsound effects, replacing a musical instrument, and repairing damaged audio.\nRecently, some diffusion-based methods achieved zero-shot audio editing by\nusing a diffusion and denoising process conditioned on the text description of\nthe output audio. However, these methods still have some problems: 1) they have\nnot been trained on editing tasks and cannot ensure good editing effects; 2)\nthey can erroneously modify audio segments that do not require editing; 3) they\nneed a complete description of the output audio, which is not always available\nor necessary in practical scenarios. In this work, we propose AUDIT, an\ninstruction-guided audio editing model based on latent diffusion models.\nSpecifically, AUDIT has three main design features: 1) we construct triplet\ntraining data (instruction, input audio, output audio) for different audio\nediting tasks and train a diffusion model using instruction and input (to be\nedited) audio as conditions and generating output (edited) audio; 2) it can\nautomatically learn to only modify segments that need to be edited by comparing\nthe difference between the input and output audio; 3) it only needs edit\ninstructions instead of full target audio descriptions as text input. AUDIT\nachieves state-of-the-art results in both objective and subjective metrics for\nseveral audio editing tasks (e.g., adding, dropping, replacement, inpainting,\nsuper-resolution). Demo samples are available at https://audit-demo.github.io/.", "published": "2023-04-03 09:15:51", "link": "http://arxiv.org/abs/2304.00830v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Ensemble prosody prediction for expressive speech synthesis", "abstract": "Generating expressive speech with rich and varied prosody continues to be a\nchallenge for Text-to-Speech. Most efforts have focused on sophisticated neural\narchitectures intended to better model the data distribution. Yet, in\nevaluations it is generally found that no single model is preferred for all\ninput texts. This suggests an approach that has rarely been used before for\nText-to-Speech: an ensemble of models. We apply ensemble learning to prosody\nprediction. We construct simple ensembles of prosody predictors by varying\neither model architecture or model parameter values. To automatically select\namongst the models in the ensemble when performing Text-to-Speech, we propose a\nnovel, and computationally trivial, variance-based criterion. We demonstrate\nthat even a small ensemble of prosody predictors yields useful diversity,\nwhich, combined with the proposed selection criterion, outperforms any\nindividual model from the ensemble.", "published": "2023-04-03 04:21:04", "link": "http://arxiv.org/abs/2304.00714v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Self-Supervised Learning-Based Source Separation for Meeting Data", "abstract": "Source separation can improve automatic speech recognition (ASR) under\nmulti-party meeting scenarios by extracting single-speaker signals from\noverlapped speech. Despite the success of self-supervised learning models in\nsingle-channel source separation, most studies have focused on simulated\nsetups. In this paper, seven SSL models were compared on both simulated and\nreal-world corpora. Then, we propose to integrate the best-performing model\nWavLM into an automatic transcription system through a novel iterative source\nselection method. To improve real-world performance, time-domain unsupervised\nmixture invariant training was adapted to the time-frequency domain.\nExperiments showed that in the transcription system when source separation was\ninserted before an ASR model fine-tuned on separated speech, absolute\nreductions of 1.9% and 1.5% in concatenated minimum-permutation word error rate\nfor an unknown number of speakers (cpWER-us) were observed on the AMI dev and\ntest sets.", "published": "2023-04-03 10:51:29", "link": "http://arxiv.org/abs/2304.00871v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Designing and Evaluating Speech Emotion Recognition Systems: A reality\n  check case study with IEMOCAP", "abstract": "There is an imminent need for guidelines and standard test sets to allow\ndirect and fair comparisons of speech emotion recognition (SER). While\nresources, such as the Interactive Emotional Dyadic Motion Capture (IEMOCAP)\ndatabase, have emerged as widely-adopted reference corpora for researchers to\ndevelop and test models for SER, published work reveals a wide range of\nassumptions and variety in its use that challenge reproducibility and\ngeneralization. Based on a critical review of the latest advances in SER using\nIEMOCAP as the use case, our work aims at two contributions: First, using an\nanalysis of the recent literature, including assumptions made and metrics used\ntherein, we provide a set of SER evaluation guidelines. Second, using recent\npublications with open-sourced implementations, we focus on reproducibility\nassessment in SER.", "published": "2023-04-03 10:16:24", "link": "http://arxiv.org/abs/2304.00860v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Artificial Dendritic Computation: The case for dendrites in neuromorphic\n  circuits", "abstract": "Bio-inspired computing has focused on neuron and synapses with great success.\nHowever, the connections between these, the dendrites, also play an important\nrole. In this paper, we investigate the motivation for replicating dendritic\ncomputation and present a framework to guide future attempts in their\nconstruction. The framework identifies key properties of the dendrites and\npresents and example of dendritic computation in the task of sound\nlocalisation. We evaluate the impact of dendrites on an BiLSTM neural network's\nperformance, finding that dendrite pre-processing reduce the size of network\nrequired for a threshold performance.", "published": "2023-04-03 13:15:32", "link": "http://arxiv.org/abs/2304.00951v2", "categories": ["cs.NE", "cs.ET", "eess.AS"], "primary_category": "cs.NE"}
{"title": "Musical creativity enabled by nonlinear oscillations of a bubble in\n  water", "abstract": "Producing original and arranging existing musical outcomes is an art that\ntakes years of learning and practice to master. Yet, despite the constant\nadvances in the field of AI-powered musical creativity, production of quality\nmusical outcomes remains a prerogative of the humans. Here we demonstrate that\na single bubble in water can be used to produce creative musical outcomes, when\nit nonlinearly oscillates under an acoustic pressure signal that encodes a\npiece of classical music. The audio signal of the response of the bubble\nresembles an electric guitar version of the original composition. We suggest,\nand provide plausible theoretical supporting arguments, that this property of\nthe bubble can be used to create physics-inspired AI systems capable of\nsimulating human creativity in arrangement and composition of music.", "published": "2023-04-03 09:08:52", "link": "http://arxiv.org/abs/2304.00822v1", "categories": ["cs.SD", "cs.NE", "eess.AS", "physics.flu-dyn", "physics.soc-ph"], "primary_category": "cs.SD"}
