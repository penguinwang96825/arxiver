{"title": "Neural Chinese Word Segmentation as Sequence to Sequence Translation", "abstract": "Recently, Chinese word segmentation (CWS) methods using neural networks have\nmade impressive progress. Most of them regard the CWS as a sequence labeling\nproblem which construct models based on local features rather than considering\nglobal information of input sequence. In this paper, we cast the CWS as a\nsequence translation problem and propose a novel sequence-to-sequence CWS model\nwith an attention-based encoder-decoder framework. The model captures the\nglobal information from the input and directly outputs the segmented sequence.\nIt can also tackle other NLP tasks with CWS jointly in an end-to-end mode.\nExperiments on Weibo, PKU and MSRA benchmark datasets show that our approach\nhas achieved competitive performances compared with state-of-the-art methods.\nMeanwhile, we successfully applied our proposed model to jointly learning CWS\nand Chinese spelling correction, which demonstrates its applicability of\nmulti-task fusion.", "published": "2019-11-29 07:22:01", "link": "http://arxiv.org/abs/1911.12982v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis of German Twitter", "abstract": "This thesis explores the ways by how people express their opinions on German\nTwitter, examines current approaches to automatic mining of these feelings, and\nproposes novel methods, which outperform state-of-the-art techniques. For this\npurpose, I introduce a new corpus of German tweets that have been manually\nannotated with sentiments, their targets and holders, as well as polar terms\nand their contextual modifiers. Using these data, I explore four major areas of\nsentiment research: (i) generation of sentiment lexicons, (ii) fine-grained\nopinion mining, (iii) message-level polarity classification, and (iv)\ndiscourse-aware sentiment analysis. In the first task, I compare three popular\ngroups of lexicon generation methods: dictionary-, corpus-, and\nword-embedding-based ones, finding that dictionary-based systems generally\nyield better lexicons than the last two groups. Apart from this, I propose a\nlinear projection algorithm, whose results surpass many existing automatic\nlexicons. Afterwords, in the second task, I examine two common approaches to\nautomatic prediction of sentiments, sources, and targets: conditional random\nfields and recurrent neural networks, obtaining higher scores with the former\nmodel and improving these results even further by redefining the structure of\nCRF graphs. When dealing with message-level polarity classification, I\njuxtapose three major sentiment paradigms: lexicon-, machine-learning-, and\ndeep-learning-based systems, and try to unite the first and last of these\ngroups by introducing a bidirectional neural network with lexicon-based\nattention. Finally, in order to make the new classifier aware of discourse\nstructure, I let it separately analyze the elementary discourse units of each\nmicroblog and infer the overall polarity of a message from the scores of its\nEDUs with the help of two new approaches: latent-marginalized CRFs and\nRecursive Dirichlet Process.", "published": "2019-11-29 11:24:10", "link": "http://arxiv.org/abs/1911.13062v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multi-cascaded Deep Model for Bilingual SMS Classification", "abstract": "Most studies on text classification are focused on the English language.\nHowever, short texts such as SMS are influenced by regional languages. This\nmakes the automatic text classification task challenging due to the\nmultilingual, informal, and noisy nature of language in the text. In this work,\nwe propose a novel multi-cascaded deep learning model called McM for bilingual\nSMS classification. McM exploits $n$-gram level information as well as\nlong-term dependencies of text for learning. Our approach aims to learn a model\nwithout any code-switching indication, lexical normalization, language\ntranslation, or language transliteration. The model relies entirely upon the\ntext as no external knowledge base is utilized for learning. For this purpose,\na 12 class bilingual text dataset is developed from SMS feedbacks of citizens\non public services containing mixed Roman Urdu and English languages. Our model\nachieves high accuracy for classification on this dataset and outperforms the\nprevious model for multilingual text classification, highlighting language\nindependence of McM.", "published": "2019-11-29 11:35:13", "link": "http://arxiv.org/abs/1911.13066v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A probabilistic assessment of the Indo-Aryan Inner-Outer Hypothesis", "abstract": "This paper uses a novel data-driven probabilistic approach to address the\ncentury-old Inner-Outer hypothesis of Indo-Aryan. I develop a Bayesian\nhierarchical mixed-membership model to assess the validity of this hypothesis\nusing a large data set of automatically extracted sound changes operating\nbetween Old Indo-Aryan and Modern Indo-Aryan speech varieties. I employ\ndifferent prior distributions in order to model sound change, one of which, the\nlogistic normal distribution, has not received much attention in linguistics\noutside of Natural Language Processing, despite its many attractive features. I\nfind evidence for cohesive dialect groups that have made their imprint on\ncontemporary Indo-Aryan languages, and find that when a logistic normal prior\nis used, the distribution of dialect components across languages is largely\ncompatible with a core-periphery pattern similar to that proposed under the\nInner-Outer hypothesis.", "published": "2019-11-29 20:09:15", "link": "http://arxiv.org/abs/1912.01957v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Merging Weak and Active Supervision for Semantic Parsing", "abstract": "A semantic parser maps natural language commands (NLs) from the users to\nexecutable meaning representations (MRs), which are later executed in certain\nenvironment to obtain user-desired results. The fully-supervised training of\nsuch parser requires NL/MR pairs, annotated by domain experts, which makes them\nexpensive to collect. However, weakly-supervised semantic parsers are learnt\nonly from pairs of NL and expected execution results, leaving the MRs latent.\nWhile weak supervision is cheaper to acquire, learning from this input poses\ndifficulties. It demands that parsers search a large space with a very weak\nlearning signal and it is hard to avoid spurious MRs that achieve the correct\nanswer in the wrong way. These factors lead to a performance gap between\nparsers trained in weakly- and fully-supervised setting. To bridge this gap, we\nexamine the intersection between weak supervision and active learning, which\nallows the learner to actively select examples and query for manual annotations\nas extra supervision to improve the model trained under weak supervision. We\nstudy different active learning heuristics for selecting examples to query, and\nvarious forms of extra supervision for such queries. We evaluate the\neffectiveness of our method on two different datasets. Experiments on the\nWikiSQL show that by annotating only 1.8% of examples, we improve over a\nstate-of-the-art weakly-supervised baseline by 6.4%, achieving an accuracy of\n79.0%, which is only 1.3% away from the model trained with full supervision.\nExperiments on WikiTableQuestions with human annotators show that our method\ncan improve the performance with only 100 active queries, especially for\nweakly-supervised parsers learnt from a cold start.", "published": "2019-11-29 07:48:19", "link": "http://arxiv.org/abs/1911.12986v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Iterative Polishing Framework based on Quality Aware Masked Language\n  Model for Chinese Poetry Generation", "abstract": "Owing to its unique literal and aesthetical characteristics, automatic\ngeneration of Chinese poetry is still challenging in Artificial Intelligence,\nwhich can hardly be straightforwardly realized by end-to-end methods. In this\npaper, we propose a novel iterative polishing framework for highly qualified\nChinese poetry generation. In the first stage, an encoder-decoder structure is\nutilized to generate a poem draft. Afterwards, our proposed Quality-Aware\nMasked Language Model (QAMLM) is employed to polish the draft towards higher\nquality in terms of linguistics and literalness. Based on a multi-task learning\nscheme, QA-MLM is able to determine whether polishing is needed based on the\npoem draft. Furthermore, QAMLM is able to localize improper characters of the\npoem draft and substitute with newly predicted ones accordingly. Benefited from\nthe masked language model structure, QAMLM incorporates global context\ninformation into the polishing process, which can obtain more appropriate\npolishing results than the unidirectional sequential decoding. Moreover, the\niterative polishing process will be terminated automatically when QA-MLM\nregards the processed poem as a qualified one. Both human and automatic\nevaluation have been conducted, and the results demonstrate that our approach\nis effective to improve the performance of encoder-decoder structure.", "published": "2019-11-29 16:34:00", "link": "http://arxiv.org/abs/1911.13182v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deconstructing and reconstructing word embedding algorithms", "abstract": "Uncontextualized word embeddings are reliable feature representations of\nwords used to obtain high quality results for various NLP applications. Given\nthe historical success of word embeddings in NLP, we propose a retrospective on\nsome of the most well-known word embedding algorithms. In this work, we\ndeconstruct Word2vec, GloVe, and others, into a common form, unveiling some of\nthe necessary and sufficient conditions required for making performant word\nembeddings. We find that each algorithm: (1) fits vector-covector dot products\nto approximate pointwise mutual information (PMI); and, (2) modulates the loss\ngradient to balance weak and strong signals. We demonstrate that these two\nalgorithmic features are sufficient conditions to construct a novel word\nembedding algorithm, Hilbert-MLE. We find that its embeddings obtain equivalent\nor better performance against other algorithms across 17 intrinsic and\nextrinsic datasets.", "published": "2019-11-29 18:27:36", "link": "http://arxiv.org/abs/1911.13280v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Kurdish (Sorani) Speech to Text: Presenting an Experimental Dataset", "abstract": "We present an experimental dataset, Basic Dataset for Sorani Kurdish\nAutomatic Speech Recognition (BD-4SK-ASR), which we used in the first attempt\nin developing an automatic speech recognition for Sorani Kurdish. The objective\nof the project was to develop a system that automatically could recognize\nsimple sentences based on the vocabulary which is used in grades one to three\nof the primary schools in the Kurdistan Region of Iraq. We used CMUSphinx as\nour experimental environment. We developed a dataset to train the system. The\ndataset is publicly available for non-commercial use under the CC BY-NC-SA 4.0\nlicense.", "published": "2019-11-29 12:55:41", "link": "http://arxiv.org/abs/1911.13087v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Method and Dataset Mining in Scientific Papers", "abstract": "Literature analysis facilitates researchers better understanding the\ndevelopment of science and technology. The conventional literature analysis\nfocuses on the topics, authors, abstracts, keywords, references, etc., and\nrarely pays attention to the content of papers. In the field of machine\nlearning, the involved methods (M) and datasets (D) are key information in\npapers. The extraction and mining of M and D are useful for discipline analysis\nand algorithm recommendation. In this paper, we propose a novel entity\nrecognition model, called MDER, and constructe datasets from the papers of the\nPAKDD conferences (2009-2019). Some preliminary experiments are conducted to\nassess the extraction performance and the mining results are visualized.", "published": "2019-11-29 13:19:45", "link": "http://arxiv.org/abs/1911.13096v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Attentive Modality Hopping Mechanism for Speech Emotion Recognition", "abstract": "In this work, we explore the impact of visual modality in addition to speech\nand text for improving the accuracy of the emotion detection system. The\ntraditional approaches tackle this task by fusing the knowledge from the\nvarious modalities independently for performing emotion classification. In\ncontrast to these approaches, we tackle the problem by introducing an attention\nmechanism to combine the information. In this regard, we first apply a neural\nnetwork to obtain hidden representations of the modalities. Then, the attention\nmechanism is defined to select and aggregate important parts of the video data\nby conditioning on the audio and text data. Furthermore, the attention\nmechanism is again applied to attend important parts of the speech and textual\ndata, by considering other modality. Experiments are performed on the standard\nIEMOCAP dataset using all three modalities (audio, text, and video). The\nachieved results show a significant improvement of 3.65% in terms of weighted\naccuracy compared to the baseline system.", "published": "2019-11-29 13:23:23", "link": "http://arxiv.org/abs/1912.00846v2", "categories": ["cs.LG", "cs.CL", "cs.SD", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Bimodal Speech Emotion Recognition Using Pre-Trained Language Models", "abstract": "Speech emotion recognition is a challenging task and an important step\ntowards more natural human-machine interaction. We show that pre-trained\nlanguage models can be fine-tuned for text emotion recognition, achieving an\naccuracy of 69.5% on Task 4A of SemEval 2017, improving upon the previous state\nof the art by over 3% absolute. We combine these language models with speech\nemotion recognition, achieving results of 73.5% accuracy when using provided\ntranscriptions and speech data on a subset of four classes of the IEMOCAP\ndataset. The use of noise-induced transcriptions and speech data results in an\naccuracy of 71.4%. For our experiments, we created IEmoNet, a modular and\nadaptable bimodal framework for speech emotion recognition based on pre-trained\nlanguage models. Lastly, we discuss the idea of using an emotional classifier\nas a reward for reinforcement learning as a step towards more successful and\nconvenient human-machine interaction.", "published": "2019-11-29 23:25:20", "link": "http://arxiv.org/abs/1912.02610v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML", "I.2.7"], "primary_category": "eess.AS"}
{"title": "Effects of a Hovering Unmanned Aerial Vehicle on Urban Soundscapes\n  Perception", "abstract": "Several industry leaders and governmental agencies are currently\ninvestigating the use of Unmanned Aerial Vehicles (UAVs), or drones as commonly\nknown, for an ever-growing number of applications from blue light services to\nparcel delivery. For the specific case of the delivery sector, drones can\nalleviate road space usage and also lead to reductions in CO2 and air pollution\nemissions, compared to traditional diesel-powered vehicles. However, due to\ntheir unconventional acoustic characteristics and operational manoeuvres, it is\nuncertain how communities will respond to drone operations. Noise has been\nsuggested as a major barrier to public acceptance of drone operations in urban\nareas. In this paper, a series of audio-visual scenarios were created to\ninvestigate the effects of drone noise on the reported loudness, annoyance and\npleasantness of seven different types of urban soundscapes. In soundscapes\nhighly impacted by road traffic noise, the presence of drone noise lead to\nsmall changes in the perceived loudness, annoyance and pleasantness. In\nsoundscapes with reduced road traffic noise, the participants reported a\nsignificantly higher perceived loudness and annoyance and a lower pleasantness\nwith the presence of the same drone noise. For instance, the reported annoyance\nincreased from 2.3 (without drone noise) to 6.8 (with drone noise), in an\n11-point scale (0-not at all, 10-extremely). Based on these results, the\nconcentration of drone operations along flight paths through busy roads might\naid in the mitigation of the overall community noise impact caused by drones.", "published": "2019-11-29 23:08:42", "link": "http://arxiv.org/abs/1912.00087v1", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
{"title": "J-Net: Randomly weighted U-Net for audio source separation", "abstract": "Several results in the computer vision literature have shown the potential of\nrandomly weighted neural networks. While they perform fairly well as feature\nextractors for discriminative tasks, a positive correlation exists between\ntheir performance and their fully trained counterparts. According to these\ndiscoveries, we pose two questions: what is the value of randomly weighted\nnetworks in difficult generative audio tasks such as audio source separation\nand does such positive correlation still exist when it comes to large random\nnetworks and their trained counterparts? In this paper, we demonstrate that the\npositive correlation still exists. Based on this discovery, we can try out\ndifferent architecture designs or tricks without training the whole model.\nMeanwhile, we find a surprising result that in comparison to the non-trained\nencoder (down-sample path) in Wave-U-Net, fixing the decoder (up-sample path)\nto random weights results in better performance, almost comparable to the fully\ntrained model.", "published": "2019-11-29 02:24:05", "link": "http://arxiv.org/abs/1911.12926v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Voice Separation by Incorporating End-to-end Speech\n  Recognition", "abstract": "Despite recent advances in voice separation methods, many challenges remain\nin realistic scenarios such as noisy recording and the limits of available\ndata. In this work, we propose to explicitly incorporate the phonetic and\nlinguistic nature of speech by taking a transfer learning approach using an\nend-to-end automatic speech recognition (E2EASR) system. The voice separation\nis conditioned on deep features extracted from E2EASR to cover the long-term\ndependence of phonetic aspects. Experimental results on speech separation and\nenhancement task on the AVSpeech dataset show that the proposed method\nsignificantly improves the signal-to-distortion ratio over the baseline model\nand even outperforms an audio visual model, that utilizes visual information of\nlip movements.", "published": "2019-11-29 02:37:17", "link": "http://arxiv.org/abs/1911.12928v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
