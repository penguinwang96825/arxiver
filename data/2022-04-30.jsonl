{"title": "Building a Role Specified Open-Domain Dialogue System Leveraging\n  Large-Scale Language Models", "abstract": "Recent open-domain dialogue models have brought numerous breakthroughs.\nHowever, building a chat system is not scalable since it often requires a\nconsiderable volume of human-human dialogue data, especially when enforcing\nfeatures such as persona, style, or safety. In this work, we study the\nchallenge of imposing roles on open-domain dialogue systems, with the goal of\nmaking the systems maintain consistent roles while conversing naturally with\nhumans. To accomplish this, the system must satisfy a role specification that\nincludes certain conditions on the stated features as well as a system policy\non whether or not certain types of utterances are allowed. For this, we propose\nan efficient data collection framework leveraging in-context few-shot learning\nof large-scale language models for building role-satisfying dialogue dataset\nfrom scratch. We then compare various architectures for open-domain dialogue\nsystems in terms of meeting role specifications while maintaining\nconversational abilities. Automatic and human evaluations show that our models\nreturn few out-of-bounds utterances, keeping competitive performance on general\nmetrics. We release a Korean dialogue dataset we built for further research.", "published": "2022-04-30 06:23:06", "link": "http://arxiv.org/abs/2205.00176v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Practice Makes a Solver Perfect: Data Augmentation for Math Word Problem\n  Solvers", "abstract": "Existing Math Word Problem (MWP) solvers have achieved high accuracy on\nbenchmark datasets. However, prior works have shown that such solvers do not\ngeneralize well and rely on superficial cues to achieve high performance. In\nthis paper, we first conduct experiments to showcase that this behaviour is\nmainly associated with the limited size and diversity present in existing MWP\ndatasets. Next, we propose several data augmentation techniques broadly\ncategorized into Substitution and Paraphrasing based methods. By deploying\nthese methods we increase the size of existing datasets by five folds.\nExtensive experiments on two benchmark datasets across three state-of-the-art\nMWP solvers show that proposed methods increase the generalization and\nrobustness of existing solvers. On average, proposed methods significantly\nincrease the state-of-the-art results by over five percentage points on\nbenchmark datasets. Further, the solvers trained on the augmented dataset\nperform comparatively better on the challenge test set. We also show the\neffectiveness of proposed techniques through ablation studies and verify the\nquality of augmented samples through human evaluation.", "published": "2022-04-30 06:23:30", "link": "http://arxiv.org/abs/2205.00177v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A New Evaluation Method: Evaluation Data and Metrics for Chinese Grammar\n  Error Correction", "abstract": "As a fundamental task in natural language processing, Chinese Grammatical\nError Correction (CGEC) has gradually received widespread attention and become\na research hotspot. However, one obvious deficiency for the existing CGEC\nevaluation system is that the evaluation values are significantly influenced by\nthe Chinese word segmentation results or different language models. The\nevaluation values of the same error correction model can vary considerably\nunder different word segmentation systems or different language models.\nHowever, it is expected that these metrics should be independent of the word\nsegmentation results and language models, as they may lead to a lack of\nuniqueness and comparability in the evaluation of different methods. To this\nend, we propose three novel evaluation metrics for CGEC in two dimensions:\nreference-based and reference-less. In terms of the reference-based metric, we\nintroduce sentence-level accuracy and char-level BLEU to evaluate the corrected\nsentences. Besides, in terms of the reference-less metric, we adopt char-level\nmeaning preservation to measure the semantic preservation degree of the\ncorrected sentences. We deeply evaluate and analyze the reasonableness and\nvalidity of the three proposed metrics, and we expect them to become a new\nstandard for CGEC.", "published": "2022-04-30 09:40:04", "link": "http://arxiv.org/abs/2205.00217v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language\n  Processing", "abstract": "The success of Pre-Trained Models (PTMs) has reshaped the development of\nNatural Language Processing (NLP). Yet, it is not easy to obtain\nhigh-performing models and deploy them online for industrial practitioners. To\nbridge this gap, EasyNLP is designed to make it easy to build NLP applications,\nwhich supports a comprehensive suite of NLP algorithms. It further features\nknowledge-enhanced pre-training, knowledge distillation and few-shot learning\nfunctionalities for large-scale PTMs, and provides a unified framework of model\ntraining, inference and deployment for real-world applications. Currently,\nEasyNLP has powered over ten business units within Alibaba Group and is\nseamlessly integrated to the Platform of AI (PAI) products on Alibaba Cloud.\nThe source code of our EasyNLP toolkit is released at GitHub\n(https://github.com/alibaba/EasyNLP).", "published": "2022-04-30 13:03:53", "link": "http://arxiv.org/abs/2205.00258v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Cross-Lingual Lexical Knowledge from Multilingual Sentence\n  Encoders", "abstract": "Pretrained multilingual language models (LMs) can be successfully transformed\ninto multilingual sentence encoders (SEs; e.g., LaBSE, xMPNet) via additional\nfine-tuning or model distillation with parallel data. However, it remains\nunclear how to best leverage them to represent sub-sentence lexical items\n(i.e., words and phrases) in cross-lingual lexical tasks. In this work, we\nprobe SEs for the amount of cross-lingual lexical knowledge stored in their\nparameters, and compare them against the original multilingual LMs. We also\ndevise a simple yet efficient method for exposing the cross-lingual lexical\nknowledge by means of additional fine-tuning through inexpensive contrastive\nlearning that requires only a small amount of word translation pairs. Using\nbilingual lexical induction (BLI), cross-lingual lexical semantic similarity,\nand cross-lingual entity linking as lexical probing tasks, we report\nsubstantial gains on standard benchmarks (e.g., +10 Precision@1 points in BLI).\nThe results indicate that the SEs such as LaBSE can be 'rewired' into effective\ncross-lingual lexical encoders via the contrastive learning procedure, and that\nthey contain more cross-lingual lexical knowledge than what 'meets the eye'\nwhen they are used as off-the-shelf SEs. This way, we also provide an effective\ntool for harnessing 'covert' multilingual lexical knowledge hidden in\nmultilingual sentence encoders.", "published": "2022-04-30 13:23:16", "link": "http://arxiv.org/abs/2205.00267v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clues Before Answers: Generation-Enhanced Multiple-Choice QA", "abstract": "A trending paradigm for multiple-choice question answering (MCQA) is using a\ntext-to-text framework. By unifying data in different tasks into a single\ntext-to-text format, it trains a generative encoder-decoder model which is both\npowerful and universal. However, a side effect of twisting a generation target\nto fit the classification nature of MCQA is the under-utilization of the\ndecoder and the knowledge that can be decoded. To exploit the generation\ncapability and underlying knowledge of a pre-trained encoder-decoder model, in\nthis paper, we propose a generation-enhanced MCQA model named GenMC. It\ngenerates a clue from the question and then leverages the clue to enhance a\nreader for MCQA. It outperforms text-to-text models on multiple MCQA datasets.", "published": "2022-04-30 14:01:40", "link": "http://arxiv.org/abs/2205.00274v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AdapterBias: Parameter-efficient Token-dependent Representation Shift\n  for Adapters in NLP Tasks", "abstract": "Transformer-based pre-trained models with millions of parameters require\nlarge storage. Recent approaches tackle this shortcoming by training adapters,\nbut these approaches still require a relatively large number of parameters. In\nthis study, AdapterBias, a surprisingly simple yet effective adapter\narchitecture, is proposed. AdapterBias adds a token-dependent shift to the\nhidden output of transformer layers to adapt to downstream tasks with only a\nvector and a linear layer. Extensive experiments are conducted to demonstrate\nthe effectiveness of AdapterBias. The experiments show that our proposed method\ncan dramatically reduce the trainable parameters compared to the previous works\nwith a minimal decrease in task performances compared with fine-tuned\npre-trained models. We further find that AdapterBias automatically learns to\nassign more significant representation shifts to the tokens related to the task\nin consideration.", "published": "2022-04-30 16:49:41", "link": "http://arxiv.org/abs/2205.00305v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detoxifying Language Models with a Toxic Corpus", "abstract": "Existing studies have investigated the tendency of autoregressive language\nmodels to generate contexts that exhibit undesired biases and toxicity. Various\ndebiasing approaches have been proposed, which are primarily categorized into\ndata-based and decoding-based. In our study, we investigate the ensemble of the\ntwo debiasing paradigms, proposing to use toxic corpus as an additional\nresource to reduce the toxicity. Our result shows that toxic corpus can indeed\nhelp to reduce the toxicity of the language generation process substantially,\ncomplementing the existing debiasing methods.", "published": "2022-04-30 18:25:18", "link": "http://arxiv.org/abs/2205.00320v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HateCheckHIn: Evaluating Hindi Hate Speech Detection Models", "abstract": "Due to the sheer volume of online hate, the AI and NLP communities have\nstarted building models to detect such hateful content. Recently, multilingual\nhate is a major emerging challenge for automated detection where code-mixing or\nmore than one language have been used for conversation in social media.\nTypically, hate speech detection models are evaluated by measuring their\nperformance on the held-out test data using metrics such as accuracy and\nF1-score. While these metrics are useful, it becomes difficult to identify\nusing them where the model is failing, and how to resolve it. To enable more\ntargeted diagnostic insights of such multilingual hate speech models, we\nintroduce a set of functionalities for the purpose of evaluation. We have been\ninspired to design this kind of functionalities based on real-world\nconversation on social media. Considering Hindi as a base language, we craft\ntest cases for each functionality. We name our evaluation dataset HateCheckHIn.\nTo illustrate the utility of these functionalities , we test state-of-the-art\ntransformer based m-BERT model and the Perspective API.", "published": "2022-04-30 19:09:09", "link": "http://arxiv.org/abs/2205.00328v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Solution of DeBERTaV3 on CommonsenseQA", "abstract": "We report the performance of DeBERTaV3 on CommonsenseQA in this report. We\nsimply formalize the answer selection as a text classification for DeBERTaV3.\nThe strong natural language inference ability of DeBERTaV3 helps its single and\nensemble model set the new (w/o external knowledge) state-of-the-art on\nCommonsenseQA.", "published": "2022-04-30 12:50:10", "link": "http://arxiv.org/abs/2206.05033v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ExSum: From Local Explanations to Model Understanding", "abstract": "Interpretability methods are developed to understand the working mechanisms\nof black-box models, which is crucial to their responsible deployment.\nFulfilling this goal requires both that the explanations generated by these\nmethods are correct and that people can easily and reliably understand them.\nWhile the former has been addressed in prior work, the latter is often\noverlooked, resulting in informal model understanding derived from a handful of\nlocal explanations. In this paper, we introduce explanation summary (ExSum), a\nmathematical framework for quantifying model understanding, and propose metrics\nfor its quality assessment. On two domains, ExSum highlights various\nlimitations in the current practice, helps develop accurate model\nunderstanding, and reveals easily overlooked properties of the model. We also\nconnect understandability to other properties of explanations such as human\nalignment, robustness, and counterfactual minimality and plausibility.", "published": "2022-04-30 02:07:20", "link": "http://arxiv.org/abs/2205.00130v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Two-Stream AMR-enhanced Model for Document-level Event Argument\n  Extraction", "abstract": "Most previous studies aim at extracting events from a single sentence, while\ndocument-level event extraction still remains under-explored. In this paper, we\nfocus on extracting event arguments from an entire document, which mainly faces\ntwo critical problems: a) the long-distance dependency between trigger and\narguments over sentences; b) the distracting context towards an event in the\ndocument. To address these issues, we propose a Two-Stream Abstract meaning\nRepresentation enhanced extraction model (TSAR). TSAR encodes the document from\ndifferent perspectives by a two-stream encoding module, to utilize local and\nglobal information and lower the impact of distracting context. Besides, TSAR\nintroduces an AMR-guided interaction module to capture both intra-sentential\nand inter-sentential features, based on the locally and globally constructed\nAMR semantic graphs. An auxiliary boundary loss is introduced to enhance the\nboundary information for text spans explicitly. Extensive experiments\nillustrate that TSAR outperforms previous state-of-the-art by a large margin,\nwith 2.54 F1 and 5.13 F1 performance gain on the public RAMS and WikiEvents\ndatasets respectively, showing the superiority in the cross-sentence arguments\nextraction. We release our code in https://github.com/ PKUnlp-icler/TSAR.", "published": "2022-04-30 11:17:26", "link": "http://arxiv.org/abs/2205.00241v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Opponent Modeling in Negotiation Dialogues by Related Data Adaptation", "abstract": "Opponent modeling is the task of inferring another party's mental state\nwithin the context of social interactions. In a multi-issue negotiation, it\ninvolves inferring the relative importance that the opponent assigns to each\nissue under discussion, which is crucial for finding high-value deals. A\npractical model for this task needs to infer these priorities of the opponent\non the fly based on partial dialogues as input, without needing additional\nannotations for training. In this work, we propose a ranker for identifying\nthese priorities from negotiation dialogues. The model takes in a partial\ndialogue as input and predicts the priority order of the opponent. We further\ndevise ways to adapt related data sources for this task to provide more\nexplicit supervision for incorporating the opponent's preferences and offers,\nas a proxy to relying on granular utterance-level annotations. We show the\nutility of our proposed approach through extensive experiments based on two\ndialogue datasets. We find that the proposed data adaptations lead to strong\nperformance in zero-shot and few-shot scenarios. Moreover, they allow the model\nto perform better than baselines while accessing fewer utterances from the\nopponent. We release our code to support future work in this direction.", "published": "2022-04-30 21:11:41", "link": "http://arxiv.org/abs/2205.00344v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Process-Oriented, Modular, and Versatile Question Generation\n  that Meets Educational Needs", "abstract": "NLP-powered automatic question generation (QG) techniques carry great\npedagogical potential of saving educators' time and benefiting student\nlearning. Yet, QG systems have not been widely adopted in classrooms to date.\nIn this work, we aim to pinpoint key impediments and investigate how to improve\nthe usability of automatic QG techniques for educational purposes by\nunderstanding how instructors construct questions and identifying touch points\nto enhance the underlying NLP models. We perform an in-depth need finding study\nwith 11 instructors across 7 different universities, and summarize their\nthought processes and needs when creating questions. While instructors show\ngreat interests in using NLP systems to support question design, none of them\nhas used such tools in practice. They resort to multiple sources of\ninformation, ranging from domain knowledge to students' misconceptions, all of\nwhich missing from today's QG systems. We argue that building effective\nhuman-NLP collaborative QG systems that emphasize instructor control and\nexplainability is imperative for real-world adoption. We call for QG systems to\nprovide process-oriented support, use modular design, and handle diverse\nsources of input.", "published": "2022-04-30 22:24:39", "link": "http://arxiv.org/abs/2205.00355v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Using Wordle for Learning to Design and Compare Strategies", "abstract": "Wordle is a very popular word game that is owned by the New York Times. We\ncan design parameterized strategies for solving Wordle, based on probabilistic,\nstatistical, and information-theoretical information about the games. The\nstrategies can handle a reasonably large family of Wordle-like games both\nsystematically and dynamically, meaning that we do not rely on precomputations\nthat may work for non-fixed games. More specifically, the answer set can be\narbitrary, not confining to the current 2315 words. The answer words may\ninclude any specific number of letters (does not have to be five), and the set\nof symbols that form the words does not have to be limited to only the English\nalphabet.\n  Exploring possible strategies for solving the Wordle-like games offers an\nattractive learning challenges for students who are learning to design computer\ngames. This paper will provide the results of using two families of\nparameterized strategies to solve the current Wordle, using the simulator that\nabides by the hard-mode rules as the baseline. The baseline simulator used an\naverage of 4.078 guesses to find the 2315 answers, and needed more than six\ntrials to solve the game 1.77% of the time. The best performing strategy of\nours used an average of 3.674 guesses to find the 2315 answers, and failed\n0.65% of the time.", "published": "2022-04-30 14:41:25", "link": "http://arxiv.org/abs/2205.11225v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Multimodal Representation Learning With Text and Images", "abstract": "In recent years, multimodal AI has seen an upward trend as researchers are\nintegrating data of different types such as text, images, speech into modelling\nto get the best results. This project leverages multimodal AI and matrix\nfactorization techniques for representation learning, on text and image data\nsimultaneously, thereby employing the widely used techniques of Natural\nLanguage Processing (NLP) and Computer Vision. The learnt representations are\nevaluated using downstream classification and regression tasks. The methodology\nadopted can be extended beyond the scope of this project as it uses\nAuto-Encoders for unsupervised representation learning.", "published": "2022-04-30 03:25:01", "link": "http://arxiv.org/abs/2205.00142v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Leveraging Emotion-specific Features to Improve Transformer Performance\n  for Emotion Classification", "abstract": "This paper describes the approach to the Emotion Classification shared task\nheld at WASSA 2022 by team PVGs AI Club. This Track 2 sub-task focuses on\nbuilding models which can predict a multi-class emotion label based on essays\nfrom news articles where a person, group or another entity is affected.\nBaseline transformer models have been demonstrating good results on sequence\nclassification tasks, and we aim to improve this performance with the help of\nensembling techniques, and by leveraging two variations of emotion-specific\nrepresentations. We observe better results than our baseline models and achieve\nan accuracy of 0.619 and a macro F1 score of 0.520 on the emotion\nclassification task.", "published": "2022-04-30 14:36:04", "link": "http://arxiv.org/abs/2205.00283v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visual Spatial Reasoning", "abstract": "Spatial relations are a basic part of human cognition. However, they are\nexpressed in natural language in a variety of ways, and previous work has\nsuggested that current vision-and-language models (VLMs) struggle to capture\nrelational information. In this paper, we present Visual Spatial Reasoning\n(VSR), a dataset containing more than 10k natural text-image pairs with 66\ntypes of spatial relations in English (such as: under, in front of, and\nfacing). While using a seemingly simple annotation format, we show how the\ndataset includes challenging linguistic phenomena, such as varying reference\nframes. We demonstrate a large gap between human and model performance: the\nhuman ceiling is above 95%, while state-of-the-art models only achieve around\n70%. We observe that VLMs' by-relation performances have little correlation\nwith the number of training examples and the tested models are in general\nincapable of recognising relations concerning the orientations of objects.", "published": "2022-04-30 23:03:49", "link": "http://arxiv.org/abs/2205.00363v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Visualizing and Explaining Language Models", "abstract": "During the last decade, Natural Language Processing has become, after\nComputer Vision, the second field of Artificial Intelligence that was massively\nchanged by the advent of Deep Learning. Regardless of the architecture, the\nlanguage models of the day need to be able to process or generate text, as well\nas predict missing words, sentences or relations depending on the task. Due to\ntheir black-box nature, such models are difficult to interpret and explain to\nthird parties. Visualization is often the bridge that language model designers\nuse to explain their work, as the coloring of the salient words and phrases,\nclustering or neuron activations can be used to quickly understand the\nunderlying models. This paper showcases the techniques used in some of the most\npopular Deep Learning for NLP visualizations, with a special focus on\ninterpretability and explainability.", "published": "2022-04-30 17:23:33", "link": "http://arxiv.org/abs/2205.10238v1", "categories": ["cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "To Know by the Company Words Keep and What Else Lies in the Vicinity", "abstract": "The development of state-of-the-art (SOTA) Natural Language Processing (NLP)\nsystems has steadily been establishing new techniques to absorb the statistics\nof linguistic data. These techniques often trace well-known constructs from\ntraditional theories, and we study these connections to close gaps around key\nNLP methods as a means to orient future work. For this, we introduce an\nanalytic model of the statistics learned by seminal algorithms (including GloVe\nand Word2Vec), and derive insights for systems that use these algorithms and\nthe statistics of co-occurrence, in general. In this work, we derive -- to the\nbest of our knowledge -- the first known solution to Word2Vec's\nsoftmax-optimized, skip-gram algorithm. This result presents exciting potential\nfor future development as a direct solution to a deep learning (DL) language\nmodel's (LM's) matrix factorization. However, we use the solution to\ndemonstrate a seemingly-universal existence of a property that word vectors\nexhibit and which allows for the prophylactic discernment of biases in data --\nprior to their absorption by DL models. To qualify our work, we conduct an\nanalysis of independence, i.e., on the density of statistical dependencies in\nco-occurrence models, which in turn renders insights on the distributional\nhypothesis' partial fulfillment by co-occurrence statistics.", "published": "2022-04-30 03:47:48", "link": "http://arxiv.org/abs/2205.00148v1", "categories": ["cs.CL", "cs.IT", "math.IT", "physics.data-an", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Taylor, Can You Hear Me Now? A Taylor-Unfolding Framework for Monaural\n  Speech Enhancement", "abstract": "While the deep learning techniques promote the rapid development of the\nspeech enhancement (SE) community, most schemes only pursue the performance in\na black-box manner and lack adequate model interpretability. Inspired by\nTaylor's approximation theory, we propose an interpretable decoupling-style SE\nframework, which disentangles the complex spectrum recovery into two separate\noptimization problems \\emph{i.e.}, magnitude and complex residual estimation.\nSpecifically, serving as the 0th-order term in Taylor's series, a filter\nnetwork is delicately devised to suppress the noise component only in the\nmagnitude domain and obtain a coarse spectrum. To refine the phase\ndistribution, we estimate the sparse complex residual, which is defined as the\ndifference between target and coarse spectra, and measures the phase gap. In\nthis study, we formulate the residual component as the combination of various\nhigh-order Taylor terms and propose a lightweight trainable module to replace\nthe complicated derivative operator between adjacent terms. Finally, following\nTaylor's formula, we can reconstruct the target spectrum by the superimposition\nbetween 0th-order and high-order terms. Experimental results on two benchmark\ndatasets show that our framework achieves state-of-the-art performance over\nprevious competing baselines in various evaluation metrics. The source code is\navailable at github.com/Andong-Lispeech/TaylorSENet.", "published": "2022-04-30 08:42:21", "link": "http://arxiv.org/abs/2205.00206v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Baselines and Protocols for Household Speaker Recognition", "abstract": "Speaker recognition on household devices, such as smart speakers, features\nseveral challenges: (i) robustness across a vast number of heterogeneous\ndomains (households), (ii) short utterances, (iii) possibly absent speaker\nlabels of the enrollment data (passive enrollment), and (iv) presence of\nunknown persons (guests). While many commercial products exist, there is less\npublished research and no publicly-available evaluation protocols or\nopen-source baselines. Our work serves to bridge this gap by providing an\naccessible evaluation benchmark derived from public resources (VoxCeleb and\nASVspoof 2019 data) along with a preliminary pool of open-source baselines.\nThis includes four algorithms for active enrollment (speaker labels available)\nand one algorithm for passive enrollment.", "published": "2022-04-30 15:04:56", "link": "http://arxiv.org/abs/2205.00288v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
