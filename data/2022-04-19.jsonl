{"title": "Multilingual Syntax-aware Language Modeling through Dependency Tree\n  Conversion", "abstract": "Incorporating stronger syntactic biases into neural language models (LMs) is\na long-standing goal, but research in this area often focuses on modeling\nEnglish text, where constituent treebanks are readily available. Extending\nconstituent tree-based LMs to the multilingual setting, where dependency\ntreebanks are more common, is possible via dependency-to-constituency\nconversion methods. However, this raises the question of which tree formats are\nbest for learning the model, and for which languages. We investigate this\nquestion by training recurrent neural network grammars (RNNGs) using various\nconversion methods, and evaluating them empirically in a multilingual setting.\nWe examine the effect on LM performance across nine conversion methods and five\nlanguages through seven types of syntactic tests. On average, the performance\nof our best model represents a 19 \\% increase in accuracy over the worst choice\nacross all languages. Our best model shows the advantage over\nsequential/overparameterized LMs, suggesting the positive effect of syntax\ninjection in a multilingual setting. Our experiments highlight the importance\nof choosing the right tree formalism, and provide insights into making an\ninformed decision.", "published": "2022-04-19 03:56:28", "link": "http://arxiv.org/abs/2204.08644v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DecBERT: Enhancing the Language Understanding of BERT with Causal\n  Attention Masks", "abstract": "Since 2017, the Transformer-based models play critical roles in various\ndownstream Natural Language Processing tasks. However, a common limitation of\nthe attention mechanism utilized in Transformer Encoder is that it cannot\nautomatically capture the information of word order, so explicit position\nembeddings are generally required to be fed into the target model. In contrast,\nTransformer Decoder with the causal attention masks is naturally sensitive to\nthe word order. In this work, we focus on improving the position encoding\nability of BERT with the causal attention masks. Furthermore, we propose a new\npre-trained language model DecBERT and evaluate it on the GLUE benchmark.\nExperimental results show that (1) the causal attention mask is effective for\nBERT on the language understanding tasks; (2) our DecBERT model without\nposition embeddings achieve comparable performance on the GLUE benchmark; and\n(3) our modification accelerates the pre-training process and DecBERT w/ PE\nachieves better overall performance than the baseline systems when pre-training\nwith the same amount of computational resources.", "published": "2022-04-19 06:12:48", "link": "http://arxiv.org/abs/2204.08688v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Authentic Adversarial Examples beyond Meaning-preserving with\n  Doubly Round-trip Translation", "abstract": "Generating adversarial examples for Neural Machine Translation (NMT) with\nsingle Round-Trip Translation (RTT) has achieved promising results by releasing\nthe meaning-preserving restriction. However, a potential pitfall for this\napproach is that we cannot decide whether the generated examples are\nadversarial to the target NMT model or the auxiliary backward one, as the\nreconstruction error through the RTT can be related to either. To remedy this\nproblem, we propose a new criterion for NMT adversarial examples based on the\nDoubly Round-Trip Translation (DRTT). Specifically, apart from the\nsource-target-source RTT, we also consider the target-source-target one, which\nis utilized to pick out the authentic adversarial examples for the target NMT\nmodel. Additionally, to enhance the robustness of the NMT model, we introduce\nthe masked language models to construct bilingual adversarial pairs based on\nDRTT, which are used to train the NMT model directly. Extensive experiments on\nboth the clean and noisy test sets (including the artificial and natural noise)\nshow that our approach substantially improves the robustness of NMT models.", "published": "2022-04-19 06:15:27", "link": "http://arxiv.org/abs/2204.08689v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing for the Usage of Grammatical Number", "abstract": "A central quest of probing is to uncover how pre-trained models encode a\nlinguistic property within their representations. An encoding, however, might\nbe spurious-i.e., the model might not rely on it when making predictions. In\nthis paper, we try to find encodings that the model actually uses, introducing\na usage-based probing setup. We first choose a behavioral task which cannot be\nsolved without using the linguistic property. Then, we attempt to remove the\nproperty by intervening on the model's representations. We contend that, if an\nencoding is used by the model, its removal should harm the performance on the\nchosen behavioral task. As a case study, we focus on how BERT encodes\ngrammatical number, and on how it uses this encoding to solve the number\nagreement task. Experimentally, we find that BERT relies on a linear encoding\nof grammatical number to produce the correct behavioral output. We also find\nthat BERT uses a separate encoding of grammatical number for nouns and verbs.\nFinally, we identify in which layers information about grammatical number is\ntransferred from a noun to its head verb.", "published": "2022-04-19 11:59:52", "link": "http://arxiv.org/abs/2204.08831v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Impact of Tokenization on Language Models: An Analysis for Turkish", "abstract": "Tokenization is an important text preprocessing step to prepare input tokens\nfor deep language models. WordPiece and BPE are de facto methods employed by\nimportant models, such as BERT and GPT. However, the impact of tokenization can\nbe different for morphologically rich languages, such as Turkic languages,\nwhere many words can be generated by adding prefixes and suffixes. We compare\nfive tokenizers at different granularity levels, i.e. their outputs vary from\nsmallest pieces of characters to the surface form of words, including a\nMorphological-level tokenizer. We train these tokenizers and pretrain\nmedium-sized language models using RoBERTa pretraining procedure on the Turkish\nsplit of the OSCAR corpus. We then fine-tune our models on six downstream\ntasks. Our experiments, supported by statistical tests, reveal that\nMorphological-level tokenizer has challenging performance with de facto\ntokenizers. Furthermore, we find that increasing the vocabulary size improves\nthe performance of Morphological and Word-level tokenizers more than that of de\nfacto tokenizers. The ratio of the number of vocabulary parameters to the total\nnumber of model parameters can be empirically chosen as 20% for de facto\ntokenizers and 40% for other tokenizers to obtain a reasonable trade-off\nbetween model size and performance.", "published": "2022-04-19 12:01:46", "link": "http://arxiv.org/abs/2204.08832v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "I still have Time(s): Extending HeidelTime for German Texts", "abstract": "HeidelTime is one of the most widespread and successful tools for detecting\ntemporal expressions in texts. Since HeidelTime's pattern matching system is\nbased on regular expression, it can be extended in a convenient way. We present\nsuch an extension for the German resources of HeidelTime: HeidelTime-EXT . The\nextension has been brought about by means of observing false negatives within\nreal world texts and various time banks. The gain in coverage is 2.7% or 8.5%,\ndepending on the admitted degree of potential overgeneralization. We describe\nthe development of HeidelTime-EXT, its evaluation on text samples from various\ngenres, and share some linguistic observations. HeidelTime ext can be obtained\nfrom https://github.com/texttechnologylab/heideltime.", "published": "2022-04-19 12:25:47", "link": "http://arxiv.org/abs/2204.08848v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Phrase Retrieval", "abstract": "Cross-lingual retrieval aims to retrieve relevant text across languages.\nCurrent methods typically achieve cross-lingual retrieval by learning\nlanguage-agnostic text representations in word or sentence level. However, how\nto learn phrase representations for cross-lingual phrase retrieval is still an\nopen problem. In this paper, we propose XPR, a cross-lingual phrase retriever\nthat extracts phrase representations from unlabeled example sentences.\nMoreover, we create a large-scale cross-lingual phrase retrieval dataset, which\ncontains 65K bilingual phrase pairs and 4.2M example sentences in 8\nEnglish-centric language pairs. Experimental results show that XPR outperforms\nstate-of-the-art baselines which utilize word-level or sentence-level\nrepresentations. XPR also shows impressive zero-shot transferability that\nenables the model to perform retrieval in an unseen language pair during\ntraining. Our dataset, code, and trained models are publicly available at\nwww.github.com/cwszz/XPR/.", "published": "2022-04-19 13:35:50", "link": "http://arxiv.org/abs/2204.08887v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A survey on improving NLP models with human explanations", "abstract": "Training a model with access to human explanations can improve data\nefficiency and model performance on in- and out-of-domain data. Adding to these\nempirical findings, similarity with the process of human learning makes\nlearning from explanations a promising way to establish a fruitful\nhuman-machine interaction. Several methods have been proposed for improving\nnatural language processing (NLP) models with human explanations, that rely on\ndifferent explanation types and mechanism for integrating these explanations\ninto the learning process. These methods are rarely compared with each other,\nmaking it hard for practitioners to choose the best combination of explanation\ntype and integration mechanism for a specific use-case. In this paper, we give\nan overview of different methods for learning from human explanations, and\ndiscuss different factors that can inform the decision of which method to\nchoose for a specific use-case.", "published": "2022-04-19 13:43:31", "link": "http://arxiv.org/abs/2204.08892v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieval Enhanced Data Augmentation for Question Answering on Privacy\n  Policies", "abstract": "Prior studies in privacy policies frame the question answering (QA) task as\nidentifying the most relevant text segment or a list of sentences from a policy\ndocument given a user query. Existing labeled datasets are heavily imbalanced\n(only a few relevant segments), limiting the QA performance in this domain. In\nthis paper, we develop a data augmentation framework based on ensembling\nretriever models that captures the relevant text segments from unlabeled policy\ndocuments and expand the positive examples in the training set. In addition, to\nimprove the diversity and quality of the augmented data, we leverage multiple\npre-trained language models (LMs) and cascade them with noise reduction filter\nmodels. Using our augmented data on the PrivacyQA benchmark, we elevate the\nexisting baseline by a large margin (10\\% F1) and achieve a new\nstate-of-the-art F1 score of 50\\%. Our ablation studies provide further\ninsights into the effectiveness of our approach.", "published": "2022-04-19 15:45:23", "link": "http://arxiv.org/abs/2204.08952v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Odia Shallow Parser", "abstract": "Shallow parsing is an essential task for many NLP applications like machine\ntranslation, summarization, sentiment analysis, aspect identification and many\nmore. Quality annotated corpora is critical for building accurate shallow\nparsers. Many Indian languages are resource poor with respect to the\navailability of corpora in general. So, this paper is an attempt towards\ncreating quality corpora for shallow parsers. The contribution of this paper is\ntwo folds: creation pos and chunk annotated corpora for Odia and development of\nbaseline systems for pos tagging and chunking in Odia.", "published": "2022-04-19 15:58:30", "link": "http://arxiv.org/abs/2204.08960v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Text Formality: A Study of Text Classification Approaches", "abstract": "Formality is one of the important characteristics of text documents. The\nautomatic detection of the formality level of a text is potentially beneficial\nfor various natural language processing tasks. Before, two large-scale datasets\nwere introduced for multiple languages featuring formality annotation -- GYAFC\nand X-FORMAL. However, they were primarily used for the training of style\ntransfer models. At the same time, the detection of text formality on its own\nmay also be a useful application. This work proposes the first to our knowledge\nsystematic study of formality detection methods based on statistical,\nneural-based, and Transformer-based machine learning methods and delivers the\nbest-performing models for public usage. We conducted three types of\nexperiments -- monolingual, multilingual, and cross-lingual. The study shows\nthe overcome of Char BiLSTM model over Transformer-based ones for the\nmonolingual and multilingual formality classification task, while\nTransformer-based classifiers are more stable to cross-lingual knowledge\ntransfer.", "published": "2022-04-19 16:23:07", "link": "http://arxiv.org/abs/2204.08975v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Benchmark for Automatic Medical Consultation System: Frameworks, Tasks\n  and Datasets", "abstract": "In recent years, interest has arisen in using machine learning to improve the\nefficiency of automatic medical consultation and enhance patient experience. In\nthis article, we propose two frameworks to support automatic medical\nconsultation, namely doctor-patient dialogue understanding and task-oriented\ninteraction. We create a new large medical dialogue dataset with multi-level\nfinegrained annotations and establish five independent tasks, including named\nentity recognition, dialogue act classification, symptom label inference,\nmedical report generation and diagnosis-oriented dialogue policy. We report a\nset of benchmark results for each task, which shows the usability of the\ndataset and sets a baseline for future studies. Both code and data is available\nfrom https://github.com/lemuria-wchen/imcs21.", "published": "2022-04-19 16:43:21", "link": "http://arxiv.org/abs/2204.08997v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Named Entity Recognition for Partially Annotated Datasets", "abstract": "The most common Named Entity Recognizers are usually sequence taggers trained\non fully annotated corpora, i.e. the class of all words for all entities is\nknown. Partially annotated corpora, i.e. some but not all entities of some\ntypes are annotated, are too noisy for training sequence taggers since the same\nentity may be annotated one time with its true type but not another time,\nmisleading the tagger. Therefore, we are comparing three training strategies\nfor partially annotated datasets and an approach to derive new datasets for new\nclasses of entities from Wikipedia without time-consuming manual data\nannotation. In order to properly verify that our data acquisition and training\napproaches are plausible, we manually annotated test datasets for two new\nclasses, namely food and drugs.", "published": "2022-04-19 18:17:09", "link": "http://arxiv.org/abs/2204.09081v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimize_Prime@DravidianLangTech-ACL2022: Emotion Analysis in Tamil", "abstract": "This paper aims to perform an emotion analysis of social media comments in\nTamil. Emotion analysis is the process of identifying the emotional context of\nthe text. In this paper, we present the findings obtained by Team\nOptimize_Prime in the ACL 2022 shared task \"Emotion Analysis in Tamil.\" The\ntask aimed to classify social media comments into categories of emotion like\nJoy, Anger, Trust, Disgust, etc. The task was further divided into two\nsubtasks, one with 11 broad categories of emotions and the other with 31\nspecific categories of emotion. We implemented three different approaches to\ntackle this problem: transformer-based models, Recurrent Neural Networks\n(RNNs), and Ensemble models. XLM-RoBERTa performed the best on the first task\nwith a macro-averaged f1 score of 0.27, while MuRIL provided the best results\non the second task with a macro-averaged f1 score of 0.13.", "published": "2022-04-19 18:47:18", "link": "http://arxiv.org/abs/2204.09087v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PICT@DravidianLangTech-ACL2022: Neural Machine Translation On Dravidian\n  Languages", "abstract": "This paper presents a summary of the findings that we obtained based on the\nshared task on machine translation of Dravidian languages. We stood first in\nthree of the five sub-tasks which were assigned to us for the main shared task.\nWe carried out neural machine translation for the following five language\npairs: Kannada to Tamil, Kannada to Telugu, Kannada to Malayalam, Kannada to\nSanskrit, and Kannada to Tulu. The datasets for each of the five language pairs\nwere used to train various translation models, including Seq2Seq models such as\nLSTM, bidirectional LSTM, Conv2Seq, and training state-of-the-art as\ntransformers from scratch, and fine-tuning already pre-trained models. For some\nmodels involving monolingual corpora, we implemented backtranslation as well.\nThese models' accuracy was later tested with a part of the same dataset using\nBLEU score as an evaluation metric.", "published": "2022-04-19 19:04:05", "link": "http://arxiv.org/abs/2204.09098v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ALBETO and DistilBETO: Lightweight Spanish Language Models", "abstract": "In recent years there have been considerable advances in pre-trained language\nmodels, where non-English language versions have also been made available. Due\nto their increasing use, many lightweight versions of these models (with\nreduced parameters) have also been released to speed up training and inference\ntimes. However, versions of these lighter models (e.g., ALBERT, DistilBERT) for\nlanguages other than English are still scarce. In this paper we present ALBETO\nand DistilBETO, which are versions of ALBERT and DistilBERT pre-trained\nexclusively on Spanish corpora. We train several versions of ALBETO ranging\nfrom 5M to 223M parameters and one of DistilBETO with 67M parameters. We\nevaluate our models in the GLUES benchmark that includes various natural\nlanguage understanding tasks in Spanish. The results show that our lightweight\nmodels achieve competitive results to those of BETO (Spanish-BERT) despite\nhaving fewer parameters. More specifically, our larger ALBETO model outperforms\nall other models on the MLDoc, PAWS-X, XNLI, MLQA, SQAC and XQuAD datasets.\nHowever, BETO remains unbeaten for POS and NER. As a further contribution, all\nmodels are publicly available to the community for future research.", "published": "2022-04-19 22:07:34", "link": "http://arxiv.org/abs/2204.09145v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimize_Prime@DravidianLangTech-ACL2022: Abusive Comment Detection in\n  Tamil", "abstract": "This paper tries to address the problem of abusive comment detection in\nlow-resource indic languages. Abusive comments are statements that are\noffensive to a person or a group of people. These comments are targeted toward\nindividuals belonging to specific ethnicities, genders, caste, race, sexuality,\netc. Abusive Comment Detection is a significant problem, especially with the\nrecent rise in social media users. This paper presents the approach used by our\nteam - Optimize_Prime, in the ACL 2022 shared task \"Abusive Comment Detection\nin Tamil.\" This task detects and classifies YouTube comments in Tamil and\nTamil- English Codemixed format into multiple categories. We have used three\nmethods to optimize our results: Ensemble models, Recurrent Neural Networks,\nand Transformers. In the Tamil data, MuRIL and XLM-RoBERTA were our best\nperforming models with a macro-averaged f1 score of 0.43. Furthermore, for the\nCode-mixed data, MuRIL and M-BERT provided sub-lime results, with a\nmacro-averaged f1 score of 0.45.", "published": "2022-04-19 18:55:18", "link": "http://arxiv.org/abs/2204.09675v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LitMC-BERT: transformer-based multi-label classification of biomedical\n  literature with an application on COVID-19 literature curation", "abstract": "The rapid growth of biomedical literature poses a significant challenge for\ncuration and interpretation. This has become more evident during the COVID-19\npandemic. LitCovid, a literature database of COVID-19 related papers in PubMed,\nhas accumulated over 180,000 articles with millions of accesses. Approximately\n10,000 new articles are added to LitCovid every month. A main curation task in\nLitCovid is topic annotation where an article is assigned with up to eight\ntopics, e.g., Treatment and Diagnosis. The annotated topics have been widely\nused both in LitCovid (e.g., accounting for ~18% of total uses) and downstream\nstudies such as network generation. However, it has been a primary curation\nbottleneck due to the nature of the task and the rapid literature growth. This\nstudy proposes LITMC-BERT, a transformer-based multi-label classification\nmethod in biomedical literature. It uses a shared transformer backbone for all\nthe labels while also captures label-specific features and the correlations\nbetween label pairs. We compare LITMC-BERT with three baseline models on two\ndatasets. Its micro-F1 and instance-based F1 are 5% and 4% higher than the\ncurrent best results, respectively, and only requires ~18% of the inference\ntime than the Binary BERT baseline. The related datasets and models are\navailable via https://github.com/ncbi/ml-transformer.", "published": "2022-04-19 04:03:45", "link": "http://arxiv.org/abs/2204.08649v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mono vs Multilingual BERT for Hate Speech Detection and Text\n  Classification: A Case Study in Marathi", "abstract": "Transformers are the most eminent architectures used for a vast range of\nNatural Language Processing tasks. These models are pre-trained over a large\ntext corpus and are meant to serve state-of-the-art results over tasks like\ntext classification. In this work, we conduct a comparative study between\nmonolingual and multilingual BERT models. We focus on the Marathi language and\nevaluate the models on the datasets for hate speech detection, sentiment\nanalysis and simple text classification in Marathi. We use standard\nmultilingual models such as mBERT, indicBERT and xlm-RoBERTa and compare with\nMahaBERT, MahaALBERT and MahaRoBERTa, the monolingual models for Marathi. We\nfurther show that Marathi monolingual models outperform the multilingual BERT\nvariants on five different downstream fine-tuning experiments. We also evaluate\nsentence embeddings from these models by freezing the BERT encoder layers. We\nshow that monolingual MahaBERT based models provide rich representations as\ncompared to sentence embeddings from multi-lingual counterparts. However, we\nobserve that these embeddings are not generic enough and do not work well on\nout of domain social media datasets. We consider two Marathi hate speech\ndatasets L3Cube-MahaHate, HASOC-2021, a Marathi sentiment classification\ndataset L3Cube-MahaSent, and Marathi Headline, Articles classification\ndatasets.", "published": "2022-04-19 05:07:58", "link": "http://arxiv.org/abs/2204.08669v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IndicXNLI: Evaluating Multilingual Inference for Indian Languages", "abstract": "While Indic NLP has made rapid advances recently in terms of the availability\nof corpora and pre-trained models, benchmark datasets on standard NLU tasks are\nlimited. To this end, we introduce IndicXNLI, an NLI dataset for 11 Indic\nlanguages. It has been created by high-quality machine translation of the\noriginal English XNLI dataset and our analysis attests to the quality of\nIndicXNLI. By finetuning different pre-trained LMs on this IndicXNLI, we\nanalyze various cross-lingual transfer techniques with respect to the impact of\nthe choice of language models, languages, multi-linguality, mix-language input,\netc. These experiments provide us with useful insights into the behaviour of\npre-trained models for a diverse set of languages.", "published": "2022-04-19 09:49:00", "link": "http://arxiv.org/abs/2204.08776v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Where Was COVID-19 First Discovered? Designing a Question-Answering\n  System for Pandemic Situations", "abstract": "The COVID-19 pandemic is accompanied by a massive \"infodemic\" that makes it\nhard to identify concise and credible information for COVID-19-related\nquestions, like incubation time, infection rates, or the effectiveness of\nvaccines. As a novel solution, our paper is concerned with designing a\nquestion-answering system based on modern technologies from natural language\nprocessing to overcome information overload and misinformation in pandemic\nsituations. To carry out our research, we followed a design science research\napproach and applied Ingwersen's cognitive model of information retrieval\ninteraction to inform our design process from a socio-technical lens. On this\nbasis, we derived prescriptive design knowledge in terms of design requirements\nand design principles, which we translated into the construction of a\nprototypical instantiation. Our implementation is based on the comprehensive\nCORD-19 dataset, and we demonstrate our artifact's usefulness by evaluating its\nanswer quality based on a sample of COVID-19 questions labeled by biomedical\nexperts.", "published": "2022-04-19 10:15:51", "link": "http://arxiv.org/abs/2204.08787v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding Toxicity Triggers on Reddit in the Context of Singapore", "abstract": "While the contagious nature of online toxicity sparked increasing interest in\nits early detection and prevention, most of the literature focuses on the\nWestern world. In this work, we demonstrate that 1) it is possible to detect\ntoxicity triggers in an Asian online community, and 2) toxicity triggers can be\nstrikingly different between Western and Eastern contexts.", "published": "2022-04-19 11:06:47", "link": "http://arxiv.org/abs/2204.08806v1", "categories": ["cs.CY", "cs.CL", "J.4; K.4"], "primary_category": "cs.CY"}
{"title": "SmartSales: Sales Script Extraction and Analysis from Sales Chatlog", "abstract": "In modern sales applications, automatic script extraction and management\ngreatly decrease the need for human labor to collect the winning sales scripts,\nwhich largely boost the success rate for sales and can be shared across the\nsales teams. In this work, we present the SmartSales system to serve both the\nsales representatives and managers to attain the sales insights from the\nlarge-scale sales chatlog. SmartSales consists of three modules: 1) Customer\nfrequently asked questions (FAQ) extraction aims to enrich the FAQ knowledge\nbase by harvesting high quality customer question-answer pairs from the\nchatlog. 2) Customer objection response assists the salespeople to figure out\nthe typical customer objections and corresponding winning sales scripts, as\nwell as search for proper sales responses for a certain customer objection. 3)\nSales manager dashboard helps sales managers to monitor whether a specific\nsales representative or team follows the sales standard operating procedures\n(SOP). The proposed prototype system is empowered by the state-of-the-art\nconversational intelligence techniques and has been running on the Tencent\nCloud to serve the sales teams from several different areas.", "published": "2022-04-19 11:18:42", "link": "http://arxiv.org/abs/2204.08811v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ATP: AMRize Then Parse! Enhancing AMR Parsing with PseudoAMRs", "abstract": "As Abstract Meaning Representation (AMR) implicitly involves compound\nsemantic annotations, we hypothesize auxiliary tasks which are semantically or\nformally related can better enhance AMR parsing. We find that 1) Semantic role\nlabeling (SRL) and dependency parsing (DP), would bring more performance gain\nthan other tasks e.g. MT and summarization in the text-to-AMR transition even\nwith much less data. 2) To make a better fit for AMR, data from auxiliary tasks\nshould be properly \"AMRized\" to PseudoAMR before training. Knowledge from\nshallow level parsing tasks can be better transferred to AMR Parsing with\nstructure transform. 3) Intermediate-task learning is a better paradigm to\nintroduce auxiliary tasks to AMR parsing, compared to multitask learning. From\nan empirical perspective, we propose a principled method to involve auxiliary\ntasks to boost AMR parsing. Extensive experiments show that our method achieves\nnew state-of-the-art performance on different benchmarks especially in\ntopology-related scores.", "published": "2022-04-19 13:15:59", "link": "http://arxiv.org/abs/2204.08875v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What Makes Instruction Learning Hard? An Investigation and a New\n  Challenge in a Synthetic Environment", "abstract": "The instruction learning paradigm -- where a model learns to perform new\ntasks from task descriptions alone -- has become popular in general-purpose\nmodel research. The capabilities of large transformer models as instruction\nlearners, however, remain poorly understood. We use a controlled synthetic\nenvironment to characterize such capabilities. Specifically, we use the task of\ndeciding whether a given string matches a regular expression (viewed as an\ninstruction) to identify properties of tasks, instructions, and instances that\nmake instruction learning challenging. For instance, we find that our model, a\nfine-tuned T5-based text2text transformer, struggles with large regular\nlanguages, suggesting that less precise instructions are challenging for\nmodels. Additionally, instruction executions that require tracking longer\ncontexts of prior steps are also more difficult. We use our findings to\nsystematically construct a challenging instruction learning dataset, which we\ncall Hard RegSet. Fine-tuning on Hard RegSet, our large transformer learns to\ncorrectly interpret only 65.6% of test instructions (with at least 90%\naccuracy), and 11%-24% of the instructions in out-of-distribution\ngeneralization settings. We propose Hard RegSet as a challenging instruction\nlearning task, and a controlled environment for studying instruction learning.", "published": "2022-04-19 22:11:47", "link": "http://arxiv.org/abs/2204.09148v2", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "DialoKG: Knowledge-Structure Aware Task-Oriented Dialogue Generation", "abstract": "Task-oriented dialogue generation is challenging since the underlying\nknowledge is often dynamic and effectively incorporating knowledge into the\nlearning process is hard. It is particularly challenging to generate both\nhuman-like and informative responses in this setting. Recent research primarily\nfocused on various knowledge distillation methods where the underlying\nrelationship between the facts in a knowledge base is not effectively captured.\nIn this paper, we go one step further and demonstrate how the structural\ninformation of a knowledge graph can improve the system's inference\ncapabilities. Specifically, we propose DialoKG, a novel task-oriented dialogue\nsystem that effectively incorporates knowledge into a language model. Our\nproposed system views relational knowledge as a knowledge graph and introduces\n(1) a structure-aware knowledge embedding technique, and (2) a knowledge\ngraph-weighted attention masking strategy to facilitate the system selecting\nrelevant information during the dialogue generation. An empirical evaluation\ndemonstrates the effectiveness of DialoKG over state-of-the-art methods on\nseveral standard benchmark datasets.", "published": "2022-04-19 22:26:18", "link": "http://arxiv.org/abs/2204.09149v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multimodal Hate Speech Detection from Bengali Memes and Texts", "abstract": "Numerous machine learning (ML) and deep learning (DL)-based approaches have\nbeen proposed to utilize textual data from social media for anti-social\nbehavior analysis like cyberbullying, fake news detection, and identification\nof hate speech mainly for highly-resourced languages such as English. However,\ndespite having a lot of diversity and millions of native speakers, some\nlanguages like Bengali are under-resourced, which is due to a lack of\ncomputational resources for natural language processing (NLP). Similar to other\nlanguages, Bengali social media contents also include images along with texts\n(e.g., multimodal memes are posted by embedding short texts into images on\nFacebook). Therefore, only the textual data is not enough to judge them since\nimages might give extra context to make a proper judgement. This paper is about\nhate speech detection from multimodal Bengali memes and texts. We prepared the\nonly multimodal hate speech dataset for-a-kind of problem for Bengali, which we\nuse to train state-of-the-art neural architectures (e.g., Bi-LSTM/Conv-LSTM\nwith word embeddings, ConvNets + pre-trained language models, e.g., monolingual\nBangla BERT, multilingual BERT-cased/uncased, and XLM-RoBERTa) to jointly\nanalyze textual and visual information for hate speech detection. Conv-LSTM and\nXLM-RoBERTa models performed best for texts, yielding F1 scores of 0.78 and\n0.82, respectively. As of memes, ResNet-152 and DenseNet-161 models yield F1\nscores of 0.78 and 0.79, respectively. As for multimodal fusion, XLM-RoBERTa +\nDenseNet-161 performed the best, yielding an F1 score of 0.83. Our study\nsuggests that text modality is most useful for hate speech detection, while\nmemes are moderately useful.", "published": "2022-04-19 11:15:25", "link": "http://arxiv.org/abs/2204.10196v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Numerical Reasoning to Extract Phenotypes from Clinical\n  Text by Leveraging External Knowledge", "abstract": "Extracting phenotypes from clinical text has been shown to be useful for a\nvariety of clinical use cases such as identifying patients with rare diseases.\nHowever, reasoning with numerical values remains challenging for phenotyping in\nclinical text, for example, temperature 102F representing Fever. Current\nstate-of-the-art phenotyping models are able to detect general phenotypes, but\nperform poorly when they detect phenotypes requiring numerical reasoning. We\npresent a novel unsupervised methodology leveraging external knowledge and\ncontextualized word embeddings from ClinicalBERT for numerical reasoning in a\nvariety of phenotypic contexts. Comparing against unsupervised benchmarks, it\nshows a substantial performance improvement with absolute gains on generalized\nRecall and F1 scores up to 79% and 71%, respectively. In the supervised\nsetting, it also surpasses the performance of alternative approaches with\nabsolute gains on generalized Recall and F1 scores up to 70% and 44%,\nrespectively.", "published": "2022-04-19 12:44:32", "link": "http://arxiv.org/abs/2204.10202v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sharing and Caring: Creating a Culture of Constructive Criticism in\n  Computational Legal Studies", "abstract": "We introduce seven foundational principles for creating a culture of\nconstructive criticism in computational legal studies. Beginning by challenging\nthe current perception of papers as the primary scholarly output, we call for a\nmore comprehensive interpretation of publications. We then suggest to make\nthese publications computationally reproducible, releasing all of the data and\nall of the code all of the time, on time, and in the most functioning form\npossible. Subsequently, we invite constructive criticism in all phases of the\npublication life cycle. We posit that our proposals will help form our field,\nand float the idea of marking this maturity by the creation of a modern\nflagship publication outlet for computational legal studies.", "published": "2022-04-19 08:27:01", "link": "http://arxiv.org/abs/2205.01071v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "On The Cross-Modal Transfer from Natural Language to Code through\n  Adapter Modules", "abstract": "Pre-trained neural Language Models (PTLM), such as CodeBERT, are recently\nused in software engineering as models pre-trained on large source code\ncorpora. Their knowledge is transferred to downstream tasks (e.g. code clone\ndetection) via fine-tuning. In natural language processing (NLP), other\nalternatives for transferring the knowledge of PTLMs are explored through using\nadapters, compact, parameter efficient modules inserted in the layers of the\nPTLM. Although adapters are known to facilitate adapting to many downstream\ntasks compared to fine-tuning the model that require retraining all of the\nmodels' parameters -- which owes to the adapters' plug and play nature and\nbeing parameter efficient -- their usage in software engineering is not\nexplored.\n  Here, we explore the knowledge transfer using adapters and based on the\nNaturalness Hypothesis proposed by Hindle et. al \\cite{hindle2016naturalness}.\nThus, studying the bimodality of adapters for two tasks of cloze test and code\nclone detection, compared to their benchmarks from the CodeXGLUE platform.\nThese adapters are trained using programming languages and are inserted in a\nPTLM that is pre-trained on English corpora (N-PTLM). Three programming\nlanguages, C/C++, Python, and Java, are studied along with extensive\nexperiments on the best setup used for adapters. Improving the results of the\nN-PTLM confirms the success of the adapters in knowledge transfer to software\nengineering, which sometimes are in par with or exceed the results of a PTLM\ntrained on source code; while being more efficient in terms of the number of\nparameters, memory usage, and inference time. Our results can open new\ndirections to build smaller models for more software engineering tasks. We open\nsource all the scripts and the trained adapters.", "published": "2022-04-19 04:18:02", "link": "http://arxiv.org/abs/2204.08653v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Table-based Fact Verification with Self-adaptive Mixture of Experts", "abstract": "The table-based fact verification task has recently gained widespread\nattention and yet remains to be a very challenging problem. It inherently\nrequires informative reasoning over natural language together with different\nnumerical and logical reasoning on tables (e.g., count, superlative,\ncomparative). Considering that, we exploit mixture-of-experts and present in\nthis paper a new method: Self-adaptive Mixture-of-Experts Network (SaMoE).\nSpecifically, we have developed a mixture-of-experts neural network to\nrecognize and execute different types of reasoning -- the network is composed\nof multiple experts, each handling a specific part of the semantics for\nreasoning, whereas a management module is applied to decide the contribution of\neach expert network to the verification result. A self-adaptive method is\ndeveloped to teach the management module combining results of different experts\nmore efficiently without external knowledge. The experimental results\nillustrate that our framework achieves 85.1% accuracy on the benchmark dataset\nTabFact, comparable with the previous state-of-the-art models. We hope our\nframework can serve as a new baseline for table-based verification. Our code is\navailable at https://github.com/THUMLP/SaMoE.", "published": "2022-04-19 08:47:39", "link": "http://arxiv.org/abs/2204.08753v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented\n  Visual Models", "abstract": "Learning visual representations from natural language supervision has\nrecently shown great promise in a number of pioneering works. In general, these\nlanguage-augmented visual models demonstrate strong transferability to a\nvariety of datasets and tasks. However, it remains challenging to evaluate the\ntransferablity of these models due to the lack of easy-to-use evaluation\ntoolkits and public benchmarks. To tackle this, we build ELEVATER (Evaluation\nof Language-augmented Visual Task-level Transfer), the first benchmark and\ntoolkit for evaluating(pre-trained) language-augmented visual models. ELEVATER\nis composed of three components. (i) Datasets. As downstream evaluation suites,\nit consists of 20 image classification datasets and 35 object detection\ndatasets, each of which is augmented with external knowledge. (ii) Toolkit. An\nautomatic hyper-parameter tuning toolkit is developed to facilitate model\nevaluation on downstream tasks. (iii) Metrics. A variety of evaluation metrics\nare used to measure sample-efficiency (zero-shot and few-shot) and\nparameter-efficiency (linear probing and full model fine-tuning). ELEVATER is a\nplatform for Computer Vision in the Wild (CVinW), and is publicly released at\nat https://computer-vision-in-the-wild.github.io/ELEVATER/", "published": "2022-04-19 10:23:42", "link": "http://arxiv.org/abs/2204.08790v6", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Blockwise Streaming Transformer for Spoken Language Understanding and\n  Simultaneous Speech Translation", "abstract": "Although Transformers have gained success in several speech processing tasks\nlike spoken language understanding (SLU) and speech translation (ST), achieving\nonline processing while keeping competitive performance is still essential for\nreal-world interaction. In this paper, we take the first step on streaming SLU\nand simultaneous ST using a blockwise streaming Transformer, which is based on\ncontextual block processing and blockwise synchronous beam search. Furthermore,\nwe design an automatic speech recognition (ASR)-based intermediate loss\nregularization for the streaming SLU task to improve the classification\nperformance further. As for the simultaneous ST task, we propose a\ncross-lingual encoding method, which employs a CTC branch optimized with target\nlanguage translations. In addition, the CTC translation output is also used to\nrefine the search space with CTC prefix score, achieving joint CTC/attention\nsimultaneous translation for the first time. Experiments for SLU are conducted\non FSC and SLURP corpora, while the ST task is evaluated on Fisher-CallHome\nSpanish and MuST-C En-De corpora. Experimental results show that the blockwise\nstreaming Transformer achieves competitive results compared to offline models,\nespecially with our proposed methods that further yield a 2.4% accuracy gain on\nthe SLU task and a 4.3 BLEU gain on the ST task over streaming baselines.", "published": "2022-04-19 14:38:40", "link": "http://arxiv.org/abs/2204.08920v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex", "abstract": "CodexDB is an SQL processing engine whose internals can be customized via\nnatural language instructions. CodexDB is based on OpenAI's GPT-3 Codex model\nwhich translates text into code. It is a framework on top of GPT-3 Codex that\ndecomposes complex SQL queries into a series of simple processing steps,\ndescribed in natural language. Processing steps are enriched with user-provided\ninstructions and descriptions of database properties. Codex translates the\nresulting text into query processing code. An early prototype of CodexDB is\nable to generate correct code for a majority of queries of the WikiSQL\nbenchmark and can be customized in various ways.", "published": "2022-04-19 15:19:35", "link": "http://arxiv.org/abs/2204.08941v1", "categories": ["cs.DB", "cs.CL", "cs.LG", "H.2.4"], "primary_category": "cs.DB"}
{"title": "On the Locality of Attention in Direct Speech Translation", "abstract": "Transformers have achieved state-of-the-art results across multiple NLP\ntasks. However, the self-attention mechanism complexity scales quadratically\nwith the sequence length, creating an obstacle for tasks involving long\nsequences, like in the speech domain. In this paper, we discuss the usefulness\nof self-attention for Direct Speech Translation. First, we analyze the\nlayer-wise token contributions in the self-attention of the encoder, unveiling\nlocal diagonal patterns. To prove that some attention weights are avoidable, we\npropose to substitute the standard self-attention with a local efficient one,\nsetting the amount of context used based on the results of the analysis. With\nthis approach, our model matches the baseline performance, and improves the\nefficiency by skipping the computation of those weights that standard attention\ndiscards.", "published": "2022-04-19 17:43:37", "link": "http://arxiv.org/abs/2204.09028v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multi-hop Question Answering", "abstract": "The task of Question Answering (QA) has attracted significant research\ninterest for long. Its relevance to language understanding and knowledge\nretrieval tasks, along with the simple setting makes the task of QA crucial for\nstrong AI systems. Recent success on simple QA tasks has shifted the focus to\nmore complex settings. Among these, Multi-Hop QA (MHQA) is one of the most\nresearched tasks over the recent years. In broad terms, MHQA is the task of\nanswering natural language questions that involve extracting and combining\nmultiple pieces of information and doing multiple steps of reasoning. An\nexample of a multi-hop question would be \"The Argentine PGA Championship record\nholder has won how many tournaments worldwide?\". Answering the question would\nneed two pieces of information: \"Who is the record holder for Argentine PGA\nChampionship tournaments?\" and \"How many tournaments did [Answer of Sub Q1]\nwin?\". The ability to answer multi-hop questions and perform multi step\nreasoning can significantly improve the utility of NLP systems. Consequently,\nthe field has seen a surge with high quality datasets, models and evaluation\nstrategies. The notion of 'multiple hops' is somewhat abstract which results in\na large variety of tasks that require multi-hop reasoning. This leads to\ndifferent datasets and models that differ significantly from each other and\nmakes the field challenging to generalize and survey. We aim to provide a\ngeneral and formal definition of the MHQA task, and organize and summarize\nexisting MHQA frameworks. We also outline some best practices for building MHQA\ndatasets. This book provides a systematic and thorough introduction as well as\nthe structuring of the existing attempts to this highly interesting, yet quite\nchallenging task.", "published": "2022-04-19 21:55:18", "link": "http://arxiv.org/abs/2204.09140v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Social Media Sentiment Analysis for Cryptocurrency Market Prediction", "abstract": "In this paper, we explore the usability of different natural language\nprocessing models for the sentiment analysis of social media applied to\nfinancial market prediction, using the cryptocurrency domain as a reference. We\nstudy how the different sentiment metrics are correlated with the price\nmovements of Bitcoin. For this purpose, we explore different methods to\ncalculate the sentiment metrics from a text finding most of them not very\naccurate for this prediction task. We find that one of the models outperforms\nmore than 20 other public ones and makes it possible to fine-tune it\nefficiently given its interpretable nature. Thus we confirm that interpretable\nartificial intelligence and natural language processing methods might be more\nvaluable practically than non-explainable and non-interpretable ones. In the\nend, we analyse potential causal connections between the different sentiment\nmetrics and the price movements.", "published": "2022-04-19 03:27:29", "link": "http://arxiv.org/abs/2204.10185v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Self Supervised Adversarial Domain Adaptation for Cross-Corpus and\n  Cross-Language Speech Emotion Recognition", "abstract": "Despite the recent advancement in speech emotion recognition (SER) within a\nsingle corpus setting, the performance of these SER systems degrades\nsignificantly for cross-corpus and cross-language scenarios. The key reason is\nthe lack of generalisation in SER systems towards unseen conditions, which\ncauses them to perform poorly in cross-corpus and cross-language settings.\nRecent studies focus on utilising adversarial methods to learn domain\ngeneralised representation for improving cross-corpus and cross-language SER to\naddress this issue. However, many of these methods only focus on cross-corpus\nSER without addressing the cross-language SER performance degradation due to a\nlarger domain gap between source and target language data. This contribution\nproposes an adversarial dual discriminator (ADDi) network that uses the\nthree-players adversarial game to learn generalised representations without\nrequiring any target data labels. We also introduce a self-supervised ADDi\n(sADDi) network that utilises self-supervised pre-training with unlabelled\ndata. We propose synthetic data generation as a pretext task in sADDi, enabling\nthe network to produce emotionally discriminative and domain invariant\nrepresentations and providing complementary synthetic data to augment the\nsystem. The proposed model is rigorously evaluated using five publicly\navailable datasets in three languages and compared with multiple studies on\ncross-corpus and cross-language SER. Experimental results demonstrate that the\nproposed model achieves improved performance compared to the state-of-the-art\nmethods.", "published": "2022-04-19 02:57:56", "link": "http://arxiv.org/abs/2204.08625v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-Visual Wake Word Spotting System For MISP Challenge 2021", "abstract": "This paper presents the details of our system designed for the Task 1 of\nMultimodal Information Based Speech Processing (MISP) Challenge 2021. The\npurpose of Task 1 is to leverage both audio and video information to improve\nthe environmental robustness of far-field wake word spotting. In the proposed\nsystem, firstly, we take advantage of speech enhancement algorithms such as\nbeamforming and weighted prediction error (WPE) to address the multi-microphone\nconversational audio. Secondly, several data augmentation techniques are\napplied to simulate a more realistic far-field scenario. For the video\ninformation, the provided region of interest (ROI) is used to obtain visual\nrepresentation. Then the multi-layer CNN is proposed to learn audio and visual\nrepresentations, and these representations are fed into our two-branch\nattention-based network which can be employed for fusion, such as transformer\nand conformed. The focal loss is used to fine-tune the model and improve the\nperformance significantly. Finally, multiple trained models are integrated by\ncasting vote to achieve our final 0.091 score.", "published": "2022-04-19 06:08:08", "link": "http://arxiv.org/abs/2204.08686v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Investigation of Monotonic Transducers for Large-Scale Automatic\n  Speech Recognition", "abstract": "The two most popular loss functions for streaming end-to-end automatic speech\nrecognition (ASR) are RNN-Transducer (RNN-T) and connectionist temporal\nclassification (CTC). Between these two loss types we can classify the\nmonotonic RNN-T (MonoRNN-T) and the recently proposed CTC-like Transducer\n(CTC-T). Monotonic transducers have a few advantages. First, RNN-T can suffer\nfrom runaway hallucination, where a model keeps emitting non-blank symbols\nwithout advancing in time. Secondly, monotonic transducers consume exactly one\nmodel score per time step and are therefore more compatible with traditional\nFST-based ASR decoders. However, the MonoRNN-T so far has been found to have\nworse accuracy than RNN-T. It does not have to be that way: By regularizing the\ntraining via joint LAS training or parameter initialization from RNN-T, both\nMonoRNN-T and CTC-T perform as well or better than RNN-T. This is demonstrated\nfor LibriSpeech and for a large-scale in-house data set.", "published": "2022-04-19 12:51:30", "link": "http://arxiv.org/abs/2204.08858v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Time Domain Adversarial Voice Conversion for ADD 2022", "abstract": "In this paper, we describe our speech generation system for the first Audio\nDeep Synthesis Detection Challenge (ADD 2022). Firstly, we build an any-to-many\nvoice conversion (VC) system to convert source speech with arbitrary language\ncontent into the target speaker%u2019s fake speech. Then the converted speech\ngenerated from VC is post-processed in the time domain to improve the deception\nability. The experimental results show that our system has adversarial ability\nagainst anti-spoofing detectors with a little compromise in audio quality and\nspeaker similarity. This system ranks top in Track 3.1 in the ADD 2022, showing\nthat our method could also gain good generalization ability against different\ndetectors.", "published": "2022-04-19 06:22:12", "link": "http://arxiv.org/abs/2204.08692v2", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio Deep Fake Detection System with Neural Stitching for ADD 2022", "abstract": "This paper describes our best system and methodology for ADD 2022: The First\nAudio Deep Synthesis Detection Challenge\\cite{Yi2022ADD}. The very same system\nwas used for both two rounds of evaluation in Track 3.2 with a similar training\nmethodology. The first round of Track 3.2 data is generated from\nText-to-Speech(TTS) or voice conversion (VC) algorithms, while the second round\nof data consists of generated fake audio from other participants in Track 3.1,\naiming to spoof our systems. Our systems use a standard 34-layer ResNet, with\nmulti-head attention pooling \\cite{india2019self} to learn the discriminative\nembedding for fake audio and spoof detection. We further utilize neural\nstitching to boost the model's generalization capability in order to perform\nequally well in different tasks, and more details will be explained in the\nfollowing sessions. The experiments show that our proposed method outperforms\nall other systems with a 10.1% equal error rate(EER) in Track 3.2.", "published": "2022-04-19 07:45:46", "link": "http://arxiv.org/abs/2204.08720v2", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Dereverberation with A Reverberation Time Shortening Target", "abstract": "This work proposes a new learning target based on reverberation time\nshortening (RTS) for speech dereverberation. The learning target for\ndereverberation is usually set as the direct-path speech or optionally with\nsome early reflections. This type of target suddenly truncates the\nreverberation, and thus it may not be suitable for network training. The\nproposed RTS target suppresses reverberation and meanwhile maintains the\nexponential decaying property of reverberation, which will ease the network\ntraining, and thus reduce signal distortion caused by the prediction error.\nMoreover, this work experimentally study to adapt our previously proposed\nFullSubNet speech denoising network to speech dereverberation. Experiments show\nthat RTS is a more suitable learning target than direct-path speech and early\nreflections, in terms of better suppressing reverberation and signal\ndistortion. FullSubNet is able to achieve outstanding dereverberation\nperformance.", "published": "2022-04-19 09:15:25", "link": "http://arxiv.org/abs/2204.08765v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A Convolutional-Attentional Neural Framework for Structure-Aware\n  Performance-Score Synchronization", "abstract": "Performance-score synchronization is an integral task in signal processing,\nwhich entails generating an accurate mapping between an audio recording of a\nperformance and the corresponding musical score. Traditional synchronization\nmethods compute alignment using knowledge-driven and stochastic approaches, and\nare typically unable to generalize well to different domains and modalities. We\npresent a novel data-driven method for structure-aware performance-score\nsynchronization. We propose a convolutional-attentional architecture trained\nwith a custom loss based on time-series divergence. We conduct experiments for\nthe audio-to-MIDI and audio-to-image alignment tasks pertained to different\nscore modalities. We validate the effectiveness of our method via ablation\nstudies and comparisons with state-of-the-art alignment approaches. We\ndemonstrate that our approach outperforms previous synchronization methods for\na variety of test settings across score modalities and acoustic conditions. Our\nmethod is also robust to structural differences between the performance and\nscore sequences, which is a common limitation of standard alignment approaches.", "published": "2022-04-19 11:41:21", "link": "http://arxiv.org/abs/2204.08822v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Disappeared Command: Spoofing Attack On Automatic Speech Recognition\n  Systems with Sound Masking", "abstract": "The development of deep learning technology has greatly promoted the\nperformance improvement of automatic speech recognition (ASR) technology, which\nhas demonstrated an ability comparable to human hearing in many tasks. Voice\ninterfaces are becoming more and more widely used as input for many\napplications and smart devices. However, existing research has shown that DNN\nis easily disturbed by slight disturbances and makes false recognition, which\nis extremely dangerous for intelligent voice applications controlled by voice.", "published": "2022-04-19 16:26:34", "link": "http://arxiv.org/abs/2204.08977v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music Source Separation with Generative Flow", "abstract": "Fully-supervised models for source separation are trained on parallel\nmixture-source data and are currently state-of-the-art. However, such parallel\ndata is often difficult to obtain, and it is cumbersome to adapt trained models\nto mixtures with new sources. Source-only supervised models, in contrast, only\nrequire individual source data for training. In this paper, we first leverage\nflow-based generators to train individual music source priors and then use\nthese models, along with likelihood-based objectives, to separate music\nmixtures. We show that in singing voice separation and music separation tasks,\nour proposed method is competitive with a fully-supervised approach. We also\ndemonstrate that we can flexibly add new types of sources, whereas\nfully-supervised approaches would require retraining of the entire model.", "published": "2022-04-19 18:06:21", "link": "http://arxiv.org/abs/2204.09079v4", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
