{"title": "Discourse Relation Embeddings: Representing the Relations between\n  Discourse Segments in Social Media", "abstract": "Discourse relations are typically modeled as a discrete class that\ncharacterizes the relation between segments of text (e.g. causal explanations,\nexpansions). However, such predefined discrete classes limits the universe of\npotential relationships and their nuanced differences. Analogous to contextual\nword embeddings, we propose representing discourse relations as points in high\ndimensional continuous space. However, unlike words, discourse relations often\nhave no surface form (relations are between two segments, often with no word or\nphrase in that gap) which presents a challenge for existing embedding\ntechniques. We present a novel method for automatically creating discourse\nrelation embeddings (DiscRE), addressing the embedding challenge through a\nweakly supervised, multitask approach to learn diverse and nuanced relations\nbetween discourse segments in social media. Results show DiscRE can: (1) obtain\nthe best performance on Twitter discourse relation classification task (macro\nF1=0.76) (2) improve the state of the art in social media causality prediction\n(from F1=.79 to .81), (3) perform beyond modern sentence and contextual word\nembeddings at traditional discourse relation classification, and (4) capture\nnovel nuanced relations (e.g. relations semantically at the intersection of\ncausal explanations and counterfactuals).", "published": "2021-05-04 05:58:27", "link": "http://arxiv.org/abs/2105.01306v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inferring the Reader: Guiding Automated Story Generation with\n  Commonsense Reasoning", "abstract": "Transformer-based language model approaches to automated story generation\ncurrently provide state-of-the-art results. However, they still suffer from\nplot incoherence when generating narratives over time, and critically lack\nbasic commonsense reasoning. Furthermore, existing methods generally focus only\non single-character stories, or fail to track characters at all. To improve the\ncoherence of generated narratives and to expand the scope of character-centric\nnarrative generation, we introduce Commonsense-inference Augmented neural\nStoryTelling (CAST), a framework for introducing commonsense reasoning into the\ngeneration process with the option to model the interaction between multiple\ncharacters. We find that our CAST method produces significantly more coherent,\non-topic, enjoyable and fluent stories than existing models in both the\nsingle-character and two-character settings in three storytelling domains.", "published": "2021-05-04 06:40:33", "link": "http://arxiv.org/abs/2105.01311v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversational Machine Reading Comprehension for Vietnamese Healthcare\n  Texts", "abstract": "Machine reading comprehension (MRC) is a sub-field in natural language\nprocessing that aims to assist computers understand unstructured texts and then\nanswer questions related to them. In practice, the conversation is an essential\nway to communicate and transfer information. To help machines understand\nconversation texts, we present UIT-ViCoQA, a new corpus for conversational\nmachine reading comprehension in the Vietnamese language. This corpus consists\nof 10,000 questions with answers over 2,000 conversations about health news\narticles. Then, we evaluate several baseline approaches for conversational\nmachine comprehension on the UIT-ViCoQA corpus. The best model obtains an F1\nscore of 45.27%, which is 30.91 points behind human performance (76.18%),\nindicating that there is ample room for improvement. Our dataset is available\nat our website: http://nlp.uit.edu.vn/datasets/ for research purposes.", "published": "2021-05-04 14:50:39", "link": "http://arxiv.org/abs/2105.01542v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Augmentation by Concatenation for Low-Resource Translation: A\n  Mystery and a Solution", "abstract": "In this paper, we investigate the driving factors behind concatenation, a\nsimple but effective data augmentation method for low-resource neural machine\ntranslation. Our experiments suggest that discourse context is unlikely the\ncause for the improvement of about +1 BLEU across four language pairs. Instead,\nwe demonstrate that the improvement comes from three other factors unrelated to\ndiscourse: context diversity, length diversity, and (to a lesser extent)\nposition shifting.", "published": "2021-05-04 18:18:07", "link": "http://arxiv.org/abs/2105.01691v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ZEN 2.0: Continue Training and Adaption for N-gram Enhanced Text\n  Encoders", "abstract": "Pre-trained text encoders have drawn sustaining attention in natural language\nprocessing (NLP) and shown their capability in obtaining promising results in\ndifferent tasks. Recent studies illustrated that external self-supervised\nsignals (or knowledge extracted by unsupervised learning, such as n-grams) are\nbeneficial to provide useful semantic evidence for understanding languages such\nas Chinese, so as to improve the performance on various downstream tasks\naccordingly. To further enhance the encoders, in this paper, we propose to\npre-train n-gram-enhanced encoders with a large volume of data and advanced\ntechniques for training. Moreover, we try to extend the encoder to different\nlanguages as well as different domains, where it is confirmed that the same\narchitecture is applicable to these varying circumstances and new\nstate-of-the-art performance is observed from a long list of NLP tasks across\nlanguages and domains.", "published": "2021-05-04 04:08:58", "link": "http://arxiv.org/abs/2105.01279v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic Extractor-Paraphraser based Abstractive Summarization", "abstract": "The anthology of spoken languages today is inundated with textual\ninformation, necessitating the development of automatic summarization models.\nIn this manuscript, we propose an extractor-paraphraser based abstractive\nsummarization system that exploits semantic overlap as opposed to its\npredecessors that focus more on syntactic information overlap. Our model\noutperforms the state-of-the-art baselines in terms of ROUGE, METEOR and word\nmover similarity (WMS), establishing the superiority of the proposed system via\nextensive ablation experiments. We have also challenged the summarization\ncapabilities of the state of the art Pointer Generator Network (PGN), and\nthrough thorough experimentation, shown that PGN is more of a paraphraser,\ncontrary to the prevailing notion of a summarizer; illustrating it's\nincapability to accumulate information across multiple sentences.", "published": "2021-05-04 05:24:28", "link": "http://arxiv.org/abs/2105.01296v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large-scale Taxonomy Induction Using Entity and Word Embeddings", "abstract": "Taxonomies are an important ingredient of knowledge organization, and serve\nas a backbone for more sophisticated knowledge representations in intelligent\nsystems, such as formal ontologies. However, building taxonomies manually is a\ncostly endeavor, and hence, automatic methods for taxonomy induction are a good\nalternative to build large-scale taxonomies. In this paper, we propose TIEmb,\nan approach for automatic unsupervised class subsumption axiom extraction from\nknowledge bases using entity and text embeddings. We apply the approach on the\nWebIsA database, a database of subsumption relations extracted from the large\nportion of the World Wide Web, to extract class hierarchies in the Person and\nPlace domain.", "published": "2021-05-04 05:53:12", "link": "http://arxiv.org/abs/2105.01305v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "GraphTMT: Unsupervised Graph-based Topic Modeling from Video Transcripts", "abstract": "To unfold the tremendous amount of multimedia data uploaded daily to social\nmedia platforms, effective topic modeling techniques are needed. Existing work\ntends to apply topic models on written text datasets. In this paper, we propose\na topic extractor on video transcripts. Exploiting neural word embeddings\nthrough graph-based clustering, we aim to improve usability and semantic\ncoherence. Unlike most topic models, this approach works without knowing the\ntrue number of topics, which is important when no such assumption can or should\nbe made. Experimental results on the real-life multimodal dataset MuSe-CaR\ndemonstrates that our approach GraphTMT extracts coherent and meaningful topics\nand outperforms baseline methods. Furthermore, we successfully demonstrate the\napplicability of our approach on the popular Citysearch corpus.", "published": "2021-05-04 12:48:17", "link": "http://arxiv.org/abs/2105.01466v4", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Sentiment and Emotion Classification of Epidemic Related Bilingual data\n  from Social Media", "abstract": "In recent years, sentiment analysis and emotion classification are two of the\nmost abundantly used techniques in the field of Natural Language Processing\n(NLP). Although sentiment analysis and emotion classification are used commonly\nin applications such as analyzing customer reviews, the popularity of\ncandidates contesting in elections, and comments about various sporting events;\nhowever, in this study, we have examined their application for epidemic\noutbreak detection. Early outbreak detection is the key to deal with epidemics\neffectively, however, the traditional ways of outbreak detection are\ntime-consuming which inhibits prompt response from the respective departments.\nSocial media platforms such as Twitter, Facebook, Instagram, etc. allow the\nusers to express their thoughts related to different aspects of life, and\ntherefore, serve as a substantial source of information in such situations. The\nproposed study exploits the bilingual (Urdu and English) data from Twitter and\nNEWS websites related to the dengue epidemic in Pakistan, and sentiment\nanalysis and emotion classification are performed to acquire deep insights from\nthe data set for gaining a fair idea related to an epidemic outbreak. Machine\nlearning and deep learning algorithms have been used to train and implement the\nmodels for the execution of both tasks. The comparative performance of each\nmodel has been evaluated using accuracy, precision, recall, and f1-measure.", "published": "2021-05-04 12:51:18", "link": "http://arxiv.org/abs/2105.01468v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "An Estimation of Online Video User Engagement from Features of\n  Continuous Emotions", "abstract": "Portraying emotion and trustworthiness is known to increase the appeal of\nvideo content. However, the causal relationship between these signals and\nonline user engagement is not well understood. This limited understanding is\npartly due to a scarcity in emotionally annotated data and the varied\nmodalities which express user engagement online. In this contribution, we\nutilise a large dataset of YouTube review videos which includes ca. 600 hours\nof dimensional arousal, valence and trustworthiness annotations. We investigate\nfeatures extracted from these signals against various user engagement\nindicators including views, like/dislike ratio, as well as the sentiment of\ncomments. In doing so, we identify the positive and negative influences which\nsingle features have, as well as interpretable patterns in each dimension which\nrelate to user engagement. Our results demonstrate that smaller boundary ranges\nand fluctuations for arousal lead to an increase in user engagement.\nFurthermore, the extracted time-series features reveal significant (p<0.05)\ncorrelations for each dimension, such as, count below signal mean (arousal),\nnumber of peaks (valence), and absolute energy (trustworthiness). From this, an\neffective combination of features is outlined for approaches aiming to\nautomatically predict several user engagement indicators. In a user engagement\nprediction paradigm we compare all features against semi-automatic\n(cross-task), and automatic (task-specific) feature selection methods. These\nselected feature sets appear to outperform the usage of all features, e.g.,\nusing all features achieves 1.55 likes per day (Lp/d) mean absolute error from\nvalence; this improves through semi-automatic and automatic selection to 1.33\nand 1.23 Lp/d, respectively (data mean 9.72 Lp/d with a std. 28.75 Lp/d).", "published": "2021-05-04 17:18:47", "link": "http://arxiv.org/abs/2105.01633v1", "categories": ["cs.MM", "cs.CL"], "primary_category": "cs.MM"}
{"title": "HerBERT: Efficiently Pretrained Transformer-based Language Model for\n  Polish", "abstract": "BERT-based models are currently used for solving nearly all Natural Language\nProcessing (NLP) tasks and most often achieve state-of-the-art results.\nTherefore, the NLP community conducts extensive research on understanding these\nmodels, but above all on designing effective and efficient training procedures.\nSeveral ablation studies investigating how to train BERT-like models have been\ncarried out, but the vast majority of them concerned only the English language.\nA training procedure designed for English does not have to be universal and\napplicable to other especially typologically different languages. Therefore,\nthis paper presents the first ablation study focused on Polish, which, unlike\nthe isolating English language, is a fusional language. We design and\nthoroughly evaluate a pretraining procedure of transferring knowledge from\nmultilingual to monolingual BERT-based models. In addition to multilingual\nmodel initialization, other factors that possibly influence pretraining are\nalso explored, i.e. training objective, corpus size, BPE-Dropout, and\npretraining length. Based on the proposed procedure, a Polish BERT-based\nlanguage model -- HerBERT -- is trained. This model achieves state-of-the-art\nresults on multiple downstream tasks.", "published": "2021-05-04 20:16:17", "link": "http://arxiv.org/abs/2105.01735v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BLM-17m: A Large-Scale Dataset for Black Lives Matter Topic Detection on\n  Twitter", "abstract": "Protection of human rights is one of the most important problems of our\nworld. In this paper, our aim is to provide a dataset which covers one of the\nmost significant human rights contradiction in recent months affected the whole\nworld, George Floyd incident. We propose a labeled dataset for topic detection\nthat contains 17 million tweets. These Tweets are collected from 25 May 2020 to\n21 August 2020 that covers 89 days from start of this incident. We labeled the\ndataset by monitoring most trending news topics from global and local\nnewspapers. Apart from that, we present two baselines, TF-IDF and LDA. We\nevaluated the results of these two methods with three different k values for\nmetrics of precision, recall and f1-score. The collected dataset is available\nat https://github.com/MeysamAsgariC/BLMT.", "published": "2021-05-04 07:27:42", "link": "http://arxiv.org/abs/2105.01331v3", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Retrieving Complex Tables with Multi-Granular Graph Representation\n  Learning", "abstract": "The task of natural language table retrieval (NLTR) seeks to retrieve\nsemantically relevant tables based on natural language queries. Existing\nlearning systems for this task often treat tables as plain text based on the\nassumption that tables are structured as dataframes. However, tables can have\ncomplex layouts which indicate diverse dependencies between subtable\nstructures, such as nested headers. As a result, queries may refer to different\nspans of relevant content that is distributed across these structures.\nMoreover, such systems fail to generalize to novel scenarios beyond those seen\nin the training set. Prior methods are still distant from a generalizable\nsolution to the NLTR problem, as they fall short in handling complex table\nlayouts or queries over multiple granularities. To address these issues, we\npropose Graph-based Table Retrieval (GTR), a generalizable NLTR framework with\nmulti-granular graph representation learning. In our framework, a table is\nfirst converted into a tabular graph, with cell nodes, row nodes and column\nnodes to capture content at different granularities. Then the tabular graph is\ninput to a Graph Transformer model that can capture both table cell content and\nthe layout structures. To enhance the robustness and generalizability of the\nmodel, we further incorporate a self-supervised pre-training task based on\ngraph-context matching. Experimental results on two benchmarks show that our\nmethod leads to significant improvements over the current state-of-the-art\nsystems. Further experiments demonstrate promising performance of our method on\ncross-dataset generalization, and enhanced capability of handling complex\ntables and fulfilling diverse query intents. Code and data are available at\nhttps://github.com/FeiWang96/GTR.", "published": "2021-05-04 20:19:03", "link": "http://arxiv.org/abs/2105.01736v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Voice Conversion Based Speaker Normalization for Acoustic Unit Discovery", "abstract": "Discovering speaker independent acoustic units purely from spoken input is\nknown to be a hard problem. In this work we propose an unsupervised speaker\nnormalization technique prior to unit discovery. It is based on separating\nspeaker related from content induced variations in a speech signal with an\nadversarial contrastive predictive coding approach. This technique does neither\nrequire transcribed speech nor speaker labels, and, furthermore, can be trained\nin a multilingual fashion, thus achieving speaker normalization even if only\nfew unlabeled data is available from the target language. The speaker\nnormalization is done by mapping all utterances to a medoid style which is\nrepresentative for the whole database. We demonstrate the effectiveness of the\napproach by conducting acoustic unit discovery with a hidden Markov model\nvariational autoencoder noting, however, that the proposed speaker\nnormalization can serve as a front end to any unit discovery system.\nExperiments on English, Yoruba and Mboshi show improvements compared to using\nnon-normalized input.", "published": "2021-05-04 22:40:41", "link": "http://arxiv.org/abs/2105.01786v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Flipped Classroom model for teaching Conditional Random Fields in an\n  NLP course", "abstract": "In this article, we show and discuss our experience in applying the flipped\nclassroom method for teaching Conditional Random Fields in a Natural Language\nProcessing course. We present the activities that we developed together with\ntheir relationship to a cognitive complexity model (Bloom's taxonomy). After\nthis, we provide our own reflections and expectations of the model itself.\nBased on the evaluation got from students, it seems that students learn about\nthe topic and also that the method is rewarding for some students.\nAdditionally, we discuss some shortcomings and we propose possible solutions to\nthem. We conclude the paper with some possible future work.", "published": "2021-05-04 11:21:03", "link": "http://arxiv.org/abs/2105.07850v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Speech Decomposition Based on a Hybrid Speech Model and Optimal\n  Segmentation", "abstract": "In a hybrid speech model, both voiced and unvoiced components can coexist in\na segment. Often, the voiced speech is regarded as the deterministic component,\nand the unvoiced speech and additive noise are the stochastic components.\nTypically, the speech signal is considered stationary within fixed segments of\n20-40 ms, but the degree of stationarity varies over time. For decomposing\nnoisy speech into its voiced and unvoiced components, a fixed segmentation may\nbe too crude, and we here propose to adapt the segment length according to the\nsignal local characteristics. The segmentation relies on parameter estimates of\na hybrid speech model and the maximum a posteriori (MAP) and log-likelihood\ncriteria as rules for model selection among the possible segment lengths, for\nvoiced and unvoiced speech, respectively. Given the optimal segmentation\nmarkers and the estimated statistics, both components are estimated using\nlinear filtering. A codebook-based approach differentiates between unvoiced\nspeech and noise. A better extraction of the components is possible by taking\ninto account the adaptive segmentation, compared to a fixed one. Also, a lower\ndistortion for voiced speech and higher segSNR for both components is possible,\nas compared to other decomposition methods.", "published": "2021-05-04 05:39:41", "link": "http://arxiv.org/abs/2105.01302v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Performance Evaluation of Deep Convolutional Maxout Neural Network in\n  Speech Recognition", "abstract": "In this paper, various structures and methods of Deep Artificial Neural\nNetworks (DNN) will be evaluated and compared for the purpose of continuous\nPersian speech recognition. One of the first models of neural networks used in\nspeech recognition applications were fully connected Neural Networks (FCNNs)\nand, consequently, Deep Neural Networks (DNNs). Although these models have\nbetter performance compared to GMM / HMM models, they do not have the proper\nstructure to model local speech information. Convolutional Neural Network (CNN)\nis a good option for modeling the local structure of biological signals,\nincluding speech signals. Another issue that Deep Artificial Neural Networks\nface, is the convergence of networks on training data. The main inhibitor of\nconvergence is the presence of local minima in the process of training. Deep\nNeural Network Pre-training methods, despite a large amount of computing, are\npowerful tools for crossing the local minima. But the use of appropriate\nneuronal models in the network structure seems to be a better solution to this\nproblem. The Rectified Linear Unit neuronal model and the Maxout model are the\nmost suitable neuronal models presented to this date. Several experiments were\ncarried out to evaluate the performance of the methods and structures\nmentioned. After verifying the proper functioning of these methods, a\ncombination of all models was implemented on FARSDAT speech database for\ncontinuous speech recognition. The results obtained from the experiments show\nthat the combined model (CMDNN) improves the performance of ANNs in speech\nrecognition versus the pre-trained fully connected NNs with sigmoid neurons by\nabout 3%.", "published": "2021-05-04 10:19:03", "link": "http://arxiv.org/abs/2105.01399v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring Disentanglement with Multilingual and Monolingual VQ-VAE", "abstract": "This work examines the content and usefulness of disentangled phone and\nspeaker representations from two separately trained VQ-VAE systems: one trained\non multilingual data and another trained on monolingual data. We explore the\nmulti- and monolingual models using four small proof-of-concept tasks:\ncopy-synthesis, voice transformation, linguistic code-switching, and\ncontent-based privacy masking. From these tasks, we reflect on how disentangled\nphone and speaker representations can be used to manipulate speech in a\nmeaningful way. Our experiments demonstrate that the VQ representations are\nsuitable for these tasks, including creating new voices by mixing speaker\nrepresentations together. We also present our novel technique to conceal the\ncontent of targeted words within an utterance by manipulating phone VQ codes,\nwhile retaining speaker identity and intelligibility of surrounding words.\nFinally, we discuss recommendations for further increasing the viability of\ndisentangled representations.", "published": "2021-05-04 15:36:32", "link": "http://arxiv.org/abs/2105.01573v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Streaming end-to-end speech recognition with jointly trained neural\n  feature enhancement", "abstract": "In this paper, we present a streaming end-to-end speech recognition model\nbased on Monotonic Chunkwise Attention (MoCha) jointly trained with enhancement\nlayers. Even though the MoCha attention enables streaming speech recognition\nwith recognition accuracy comparable to a full attention-based approach,\ntraining this model is sensitive to various factors such as the difficulty of\ntraining examples, hyper-parameters, and so on. Because of these issues, speech\nrecognition accuracy of a MoCha-based model for clean speech drops\nsignificantly when a multi-style training approach is applied. Inspired by\nCurriculum Learning [1], we introduce two training strategies: Gradual\nApplication of Enhanced Features (GAEF) and Gradual Reduction of Enhanced Loss\n(GREL). With GAEF, the model is initially trained using clean features.\nSubsequently, the portion of outputs from the enhancement layers gradually\nincreases. With GREL, the portion of the Mean Squared Error (MSE) loss for the\nenhanced output gradually reduces as training proceeds. In experimental results\non the LibriSpeech corpus and noisy far-field test sets, the proposed model\nwith GAEF-GREL training strategies shows significantly better results than the\nconventional multi-style training approach.", "published": "2021-05-04 02:25:41", "link": "http://arxiv.org/abs/2105.01254v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VQCPC-GAN: Variable-Length Adversarial Audio Synthesis Using\n  Vector-Quantized Contrastive Predictive Coding", "abstract": "Influenced by the field of Computer Vision, Generative Adversarial Networks\n(GANs) are often adopted for the audio domain using fixed-size two-dimensional\nspectrogram representations as the \"image data\". However, in the (musical)\naudio domain, it is often desired to generate output of variable duration. This\npaper presents VQCPC-GAN, an adversarial framework for synthesizing\nvariable-length audio by exploiting Vector-Quantized Contrastive Predictive\nCoding (VQCPC). A sequence of VQCPC tokens extracted from real audio data\nserves as conditional input to a GAN architecture, providing step-wise\ntime-dependent features of the generated content. The input noise z\n(characteristic in adversarial architectures) remains fixed over time, ensuring\ntemporal consistency of global features. We evaluate the proposed model by\ncomparing a diverse set of metrics against various strong baselines. Results\nshow that, even though the baselines score best, VQCPC-GAN achieves comparable\nperformance even when generating variable-length audio. Numerous sound examples\nare provided in the accompanying website, and we release the code for\nreproducibility.", "published": "2021-05-04 14:35:51", "link": "http://arxiv.org/abs/2105.01531v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
