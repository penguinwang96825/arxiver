{"title": "Vision-and-Language Pretraining", "abstract": "With the burgeoning amount of data of image-text pairs and diversity of\nVision-and-Language (V\\&L) tasks, scholars have introduced an abundance of deep\nlearning models in this research domain. Furthermore, in recent years, transfer\nlearning has also shown tremendous success in Computer Vision for tasks such as\nImage Classification, Object Detection, etc., and in Natural Language\nProcessing for Question Answering, Machine Translation, etc. Inheriting the\nspirit of Transfer Learning, research works in V\\&L have devised multiple\npretraining techniques on large-scale datasets in order to enhance the\nperformance of downstream tasks. The aim of this article is to provide a\ncomprehensive revision of contemporary V\\&L pretraining models. In particular,\nwe categorize and delineate pretraining approaches, along with the summary of\nstate-of-the-art vision-and-language pretrained models. Moreover, a list of\ntraining datasets and downstream tasks is supplied to further polish the\nperspective into V\\&L pretraining. Lastly, we decided to take a further step to\ndiscuss numerous directions for future research.", "published": "2022-07-05 02:18:49", "link": "http://arxiv.org/abs/2207.01772v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Keyword Extraction in Scientific Documents", "abstract": "The scientific publication output grows exponentially. Therefore, it is\nincreasingly challenging to keep track of trends and changes. Understanding\nscientific documents is an important step in downstream tasks such as knowledge\ngraph building, text mining, and discipline classification. In this workshop,\nwe provide a better understanding of keyword and keyphrase extraction from the\nabstract of scientific publications.", "published": "2022-07-05 08:33:47", "link": "http://arxiv.org/abs/2207.01888v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ASR-Generated Text for Language Model Pre-training Applied to Speech\n  Tasks", "abstract": "We aim at improving spoken language modeling (LM) using very large amount of\nautomatically transcribed speech. We leverage the INA (French National\nAudiovisual Institute) collection and obtain 19GB of text after applying ASR on\n350,000 hours of diverse TV shows. From this, spoken language models are\ntrained either by fine-tuning an existing LM (FlauBERT) or through training a\nLM from scratch. New models (FlauBERT-Oral) are shared with the community and\nevaluated for 3 downstream tasks: spoken language understanding, classification\nof TV shows and speech syntactic parsing. Results show that FlauBERT-Oral can\nbe beneficial compared to its initial FlauBERT version demonstrating that,\ndespite its inherent noisy nature, ASR-generated text can be used to build\nspoken language models.", "published": "2022-07-05 08:47:51", "link": "http://arxiv.org/abs/2207.01893v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Betti numbers of attention graphs is all you really need", "abstract": "We apply methods of topological analysis to the attention graphs, calculated\non the attention heads of the BERT model ( arXiv:1810.04805v2 ). Our research\nshows that the classifier built upon basic persistent topological features\n(namely, Betti numbers) of the trained neural network can achieve\nclassification results on par with the conventional classification method. We\nshow the relevance of such topological text representation on three text\nclassification benchmarks. For the best of our knowledge, it is the first\nattempt to analyze the topology of an attention-based neural network, widely\nused for Natural Language Processing.", "published": "2022-07-05 09:10:47", "link": "http://arxiv.org/abs/2207.01903v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual QA as a Stepping Stone for Monolingual Open QA in\n  Icelandic", "abstract": "It can be challenging to build effective open question answering (open QA)\nsystems for languages other than English, mainly due to a lack of labeled data\nfor training. We present a data efficient method to bootstrap such a system for\nlanguages other than English. Our approach requires only limited QA resources\nin the given language, along with machine-translated data, and at least a\nbilingual language model. To evaluate our approach, we build such a system for\nthe Icelandic language and evaluate performance over trivia style datasets. The\ncorpora used for training are English in origin but machine translated into\nIcelandic. We train a bilingual Icelandic/English language model to embed\nEnglish context and Icelandic questions following methodology introduced with\nDensePhrases (Lee et al., 2021). The resulting system is an open domain\ncross-lingual QA system between Icelandic and English. Finally, the system is\nadapted for Icelandic only open QA, demonstrating how it is possible to\nefficiently create an open QA system with limited access to curated datasets in\nthe language of interest.", "published": "2022-07-05 09:52:34", "link": "http://arxiv.org/abs/2207.01918v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MIA 2022 Shared Task Submission: Leveraging Entity Representations,\n  Dense-Sparse Hybrids, and Fusion-in-Decoder for Cross-Lingual Question\n  Answering", "abstract": "We describe our two-stage system for the Multilingual Information Access\n(MIA) 2022 Shared Task on Cross-Lingual Open-Retrieval Question Answering. The\nfirst stage consists of multilingual passage retrieval with a hybrid dense and\nsparse retrieval strategy. The second stage consists of a reader which outputs\nthe answer from the top passages returned by the first stage. We show the\nefficacy of using a multilingual language model with entity representations in\npretraining, sparse retrieval signals to help dense retrieval, and\nFusion-in-Decoder. On the development set, we obtain 43.46 F1 on XOR-TyDi QA\nand 21.99 F1 on MKQA, for an average F1 score of 32.73. On the test set, we\nobtain 40.93 F1 on XOR-TyDi QA and 22.29 F1 on MKQA, for an average F1 score of\n31.61. We improve over the official baseline by over 4 F1 points on both the\ndevelopment and test sets.", "published": "2022-07-05 10:27:17", "link": "http://arxiv.org/abs/2207.01940v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Making sense of spoken plurals", "abstract": "Distributional semantics offers new ways to study the semantics of\nmorphology. This study focuses on the semantics of noun singulars and their\nplural inflectional variants in English. Our goal is to compare two models for\nthe conceptualization of plurality. One model (FRACSS) proposes that all\nsingular-plural pairs should be taken into account when predicting plural\nsemantics from singular semantics. The other model (CCA) argues that\nconceptualization for plurality depends primarily on the semantic class of the\nbase word. We compare the two models on the basis of how well the speech signal\nof plural tokens in a large corpus of spoken American English aligns with the\nsemantic vectors predicted by the two models. Two measures are employed: the\nperformance of a form-to-meaning mapping and the correlations between form\ndistances and meaning distances. Results converge on a superior alignment for\nCCA. Our results suggest that usage-based approaches to pluralization in which\na given word's own semantic neighborhood is given priority outperform theories\naccording to which pluralization is conceptualized as a process building on\nhigh-level abstraction. We see that what has often been conceived of as a\nhighly abstract concept, [+plural], is better captured via a family of\nmid-level partial generalizations.", "published": "2022-07-05 10:44:26", "link": "http://arxiv.org/abs/2207.01947v2", "categories": ["cs.CL", "J.5"], "primary_category": "cs.CL"}
{"title": "Block-SCL: Blocking Matters for Supervised Contrastive Learning in\n  Product Matching", "abstract": "Product matching is a fundamental step for the global understanding of\nconsumer behavior in e-commerce. In practice, product matching refers to the\ntask of deciding if two product offers from different data sources (e.g.\nretailers) represent the same product. Standard pipelines use a previous stage\ncalled blocking, where for a given product offer a set of potential matching\ncandidates are retrieved based on similar characteristics (e.g. same brand,\ncategory, flavor, etc.). From these similar product candidates, those that are\nnot a match can be considered hard negatives. We present Block-SCL, a strategy\nthat uses the blocking output to make the most of Supervised Contrastive\nLearning (SCL). Concretely, Block-SCL builds enriched batches using the\nhard-negatives samples obtained in the blocking stage. These batches provide a\nstrong training signal leading the model to learn more meaningful sentence\nembeddings for product matching. Experimental results in several public\ndatasets demonstrate that Block-SCL achieves state-of-the-art results despite\nonly using short product titles as input, no data augmentation, and a lighter\ntransformer backbone than competing methods.", "published": "2022-07-05 12:44:06", "link": "http://arxiv.org/abs/2207.02008v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the Faithfulness of Abstractive Summarization via Entity\n  Coverage Control", "abstract": "Abstractive summarization systems leveraging pre-training language models\nhave achieved superior results on benchmark datasets. However, such models have\nbeen shown to be more prone to hallucinate facts that are unfaithful to the\ninput context. In this paper, we propose a method to remedy entity-level\nextrinsic hallucinations with Entity Coverage Control (ECC). We first compute\nentity coverage precision and prepend the corresponding control code for each\ntraining example, which implicitly guides the model to recognize faithfulness\ncontents in the training phase. We further extend our method via intermediate\nfine-tuning on large but noisy data extracted from Wikipedia to unlock\nzero-shot summarization. We show that the proposed method leads to more\nfaithful and salient abstractive summarization in supervised fine-tuning and\nzero-shot settings according to our experimental results on three benchmark\ndatasets XSum, Pubmed, and SAMSum of very different domains and styles.", "published": "2022-07-05 18:52:19", "link": "http://arxiv.org/abs/2207.02263v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot Cross-Linguistic Learning of Event Semantics", "abstract": "Typologically diverse languages offer systems of lexical and grammatical\naspect that allow speakers to focus on facets of event structure in ways that\ncomport with the specific communicative setting and discourse constraints they\nface. In this paper, we look specifically at captions of images across Arabic,\nChinese, Farsi, German, Russian, and Turkish and describe a computational model\nfor predicting lexical aspects. Despite the heterogeneity of these languages,\nand the salient invocation of distinctive linguistic resources across their\ncaption corpora, speakers of these languages show surprising similarities in\nthe ways they frame image content. We leverage this observation for zero-shot\ncross-lingual learning and show that lexical aspects can be predicted for a\ngiven language despite not having observed any annotated data for this language\nat all.", "published": "2022-07-05 23:18:36", "link": "http://arxiv.org/abs/2207.02356v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scene-Aware Prompt for Multi-modal Dialogue Understanding and Generation", "abstract": "This paper introduces the schemes of Team LingJing's experiments in\nNLPCC-2022-Shared-Task-4 Multi-modal Dialogue Understanding and Generation\n(MDUG). The MDUG task can be divided into two phases: multi-modal context\nunderstanding and response generation. To fully leverage the visual information\nfor both scene understanding and dialogue generation, we propose the\nscene-aware prompt for the MDUG task. Specifically, we utilize the\nmulti-tasking strategy for jointly modelling the scene- and session-\nmulti-modal understanding. The visual captions are adopted to aware the scene\ninformation, while the fixed-type templated prompt based on the scene- and\nsession-aware labels are used to further improve the dialogue generation\nperformance. Extensive experimental results show that the proposed method has\nachieved state-of-the-art (SOTA) performance compared with other competitive\nmethods, where we rank the 1-st in all three subtasks in this MDUG competition.", "published": "2022-07-05 05:54:20", "link": "http://arxiv.org/abs/2207.01823v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Pretraining on Interactions for Learning Grounded Affordance\n  Representations", "abstract": "Lexical semantics and cognitive science point to affordances (i.e. the\nactions that objects support) as critical for understanding and representing\nnouns and verbs. However, study of these semantic features has not yet been\nintegrated with the \"foundation\" models that currently dominate language\nrepresentation research. We hypothesize that predictive modeling of object\nstate over time will result in representations that encode object affordance\ninformation \"for free\". We train a neural network to predict objects'\ntrajectories in a simulated interaction and show that our network's latent\nrepresentations differentiate between both observed and unobserved affordances.\nWe find that models trained using 3D simulations from our SPATIAL dataset\noutperform conventional 2D computer vision models trained on a similar task,\nand, on initial inspection, that differences between concepts correspond to\nexpected features (e.g., roll entails rotation). Our results suggest a way in\nwhich modern deep learning approaches to grounded language learning can be\nintegrated with traditional formal semantic notions of lexical representations.", "published": "2022-07-05 19:19:53", "link": "http://arxiv.org/abs/2207.02272v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PReGAN: Answer Oriented Passage Ranking with Weakly Supervised GAN", "abstract": "Beyond topical relevance, passage ranking for open-domain factoid question\nanswering also requires a passage to contain an answer (answerability). While a\nfew recent studies have incorporated some reading capability into a ranker to\naccount for answerability, the ranker is still hindered by the noisy nature of\nthe training data typically available in this area, which considers any passage\ncontaining an answer entity as a positive sample. However, the answer entity in\na passage is not necessarily mentioned in relation with the given question. To\naddress the problem, we propose an approach called \\ttt{PReGAN} for Passage\nReranking based on Generative Adversarial Neural networks, which incorporates a\ndiscriminator on answerability, in addition to a discriminator on topical\nrelevance. The goal is to force the generator to rank higher a passage that is\ntopically relevant and contains an answer. Experiments on five public datasets\nshow that \\ttt{PReGAN} can better rank appropriate passages, which in turn,\nboosts the effectiveness of QA systems, and outperforms the existing approaches\nwithout using external data.", "published": "2022-07-05 01:43:35", "link": "http://arxiv.org/abs/2207.01762v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CodeRL: Mastering Code Generation through Pretrained Models and Deep\n  Reinforcement Learning", "abstract": "Program synthesis or code generation aims to generate a program that\nsatisfies a problem specification. Recent approaches using large-scale\npretrained language models (LMs) have shown promising results, yet they have\nsome critical limitations. In particular, they often follow a standard\nsupervised fine-tuning procedure to train a code generation model only from the\npairs of natural-language problem descriptions and ground-truth programs. Such\nparadigm largely ignores some important but potentially useful signals in the\nproblem specification such as unit tests, which thus often results in poor\nperformance when solving complex unseen coding tasks. To address the\nlimitations, we propose \"CodeRL\", a new framework for program synthesis tasks\nthrough pretrained LMs and deep reinforcement learning (RL). Specifically,\nduring training, we treat the code-generating LM as an actor network, and\nintroduce a critic network that is trained to predict the functional\ncorrectness of generated programs and provide dense feedback signals to the\nactor. During inference, we introduce a new generation procedure with a\ncritical sampling strategy that allows a model to automatically regenerate\nprograms based on feedback from example unit tests and critic scores. For the\nmodel backbones, we extended the encoder-decoder architecture of CodeT5 with\nenhanced learning objectives, larger model sizes, and better pretraining data.\nOur method not only achieves new SOTA results on the challenging APPS\nbenchmark, but also shows strong zero-shot transfer capability with new SOTA\nresults on the simpler MBPP benchmark.", "published": "2022-07-05 02:42:15", "link": "http://arxiv.org/abs/2207.01780v3", "categories": ["cs.LG", "cs.CL", "cs.PL"], "primary_category": "cs.LG"}
{"title": "Entity Linking in Tabular Data Needs the Right Attention", "abstract": "Understanding the semantic meaning of tabular data requires Entity Linking\n(EL), in order to associate each cell value to a real-world entity in a\nKnowledge Base (KB). In this work, we focus on end-to-end solutions for EL on\ntabular data that do not rely on fact lookup in the target KB. Tabular data\ncontains heterogeneous and sparse context, including column headers, cell\nvalues and table captions. We experiment with various models to generate a\nvector representation for each cell value to be linked. Our results show that\nit is critical to apply an attention mechanism as well as an attention mask, so\nthat the model can only attend to the most relevant context and avoid\ninformation dilution. The most relevant context includes: same-row cells,\nsame-column cells, headers and caption. Computational complexity, however,\ngrows quadratically with the size of tabular data for such a complex model. We\nachieve constant memory usage by introducing a Tabular Entity Linking Lite\nmodel (TELL ) that generates vector representation for a cell based only on its\nvalue, the table headers and the table caption. TELL achieves 80.8% accuracy on\nWikipedia tables, which is only 0.1% lower than the state-of-the-art model with\nquadratic memory usage.", "published": "2022-07-05 10:20:44", "link": "http://arxiv.org/abs/2207.01937v1", "categories": ["cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quantum Circuit Compiler for a Shuttling-Based Trapped-Ion Quantum\n  Computer", "abstract": "The increasing capabilities of quantum computing hardware and the challenge\nof realizing deep quantum circuits require fully automated and efficient tools\nfor compiling quantum circuits. To express arbitrary circuits in a sequence of\nnative gates specific to the quantum computer architecture, it is necessary to\nmake algorithms portable across the landscape of quantum hardware providers. In\nthis work, we present a compiler capable of transforming and optimizing a\nquantum circuit targeting a shuttling-based trapped-ion quantum processor. It\nconsists of custom algorithms set on top of the quantum circuit framework\nPytket. The performance was evaluated for a wide range of quantum circuits and\nthe results show that the gate counts can be reduced by factors up to 5.1\ncompared to standard Pytket and up to 2.2 compared to standard Qiskit\ncompilation.", "published": "2022-07-05 11:21:09", "link": "http://arxiv.org/abs/2207.01964v4", "categories": ["quant-ph", "cs.CL", "cs.ET", "81P65, 81P68, 68Q09", "D.3.4"], "primary_category": "quant-ph"}
{"title": "Neural Networks and the Chomsky Hierarchy", "abstract": "Reliable generalization lies at the heart of safe ML and AI. However,\nunderstanding when and how neural networks generalize remains one of the most\nimportant unsolved problems in the field. In this work, we conduct an extensive\nempirical study (20'910 models, 15 tasks) to investigate whether insights from\nthe theory of computation can predict the limits of neural network\ngeneralization in practice. We demonstrate that grouping tasks according to the\nChomsky hierarchy allows us to forecast whether certain architectures will be\nable to generalize to out-of-distribution inputs. This includes negative\nresults where even extensive amounts of data and training time never lead to\nany non-trivial generalization, despite models having sufficient capacity to\nfit the training data perfectly. Our results show that, for our subset of\ntasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can\nsolve regular and counter-language tasks, and only networks augmented with\nstructured memory (such as a stack or memory tape) can successfully generalize\non context-free and context-sensitive tasks.", "published": "2022-07-05 15:06:11", "link": "http://arxiv.org/abs/2207.02098v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.FL"], "primary_category": "cs.LG"}
{"title": "A cross-corpus study on speech emotion recognition", "abstract": "For speech emotion datasets, it has been difficult to acquire large\nquantities of reliable data and acted emotions may be over the top compared to\nless expressive emotions displayed in everyday life. Lately, larger datasets\nwith natural emotions have been created. Instead of ignoring smaller, acted\ndatasets, this study investigates whether information learnt from acted\nemotions is useful for detecting natural emotions. Cross-corpus research has\nmostly considered cross-lingual and even cross-age datasets, and difficulties\narise from different methods of annotating emotions causing a drop in\nperformance. To be consistent, four adult English datasets covering acted,\nelicited and natural emotions are considered. A state-of-the-art model is\nproposed to accurately investigate the degradation of performance. The system\ninvolves a bi-directional LSTM with an attention mechanism to classify emotions\nacross datasets. Experiments study the effects of training models in a\ncross-corpus and multi-domain fashion and results show the transfer of\ninformation is not successful. Out-of-domain models, followed by adapting to\nthe missing dataset, and domain adversarial training (DAT) are shown to be more\nsuitable to generalising to emotions across datasets. This shows positive\ninformation transfer from acted datasets to those with more natural emotions\nand the benefits from training on different corpora.", "published": "2022-07-05 15:15:22", "link": "http://arxiv.org/abs/2207.02104v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Review of Visual-Textual Sentiment Analysis from Social\n  Media Networks", "abstract": "Social media networks have become a significant aspect of people's lives,\nserving as a platform for their ideas, opinions and emotions. Consequently,\nautomated sentiment analysis (SA) is critical for recognising people's feelings\nin ways that other information sources cannot. The analysis of these feelings\nrevealed various applications, including brand evaluations, YouTube film\nreviews and healthcare applications. As social media continues to develop,\npeople post a massive amount of information in different forms, including text,\nphotos, audio and video. Thus, traditional SA algorithms have become limited,\nas they do not consider the expressiveness of other modalities. By including\nsuch characteristics from various material sources, these multimodal data\nstreams provide new opportunities for optimising the expected results beyond\ntext-based SA. Our study focuses on the forefront field of multimodal SA, which\nexamines visual and textual data posted on social media networks. Many people\nare more likely to utilise this information to express themselves on these\nplatforms. To serve as a resource for academics in this rapidly growing field,\nwe introduce a comprehensive overview of textual and visual SA, including data\npre-processing, feature extraction techniques, sentiment benchmark datasets,\nand the efficacy of multiple classification methodologies suited to each field.\nWe also provide a brief introduction of the most frequently utilised data\nfusion strategies and a summary of existing research on visual-textual SA.\nFinally, we highlight the most significant challenges and investigate several\nimportant sentiment applications.", "published": "2022-07-05 16:28:47", "link": "http://arxiv.org/abs/2207.02160v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CLEAR: Improving Vision-Language Navigation with Cross-Lingual,\n  Environment-Agnostic Representations", "abstract": "Vision-and-Language Navigation (VLN) tasks require an agent to navigate\nthrough the environment based on language instructions. In this paper, we aim\nto solve two key challenges in this task: utilizing multilingual instructions\nfor improved instruction-path grounding and navigating through new environments\nthat are unseen during training. To address these challenges, we propose CLEAR:\nCross-Lingual and Environment-Agnostic Representations. First, our agent learns\na shared and visually-aligned cross-lingual language representation for the\nthree languages (English, Hindi and Telugu) in the Room-Across-Room dataset.\nOur language representation learning is guided by text pairs that are aligned\nby visual information. Second, our agent learns an environment-agnostic visual\nrepresentation by maximizing the similarity between semantically-aligned image\npairs (with constraints on object-matching) from different environments. Our\nenvironment agnostic visual representation can mitigate the environment bias\ninduced by low-level visual information. Empirically, on the Room-Across-Room\ndataset, we show that our multilingual agent gets large improvements in all\nmetrics over the strong baseline model when generalizing to unseen environments\nwith the cross-lingual language representation and the environment-agnostic\nvisual representation. Furthermore, we show that our learned language and\nvisual representations can be successfully transferred to the Room-to-Room and\nCooperative Vision-and-Dialogue Navigation task, and present detailed\nqualitative and quantitative generalization and grounding analysis. Our code is\navailable at https://github.com/jialuli-luka/CLEAR", "published": "2022-07-05 17:38:59", "link": "http://arxiv.org/abs/2207.02185v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Putting the Con in Context: Identifying Deceptive Actors in the Game of\n  Mafia", "abstract": "While neural networks demonstrate a remarkable ability to model linguistic\ncontent, capturing contextual information related to a speaker's conversational\nrole is an open area of research. In this work, we analyze the effect of\nspeaker role on language use through the game of Mafia, in which participants\nare assigned either an honest or a deceptive role. In addition to building a\nframework to collect a dataset of Mafia game records, we demonstrate that there\nare differences in the language produced by players with different roles. We\nconfirm that classification models are able to rank deceptive players as more\nsuspicious than honest ones based only on their use of language. Furthermore,\nwe show that training models on two auxiliary tasks outperforms a standard\nBERT-based text classification approach. We also present methods for using our\ntrained models to identify features that distinguish between player roles,\nwhich could be used to assist players during the Mafia game.", "published": "2022-07-05 18:29:27", "link": "http://arxiv.org/abs/2207.02253v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Compute Cost Amortized Transformer for Streaming ASR", "abstract": "We present a streaming, Transformer-based end-to-end automatic speech\nrecognition (ASR) architecture which achieves efficient neural inference\nthrough compute cost amortization. Our architecture creates sparse computation\npathways dynamically at inference time, resulting in selective use of compute\nresources throughout decoding, enabling significant reductions in compute with\nminimal impact on accuracy. The fully differentiable architecture is trained\nend-to-end with an accompanying lightweight arbitrator mechanism operating at\nthe frame-level to make dynamic decisions on each input while a tunable loss\nfunction is used to regularize the overall level of compute against predictive\nperformance. We report empirical results from experiments using the compute\namortized Transformer-Transducer (T-T) model conducted on LibriSpeech data. Our\nbest model can achieve a 60% compute cost reduction with only a 3% relative\nword error rate (WER) increase.", "published": "2022-07-05 03:06:53", "link": "http://arxiv.org/abs/2207.02393v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Machine Learning Model Sizes and the Parameter Gap", "abstract": "We study trends in model size of notable machine learning systems over time\nusing a curated dataset. From 1950 to 2018, model size in language models\nincreased steadily by seven orders of magnitude. The trend then accelerated,\nwith model size increasing by another five orders of magnitude in just 4 years\nfrom 2018 to 2022. Vision models grew at a more constant pace, totaling 7\norders of magnitude of growth between 1950 and 2022.\n  We also identify that, since 2020, there have been many language models below\n20B parameters, many models above 70B parameters, but a scarcity of models in\nthe 20-70B parameter range. We refer to that scarcity as the parameter gap.\n  We provide some stylized facts about the parameter gap and propose a few\nhypotheses to explain it. The explanations we favor are: (a) increasing model\nsize beyond 20B parameters requires adopting different parallelism techniques,\nwhich makes mid-sized models less cost-effective, (b) GPT-3 was one order of\nmagnitude larger than previous language models, and researchers afterwards\nprimarily experimented with bigger models to outperform it. While these\ndynamics likely exist, and we believe they play some role in generating the\ngap, we don't have high confidence that there are no other, more important\ndynamics at play.", "published": "2022-07-05 20:55:38", "link": "http://arxiv.org/abs/2207.02852v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Deep Learning Reveals Patterns of Diverse and Changing Sentiments\n  Towards COVID-19 Vaccines Based on 11 Million Tweets", "abstract": "Over 12 billion doses of COVID-19 vaccines have been administered at the time\nof writing. However, public perceptions of vaccines have been complex. We\nanalyzed COVID-19 vaccine-related tweets to understand the evolving perceptions\nof COVID-19 vaccines. We finetuned a deep learning classifier using a\nstate-of-the-art model, XLNet, to detect each tweet's sentiment automatically.\nWe employed validated methods to extract the users' race or ethnicity, gender,\nage, and geographical locations from user profiles. Incorporating multiple data\nsources, we assessed the sentiment patterns among subpopulations and juxtaposed\nthem against vaccine uptake data to unravel their interactive patterns.\n11,211,672 COVID-19 vaccine-related tweets corresponding to 2,203,681 users\nover two years were analyzed. The finetuned model for sentiment classification\nyielded an accuracy of 0.92 on testing set. Users from various demographic\ngroups demonstrated distinct patterns in sentiments towards COVID-19 vaccines.\nUser sentiments became more positive over time, upon which we observed\nsubsequent upswing in the population-level vaccine uptake. Surrounding dates\nwhere positive sentiments crest, we detected encouraging news or events\nregarding vaccine development and distribution. Positive sentiments in\npregnancy-related tweets demonstrated a delayed pattern compared with trends in\ngeneral population, with postponed vaccine uptake trends. Distinctive patterns\nacross subpopulations suggest the need of tailored strategies. Global news and\nevents profoundly involved in shaping users' thoughts on social media.\nPopulations with additional concerns, such as pregnancy, demonstrated more\nsubstantial hesitancy since lack of timely recommendations. Feature analysis\nrevealed hesitancies of various subpopulations stemmed from clinical trial\nlogics, risks and complications, and urgency of scientific evidence.", "published": "2022-07-05 13:53:16", "link": "http://arxiv.org/abs/2207.10641v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Relating the fundamental frequency of speech with EEG using a dilated\n  convolutional network", "abstract": "To investigate how speech is processed in the brain, we can model the\nrelation between features of a natural speech signal and the corresponding\nrecorded electroencephalogram (EEG). Usually, linear models are used in\nregression tasks. Either EEG is predicted, or speech is reconstructed, and the\ncorrelation between predicted and actual signal is used to measure the brain's\ndecoding ability. However, given the nonlinear nature of the brain, the\nmodeling ability of linear models is limited. Recent studies introduced\nnonlinear models to relate the speech envelope to EEG. We set out to include\nother features of speech that are not coded in the envelope, notably the\nfundamental frequency of the voice (f0). F0 is a higher-frequency feature\nprimarily coded at the brainstem to midbrain level. We present a\ndilated-convolutional model to provide evidence of neural tracking of the f0.\nWe show that a combination of f0 and the speech envelope improves the\nperformance of a state-of-the-art envelope-based model. This suggests the\ndilated-convolutional model can extract non-redundant information from both f0\nand the envelope. We also show the ability of the dilated-convolutional model\nto generalize to subjects not included during training. This latter finding\nwill accelerate f0-based hearing diagnosis.", "published": "2022-07-05 11:20:05", "link": "http://arxiv.org/abs/2207.01963v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Backend Ensemble for Speaker Verification and Spoofing Countermeasure", "abstract": "This paper describes the NPU system submitted to Spoofing Aware Speaker\nVerification Challenge 2022. We particularly focus on the \\textit{backend\nensemble} for speaker verification and spoofing countermeasure from three\naspects. Firstly, besides simple concatenation, we propose circulant matrix\ntransformation and stacking for speaker embeddings and countermeasure\nembeddings. With the stacking operation of newly-defined circulant embeddings,\nwe almost explore all the possible interactions between speaker embeddings and\ncountermeasure embeddings. Secondly, we attempt different convolution neural\nnetworks to selectively fuse the embeddings' salient regions into channels with\nconvolution kernels. Finally, we design parallel attention in 1D convolution\nneural networks to learn the global correlation in channel dimensions as well\nas to learn the important parts in feature dimensions. Meanwhile, we embed\nsqueeze-and-excitation attention in 2D convolutional neural networks to learn\nthe global dependence among speaker embeddings and countermeasure embeddings.\nExperimental results demonstrate that all the above methods are effective.\nAfter fusion of four well-trained models enhanced by the mentioned methods, the\nbest SASV-EER, SPF-EER and SV-EER we achieve are 0.559\\%, 0.354\\% and 0.857\\%\non the evaluation set respectively. Together with the above contributions, our\nsubmission system achieves the fifth place in this challenge.", "published": "2022-07-05 04:10:45", "link": "http://arxiv.org/abs/2207.01802v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Glow-WaveGAN 2: High-quality Zero-shot Text-to-speech Synthesis and\n  Any-to-any Voice Conversion", "abstract": "The zero-shot scenario for speech generation aims at synthesizing a novel\nunseen voice with only one utterance of the target speaker. Although the\nchallenges of adapting new voices in zero-shot scenario exist in both stages --\nacoustic modeling and vocoder, previous works usually consider the problem from\nonly one stage. In this paper, we extend our previous Glow-WaveGAN to\nGlow-WaveGAN 2, aiming to solve the problem from both stages for high-quality\nzero-shot text-to-speech and any-to-any voice conversion. We first build a\nuniversal WaveGAN model for extracting latent distribution $p(z)$ of speech and\nreconstructing waveform from it. Then a flow-based acoustic model only needs to\nlearn the same $p(z)$ from texts, which naturally avoids the mismatch between\nthe acoustic model and the vocoder, resulting in high-quality generated speech\nwithout model fine-tuning. Based on a continuous speaker space and the\nreversible property of flows, the conditional distribution can be obtained for\nany speaker, and thus we can further conduct high-quality zero-shot speech\ngeneration for new speakers. We particularly investigate two methods to\nconstruct the speaker space, namely pre-trained speaker encoder and\njointly-trained speaker encoder. The superiority of Glow-WaveGAN 2 has been\nproved through TTS and VC experiments conducted on LibriTTS corpus and VTCK\ncorpus.", "published": "2022-07-05 06:31:55", "link": "http://arxiv.org/abs/2207.01832v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WeSinger 2: Fully Parallel Singing Voice Synthesis via Multi-Singer\n  Conditional Adversarial Training", "abstract": "This paper aims to introduce a robust singing voice synthesis (SVS) system to\nproduce very natural and realistic singing voices efficiently by leveraging the\nadversarial training strategy. On one hand, we designed simple but generic\nrandom area conditional discriminators to help supervise the acoustic model,\nwhich can effectively avoid the over-smoothed spectrogram prediction and\nimprove the expressiveness of SVS. On the other hand, we subtly combined the\nspectrogram with the frame-level linearly-interpolated F0 sequence as the input\nfor the neural vocoder, which is then optimized with the help of multiple\nadversarial conditional discriminators in the waveform domain and multi-scale\ndistance functions in the frequency domain. The experimental results and\nablation studies concluded that, compared with our previous auto-regressive\nwork, our new system can produce high-quality singing voices efficiently by\nfine-tuning different singing datasets covering from several minutes to a few\nhours. A large number of synthesized songs with different timbres are available\nonline https://zzw922cn.github.io/wesinger2 and we highly recommend readers to\nlisten to them.", "published": "2022-07-05 08:31:02", "link": "http://arxiv.org/abs/2207.01886v8", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Ultra-Low-Bitrate Speech Coding with Pretrained Transformers", "abstract": "Speech coding facilitates the transmission of speech over low-bandwidth\nnetworks with minimal distortion. Neural-network based speech codecs have\nrecently demonstrated significant improvements in quality over traditional\napproaches. While this new generation of codecs is capable of synthesizing\nhigh-fidelity speech, their use of recurrent or convolutional layers often\nrestricts their effective receptive fields, which prevents them from\ncompressing speech efficiently. We propose to further reduce the bitrate of\nneural speech codecs through the use of pretrained Transformers, capable of\nexploiting long-range dependencies in the input signal due to their inductive\nbias. As such, we use a pretrained Transformer in tandem with a convolutional\nencoder, which is trained end-to-end with a quantizer and a generative\nadversarial net decoder. Our numerical experiments show that supplementing the\nconvolutional encoder of a neural speech codec with Transformer speech\nembeddings yields a speech codec with a bitrate of $600\\,\\mathrm{bps}$ that\noutperforms the original neural speech codec in synthesized speech quality when\ntrained at the same bitrate. Subjective human evaluations suggest that the\nquality of the resulting codec is comparable or better than that of\nconventional codecs operating at three to four times the rate.", "published": "2022-07-05 18:52:11", "link": "http://arxiv.org/abs/2207.02262v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
