{"title": "Cross-Language Question Re-Ranking", "abstract": "We study how to find relevant questions in community forums when the language\nof the new questions is different from that of the existing questions in the\nforum. In particular, we explore the Arabic-English language pair. We compare a\nkernel-based system with a feed-forward neural network in a scenario where a\nlarge parallel corpus is available for training a machine translation system,\nbilingual dictionaries, and cross-language word embeddings. We observe that\nboth approaches degrade the performance of the system when working on the\ntranslated text, especially the kernel-based system, which depends heavily on a\nsyntactic kernel. We address this issue using a cross-language tree kernel,\nwhich compares the original Arabic tree to the English trees of the related\nquestions. We show that this kernel almost closes the performance gap with\nrespect to the monolingual system. On the neural network side, we use the\nparallel corpus to train cross-language embeddings, which we then use to\nrepresent the Arabic input and the English related questions in the same space.\nThe results also improve to close to those of the monolingual neural network.\nOverall, the kernel system shows a better performance compared to the neural\nnetwork in all cases.", "published": "2017-10-04 07:23:23", "link": "http://arxiv.org/abs/1710.01487v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Semantic Sentiment Analysis of Twitter Data", "abstract": "Internet and the proliferation of smart mobile devices have changed the way\ninformation is created, shared, and spreads, e.g., microblogs such as Twitter,\nweblogs such as LiveJournal, social networks such as Facebook, and instant\nmessengers such as Skype and WhatsApp are now commonly used to share thoughts\nand opinions about anything in the surrounding world. This has resulted in the\nproliferation of social media content, thus creating new opportunities to study\npublic opinion at a scale that was never possible before. Naturally, this\nabundance of data has quickly attracted business and research interest from\nvarious fields including marketing, political science, and social studies,\namong many others, which are interested in questions like these: Do people like\nthe new Apple Watch? Do Americans support ObamaCare? How do Scottish feel about\nthe Brexit? Answering these questions requires studying the sentiment of\nopinions people express in social media, which has given rise to the fast\ngrowth of the field of sentiment analysis in social media, with Twitter being\nespecially popular for research due to its scale, representativeness, variety\nof topics discussed, as well as ease of public access to its messages. Here we\npresent an overview of work on sentiment analysis on Twitter.", "published": "2017-10-04 07:57:59", "link": "http://arxiv.org/abs/1710.01492v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Discourse Structure in Machine Translation Evaluation", "abstract": "In this article, we explore the potential of using sentence-level discourse\nstructure for machine translation evaluation. We first design discourse-aware\nsimilarity measures, which use all-subtree kernels to compare discourse parse\ntrees in accordance with the Rhetorical Structure Theory (RST). Then, we show\nthat a simple linear combination with these measures can help improve various\nexisting machine translation evaluation metrics regarding correlation with\nhuman judgments both at the segment- and at the system-level. This suggests\nthat discourse information is complementary to the information used by many of\nthe existing evaluation metrics, and thus it could be taken into account when\ndeveloping richer evaluation metrics, such as the WMT-14 winning combined\nmetric DiscoTKparty. We also provide a detailed analysis of the relevance of\nvarious discourse elements and relations from the RST parse trees for machine\ntranslation evaluation. In particular we show that: (i) all aspects of the RST\ntree are relevant, (ii) nuclearity is more useful than relation type, and (iii)\nthe similarity of the translation RST tree to the reference tree is positively\ncorrelated with translation quality.", "published": "2017-10-04 08:28:24", "link": "http://arxiv.org/abs/1710.01504v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Building a Web-Scale Dependency-Parsed Corpus from CommonCrawl", "abstract": "We present DepCC, the largest-to-date linguistically analyzed corpus in\nEnglish including 365 million documents, composed of 252 billion tokens and 7.5\nbillion of named entity occurrences in 14.3 billion sentences from a web-scale\ncrawl of the \\textsc{Common Crawl} project. The sentences are processed with a\ndependency parser and with a named entity tagger and contain provenance\ninformation, enabling various applications ranging from training syntax-based\nword embeddings to open information extraction and question answering. We built\nan index of all sentences and their linguistic meta-data enabling quick search\nacross the corpus. We demonstrate the utility of this corpus on the verb\nsimilarity task by showing that a distributional model trained on our corpus\nyields better results than models trained on smaller corpora, like Wikipedia.\nThis distributional model outperforms the state of art models of verb\nsimilarity trained on smaller corpora on the SimVerb3500 dataset.", "published": "2017-10-04 19:42:37", "link": "http://arxiv.org/abs/1710.01779v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhanced Neural Machine Translation by Learning from Draft", "abstract": "Neural machine translation (NMT) has recently achieved impressive results. A\npotential problem of the existing NMT algorithm, however, is that the decoding\nis conducted from left to right, without considering the right context. This\npaper proposes an two-stage approach to solve the problem. In the first stage,\na conventional attention-based NMT system is used to produce a draft\ntranslation, and in the second stage, a novel double-attention NMT system is\nused to refine the translation, by looking at the original input as well as the\ndraft translation. This drafting-and-refinement can obtain the right-context\ninformation from the draft, hence producing more consistent translations. We\nevaluated this approach using two Chinese-English translation tasks, one with\n44k pairs and 1M pairs respectively. The experiments showed that our approach\nachieved positive improvements over the conventional NMT system: the\nimprovements are 2.4 and 0.9 BLEU points on the small-scale and large-scale\ntasks, respectively.", "published": "2017-10-04 20:13:43", "link": "http://arxiv.org/abs/1710.01789v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Counterfactual Language Model Adaptation for Suggesting Phrases", "abstract": "Mobile devices use language models to suggest words and phrases for use in\ntext entry. Traditional language models are based on contextual word frequency\nin a static corpus of text. However, certain types of phrases, when offered to\nwriters as suggestions, may be systematically chosen more often than their\nfrequency would predict. In this paper, we propose the task of generating\nsuggestions that writers accept, a related but distinct task to making accurate\npredictions. Although this task is fundamentally interactive, we propose a\ncounterfactual setting that permits offline training and evaluation. We find\nthat even a simple language model can capture text characteristics that improve\nacceptability.", "published": "2017-10-04 20:49:52", "link": "http://arxiv.org/abs/1710.01799v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntactic and Semantic Features For Code-Switching Factored Language\n  Models", "abstract": "This paper presents our latest investigations on different features for\nfactored language models for Code-Switching speech and their effect on\nautomatic speech recognition (ASR) performance. We focus on syntactic and\nsemantic features which can be extracted from Code-Switching text data and\nintegrate them into factored language models. Different possible factors, such\nas words, part-of-speech tags, Brown word clusters, open class words and\nclusters of open class word embeddings are explored. The experimental results\nreveal that Brown word clusters, part-of-speech tags and open-class words are\nthe most effective at reducing the perplexity of factored language models on\nthe Mandarin-English Code-Switching corpus SEAME. In ASR experiments, the model\ncontaining Brown word clusters and part-of-speech tags and the model also\nincluding clusters of open class word embeddings yield the best mixed error\nrate results. In summary, the best language model can significantly reduce the\nperplexity on the SEAME evaluation set by up to 10.8% relative and the mixed\nerror rate by up to 3.4% relative.", "published": "2017-10-04 21:21:30", "link": "http://arxiv.org/abs/1710.01809v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crowdsourcing for Beyond Polarity Sentiment Analysis A Pure Emotion\n  Lexicon", "abstract": "Sentiment analysis aims to uncover emotions conveyed through information. In\nits simplest form, it is performed on a polarity basis, where the goal is to\nclassify information with positive or negative emotion. Recent research has\nexplored more nuanced ways to capture emotions that go beyond polarity. For\nthese methods to work, they require a critical resource: a lexicon that is\nappropriate for the task at hand, in terms of the range of emotions it captures\ndiversity. In the past, sentiment analysis lexicons have been created by\nexperts, such as linguists and behavioural scientists, with strict rules.\nLexicon evaluation was also performed by experts or gold standards. In our\npaper, we propose a crowdsourcing method for lexicon acquisition, which is\nscalable, cost-effective, and doesn't require experts or gold standards. We\nalso compare crowd and expert evaluations of the lexicon, to assess the overall\nlexicon quality, and the evaluation capabilities of the crowd.", "published": "2017-10-04 21:38:48", "link": "http://arxiv.org/abs/1710.04203v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Identifying Clickbait: A Multi-Strategy Approach Using Neural Networks", "abstract": "Online media outlets, in a bid to expand their reach and subsequently\nincrease revenue through ad monetisation, have begun adopting clickbait\ntechniques to lure readers to click on articles. The article fails to fulfill\nthe promise made by the headline. Traditional methods for clickbait detection\nhave relied heavily on feature engineering which, in turn, is dependent on the\ndataset it is built for. The application of neural networks for this task has\nonly been explored partially. We propose a novel approach considering all\ninformation found in a social media post. We train a bidirectional LSTM with an\nattention mechanism to learn the extent to which a word contributes to the\npost's clickbait score in a differential manner. We also employ a Siamese net\nto capture the similarity between source and target information. Information\ngleaned from images has not been considered in previous approaches. We learn\nimage embeddings from large amounts of data using Convolutional Neural Networks\nto add another layer of complexity to our model. Finally, we concatenate the\noutputs from the three separate components, serving it as input to a fully\nconnected layer. We conduct experiments over a test corpus of 19538 social\nmedia posts, attaining an F1 score of 65.37% on the dataset bettering the\nprevious state-of-the-art, as well as other proposed approaches, feature\nengineering or otherwise.", "published": "2017-10-04 08:53:12", "link": "http://arxiv.org/abs/1710.01507v4", "categories": ["cs.IR", "cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.IR"}
{"title": "Independent Low-Rank Matrix Analysis Based on Parametric\n  Majorization-Equalization Algorithm", "abstract": "In this paper, we propose a new optimization method for independent low-rank\nmatrix analysis (ILRMA) based on a parametric majorization-equalization\nalgorithm. ILRMA is an efficient blind source separation technique that\nsimultaneously estimates a spatial demixing matrix (spatial model) and the\npower spectrograms of each estimated source (source model). In ILRMA, since\nboth models are alternately optimized by iterative update rules, the difference\nin the convergence speeds between these models often results in a poor local\nsolution. To solve this problem, we introduce a new parameter that controls the\nconvergence speed of the source model and find the best balance between the\noptimizations in the spatial and source models for ILRMA.", "published": "2017-10-04 13:12:44", "link": "http://arxiv.org/abs/1710.01589v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Compression Based Dissimilarity Measure for Music Score\n  Analysis", "abstract": "In this paper, we propose a way to improve the compression based\ndissimilarity measure, CDM. We propose to use a modified value of the file\nsize, where the original CDM uses an unmodified file size. Our application is a\nmusic score analysis. We have chosen piano pieces from five different\ncomposers. We have selected 75 famous pieces (15 pieces for each composer). We\ncomputed the distances among all pieces by using the modified CDM. We use the\nK-nearest neighbor method when we estimate the composer of each piece of music.\nThe modified CDM shows improved accuracy. The difference is statistically\nsignificant.", "published": "2017-10-04 03:11:31", "link": "http://arxiv.org/abs/1710.01446v1", "categories": ["cs.SD", "cs.OH", "eess.AS"], "primary_category": "cs.SD"}
