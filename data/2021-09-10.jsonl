{"title": "Augmenting BERT-style Models with Predictive Coding to Improve\n  Discourse-level Representations", "abstract": "Current language models are usually trained using a self-supervised scheme,\nwhere the main focus is learning representations at the word or sentence level.\nHowever, there has been limited progress in generating useful discourse-level\nrepresentations. In this work, we propose to use ideas from predictive coding\ntheory to augment BERT-style language models with a mechanism that allows them\nto learn suitable discourse-level representations. As a result, our proposed\napproach is able to predict future sentences using explicit top-down\nconnections that operate at the intermediate layers of the network. By\nexperimenting with benchmarks designed to evaluate discourse-related knowledge\nusing pre-trained sentence representations, we demonstrate that our approach\nimproves performance in 6 out of 11 tasks by excelling in discourse\nrelationship detection.", "published": "2021-09-10 00:45:28", "link": "http://arxiv.org/abs/2109.04602v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How May I Help You? Using Neural Text Simplification to Improve\n  Downstream NLP Tasks", "abstract": "The general goal of text simplification (TS) is to reduce text complexity for\nhuman consumption. This paper investigates another potential use of neural TS:\nassisting machines performing natural language processing (NLP) tasks. We\nevaluate the use of neural TS in two ways: simplifying input texts at\nprediction time and augmenting data to provide machines with additional\ninformation during training. We demonstrate that the latter scenario provides\npositive effects on machine performance on two separate datasets. In\nparticular, the latter use of TS improves the performances of LSTM (1.82-1.98%)\nand SpanBERT (0.7-1.3%) extractors on TACRED, a complex, large-scale,\nreal-world relation extraction task. Further, the same setting yields\nimprovements of up to 0.65% matched and 0.62% mismatched accuracies for a BERT\ntext classifier on MNLI, a practical natural language inference dataset.", "published": "2021-09-10 01:04:52", "link": "http://arxiv.org/abs/2109.04604v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IndoBERTweet: A Pretrained Language Model for Indonesian Twitter with\n  Effective Domain-Specific Vocabulary Initialization", "abstract": "We present IndoBERTweet, the first large-scale pretrained model for\nIndonesian Twitter that is trained by extending a monolingually-trained\nIndonesian BERT model with additive domain-specific vocabulary. We focus in\nparticular on efficient model adaptation under vocabulary mismatch, and\nbenchmark different ways of initializing the BERT embedding layer for new word\ntypes. We find that initializing with the average BERT subword embedding makes\npretraining five times faster, and is more effective than proposed methods for\nvocabulary adaptation in terms of extrinsic evaluation over seven Twitter-based\ndatasets.", "published": "2021-09-10 01:27:51", "link": "http://arxiv.org/abs/2109.04607v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Exploratory Study on Long Dialogue Summarization: What Works and\n  What's Next", "abstract": "Dialogue summarization helps readers capture salient information from long\nconversations in meetings, interviews, and TV series. However, real-world\ndialogues pose a great challenge to current summarization models, as the\ndialogue length typically exceeds the input limits imposed by recent\ntransformer-based pre-trained models, and the interactive nature of dialogues\nmakes relevant information more context-dependent and sparsely distributed than\nnews articles. In this work, we perform a comprehensive study on long dialogue\nsummarization by investigating three strategies to deal with the lengthy input\nproblem and locate relevant information: (1) extended transformer models such\nas Longformer, (2) retrieve-then-summarize pipeline models with several\ndialogue utterance retrieval methods, and (3) hierarchical dialogue encoding\nmodels such as HMNet. Our experimental results on three long dialogue datasets\n(QMSum, MediaSum, SummScreen) show that the retrieve-then-summarize pipeline\nmodels yield the best performance. We also demonstrate that the summary quality\ncan be further improved with a stronger retrieval model and pretraining on\nproper external summarization datasets.", "published": "2021-09-10 01:38:26", "link": "http://arxiv.org/abs/2109.04609v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rule-based Morphological Inflection Improves Neural Terminology\n  Translation", "abstract": "Current approaches to incorporating terminology constraints in machine\ntranslation (MT) typically assume that the constraint terms are provided in\ntheir correct morphological forms. This limits their application to real-world\nscenarios where constraint terms are provided as lemmas. In this paper, we\nintroduce a modular framework for incorporating lemma constraints in neural MT\n(NMT) in which linguistic knowledge and diverse types of NMT models can be\nflexibly applied. It is based on a novel cross-lingual inflection module that\ninflects the target lemma constraints based on the source context. We explore\nlinguistically motivated rule-based and data-driven neural-based inflection\nmodules and design English-German health and English-Lithuanian news test\nsuites to evaluate them in domain adaptation and low-resource MT settings.\nResults show that our rule-based inflection module helps NMT models incorporate\nlemma constraints more accurately than a neural module and outperforms the\nexisting end-to-end approach with lower training costs.", "published": "2021-09-10 02:06:48", "link": "http://arxiv.org/abs/2109.04620v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Changes Can Large-scale Language Models Bring? Intensive Study on\n  HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers", "abstract": "GPT-3 shows remarkable in-context learning ability of large-scale language\nmodels (LMs) trained on hundreds of billion scale data. Here we address some\nremaining issues less reported by the GPT-3 paper, such as a non-English LM,\nthe performances of different sized models, and the effect of recently\nintroduced prompt optimization on in-context learning. To achieve this, we\nintroduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric\ncorpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA\nwith our training configuration shows state-of-the-art in-context zero-shot and\nfew-shot learning performances on various downstream tasks in Korean. Also, we\nshow the performance benefits of prompt-based learning and demonstrate how it\ncan be integrated into the prompt engineering pipeline. Then we discuss the\npossibility of materializing the No Code AI paradigm by providing AI\nprototyping capabilities to non-experts of ML by introducing HyperCLOVA studio,\nan interactive prompt engineering interface. Lastly, we demonstrate the\npotential of our methods with three successful in-house applications.", "published": "2021-09-10 03:32:19", "link": "http://arxiv.org/abs/2109.04650v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting emergent linguistic compositions through time: Syntactic\n  frame extension via multimodal chaining", "abstract": "Natural language relies on a finite lexicon to express an unbounded set of\nemerging ideas. One result of this tension is the formation of new\ncompositions, such that existing linguistic units can be combined with emerging\nitems into novel expressions. We develop a framework that exploits the\ncognitive mechanisms of chaining and multimodal knowledge to predict emergent\ncompositional expressions through time. We present the syntactic frame\nextension model (SFEM) that draws on the theory of chaining and knowledge from\n\"percept\", \"concept\", and \"language\" to infer how verbs extend their frames to\nform new compositions with existing and novel nouns. We evaluate SFEM\nrigorously on the 1) modalities of knowledge and 2) categorization models of\nchaining, in a syntactically parsed English corpus over the past 150 years. We\nshow that multimodal SFEM predicts newly emerged verb syntax and arguments\nsubstantially better than competing models using purely linguistic or unimodal\nknowledge. We find support for an exemplar view of chaining as opposed to a\nprototype view and reveal how the joint approach of multimodal chaining may be\nfundamental to the creation of literal and figurative language uses including\nmetaphor and metonymy.", "published": "2021-09-10 03:42:07", "link": "http://arxiv.org/abs/2109.04652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Developing a Multilingual and Code-Mixed Visual Question\n  Answering System by Knowledge Distillation", "abstract": "Pre-trained language-vision models have shown remarkable performance on the\nvisual question answering (VQA) task. However, most pre-trained models are\ntrained by only considering monolingual learning, especially the resource-rich\nlanguage like English. Training such models for multilingual setups demand high\ncomputing resources and multilingual language-vision dataset which hinders\ntheir application in practice. To alleviate these challenges, we propose a\nknowledge distillation approach to extend an English language-vision model\n(teacher) into an equally effective multilingual and code-mixed model\n(student). Unlike the existing knowledge distillation methods, which only use\nthe output from the last layer of the teacher network for distillation, our\nstudent model learns and imitates the teacher from multiple intermediate layers\n(language and vision encoders) with appropriately designed distillation\nobjectives for incremental knowledge extraction. We also create the large-scale\nmultilingual and code-mixed VQA dataset in eleven different language setups\nconsidering the multiple Indian and European languages. Experimental results\nand in-depth analysis show the effectiveness of the proposed VQA model over the\npre-trained language-vision models on eleven diverse language setups.", "published": "2021-09-10 03:47:29", "link": "http://arxiv.org/abs/2109.04653v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Dialogue State Tracking via Cross-Task Transfer", "abstract": "Zero-shot transfer learning for dialogue state tracking (DST) enables us to\nhandle a variety of task-oriented dialogue domains without the expense of\ncollecting in-domain data. In this work, we propose to transfer the\n\\textit{cross-task} knowledge from general question answering (QA) corpora for\nthe zero-shot DST task. Specifically, we propose TransferQA, a transferable\ngenerative QA model that seamlessly combines extractive QA and multi-choice QA\nvia a text-to-text transformer framework, and tracks both categorical slots and\nnon-categorical slots in DST. In addition, we introduce two effective ways to\nconstruct unanswerable questions, namely, negative question sampling and\ncontext truncation, which enable our model to handle \"none\" value slots in the\nzero-shot DST setting. The extensive experiments show that our approaches\nsubstantially improve the existing zero-shot and few-shot results on MultiWoz.\nMoreover, compared to the fully trained baseline on the Schema-Guided Dialogue\ndataset, our approach shows better generalization ability in unseen domains.", "published": "2021-09-10 03:57:56", "link": "http://arxiv.org/abs/2109.04655v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Euphemistic Phrase Detection by Masked Language Model", "abstract": "It is a well-known approach for fringe groups and organizations to use\neuphemisms -- ordinary-sounding and innocent-looking words with a secret\nmeaning -- to conceal what they are discussing. For instance, drug dealers\noften use \"pot\" for marijuana and \"avocado\" for heroin. From a social media\ncontent moderation perspective, though recent advances in NLP have enabled the\nautomatic detection of such single-word euphemisms, no existing work is capable\nof automatically detecting multi-word euphemisms, such as \"blue dream\"\n(marijuana) and \"black tar\" (heroin). Our paper tackles the problem of\neuphemistic phrase detection without human effort for the first time, as far as\nwe are aware. We first perform phrase mining on a raw text corpus (e.g., social\nmedia posts) to extract quality phrases. Then, we utilize word embedding\nsimilarities to select a set of euphemistic phrase candidates. Finally, we rank\nthose candidates by a masked language model -- SpanBERT. Compared to strong\nbaselines, we report 20-50% higher detection accuracies using our algorithm for\ndetecting euphemistic phrases.", "published": "2021-09-10 04:57:30", "link": "http://arxiv.org/abs/2109.04666v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DIALKI: Knowledge Identification in Conversational Systems through\n  Dialogue-Document Contextualization", "abstract": "Identifying relevant knowledge to be used in conversational systems that are\ngrounded in long documents is critical to effective response generation. We\nintroduce a knowledge identification model that leverages the document\nstructure to provide dialogue-contextualized passage encodings and better\nlocate knowledge relevant to the conversation. An auxiliary loss captures the\nhistory of dialogue-document connections. We demonstrate the effectiveness of\nour model on two document-grounded conversational datasets and provide analyses\nshowing generalization to unseen documents and long dialogue contexts.", "published": "2021-09-10 05:40:37", "link": "http://arxiv.org/abs/2109.04673v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Zero-shot Neural Machine Translation: From a Perspective of\n  Latent Variables", "abstract": "Zero-shot translation, directly translating between language pairs unseen in\ntraining, is a promising capability of multilingual neural machine translation\n(NMT). However, it usually suffers from capturing spurious correlations between\nthe output language and language invariant semantics due to the maximum\nlikelihood training objective, leading to poor transfer performance on\nzero-shot translation. In this paper, we introduce a denoising autoencoder\nobjective based on pivot language into traditional training objective to\nimprove the translation accuracy on zero-shot directions. The theoretical\nanalysis from the perspective of latent variables shows that our approach\nactually implicitly maximizes the probability distributions for zero-shot\ndirections. On two benchmark machine translation datasets, we demonstrate that\nthe proposed method is able to effectively eliminate the spurious correlations\nand significantly outperforms state-of-the-art methods with a remarkable\nperformance. Our code is available at https://github.com/Victorwz/zs-nmt-dae.", "published": "2021-09-10 07:18:53", "link": "http://arxiv.org/abs/2109.04705v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-train or Annotate? Domain Adaptation with a Constrained Budget", "abstract": "Recent work has demonstrated that pre-training in-domain language models can\nboost performance when adapting to a new domain. However, the costs associated\nwith pre-training raise an important question: given a fixed budget, what steps\nshould an NLP practitioner take to maximize performance? In this paper, we view\ndomain adaptation with a constrained budget as a consumer choice problem, where\nthe goal is to select an optimal combination of data annotation and\npre-training. We measure annotation costs of three procedural text datasets,\nalong with the pre-training costs of several in-domain language models. The\nutility of different combinations of pre-training and data annotation are\nevaluated under varying budget constraints to assess which combination strategy\nworks best. We find that for small budgets, spending all funds on annotation\nleads to the best performance; once the budget becomes large enough, however, a\ncombination of data annotation and in-domain pre-training yields better\nperformance. Our experiments suggest task-specific data annotation should be\npart of an economical strategy when adapting an NLP model to a new domain.", "published": "2021-09-10 07:28:26", "link": "http://arxiv.org/abs/2109.04711v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Balancing Methods for Multi-label Text Classification with Long-Tailed\n  Class Distribution", "abstract": "Multi-label text classification is a challenging task because it requires\ncapturing label dependencies. It becomes even more challenging when class\ndistribution is long-tailed. Resampling and re-weighting are common approaches\nused for addressing the class imbalance problem, however, they are not\neffective when there is label dependency besides class imbalance because they\nresult in oversampling of common labels. Here, we introduce the application of\nbalancing loss functions for multi-label text classification. We perform\nexperiments on a general domain dataset with 90 labels (Reuters-21578) and a\ndomain-specific dataset from PubMed with 18211 labels. We find that a\ndistribution-balanced loss function, which inherently addresses both the class\nimbalance and label linkage problems, outperforms commonly used loss functions.\nDistribution balancing methods have been successfully used in the image\nrecognition field. Here, we show their effectiveness in natural language\nprocessing. Source code is available at\nhttps://github.com/Roche/BalancedLossNLP.", "published": "2021-09-10 07:39:10", "link": "http://arxiv.org/abs/2109.04712v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AfroMT: Pretraining Strategies and Reproducible Benchmarks for\n  Translation of 8 African Languages", "abstract": "Reproducible benchmarks are crucial in driving progress of machine\ntranslation research. However, existing machine translation benchmarks have\nbeen mostly limited to high-resource or well-represented languages. Despite an\nincreasing interest in low-resource machine translation, there are no\nstandardized reproducible benchmarks for many African languages, many of which\nare used by millions of speakers but have less digitized textual data. To\ntackle these challenges, we propose AfroMT, a standardized, clean, and\nreproducible machine translation benchmark for eight widely spoken African\nlanguages. We also develop a suite of analysis tools for system diagnosis\ntaking into account the unique properties of these languages. Furthermore, we\nexplore the newly considered case of low-resource focused pretraining and\ndevelop two novel data augmentation-based strategies, leveraging word-level\nalignment information and pseudo-monolingual data for pretraining multilingual\nsequence-to-sequence models. We demonstrate significant improvements when\npretraining on 11 languages, with gains of up to 2 BLEU points over strong\nbaselines. We also show gains of up to 12 BLEU points over cross-lingual\ntransfer baselines in data-constrained scenarios. All code and pretrained\nmodels will be released as further steps towards larger reproducible benchmarks\nfor African languages.", "published": "2021-09-10 07:45:21", "link": "http://arxiv.org/abs/2109.04715v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Reliability of Word Embedding Gender Bias Measures", "abstract": "Various measures have been proposed to quantify human-like social biases in\nword embeddings. However, bias scores based on these measures can suffer from\nmeasurement error. One indication of measurement quality is reliability,\nconcerning the extent to which a measure produces consistent results. In this\npaper, we assess three types of reliability of word embedding gender bias\nmeasures, namely test-retest reliability, inter-rater consistency and internal\nconsistency. Specifically, we investigate the consistency of bias scores across\ndifferent choices of random seeds, scoring rules and words. Furthermore, we\nanalyse the effects of various factors on these measures' reliability scores.\nOur findings inform better design of word embedding gender bias measures.\nMoreover, we urge researchers to be more critical about the application of such\nmeasures.", "published": "2021-09-10 08:23:50", "link": "http://arxiv.org/abs/2109.04732v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Genre as Weak Supervision for Cross-lingual Dependency Parsing", "abstract": "Recent work has shown that monolingual masked language models learn to\nrepresent data-driven notions of language variation which can be used for\ndomain-targeted training data selection. Dataset genre labels are already\nfrequently available, yet remain largely unexplored in cross-lingual setups. We\nharness this genre metadata as a weak supervision signal for targeted data\nselection in zero-shot dependency parsing. Specifically, we project\ntreebank-level genre information to the finer-grained sentence level, with the\ngoal to amplify information implicitly stored in unsupervised contextualized\nrepresentations. We demonstrate that genre is recoverable from multilingual\ncontextual embeddings and that it provides an effective signal for training\ndata selection in cross-lingual, zero-shot scenarios. For 12 low-resource\nlanguage treebanks, six of which are test-only, our genre-specific methods\nsignificantly outperform competitive baselines as well as recent\nembedding-based methods for data selection. Moreover, genre-based data\nselection provides new state-of-the-art results for three of these target\nlanguages.", "published": "2021-09-10 08:24:54", "link": "http://arxiv.org/abs/2109.04733v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Does Fine-tuning Affect the Geometry of Embedding Space: A Case\n  Study on Isotropy", "abstract": "It is widely accepted that fine-tuning pre-trained language models usually\nbrings about performance improvements in downstream tasks. However, there are\nlimited studies on the reasons behind this effectiveness, particularly from the\nviewpoint of structural changes in the embedding space. Trying to fill this\ngap, in this paper, we analyze the extent to which the isotropy of the\nembedding space changes after fine-tuning. We demonstrate that, even though\nisotropy is a desirable geometrical property, fine-tuning does not necessarily\nresult in isotropy enhancements. Moreover, local structures in pre-trained\ncontextual word representations (CWRs), such as those encoding token types or\nfrequency, undergo a massive change during fine-tuning. Our experiments show\ndramatic growth in the number of elongated directions in the embedding space,\nwhich, in contrast to pre-trained CWRs, carry the essential linguistic\nknowledge in the fine-tuned embedding space, making existing isotropy\nenhancement methods ineffective.", "published": "2021-09-10 08:58:59", "link": "http://arxiv.org/abs/2109.04740v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Strong Baseline for Query Efficient Attacks in a Black Box Setting", "abstract": "Existing black box search methods have achieved high success rate in\ngenerating adversarial attacks against NLP models. However, such search methods\nare inefficient as they do not consider the amount of queries required to\ngenerate adversarial attacks. Also, prior attacks do not maintain a consistent\nsearch space while comparing different search methods. In this paper, we\npropose a query efficient attack strategy to generate plausible adversarial\nexamples on text classification and entailment tasks. Our attack jointly\nleverages attention mechanism and locality sensitive hashing (LSH) to reduce\nthe query count. We demonstrate the efficacy of our approach by comparing our\nattack with four baselines across three different search spaces. Further, we\nbenchmark our results across the same search space used in prior attacks. In\ncomparison to attacks proposed, on an average, we are able to reduce the query\ncount by 75% across all datasets and target models. We also demonstrate that\nour attack achieves a higher success rate when compared to prior attacks in a\nlimited query setting.", "published": "2021-09-10 10:46:32", "link": "http://arxiv.org/abs/2109.04775v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RoR: Read-over-Read for Long Document Machine Reading Comprehension", "abstract": "Transformer-based pre-trained models, such as BERT, have achieved remarkable\nresults on machine reading comprehension. However, due to the constraint of\nencoding length (e.g., 512 WordPiece tokens), a long document is usually split\ninto multiple chunks that are independently read. It results in the reading\nfield being limited to individual chunks without information collaboration for\nlong document machine reading comprehension. To address this problem, we\npropose RoR, a read-over-read method, which expands the reading field from\nchunk to document. Specifically, RoR includes a chunk reader and a document\nreader. The former first predicts a set of regional answers for each chunk,\nwhich are then compacted into a highly-condensed version of the original\ndocument, guaranteeing to be encoded once. The latter further predicts the\nglobal answers from this condensed document. Eventually, a voting strategy is\nutilized to aggregate and rerank the regional and global answers for final\nprediction. Extensive experiments on two benchmarks QuAC and TriviaQA\ndemonstrate the effectiveness of RoR for long document reading. Notably, RoR\nranks 1st place on the QuAC leaderboard (https://quac.ai/) at the time of\nsubmission (May 17th, 2021).", "published": "2021-09-10 10:55:22", "link": "http://arxiv.org/abs/2109.04780v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exophoric Pronoun Resolution in Dialogues with Topic Regularization", "abstract": "Resolving pronouns to their referents has long been studied as a fundamental\nnatural language understanding problem. Previous works on pronoun coreference\nresolution (PCR) mostly focus on resolving pronouns to mentions in text while\nignoring the exophoric scenario. Exophoric pronouns are common in daily\ncommunications, where speakers may directly use pronouns to refer to some\nobjects present in the environment without introducing the objects first.\nAlthough such objects are not mentioned in the dialogue text, they can often be\ndisambiguated by the general topics of the dialogue. Motivated by this, we\npropose to jointly leverage the local context and global topics of dialogues to\nsolve the out-of-text PCR problem. Extensive experiments demonstrate the\neffectiveness of adding topic regularization for resolving exophoric pronouns.", "published": "2021-09-10 11:08:31", "link": "http://arxiv.org/abs/2109.04787v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixture-of-Partitions: Infusing Large Biomedical Knowledge Graphs into\n  BERT", "abstract": "Infusing factual knowledge into pre-trained models is fundamental for many\nknowledge-intensive tasks. In this paper, we proposed Mixture-of-Partitions\n(MoP), an infusion approach that can handle a very large knowledge graph (KG)\nby partitioning it into smaller sub-graphs and infusing their specific\nknowledge into various BERT models using lightweight adapters. To leverage the\noverall factual knowledge for a target task, these sub-graph adapters are\nfurther fine-tuned along with the underlying BERT through a mixture layer. We\nevaluate our MoP with three biomedical BERTs (SciBERT, BioBERT, PubmedBERT) on\nsix downstream tasks (inc. NLI, QA, Classification), and the results show that\nour MoP consistently enhances the underlying BERTs in task performance, and\nachieves new SOTA performances on five evaluated datasets.", "published": "2021-09-10 11:54:25", "link": "http://arxiv.org/abs/2109.04810v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does It Capture STEL? A Modular, Similarity-based Linguistic Style\n  Evaluation Framework", "abstract": "Style is an integral part of natural language. However, evaluation methods\nfor style measures are rare, often task-specific and usually do not control for\ncontent. We propose the modular, fine-grained and content-controlled\nsimilarity-based STyle EvaLuation framework (STEL) to test the performance of\nany model that can compare two sentences on style. We illustrate STEL with two\ngeneral dimensions of style (formal/informal and simple/complex) as well as two\nspecific characteristics of style (contrac'tion and numb3r substitution). We\nfind that BERT-based methods outperform simple versions of commonly used style\nmeasures like 3-grams, punctuation frequency and LIWC-based approaches. We\ninvite the addition of further tasks and task instances to STEL and hope to\nfacilitate the improvement of style-sensitive measures.", "published": "2021-09-10 12:03:19", "link": "http://arxiv.org/abs/2109.04817v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Asking It All: Generating Contextualized Questions for any Semantic Role", "abstract": "Asking questions about a situation is an inherent step towards understanding\nit. To this end, we introduce the task of role question generation, which,\ngiven a predicate mention and a passage, requires producing a set of questions\nasking about all possible semantic roles of the predicate. We develop a\ntwo-stage model for this task, which first produces a context-independent\nquestion prototype for each role and then revises it to be contextually\nappropriate for the passage. Unlike most existing approaches to question\ngeneration, our approach does not require conditioning on existing answers in\nthe text. Instead, we condition on the type of information to inquire about,\nregardless of whether the answer appears explicitly in the text, could be\ninferred from it, or should be sought elsewhere. Our evaluation demonstrates\nthat we generate diverse and well-formed questions for a large, broad-coverage\nontology of predicates and roles.", "published": "2021-09-10 12:31:14", "link": "http://arxiv.org/abs/2109.04832v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoPHE: A Count-Preserving Hierarchical Evaluation Metric in Large-Scale\n  Multi-Label Text Classification", "abstract": "Large-Scale Multi-Label Text Classification (LMTC) includes tasks with\nhierarchical label spaces, such as automatic assignment of ICD-9 codes to\ndischarge summaries. Performance of models in prior art is evaluated with\nstandard precision, recall, and F1 measures without regard for the rich\nhierarchical structure. In this work we argue for hierarchical evaluation of\nthe predictions of neural LMTC models. With the example of the ICD-9 ontology\nwe describe a structural issue in the representation of the structured label\nspace in prior art, and propose an alternative representation based on the\ndepth of the ontology. We propose a set of metrics for hierarchical evaluation\nusing the depth-based representation. We compare the evaluation scores from the\nproposed metrics with previously used metrics on prior art LMTC models for\nICD-9 coding in MIMIC-III. We also propose further avenues of research\ninvolving the proposed ontological representation.", "published": "2021-09-10 13:09:12", "link": "http://arxiv.org/abs/2109.04853v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Studying word order through iterative shuffling", "abstract": "As neural language models approach human performance on NLP benchmark tasks,\ntheir advances are widely seen as evidence of an increasingly complex\nunderstanding of syntax. This view rests upon a hypothesis that has not yet\nbeen empirically tested: that word order encodes meaning essential to\nperforming these tasks. We refute this hypothesis in many cases: in the GLUE\nsuite and in various genres of English text, the words in a sentence or phrase\ncan rarely be permuted to form a phrase carrying substantially different\ninformation. Our surprising result relies on inference by iterative shuffling\n(IBIS), a novel, efficient procedure that finds the ordering of a bag of words\nhaving the highest likelihood under a fixed language model. IBIS can use any\nblack-box model without additional training and is superior to existing word\nordering algorithms. Coalescing our findings, we discuss how shuffling\ninference procedures such as IBIS can benefit language modeling and constrained\ngeneration.", "published": "2021-09-10 13:27:06", "link": "http://arxiv.org/abs/2109.04867v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document-level Entity-based Extraction as Template Generation", "abstract": "Document-level entity-based extraction (EE), aiming at extracting\nentity-centric information such as entity roles and entity relations, is key to\nautomatic knowledge acquisition from text corpora for various domains. Most\ndocument-level EE systems build extractive models, which struggle to model\nlong-term dependencies among entities at the document level. To address this\nissue, we propose a generative framework for two document-level EE tasks:\nrole-filler entity extraction (REE) and relation extraction (RE). We first\nformulate them as a template generation problem, allowing models to efficiently\ncapture cross-entity dependencies, exploit label semantics, and avoid the\nexponential computation complexity of identifying N-ary relations. A novel\ncross-attention guided copy mechanism, TopK Copy, is incorporated into a\npre-trained sequence-to-sequence model to enhance the capabilities of\nidentifying key information in the input document. Experiments done on the\nMUC-4 and SciREX dataset show new state-of-the-art results on REE (+3.26%),\nbinary RE (+4.8%), and 4-ary RE (+2.7%) in F1 score.", "published": "2021-09-10 14:18:22", "link": "http://arxiv.org/abs/2109.04901v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EmoWOZ: A Large-Scale Corpus and Labelling Scheme for Emotion\n  Recognition in Task-Oriented Dialogue Systems", "abstract": "The ability to recognise emotions lends a conversational artificial\nintelligence a human touch. While emotions in chit-chat dialogues have received\nsubstantial attention, emotions in task-oriented dialogues remain largely\nunaddressed. This is despite emotions and dialogue success having equally\nimportant roles in a natural system. Existing emotion-annotated task-oriented\ncorpora are limited in size, label richness, and public availability, creating\na bottleneck for downstream tasks. To lay a foundation for studies on emotions\nin task-oriented dialogues, we introduce EmoWOZ, a large-scale manually\nemotion-annotated corpus of task-oriented dialogues. EmoWOZ is based on\nMultiWOZ, a multi-domain task-oriented dialogue dataset. It contains more than\n11K dialogues with more than 83K emotion annotations of user utterances. In\naddition to Wizard-of-Oz dialogues from MultiWOZ, we collect human-machine\ndialogues within the same set of domains to sufficiently cover the space of\nvarious emotions that can happen during the lifetime of a data-driven dialogue\nsystem. To the best of our knowledge, this is the first large-scale open-source\ncorpus of its kind. We propose a novel emotion labelling scheme, which is\ntailored to task-oriented dialogues. We report a set of experimental results to\nshow the usability of this corpus for emotion recognition and state tracking in\ntask-oriented dialogues.", "published": "2021-09-10 15:00:01", "link": "http://arxiv.org/abs/2109.04919v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond the Tip of the Iceberg: Assessing Coherence of Text Classifiers", "abstract": "As large-scale, pre-trained language models achieve human-level and\nsuperhuman accuracy on existing language understanding tasks, statistical bias\nin benchmark data and probing studies have recently called into question their\ntrue capabilities. For a more informative evaluation than accuracy on text\nclassification tasks can offer, we propose evaluating systems through a novel\nmeasure of prediction coherence. We apply our framework to two existing\nlanguage understanding benchmarks with different properties to demonstrate its\nversatility. Our experimental results show that this evaluation framework,\nalthough simple in ideas and implementation, is a quick, effective, and\nversatile measure to provide insight into the coherence of machines'\npredictions.", "published": "2021-09-10 15:04:23", "link": "http://arxiv.org/abs/2109.04922v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Human Sentence Processing with Left-Corner Recurrent Neural\n  Network Grammars", "abstract": "In computational linguistics, it has been shown that hierarchical structures\nmake language models (LMs) more human-like. However, the previous literature\nhas been agnostic about a parsing strategy of the hierarchical models. In this\npaper, we investigated whether hierarchical structures make LMs more\nhuman-like, and if so, which parsing strategy is most cognitively plausible. In\norder to address this question, we evaluated three LMs against human reading\ntimes in Japanese with head-final left-branching structures: Long Short-Term\nMemory (LSTM) as a sequential model and Recurrent Neural Network Grammars\n(RNNGs) with top-down and left-corner parsing strategies as hierarchical\nmodels. Our computational modeling demonstrated that left-corner RNNGs\noutperformed top-down RNNGs and LSTM, suggesting that hierarchical and\nleft-corner architectures are more cognitively plausible than top-down or\nsequential architectures. In addition, the relationships between the cognitive\nplausibility and (i) perplexity, (ii) parsing, and (iii) beam size will also be\ndiscussed.", "published": "2021-09-10 15:35:00", "link": "http://arxiv.org/abs/2109.04939v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense\n  Language Understanding", "abstract": "Large-scale, pre-trained language models (LMs) have achieved human-level\nperformance on a breadth of language understanding tasks. However, evaluations\nonly based on end task performance shed little light on machines' true ability\nin language understanding and reasoning. In this paper, we highlight the\nimportance of evaluating the underlying reasoning process in addition to end\nperformance. Toward this goal, we introduce Tiered Reasoning for Intuitive\nPhysics (TRIP), a novel commonsense reasoning dataset with dense annotations\nthat enable multi-tiered evaluation of machines' reasoning process. Our\nempirical results show that while large LMs can achieve high end performance,\nthey struggle to support their predictions with valid supporting evidence. The\nTRIP dataset and our baseline results will motivate verifiable evaluation of\ncommonsense reasoning and facilitate future research toward developing better\nlanguage understanding and reasoning models.", "published": "2021-09-10 15:47:22", "link": "http://arxiv.org/abs/2109.04947v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "We went to look for meaning and all we got were these lousy\n  representations: aspects of meaning representation for computational\n  semantics", "abstract": "In this paper we examine different meaning representations that are commonly\nused in different natural language applications today and discuss their limits,\nboth in terms of the aspects of the natural language meaning they are modelling\nand in terms of the aspects of the application for which they are used.", "published": "2021-09-10 15:51:38", "link": "http://arxiv.org/abs/2109.04949v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlled Neural Sentence-Level Reframing of News Articles", "abstract": "Framing a news article means to portray the reported event from a specific\nperspective, e.g., from an economic or a health perspective. Reframing means to\nchange this perspective. Depending on the audience or the submessage, reframing\ncan become necessary to achieve the desired effect on the readers. Reframing is\nrelated to adapting style and sentiment, which can be tackled with neural text\ngeneration techniques. However, it is more challenging since changing a frame\nrequires rewriting entire sentences rather than single phrases. In this paper,\nwe study how to computationally reframe sentences in news articles while\nmaintaining their coherence to the context. We treat reframing as a\nsentence-level fill-in-the-blank task for which we train neural models on an\nexisting media frame corpus. To guide the training, we propose three\nstrategies: framed-language pretraining, named-entity preservation, and\nadversarial learning. We evaluate respective models automatically and manually\nfor topic consistency, coherence, and successful reframing. Our results\nindicate that generating properly-framed text works well but with tradeoffs.", "published": "2021-09-10 15:57:24", "link": "http://arxiv.org/abs/2109.04957v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BiSECT: Learning to Split and Rephrase Sentences with Bitexts", "abstract": "An important task in NLP applications such as sentence simplification is the\nability to take a long, complex sentence and split it into shorter sentences,\nrephrasing as necessary. We introduce a novel dataset and a new model for this\n`split and rephrase' task. Our BiSECT training data consists of 1 million long\nEnglish sentences paired with shorter, meaning-equivalent English sentences. We\nobtain these by extracting 1-2 sentence alignments in bilingual parallel\ncorpora and then using machine translation to convert both sides of the corpus\ninto the same language. BiSECT contains higher quality training examples than\nprevious Split and Rephrase corpora, with sentence splits that require more\nsignificant modifications. We categorize examples in our corpus, and use these\ncategories in a novel model that allows us to target specific regions of the\ninput sentence to be split and edited. Moreover, we show that models trained on\nBiSECT can perform a wider variety of split operations and improve upon\nprevious state-of-the-art approaches in automatic and human evaluations.", "published": "2021-09-10 17:30:14", "link": "http://arxiv.org/abs/2109.05006v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reference-Centric Models for Grounded Collaborative Dialogue", "abstract": "We present a grounded neural dialogue model that successfully collaborates\nwith people in a partially-observable reference game. We focus on a setting\nwhere two agents each observe an overlapping part of a world context and need\nto identify and agree on some object they share. Therefore, the agents should\npool their information and communicate pragmatically to solve the task. Our\ndialogue agent accurately grounds referents from the partner's utterances using\na structured reference resolver, conditions on these referents using a\nrecurrent memory, and uses a pragmatic generation procedure to ensure the\npartner can resolve the references the agent produces. We evaluate on the\nOneCommon spatial grounding dialogue task (Udagawa and Aizawa 2019), involving\na number of dots arranged on a board with continuously varying positions,\nsizes, and shades. Our agent substantially outperforms the previous state of\nthe art for the task, obtaining a 20% relative improvement in successful task\ncompletion in self-play evaluations and a 50% relative improvement in success\nin human evaluations.", "published": "2021-09-10 18:03:54", "link": "http://arxiv.org/abs/2109.05042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Self-Disclosure In Neural Dialog Models By Candidate\n  Re-ranking", "abstract": "Neural language modelling has progressed the state-of-the-art in different\ndownstream Natural Language Processing (NLP) tasks. One such area is of\nopen-domain dialog modelling, neural dialog models based on GPT-2 such as\nDialoGPT have shown promising performance in single-turn conversation. However,\nsuch (neural) dialog models have been criticized for generating responses which\nalthough may have relevance to the previous human response, tend to quickly\ndissipate human interest and descend into trivial conversation. One reason for\nsuch performance is the lack of explicit conversation strategy being employed\nin human-machine conversation. Humans employ a range of conversation strategies\nwhile engaging in a conversation, one such key social strategies is\nSelf-disclosure(SD). A phenomenon of revealing information about one-self to\nothers. Social penetration theory (SPT) proposes that communication between two\npeople moves from shallow to deeper levels as the relationship progresses\nprimarily through self-disclosure. Disclosure helps in creating rapport among\nthe participants engaged in a conversation. In this paper, Self-disclosure\nenhancement architecture (SDEA) is introduced utilizing Self-disclosure Topic\nModel (SDTM) during inference stage of a neural dialog model to re-rank\nresponse candidates to enhance self-disclosure in single-turn responses from\nfrom the model.", "published": "2021-09-10 20:06:27", "link": "http://arxiv.org/abs/2109.05090v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Zero-shot Commonsense Reasoning with Self-supervised Refinement\n  of Language Models", "abstract": "Can we get existing language models and refine them for zero-shot commonsense\nreasoning? This paper presents an initial study exploring the feasibility of\nzero-shot commonsense reasoning for the Winograd Schema Challenge by\nformulating the task as self-supervised refinement of a pre-trained language\nmodel. In contrast to previous studies that rely on fine-tuning annotated\ndatasets, we seek to boost conceptualization via loss landscape refinement. To\nthis end, we propose a novel self-supervised learning approach that refines the\nlanguage model utilizing a set of linguistic perturbations of similar concept\nrelationships. Empirical analysis of our conceptually simple framework\ndemonstrates the viability of zero-shot commonsense reasoning on multiple\nbenchmarks.", "published": "2021-09-10 21:02:24", "link": "http://arxiv.org/abs/2109.05105v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention-based Contrastive Learning for Winograd Schemas", "abstract": "Self-supervised learning has recently attracted considerable attention in the\nNLP community for its ability to learn discriminative features using a\ncontrastive objective. This paper investigates whether contrastive learning can\nbe extended to Transfomer attention to tackling the Winograd Schema Challenge.\nTo this end, we propose a novel self-supervised framework, leveraging a\ncontrastive loss directly at the level of self-attention. Experimental analysis\nof our attention-based models on multiple datasets demonstrates superior\ncommonsense reasoning capabilities. The proposed approach outperforms all\ncomparable unsupervised approaches while occasionally surpassing supervised\nones.", "published": "2021-09-10 21:10:22", "link": "http://arxiv.org/abs/2109.05108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Latent Tree Induction with Distant Supervision via Span\n  Constraints", "abstract": "For over thirty years, researchers have developed and analyzed methods for\nlatent tree induction as an approach for unsupervised syntactic parsing.\nNonetheless, modern systems still do not perform well enough compared to their\nsupervised counterparts to have any practical use as structural annotation of\ntext. In this work, we present a technique that uses distant supervision in the\nform of span constraints (i.e. phrase bracketing) to improve performance in\nunsupervised constituency parsing. Using a relatively small number of span\nconstraints we can substantially improve the output from DIORA, an already\ncompetitive unsupervised parsing system. Compared with full parse tree\nannotation, span constraints can be acquired with minimal effort, such as with\na lexicon derived from Wikipedia, to find exact text matches. Our experiments\nshow span constraints based on entities improves constituency parsing on\nEnglish WSJ Penn Treebank by more than 5 F1. Furthermore, our method extends to\nany domain where span constraints are easily attainable, and as a case study we\ndemonstrate its effectiveness by parsing biomedical text from the CRAFT\ndataset.", "published": "2021-09-10 21:22:09", "link": "http://arxiv.org/abs/2109.05112v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "D-REX: Dialogue Relation Extraction with Explanations", "abstract": "Existing research studies on cross-sentence relation extraction in long-form\nmulti-party conversations aim to improve relation extraction without\nconsidering the explainability of such methods. This work addresses that gap by\nfocusing on extracting explanations that indicate that a relation exists while\nusing only partially labeled data. We propose our model-agnostic framework,\nD-REX, a policy-guided semi-supervised algorithm that explains and ranks\nrelations. We frame relation extraction as a re-ranking task and include\nrelation- and entity-specific explanations as an intermediate step of the\ninference process. We find that about 90% of the time, human annotators prefer\nD-REX's explanations over a strong BERT-based joint relation extraction and\nexplanation model. Finally, our evaluations on a dialogue relation extraction\ndataset show that our method is simple yet effective and achieves a\nstate-of-the-art F1 score on relation extraction, improving upon existing\nmethods by 13.5%.", "published": "2021-09-10 22:30:48", "link": "http://arxiv.org/abs/2109.05126v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CINS: Comprehensive Instruction for Few-shot Learning in Task-oriented\n  Dialog Systems", "abstract": "As labeling cost for different modules in task-oriented dialog (ToD) systems\nis high, a major challenge in practice is to learn different tasks with the\nleast amount of labeled data. Recently, prompting methods over pre-trained\nlanguage models (PLMs) have shown promising results for few-shot learning in\nToD. To better utilize the power of PLMs, this paper proposes Comprehensive\nInstruction (CINS) that exploits PLMs with extra task-specific instructions. We\ndesign a schema (definition, constraint, prompt) of instructions and their\ncustomized realizations for three important downstream tasks in ToD, i.e.\nintent classification, dialog state tracking, and natural language generation.\nA sequence-to-sequence model (T5) is adopted to solve these three tasks in a\nunified framework. Extensive experiments are conducted on these ToD tasks in\nrealistic few-shot learning scenarios with small validation data. Empirical\nresults demonstrate that the proposed CINS approach consistently improves\ntechniques that finetune PLMs with raw input or short prompts.", "published": "2021-09-10 03:23:06", "link": "http://arxiv.org/abs/2109.04645v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Heterogeneous Graph Neural Networks for Keyphrase Generation", "abstract": "The encoder-decoder framework achieves state-of-the-art results in keyphrase\ngeneration (KG) tasks by predicting both present keyphrases that appear in the\nsource document and absent keyphrases that do not. However, relying solely on\nthe source document can result in generating uncontrollable and inaccurate\nabsent keyphrases. To address these problems, we propose a novel graph-based\nmethod that can capture explicit knowledge from related references. Our model\nfirst retrieves some document-keyphrases pairs similar to the source document\nfrom a pre-defined index as references. Then a heterogeneous graph is\nconstructed to capture relationships of different granularities between the\nsource document and its references. To guide the decoding process, a\nhierarchical attention and copy mechanism is introduced, which directly copies\nappropriate words from both the source document and its references based on\ntheir relevance and significance. The experimental results on multiple KG\nbenchmarks show that the proposed model achieves significant improvements\nagainst other baseline models, especially with regard to the absent keyphrase\nprediction.", "published": "2021-09-10 07:17:07", "link": "http://arxiv.org/abs/2109.04703v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge-Aware Meta-learning for Low-Resource Text Classification", "abstract": "Meta-learning has achieved great success in leveraging the historical learned\nknowledge to facilitate the learning process of the new task. However, merely\nlearning the knowledge from the historical tasks, adopted by current\nmeta-learning algorithms, may not generalize well to testing tasks when they\nare not well-supported by training tasks. This paper studies a low-resource\ntext classification problem and bridges the gap between meta-training and\nmeta-testing tasks by leveraging the external knowledge bases. Specifically, we\npropose KGML to introduce additional representation for each sentence learned\nfrom the extracted sentence-specific knowledge graph. The extensive experiments\non three datasets demonstrate the effectiveness of KGML under both supervised\nadaptation and unsupervised adaptation settings.", "published": "2021-09-10 07:20:43", "link": "http://arxiv.org/abs/2109.04707v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dynamic Terminology Integration for COVID-19 and other Emerging Domains", "abstract": "The majority of language domains require prudent use of terminology to ensure\nclarity and adequacy of information conveyed. While the correct use of\nterminology for some languages and domains can be achieved by adapting\ngeneral-purpose MT systems on large volumes of in-domain parallel data, such\nquantities of domain-specific data are seldom available for less-resourced\nlanguages and niche domains. Furthermore, as exemplified by COVID-19 recently,\nno domain-specific parallel data is readily available for emerging domains.\nHowever, the gravity of this recent calamity created a high demand for reliable\ntranslation of critical information regarding pandemic and infection\nprevention. This work is part of WMT2021 Shared Task: Machine Translation using\nTerminologies, where we describe Tilde MT systems that are capable of dynamic\nterminology integration at the time of translation. Our systems achieve up to\n94% COVID-19 term use accuracy on the test set of the EN-FR language pair\nwithout having access to any form of in-domain information during system\ntraining. We conclude our work with a broader discussion considering the Shared\nTask itself and terminology translation in MT.", "published": "2021-09-10 07:23:55", "link": "http://arxiv.org/abs/2109.04708v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "AutoTriggER: Label-Efficient and Robust Named Entity Recognition with\n  Auxiliary Trigger Extraction", "abstract": "Deep neural models for named entity recognition (NER) have shown impressive\nresults in overcoming label scarcity and generalizing to unseen entities by\nleveraging distant supervision and auxiliary information such as explanations.\nHowever, the costs of acquiring such additional information are generally\nprohibitive. In this paper, we present a novel two-stage framework\n(AutoTriggER) to improve NER performance by automatically generating and\nleveraging ``entity triggers'' which are human-readable cues in the text that\nhelp guide the model to make better decisions. Our framework leverages post-hoc\nexplanation to generate rationales and strengthens a model's prior knowledge\nusing an embedding interpolation technique. This approach allows models to\nexploit triggers to infer entity boundaries and types instead of solely\nmemorizing the entity words themselves. Through experiments on three\nwell-studied NER datasets, AutoTriggER shows strong label-efficiency, is\ncapable of generalizing to unseen entities, and outperforms the RoBERTa-CRF\nbaseline by nearly 0.5 F1 points on average.", "published": "2021-09-10 08:11:56", "link": "http://arxiv.org/abs/2109.04726v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Simple and Effective Method To Eliminate the Self Language Bias in\n  Multilingual Representations", "abstract": "Language agnostic and semantic-language information isolation is an emerging\nresearch direction for multilingual representations models. We explore this\nproblem from a novel angle of geometric algebra and semantic space. A simple\nbut highly effective method \"Language Information Removal (LIR)\" factors out\nlanguage identity information from semantic related components in multilingual\nrepresentations pre-trained on multi-monolingual data. A post-training and\nmodel-agnostic method, LIR only uses simple linear operations, e.g. matrix\nfactorization and orthogonal projection. LIR reveals that for weak-alignment\nmultilingual systems, the principal components of semantic spaces primarily\nencodes language identity information. We first evaluate the LIR on a\ncross-lingual question answer retrieval task (LAReQA), which requires the\nstrong alignment for the multilingual embedding space. Experiment shows that\nLIR is highly effectively on this task, yielding almost 100% relative\nimprovement in MAP for weak-alignment models. We then evaluate the LIR on\nAmazon Reviews and XEVAL dataset, with the observation that removing language\ninformation is able to improve the cross-lingual transfer performance.", "published": "2021-09-10 08:15:37", "link": "http://arxiv.org/abs/2109.04727v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dual-State Capsule Networks for Text Classification", "abstract": "Text classification systems based on contextual embeddings are not viable\noptions for many of the low resource languages. On the other hand, recently\nintroduced capsule networks have shown performance in par with these text\nclassification models. Thus, they could be considered as a viable alternative\nfor text classification for languages that do not have pre-trained contextual\nembedding models. However, current capsule networks depend upon spatial\npatterns without considering the sequential features of the text. They are also\nsub-optimal in capturing the context-level information in longer sequences.\nThis paper presents a novel Dual-State Capsule (DS-Caps) network-based\ntechnique for text classification, which is optimized to mitigate these issues.\nTwo varieties of states, namely sentence-level and word-level, are integrated\nwith capsule layers to capture deeper context-level information for language\nmodeling. The dynamic routing process among capsules was also optimized using\nthe context-level information obtained through sentence-level states. The\nDS-Caps networks outperform the existing capsule network architectures for\nmultiple datasets, particularly for tasks with longer sequences of text. We\nalso demonstrate the superiority of DS-Caps in text classification for a low\nresource language.", "published": "2021-09-10 09:59:55", "link": "http://arxiv.org/abs/2109.04762v1", "categories": ["cs.CL", "cs.LG", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Improving Multilingual Translation by Representation and Gradient\n  Regularization", "abstract": "Multilingual Neural Machine Translation (NMT) enables one model to serve all\ntranslation directions, including ones that are unseen during training, i.e.\nzero-shot translation. Despite being theoretically attractive, current models\noften produce low quality translations -- commonly failing to even produce\noutputs in the right target language. In this work, we observe that off-target\ntranslation is dominant even in strong multilingual systems, trained on massive\nmultilingual corpora. To address this issue, we propose a joint approach to\nregularize NMT models at both representation-level and gradient-level. At the\nrepresentation level, we leverage an auxiliary target language prediction task\nto regularize decoder outputs to retain information about the target language.\nAt the gradient level, we leverage a small amount of direct data (in thousands\nof sentence pairs) to regularize model gradients. Our results demonstrate that\nour approach is highly effective in both reducing off-target translation\noccurrences and improving zero-shot translation performance by +5.59 and +10.38\nBLEU on WMT and OPUS datasets respectively. Moreover, experiments show that our\nmethod also works well when the small amount of direct data is not available.", "published": "2021-09-10 10:52:21", "link": "http://arxiv.org/abs/2109.04778v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Evaluation Dataset and Strategy for Building Robust Multi-turn\n  Response Selection Model", "abstract": "Multi-turn response selection models have recently shown comparable\nperformance to humans in several benchmark datasets. However, in the real\nenvironment, these models often have weaknesses, such as making incorrect\npredictions based heavily on superficial patterns without a comprehensive\nunderstanding of the context. For example, these models often give a high score\nto the wrong response candidate containing several keywords related to the\ncontext but using the inconsistent tense. In this study, we analyze the\nweaknesses of the open-domain Korean Multi-turn response selection models and\npublish an adversarial dataset to evaluate these weaknesses. We also suggest a\nstrategy to build a robust model in this adversarial environment.", "published": "2021-09-10 12:36:13", "link": "http://arxiv.org/abs/2109.04834v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Block Pruning For Faster Transformers", "abstract": "Pre-training has improved model accuracy for both classification and\ngeneration tasks at the cost of introducing much larger and slower models.\nPruning methods have proven to be an effective way of reducing model size,\nwhereas distillation methods are proven for speeding up inference. We introduce\na block pruning approach targeting both small and fast models. Our approach\nextends structured methods by considering blocks of any size and integrates\nthis structure into the movement pruning paradigm for fine-tuning. We find that\nthis approach learns to prune out full components of the underlying model, such\nas attention heads. Experiments consider classification and generation tasks,\nyielding among other results a pruned model that is a 2.4x faster, 74% smaller\nBERT on SQuAD v1, with a 1% drop on F1, competitive both with distilled models\nin speed and pruned models in size.", "published": "2021-09-10 12:46:32", "link": "http://arxiv.org/abs/2109.04838v1", "categories": ["cs.LG", "cs.CL", "I.2.6; I.2.7"], "primary_category": "cs.LG"}
{"title": "Active learning for reducing labeling effort in text classification\n  tasks", "abstract": "Labeling data can be an expensive task as it is usually performed manually by\ndomain experts. This is cumbersome for deep learning, as it is dependent on\nlarge labeled datasets. Active learning (AL) is a paradigm that aims to reduce\nlabeling effort by only using the data which the used model deems most\ninformative. Little research has been done on AL in a text classification\nsetting and next to none has involved the more recent, state-of-the-art Natural\nLanguage Processing (NLP) models. Here, we present an empirical study that\ncompares different uncertainty-based algorithms with BERT$_{base}$ as the used\nclassifier. We evaluate the algorithms on two NLP classification datasets:\nStanford Sentiment Treebank and KvK-Frontpages. Additionally, we explore\nheuristics that aim to solve presupposed problems of uncertainty-based AL;\nnamely, that it is unscalable and that it is prone to selecting outliers.\nFurthermore, we explore the influence of the query-pool size on the performance\nof AL. Whereas it was found that the proposed heuristics for AL did not improve\nperformance of AL; our results show that using uncertainty-based AL with\nBERT$_{base}$ outperforms random sampling of data. This difference in\nperformance can decrease as the query-pool size gets larger.", "published": "2021-09-10 13:00:36", "link": "http://arxiv.org/abs/2109.04847v2", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "MultiAzterTest: a Multilingual Analyzer on Multiple Levels of Language\n  for Readability Assessment", "abstract": "Readability assessment is the task of determining how difficult or easy a\ntext is or which level/grade it has. Traditionally, language dependent\nreadability formula have been used, but these formulae take few text\ncharacteristics into account. However, Natural Language Processing (NLP) tools\nthat assess the complexity of texts are able to measure more different features\nand can be adapted to different languages. In this paper, we present the\nMultiAzterTest tool: (i) an open source NLP tool which analyzes texts on over\n125 measures of cohesion,language, and readability for English, Spanish and\nBasque, but whose architecture is designed to easily adapt other languages;\n(ii) readability assessment classifiers that improve the performance of\nCoh-Metrix in English, Coh-Metrix-Esp in Spanish and ErreXail in Basque; iii) a\nweb tool. MultiAzterTest obtains 90.09 % in accuracy when classifying into\nthree reading levels (elementary, intermediate, and advanced) in English and\n95.50 % in Basque and 90 % in Spanish when classifying into two reading levels\n(simple and complex) using a SMO classifier. Using cross-lingual features,\nMultiAzterTest also obtains competitive results above all in a complex vs\nsimple distinction.", "published": "2021-09-10 13:34:52", "link": "http://arxiv.org/abs/2109.04870v1", "categories": ["cs.CL", "cs.AI", "68T50, 91F20", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Integrating Approaches to Word Representation", "abstract": "The problem of representing the atomic elements of language in modern neural\nlearning systems is one of the central challenges of the field of natural\nlanguage processing. I present a survey of the distributional, compositional,\nand relational approaches to addressing this task, and discuss various means of\nintegrating them into systems, with special emphasis on the word level and the\nout-of-vocabulary phenomenon.", "published": "2021-09-10 13:44:15", "link": "http://arxiv.org/abs/2109.04876v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Test Time Adapter Ensembling for Low-resource Language\n  Varieties", "abstract": "Adapters are light-weight modules that allow parameter-efficient fine-tuning\nof pretrained models. Specialized language and task adapters have recently been\nproposed to facilitate cross-lingual transfer of multilingual pretrained models\n(Pfeiffer et al., 2020b). However, this approach requires training a separate\nlanguage adapter for every language one wishes to support, which can be\nimpractical for languages with limited data. An intuitive solution is to use a\nrelated language adapter for the new language variety, but we observe that this\nsolution can lead to sub-optimal performance. In this paper, we aim to improve\nthe robustness of language adapters to uncovered languages without training new\nadapters. We find that ensembling multiple existing language adapters makes the\nfine-tuned model significantly more robust to other language varieties not\nincluded in these adapters. Building upon this observation, we propose Entropy\nMinimized Ensemble of Adapters (EMEA), a method that optimizes the ensemble\nweights of the pretrained language adapters for each test sentence by\nminimizing the entropy of its predictions. Experiments on three diverse groups\nof language varieties show that our method leads to significant improvements on\nboth named entity recognition and part-of-speech tagging across all languages.", "published": "2021-09-10 13:44:46", "link": "http://arxiv.org/abs/2109.04877v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Examining Cross-lingual Contextual Embeddings with Orthogonal Structural\n  Probes", "abstract": "State-of-the-art contextual embeddings are obtained from large language\nmodels available only for a few languages. For others, we need to learn\nrepresentations using a multilingual model. There is an ongoing debate on\nwhether multilingual embeddings can be aligned in a space shared across many\nlanguages. The novel Orthogonal Structural Probe (Limisiewicz and Mare\\v{c}ek,\n2021) allows us to answer this question for specific linguistic features and\nlearn a projection based only on mono-lingual annotated datasets. We evaluate\nsyntactic (UD) and lexical (WordNet) structural information encoded inmBERT's\ncontextual representations for nine diverse languages. We observe that for\nlanguages closely related to English, no transformation is needed. The\nevaluated information is encoded in a shared cross-lingual embedding space. For\nother languages, it is beneficial to apply orthogonal transformation learned\nseparately for each language. We successfully apply our findings to zero-shot\nand few-shot cross-lingual parsing.", "published": "2021-09-10 15:03:11", "link": "http://arxiv.org/abs/2109.04921v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Does Pretraining for Summarization Require Knowledge Transfer?", "abstract": "Pretraining techniques leveraging enormous datasets have driven recent\nadvances in text summarization. While folk explanations suggest that knowledge\ntransfer accounts for pretraining's benefits, little is known about why it\nworks or what makes a pretraining task or dataset suitable. In this paper, we\nchallenge the knowledge transfer story, showing that pretraining on documents\nconsisting of character n-grams selected at random, we can nearly match the\nperformance of models pretrained on real corpora. This work holds the promise\nof eliminating upstream corpora, which may alleviate some concerns over\noffensive language, bias, and copyright issues. To see whether the small\nresidual benefit of using real data could be accounted for by the structure of\nthe pretraining task, we design several tasks motivated by a qualitative study\nof summarization corpora. However, these tasks confer no appreciable benefit,\nleaving open the possibility of a small role for knowledge transfer.", "published": "2021-09-10 15:54:15", "link": "http://arxiv.org/abs/2109.04953v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Box Embeddings: An open-source library for representation learning using\n  geometric structures", "abstract": "A major factor contributing to the success of modern representation learning\nis the ease of performing various vector operations. Recently, objects with\ngeometric structures (eg. distributions, complex or hyperbolic vectors, or\nregions such as cones, disks, or boxes) have been explored for their\nalternative inductive biases and additional representational capacities. In\nthis work, we introduce Box Embeddings, a Python library that enables\nresearchers to easily apply and extend probabilistic box embeddings.", "published": "2021-09-10 17:08:17", "link": "http://arxiv.org/abs/2109.04997v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Distantly-Supervised Named Entity Recognition with Noise-Robust Learning\n  and Language Model Augmented Self-Training", "abstract": "We study the problem of training named entity recognition (NER) models using\nonly distantly-labeled data, which can be automatically obtained by matching\nentity mentions in the raw text with entity types in a knowledge base. The\nbiggest challenge of distantly-supervised NER is that the distant supervision\nmay induce incomplete and noisy labels, rendering the straightforward\napplication of supervised learning ineffective. In this paper, we propose (1) a\nnoise-robust learning scheme comprised of a new loss function and a noisy label\nremoval step, for training NER models on distantly-labeled data, and (2) a\nself-training method that uses contextualized augmentations created by\npre-trained language models to improve the generalization ability of the NER\nmodel. On three benchmark datasets, our method achieves superior performance,\noutperforming existing distantly-supervised NER models by significant margins.", "published": "2021-09-10 17:19:56", "link": "http://arxiv.org/abs/2109.05003v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation Quality and Post-Editing Performance", "abstract": "We test the natural expectation that using MT in professional translation\nsaves human processing time. The last such study was carried out by\nSanchez-Torron and Koehn (2016) with phrase-based MT, artificially reducing the\ntranslation quality. In contrast, we focus on neural MT (NMT) of high quality,\nwhich has become the state-of-the-art approach since then and also got adopted\nby most translation companies.\n  Through an experimental study involving over 30 professional translators for\nEnglish -> Czech translation, we examine the relationship between NMT\nperformance and post-editing time and quality. Across all models, we found that\nbetter MT systems indeed lead to fewer changes in the sentences in this\nindustry setting. The relation between system quality and post-editing time is\nhowever not straightforward and, contrary to the results on phrase-based MT,\nBLEU is definitely not a stable predictor of the time or final output quality.", "published": "2021-09-10 17:56:02", "link": "http://arxiv.org/abs/2109.05016v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Entity-Based Knowledge Conflicts in Question Answering", "abstract": "Knowledge-dependent tasks typically use two sources of knowledge: parametric,\nlearned at training time, and contextual, given as a passage at inference time.\nTo understand how models use these sources together, we formalize the problem\nof knowledge conflicts, where the contextual information contradicts the\nlearned information. Analyzing the behaviour of popular models, we measure\ntheir over-reliance on memorized information (the cause of hallucinations), and\nuncover important factors that exacerbate this behaviour. Lastly, we propose a\nsimple method to mitigate over-reliance on parametric knowledge, which\nminimizes hallucination, and improves out-of-distribution generalization by\n4%-7%. Our findings demonstrate the importance for practitioners to evaluate\nmodel tendency to hallucinate rather than read, and show that our mitigation\nstrategy encourages generalization to evolving information (i.e.,\ntime-dependent queries). To encourage these practices, we have released our\nframework for generating knowledge conflicts.", "published": "2021-09-10 18:29:44", "link": "http://arxiv.org/abs/2109.05052v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding\n  from Language Models", "abstract": "Large pre-trained language models for textual data have an unconstrained\noutput space; at each decoding step, they can produce any of 10,000s of\nsub-word tokens. When fine-tuned to target constrained formal languages like\nSQL, these models often generate invalid code, rendering it unusable. We\npropose PICARD (code and trained models available at\nhttps://github.com/ElementAI/picard), a method for constraining auto-regressive\ndecoders of language models through incremental parsing. PICARD helps to find\nvalid output sequences by rejecting inadmissible tokens at each decoding step.\nOn the challenging Spider and CoSQL text-to-SQL translation tasks, we show that\nPICARD transforms fine-tuned T5 models with passable performance into\nstate-of-the-art solutions.", "published": "2021-09-10 20:14:08", "link": "http://arxiv.org/abs/2109.05093v1", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Partially-Supervised Novel Object Captioning Leveraging Context from\n  Paired Data", "abstract": "In this paper, we propose an approach to improve image captioning solution\nfor images with novel objects that do not have caption labels in the training\ndataset. We refer to our approach as Partially-Supervised Novel Object\nCaptioning (PS-NOC). PS-NOC is agnostic to model architecture, and primarily\nfocuses on the training approach that uses existing fully paired image-caption\ndata and the images with only the novel object detection labels (partially\npaired data). We create synthetic paired captioning data for novel objects by\nleveraging context from existing image-caption pairs. We then create\npseudo-label captions for partially paired images with novel objects, and use\nthis additional data to fine-tune the captioning model. We also propose a\nvariant of SCST within PS-NOC, called SCST-F1, that directly optimizes the\nF1-score of novel objects. Using a popular captioning model (Up-Down) as\nbaseline, PS-NOC sets new state-of-the-art results on held-out MS COCO\nout-of-domain test split, i.e., 85.9 F1-score and 103.8 CIDEr. This is an\nimprovement of 85.9 and 34.1 points respectively compared to baseline model\nthat does not use partially paired data during training. We also perform\ndetailed ablation studies to demonstrate the effectiveness of our approach.", "published": "2021-09-10 21:31:42", "link": "http://arxiv.org/abs/2109.05115v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Efficient Contrastive Learning via Novel Data Augmentation and\n  Curriculum Learning", "abstract": "We introduce EfficientCL, a memory-efficient continual pretraining method\nthat applies contrastive learning with novel data augmentation and curriculum\nlearning. For data augmentation, we stack two types of operation sequentially:\ncutoff and PCA jittering. While pretraining steps proceed, we apply curriculum\nlearning by incrementing the augmentation degree for each difficulty step.\nAfter data augmentation is finished, contrastive learning is applied on\nprojected embeddings of original and augmented examples. When finetuned on GLUE\nbenchmark, our model outperforms baseline models, especially for sentence-level\ntasks. Additionally, this improvement is capable with only 70% of computational\nmemory compared to the baseline model.", "published": "2021-09-10 05:49:55", "link": "http://arxiv.org/abs/2109.05941v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EVOQUER: Enhancing Temporal Grounding with Video-Pivoted BackQuery\n  Generation", "abstract": "Temporal grounding aims to predict a time interval of a video clip\ncorresponding to a natural language query input. In this work, we present\nEVOQUER, a temporal grounding framework incorporating an existing text-to-video\ngrounding model and a video-assisted query generation network. Given a query\nand an untrimmed video, the temporal grounding model predicts the target\ninterval, and the predicted video clip is fed into a video translation task by\ngenerating a simplified version of the input query. EVOQUER forms closed-loop\nlearning by incorporating loss functions from both temporal grounding and query\ngeneration serving as feedback. Our experiments on two widely used datasets,\nCharades-STA and ActivityNet, show that EVOQUER achieves promising improvements\nby 1.05 and 1.31 at R@0.7. We also discuss how the query generation task could\nfacilitate error analysis by explaining temporal grounding model behavior.", "published": "2021-09-10 00:30:36", "link": "http://arxiv.org/abs/2109.04600v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Query-driven Segment Selection for Ranking Long Documents", "abstract": "Transformer-based rankers have shown state-of-the-art performance. However,\ntheir self-attention operation is mostly unable to process long sequences. One\nof the common approaches to train these rankers is to heuristically select some\nsegments of each document, such as the first segment, as training data.\nHowever, these segments may not contain the query-related parts of documents.\nTo address this problem, we propose query-driven segment selection from long\ndocuments to build training data. The segment selector provides relevant\nsamples with more accurate labels and non-relevant samples which are harder to\nbe predicted. The experimental results show that the basic BERT-based ranker\ntrained with the proposed segment selector significantly outperforms that\ntrained by the heuristically selected segments, and performs equally to the\nstate-of-the-art model with localized self-attention that can process longer\ninput sequences. Our findings open up new direction to design efficient\ntransformer-based rankers.", "published": "2021-09-10 01:50:12", "link": "http://arxiv.org/abs/2109.04611v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "H.3.3"], "primary_category": "cs.IR"}
{"title": "Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model", "abstract": "The transformer-based pre-trained language models have been tremendously\nsuccessful in most of the conventional NLP tasks. But they often struggle in\nthose tasks where numerical understanding is required. Some possible reasons\ncan be the tokenizers and pre-training objectives which are not specifically\ndesigned to learn and preserve numeracy. Here we investigate the ability of\ntext-to-text transfer learning model (T5), which has outperformed its\npredecessors in the conventional NLP tasks, to learn numeracy. We consider four\nnumeracy tasks: numeration, magnitude order prediction, finding minimum and\nmaximum in a series, and sorting. We find that, although T5 models perform\nreasonably well in the interpolation setting, they struggle considerably in the\nextrapolation setting across all four tasks.", "published": "2021-09-10 05:33:17", "link": "http://arxiv.org/abs/2109.04672v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generating Self-Contained and Summary-Centric Question Answer Pairs via\n  Differentiable Reward Imitation Learning", "abstract": "Motivated by suggested question generation in conversational news\nrecommendation systems, we propose a model for generating question-answer pairs\n(QA pairs) with self-contained, summary-centric questions and\nlength-constrained, article-summarizing answers. We begin by collecting a new\ndataset of news articles with questions as titles and pairing them with\nsummaries of varying length. This dataset is used to learn a QA pair generation\nmodel producing summaries as answers that balance brevity with sufficiency\njointly with their corresponding questions. We then reinforce the QA pair\ngeneration process with a differentiable reward function to mitigate exposure\nbias, a common problem in natural language generation. Both automatic metrics\nand human evaluation demonstrate these QA pairs successfully capture the\ncentral gists of the articles and achieve high answer accuracy.", "published": "2021-09-10 06:34:55", "link": "http://arxiv.org/abs/2109.04689v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident\n  Learning and Language Modeling", "abstract": "While large scale pre-training has achieved great achievements in bridging\nthe gap between vision and language, it still faces several challenges. First,\nthe cost for pre-training is expensive. Second, there is no efficient way to\nhandle the data noise which degrades model performance. Third, previous methods\nonly leverage limited image-text paired data, while ignoring richer\nsingle-modal data, which may result in poor generalization to single-modal\ndownstream tasks. In this work, we propose an EfficientCLIP method via Ensemble\nConfident Learning to obtain a less noisy data subset. Extra rich non-paired\nsingle-modal text data is used for boosting the generalization of text branch.\nWe achieve the state-of-the-art performance on Chinese cross-modal retrieval\ntasks with only 1/10 training resources compared to CLIP and WenLan, while\nshowing excellent generalization to single-modal tasks, including text\nretrieval and text classification.", "published": "2021-09-10 07:09:39", "link": "http://arxiv.org/abs/2109.04699v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Artificial Text Detection via Examining the Topology of Attention Maps", "abstract": "The impressive capabilities of recent generative models to create texts that\nare challenging to distinguish from the human-written ones can be misused for\ngenerating fake news, product reviews, and even abusive content. Despite the\nprominent performance of existing methods for artificial text detection, they\nstill lack interpretability and robustness towards unseen models. To this end,\nwe propose three novel types of interpretable topological features for this\ntask based on Topological Data Analysis (TDA) which is currently understudied\nin the field of NLP. We empirically show that the features derived from the\nBERT model outperform count- and neural-based baselines up to 10\\% on three\ncommon datasets, and tend to be the most robust towards unseen GPT-style\ngeneration models as opposed to existing methods. The probing analysis of the\nfeatures reveals their sensitivity to the surface and syntactic properties. The\nresults demonstrate that TDA is a promising line with respect to NLP tasks,\nspecifically the ones that incorporate surface and structural information.", "published": "2021-09-10 12:13:45", "link": "http://arxiv.org/abs/2109.04825v2", "categories": ["cs.CL", "cs.LG", "math.AT"], "primary_category": "cs.CL"}
{"title": "FR-Detect: A Multi-Modal Framework for Early Fake News Detection on\n  Social Media Using Publishers Features", "abstract": "In recent years, with the expansion of the Internet and attractive social\nmedia infrastructures, people prefer to follow the news through these media.\nDespite the many advantages of these media in the news field, the lack of any\ncontrol and verification mechanism has led to the spread of fake news, as one\nof the most important threats to democracy, economy, journalism and freedom of\nexpression. Designing and using automatic methods to detect fake news on social\nmedia has become a significant challenge. In this paper, we examine the\npublishers' role in detecting fake news on social media. We also suggest a high\naccurate multi-modal framework, namely FR-Detect, using user-related and\ncontent-related features with early detection capability. For this purpose, two\nnew user-related features, namely Activity Credibility and Influence, have been\nintroduced for publishers. Furthermore, a sentence-level convolutional neural\nnetwork is provided to combine these features with latent textual content\nfeatures properly. Experimental results have shown that the publishers'\nfeatures can improve the performance of content-based models by up to 13% and\n29% in accuracy and F1-score, respectively.", "published": "2021-09-10 12:39:00", "link": "http://arxiv.org/abs/2109.04835v1", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "ReasonBERT: Pre-trained to Reason with Distant Supervision", "abstract": "We present ReasonBert, a pre-training method that augments language models\nwith the ability to reason over long-range relations and multiple, possibly\nhybrid contexts. Unlike existing pre-training methods that only harvest\nlearning signals from local contexts of naturally occurring texts, we propose a\ngeneralized notion of distant supervision to automatically connect multiple\npieces of text and tables to create pre-training examples that require\nlong-range reasoning. Different types of reasoning are simulated, including\nintersecting multiple pieces of evidence, bridging from one piece of evidence\nto another, and detecting unanswerable cases. We conduct a comprehensive\nevaluation on a variety of extractive question answering datasets ranging from\nsingle-hop to multi-hop and from text-only to table-only to hybrid that require\nvarious reasoning capabilities and show that ReasonBert achieves remarkable\nimprovement over an array of strong baselines. Few-shot experiments further\ndemonstrate that our pre-training method substantially improves sample\nefficiency.", "published": "2021-09-10 14:49:44", "link": "http://arxiv.org/abs/2109.04912v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization", "abstract": "Unlike well-structured text, such as news reports and encyclopedia articles,\ndialogue content often comes from two or more interlocutors, exchanging\ninformation with each other. In such a scenario, the topic of a conversation\ncan vary upon progression and the key information for a certain topic is often\nscattered across multiple utterances of different speakers, which poses\nchallenges to abstractly summarize dialogues. To capture the various topic\ninformation of a conversation and outline salient facts for the captured\ntopics, this work proposes two topic-aware contrastive learning objectives,\nnamely coherence detection and sub-summary generation objectives, which are\nexpected to implicitly model the topic change and handle information scattering\nchallenges for the dialogue summarization task. The proposed contrastive\nobjectives are framed as auxiliary tasks for the primary dialogue summarization\ntask, united via an alternative parameter updating strategy. Extensive\nexperiments on benchmark datasets demonstrate that the proposed simple method\nsignificantly outperforms strong baselines and achieves new state-of-the-art\nperformance. The code and trained models are publicly available via\n\\href{https://github.com/Junpliu/ConDigSum}{https://github.com/Junpliu/ConDigSum}.", "published": "2021-09-10 17:03:25", "link": "http://arxiv.org/abs/2109.04994v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speaker Turn Modeling for Dialogue Act Classification", "abstract": "Dialogue Act (DA) classification is the task of classifying utterances with\nrespect to the function they serve in a dialogue. Existing approaches to DA\nclassification model utterances without incorporating the turn changes among\nspeakers throughout the dialogue, therefore treating it no different than\nnon-interactive written text. In this paper, we propose to integrate the turn\nchanges in conversations among speakers when modeling DAs. Specifically, we\nlearn conversation-invariant speaker turn embeddings to represent the speaker\nturns in a conversation; the learned speaker turn embeddings are then merged\nwith the utterance embeddings for the downstream task of DA classification.\nWith this simple yet effective mechanism, our model is able to capture the\nsemantics from the dialogue content while accounting for different speaker\nturns in a conversation. Validation on three benchmark public datasets\ndemonstrates superior performance of our model.", "published": "2021-09-10 18:36:35", "link": "http://arxiv.org/abs/2109.05056v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "FBERT: A Neural Transformer for Identifying Offensive Content", "abstract": "Transformer-based models such as BERT, XLNET, and XLM-R have achieved\nstate-of-the-art performance across various NLP tasks including the\nidentification of offensive language and hate speech, an important problem in\nsocial media. In this paper, we present fBERT, a BERT model retrained on SOLID,\nthe largest English offensive language identification corpus available with\nover $1.4$ million offensive instances. We evaluate fBERT's performance on\nidentifying offensive content on multiple English datasets and we test several\nthresholds for selecting instances from SOLID. The fBERT model will be made\nfreely available to the community.", "published": "2021-09-10 19:19:26", "link": "http://arxiv.org/abs/2109.05074v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "HypoGen: Hyperbole Generation with Commonsense and Counterfactual\n  Knowledge", "abstract": "A hyperbole is an intentional and creative exaggeration not to be taken\nliterally. Despite its ubiquity in daily life, the computational explorations\nof hyperboles are scarce. In this paper, we tackle the under-explored and\nchallenging task: sentence-level hyperbole generation. We start with a\nrepresentative syntactic pattern for intensification and systematically study\nthe semantic (commonsense and counterfactual) relationships between each\ncomponent in such hyperboles. Next, we leverage the COMeT and reverse COMeT\nmodels to do commonsense and counterfactual inference. We then generate\nmultiple hyperbole candidates based on our findings from the pattern, and train\nneural classifiers to rank and select high-quality hyperboles. Automatic and\nhuman evaluations show that our generation method is able to generate\nhyperboles creatively with high success rate and intensity scores.", "published": "2021-09-10 20:19:52", "link": "http://arxiv.org/abs/2109.05097v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MURAL: Multimodal, Multitask Retrieval Across Languages", "abstract": "Both image-caption pairs and translation pairs provide the means to learn\ndeep representations of and connections between languages. We use both types of\npairs in MURAL (MUltimodal, MUltitask Representations Across Languages), a dual\nencoder that solves two tasks: 1) image-text matching and 2) translation pair\nmatching. By incorporating billions of translation pairs, MURAL extends ALIGN\n(Jia et al. PMLR'21)--a state-of-the-art dual encoder learned from 1.8 billion\nnoisy image-text pairs. When using the same encoders, MURAL's performance\nmatches or exceeds ALIGN's cross-modal retrieval performance on well-resourced\nlanguages across several datasets. More importantly, it considerably improves\nperformance on under-resourced languages, showing that text-text learning can\novercome a paucity of image-caption examples for these languages. On the\nWikipedia Image-Text dataset, for example, MURAL-base improves zero-shot mean\nrecall by 8.1% on average for eight under-resourced languages and by 6.8% on\naverage when fine-tuning. We additionally show that MURAL's text\nrepresentations cluster not only with respect to genealogical connections but\nalso based on areal linguistics, such as the Balkan Sprachbund.", "published": "2021-09-10 22:26:05", "link": "http://arxiv.org/abs/2109.05125v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Refocusing on Relevance: Personalization in NLG", "abstract": "Many NLG tasks such as summarization, dialogue response, or open domain\nquestion answering focus primarily on a source text in order to generate a\ntarget response. This standard approach falls short, however, when a user's\nintent or context of work is not easily recoverable based solely on that source\ntext -- a scenario that we argue is more of the rule than the exception. In\nthis work, we argue that NLG systems in general should place a much higher\nlevel of emphasis on making use of additional context, and suggest that\nrelevance (as used in Information Retrieval) be thought of as a crucial tool\nfor designing user-oriented text-generating tasks. We further discuss possible\nharms and hazards around such personalization, and argue that value-sensitive\ndesign represents a crucial path forward through these challenges.", "published": "2021-09-10 23:50:02", "link": "http://arxiv.org/abs/2109.05140v1", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Scalable Font Reconstruction with Dual Latent Manifolds", "abstract": "We propose a deep generative model that performs typography analysis and font\nreconstruction by learning disentangled manifolds of both font style and\ncharacter shape. Our approach enables us to massively scale up the number of\ncharacter types we can effectively model compared to previous methods.\nSpecifically, we infer separate latent variables representing character and\nfont via a pair of inference networks which take as input sets of glyphs that\neither all share a character type, or belong to the same font. This design\nallows our model to generalize to characters that were not observed during\ntraining time, an important task in light of the relative sparsity of most\nfonts. We also put forward a new loss, adapted from prior work that measures\nlikelihood using an adaptive distribution in a projected space, resulting in\nmore natural images without requiring a discriminator. We evaluate on the task\nof font reconstruction over various datasets representing character types of\nmany languages, and compare favorably to modern style transfer systems\naccording to both automatic and manually-evaluated metrics.", "published": "2021-09-10 20:37:43", "link": "http://arxiv.org/abs/2109.06627v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Self-Attention Channel Combinator Frontend for End-to-End Multichannel\n  Far-field Speech Recognition", "abstract": "When a sufficiently large far-field training data is presented, jointly\noptimizing a multichannel frontend and an end-to-end (E2E) Automatic Speech\nRecognition (ASR) backend shows promising results. Recent literature has shown\ntraditional beamformer designs, such as MVDR (Minimum Variance Distortionless\nResponse) or fixed beamformers can be successfully integrated as the frontend\ninto an E2E ASR system with learnable parameters. In this work, we propose the\nself-attention channel combinator (SACC) ASR frontend, which leverages the\nself-attention mechanism to combine multichannel audio signals in the magnitude\nspectral domain. Experiments conducted on a multichannel playback test data\nshows that the SACC achieved a 9.3% WERR compared to a state-of-the-art fixed\nbeamformer-based frontend, both jointly optimized with a ContextNet-based ASR\nbackend. We also demonstrate the connection between the SACC and the\ntraditional beamformers, and analyze the intermediate outputs of the SACC.", "published": "2021-09-10 11:03:43", "link": "http://arxiv.org/abs/2109.04783v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Enhancement by Noise Self-Supervised Rank-Constrained Spatial\n  Covariance Matrix Estimation via Independent Deeply Learned Matrix Analysis", "abstract": "Rank-constrained spatial covariance matrix estimation (RCSCME) is a method\nfor the situation that the directional target speech and the diffuse noise are\nmixed. In conventional RCSCME, independent low-rank matrix analysis (ILRMA) is\nused as the preprocessing method. We propose RCSCME using independent deeply\nlearned matrix analysis (IDLMA), which is a supervised extension of ILRMA. In\nthis method, IDLMA requires deep neural networks (DNNs) to separate the target\nspeech and the noise. We use Denoiser, which is a single-channel speech\nenhancement DNN, in IDLMA to estimate not only the target speech but also the\nnoise. We also propose noise self-supervised RCSCME, in which we estimate the\nnoise-only time intervals using the output of Denoiser and design the prior\ndistribution of the noise spatial covariance matrix for RCSCME. We confirm that\nthe proposed methods outperform the conventional methods under several noise\nconditions.", "published": "2021-09-10 04:25:27", "link": "http://arxiv.org/abs/2109.04658v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Remember the context! ASR slot error correction through memorization", "abstract": "Accurate recognition of slot values such as domain specific words or named\nentities by automatic speech recognition (ASR) systems forms the core of the\nGoal-oriented Dialogue Systems. Although it is a critical step with direct\nimpact on downstream tasks such as language understanding, many domain agnostic\nASR systems tend to perform poorly on domain specific or long tail words. They\nare often supplemented with slot error correcting systems but it is often hard\nfor any neural model to directly output such rare entity words. To address this\nproblem, we propose k-nearest neighbor (k-NN) search that outputs\ndomain-specific entities from an explicit datastore. We improve error\ncorrection rate by conveniently augmenting a pretrained joint phoneme and text\nbased transformer sequence to sequence model with k-NN search during inference.\nWe evaluate our proposed approach on five different domains containing long\ntail slot entities such as full names, airports, street names, cities, states.\nOur best performing error correction model shows a relative improvement of 7.4%\nin word error rate (WER) on rare word entities over the baseline and also\nachieves a relative WER improvement of 9.8% on an out of vocabulary (OOV) test\nset.", "published": "2021-09-10 20:07:56", "link": "http://arxiv.org/abs/2109.05092v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Large-vocabulary Audio-visual Speech Recognition in Noisy Environments", "abstract": "Audio-visual speech recognition (AVSR) can effectively and significantly\nimprove the recognition rates of small-vocabulary systems, compared to their\naudio-only counterparts. For large-vocabulary systems, however, there are still\nmany difficulties, such as unsatisfactory video recognition accuracies, that\nmake it hard to improve over audio-only baselines. In this paper, we\nspecifically consider such scenarios, focusing on the large-vocabulary task of\nthe LRS2 database, where audio-only performance is far superior to video-only\naccuracies, making this an interesting and challenging setup for multi-modal\nintegration.\n  To address the inherent difficulties, we propose a new fusion strategy: a\nrecurrent integration network is trained to fuse the state posteriors of\nmultiple single-modality models, guided by a set of model-based and\nsignal-based stream reliability measures. During decoding, this network is used\nfor stream integration within a hybrid recognizer, where it can thus cope with\nthe time-variant reliability and information content of its multiple feature\ninputs.\n  We compare the results with end-to-end AVSR systems as well as with\ncompetitive hybrid baseline models, finding that the new fusion strategy shows\nsuperior results, on average even outperforming oracle dynamic stream\nweighting, which has so far marked the -- realistically unachievable -- upper\nbound for standard stream weighting. Even though the pure lipreading\nperformance is low, audio-visual integration is helpful under all -- clean,\nnoisy, and reverberant -- conditions. On average, the new system achieves a\nrelative word error rate reduction of 42.18\\% compared to the audio-only model,\npointing at a high effectiveness of the proposed integration approach.", "published": "2021-09-10 14:05:32", "link": "http://arxiv.org/abs/2109.04894v1", "categories": ["eess.AS", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
