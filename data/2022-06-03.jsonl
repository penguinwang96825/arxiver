{"title": "Task-Adaptive Pre-Training for Boosting Learning With Noisy Labels: A\n  Study on Text Classification for African Languages", "abstract": "For high-resource languages like English, text classification is a\nwell-studied task. The performance of modern NLP models easily achieves an\naccuracy of more than 90% in many standard datasets for text classification in\nEnglish (Xie et al., 2019; Yang et al., 2019; Zaheer et al., 2020). However,\ntext classification in low-resource languages is still challenging due to the\nlack of annotated data. Although methods like weak supervision and\ncrowdsourcing can help ease the annotation bottleneck, the annotations obtained\nby these methods contain label noise. Models trained with label noise may not\ngeneralize well. To this end, a variety of noise-handling techniques have been\nproposed to alleviate the negative impact caused by the errors in the\nannotations (for extensive surveys see (Hedderich et al., 2021; Algan & Ulusoy,\n2021)). In this work, we experiment with a group of standard noisy-handling\nmethods on text classification tasks with noisy labels. We study both simulated\nnoise and realistic noise induced by weak supervision. Moreover, we find\ntask-adaptive pre-training techniques (Gururangan et al., 2020) are beneficial\nfor learning with noisy labels.", "published": "2022-06-03 09:56:26", "link": "http://arxiv.org/abs/2206.01476v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Findings of the The RuATD Shared Task 2022 on Artificial Text Detection\n  in Russian", "abstract": "We present the shared task on artificial text detection in Russian, which is\norganized as a part of the Dialogue Evaluation initiative, held in 2022. The\nshared task dataset includes texts from 14 text generators, i.e., one human\nwriter and 13 text generative models fine-tuned for one or more of the\nfollowing generation tasks: machine translation, paraphrase generation, text\nsummarization, text simplification. We also consider back-translation and\nzero-shot generation approaches. The human-written texts are collected from\npublicly available resources across multiple domains. The shared task consists\nof two sub-tasks: (i) to determine if a given text is automatically generated\nor written by a human; (ii) to identify the author of a given text. The first\ntask is framed as a binary classification problem. The second task is a\nmulti-class classification problem. We provide count-based and BERT-based\nbaselines, along with the human evaluation on the first sub-task. A total of 30\nand 8 systems have been submitted to the binary and multi-class sub-tasks,\ncorrespondingly. Most teams outperform the baselines by a wide margin. We\npublicly release our codebase, human evaluation results, and other materials in\nour GitHub repository (https://github.com/dialogue-evaluation/RuATD).", "published": "2022-06-03 14:12:33", "link": "http://arxiv.org/abs/2206.01583v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Relevance in Dialogue: Is Less More? An Empirical Comparison of Existing\n  Metrics, and a Novel Simple Metric", "abstract": "In this work, we evaluate various existing dialogue relevance metrics, find\nstrong dependency on the dataset, often with poor correlation with human scores\nof relevance, and propose modifications to reduce data requirements and domain\nsensitivity while improving correlation. Our proposed metric achieves\nstate-of-the-art performance on the HUMOD dataset while reducing measured\nsensitivity to dataset by 37%-66%. We achieve this without fine-tuning a\npretrained language model, and using only 3,750 unannotated human dialogues and\na single negative example. Despite these limitations, we demonstrate\ncompetitive performance on four datasets from different domains. Our code,\nincluding our metric and experiments, is open sourced.", "published": "2022-06-03 21:23:05", "link": "http://arxiv.org/abs/2206.01823v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a Deep Multi-layered Dialectal Language Analysis: A Case Study\n  of African-American English", "abstract": "Currently, natural language processing (NLP) models proliferate language\ndiscrimination leading to potentially harmful societal impacts as a result of\nbiased outcomes. For example, part-of-speech taggers trained on Mainstream\nAmerican English (MAE) produce non-interpretable results when applied to\nAfrican American English (AAE) as a result of language features not seen during\ntraining. In this work, we incorporate a human-in-the-loop paradigm to gain a\nbetter understanding of AAE speakers' behavior and their language use, and\nhighlight the need for dialectal language inclusivity so that native AAE\nspeakers can extensively interact with NLP systems while reducing feelings of\ndisenfranchisement.", "published": "2022-06-03 01:05:58", "link": "http://arxiv.org/abs/2206.08978v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Plumber: A Modular Framework to Create Information Extraction Pipelines", "abstract": "Information Extraction (IE) tasks are commonly studied topics in various\ndomains of research. Hence, the community continuously produces multiple\ntechniques, solutions, and tools to perform such tasks. However, running those\ntools and integrating them within existing infrastructure requires time,\nexpertise, and resources. One pertinent task here is triples extraction and\nlinking, where structured triples are extracted from a text and aligned to an\nexisting Knowledge Graph (KG). In this paper, we present PLUMBER, the first\nframework that allows users to manually and automatically create suitable IE\npipelines from a community-created pool of tools to perform triple extraction\nand alignment on unstructured text. Our approach provides an interactive medium\nto alter the pipelines and perform IE tasks. A short video to show the working\nof the framework for different use-cases is available online under:\nhttps://www.youtube.com/watch?v=XC9rJNIUv8g", "published": "2022-06-03 08:10:35", "link": "http://arxiv.org/abs/2206.01442v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Acquiring and Modelling Abstract Commonsense Knowledge via\n  Conceptualization", "abstract": "Conceptualization, or viewing entities and situations as instances of\nabstract concepts in mind and making inferences based on that, is a vital\ncomponent in human intelligence for commonsense reasoning. Despite recent\nprogress in artificial intelligence to acquire and model commonsense attributed\nto neural language models and commonsense knowledge graphs (CKGs),\nconceptualization is yet to be introduced thoroughly, making current approaches\nineffective to cover knowledge about countless diverse entities and situations\nin the real world.\n  To address the problem, we thoroughly study the role of conceptualization in\ncommonsense reasoning, and formulate a framework to replicate human conceptual\ninduction by acquiring abstract knowledge about events regarding abstract\nconcepts, as well as higher-level triples or inferences upon them. We then\napply the framework to ATOMIC, a large-scale human-annotated CKG, aided by the\ntaxonomy Probase. We annotate a dataset on the validity of contextualized\nconceptualizations from ATOMIC on both event and triple levels, develop a\nseries of heuristic rules based on linguistic features, and train a set of\nneural models to generate and verify abstract knowledge. Based on these\ncomponents, a pipeline to acquire abstract knowledge is built. A large abstract\nCKG upon ATOMIC is then induced, ready to be instantiated to infer about unseen\nentities or situations. Finally, we empirically show the benefits of augmenting\nCKGs with abstract knowledge in downstream tasks like commonsense inference and\nzero-shot commonsense QA.", "published": "2022-06-03 12:24:49", "link": "http://arxiv.org/abs/2206.01532v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TCE at Qur'an QA 2022: Arabic Language Question Answering Over Holy\n  Qur'an Using a Post-Processed Ensemble of BERT-based Models", "abstract": "In recent years, we witnessed great progress in different tasks of natural\nlanguage understanding using machine learning. Question answering is one of\nthese tasks which is used by search engines and social media platforms for\nimproved user experience. Arabic is the language of the Holy Qur'an; the sacred\ntext for 1.8 billion people across the world. Arabic is a challenging language\nfor Natural Language Processing (NLP) due to its complex structures. In this\narticle, we describe our attempts at OSACT5 Qur'an QA 2022 Shared Task, which\nis a question answering challenge on the Holy Qur'an in Arabic. We propose an\nensemble learning model based on Arabic variants of BERT models. In addition,\nwe perform post-processing to enhance the model predictions. Our system\nachieves a Partial Reciprocal Rank (pRR) score of 56.6% on the official test\nset.", "published": "2022-06-03 13:00:48", "link": "http://arxiv.org/abs/2206.01550v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Extracting Similar Questions From Naturally-occurring Business\n  Conversations", "abstract": "Pre-trained contextualized embedding models such as BERT are a standard\nbuilding block in many natural language processing systems. We demonstrate that\nthe sentence-level representations produced by some off-the-shelf\ncontextualized embedding models have a narrow distribution in the embedding\nspace, and thus perform poorly for the task of identifying semantically similar\nquestions in real-world English business conversations. We describe a method\nthat uses appropriately tuned representations and a small set of exemplars to\ngroup questions of interest to business users in a visualization that can be\nused for data exploration or employee coaching.", "published": "2022-06-03 14:13:44", "link": "http://arxiv.org/abs/2206.01585v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ArgRewrite V.2: an Annotated Argumentative Revisions Corpus", "abstract": "Analyzing how humans revise their writings is an interesting research\nquestion, not only from an educational perspective but also in terms of\nartificial intelligence. Better understanding of this process could facilitate\nmany NLP applications, from intelligent tutoring systems to supportive and\ncollaborative writing environments. Developing these applications, however,\nrequires revision corpora, which are not widely available. In this work, we\npresent ArgRewrite V.2, a corpus of annotated argumentative revisions,\ncollected from two cycles of revisions to argumentative essays about\nself-driving cars. Annotations are provided at different levels of purpose\ngranularity (coarse and fine) and scope (sentential and subsentential). In\naddition, the corpus includes the revision goal given to each writer, essay\nscores, annotation verification, pre- and post-study surveys collected from\nparticipants as meta-data. The variety of revision unit scope and purpose\ngranularity levels in ArgRewrite, along with the inclusion of new types of\nmeta-data, can make it a useful resource for research and applications that\ninvolve revision analysis. We demonstrate some potential applications of\nArgRewrite V.2 in the development of automatic revision purpose predictors, as\na training source and benchmark.", "published": "2022-06-03 16:40:51", "link": "http://arxiv.org/abs/2206.01677v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge", "abstract": "The Visual Question Answering (VQA) task aspires to provide a meaningful\ntestbed for the development of AI models that can jointly reason over visual\nand natural language inputs. Despite a proliferation of VQA datasets, this goal\nis hindered by a set of common limitations. These include a reliance on\nrelatively simplistic questions that are repetitive in both concepts and\nlinguistic structure, little world knowledge needed outside of the paired\nimage, and limited reasoning required to arrive at the correct answer. We\nintroduce A-OKVQA, a crowdsourced dataset composed of a diverse set of about\n25K questions requiring a broad base of commonsense and world knowledge to\nanswer. In contrast to the existing knowledge-based VQA datasets, the questions\ngenerally cannot be answered by simply querying a knowledge base, and instead\nrequire some form of commonsense reasoning about the scene depicted in the\nimage. We demonstrate the potential of this new dataset through a detailed\nanalysis of its contents and baseline performance measurements over a variety\nof state-of-the-art vision-language models. Project page:\nhttp://a-okvqa.allenai.org/", "published": "2022-06-03 17:52:27", "link": "http://arxiv.org/abs/2206.01718v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Kallima: A Clean-label Framework for Textual Backdoor Attacks", "abstract": "Although Deep Neural Network (DNN) has led to unprecedented progress in\nvarious natural language processing (NLP) tasks, research shows that deep\nmodels are extremely vulnerable to backdoor attacks. The existing backdoor\nattacks mainly inject a small number of poisoned samples into the training\ndataset with the labels changed to the target one. Such mislabeled samples\nwould raise suspicion upon human inspection, potentially revealing the attack.\nTo improve the stealthiness of textual backdoor attacks, we propose the first\nclean-label framework Kallima for synthesizing mimesis-style backdoor samples\nto develop insidious textual backdoor attacks. We modify inputs belonging to\nthe target class with adversarial perturbations, making the model rely more on\nthe backdoor trigger. Our framework is compatible with most existing backdoor\ntriggers. The experimental results on three benchmark datasets demonstrate the\neffectiveness of the proposed method.", "published": "2022-06-03 21:44:43", "link": "http://arxiv.org/abs/2206.01832v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Latent Topology Induction for Understanding Contextualized\n  Representations", "abstract": "In this work, we study the representation space of contextualized embeddings\nand gain insight into the hidden topology of large language models. We show\nthere exists a network of latent states that summarize linguistic properties of\ncontextualized representations. Instead of seeking alignments to existing\nwell-defined annotations, we infer this latent network in a fully unsupervised\nway using a structured variational autoencoder. The induced states not only\nserve as anchors that mark the topology (neighbors and connectivity) of the\nrepresentation manifold but also reveal the internal mechanism of encoding\nsentences. With the induced network, we: (1). decompose the representation\nspace into a spectrum of latent states which encode fine-grained word meanings\nwith lexical, morphological, syntactic and semantic information; (2). show\nstate-state transitions encode rich phrase constructions and serve as the\nbackbones of the latent space. Putting the two together, we show that sentences\nare represented as a traversal over the latent network where state-state\ntransition chains encode syntactic templates and state-word emissions fill in\nthe content. We demonstrate these insights with extensive experiments and\nvisualizations.", "published": "2022-06-03 11:22:48", "link": "http://arxiv.org/abs/2206.01512v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Beyond Opinion Mining: Summarizing Opinions of Customer Reviews", "abstract": "Customer reviews are vital for making purchasing decisions in the Information\nAge. Such reviews can be automatically summarized to provide the user with an\noverview of opinions. In this tutorial, we present various aspects of opinion\nsummarization that are useful for researchers and practitioners. First, we will\nintroduce the task and major challenges. Then, we will present existing opinion\nsummarization solutions, both pre-neural and neural. We will discuss how\nsummarizers can be trained in the unsupervised, few-shot, and supervised\nregimes. Each regime has roots in different machine learning methods, such as\nauto-encoding, controllable text generation, and variational inference.\nFinally, we will discuss resources and evaluation methods and conclude with the\nfuture directions. This three-hour tutorial will provide a comprehensive\noverview over major advances in opinion summarization. The listeners will be\nwell-equipped with the knowledge that is both useful for research and practical\napplications.", "published": "2022-06-03 12:43:40", "link": "http://arxiv.org/abs/2206.01543v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Toward a realistic model of speech processing in the brain with\n  self-supervised learning", "abstract": "Several deep neural networks have recently been shown to generate activations\nsimilar to those of the brain in response to the same input. These algorithms,\nhowever, remain largely implausible: they require (1) extraordinarily large\namounts of data, (2) unobtainable supervised labels, (3) textual rather than\nraw sensory input, and / or (4) implausibly large memory (e.g. thousands of\ncontextual words). These elements highlight the need to identify algorithms\nthat, under these limitations, would suffice to account for both behavioral and\nbrain responses. Focusing on the issue of speech processing, we here\nhypothesize that self-supervised algorithms trained on the raw waveform\nconstitute a promising candidate. Specifically, we compare a recent\nself-supervised architecture, Wav2Vec 2.0, to the brain activity of 412\nEnglish, French, and Mandarin individuals recorded with functional Magnetic\nResonance Imaging (fMRI), while they listened to ~1h of audio books. Our\nresults are four-fold. First, we show that this algorithm learns brain-like\nrepresentations with as little as 600 hours of unlabelled speech -- a quantity\ncomparable to what infants can be exposed to during language acquisition.\nSecond, its functional hierarchy aligns with the cortical hierarchy of speech\nprocessing. Third, different training regimes reveal a functional\nspecialization akin to the cortex: Wav2Vec 2.0 learns sound-generic,\nspeech-specific and language-specific representations similar to those of the\nprefrontal and temporal cortices. Fourth, we confirm the similarity of this\nspecialization with the behavior of 386 additional participants. These\nelements, resulting from the largest neuroimaging benchmark to date, show how\nself-supervised learning can account for a rich organization of speech\nprocessing in the brain, and thus delineate a path to identify the laws of\nlanguage acquisition which shape the human brain.", "published": "2022-06-03 17:01:46", "link": "http://arxiv.org/abs/2206.01685v2", "categories": ["q-bio.NC", "cs.AI", "cs.CL"], "primary_category": "q-bio.NC"}
{"title": "Measuring Gender Bias in Word Embeddings of Gendered Languages Requires\n  Disentangling Grammatical Gender Signals", "abstract": "Does the grammatical gender of a language interfere when measuring the\nsemantic gender information captured by its word embeddings? A number of\nanomalous gender bias measurements in the embeddings of gendered languages\nsuggest this possibility. We demonstrate that word embeddings learn the\nassociation between a noun and its grammatical gender in grammatically gendered\nlanguages, which can skew social gender bias measurements. Consequently, word\nembedding post-processing methods are introduced to quantify, disentangle, and\nevaluate grammatical gender signals. The evaluation is performed on five\ngendered languages from the Germanic, Romance, and Slavic branches of the\nIndo-European language family. Our method reduces the strength of grammatical\ngender signals, which is measured in terms of effect size (Cohen's d), by a\nsignificant average of d = 1.3 for French, German, and Italian, and d = 0.56\nfor Polish and Spanish. Once grammatical gender is disentangled, the\nassociation between over 90% of 10,000 inanimate nouns and their assigned\ngrammatical gender weakens, and cross-lingual bias results from the Word\nEmbedding Association Test (WEAT) become more congruent with country-level\nimplicit bias measurements. The results further suggest that disentangling\ngrammatical gender signals from word embeddings may lead to improvement in\nsemantic machine learning tasks.", "published": "2022-06-03 17:11:00", "link": "http://arxiv.org/abs/2206.01691v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Revisiting the \"Video\" in Video-Language Understanding", "abstract": "What makes a video task uniquely suited for videos, beyond what can be\nunderstood from a single image? Building on recent progress in self-supervised\nimage-language models, we revisit this question in the context of video and\nlanguage tasks. We propose the atemporal probe (ATP), a new model for\nvideo-language analysis which provides a stronger bound on the baseline\naccuracy of multimodal models constrained by image-level understanding. By\napplying this model to standard discriminative video and language tasks, such\nas video question answering and text-to-video retrieval, we characterize the\nlimitations and potential of current video-language benchmarks. We find that\nunderstanding of event temporality is often not necessary to achieve strong or\nstate-of-the-art performance, even compared with recent large-scale\nvideo-language models and in contexts intended to benchmark deeper video-level\nunderstanding. We also demonstrate how ATP can improve both video-language\ndataset and model design. We describe a technique for leveraging ATP to better\ndisentangle dataset subsets with a higher concentration of temporally\nchallenging data, improving benchmarking efficacy for causal and temporal\nunderstanding. Further, we show that effectively integrating ATP into full\nvideo-level temporal models can improve efficiency and state-of-the-art\naccuracy.", "published": "2022-06-03 17:57:33", "link": "http://arxiv.org/abs/2206.01720v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "[Re] Badder Seeds: Reproducing the Evaluation of Lexical Methods for\n  Bias Measurement", "abstract": "Combating bias in NLP requires bias measurement. Bias measurement is almost\nalways achieved by using lexicons of seed terms, i.e. sets of words specifying\nstereotypes or dimensions of interest. This reproducibility study focuses on\nthe original authors' main claim that the rationale for the construction of\nthese lexicons needs thorough checking before usage, as the seeds used for bias\nmeasurement can themselves exhibit biases. The study aims to evaluate the\nreproducibility of the quantitative and qualitative results presented in the\npaper and the conclusions drawn thereof. We reproduce most of the results\nsupporting the original authors' general claim: seed sets often suffer from\nbiases that affect their performance as a baseline for bias metrics. Generally,\nour results mirror the original paper's. They are slightly different on select\noccasions, but not in ways that undermine the paper's general intent to show\nthe fragility of seed sets.", "published": "2022-06-03 18:00:29", "link": "http://arxiv.org/abs/2206.01767v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "QAGCN: Answering Multi-Relation Questions via Single-Step Implicit\n  Reasoning over Knowledge Graphs", "abstract": "Multi-relation question answering (QA) is a challenging task, where given\nquestions usually require long reasoning chains in KGs that consist of multiple\nrelations. Recently, methods with explicit multi-step reasoning over KGs have\nbeen prominently used in this task and have demonstrated promising performance.\nExamples include methods that perform stepwise label propagation through KG\ntriples and methods that navigate over KG triples based on reinforcement\nlearning. A main weakness of these methods is that their reasoning mechanisms\nare usually complex and difficult to implement or train. In this paper, we\nargue that multi-relation QA can be achieved via end-to-end single-step\nimplicit reasoning, which is simpler, more efficient, and easier to adopt. We\npropose QAGCN -- a Question-Aware Graph Convolutional Network (GCN)-based\nmethod that includes a novel GCN architecture with controlled\nquestion-dependent message propagation for the implicit reasoning. Extensive\nexperiments have been conducted, where QAGCN achieved competitive and even\nsuperior performance compared to state-of-the-art explicit-reasoning methods.\nOur code and pre-trained models are available in the repository:\nhttps://github.com/ruijie-wang-uzh/QAGCN", "published": "2022-06-03 21:01:48", "link": "http://arxiv.org/abs/2206.01818v3", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Visual Clues: Bridging Vision and Language Foundations for Image\n  Paragraph Captioning", "abstract": "People say, \"A picture is worth a thousand words\". Then how can we get the\nrich information out of the image? We argue that by using visual clues to\nbridge large pretrained vision foundation models and language models, we can do\nso without any extra cross-modal training. Thanks to the strong zero-shot\ncapability of foundation models, we start by constructing a rich semantic\nrepresentation of the image (e.g., image tags, object attributes / locations,\ncaptions) as a structured textual prompt, called visual clues, using a vision\nfoundation model. Based on visual clues, we use large language model to produce\na series of comprehensive descriptions for the visual content, which is then\nverified by the vision model again to select the candidate that aligns best\nwith the image. We evaluate the quality of generated descriptions by\nquantitative and qualitative measurement. The results demonstrate the\neffectiveness of such a structured semantic representation.", "published": "2022-06-03 22:33:09", "link": "http://arxiv.org/abs/2206.01843v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Automatic Generation of Programming Exercises and Code Explanations\n  using Large Language Models", "abstract": "This article explores the natural language generation capabilities of large\nlanguage models with application to the production of two types of learning\nresources common in programming courses. Using OpenAI Codex as the large\nlanguage model, we create programming exercises (including sample solutions and\ntest cases) and code explanations, assessing these qualitatively and\nquantitatively. Our results suggest that the majority of the automatically\ngenerated content is both novel and sensible, and in some cases ready to use as\nis. When creating exercises we find that it is remarkably easy to influence\nboth the programming concepts and the contextual themes they contain, simply by\nsupplying keywords as input to the model. Our analysis suggests that there is\nsignificant value in massive generative machine learning models as a tool for\ninstructors, although there remains a need for some oversight to ensure the\nquality of the generated content before it is delivered to students. We further\ndiscuss the implications of OpenAI Codex and similar tools for introductory\nprogramming education and highlight future research streams that have the\npotential to improve the quality of the educational experience for both\nteachers and students alike.", "published": "2022-06-03 11:00:43", "link": "http://arxiv.org/abs/2206.11861v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Constraining Gaussian processes for physics-informed acoustic emission\n  mapping", "abstract": "The automated localisation of damage in structures is a challenging but\ncritical ingredient in the path towards predictive or condition-based\nmaintenance of high value structures. The use of acoustic emission time of\narrival mapping is a promising approach to this challenge, but is severely\nhindered by the need to collect a dense set of artificial acoustic emission\nmeasurements across the structure, resulting in a lengthy and often impractical\ndata acquisition process. In this paper, we consider the use of\nphysics-informed Gaussian processes for learning these maps to alleviate this\nproblem. In the approach, the Gaussian process is constrained to the physical\ndomain such that information relating to the geometry and boundary conditions\nof the structure are embedded directly into the learning process, returning a\nmodel that guarantees that any predictions made satisfy physically-consistent\nbehaviour at the boundary. A number of scenarios that arise when training\nmeasurement acquisition is limited, including where training data are sparse,\nand also of limited coverage over the structure of interest. Using a complex\nplate-like structure as an experimental case study, we show that our approach\nsignificantly reduces the burden of data collection, where it is seen that\nincorporation of boundary condition knowledge significantly improves predictive\naccuracy as training observations are reduced, particularly when training\nmeasurements are not available across all parts of the structure.", "published": "2022-06-03 10:42:57", "link": "http://arxiv.org/abs/2206.01495v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
