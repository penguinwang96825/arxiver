{"title": "Selective Attention for Context-aware Neural Machine Translation", "abstract": "Despite the progress made in sentence-level NMT, current systems still fall\nshort at achieving fluent, good quality translation for a full document. Recent\nworks in context-aware NMT consider only a few previous sentences as context\nand may not scale to entire documents. To this end, we propose a novel and\nscalable top-down approach to hierarchical attention for context-aware NMT\nwhich uses sparse attention to selectively focus on relevant sentences in the\ndocument context and then attends to key words in those sentences. We also\npropose single-level attention approaches based on sentence or word-level\ninformation in the context. The document-level context representation, produced\nfrom these attention modules, is integrated into the encoder or decoder of the\nTransformer model depending on whether we use monolingual or bilingual context.\nOur experiments and evaluation on English-German datasets in different document\nMT settings show that our selective attention approach not only significantly\noutperforms context-agnostic baselines but also surpasses context-aware\nbaselines in most cases.", "published": "2019-03-21 01:01:22", "link": "http://arxiv.org/abs/1903.08788v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Knowledge and Transferability of Contextual Representations", "abstract": "Contextual word representations derived from large-scale neural language\nmodels are successful across a diverse set of NLP tasks, suggesting that they\nencode useful and transferable features of language. To shed light on the\nlinguistic knowledge they capture, we study the representations produced by\nseveral recent pretrained contextualizers (variants of ELMo, the OpenAI\ntransformer language model, and BERT) with a suite of seventeen diverse probing\ntasks. We find that linear models trained on top of frozen contextual\nrepresentations are competitive with state-of-the-art task-specific models in\nmany cases, but fail on tasks requiring fine-grained linguistic knowledge\n(e.g., conjunct identification). To investigate the transferability of\ncontextual word representations, we quantify differences in the transferability\nof individual layers within contextualizers, especially between recurrent\nneural networks (RNNs) and transformers. For instance, higher layers of RNNs\nare more task-specific, while transformer layers do not exhibit the same\nmonotonic trend. In addition, to better understand what makes contextual word\nrepresentations transferable, we compare language model pretraining with eleven\nsupervised pretraining tasks. For any given task, pretraining on a closely\nrelated task yields better performance than language model pretraining (which\nis better on average) when the pretraining dataset is fixed. However, language\nmodel pretraining on more data gives the best results.", "published": "2019-03-21 07:19:45", "link": "http://arxiv.org/abs/1903.08855v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RAP-Net: Recurrent Attention Pooling Networks for Dialogue Response\n  Selection", "abstract": "The response selection has been an emerging research topic due to the growing\ninterest in dialogue modeling, where the goal of the task is to select an\nappropriate response for continuing dialogues. To further push the end-to-end\ndialogue model toward real-world scenarios, the seventh Dialog System\nTechnology Challenge (DSTC7) proposed a challenging track based on real chatlog\ndatasets. The competition focuses on dialogue modeling with several advanced\ncharacteristics: (1) natural language diversity, (2) capability of precisely\nselecting a proper response from a large set of candidates or the scenario\nwithout any correct answer, and (3) knowledge grounding. This paper introduces\nrecurrent attention pooling networks (RAP-Net), a novel framework for response\nselection, which can well estimate the relevance between the dialogue contexts\nand the candidates. The proposed RAP-Net is shown to be effective and can be\ngeneralized across different datasets and settings in the DSTC7 experiments.", "published": "2019-03-21 10:09:43", "link": "http://arxiv.org/abs/1903.08905v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Multi-Level Information for Dialogue Response Selection by\n  Highway Recurrent Transformer", "abstract": "With the increasing research interest in dialogue response generation, there\nis an emerging branch formulating this task as selecting next sentences, where\ngiven the partial dialogue contexts, the goal is to determine the most probable\nnext sentence. Following the recent success of the Transformer model, this\npaper proposes (1) a new variant of attention mechanism based on multi-head\nattention, called highway attention, and (2) a recurrent model based on\ntransformer and the proposed highway attention, so-called Highway Recurrent\nTransformer. Experiments on the response selection task in the seventh Dialog\nSystem Technology Challenge (DSTC7) show the capability of the proposed model\nof modeling both utterance-level and dialogue-level information; the\neffectiveness of each module is further analyzed as well.", "published": "2019-03-21 12:39:02", "link": "http://arxiv.org/abs/1903.08953v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Iteratively Learning Embeddings and Rules for Knowledge Graph Reasoning", "abstract": "Reasoning is essential for the development of large knowledge graphs,\nespecially for completion, which aims to infer new triples based on existing\nones. Both rules and embeddings can be used for knowledge graph reasoning and\nthey have their own advantages and difficulties. Rule-based reasoning is\naccurate and explainable but rule learning with searching over the graph always\nsuffers from efficiency due to huge search space. Embedding-based reasoning is\nmore scalable and efficient as the reasoning is conducted via computation\nbetween embeddings, but it has difficulty learning good representations for\nsparse entities because a good embedding relies heavily on data richness. Based\non this observation, in this paper we explore how embedding and rule learning\ncan be combined together and complement each other's difficulties with their\nadvantages. We propose a novel framework IterE iteratively learning embeddings\nand rules, in which rules are learned from embeddings with proper pruning\nstrategy and embeddings are learned from existing triples and new triples\ninferred by rules. Evaluations on embedding qualities of IterE show that rules\nhelp improve the quality of sparse entity embeddings and their link prediction\nresults. We also evaluate the efficiency of rule learning and quality of rules\nfrom IterE compared with AMIE+, showing that IterE is capable of generating\nhigh quality rules more efficiently. Experiments show that iteratively learning\nembeddings and rules benefit each other during learning and prediction.", "published": "2019-03-21 12:26:44", "link": "http://arxiv.org/abs/1903.08948v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Recent advances in conversational NLP : Towards the standardization of\n  Chatbot building", "abstract": "Dialogue systems have become recently essential in our life. Their use is\ngetting more and more fluid and easy throughout the time. This boils down to\nthe improvements made in NLP and AI fields. In this paper, we try to provide an\noverview to the current state of the art of dialogue systems, their categories\nand the different approaches to build them. We end up with a discussion that\ncompares all the techniques and analyzes the strengths and weaknesses of each.\nFinally, we present an opinion piece suggesting to orientate the research\ntowards the standardization of dialogue systems building.", "published": "2019-03-21 14:30:55", "link": "http://arxiv.org/abs/1903.09025v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Low Resource Text Classification with ULMFit and Backtranslation", "abstract": "In computer vision, virtually every state-of-the-art deep learning system is\ntrained with data augmentation. In text classification, however, data\naugmentation is less widely practiced because it must be performed before\ntraining and risks introducing label noise. We augment the IMDB movie reviews\ndataset with examples generated by two families of techniques: random token\nperturbations introduced by Wei and Zou [2019] and backtranslation --\ntranslating to a second language then back to English. In low resource\nenvironments, backtranslation generates significant improvement on top of the\nstate of-the-art ULMFit model. A ULMFit model pretrained on wikitext103 and\nthen fine-tuned on only 50 IMDB examples and 500 synthetic examples generated\nby backtranslation achieves 80.6% accuracy, an 8.1% improvement over the\naugmentation-free baseline with only 9 minutes of additional training time.\nRandom token perturbations do not yield any improvements but incur equivalent\ncomputational cost. The benefits of training with backtranslated examples\ndecreases with the size of the available training data. On the full dataset,\nneither augmentation technique improves upon ULMFit's state of the art\nperformance. We address this by using backtranslations as a form of test time\naugmentation as well as ensembling ULMFit with other models, and achieve small\nimprovements.", "published": "2019-03-21 21:40:09", "link": "http://arxiv.org/abs/1903.09244v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Inferring Compact Representations for Efficient Natural Language\n  Understanding of Robot Instructions", "abstract": "The speed and accuracy with which robots are able to interpret natural\nlanguage is fundamental to realizing effective human-robot interaction. A great\ndeal of attention has been paid to developing models and approximate inference\nalgorithms that improve the efficiency of language understanding. However,\nexisting methods still attempt to reason over a representation of the\nenvironment that is flat and unnecessarily detailed, which limits scalability.\nAn open problem is then to develop methods capable of producing the most\ncompact environment model sufficient for accurate and efficient natural\nlanguage understanding. We propose a model that leverages environment-related\ninformation encoded within instructions to identify the subset of observations\nand perceptual classifiers necessary to perceive a succinct,\ninstruction-specific environment representation. The framework uses three\nprobabilistic graphical models trained from a corpus of annotated instructions\nto infer salient scene semantics, perceptual classifiers, and grounded symbols.\nExperimental results on two robots operating in different environments\ndemonstrate that by exploiting the content and the structure of the\ninstructions, our method learns compact environment representations that\nsignificantly improve the efficiency of natural language symbol grounding.", "published": "2019-03-21 21:38:20", "link": "http://arxiv.org/abs/1903.09243v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Data-driven design of perfect reconstruction filterbank for DNN-based\n  sound source enhancement", "abstract": "We propose a data-driven design method of perfect-reconstruction filterbank\n(PRFB) for sound-source enhancement (SSE) based on deep neural network (DNN).\nDNNs have been used to estimate a time-frequency (T-F) mask in the short-time\nFourier transform (STFT) domain. Their training is more stable when a simple\ncost function as mean-squared error (MSE) is utilized comparing to some\nadvanced cost such as objective sound quality assessments. However, such a\nsimple cost function inherits strong assumptions on the statistics of the\ntarget and/or noise which is often not satisfied, and the mismatch of\nassumption results in degraded performance. In this paper, we propose to design\nthe frequency scale of PRFB from training data so that the assumption on MSE is\nsatisfied. For designing the frequency scale, the warped filterbank frame\n(WFBF) is considered as PRFB. The frequency characteristic of learned WFBF was\nin between STFT and the wavelet transform, and its effectiveness was confirmed\nby comparison with a standard STFT-based DNN whose input feature is compressed\ninto the mel scale.", "published": "2019-03-21 08:50:12", "link": "http://arxiv.org/abs/1903.08876v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Bandwidth Extension on Raw Audio via Generative Adversarial Networks", "abstract": "Neural network-based methods have recently demonstrated state-of-the-art\nresults on image synthesis and super-resolution tasks, in particular by using\nvariants of generative adversarial networks (GANs) with supervised feature\nlosses. Nevertheless, previous feature loss formulations rely on the\navailability of large auxiliary classifier networks, and labeled datasets that\nenable such classifiers to be trained. Furthermore, there has been\ncomparatively little work to explore the applicability of GAN-based methods to\ndomains other than images and video. In this work we explore a GAN-based method\nfor audio processing, and develop a convolutional neural network architecture\nto perform audio super-resolution. In addition to several new architectural\nbuilding blocks for audio processing, a key component of our approach is the\nuse of an autoencoder-based loss that enables training in the GAN framework,\nwith feature losses derived from unlabeled data. We explore the impact of our\narchitectural choices, and demonstrate significant improvements over previous\nworks in terms of both objective and perceptual quality.", "published": "2019-03-21 14:32:02", "link": "http://arxiv.org/abs/1903.09027v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Machine Hearing on Limited Data Sets", "abstract": "Convolutional neural network (CNN) architectures have originated and\nrevolutionized machine learning for images. In order to take advantage of CNNs\nin predictive modeling with audio data, standard FFT-based signal processing\nmethods are often applied to convert the raw audio waveforms into an image-like\nrepresentations (e.g. spectrograms). Even though conventional images and\nspectrograms differ in their feature properties, this kind of pre-processing\nreduces the amount of training data necessary for successful training. In this\ncontribution we investigate how input and target representations interplay with\nthe amount of available training data in a music information retrieval setting.\nWe compare the standard mel-spectrogram inputs with a newly proposed\nrepresentation, called Mel scattering. Furthermore, we investigate the impact\nof additional target data representations by using an augmented target loss\nfunction which incorporates unused available information. We observe that all\nproposed methods outperform the standard mel-transform representation when\nusing a limited data set and discuss their strengths and limitations. The\nsource code for reproducibility of our experiments as well as intermediate\nresults and model checkpoints are available in an online repository.", "published": "2019-03-21 12:29:44", "link": "http://arxiv.org/abs/1903.08950v3", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP", "stat.ML"], "primary_category": "cs.SD"}
