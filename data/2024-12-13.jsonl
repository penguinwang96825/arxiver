{"title": "Reciprocity in Interbank Markets", "abstract": "Weighted reciprocity between two agents can be defined as the minimum of\nsending and receiving value in their bilateral relationship. In financial\nnetworks, such reciprocity characterizes the importance of individual banks as\nboth liquidity absorber and provider, a feature typically attributed to large,\nintermediating dealer banks. In this paper we develop an exponential random\ngraph model that can account for reciprocal links of each node simultaneously\non the topological as well as on the weighted level. We provide an exact\nexpression for the normalizing constant and thus a closed-form solution for the\ngraph probability distribution. Applying this statistical null model to Italian\ninterbank data, we find that before the great financial crisis (i) banks\ndisplayed significantly more weighted reciprocity compared to what the\nlower-order network features (size and volume distributions) would predict (ii)\nwith a disappearance of this deviation once the early periods of the crisis set\nin, (iii) a trend which can be attributed in particular to smaller banks\n(dis)engaging in bilateral high-value trading relationships. Moreover, we show\nthat neglecting reciprocal links and weights can lead to spurious findings of\ntriadic relationships. As the hierarchical structure in the network is found to\nbe compatible with its transitive but not with its intransitive triadic\nsub-graphs, the interbank market seems to be well-characterized by a\nhierarchical core-periphery structure enhanced by non-hierarchical reciprocal\ntrading relationships.", "published": "2024-12-13 18:07:12", "link": "http://arxiv.org/abs/2412.10329v1", "categories": ["q-fin.CP", "physics.data-an", "physics.soc-ph"], "primary_category": "q-fin.CP"}
{"title": "Integrative Analysis of Financial Market Sentiment Using CNN and GRU for Risk Prediction and Alert Systems", "abstract": "This document presents an in-depth examination of stock market sentiment\nthrough the integration of Convolutional Neural Networks (CNN) and Gated\nRecurrent Units (GRU), enabling precise risk alerts. The robust feature\nextraction capability of CNN is utilized to preprocess and analyze extensive\nnetwork text data, identifying local features and patterns. The extracted\nfeature sequences are then input into the GRU model to understand the\nprogression of emotional states over time and their potential impact on future\nmarket sentiment and risk. This approach addresses the order dependence and\nlong-term dependencies inherent in time series data, resulting in a detailed\nanalysis of stock market sentiment and effective early warnings of future\nrisks.", "published": "2024-12-13 15:17:23", "link": "http://arxiv.org/abs/2412.10199v1", "categories": ["cs.LG", "q-fin.CP"], "primary_category": "cs.LG"}
{"title": "Financial Fine-tuning a Large Time Series Model", "abstract": "Large models have shown unprecedented capabilities in natural language\nprocessing, image generation, and most recently, time series forecasting. This\nleads us to ask the question: treating market prices as a time series, can\nlarge models be used to predict the market? In this paper, we answer this by\nevaluating the performance of the latest time series foundation model TimesFM\non price prediction. We find that due to the irregular nature of price data,\ndirectly applying TimesFM gives unsatisfactory results and propose to fine-tune\nTimeFM on financial data for the task of price prediction. This is done by\ncontinual pre-training of the latest time series foundation model TimesFM on\nprice data containing 100 million time points, spanning a range of financial\ninstruments spanning hourly and daily granularities. The fine-tuned model\ndemonstrates higher price prediction accuracy than the baseline model. We\nconduct mock trading for our model in various financial markets and show that\nit outperforms various benchmarks in terms of returns, sharpe ratio, max\ndrawdown and trading cost.", "published": "2024-12-13 05:51:00", "link": "http://arxiv.org/abs/2412.09880v1", "categories": ["q-fin.CP", "cs.LG"], "primary_category": "q-fin.CP"}
{"title": "Self-Exciting Random Evolutions (SEREs) and their Applications (Version 2)", "abstract": "This paper is devoted to the study of a new class of random evolutions (RE),\nso-called self-exciting random evolutions (SEREs), and their applications. We\nalso introduce a new random process $x(t)$ such that it is based on a\nsuperposition of a Markov chain $x_n$ and a Hawkes process $N(t),$ i.e.,\n$x(t):=x_{N(t)}.$ We call this process self-walking imbedded semi-Hawkes\nprocess (Swish Process or SwishP). Then the self-exciting REs (SEREs) can be\nconstructed in similar way as, e.g., semi-Markov REs, but instead of\nsemi-Markov process $x(t)$ we have SwishP. We give classifications and examples\nof self-exciting REs (SEREs). Then we consider two limit theorems for SEREs\nsuch as averaging (Theorem 1) and diffusion approximation (Theorem 2).\nApplications of SEREs are devoted to the so-called self-exciting\ntraffic/transport process and self-exciting summation on a Markov chain, which\nare examples of continuous and discrete SERE. From these processes we can\nconstruct many other self-exciting processes, e.g., such as impulse\ntraffic/transport process, self-exciting risk process, general compound Hawkes\nprocess for a stock price, etc. We present averaged and diffusion approximation\nof self-exciting processes. The novelty of the paper associated with new\nmodels, such as $x(t)$ and SERE, and also new features of SEREs and their many\napplications, namely, self-exciting and clustering effects.", "published": "2024-12-13 22:27:20", "link": "http://arxiv.org/abs/2412.10592v2", "categories": ["math.PR", "q-fin.MF", "60G55, 60H25, 47H40, 47D06, 60H20, 60F17, 60F05"], "primary_category": "math.PR"}
{"title": "Higher Order Transformers: Enhancing Stock Movement Prediction On Multimodal Time-Series Data", "abstract": "In this paper, we tackle the challenge of predicting stock movements in\nfinancial markets by introducing Higher Order Transformers, a novel\narchitecture designed for processing multivariate time-series data. We extend\nthe self-attention mechanism and the transformer architecture to a higher\norder, effectively capturing complex market dynamics across time and variables.\nTo manage computational complexity, we propose a low-rank approximation of the\npotentially large attention tensor using tensor decomposition and employ kernel\nattention, reducing complexity to linear with respect to the data size.\nAdditionally, we present an encoder-decoder model that integrates technical and\nfundamental analysis, utilizing multimodal signals from historical prices and\nrelated tweets. Our experiments on the Stocknet dataset demonstrate the\neffectiveness of our method, highlighting its potential for enhancing stock\nmovement prediction in financial markets.", "published": "2024-12-13 20:26:35", "link": "http://arxiv.org/abs/2412.10540v1", "categories": ["cs.LG", "q-fin.ST"], "primary_category": "cs.LG"}
