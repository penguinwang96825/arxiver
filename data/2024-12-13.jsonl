{"title": "Higher Order Transformers: Enhancing Stock Movement Prediction On Multimodal Time-Series Data", "abstract": "In this paper, we tackle the challenge of predicting stock movements in\nfinancial markets by introducing Higher Order Transformers, a novel\narchitecture designed for processing multivariate time-series data. We extend\nthe self-attention mechanism and the transformer architecture to a higher\norder, effectively capturing complex market dynamics across time and variables.\nTo manage computational complexity, we propose a low-rank approximation of the\npotentially large attention tensor using tensor decomposition and employ kernel\nattention, reducing complexity to linear with respect to the data size.\nAdditionally, we present an encoder-decoder model that integrates technical and\nfundamental analysis, utilizing multimodal signals from historical prices and\nrelated tweets. Our experiments on the Stocknet dataset demonstrate the\neffectiveness of our method, highlighting its potential for enhancing stock\nmovement prediction in financial markets.", "published": "2024-12-13 20:26:35", "link": "http://arxiv.org/abs/2412.10540v1", "categories": ["cs.LG", "q-fin.ST"], "primary_category": "cs.LG"}
