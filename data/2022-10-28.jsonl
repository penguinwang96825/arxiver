{"title": "Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction", "abstract": "Recently, prompt-tuning has attracted growing interests in event argument\nextraction (EAE). However, the existing prompt-tuning methods have not achieved\nsatisfactory performance due to the lack of consideration of entity\ninformation. In this paper, we propose a bi-directional iterative prompt-tuning\nmethod for EAE, where the EAE task is treated as a cloze-style task to take\nfull advantage of entity information and pre-trained language models (PLMs).\nFurthermore, our method explores event argument interactions by introducing the\nargument roles of contextual entities into prompt construction. Since template\nand verbalizer are two crucial components in a cloze-style prompt, we propose\nto utilize the role label semantic knowledge to construct a semantic verbalizer\nand design three kinds of templates for the EAE task. Experiments on the ACE\n2005 English dataset with standard and low-resource settings show that the\nproposed method significantly outperforms the peer state-of-the-art methods.\nOur code is available at https://github.com/HustMinsLab/BIP.", "published": "2022-10-28 02:31:59", "link": "http://arxiv.org/abs/2210.15843v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Zero-Shot Multilingual Translation with Universal\n  Representations and Cross-Mappings", "abstract": "The many-to-many multilingual neural machine translation can translate\nbetween language pairs unseen during training, i.e., zero-shot translation.\nImproving zero-shot translation requires the model to learn universal\nrepresentations and cross-mapping relationships to transfer the knowledge\nlearned on the supervised directions to the zero-shot directions. In this work,\nwe propose the state mover's distance based on the optimal theory to model the\ndifference of the representations output by the encoder. Then, we bridge the\ngap between the semantic-equivalent representations of different languages at\nthe token level by minimizing the proposed distance to learn universal\nrepresentations. Besides, we propose an agreement-based training scheme, which\ncan help the model make consistent predictions based on the semantic-equivalent\nsentences to learn universal cross-mapping relationships for all translation\ndirections. The experimental results on diverse multilingual datasets show that\nour method can improve consistently compared with the baseline system and other\ncontrast methods. The analysis proves that our method can better align the\nsemantic space and improve the prediction consistency.", "published": "2022-10-28 02:47:05", "link": "http://arxiv.org/abs/2210.15851v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Adaptation of Machine Translation with Crowdworkers", "abstract": "Although a machine translation model trained with a large in-domain parallel\ncorpus achieves remarkable results, it still works poorly when no in-domain\ndata are available. This situation restricts the applicability of machine\ntranslation when the target domain's data are limited. However, there is great\ndemand for high-quality domain-specific machine translation models for many\ndomains. We propose a framework that efficiently and effectively collects\nparallel sentences in a target domain from the web with the help of\ncrowdworkers. With the collected parallel data, we can quickly adapt a machine\ntranslation model to the target domain. Our experiments show that the proposed\nmethod can collect target-domain parallel data over a few days at a reasonable\ncost. We tested it with five domains, and the domain-adapted model improved the\nBLEU scores to +19.7 by an average of +7.8 points compared to a general-purpose\ntranslation model.", "published": "2022-10-28 03:11:17", "link": "http://arxiv.org/abs/2210.15861v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"It's Not Just Hate'': A Multi-Dimensional Perspective on Detecting\n  Harmful Speech Online", "abstract": "Well-annotated data is a prerequisite for good Natural Language Processing\nmodels. Too often, though, annotation decisions are governed by optimizing time\nor annotator agreement. We make a case for nuanced efforts in an\ninterdisciplinary setting for annotating offensive online speech. Detecting\noffensive content is rapidly becoming one of the most important real-world NLP\ntasks. However, most datasets use a single binary label, e.g., for hate or\nincivility, even though each concept is multi-faceted. This modeling choice\nseverely limits nuanced insights, but also performance. We show that a more\nfine-grained multi-label approach to predicting incivility and hateful or\nintolerant content addresses both conceptual and performance issues. We release\na novel dataset of over 40,000 tweets about immigration from the US and UK,\nannotated with six labels for different aspects of incivility and intolerance.\nOur dataset not only allows for a more nuanced understanding of harmful speech\nonline, models trained on it also outperform or match performance on benchmark\ndatasets.", "published": "2022-10-28 03:34:50", "link": "http://arxiv.org/abs/2210.15870v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RoChBert: Towards Robust BERT Fine-tuning for Chinese", "abstract": "Despite of the superb performance on a wide range of tasks, pre-trained\nlanguage models (e.g., BERT) have been proved vulnerable to adversarial texts.\nIn this paper, we present RoChBERT, a framework to build more Robust BERT-based\nmodels by utilizing a more comprehensive adversarial graph to fuse Chinese\nphonetic and glyph features into pre-trained representations during\nfine-tuning. Inspired by curriculum learning, we further propose to augment the\ntraining dataset with adversarial texts in combination with intermediate\nsamples. Extensive experiments demonstrate that RoChBERT outperforms previous\nmethods in significant ways: (i) robust -- RoChBERT greatly improves the model\nrobustness without sacrificing accuracy on benign texts. Specifically, the\ndefense lowers the success rates of unlimited and limited attacks by 59.43% and\n39.33% respectively, while remaining accuracy of 93.30%; (ii) flexible --\nRoChBERT can easily extend to various language models to solve different\ndownstream tasks with excellent performance; and (iii) efficient -- RoChBERT\ncan be directly applied to the fine-tuning stage without pre-training language\nmodel from scratch, and the proposed data augmentation method is also low-cost.", "published": "2022-10-28 07:08:00", "link": "http://arxiv.org/abs/2210.15944v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stanceosaurus: Classifying Stance Towards Multilingual Misinformation", "abstract": "We present Stanceosaurus, a new corpus of 28,033 tweets in English, Hindi,\nand Arabic annotated with stance towards 251 misinformation claims. As far as\nwe are aware, it is the largest corpus annotated with stance towards\nmisinformation claims. The claims in Stanceosaurus originate from 15\nfact-checking sources that cover diverse geographical regions and cultures.\nUnlike existing stance datasets, we introduce a more fine-grained 5-class\nlabeling strategy with additional subcategories to distinguish implicit stance.\nPre-trained transformer-based stance classifiers that are fine-tuned on our\ncorpus show good generalization on unseen claims and regional claims from\ncountries outside the training data. Cross-lingual experiments demonstrate\nStanceosaurus' capability of training multi-lingual models, achieving 53.1 F1\non Hindi and 50.4 F1 on Arabic without any target-language fine-tuning.\nFinally, we show how a domain adaptation method can be used to improve\nperformance on Stanceosaurus using additional RumourEval-2019 data. We make\nStanceosaurus publicly available to the research community and hope it will\nencourage further work on misinformation identification across languages and\ncultures.", "published": "2022-10-28 07:18:32", "link": "http://arxiv.org/abs/2210.15954v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Development of a rule-based lemmatization algorithm through Finite State\n  Machine for Uzbek language", "abstract": "Lemmatization is one of the core concepts in natural language processing,\nthus creating a lemmatization tool is an important task. This paper discusses\nthe construction of a lemmatization algorithm for the Uzbek language. The main\npurpose of the work is to remove affixes of words in the Uzbek language by\nmeans of the finite state machine and to identify a lemma (a word that can be\nfound in the dictionary) of the word. The process of removing affixes uses a\ndatabase of affixes and part of speech knowledge. This lemmatization consists\nof the general rules and a part of speech data of the Uzbek language, affixes,\nclassification of affixes, removing affixes on the basis of the finite state\nmachine for each class, as well as a definition of this word lemma.", "published": "2022-10-28 09:21:06", "link": "http://arxiv.org/abs/2210.16006v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UzbekStemmer: Development of a Rule-Based Stemming Algorithm for Uzbek\n  Language", "abstract": "In this paper we present a rule-based stemming algorithm for the Uzbek\nlanguage. Uzbek is an agglutinative language, so many words are formed by\nadding suffixes, and the number of suffixes is also large. For this reason, it\nis difficult to find a stem of words. The methodology is proposed for doing the\nstemming of the Uzbek words with an affix stripping approach whereas not\nincluding any database of the normal word forms of the Uzbek language. Word\naffixes are classified into fifteen classes and designed as finite state\nmachines (FSMs) for each class according to morphological rules. We created\nfifteen FSMs and linked them together to create the Basic FSM. A lexicon of\naffixes in XML format was created and a stemming application for Uzbek words\nhas been developed based on the FSMs.", "published": "2022-10-28 09:29:22", "link": "http://arxiv.org/abs/2210.16011v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DORE: Document Ordered Relation Extraction based on Generative Framework", "abstract": "In recent years, there is a surge of generation-based information extraction\nwork, which allows a more direct use of pre-trained language models and\nefficiently captures output dependencies. However, previous generative methods\nusing lexical representation do not naturally fit document-level relation\nextraction (DocRE) where there are multiple entities and relational facts. In\nthis paper, we investigate the root cause of the underwhelming performance of\nthe existing generative DocRE models and discover that the culprit is the\ninadequacy of the training paradigm, instead of the capacities of the models.\nWe propose to generate a symbolic and ordered sequence from the relation matrix\nwhich is deterministic and easier for model to learn. Moreover, we design a\nparallel row generation method to process overlong target sequences. Besides,\nwe introduce several negative sampling strategies to improve the performance\nwith balanced signals. Experimental results on four datasets show that our\nproposed method can improve the performance of the generative DocRE models. We\nhave released our code at https://github.com/ayyyq/DORE.", "published": "2022-10-28 11:18:10", "link": "http://arxiv.org/abs/2210.16064v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Debiasing Masks: A New Framework for Shortcut Mitigation in NLU", "abstract": "Debiasing language models from unwanted behaviors in Natural Language\nUnderstanding tasks is a topic with rapidly increasing interest in the NLP\ncommunity. Spurious statistical correlations in the data allow models to\nperform shortcuts and avoid uncovering more advanced and desirable linguistic\nfeatures. A multitude of effective debiasing approaches has been proposed, but\nflexibility remains a major issue. For the most part, models must be retrained\nto find a new set of weights with debiased behavior. We propose a new debiasing\nmethod in which we identify debiased pruning masks that can be applied to a\nfinetuned model. This enables the selective and conditional application of\ndebiasing behaviors. We assume that bias is caused by a certain subset of\nweights in the network; our method is, in essence, a mask search to identify\nand remove biased weights. Our masks show equivalent or superior performance to\nthe standard counterparts, while offering important benefits. Pruning masks can\nbe stored with high efficiency in memory, and it becomes possible to switch\namong several debiasing behaviors (or revert back to the original biased model)\nat inference time. Finally, it opens the doors to further research on how\nbiases are acquired by studying the generated masks. For example, we observed\nthat the early layers and attention heads were pruned more aggressively,\npossibly hinting towards the location in which biases may be encoded.", "published": "2022-10-28 11:57:55", "link": "http://arxiv.org/abs/2210.16079v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling structure-building in the brain with CCG parsing and large\n  language models", "abstract": "To model behavioral and neural correlates of language comprehension in\nnaturalistic environments researchers have turned to broad-coverage tools from\nnatural-language processing and machine learning. Where syntactic structure is\nexplicitly modeled, prior work has relied predominantly on context-free\ngrammars (CFG), yet such formalisms are not sufficiently expressive for human\nlanguages. Combinatory Categorial Grammars (CCGs) are sufficiently expressive\ndirectly compositional models of grammar with flexible constituency that\naffords incremental interpretation. In this work we evaluate whether a more\nexpressive CCG provides a better model than a CFG for human neural signals\ncollected with fMRI while participants listen to an audiobook story. We further\ntest between variants of CCG that differ in how they handle optional adjuncts.\nThese evaluations are carried out against a baseline that includes estimates of\nnext-word predictability from a Transformer neural network language model. Such\na comparison reveals unique contributions of CCG structure-building\npredominantly in the left posterior temporal lobe: CCG-derived measures offer a\nsuperior fit to neural signals compared to those derived from a CFG. These\neffects are spatially distinct from bilateral superior temporal effects that\nare unique to predictability. Neural effects for structure-building are thus\nseparable from predictability during naturalistic listening, and those effects\nare best characterized by a grammar whose expressive power is motivated on\nindependent linguistic grounds.", "published": "2022-10-28 14:21:29", "link": "http://arxiv.org/abs/2210.16147v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing for targeted syntactic knowledge through grammatical error\n  detection", "abstract": "Targeted studies testing knowledge of subject-verb agreement (SVA) indicate\nthat pre-trained language models encode syntactic information. We assert that\nif models robustly encode subject-verb agreement, they should be able to\nidentify when agreement is correct and when it is incorrect. To that end, we\npropose grammatical error detection as a diagnostic probe to evaluate\ntoken-level contextual representations for their knowledge of SVA. We evaluate\ncontextual representations at each layer from five pre-trained English language\nmodels: BERT, XLNet, GPT-2, RoBERTa, and ELECTRA. We leverage public annotated\ntraining data from both English second language learners and Wikipedia edits,\nand report results on manually crafted stimuli for subject-verb agreement. We\nfind that masked language models linearly encode information relevant to the\ndetection of SVA errors, while the autoregressive models perform on par with\nour baseline. However, we also observe a divergence in performance when probes\nare trained on different training sets, and when they are evaluated on\ndifferent syntactic constructions, suggesting the information pertaining to SVA\nerror detection is not robustly encoded.", "published": "2022-10-28 16:01:25", "link": "http://arxiv.org/abs/2210.16228v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Solving Math Word Problems via Cooperative Reasoning induced Language\n  Models", "abstract": "Large-scale pre-trained language models (PLMs) bring new opportunities to\nchallenging problems, especially those that need high-level intelligence, such\nas the math word problem (MWPs). However, directly applying existing PLMs to\nMWPs can fail as the generation process lacks sufficient supervision and thus\nlacks fast adaptivity as humans. We notice that human reasoning has a dual\nreasoning framework that consists of an immediate reaction system (system 1)\nand a delicate reasoning system (system 2), where the entire reasoning is\ndetermined by their interaction. This inspires us to develop a cooperative\nreasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe),\nresulting in a human-like reasoning architecture with system 1 as the generator\nand system 2 as the verifier. In our approach, the generator is responsible for\ngenerating reasoning paths, and the verifiers are used to supervise the\nevaluation in order to obtain reliable feedback for the generator. We evaluate\nour CoRe framework on several mathematical reasoning datasets and achieve\ndecent improvement over state-of-the-art methods, up to 9.6% increase over best\nbaselines. Our codes are available at https://github.com/TianHongZXY/CoRe", "published": "2022-10-28 16:47:03", "link": "http://arxiv.org/abs/2210.16257v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AGReE: A system for generating Automated Grammar Reading Exercises", "abstract": "We describe the AGReE system, which takes user-submitted passages as input\nand automatically generates grammar practice exercises that can be completed\nwhile reading. Multiple-choice practice items are generated for a variety of\ndifferent grammar constructs: punctuation, articles, conjunctions, pronouns,\nprepositions, verbs, and nouns. We also conducted a large-scale human\nevaluation with around 4,500 multiple-choice practice items. We notice for 95%\nof items, a majority of raters out of five were able to identify the correct\nanswer and for 85% of cases, raters agree that there is only one correct answer\namong the choices. Finally, the error analysis shows that raters made the most\nmistakes for punctuation and conjunctions.", "published": "2022-10-28 17:58:04", "link": "http://arxiv.org/abs/2210.16302v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Radically Lower Data-Labeling Costs for Visually Rich Document\n  Extraction Models", "abstract": "A key bottleneck in building automatic extraction models for visually rich\ndocuments like invoices is the cost of acquiring the several thousand\nhigh-quality labeled documents that are needed to train a model with acceptable\naccuracy. We propose Selective Labeling to simplify the labeling task to\nprovide \"yes/no\" labels for candidate extractions predicted by a model trained\non partially labeled documents. We combine this with a custom active learning\nstrategy to find the predictions that the model is most uncertain about. We\nshow through experiments on document types drawn from 3 different domains that\nselective labeling can reduce the cost of acquiring labeled data by $10\\times$\nwith a negligible loss in accuracy.", "published": "2022-10-28 20:10:16", "link": "http://arxiv.org/abs/2210.16391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Just-DREAM-about-it: Figurative Language Understanding with DREAM-FLUTE", "abstract": "Figurative language (e.g., \"he flew like the wind\") is challenging to\nunderstand, as it is hard to tell what implicit information is being conveyed\nfrom the surface form alone. We hypothesize that to perform this task well, the\nreader needs to mentally elaborate the scene being described to identify a\nsensible meaning of the language. We present DREAM-FLUTE, a figurative language\nunderstanding system that does this, first forming a \"mental model\" of\nsituations described in a premise and hypothesis before making an\nentailment/contradiction decision and generating an explanation. DREAM-FLUTE\nuses an existing scene elaboration model, DREAM, for constructing its \"mental\nmodel.\" In the FigLang2022 Shared Task evaluation, DREAM-FLUTE achieved (joint)\nfirst place (Acc@60=63.3%), and can perform even better with ensemble\ntechniques, demonstrating the effectiveness of this approach. More generally,\nthis work suggests that adding a reflective component to pretrained language\nmodels can improve their performance beyond standard fine-tuning (3.3%\nimprovement in Acc@60).", "published": "2022-10-28 21:14:23", "link": "http://arxiv.org/abs/2210.16407v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Unifying Text Segmentation and Long Document Summarization", "abstract": "Text segmentation is important for signaling a document's structure. Without\nsegmenting a long document into topically coherent sections, it is difficult\nfor readers to comprehend the text, let alone find important information. The\nproblem is only exacerbated by a lack of segmentation in transcripts of\naudio/video recordings. In this paper, we explore the role that section\nsegmentation plays in extractive summarization of written and spoken documents.\nOur approach learns robust sentence representations by performing summarization\nand segmentation simultaneously, which is further enhanced by an\noptimization-based regularizer to promote selection of diverse summary\nsentences. We conduct experiments on multiple datasets ranging from scientific\narticles to spoken transcripts to evaluate the model's performance. Our\nfindings suggest that the model can not only achieve state-of-the-art\nperformance on publicly available benchmarks, but demonstrate better\ncross-genre transferability when equipped with text segmentation. We perform a\nseries of analyses to quantify the impact of section segmentation on\nsummarizing written and spoken documents of substantial length and complexity.", "published": "2022-10-28 22:07:10", "link": "http://arxiv.org/abs/2210.16422v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language\n  Models", "abstract": "Fully-parametric language models generally require a huge number of model\nparameters to store the necessary knowledge for solving multiple natural\nlanguage tasks in zero/few-shot settings. In addition, it is hard to adapt to\nthe evolving world knowledge without the costly model re-training. In this\npaper, we develop a novel semi-parametric language model architecture,\nKnowledge-in-Context (KiC), which empowers a parametric text-to-text language\nmodel with a knowledge-rich external memory. Specifically, the external memory\ncontains six different types of knowledge: entity, dictionary, commonsense,\nevent, script, and causality knowledge. For each input instance, the KiC model\nadaptively selects a knowledge type and retrieves the most helpful pieces of\nknowledge. The input instance along with its knowledge augmentation is fed into\na text-to-text model (e.g., T5) to generate the output answer, where both the\ninput and the output are in natural language forms after prompting.\nInterestingly, we find that KiC can be identified as a special\nmixture-of-experts (MoE) model, where the knowledge selector plays the role of\na router that is used to determine the sequence-to-expert assignment in MoE.\nThis key observation inspires us to develop a novel algorithm for training KiC\nwith an instance-adaptive knowledge selector. As a knowledge-rich\nsemi-parametric language model, KiC only needs a much smaller parametric part\nto achieve superior zero-shot performance on unseen tasks. By evaluating on 40+\ndifferent tasks, we show that KiC_Large with 770M parameters easily outperforms\nlarge language models (LMs) that are 4-39x larger by a large margin. We also\ndemonstrate that KiC exhibits emergent abilities at a much smaller model scale\ncompared to the fully-parametric models.", "published": "2022-10-28 23:18:43", "link": "http://arxiv.org/abs/2210.16433v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Moving beyond word lists: towards abstractive topic labels for\n  human-like topics of scientific documents", "abstract": "Topic models represent groups of documents as a list of words (the topic\nlabels). This work asks whether an alternative approach to topic labeling can\nbe developed that is closer to a natural language description of a topic than a\nword list. To this end, we present an approach to generating human-like topic\nlabels using abstractive multi-document summarization (MDS). We investigate our\napproach with an exploratory case study. We model topics in citation sentences\nin order to understand what further research needs to be done to fully\noperationalize MDS for topic labeling. Our case study shows that in addition to\nmore human-like topics there are additional advantages to evaluation by using\nclustering and summarization measures instead of topic model measures. However,\nwe find that there are several developments needed before we can design a\nwell-powered study to evaluate MDS for topic modeling fully. Namely, improving\ncluster cohesion, improving the factuality and faithfulness of MDS, and\nincreasing the number of documents that might be supported by MDS. We present a\nnumber of ideas on how these can be tackled and conclude with some thoughts on\nhow topic modeling can also be used to improve MDS in general.", "published": "2022-10-28 17:47:12", "link": "http://arxiv.org/abs/2211.05599v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "You can't pick your neighbors, or can you? When and how to rely on\n  retrieval in the $k$NN-LM", "abstract": "Retrieval-enhanced language models (LMs), which condition their predictions\non text retrieved from large external datastores, have recently shown\nsignificant perplexity improvements compared to standard LMs. One such\napproach, the $k$NN-LM, interpolates any existing LM's predictions with the\noutput of a $k$-nearest neighbors model and requires no additional training. In\nthis paper, we explore the importance of lexical and semantic matching in the\ncontext of items retrieved by $k$NN-LM. We find two trends: (1) the presence of\nlarge overlapping $n$-grams between the datastore and evaluation set plays an\nimportant factor in strong performance, even when the datastore is derived from\nthe training data; and (2) the $k$NN-LM is most beneficial when retrieved items\nhave high semantic similarity with the query. Based on our analysis, we define\na new formulation of the $k$NN-LM that uses retrieval quality to assign the\ninterpolation coefficient. We empirically measure the effectiveness of our\napproach on two English language modeling datasets, Wikitext-103 and PG-19. Our\nre-formulation of the $k$NN-LM is beneficial in both cases, and leads to nearly\n4% improvement in perplexity on the Wikitext-103 test set.", "published": "2022-10-28 02:57:40", "link": "http://arxiv.org/abs/2210.15859v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "When Life Gives You Lemons, Make Cherryade: Converting Feedback from Bad\n  Responses into Good Labels", "abstract": "Deployed dialogue agents have the potential to integrate human feedback to\ncontinuously improve themselves. However, humans may not always provide\nexplicit signals when the chatbot makes mistakes during interactions. In this\nwork, we propose Juicer, a framework to make use of both binary and free-form\ntextual human feedback. It works by: (i) extending sparse binary feedback by\ntraining a satisfaction classifier to label the unlabeled data; and (ii)\ntraining a reply corrector to map the bad replies to good ones. We find that\naugmenting training with model-corrected replies improves the final dialogue\nmodel, and we can further improve performance by using both positive and\nnegative replies through the recently proposed Director model.", "published": "2022-10-28 04:57:21", "link": "http://arxiv.org/abs/2210.15893v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BEBERT: Efficient and Robust Binary Ensemble BERT", "abstract": "Pre-trained BERT models have achieved impressive accuracy on natural language\nprocessing (NLP) tasks. However, their excessive amount of parameters hinders\nthem from efficient deployment on edge devices. Binarization of the BERT models\ncan significantly alleviate this issue but comes with a severe accuracy drop\ncompared with their full-precision counterparts. In this paper, we propose an\nefficient and robust binary ensemble BERT (BEBERT) to bridge the accuracy gap.\nTo the best of our knowledge, this is the first work employing ensemble\ntechniques on binary BERTs, yielding BEBERT, which achieves superior accuracy\nwhile retaining computational efficiency. Furthermore, we remove the knowledge\ndistillation procedures during ensemble to speed up the training process\nwithout compromising accuracy. Experimental results on the GLUE benchmark show\nthat the proposed BEBERT significantly outperforms the existing binary BERT\nmodels in accuracy and robustness with a 2x speedup on training time. Moreover,\nour BEBERT has only a negligible accuracy loss of 0.3% compared to the\nfull-precision baseline while saving 15x and 13x in FLOPs and model size,\nrespectively. In addition, BEBERT also outperforms other compressed BERTs in\naccuracy by up to 6.7%.", "published": "2022-10-28 08:15:26", "link": "http://arxiv.org/abs/2210.15976v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal\n  Guidance", "abstract": "Diffusion generative models have recently greatly improved the power of\ntext-conditioned image generation. Existing image generation models mainly\ninclude text conditional diffusion model and cross-modal guided diffusion\nmodel, which are good at small scene image generation and complex scene image\ngeneration respectively. In this work, we propose a simple yet effective\napproach, namely UPainting, to unify simple and complex scene image generation,\nas shown in Figure 1. Based on architecture improvements and diverse guidance\nschedules, UPainting effectively integrates cross-modal guidance from a\npretrained image-text matching model into a text conditional diffusion model\nthat utilizes a pretrained Transformer language model as the text encoder. Our\nkey findings is that combining the power of large-scale Transformer language\nmodel in understanding language and image-text matching model in capturing\ncross-modal semantics and style, is effective to improve sample fidelity and\nimage-text alignment of image generation. In this way, UPainting has a more\ngeneral image generation capability, which can generate images of both simple\nand complex scenes more effectively. To comprehensively compare text-to-image\nmodels, we further create a more general benchmark, UniBench, with well-written\nChinese and English prompts in both simple and complex scenes. We compare\nUPainting with recent models and find that UPainting greatly outperforms other\nmodels in terms of caption similarity and image fidelity in both simple and\ncomplex scenes. UPainting project page \\url{https://upainting.github.io/}.", "published": "2022-10-28 10:07:25", "link": "http://arxiv.org/abs/2210.16031v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Are Neural Topic Models Broken?", "abstract": "Recently, the relationship between automated and human evaluation of topic\nmodels has been called into question. Method developers have staked the\nefficacy of new topic model variants on automated measures, and their failure\nto approximate human preferences places these models on uncertain ground.\nMoreover, existing evaluation paradigms are often divorced from real-world use.\n  Motivated by content analysis as a dominant real-world use case for topic\nmodeling, we analyze two related aspects of topic models that affect their\neffectiveness and trustworthiness in practice for that purpose: the stability\nof their estimates and the extent to which the model's discovered categories\nalign with human-determined categories in the data. We find that neural topic\nmodels fare worse in both respects compared to an established classical method.\nWe take a step toward addressing both issues in tandem by demonstrating that a\nstraightforward ensembling method can reliably outperform the members of the\nensemble.", "published": "2022-10-28 14:38:50", "link": "http://arxiv.org/abs/2210.16162v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Feature Engineering vs BERT on Twitter Data", "abstract": "In this paper, we compare the performances of traditional machine learning\nmodels using feature engineering and word vectors and the state-of-the-art\nlanguage model BERT using word embeddings on three datasets. We also consider\nthe time and cost efficiency of feature engineering compared to BERT. From our\nresults we conclude that the use of the BERT model was only worth the time and\ncost trade-off for one of the three datasets we used for comparison, where the\nBERT model significantly outperformed any kind of traditional classifier that\nuses feature vectors, instead of embeddings. Using the BERT model for the other\ndatasets only achieved an increase of 0.03 and 0.05 of accuracy and F1 score\nrespectively, which could be argued makes its use not worth the time and cost\nof GPU.", "published": "2022-10-28 14:43:13", "link": "http://arxiv.org/abs/2210.16168v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigating Ensemble Methods for Model Robustness Improvement of Text\n  Classifiers", "abstract": "Large pre-trained language models have shown remarkable performance over the\npast few years. These models, however, sometimes learn superficial features\nfrom the dataset and cannot generalize to the distributions that are dissimilar\nto the training scenario. There have been several approaches proposed to reduce\nmodel's reliance on these bias features which can improve model robustness in\nthe out-of-distribution setting. However, existing methods usually use a fixed\nlow-capacity model to deal with various bias features, which ignore the\nlearnability of those features. In this paper, we analyze a set of existing\nbias features and demonstrate there is no single model that works best for all\nthe cases. We further show that by choosing an appropriate bias model, we can\nobtain a better robustness result than baselines with a more sophisticated\nmodel design.", "published": "2022-10-28 17:52:10", "link": "http://arxiv.org/abs/2210.16298v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "System Demo: Tool and Infrastructure for Offensive Language Error\n  Analysis (OLEA) in English", "abstract": "The automatic detection of offensive language is a pressing societal need.\nMany systems perform well on explicit offensive language but struggle to detect\nmore complex, nuanced, or implicit cases of offensive and hateful language.\nOLEA is an open-source Python library that provides easy-to-use tools for error\nanalysis in the context of detecting offensive language in English. OLEA also\nprovides an infrastructure for re-distribution of new datasets and analysis\nmethods requiring very little coding.", "published": "2022-10-28 20:38:34", "link": "http://arxiv.org/abs/2210.16398v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DiMBERT: Learning Vision-Language Grounded Representations with\n  Disentangled Multimodal-Attention", "abstract": "Vision-and-language (V-L) tasks require the system to understand both vision\ncontent and natural language, thus learning fine-grained joint representations\nof vision and language (a.k.a. V-L representations) is of paramount importance.\nRecently, various pre-trained V-L models are proposed to learn V-L\nrepresentations and achieve improved results in many tasks. However, the\nmainstream models process both vision and language inputs with the same set of\nattention matrices. As a result, the generated V-L representations are\nentangled in one common latent space. To tackle this problem, we propose\nDiMBERT (short for Disentangled Multimodal-Attention BERT), which is a novel\nframework that applies separated attention spaces for vision and language, and\nthe representations of multi-modalities can thus be disentangled explicitly. To\nenhance the correlation between vision and language in disentangled spaces, we\nintroduce the visual concepts to DiMBERT which represent visual information in\ntextual format. In this manner, visual concepts help to bridge the gap between\nthe two modalities. We pre-train DiMBERT on a large amount of image-sentence\npairs on two tasks: bidirectional language modeling and sequence-to-sequence\nlanguage modeling. After pre-train, DiMBERT is further fine-tuned for the\ndownstream tasks. Experiments show that DiMBERT sets new state-of-the-art\nperformance on three tasks (over four datasets), including both generation\ntasks (image captioning and visual storytelling) and classification tasks\n(referring expressions). The proposed DiM (short for Disentangled\nMultimodal-Attention) module can be easily incorporated into existing\npre-trained V-L models to boost their performance, up to a 5% increase on the\nrepresentative task. Finally, we conduct a systematic analysis and demonstrate\nthe effectiveness of our DiM and the introduced visual concepts.", "published": "2022-10-28 23:00:40", "link": "http://arxiv.org/abs/2210.16431v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Zero-Shot Text Matching for Automated Auditing using Sentence\n  Transformers", "abstract": "Natural language processing methods have several applications in automated\nauditing, including document or passage classification, information retrieval,\nand question answering. However, training such models requires a large amount\nof annotated data which is scarce in industrial settings. At the same time,\ntechniques like zero-shot and unsupervised learning allow for application of\nmodels pre-trained using general domain data to unseen domains.\n  In this work, we study the efficiency of unsupervised text matching using\nSentence-Bert, a transformer-based model, by applying it to the semantic\nsimilarity of financial passages. Experimental results show that this model is\nrobust to documents from in- and out-of-domain data.", "published": "2022-10-28 11:52:16", "link": "http://arxiv.org/abs/2211.07716v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Temporal Modelling of Clinical Depression through Social Media Text", "abstract": "We describe the development of a model to detect user-level clinical\ndepression based on a user's temporal social media posts. Our model uses a\nDepression Symptoms Detection (DSD) classifier, which is trained on the largest\nexisting samples of clinician annotated tweets for clinical depression\nsymptoms. We subsequently use our DSD model to extract clinically relevant\nfeatures, e.g., depression scores and their consequent temporal patterns, as\nwell as user posting activity patterns, e.g., quantifying their ``no activity''\nor ``silence.'' Furthermore, to evaluate the efficacy of these extracted\nfeatures, we create three kinds of datasets including a test dataset, from two\nexisting well-known benchmark datasets for user-level depression detection. We\nthen provide accuracy measures based on single features, baseline features and\nfeature ablation tests, at several different levels of temporal granularity.\nThe relevant data distributions and clinical depression detection related\nsettings can be exploited to draw a complete picture of the impact of different\nfeatures across our created datasets. Finally, we show that, in general, only\nsemantic oriented representation models perform well. However, clinical\nfeatures may enhance overall performance provided that the training and testing\ndistribution is similar, and there is more data in a user's timeline. The\nconsequence is that the predictive capability of depression scores increase\nsignificantly while used in a more sensitive clinical depression detection\nsettings.", "published": "2022-10-28 18:31:52", "link": "http://arxiv.org/abs/2211.07717v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Label Correlations in a Multi-label Setting: A Case Study in\n  Emotion", "abstract": "Detecting emotions expressed in text has become critical to a range of\nfields. In this work, we investigate ways to exploit label correlations in\nmulti-label emotion recognition models to improve emotion detection. First, we\ndevelop two modeling approaches to the problem in order to capture word\nassociations of the emotion words themselves, by either including the emotions\nin the input, or by leveraging Masked Language Modeling (MLM). Second, we\nintegrate pairwise constraints of emotion representations as regularization\nterms alongside the classification loss of the models. We split these terms\ninto two categories, local and global. The former dynamically change based on\nthe gold labels, while the latter remain static during training. We demonstrate\nstate-of-the-art performance across Spanish, English, and Arabic in SemEval\n2018 Task 1 E-c using monolingual BERT-based models. On top of better\nperformance, we also demonstrate improved robustness. Code is available at\nhttps://github.com/gchochla/Demux-MEmo.", "published": "2022-10-28 02:27:18", "link": "http://arxiv.org/abs/2210.15842v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Residual Adapters for Few-Shot Text-to-Speech Speaker Adaptation", "abstract": "Adapting a neural text-to-speech (TTS) model to a target speaker typically\ninvolves fine-tuning most if not all of the parameters of a pretrained\nmulti-speaker backbone model. However, serving hundreds of fine-tuned neural\nTTS models is expensive as each of them requires significant footprint and\nseparate computational resources (e.g., accelerators, memory). To scale speaker\nadapted neural TTS voices to hundreds of speakers while preserving the\nnaturalness and speaker similarity, this paper proposes a parameter-efficient\nfew-shot speaker adaptation, where the backbone model is augmented with\ntrainable lightweight modules called residual adapters. This architecture\nallows the backbone model to be shared across different target speakers.\nExperimental results show that the proposed approach can achieve competitive\nnaturalness and speaker similarity compared to the full fine-tuning approaches,\nwhile requiring only $\\sim$0.1% of the backbone model parameters for each\nspeaker.", "published": "2022-10-28 03:33:07", "link": "http://arxiv.org/abs/2210.15868v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Random Utterance Concatenation Based Data Augmentation for Improving\n  Short-video Speech Recognition", "abstract": "One of limitations in end-to-end automatic speech recognition (ASR) framework\nis its performance would be compromised if train-test utterance lengths are\nmismatched. In this paper, we propose an on-the-fly random utterance\nconcatenation (RUC) based data augmentation method to alleviate train-test\nutterance length mismatch issue for short-video ASR task. Specifically, we are\nmotivated by observations that our human-transcribed training utterances tend\nto be much shorter for short-video spontaneous speech (~3 seconds on average),\nwhile our test utterance generated from voice activity detection front-end is\nmuch longer (~10 seconds on average). Such a mismatch can lead to suboptimal\nperformance. Empirically, it's observed the proposed RUC method significantly\nimproves long utterance recognition without performance drop on short one.\nOverall, it achieves 5.72% word error rate reduction on average for 15\nlanguages and improved robustness to various utterance length.", "published": "2022-10-28 03:54:57", "link": "http://arxiv.org/abs/2210.15876v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Can Current Explainability Help Provide References in Clinical Notes to\n  Support Humans Annotate Medical Codes?", "abstract": "The medical codes prediction problem from clinical notes has received\nsubstantial interest in the NLP community, and several recent studies have\nshown the state-of-the-art (SOTA) code prediction results of full-fledged deep\nlearning-based methods. However, most previous SOTA works based on deep\nlearning are still in early stages in terms of providing textual references and\nexplanations of the predicted codes, despite the fact that this level of\nexplainability of the prediction outcomes is critical to gaining trust from\nprofessional medical coders. This raises the important question of how well\ncurrent explainability methods apply to advanced neural network models such as\ntransformers to predict correct codes and present references in clinical notes\nthat support code prediction. First, we present an explainable Read, Attend,\nand Code (xRAC) framework and assess two approaches, attention score-based\nxRAC-ATTN and model-agnostic knowledge-distillation-based xRAC-KD, through\nsimplified but thorough human-grounded evaluations with SOTA transformer-based\nmodel, RAC. We find that the supporting evidence text highlighted by xRAC-ATTN\nis of higher quality than xRAC-KD whereas xRAC-KD has potential advantages in\nproduction deployment scenarios. More importantly, we show for the first time\nthat, given the current state of explainability methodologies, using the SOTA\nmedical codes prediction system still requires the expertise and competencies\nof professional coders, even though its prediction accuracy is superior to that\nof human coders. This, we believe, is a very meaningful step toward developing\nexplainable and accurate machine learning systems for fully autonomous medical\ncode prediction from clinical notes.", "published": "2022-10-28 04:06:07", "link": "http://arxiv.org/abs/2210.15882v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "On the Use of Modality-Specific Large-Scale Pre-Trained Encoders for\n  Multimodal Sentiment Analysis", "abstract": "This paper investigates the effectiveness and implementation of\nmodality-specific large-scale pre-trained encoders for multimodal sentiment\nanalysis~(MSA). Although the effectiveness of pre-trained encoders in various\nfields has been reported, conventional MSA methods employ them for only\nlinguistic modality, and their application has not been investigated. This\npaper compares the features yielded by large-scale pre-trained encoders with\nconventional heuristic features. One each of the largest pre-trained encoders\npublicly available for each modality are used; CLIP-ViT, WavLM, and BERT for\nvisual, acoustic, and linguistic modalities, respectively. Experiments on two\ndatasets reveal that methods with domain-specific pre-trained encoders attain\nbetter performance than those with conventional features in both unimodal and\nmultimodal scenarios. We also find it better to use the outputs of the\nintermediate layers of the encoders than those of the output layer. The codes\nare available at https://github.com/ando-hub/MSA_Pretrain.", "published": "2022-10-28 06:48:35", "link": "http://arxiv.org/abs/2210.15937v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Assessing Phrase Break of ESL speech with Pre-trained Language Models", "abstract": "This work introduces an approach to assessing phrase break in ESL learners'\nspeech with pre-trained language models (PLMs). Different with traditional\nmethods, this proposal converts speech to token sequences, and then leverages\nthe power of PLMs. There are two sub-tasks: overall assessment of phrase break\nfor a speech clip; fine-grained assessment of every possible phrase break\nposition. Speech input is first force-aligned with texts, then pre-processed to\na token sequence, including words and associated phrase break information. The\ntoken sequence is then fed into the pre-training and fine-tuning pipeline. In\npre-training, a replaced break token detection module is trained with token\ndata where each token has a certain percentage chance to be randomly replaced.\nIn fine-tuning, overall and fine-grained scoring are optimized with text\nclassification and sequence labeling pipeline, respectively. With the\nintroduction of PLMs, the dependence on labeled training data has been greatly\nreduced, and performance has improved.", "published": "2022-10-28 10:06:06", "link": "http://arxiv.org/abs/2210.16029v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Analyzing Acoustic Word Embeddings from Pre-trained Self-supervised\n  Speech Models", "abstract": "Given the strong results of self-supervised models on various tasks, there\nhave been surprisingly few studies exploring self-supervised representations\nfor acoustic word embeddings (AWE), fixed-dimensional vectors representing\nvariable-length spoken word segments. In this work, we study several\npre-trained models and pooling methods for constructing AWEs with\nself-supervised representations. Owing to the contextualized nature of\nself-supervised representations, we hypothesize that simple pooling methods,\nsuch as averaging, might already be useful for constructing AWEs. When\nevaluating on a standard word discrimination task, we find that HuBERT\nrepresentations with mean-pooling rival the state of the art on English AWEs.\nMore surprisingly, despite being trained only on English, HuBERT\nrepresentations evaluated on Xitsonga, Mandarin, and French consistently\noutperform the multilingual model XLSR-53 (as well as Wav2Vec 2.0 trained on\nEnglish).", "published": "2022-10-28 10:26:46", "link": "http://arxiv.org/abs/2210.16043v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards zero-shot Text-based voice editing using acoustic context\n  conditioning, utterance embeddings, and reference encoders", "abstract": "Text-based voice editing (TBVE) uses synthetic output from text-to-speech\n(TTS) systems to replace words in an original recording. Recent work has used\nneural models to produce edited speech that is similar to the original speech\nin terms of clarity, speaker identity, and prosody. However, one limitation of\nprior work is the usage of finetuning to optimise performance: this requires\nfurther model training on data from the target speaker, which is a costly\nprocess that may incorporate potentially sensitive data into server-side\nmodels. In contrast, this work focuses on the zero-shot approach which avoids\nfinetuning altogether, and instead uses pretrained speaker verification\nembeddings together with a jointly trained reference encoder to encode\nutterance-level information that helps capture aspects such as speaker identity\nand prosody. Subjective listening tests find that both utterance embeddings and\na reference encoder improve the continuity of speaker identity and prosody\nbetween the edited synthetic speech and unedited original recording in the\nzero-shot setting.", "published": "2022-10-28 10:31:44", "link": "http://arxiv.org/abs/2210.16045v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neural Network based Formation of Cognitive Maps of Semantic Spaces and\n  the Emergence of Abstract Concepts", "abstract": "The hippocampal-entorhinal complex plays a major role in the organization of\nmemory and thought. The formation of and navigation in cognitive maps of\narbitrary mental spaces via place and grid cells can serve as a representation\nof memories and experiences and their relations to each other. The multi-scale\nsuccessor representation is proposed to be the mathematical principle\nunderlying place and grid cell computations. Here, we present a neural network,\nwhich learns a cognitive map of a semantic space based on 32 different animal\nspecies encoded as feature vectors. The neural network successfully learns the\nsimilarities between different animal species, and constructs a cognitive map\nof 'animal space' based on the principle of successor representations with an\naccuracy of around 30% which is near to the theoretical maximum regarding the\nfact that all animal species have more than one possible successor, i.e.\nnearest neighbor in feature space. Furthermore, a hierarchical structure, i.e.\ndifferent scales of cognitive maps, can be modeled based on multi-scale\nsuccessor representations. We find that, in fine-grained cognitive maps, the\nanimal vectors are evenly distributed in feature space. In contrast, in\ncoarse-grained maps, animal vectors are highly clustered according to their\nbiological class, i.e. amphibians, mammals and insects. This could be a\npossible mechanism explaining the emergence of new abstract semantic concepts.\nFinally, even completely new or incomplete input can be represented by\ninterpolation of the representations from the cognitive map with remarkable\nhigh accuracy of up to 95%. We conclude that the successor representation can\nserve as a weighted pointer to past memories and experiences, and may therefore\nbe a crucial building block for future machine learning to include prior\nknowledge, and to derive context knowledge from novel input.", "published": "2022-10-28 11:16:33", "link": "http://arxiv.org/abs/2210.16062v1", "categories": ["q-bio.NC", "cs.AI", "cs.CL"], "primary_category": "q-bio.NC"}
{"title": "Stop Measuring Calibration When Humans Disagree", "abstract": "Calibration is a popular framework to evaluate whether a classifier knows\nwhen it does not know - i.e., its predictive probabilities are a good\nindication of how likely a prediction is to be correct. Correctness is commonly\nestimated against the human majority class. Recently, calibration to human\nmajority has been measured on tasks where humans inherently disagree about\nwhich class applies. We show that measuring calibration to human majority given\ninherent disagreements is theoretically problematic, demonstrate this\nempirically on the ChaosNLI dataset, and derive several instance-level measures\nof calibration that capture key statistical properties of human judgements -\nclass frequency, ranking and entropy.", "published": "2022-10-28 14:01:32", "link": "http://arxiv.org/abs/2210.16133v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Speech Translation with Dynamic Latent Perceivers", "abstract": "Transformers have been the dominant architecture for Speech Translation in\nrecent years, achieving significant improvements in translation quality. Since\nspeech signals are longer than their textual counterparts, and due to the\nquadratic complexity of the Transformer, a down-sampling step is essential for\nits adoption in Speech Translation. Instead, in this research, we propose to\nease the complexity by using a Perceiver encoder to map the speech inputs to a\nfixed-length latent representation. Furthermore, we introduce a novel way of\ntraining Perceivers, with Dynamic Latent Access (DLA), unlocking larger latent\nspaces without any additional computational overhead. Speech-to-Text Perceivers\nwith DLA can match the performance of Transformer baselines across three\nlanguage pairs in MuST-C. Finally, a DLA-trained model is easily adaptable to\nDLA at inference, and can be flexibly deployed with various computational\nbudgets, without significant drops in translation quality.", "published": "2022-10-28 16:52:48", "link": "http://arxiv.org/abs/2210.16264v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Kuaipedia: a Large-scale Multi-modal Short-video Encyclopedia", "abstract": "Online encyclopedias, such as Wikipedia, have been well-developed and\nresearched in the last two decades. One can find any attributes or other\ninformation of a wiki item on a wiki page edited by a community of volunteers.\nHowever, the traditional text, images and tables can hardly express some\naspects of an wiki item. For example, when we talk about ``Shiba Inu'', one may\ncare more about ``How to feed it'' or ``How to train it not to protect its\nfood''. Currently, short-video platforms have become a hallmark in the online\nworld. Whether you're on TikTok, Instagram, Kuaishou, or YouTube Shorts,\nshort-video apps have changed how we consume and create content today. Except\nfor producing short videos for entertainment, we can find more and more authors\nsharing insightful knowledge widely across all walks of life. These short\nvideos, which we call knowledge videos, can easily express any aspects (e.g.\nhair or how-to-feed) consumers want to know about an item (e.g. Shiba Inu), and\nthey can be systematically analyzed and organized like an online encyclopedia.\nIn this paper, we propose Kuaipedia, a large-scale multi-modal encyclopedia\nconsisting of items, aspects, and short videos lined to them, which was\nextracted from billions of videos of Kuaishou (Kwai), a well-known short-video\nplatform in China. We first collected items from multiple sources and mined\nuser-centered aspects from millions of users' queries to build an item-aspect\ntree. Then we propose a new task called ``multi-modal item-aspect linking'' as\nan expansion of ``entity linking'' to link short videos into item-aspect pairs\nand build the whole short-video encyclopedia. Intrinsic evaluations show that\nour encyclopedia is of large scale and highly accurate. We also conduct\nsufficient extrinsic experiments to show how Kuaipedia can help fundamental\napplications such as entity typing and entity linking.", "published": "2022-10-28 12:54:30", "link": "http://arxiv.org/abs/2211.00732v3", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "UX-NET: Filter-and-Process-based Improved U-Net for Real-time\n  Time-domain Audio Separation", "abstract": "This study presents UX-Net, a time-domain audio separation network (TasNet)\nbased on a modified U-Net architecture. The proposed UX-Net works in real-time\nand handles either single or multi-microphone input. Inspired by the\nfilter-and-process-based human auditory behavior, the proposed system\nintroduces novel mixer and separation modules, which result in cost and memory\nefficient modeling of speech sources. The mixer module combines encoded input\nin a latent feature space and outputs a desired number of output streams. Then,\nin the separation module, a modified U-Net (UX) block is applied. The UX block\nfirst filters the encoded input at various resolutions followed by aggregating\nthe filtered information and applying recurrent processing to estimate masks of\nseparated sources. The letter 'X' in UX-Net is a name placeholder for the type\nof recurrent layer employed in the UX block. Empirical findings on the\nWSJ0-2mix benchmark dataset show that one of the UX-Net configurations\noutperforms the state-of-the-art Conv-TasNet system by 0.85 dB SI-SNR while\nusing only 16% of the model parameters, 58% fewer computations, and maintaining\nlow latency.", "published": "2022-10-28 01:24:21", "link": "http://arxiv.org/abs/2210.15822v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Hierarchical speaker representation for target speaker extraction", "abstract": "Target speaker extraction aims to isolate a specific speaker's voice from a\ncomposite of multiple sound sources, guided by an enrollment utterance or\ncalled anchor. Current methods predominantly derive speaker embeddings from the\nanchor and integrate them into the separation network to separate the voice of\nthe target speaker. However, the representation of the speaker embedding is too\nsimplistic, often being merely a 1*1024 vector. This dense information makes it\ndifficult for the separation network to harness effectively. To address this\nlimitation, we introduce a pioneering methodology called Hierarchical\nRepresentation (HR) that seamlessly fuses anchor data across granular and\noverarching 5 layers of the separation network, enhancing the precision of\ntarget extraction. HR amplifies the efficacy of anchors to improve target\nspeaker isolation. On the Libri-2talker dataset, HR substantially outperforms\nstate-of-the-art time-frequency domain techniques. Further demonstrating HR's\ncapabilities, we achieved first place in the prestigious ICASSP 2023 Deep Noise\nSuppression Challenge. The proposed HR methodology shows great promise for\nadvancing target speaker extraction through enhanced anchor utilization.", "published": "2022-10-28 02:46:47", "link": "http://arxiv.org/abs/2210.15849v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Enhancement with Intelligent Neural Homomorphic Synthesis", "abstract": "Most neural network speech enhancement models ignore speech production\nmathematical models by directly mapping Fourier transform spectrums or\nwaveforms. In this work, we propose a neural source filter network for speech\nenhancement. Specifically, we use homomorphic signal processing and cepstral\nanalysis to obtain noisy speech's excitation and vocal tract. Unlike\ntraditional signal processing, we use an attentive recurrent network (ARN)\nmodel predicted ratio mask to replace the liftering separation function. Then\ntwo convolutional attentive recurrent network (CARN) networks are used to\npredict the excitation and vocal tract of clean speech, respectively. The\nsystem's output is synthesized from the estimated excitation and vocal.\nExperiments prove that our proposed method performs better, with SI-SNR\nimproving by 1.363dB compared to FullSubNet.", "published": "2022-10-28 02:49:48", "link": "http://arxiv.org/abs/2210.15853v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A comprehensive study on self-supervised distillation for speaker\n  representation learning", "abstract": "In real application scenarios, it is often challenging to obtain a large\namount of labeled data for speaker representation learning due to speaker\nprivacy concerns. Self-supervised learning with no labels has become a more and\nmore promising way to solve it. Compared with contrastive learning,\nself-distilled approaches use only positive samples in the loss function and\nthus are more attractive. In this paper, we present a comprehensive study on\nself-distilled self-supervised speaker representation learning, especially on\ncritical data augmentation. Our proposed strategy of audio perturbation\naugmentation has pushed the performance of the speaker representation to a new\nlimit. The experimental results show that our model can achieve a new SoTA on\nVoxceleb1 speaker verification evaluation benchmark ( i.e., equal error rate\n(EER) 2.505%, 2.473%, and 4.791% for trial Vox1-O, Vox1-E and Vox1-H ,\nrespectively), discarding any speaker labels in the training phase.", "published": "2022-10-28 06:48:28", "link": "http://arxiv.org/abs/2210.15936v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Influence of Utterance and Speaker Characteristics on the Classification\n  of Children with Cleft Lip and Palate", "abstract": "Recent findings show that pre-trained wav2vec 2.0 models are reliable feature\nextractors for various speaker characteristics classification tasks. We show\nthat latent representations extracted at different layers of a pre-trained\nwav2vec 2.0 system can be used as features for binary classification to\ndistinguish between children with Cleft Lip and Palate (CLP) and a healthy\ncontrol group. The results indicate that the distinction between CLP and\nhealthy voices, especially with latent representations from the lower and\nmiddle encoder layers, reaches an accuracy of 100%. We test the classifier to\nfind influencing factors for classification using unseen out-of-domain healthy\nand pathologic corpora with varying characteristics: age, spoken content, and\nacoustic conditions. Cross-pathology and cross-healthy tests reveal that the\ntrained classifiers are unreliable if there is a mismatch between training and\nout-of-domain test data in, e.g., age, spoken content, or acoustic conditions.", "published": "2022-10-28 07:02:44", "link": "http://arxiv.org/abs/2210.15941v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring the Effects of Channel Sparsity on Neural Network Pruning for\n  Acoustic Scene Classification", "abstract": "Acoustic Scene Classification (ASC) algorithms are usually expected to be\ndeployed in resource-constrained systems. Existing works reduce the complexity\nof ASC algorithms by pruning some components, e.g. pruning channels in neural\nnetwork. In practice, neural networks are often trained with sparsification\nsuch that unimportant channels can be found and further pruned. However, little\nefforts have been made to explore the the impact of channel sparsity on neural\nnetwork pruning. To fully utilize the benefits of pruning for ASC, and to make\nsure the model performs consistently, we need a more profound comprehension of\nchannel sparsification and its effects. This paper examines the internal\nweights acquired by convolutional neural networks that will undergone pruning.\nThe study discusses how these weights can be utilized to create a novel metric,\nWeight Skewness (WS), for quantifying the sparsity of channels. We also provide\na new approach to compare the performance of different pruning methods, which\nbalances the trade-off between accuracy and complexity. The experiment results\ndemonstrate that 1) applying higher channel sparsity to models can achieve\ngreater compression rates while maintaining acceptable levels of accuracy; 2)\nthe selection of pruning method has little influence on result 1); 3)\nMobileNets exhibit more significant benefits from channel sparsification than\nVGGNets and ResNets.", "published": "2022-10-28 07:41:16", "link": "http://arxiv.org/abs/2210.15960v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-to-end Ensemble-based Feature Selection for Paralinguistics Tasks", "abstract": "The events of recent years have highlighted the importance of telemedicine\nsolutions which could potentially allow remote treatment and diagnosis.\nRelatedly, Computational Paralinguistics, a unique subfield of Speech\nProcessing, aims to extract information about the speaker and form an important\npart of telemedicine applications. In this work, we focus on two paralinguistic\nproblems: mask detection and breathing state prediction. Solutions developed\nfor these tasks could be invaluable and have the potential to help monitor and\nlimit the spread of a virus like COVID-19. The current state-of-the-art methods\nproposed for these tasks are ensembles based on deep neural networks like\nResNets in conjunction with feature engineering. Although these ensembles can\nachieve high accuracy, they also have a large footprint and require substantial\ncomputational power reducing portability to devices with limited resources.\nThese drawbacks also mean that the previously proposed solutions are infeasible\nto be used in a telemedicine system due to their size and speed. On the other\nhand, employing lighter feature-engineered systems can be laborious and add\nfurther complexity making them difficult to create a deployable system quickly.\nThis work proposes an ensemble-based automatic feature selection method to\nenable the development of fast and memory-efficient systems. In particular, we\npropose an output-gradient-based method to discover essential features using\nlarge, well-performing ensembles before training a smaller one. In our\nexperiments, we observed considerable (25-32%) reductions in inference times\nusing neural network ensembles based on output-gradient-based features. Our\nmethod offers a simple way to increase the speed of the system and enable\nreal-time usage while maintaining competitive results with larger-footprint\nensemble using all spectral features.", "published": "2022-10-28 08:18:56", "link": "http://arxiv.org/abs/2210.15978v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dysfluencies Seldom Come Alone -- Detection as a Multi-Label Problem", "abstract": "Specially adapted speech recognition models are necessary to handle stuttered\nspeech. For these to be used in a targeted manner, stuttered speech must be\nreliably detected. Recent works have treated stuttering as a multi-class\nclassification problem or viewed detecting each dysfluency type as an isolated\ntask; that does not capture the nature of stuttering, where one dysfluency\nseldom comes alone, i.e., co-occurs with others. This work explores an approach\nbased on a modified wav2vec 2.0 system for end-to-end stuttering detection and\nclassification as a multi-label problem. The method is evaluated on\ncombinations of three datasets containing English and German stuttered speech,\nyielding state-of-the-art results for stuttering detection on the\nSEP-28k-Extended dataset. Experimental results provide evidence for the\ntransferability of features and the generalizability of the method across\ndatasets and languages.", "published": "2022-10-28 08:22:23", "link": "http://arxiv.org/abs/2210.15982v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SG-VAD: Stochastic Gates Based Speech Activity Detection", "abstract": "We propose a novel voice activity detection (VAD) model in a low-resource\nenvironment. Our key idea is to model VAD as a denoising task, and construct a\nnetwork that is designed to identify nuisance features for a speech\nclassification task. We train the model to simultaneously identify irrelevant\nfeatures while predicting the type of speech event. Our model contains only\n7.8K parameters, outperforms the previously proposed methods on the AVA-Speech\nevaluation set, and provides comparative results on the HAVIC dataset. We\npresent its architecture, experimental results, and ablation study on the\nmodel's components. We publish the code and the models here\nhttps://www.github.com/jsvir/vad.", "published": "2022-10-28 09:50:59", "link": "http://arxiv.org/abs/2210.16022v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Laugh Betrays You? Learning Robust Speaker Representation From Speech\n  Containing Non-Verbal Fragments", "abstract": "The success of automatic speaker verification shows that discriminative\nspeaker representations can be extracted from neutral speech. However, as a\nkind of non-verbal voice, laughter should also carry speaker information\nintuitively. Thus, this paper focuses on exploring speaker verification about\nutterances containing non-verbal laughter segments. We collect a set of clips\nwith laughter components by conducting a laughter detection script on VoxCeleb\nand part of the CN-Celeb dataset. To further filter untrusted clips,\nprobability scores are calculated by our binary laughter detection classifier,\nwhich is pre-trained by pure laughter and neutral speech. After that, based on\nthe clips whose scores are over the threshold, we construct trials under two\ndifferent evaluation scenarios: Laughter-Laughter (LL) and Speech-Laughter\n(SL). Then a novel method called Laughter-Splicing based Network (LSN) is\nproposed, which can significantly boost performance in both scenarios and\nmaintain the performance on the neutral speech, such as the VoxCeleb1 test set.\nSpecifically, our system achieves relative 20% and 22% improvement on\nLaughter-Laughter and Speech-Laughter trials, respectively. The meta-data and\nsample clips have been released at https://github.com/nevermoreLin/Laugh_LSN.", "published": "2022-10-28 10:05:30", "link": "http://arxiv.org/abs/2210.16028v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Target-Speaker Voice Activity Detection via Sequence-to-Sequence\n  Prediction", "abstract": "Target-speaker voice activity detection is currently a promising approach for\nspeaker diarization in complex acoustic environments. This paper presents a\nnovel Sequence-to-Sequence Target-Speaker Voice Activity Detection\n(Seq2Seq-TSVAD) method that can efficiently address the joint modeling of\nlarge-scale speakers and predict high-resolution voice activities. Experimental\nresults show that larger speaker capacity and higher output resolution can\nsignificantly reduce the diarization error rate (DER), which achieves the new\nstate-of-the-art performance of 4.55% on the VoxConverse test set and 10.77% on\nTrack 1 of the DIHARD-III evaluation set under the widely-used evaluation\nmetrics.", "published": "2022-10-28 13:50:04", "link": "http://arxiv.org/abs/2210.16127v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On the Role of Visual Context in Enriching Music Representations", "abstract": "Human perception and experience of music is highly context-dependent.\nContextual variability contributes to differences in how we interpret and\ninteract with music, challenging the design of robust models for information\nretrieval. Incorporating multimodal context from diverse sources provides a\npromising approach toward modeling this variability. Music presented in media\nsuch as movies and music videos provide rich multimodal context that modulates\nunderlying human experiences. However, such context modeling is underexplored,\nas it requires large amounts of multimodal data along with relevant\nannotations. Self-supervised learning can help address these challenges by\nautomatically extracting rich, high-level correspondences between different\nmodalities, hence alleviating the need for fine-grained annotations at scale.\nIn this study, we propose VCMR -- Video-Conditioned Music Representations, a\ncontrastive learning framework that learns music representations from audio and\nthe accompanying music videos. The contextual visual information enhances\nrepresentations of music audio, as evaluated on the downstream task of music\ntagging. Experimental results show that the proposed framework can contribute\nadditive robustness to audio representations and indicates to what extent\nmusical elements are affected or determined by visual context.", "published": "2022-10-28 01:45:07", "link": "http://arxiv.org/abs/2210.15828v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GM-TCNet: Gated Multi-scale Temporal Convolutional Network using Emotion\n  Causality for Speech Emotion Recognition", "abstract": "In human-computer interaction, Speech Emotion Recognition (SER) plays an\nessential role in understanding the user's intent and improving the interactive\nexperience. While similar sentimental speeches own diverse speaker\ncharacteristics but share common antecedents and consequences, an essential\nchallenge for SER is how to produce robust and discriminative representations\nthrough causality between speech emotions. In this paper, we propose a Gated\nMulti-scale Temporal Convolutional Network (GM-TCNet) to construct a novel\nemotional causality representation learning component with a multi-scale\nreceptive field. GM-TCNet deploys a novel emotional causality representation\nlearning component to capture the dynamics of emotion across the time domain,\nconstructed with dilated causal convolution layer and gating mechanism.\nBesides, it utilizes skip connection fusing high-level features from different\ngated convolution blocks to capture abundant and subtle emotion changes in\nhuman speech. GM-TCNet first uses a single type of feature, mel-frequency\ncepstral coefficients, as inputs and then passes them through the gated\ntemporal convolutional module to generate the high-level features. Finally, the\nfeatures are fed to the emotion classifier to accomplish the SER task. The\nexperimental results show that our model maintains the highest performance in\nmost cases compared to state-of-the-art techniques.", "published": "2022-10-28 02:00:40", "link": "http://arxiv.org/abs/2210.15834v1", "categories": ["cs.SD", "cs.AI", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Nonparallel High-Quality Audio Super Resolution with Domain Adaptation\n  and Resampling CycleGANs", "abstract": "Neural audio super-resolution models are typically trained on low- and\nhigh-resolution audio signal pairs. Although these methods achieve highly\naccurate super-resolution if the acoustic characteristics of the input data are\nsimilar to those of the training data, challenges remain: the models suffer\nfrom quality degradation for out-of-domain data, and paired data are required\nfor training. To address these problems, we propose Dual-CycleGAN, a\nhigh-quality audio super-resolution method that can utilize unpaired data based\non two connected cycle consistent generative adversarial networks (CycleGAN).\nOur method decomposes the super-resolution method into domain adaptation and\nresampling processes to handle acoustic mismatch in the unpaired low- and\nhigh-resolution signals. The two processes are then jointly optimized within\nthe CycleGAN framework. Experimental results verify that the proposed method\nsignificantly outperforms conventional methods when paired data are not\navailable. Code and audio samples are available from\nhttps://chomeyama.github.io/DualCycleGAN-Demo/.", "published": "2022-10-28 04:32:59", "link": "http://arxiv.org/abs/2210.15887v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Speaker recognition with two-step multi-modal deep cleansing", "abstract": "Neural network-based speaker recognition has achieved significant improvement\nin recent years. A robust speaker representation learns meaningful knowledge\nfrom both hard and easy samples in the training set to achieve good\nperformance. However, noisy samples (i.e., with wrong labels) in the training\nset induce confusion and cause the network to learn the incorrect\nrepresentation. In this paper, we propose a two-step audio-visual deep\ncleansing framework to eliminate the effect of noisy labels in speaker\nrepresentation learning. This framework contains a coarse-grained cleansing\nstep to search for the peculiar samples, followed by a fine-grained cleansing\nstep to filter out the noisy labels. Our study starts from an efficient\naudio-visual speaker recognition system, which achieves a close to perfect\nequal-error-rate (EER) of 0.01\\%, 0.07\\% and 0.13\\% on the Vox-O, E and H test\nsets. With the proposed multi-modal cleansing mechanism, four different speaker\nrecognition networks achieve an average improvement of 5.9\\%. Code has been\nmade available at:\n\\textcolor{magenta}{\\url{https://github.com/TaoRuijie/AVCleanse}}.", "published": "2022-10-28 05:20:31", "link": "http://arxiv.org/abs/2210.15903v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Period VITS: Variational Inference with Explicit Pitch Modeling for\n  End-to-end Emotional Speech Synthesis", "abstract": "Several fully end-to-end text-to-speech (TTS) models have been proposed that\nhave shown better performance compared to cascade models (i.e., training\nacoustic and vocoder models separately). However, they often generate unstable\npitch contour with audible artifacts when the dataset contains emotional\nattributes, i.e., large diversity of pronunciation and prosody. To address this\nproblem, we propose Period VITS, a novel end-to-end TTS model that incorporates\nan explicit periodicity generator. In the proposed method, we introduce a frame\npitch predictor that predicts prosodic features, such as pitch and voicing\nflags, from the input text. From these features, the proposed periodicity\ngenerator produces a sample-level sinusoidal source that enables the waveform\ndecoder to accurately reproduce the pitch. Finally, the entire model is jointly\noptimized in an end-to-end manner with variational inference and adversarial\nobjectives. As a result, the decoder becomes capable of generating more stable,\nexpressive, and natural output waveforms. The experimental results showed that\nthe proposed model significantly outperforms baseline models in terms of\nnaturalness, with improved pitch stability in the generated samples.", "published": "2022-10-28 07:52:30", "link": "http://arxiv.org/abs/2210.15964v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Lightweight and High-Fidelity End-to-End Text-to-Speech with Multi-Band\n  Generation and Inverse Short-Time Fourier Transform", "abstract": "We propose a lightweight end-to-end text-to-speech model using multi-band\ngeneration and inverse short-time Fourier transform. Our model is based on\nVITS, a high-quality end-to-end text-to-speech model, but adopts two changes\nfor more efficient inference: 1) the most computationally expensive component\nis partially replaced with a simple inverse short-time Fourier transform, and\n2) multi-band generation, with fixed or trainable synthesis filters, is used to\ngenerate waveforms. Unlike conventional lightweight models, which employ\noptimization or knowledge distillation separately to train two cascaded\ncomponents, our method enjoys the full benefits of end-to-end optimization.\nExperimental results show that our model synthesized speech as natural as that\nsynthesized by VITS, while achieving a real-time factor of 0.066 on an Intel\nCore i7 CPU, 4.1 times faster than VITS. Moreover, a smaller version of the\nmodel significantly outperformed a lightweight baseline model with respect to\nboth naturalness and inference speed. Code and audio samples are available from\nhttps://github.com/MasayaKawamura/MB-iSTFT-VITS.", "published": "2022-10-28 08:15:05", "link": "http://arxiv.org/abs/2210.15975v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "NNSVS: A Neural Network-Based Singing Voice Synthesis Toolkit", "abstract": "This paper describes the design of NNSVS, an open-source software for neural\nnetwork-based singing voice synthesis research. NNSVS is inspired by Sinsy, an\nopen-source pioneer in singing voice synthesis research, and provides many\nadditional features such as multi-stream models, autoregressive fundamental\nfrequency models, and neural vocoders. Furthermore, NNSVS provides extensive\ndocumentation and numerous scripts to build complete singing voice synthesis\nsystems. Experimental results demonstrate that our best system significantly\noutperforms our reproduction of Sinsy and other baseline systems. The toolkit\nis available at https://github.com/nnsvs/nnsvs.", "published": "2022-10-28 08:37:13", "link": "http://arxiv.org/abs/2210.15987v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Spectrograms Are Sequences of Patches", "abstract": "Self-supervised pre-training models have been used successfully in several\nmachine learning domains. However, only a tiny amount of work is related to\nmusic. In our work, we treat a spectrogram of music as a series of patches and\ndesign a self-supervised model that captures the features of these sequential\npatches: Patchifier, which makes good use of self-supervised learning methods\nfrom both NLP and CV domains. We do not use labeled data for the pre-training\nprocess, only a subset of the MTAT dataset containing 16k music clips. After\npre-training, we apply the model to several downstream tasks. Our model\nachieves a considerably acceptable result compared to other audio\nrepresentation models. Meanwhile, our work demonstrates that it makes sense to\nconsider audio as a series of patch segments.", "published": "2022-10-28 08:39:36", "link": "http://arxiv.org/abs/2210.15988v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Parameter-efficient transfer learning of pre-trained Transformer models\n  for speaker verification using adapters", "abstract": "Recently, the pre-trained Transformer models have received a rising interest\nin the field of speech processing thanks to their great success in various\ndownstream tasks. However, most fine-tuning approaches update all the\nparameters of the pre-trained model, which becomes prohibitive as the model\nsize grows and sometimes results in overfitting on small datasets. In this\npaper, we conduct a comprehensive analysis of applying parameter-efficient\ntransfer learning (PETL) methods to reduce the required learnable parameters\nfor adapting to speaker verification tasks. Specifically, during the\nfine-tuning process, the pre-trained models are frozen, and only lightweight\nmodules inserted in each Transformer block are trainable (a method known as\nadapters). Moreover, to boost the performance in a cross-language low-resource\nscenario, the Transformer model is further tuned on a large intermediate\ndataset before directly fine-tuning it on a small dataset. With updating fewer\nthan 4% of parameters, (our proposed) PETL-based methods achieve comparable\nperformances with full fine-tuning methods (Vox1-O: 0.55%, Vox1-E: 0.82%,\nVox1-H:1.73%).", "published": "2022-10-28 10:07:42", "link": "http://arxiv.org/abs/2210.16032v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Universal speaker recognition encoders for different speech segments\n  duration", "abstract": "Creating universal speaker encoders which are robust for different acoustic\nand speech duration conditions is a big challenge today. According to our\nobservations systems trained on short speech segments are optimal for short\nphrase speaker verification and systems trained on long segments are superior\nfor long segments verification. A system trained simultaneously on pooled short\nand long speech segments does not give optimal verification results and usually\ndegrades both for short and long segments. This paper addresses the problem of\ncreating universal speaker encoders for different speech segments duration. We\ndescribe our simple recipe for training universal speaker encoder for any type\nof selected neural network architecture. According to our evaluation results of\nwav2vec-TDNN based systems obtained for NIST SRE and VoxCeleb1 benchmarks the\nproposed universal encoder provides speaker verification improvements in case\nof different enrollment and test speech segment duration. The key feature of\nthe proposed encoder is that it has the same inference time as the selected\nneural network architecture.", "published": "2022-10-28 16:06:00", "link": "http://arxiv.org/abs/2210.16231v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Filter and evolve: progressive pseudo label refining for semi-supervised\n  automatic speech recognition", "abstract": "Fine tuning self supervised pretrained models using pseudo labels can\neffectively improve speech recognition performance. But, low quality pseudo\nlabels can misguide decision boundaries and degrade performance. We propose a\nsimple yet effective strategy to filter low quality pseudo labels to alleviate\nthis problem. Specifically, pseudo-labels are produced over the entire training\nset and filtered via average probability scores calculated from the model\noutput. Subsequently, an optimal percentage of utterances with high probability\nscores are considered reliable training data with trustworthy labels. The model\nis iteratively updated to correct the unreliable pseudo labels to minimize the\neffect of noisy labels. The process above is repeated until unreliable pseudo\nabels have been adequately corrected. Extensive experiments on LibriSpeech show\nthat these filtered samples enable the refined model to yield more correct\npredictions, leading to better ASR performances under various experimental\nsettings.", "published": "2022-10-28 16:15:58", "link": "http://arxiv.org/abs/2210.16318v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "HeartSiam: A Domain Invariant Model for Heart Sound Classification", "abstract": "Cardiovascular disease is one of the leading causes of death according to\nWHO. Phonocardiography (PCG) is a costeffective, non-invasive method suitable\nfor heart monitoring. The main aim of this work is to classify heart sounds\ninto normal/abnormal categories. Heart sounds are recorded using different\nstethoscopes, thus varying in the domain. Based on recent studies, this\nvariability can affect heart sound classification. This work presents a Siamese\nnetwork architecture for learning the similarity between normal vs. normal or\nabnormal vs. abnormal signals and the difference between normal vs. abnormal\nsignals. By applying this similarity and difference learning across all\ndomains, the task of domain invariant heart sound classification can be well\nachieved. We have used the multi-domain 2016 Physionet/CinC challenge dataset\nfor the evaluation method. Results: On the evaluation set provided by the\nchallenge, we have achieved a sensitivity of 82.8%, specificity of 75.3%, and\nmean accuracy of 79.1%. While overcoming the multi-domain problem, the proposed\nmethod has surpassed the first-place method of the Physionet challenge in terms\nof specificity up to 10.9% and mean accuracy up to 5.6%. Also, compared with\nsimilar state-of-the-art domain invariant methods, our model converges faster\nand performs better in specificity (4.1%) and mean accuracy (1.5%) with an\nequal number of epochs learned.", "published": "2022-10-28 20:26:42", "link": "http://arxiv.org/abs/2210.16394v2", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention", "abstract": "Audio captioning aims to generate text descriptions of audio clips. In the\nreal world, many objects produce similar sounds. How to accurately recognize\nambiguous sounds is a major challenge for audio captioning. In this work,\ninspired by inherent human multimodal perception, we propose visually-aware\naudio captioning, which makes use of visual information to help the description\nof ambiguous sounding objects. Specifically, we introduce an off-the-shelf\nvisual encoder to extract video features and incorporate the visual features\ninto an audio captioning system. Furthermore, to better exploit complementary\naudio-visual contexts, we propose an audio-visual attention mechanism that\nadaptively integrates audio and visual context and removes the redundant\ninformation in the latent space. Experimental results on AudioCaps, the largest\naudio captioning dataset, show that our proposed method achieves\nstate-of-the-art results on machine translation metrics.", "published": "2022-10-28 22:45:41", "link": "http://arxiv.org/abs/2210.16428v3", "categories": ["eess.AS", "cs.AI", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Introducing topography in convolutional neural networks", "abstract": "Parts of the brain that carry sensory tasks are organized topographically:\nnearby neurons are responsive to the same properties of input signals. Thus, in\nthis work, inspired by the neuroscience literature, we proposed a new\ntopographic inductive bias in Convolutional Neural Networks (CNNs). To achieve\nthis, we introduced a new topographic loss and an efficient implementation to\ntopographically organize each convolutional layer of any CNN. We benchmarked\nour new method on 4 datasets and 3 models in vision and audio tasks and showed\nequivalent performance to all benchmarks. Besides, we also showcased the\ngeneralizability of our topographic loss with how it can be used with different\ntopographic organizations in CNNs. Finally, we demonstrated that adding the\ntopographic inductive bias made CNNs more resistant to pruning. Our approach\nprovides a new avenue to obtain models that are more memory efficient while\nmaintaining better accuracy.", "published": "2022-10-28 13:20:31", "link": "http://arxiv.org/abs/2211.13152v1", "categories": ["cs.NE", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.NE"}
