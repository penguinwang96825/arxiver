{"title": "Many Languages, One Parser", "abstract": "We train one multilingual model for dependency parsing and use it to parse\nsentences in several languages. The parsing model uses (i) multilingual word\nclusters and embeddings; (ii) token-level language information; and (iii)\nlanguage-specific features (fine-grained POS tags). This input representation\nenables the parser not only to parse effectively in multiple languages, but\nalso to generalize across languages based on linguistic universals and\ntypological similarities, making it more effective to learn from limited\nannotations. Our parser's performance compares favorably to strong baselines in\na range of data scenarios, including when the target language has a large\ntreebank, a small treebank, or no treebank for training.", "published": "2016-02-04 08:51:18", "link": "http://arxiv.org/abs/1602.01595v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Factorized Recurrent Neural Network based architecture for medium to\n  large vocabulary Language Modelling", "abstract": "Statistical language models are central to many applications that use\nsemantics. Recurrent Neural Networks (RNN) are known to produce state of the\nart results for language modelling, outperforming their traditional n-gram\ncounterparts in many cases. To generate a probability distribution across a\nvocabulary, these models require a softmax output layer that linearly increases\nin size with the size of the vocabulary. Large vocabularies need a\ncommensurately large softmax layer and training them on typical laptops/PCs\nrequires significant time and machine resources. In this paper we present a new\ntechnique for implementing RNN based large vocabulary language models that\nsubstantially speeds up computation while optimally using the limited memory\nresources. Our technique, while building on the notion of factorizing the\noutput layer by having multiple output layers, improves on the earlier work by\nsubstantially optimizing on the individual output layer size and also\neliminating the need for a multistep prediction process.", "published": "2016-02-04 07:53:11", "link": "http://arxiv.org/abs/1602.01576v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Complex Networks of Words in Fables", "abstract": "In this chapter we give an overview of the application of complex network\ntheory to quantify some properties of language. Our study is based on two\nfables in Ukrainian, Mykyta the Fox and Abu-Kasym's slippers. It consists of\ntwo parts: the analysis of frequency-rank distributions of words and the\napplication of complex-network theory. The first part shows that the text sizes\nare sufficiently large to observe statistical properties. This supports their\nselection for the analysis of typical properties of the language networks in\nthe second part of the chapter. In describing language as a complex network,\nwhile words are usually associated with nodes, there is more variability in the\nchoice of links and different representations result in different networks.\nHere, we examine a number of such representations of the language network and\nperform a comparative analysis of their characteristics. Our results suggest\nthat, irrespective of link representation, the Ukrainian language network used\nin the selected fables is a strongly correlated, scale-free, small world. We\ndiscuss how such empirical approaches may help form a useful basis for a\ntheoretical description of language evolution and how they may be used in\nanalyses of other textual narratives.", "published": "2016-02-04 14:32:57", "link": "http://arxiv.org/abs/1602.04853v1", "categories": ["physics.soc-ph", "cs.CL"], "primary_category": "physics.soc-ph"}
{"title": "A Generalised Quantifier Theory of Natural Language in Categorical\n  Compositional Distributional Semantics with Bialgebras", "abstract": "Categorical compositional distributional semantics is a model of natural\nlanguage; it combines the statistical vector space models of words with the\ncompositional models of grammar. We formalise in this model the generalised\nquantifier theory of natural language, due to Barwise and Cooper. The\nunderlying setting is a compact closed category with bialgebras. We start from\na generative grammar formalisation and develop an abstract categorical\ncompositional semantics for it, then instantiate the abstract setting to sets\nand relations and to finite dimensional vector spaces and linear maps. We prove\nthe equivalence of the relational instantiation to the truth theoretic\nsemantics of generalised quantifiers. The vector space instantiation formalises\nthe statistical usages of words and enables us to, for the first time, reason\nabout quantified phrases and sentences compositionally in distributional\nsemantics.", "published": "2016-02-04 11:15:28", "link": "http://arxiv.org/abs/1602.01635v2", "categories": ["cs.CL", "cs.AI", "math.CT", "I.2.7"], "primary_category": "cs.CL"}
