{"title": "A Decision Support System for Stock Selection and Asset Allocation Based on Fundamental Data Analysis", "abstract": "Financial markets are integral to a country's economic success, yet their\ncomplex nature raises challenging issues for predicting their behaviors. There\nis a growing demand for an integrated system that explores the vast and diverse\ndata in financial reports with powerful machine-learning models to analyze\nfinancial markets and suggest appropriate investment strategies. This research\nprovides an end-to-end decision support system (DSS) that pervasively covers\nthe stages of gathering, cleaning, and modeling the stock's financial and\nfundamental data alongside the country's macroeconomic conditions. Analyzing\nand modeling the fundamental data of securities is a noteworthy method that,\ndespite its greater power, has been used by fewer researchers due to its more\ncomplex and challenging issues. By precisely analyzing securities' fundamental\ndata, the proposed system assists investors in predicting stock future prices\nand allocating assets in major financial markets: stock, bond, and commodity.\nThe most notable contributions and innovations of this research are: (1)\nDeveloping a robust predictive model for mid- to long-term stock returns,\ntailored for investors rather than traders, (2) The proposed DSS considers a\ndiverse set of features relating to the economic conditions of the company,\nincluding fundamental data, stock trading characteristics, and macro-economic\nattributes to enhance predictive accuracy, (3) Evaluating the DSS performance\non the Tehran Stock Exchange that has specific characteristics of small to\nmedium-sized economies with high inflation rates and showing the superiority to\nnovel researches, and (4) Empowering the DSS to generate different asset\nallocation strategies in various economic situations by simulating expert\ninvestor decision-making.", "published": "2024-11-24 17:27:30", "link": "http://arxiv.org/abs/2412.05297v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "Quantile deep learning models for multi-step ahead time series prediction", "abstract": "Uncertainty quantification is crucial in time series prediction, and quantile\nregression offers a valuable mechanism for uncertainty quantification which is\nuseful for extreme value forecasting. Although deep learning models have been\nprominent in multi-step ahead prediction, the development and evaluation of\nquantile deep learning models have been limited. We present a novel quantile\nregression deep learning framework for multi-step time series prediction. In\nthis way, we elevate the capabilities of deep learning models by incorporating\nquantile regression, thus providing a more nuanced understanding of predictive\nvalues. We provide an implementation of prominent deep learning models for\nmulti-step ahead time series prediction and evaluate their performance under\nhigh volatility and extreme conditions. We include multivariate and univariate\nmodelling, strategies and provide a comparison with conventional deep learning\nmodels from the literature. Our models are tested on two cryptocurrencies:\nBitcoin and Ethereum, using daily close-price data and selected benchmark time\nseries datasets. The results show that integrating a quantile loss function\nwith deep learning provides additional predictions for selected quantiles\nwithout a loss in the prediction accuracy when compared to the literature. Our\nquantile model has the ability to handle volatility more effectively and\nprovides additional information for decision-making and uncertainty\nquantification through the use of quantiles when compared to conventional deep\nlearning models.", "published": "2024-11-24 00:00:10", "link": "http://arxiv.org/abs/2411.15674v1", "categories": ["cs.LG", "cs.AI", "q-fin.ST", "stat.ME"], "primary_category": "cs.LG"}
{"title": "Deep Sparse Latent Feature Models for Knowledge Graph Completion", "abstract": "Recent progress in knowledge graph completion (KGC) has focused on text-based\napproaches to address the challenges of large-scale knowledge graphs (KGs).\nDespite their achievements, these methods often overlook the intricate\ninterconnections between entities, a key aspect of the underlying topological\nstructure of a KG. Stochastic blockmodels (SBMs), particularly the latent\nfeature relational model (LFRM), offer robust probabilistic frameworks that can\ndynamically capture latent community structures and enhance link prediction. In\nthis paper, we introduce a novel framework of sparse latent feature models for\nKGC, optimized through a deep variational autoencoder (VAE). Our approach not\nonly effectively completes missing triples but also provides clear\ninterpretability of the latent structures, leveraging textual information.\nComprehensive experiments on the WN18RR, FB15k-237, and Wikidata5M datasets\nshow that our method significantly improves performance by revealing latent\ncommunities and producing interpretable representations.", "published": "2024-11-24 03:17:37", "link": "http://arxiv.org/abs/2411.15694v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLaMA-MoE v2: Exploring Sparsity of LLaMA from Perspective of\n  Mixture-of-Experts with Post-Training", "abstract": "Recently, inspired by the concept of sparsity, Mixture-of-Experts (MoE)\nmodels have gained increasing popularity for scaling model size while keeping\nthe number of activated parameters constant. In this study, we thoroughly\ninvestigate the sparsity of the dense LLaMA model by constructing MoE for both\nthe attention (i.e., Attention MoE) and MLP (i.e., MLP MoE) modules in the\ntransformer blocks. Specifically, we investigate different expert construction\nmethods and granularities under the same activation conditions to analyze the\nimpact of sparsifying the model. Additionally, to comprehensively evaluate the\nmodel's capabilities across various domains (e.g., conversation, code, math)\nafter sparsification, we apply sparsity to the instructed large language models\n(LLMs) and construct instructed MoE models. To counteract the performance\ndegradation resulting from increased sparsity, we design a two-stage\npost-training strategy to enhance model performance. Experiments on the LLaMA3\nmodel demonstrate the potential effectiveness of this approach for future\ndevelopments of instructed MoE models. The source codes and models are\navailable at: \\url{https://github.com/OpenSparseLLMs/LLaMA-MoE-v2}.", "published": "2024-11-24 04:26:04", "link": "http://arxiv.org/abs/2411.15708v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Turkish Synonyms Used in Different Time Periods", "abstract": "Dynamic structure of languages poses significant challenges in applying\nnatural language processing models on historical texts, causing decreased\nperformance in various downstream tasks. Turkish is a prominent example of\nrapid linguistic transformation due to the language reform in the 20th century.\nIn this paper, we propose two methods for detecting synonyms used in different\ntime periods, focusing on Turkish. In our first method, we use Orthogonal\nProcrustes method to align the embedding spaces created using documents written\nin the corresponding time periods. In our second method, we extend the first\none by incorporating Spearman's correlation between frequencies of words\nthroughout the years. In our experiments, we show that our proposed methods\noutperform the baseline method. Furthermore, we observe that the efficacy of\nour methods remains consistent when the target time period shifts from the\n1960s to the 1980s. However, their performance slightly decreases for\nsubsequent time periods.", "published": "2024-11-24 09:31:38", "link": "http://arxiv.org/abs/2411.15768v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity", "abstract": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models.", "published": "2024-11-24 11:30:00", "link": "http://arxiv.org/abs/2411.15785v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Large Language Models for Causal Modeling", "abstract": "In this paper, we consider the process of transforming causal domain\nknowledge into a representation that aligns more closely with guidelines from\ncausal data science. To this end, we introduce two novel tasks related to\ndistilling causal domain knowledge into causal variables and detecting\ninteraction entities using LLMs. We have determined that contemporary LLMs are\nhelpful tools for conducting causal modeling tasks in collaboration with human\nexperts, as they can provide a wider perspective. Specifically, LLMs, such as\nGPT-4-turbo and Llama3-70b, perform better in distilling causal domain\nknowledge into causal variables compared to sparse expert models, such as\nMixtral-8x22b. On the contrary, sparse expert models such as Mixtral-8x22b\nstand out as the most effective in identifying interaction entities. Finally,\nwe highlight the dependency between the domain where the entities are generated\nand the performance of the chosen LLM for causal modeling.", "published": "2024-11-24 15:51:56", "link": "http://arxiv.org/abs/2411.15888v1", "categories": ["cs.CL", "I.2.0", "I.2.0"], "primary_category": "cs.CL"}
{"title": "Investigating Factuality in Long-Form Text Generation: The Roles of\n  Self-Known and Self-Unknown", "abstract": "Large language models (LLMs) have demonstrated strong capabilities in text\nunderstanding and generation. However, they often lack factuality, producing a\nmixture of true and false information, especially in long-form generation. In\nthis work, we investigates the factuality of long-form text generation across\nvarious large language models (LLMs), including GPT-4, Gemini-1.5-Pro,\nClaude-3-Opus, Llama-3-70B, and Mistral. Our analysis reveals that factuality\nscores tend to decline in later sentences of the generated text, accompanied by\na rise in the number of unsupported claims. Furthermore, we explore the\neffectiveness of different evaluation settings to assess whether LLMs can\naccurately judge the correctness of their own outputs: Self-Known (the\npercentage of supported atomic claims, decomposed from LLM outputs, that the\ncorresponding LLMs judge as correct) and Self-Unknown (the percentage of\nunsupported atomic claims that the corresponding LLMs judge as incorrect). The\nresults indicate that even advanced models like GPT-4 and Gemini-1.5-Pro fail\nto achieve perfect Self-Known scores, while their Self-Unknown scores remain\nnotably above zero, reflecting ongoing uncertainty in their self-assessments.\nMoreover, we find a correlation between higher Self-Known scores and improved\nfactuality, while higher Self-Unknown scores are associated with lower\nfactuality. Interestingly, even without significant changes in the models'\nself-judgment (Self-Known and Self-Unknown), the number of unsupported claims\ncan increases, likely as an artifact of long-form generation. These findings\nshow the limitations of current LLMs in long-form generation, and provide\nvaluable insights for improving factuality in long-form text generation.", "published": "2024-11-24 22:06:26", "link": "http://arxiv.org/abs/2411.15993v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-ToM: Evaluating Multilingual Theory of Mind Capabilities in Large\n  Language Models", "abstract": "Theory of Mind (ToM) refers to the cognitive ability to infer and attribute\nmental states to oneself and others. As large language models (LLMs) are\nincreasingly evaluated for social and cognitive capabilities, it remains\nunclear to what extent these models demonstrate ToM across diverse languages\nand cultural contexts. In this paper, we introduce a comprehensive study of\nmultilingual ToM capabilities aimed at addressing this gap. Our approach\nincludes two key components: (1) We translate existing ToM datasets into\nmultiple languages, effectively creating a multilingual ToM dataset and (2) We\nenrich these translations with culturally specific elements to reflect the\nsocial and cognitive scenarios relevant to diverse populations. We conduct\nextensive evaluations of six state-of-the-art LLMs to measure their ToM\nperformance across both the translated and culturally adapted datasets. The\nresults highlight the influence of linguistic and cultural diversity on the\nmodels' ability to exhibit ToM, and questions their social reasoning\ncapabilities. This work lays the groundwork for future research into enhancing\nLLMs' cross-cultural social cognition and contributes to the development of\nmore culturally aware and socially intelligent AI systems. All our data and\ncode are publicly available.", "published": "2024-11-24 22:37:59", "link": "http://arxiv.org/abs/2411.15999v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Performance Contrasts in TableQA: Step-by-Step Reasoning\n  Boosts Bigger Language Models, Limits Smaller Language Models", "abstract": "This paper proposes a detailed prompting flow, termed Table-Logic, to\ninvestigate the performance contrasts between bigger and smaller language\nmodels (LMs) utilizing step-by-step reasoning methods in the TableQA task. The\nmethod processes tasks by sequentially identifying critical columns and rows\ngiven question and table with its structure, determining necessary\naggregations, calculations, or comparisons, and finally inferring the results\nto generate a precise prediction. By deploying this method, we observe a 7.8%\naccuracy improvement in bigger LMs like Llama-3-70B compared to the vanilla on\nHybridQA, while smaller LMs like Llama-2-7B shows an 11% performance decline.\nWe empirically investigate the potential causes of performance contrasts by\nexploring the capabilities of bigger and smaller LMs from various dimensions in\nTableQA task. Our findings highlight the limitations of the step-by-step\nreasoning method in small models and provide potential insights for making\nimprovements.", "published": "2024-11-24 22:48:44", "link": "http://arxiv.org/abs/2411.16002v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Development of Pre-Trained Transformer-based Models for the Nepali\n  Language", "abstract": "Transformer-based pre-trained language models have dominated the field of\nNatural Language Processing (NLP) for quite some time now. However, the Nepali\nlanguage, spoken by approximately 32 million people worldwide, remains\nsignificantly underrepresented in this domain. This underrepresentation is\nprimarily attributed to the scarcity of monolingual data corpora and limited\navailable resources for the Nepali language. While existing efforts have\npredominantly concentrated on basic encoder-based models, there is a notable\ngap in the exploration of decoder-based architectures. To address this gap, we\nhave collected 27.5 GB of Nepali text data, approximately 2.4x larger than any\npreviously available Nepali language corpus. Leveraging this data, we\npre-trained three different models i.e., BERT, RoBERTa, and GPT-2, exclusively\nfor the Nepali Language. Furthermore, we performed instruction tuning and\nexplored its potential for monolingual Nepali data, providing a foundation for\nfuture research. Our models outperformed the existing best model by 2 points on\nNep-gLUE benchmark, scoring 95.60 and also outperformed existing models on text\ngeneration tasks, demonstrating improvements in both understanding and\ngenerating Nepali text.", "published": "2024-11-24 06:38:24", "link": "http://arxiv.org/abs/2411.15734v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Do LLMs Really Think Step-by-step In Implicit Reasoning?", "abstract": "It has been well-known that Chain-of-Thought can remarkably enhance LLMs'\nperformance on complex tasks. However, because it also introduces slower\ninference speeds and higher computational costs, many researches have attempted\nto use implicit CoT, which does not need LLMs to explicitly generate the\nintermediate steps. However, the invisible reasoning process leaves us a doubt\nthat, can implicit CoT really be equal to explicit CoT? Therefore, in this\nstudy, we address this question through experiments. We probe the information\nof intermediate steps from the model's hidden states when it is either trained\nor prompted to perform implicit CoT. The results surprisingly indicate that\nwhen prompted, LLMs hardly think about intermediate steps, suggesting they may\njust rely on experience rather than strict step-by-step reasoning. But when\ntrained, they indeed calculate intermediate steps. Moreover, in both\nsituations, we find the effect of using implicit CoT is susceptible to the\nformat of the problem, reaffirming the current deficiency of implicit CoT.", "published": "2024-11-24 14:38:59", "link": "http://arxiv.org/abs/2411.15862v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generative Prompt Internalization", "abstract": "Prompts used in recent large language model based applications are often\nfixed and lengthy, leading to significant computational overhead. To address\nthis challenge, we propose Generative Prompt Internalization (GenPI), a\nlightweight method that employs a joint training approach. GenPI not only\nreplicates the behavior of models with prompt inputs but also generates the\ncontent of the prompt along with reasons for why the model's behavior should\nchange accordingly. We demonstrate that our approach effectively internalizes\ncomplex prompts across various agent-based application scenarios. For effective\ntraining without interactions with the dedicated environments, we introduce a\ndata synthesis technique that autonomously collects conversational datasets by\nswapping the roles of the agent and environment. This method is especially\nuseful in scenarios where only a predefined prompt is available without a\ncorresponding training dataset. By internalizing complex prompts, Generative\nPrompt Internalization enables high performance and efficient inference without\nthe need for explicit prompts.", "published": "2024-11-24 17:32:20", "link": "http://arxiv.org/abs/2411.15927v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RAMIE: Retrieval-Augmented Multi-task Information Extraction with Large\n  Language Models on Dietary Supplements", "abstract": "\\textbf{Objective:} We aimed to develop an advanced multi-task large language\nmodel (LLM) framework to extract multiple types of information about dietary\nsupplements (DS) from clinical records.\n  \\textbf{Methods:} We used four core DS information extraction tasks - namely,\nnamed entity recognition (NER: 2,949 clinical sentences), relation extraction\n(RE: 4,892 sentences), triple extraction (TE: 2,949 sentences), and usage\nclassification (UC: 2,460 sentences) as our multitasks. We introduced a novel\nRetrieval-Augmented Multi-task Information Extraction (RAMIE) Framework,\nincluding: 1) employed instruction fine-tuning techniques with task-specific\nprompts, 2) trained LLMs for multiple tasks with improved storage efficiency\nand lower training costs, and 3) incorporated retrieval augmentation generation\n(RAG) techniques by retrieving similar examples from the training set. We\ncompared RAMIE's performance to LLMs with instruction fine-tuning alone and\nconducted an ablation study to assess the contributions of multi-task learning\nand RAG to improved multitasking performance.\n  \\textbf{Results:} With the aid of the RAMIE framework, Llama2-13B achieved an\nF1 score of 87.39 (3.51\\% improvement) on the NER task and demonstrated\noutstanding performance on the RE task with an F1 score of 93.74 (1.15\\%\nimprovement). For the TE task, Llama2-7B scored 79.45 (14.26\\% improvement),\nand MedAlpaca-7B achieved the highest F1 score of 93.45 (0.94\\% improvement) on\nthe UC task. The ablation study revealed that while MTL increased efficiency\nwith a slight trade-off in performance, RAG significantly boosted overall\naccuracy.\n  \\textbf{Conclusion:} This study presents a novel RAMIE framework that\ndemonstrates substantial improvements in multi-task information extraction for\nDS-related data from clinical records. Our framework can potentially be applied\nto other domains.", "published": "2024-11-24 03:56:43", "link": "http://arxiv.org/abs/2411.15700v1", "categories": ["cs.CL", "cs.AI", "cs.CE"], "primary_category": "cs.CL"}
{"title": "TableTime: Reformulating Time Series Classification as Training-Free\n  Table Understanding with Large Language Models", "abstract": "Large language models (LLMs) have demonstrated their effectiveness in\nmultivariate time series classification (MTSC). Effective adaptation of LLMs\nfor MTSC necessitates informative data representations. Existing LLM-based\nmethods directly encode embeddings for time series within the latent space of\nLLMs from scratch to align with semantic space of LLMs. Despite their\neffectiveness, we reveal that these methods conceal three inherent bottlenecks:\n(1) they struggle to encode temporal and channel-specific information in a\nlossless manner, both of which are critical components of multivariate time\nseries; (2) it is much difficult to align the learned representation space with\nthe semantic space of the LLMs; (3) they require task-specific retraining,\nwhich is both computationally expensive and labor-intensive. To bridge these\ngaps, we propose TableTime, which reformulates MTSC as a table understanding\ntask. Specifically, TableTime introduces the following strategies: (1) convert\nmultivariate time series into a tabular form, thus minimizing information loss\nto the greatest extent; (2) represent tabular time series in text format to\nachieve natural alignment with the semantic space of LLMs; (3) design a\nreasoning framework that integrates contextual text information, neighborhood\nassistance, multi-path inference and problem decomposition to enhance the\nreasoning ability of LLMs and realize zero-shot classification. Extensive\nexperiments performed on 10 publicly representative datasets from UEA archive\nverify the superiorities of the TableTime.", "published": "2024-11-24 07:02:32", "link": "http://arxiv.org/abs/2411.15737v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "LoRA-Mini : Adaptation Matrices Decomposition and Selective Training", "abstract": "The rapid advancements in large language models (LLMs) have revolutionized\nnatural language processing, creating an increased need for efficient,\ntask-specific fine-tuning methods. Traditional fine-tuning of LLMs involves\nupdating a large number of parameters, which is computationally expensive and\nmemory-intensive. Low-Rank Adaptation (LoRA) has emerged as a promising\nsolution, enabling parameter-efficient fine-tuning by reducing the number of\ntrainable parameters. However, while LoRA reduces the number of trainable\nparameters, LoRA modules still create significant storage challenges. We\npropose LoRA-Mini, an optimized adaptation of LoRA that improves parameter\nefficiency by splitting low-rank matrices into four parts, with only the two\ninner matrices being trainable. This approach achieves upto a 20x reduction\ncompared to standard LoRA in the number of trainable parameters while\npreserving performance levels comparable to standard LoRA, addressing both\ncomputational and storage efficiency in LLM fine-tuning.", "published": "2024-11-24 12:21:14", "link": "http://arxiv.org/abs/2411.15804v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is Training Data Quality or Quantity More Impactful to Small Language\n  Model Performance?", "abstract": "This study investigates the relative impact of training data quality versus\nquantity on the performance of small language models (SLMs), utilizing the\nTinyStories dataset for empirical analysis. Analysis of dataset variations with\nrespect to size (25% and 50% of the original size) and duplication (controlled\nrates of 25%, 50%, 75%, and 100%) were performed. Model performance was\nevaluated based on the validation loss, accuracy, and perplexity metrics.\nResults indicate training data quality plays a more significant role in the\noverall performance of SLMs, especially given scale of this experiment. Minimal\nduplication positively impacted model accuracy (+0.87% increase in accuracy at\n25% duplication) without significantly increasing perplexity (+0.52% increase\ngoing from 0% to 25% duplication) but excessive duplication led to pronounced\nperformance degradation (-40% drop in accuracy at 100% duplication). The\nimplications of this exploration extend beyond just model performance; training\nlarge-scale models imposes significant financial and computational burdens,\nwhich can be prohibitive for organizations, individuals, and the public at\nlarge, especially in developing countries. Additionally, the energy consumption\nassociated with large-scale training raises environmental concerns.\nUnderstanding the relative importance of data quality versus quantity could\ndemocratize AI technology, making advanced models more accessible and\nsustainable for all.", "published": "2024-11-24 12:51:50", "link": "http://arxiv.org/abs/2411.15821v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PriorDiffusion: Leverage Language Prior in Diffusion Models for\n  Monocular Depth Estimation", "abstract": "This paper explores the potential of leveraging language priors learned by\ntext-to-image diffusion models to address ambiguity and visual nuisance in\nmonocular depth estimation. Particularly, traditional monocular depth\nestimation suffers from inherent ambiguity due to the absence of stereo or\nmulti-view depth cues, and nuisance due to lack of robustness of vision. We\nargue that language prior in diffusion models can enhance monocular depth\nestimation by leveraging the geometric prior aligned with the language\ndescription, which is learned during text-to-image pre-training. To generate\nimages that reflect the text properly, the model must comprehend the size and\nshape of specified objects, their spatial relationship, and the scale of the\nscene. Thus, we propose PriorDiffusion, using a pre-trained text-to-image\ndiffusion model that takes both image and text description that aligned with\nthe scene to infer affine-invariant depth through a denoising process. We also\nshow that language priors can guide the model's attention to specific regions\nand help it perceive the 3D scene in alignment with user intent.\nSimultaneously, it acts as a constraint to accelerate the convergence of the\ndiffusion trajectory, since learning 3D properties from a condensed,\nlow-dimensional language feature is more efficient compared with learning from\na redundant, high-dimensional image feature. By training on HyperSim and\nVirtual KITTI, we achieve state-of-the-art zero-shot performance and a faster\nconvergence speed, compared with other diffusion-based depth estimators, across\nNYUv2, KITTI, ETH3D, and ScanNet.", "published": "2024-11-24 05:07:10", "link": "http://arxiv.org/abs/2411.16750v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "LeMoLE: LLM-Enhanced Mixture of Linear Experts for Time Series\n  Forecasting", "abstract": "Recent research has shown that large language models (LLMs) can be\neffectively used for real-world time series forecasting due to their strong\nnatural language understanding capabilities. However, aligning time series into\nsemantic spaces of LLMs comes with high computational costs and inference\ncomplexity, particularly for long-range time series generation. Building on\nrecent advancements in using linear models for time series, this paper\nintroduces an LLM-enhanced mixture of linear experts for precise and efficient\ntime series forecasting. This approach involves developing a mixture of linear\nexperts with multiple lookback lengths and a new multimodal fusion mechanism.\nThe use of a mixture of linear experts is efficient due to its simplicity,\nwhile the multimodal fusion mechanism adaptively combines multiple linear\nexperts based on the learned features of the text modality from pre-trained\nlarge language models. In experiments, we rethink the need to align time series\nto LLMs by existing time-series large language models and further discuss their\nefficiency and effectiveness in time series forecasting. Our experimental\nresults show that the proposed LeMoLE model presents lower prediction errors\nand higher computational efficiency than existing LLM models.", "published": "2024-11-24 12:40:50", "link": "http://arxiv.org/abs/2412.00053v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "High-precision medical speech recognition through synthetic data and\n  semantic correction: UNITED-MEDASR", "abstract": "Automatic Speech Recognition (ASR) systems in the clinical domain face\nsignificant challenges, notably the need to recognise specialised medical\nvocabulary accurately and meet stringent precision requirements. We introduce\nUnited-MedASR, a novel architecture that addresses these challenges by\nintegrating synthetic data generation, precision ASR fine-tuning, and advanced\nsemantic enhancement techniques. United-MedASR constructs a specialised medical\nvocabulary by synthesising data from authoritative sources such as ICD-10\n(International Classification of Diseases, 10th Revision), MIMS (Monthly Index\nof Medical Specialties), and FDA databases. This enriched vocabulary helps\nfinetune the Whisper ASR model to better cater to clinical needs. To enhance\nprocessing speed, we incorporate Faster Whisper, ensuring streamlined and\nhigh-speed ASR performance. Additionally, we employ a customised BART-based\nsemantic enhancer to handle intricate medical terminology, thereby increasing\naccuracy efficiently. Our layered approach establishes new benchmarks in ASR\nperformance, achieving a Word Error Rate (WER) of 0.985% on LibriSpeech\ntest-clean, 0.26% on Europarl-ASR EN Guest-test, and demonstrating robust\nperformance on Tedlium (0.29% WER) and FLEURS (0.336% WER). Furthermore, we\npresent an adaptable architecture that can be replicated across different\ndomains, making it a versatile solution for domain-specific ASR systems.", "published": "2024-11-24 17:02:48", "link": "http://arxiv.org/abs/2412.00055v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Kleene algebra with commutativity conditions is undecidable", "abstract": "We prove that the equational theory of Kleene algebra with commutativity\n  conditions on primitives (or atomic terms) is undecidable, thereby settling a\n  longstanding open question in the theory of Kleene algebra. While this\n  question has also been recently solved independently by Kuznetsov, our\nresults\n  hold even for weaker theories that do not support the induction axioms\n  of Kleene algebra.", "published": "2024-11-24 20:44:27", "link": "http://arxiv.org/abs/2411.15979v1", "categories": ["math.LO", "cs.CC", "cs.CL", "cs.LO", "cs.PL"], "primary_category": "math.LO"}
{"title": "State-Space Large Audio Language Models", "abstract": "Large Audio Language Models (LALM) combine the audio perception models and\nthe Large Language Models (LLM) and show a remarkable ability to reason about\nthe input audio, infer the meaning, and understand the intent. However, these\nsystems rely on Transformers which scale quadratically with the input sequence\nlengths which poses computational challenges in deploying these systems in\nmemory and time-constrained scenarios. Recently, the state-space models (SSMs)\nhave emerged as an alternative to transformer networks.\n  While there have been successful attempts to replace transformer-based audio\nperception models with state-space ones, state-space-based LALMs remain\nunexplored. First, we begin by replacing the transformer-based audio perception\nmodule and then replace the transformer-based LLM and propose the first\nstate-space-based LALM. Experimental results demonstrate that space-based LALM\ndespite having a significantly lower number of parameters performs\ncompetitively with transformer-based LALMs on close-ended tasks on a variety of\ndatasets.", "published": "2024-11-24 02:21:28", "link": "http://arxiv.org/abs/2411.15685v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "A Training-Free Approach for Music Style Transfer with Latent Diffusion\n  Models", "abstract": "Music style transfer, while offering exciting possibilities for personalized\nmusic generation, often requires extensive training or detailed textual\ndescriptions. This paper introduces a novel training-free approach leveraging\npre-trained Latent Diffusion Models (LDMs). By manipulating the self-attention\nfeatures of the LDM, we effectively transfer the style of reference music onto\ncontent music without additional training. Our method achieves superior style\ntransfer and melody preservation compared to existing methods. This work opens\nnew creative avenues for personalized music generation.", "published": "2024-11-24 16:53:34", "link": "http://arxiv.org/abs/2411.15913v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Revisiting Your Memory: Reconstruction of Affect-Contextualized Memory\n  via EEG-guided Audiovisual Generation", "abstract": "In this paper, we introduce RecallAffectiveMemory, a novel task designed to\nreconstruct autobiographical memories through audio-visual generation guided by\naffect extracted from electroencephalogram (EEG) signals. To support this\npioneering task, we present the EEG-AffectiveMemory dataset, which encompasses\ntextual descriptions, visuals, music, and EEG recordings collected during\nmemory recall from nine participants. Furthermore, we propose RYM (Recall Your\nMemory), a three-stage framework for generating synchronized audio-visual\ncontents while maintaining dynamic personal memory affect trajectories.\nExperimental results indicate that our method can faithfully reconstruct\naffect-contextualized audio-visual memory across all subjects, both\nqualitatively and quantitatively, with participants reporting strong affective\nconcordance between their recalled memories and the generated content. Our\napproaches advance affect decoding research and its practical applications in\npersonalized media creation via neural-based affect comprehension.", "published": "2024-11-24 16:04:03", "link": "http://arxiv.org/abs/2412.05296v1", "categories": ["cs.AI", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "A Survey of Recent Advances and Challenges in Deep Audio-Visual\n  Correlation Learning", "abstract": "Audio-visual correlation learning aims to capture and understand natural\nphenomena between audio and visual data. The rapid growth of Deep Learning\npropelled the development of proposals that process audio-visual data and can\nbe observed in the number of proposals in the past years. Thus encouraging the\ndevelopment of a comprehensive survey. Besides analyzing the models used in\nthis context, we also discuss some tasks of definition and paradigm applied in\nAI multimedia. In addition, we investigate objective functions frequently used\nand discuss how audio-visual data is exploited in the optimization process,\ni.e., the different methodologies for representing knowledge in the\naudio-visual domain. In fact, we focus on how human-understandable mechanisms,\ni.e., structured knowledge that reflects comprehensible knowledge, can guide\nthe learning process. Most importantly, we provide a summarization of the\nrecent progress of Audio-Visual Correlation Learning (AVCL) and discuss the\nfuture research directions.", "published": "2024-11-24 03:26:34", "link": "http://arxiv.org/abs/2412.00049v1", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
