{"title": "Statistical Uncertainty in Word Embeddings: GloVe-V", "abstract": "Static word embeddings are ubiquitous in computational social science\napplications and contribute to practical decision-making in a variety of fields\nincluding law and healthcare. However, assessing the statistical uncertainty in\ndownstream conclusions drawn from word embedding statistics has remained\nchallenging. When using only point estimates for embeddings, researchers have\nno streamlined way of assessing the degree to which their model selection\ncriteria or scientific conclusions are subject to noise due to sparsity in the\nunderlying data used to generate the embeddings. We introduce a method to\nobtain approximate, easy-to-use, and scalable reconstruction error variance\nestimates for GloVe (Pennington et al., 2014), one of the most widely used word\nembedding models, using an analytical approximation to a multivariate normal\nmodel. To demonstrate the value of embeddings with variance (GloVe-V), we\nillustrate how our approach enables principled hypothesis testing in core word\nembedding tasks, such as comparing the similarity between different word pairs\nin vector space, assessing the performance of different models, and analyzing\nthe relative degree of ethnic or gender bias in a corpus using different word\nlists.", "published": "2024-06-18 00:35:02", "link": "http://arxiv.org/abs/2406.12165v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On-Policy Self-Alignment with Fine-grained Knowledge Feedback for\n  Hallucination Mitigation", "abstract": "Hallucination occurs when large language models exhibit behavior that\ndeviates from the boundaries of their knowledge during response generation. To\naddress this critical issue, previous learning-based methods attempt to\nfinetune models but are limited by off-policy sampling and coarse-grained\nfeedback. In this paper, we present \\textit{\\b{R}einforcement \\b{L}earning\n\\b{f}or \\b{H}allucination} (RLFH), an on-policy self-alignment approach that\nenables LLMs to actively explore their knowledge boundaries and self-correct\ngeneration behavior through fine-grained feedback signals. RLFH introduces a\nself-assessment framework where the policy serves as its own judge. Through\nthis framework, responses are automatically decomposed into atomic facts and\ntheir truthfulness and informativeness are assessed against external knowledge\nsources. The resulting fine-grained feedback at the statement level are then\nconverted into token-level dense reward signals. This enables online\nreinforcement learning to achieve precise and timely optimization without human\nintervention. Comprehensive evaluations on HotpotQA, SQuADv2, and Biography\nbenchmarks validate RLFH's effectiveness in hallucination mitigation.", "published": "2024-06-18 02:43:49", "link": "http://arxiv.org/abs/2406.12221v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PFID: Privacy First Inference Delegation Framework for LLMs", "abstract": "This paper introduces a novel privacy-preservation framework named PFID for\nLLMs that addresses critical privacy concerns by localizing user data through\nmodel sharding and singular value decomposition. When users are interacting\nwith LLM systems, their prompts could be subject to being exposed to\neavesdroppers within or outside LLM system providers who are interested in\ncollecting users' input. In this work, we proposed a framework to camouflage\nuser input, so as to alleviate privacy issues. Our framework proposes to place\nmodel shards on the client and the public server, we sent compressed hidden\nstates instead of prompts to and from servers. Clients have held back\ninformation that can re-privatized the hidden states so that overall system\nperformance is comparable to traditional LLMs services. Our framework was\ndesigned to be communication efficient, computation can be delegated to the\nlocal client so that the server's computation burden can be lightened. We\nconduct extensive experiments on machine translation tasks to verify our\nframework's performance.", "published": "2024-06-18 03:27:09", "link": "http://arxiv.org/abs/2406.12238v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language and Multimodal Models in Sports: A Survey of Datasets and\n  Applications", "abstract": "Recent integration of Natural Language Processing (NLP) and multimodal models\nhas advanced the field of sports analytics. This survey presents a\ncomprehensive review of the datasets and applications driving these innovations\npost-2020. We overviewed and categorized datasets into three primary types:\nlanguage-based, multimodal, and convertible datasets. Language-based and\nmultimodal datasets are for tasks involving text or multimodality (e.g., text,\nvideo, audio), respectively. Convertible datasets, initially single-modal\n(video), can be enriched with additional annotations, such as explanations of\nactions and video descriptions, to become multimodal, offering future potential\nfor richer and more diverse applications. Our study highlights the\ncontributions of these datasets to various applications, from improving fan\nexperiences to supporting tactical analysis and medical diagnostics. We also\ndiscuss the challenges and future directions in dataset development,\nemphasizing the need for diverse, high-quality data to support real-time\nprocessing and personalized user experiences. This survey provides a\nfoundational resource for researchers and practitioners aiming to leverage NLP\nand multimodal models in sports, offering insights into current trends and\nfuture opportunities in the field.", "published": "2024-06-18 03:59:26", "link": "http://arxiv.org/abs/2406.12252v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Defending Against Social Engineering Attacks in the Age of LLMs", "abstract": "The proliferation of Large Language Models (LLMs) poses challenges in\ndetecting and mitigating digital deception, as these models can emulate human\nconversational patterns and facilitate chat-based social engineering (CSE)\nattacks. This study investigates the dual capabilities of LLMs as both\nfacilitators and defenders against CSE threats. We develop a novel dataset,\nSEConvo, simulating CSE scenarios in academic and recruitment contexts, and\ndesigned to examine how LLMs can be exploited in these situations. Our findings\nreveal that, while off-the-shelf LLMs generate high-quality CSE content, their\ndetection capabilities are suboptimal, leading to increased operational costs\nfor defense. In response, we propose ConvoSentinel, a modular defense pipeline\nthat improves detection at both the message and the conversation levels,\noffering enhanced adaptability and cost-effectiveness. The retrieval-augmented\nmodule in ConvoSentinel identifies malicious intent by comparing messages to a\ndatabase of similar conversations, enhancing CSE detection at all stages. Our\nstudy highlights the need for advanced strategies to leverage LLMs in\ncybersecurity.", "published": "2024-06-18 04:39:40", "link": "http://arxiv.org/abs/2406.12263v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a Client-Centered Assessment of LLM Therapists by Client\n  Simulation", "abstract": "Although there is a growing belief that LLMs can be used as therapists,\nexploring LLMs' capabilities and inefficacy, particularly from the client's\nperspective, is limited. This work focuses on a client-centered assessment of\nLLM therapists with the involvement of simulated clients, a standard approach\nin clinical medical education. However, there are two challenges when applying\nthe approach to assess LLM therapists at scale. Ethically, asking humans to\nfrequently mimic clients and exposing them to potentially harmful LLM outputs\ncan be risky and unsafe. Technically, it can be difficult to consistently\ncompare the performances of different LLM therapists interacting with the same\nclient. To this end, we adopt LLMs to simulate clients and propose ClientCAST,\na client-centered approach to assessing LLM therapists by client simulation.\nSpecifically, the simulated client is utilized to interact with LLM therapists\nand complete questionnaires related to the interaction. Based on the\nquestionnaire results, we assess LLM therapists from three client-centered\naspects: session outcome, therapeutic alliance, and self-reported feelings. We\nconduct experiments to examine the reliability of ClientCAST and use it to\nevaluate LLMs therapists implemented by Claude-3, GPT-3.5, LLaMA3-70B, and\nMixtral 8*7B. Codes are released at https://github.com/wangjs9/ClientCAST.", "published": "2024-06-18 04:46:55", "link": "http://arxiv.org/abs/2406.12266v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling Implicit Table Knowledge with Question-Then-Pinpoint Reasoner\n  for Insightful Table Summarization", "abstract": "Implicit knowledge hidden within the explicit table cells, such as data\ninsights, is the key to generating a high-quality table summary. However,\nunveiling such implicit knowledge is a non-trivial task. Due to the complex\nnature of structured tables, it is challenging even for large language models\n(LLMs) to mine the implicit knowledge in an insightful and faithful manner. To\naddress this challenge, we propose a novel table reasoning framework\nQuestion-then-Pinpoint. Our work focuses on building a plug-and-play table\nreasoner that can self-question the insightful knowledge and answer it by\nfaithfully pinpointing evidence on the table to provide explainable guidance\nfor the summarizer. To train a reliable reasoner, we collect table knowledge by\nguiding a teacher LLM to follow the coarse-to-fine reasoning paths and refine\nit through two quality enhancement strategies to selectively distill the\nhigh-quality knowledge to the reasoner. Extensive experiments on two table\nsummarization datasets, including our newly proposed InsTaSumm, validate the\ngeneral effectiveness of our framework.", "published": "2024-06-18 04:55:09", "link": "http://arxiv.org/abs/2406.12269v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SafeInfer: Context Adaptive Decoding Time Safety Alignment for Large\n  Language Models", "abstract": "Safety-aligned language models often exhibit fragile and imbalanced safety\nmechanisms, increasing the likelihood of generating unsafe content. In\naddition, incorporating new knowledge through editing techniques to language\nmodels can further compromise safety. To address these issues, we propose\nSafeInfer, a context-adaptive, decoding-time safety alignment strategy for\ngenerating safe responses to user queries. SafeInfer comprises two phases: the\nsafety amplification phase, which employs safe demonstration examples to adjust\nthe model's hidden states and increase the likelihood of safer outputs, and the\nsafety-guided decoding phase, which influences token selection based on\nsafety-optimized distributions, ensuring the generated content complies with\nethical guidelines. Further, we present HarmEval, a novel benchmark for\nextensive safety evaluations, designed to address potential misuse scenarios in\naccordance with the policies of leading AI tech giants.", "published": "2024-06-18 05:03:23", "link": "http://arxiv.org/abs/2406.12274v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Matters in Memorizing and Recalling Facts? Multifaceted Benchmarks\n  for Knowledge Probing in Language Models", "abstract": "Language models often struggle with handling factual knowledge, exhibiting\nfactual hallucination issue. This makes it vital to evaluate the models'\nability to recall its parametric knowledge about facts. In this study, we\nintroduce a knowledge probing benchmark, BELIEF(ICL), to evaluate the knowledge\nrecall ability of both encoder- and decoder-based pre-trained language models\n(PLMs) from diverse perspectives. BELIEFs utilize a multi-prompt dataset to\nevaluate PLM's accuracy, consistency, and reliability in factual knowledge\nrecall. To enable a more reliable evaluation with BELIEFs, we\nsemi-automatically create MyriadLAMA, which has massively diverse prompts. We\nvalidate the effectiveness of BELIEFs in comprehensively evaluating PLM's\nknowledge recall ability on diverse PLMs, including recent large language\nmodels (LLMs). We then investigate key factors in memorizing and recalling\nfacts in PLMs, such as model size, pretraining strategy and corpora,\ninstruction-tuning process and in-context learning settings. Finally, we reveal\nthe limitation of the prompt-based knowledge probing. The MyriadLAMA is\npublicized.", "published": "2024-06-18 05:11:35", "link": "http://arxiv.org/abs/2406.12277v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast and Slow Generating: An Empirical Study on Large and Small Language\n  Models Collaborative Decoding", "abstract": "Large Language Models (LLMs) exhibit impressive capabilities across various\napplications but encounter substantial challenges such as high inference\nlatency, considerable training costs, and the generation of hallucinations.\nCollaborative decoding between large and small language models (SLMs) presents\na promising strategy to mitigate these issues through methods including\nspeculative decoding, contrastive decoding, and emulator or proxy fine-tuning.\nHowever, the specifics of such collaborations, particularly from a unified\nperspective, remain largely unexplored. Inspired by dual-process cognitive\ntheory, we propose a unified framework in this paper, termed Fast and Slow\nGenerating (FS-GEN). Within this framework, LLMs (sometimes along with SLMs)\nare categorized as System 2 (slow and deliberate), while independent SLMs are\ndesignated as System 1 (fast and intuitive). We provide a comprehensive\nanalysis of these collaborative methodologies, elucidating their common\nproperties and shedding light on the differential knowledge capabilities of\nSystem 2 versus System 1 through the FS-GEN framework. Our findings indicate\nthat only a small proportion of collaborative interactions (approximately less\nthan 20\\% in most instances) are necessary across various methods. These\ninteractions between System 1 and System 2 conform to a scaling law related to\nthe parameter ratios, enabling predictable collaboration. Furthermore, we\nexplore the specific conditions under which collaboration proves most\neffective, particularly from an uncertainty perspective, offering novel\ninsights that may guide future optimization efforts. Our research underscores\nthat the fundamental distinction between System 1 and System 2 lies in the\nuncertainty of next token predictions, where interventions by System 2 are\ncrucial to support System 1. Code for Reproduction:\nhttps://github.com/TsinghuaC3I/FS-GEN", "published": "2024-06-18 05:59:28", "link": "http://arxiv.org/abs/2406.12295v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COT: A Generative Approach for Hate Speech Counter-Narratives via\n  Contrastive Optimal Transport", "abstract": "Counter-narratives, which are direct responses consisting of non-aggressive\nfact-based arguments, have emerged as a highly effective approach to combat the\nproliferation of hate speech. Previous methodologies have primarily focused on\nfine-tuning and post-editing techniques to ensure the fluency of generated\ncontents, while overlooking the critical aspects of individualization and\nrelevance concerning the specific hatred targets, such as LGBT groups,\nimmigrants, etc. This research paper introduces a novel framework based on\ncontrastive optimal transport, which effectively addresses the challenges of\nmaintaining target interaction and promoting diversification in generating\ncounter-narratives. Firstly, an Optimal Transport Kernel (OTK) module is\nleveraged to incorporate hatred target information in the token\nrepresentations, in which the comparison pairs are extracted between original\nand transported features. Secondly, a self-contrastive learning module is\nemployed to address the issue of model degeneration. This module achieves this\nby generating an anisotropic distribution of token representations. Finally, a\ntarget-oriented search method is integrated as an improved decoding strategy to\nexplicitly promote domain relevance and diversification in the inference\nprocess. This strategy modifies the model's confidence score by considering\nboth token similarity and target relevance. Quantitative and qualitative\nexperiments have been evaluated on two benchmark datasets, which demonstrate\nthat our proposed model significantly outperforms current methods evaluated by\nmetrics from multiple aspects.", "published": "2024-06-18 06:24:26", "link": "http://arxiv.org/abs/2406.12304v1", "categories": ["cs.CL", "68U15", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Can Tool-augmented Large Language Models be Aware of Incomplete\n  Conditions?", "abstract": "Recent advancements in integrating large language models (LLMs) with tools\nhave allowed the models to interact with real-world environments. However,\nthese tool-augmented LLMs often encounter incomplete scenarios when users\nprovide partial information or the necessary tools are unavailable. Recognizing\nand managing such scenarios is crucial for LLMs to ensure their reliability,\nbut this exploration remains understudied. This study examines whether LLMs can\nidentify incomplete conditions and appropriately determine when to refrain from\nusing tools. To this end, we address a dataset by manipulating instances from\ntwo datasets by removing necessary tools or essential information for tool\ninvocation. Our experiments show that LLMs often struggle to identify the\nabsence of information required to utilize specific tools and recognize the\nabsence of appropriate tools. We further analyze model behaviors in different\nenvironments and compare their performance against humans. Our research can\ncontribute to advancing reliable LLMs by addressing common scenarios during\ninteractions between humans and LLMs. Our code and dataset will be publicly\navailable.", "published": "2024-06-18 06:28:06", "link": "http://arxiv.org/abs/2406.12307v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences\n  of LLM Evaluators", "abstract": "As large language models (LLMs) are increasingly used as evaluators for\nnatural language generation tasks, ensuring unbiased assessments is essential.\nHowever, LLM evaluators often display biased preferences, such as favoring\nverbosity and authoritative tones. Our empirical analysis reveals that these\nbiases are exacerbated in pairwise evaluation, where LLMs directly compare two\noutputs and easily prioritize superficial attributes. In contrast, pointwise\nevaluation, which assesses outputs independently, is less susceptible to such\nbias because each output is judged in isolation. To address the limitations of\nthe pairwise evaluation, we introduce a novel evaluation method, PRePair, which\nintegrates pointwise reasoning within a pairwise framework. PRePair effectively\nalleviates biased preference, improving performance on the adversarial\nbenchmark (LLMBar) while outperforming pointwise evaluation on the standard\nbenchmark (MT-Bench).", "published": "2024-06-18 06:43:04", "link": "http://arxiv.org/abs/2406.12319v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Opt-Out: Investigating Entity-Level Unlearning for Large Language Models\n  via Optimal Transport", "abstract": "Instruction-following large language models (LLMs), such as ChatGPT, have\nbecome widely popular among everyday users. However, these models inadvertently\ndisclose private, sensitive information to their users, underscoring the need\nfor machine unlearning techniques to remove selective information from the\nmodels. While prior work has focused on forgetting small, random subsets of\ntraining data at the instance-level, we argue that real-world scenarios often\nrequire the removal of an entire user data, which may require a more careful\nmaneuver. In this study, we explore entity-level unlearning, which aims to\nerase all knowledge related to a target entity while preserving the remaining\nmodel capabilities. To address this, we introduce Opt-Out, an optimal\ntransport-based unlearning method that utilizes the Wasserstein distance from\nthe model's initial parameters to achieve more effective and fine-grained\nunlearning. We also present the first Entity-Level Unlearning Dataset (ELUDe)\ndesigned to evaluate entity-level unlearning. Our empirical results demonstrate\nthat Opt-Out surpasses existing methods, establishing a new standard for secure\nand adaptable LLMs that can accommodate user data removal requests without the\nneed for full retraining.", "published": "2024-06-18 06:54:05", "link": "http://arxiv.org/abs/2406.12329v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpreting Bias in Large Language Models: A Feature-Based Approach", "abstract": "Large Language Models (LLMs) such as Mistral and LLaMA have showcased\nremarkable performance across various natural language processing (NLP) tasks.\nDespite their success, these models inherit social biases from the diverse\ndatasets on which they are trained. This paper investigates the propagation of\nbiases within LLMs through a novel feature-based analytical approach. Drawing\ninspiration from causal mediation analysis, we hypothesize the evolution of\nbias-related features and validate them using interpretability techniques like\nactivation and attribution patching. Our contributions are threefold: (1) We\nintroduce and empirically validate a feature-based method for bias analysis in\nLLMs, applied to LLaMA-2-7B, LLaMA-3-8B, and Mistral-7B-v0.3 with templates\nfrom a professions dataset. (2) We extend our method to another form of gender\nbias, demonstrating its generalizability. (3) We differentiate the roles of\nMLPs and attention heads in bias propagation and implement targeted debiasing\nusing a counterfactual dataset. Our findings reveal the complex nature of bias\nin LLMs and emphasize the necessity for tailored debiasing strategies, offering\na deeper understanding of bias mechanisms and pathways for effective\nmitigation.", "published": "2024-06-18 07:28:15", "link": "http://arxiv.org/abs/2406.12347v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language\n  Models", "abstract": "Pretrained language models memorize vast amounts of information, including\nprivate and copyrighted data, raising significant safety concerns. Retraining\nthese models after excluding sensitive data is prohibitively expensive, making\nmachine unlearning a viable, cost-effective alternative. Previous research has\nfocused on machine unlearning for monolingual models, but we find that\nunlearning in one language does not necessarily transfer to others. This\nvulnerability makes models susceptible to low-resource language attacks, where\nsensitive information remains accessible in less dominant languages. This paper\npresents a pioneering approach to machine unlearning for multilingual language\nmodels, selectively erasing information across different languages while\nmaintaining overall performance. Specifically, our method employs an adaptive\nunlearning scheme that assigns language-dependent weights to address different\nlanguage performances of multilingual language models. Empirical results\ndemonstrate the effectiveness of our framework compared to existing unlearning\nbaselines, setting a new standard for secure and adaptable multilingual\nlanguage models.", "published": "2024-06-18 07:40:18", "link": "http://arxiv.org/abs/2406.12354v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does Context Help Mitigate Gender Bias in Neural Machine Translation?", "abstract": "Neural Machine Translation models tend to perpetuate gender bias present in\ntheir training data distribution. Context-aware models have been previously\nsuggested as a means to mitigate this type of bias. In this work, we examine\nthis claim by analysing in detail the translation of stereotypical professions\nin English to German, and translation with non-informative context in Basque to\nSpanish. Our results show that, although context-aware models can significantly\nenhance translation accuracy for feminine terms, they can still maintain or\neven amplify gender bias. These results highlight the need for more\nfine-grained approaches to bias mitigation in Neural Machine Translation.", "published": "2024-06-18 07:44:13", "link": "http://arxiv.org/abs/2406.12364v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Instance Training to Instruction Learning: Task Adapters Generation\n  from Instructions", "abstract": "Large language models (LLMs) have acquired the ability to solve general tasks\nby utilizing instruction finetuning (IFT). However, IFT still relies heavily on\ninstance training of extensive task data, which greatly limits the adaptability\nof LLMs to real-world scenarios where labeled task instances are scarce and\nbroader task generalization becomes paramount. Contrary to LLMs, humans acquire\nskills and complete tasks not merely through repeated practice but also by\nunderstanding and following instructional guidelines. This paper is dedicated\nto simulating human learning to address the shortcomings of instance training,\nfocusing on instruction learning to enhance cross-task generalization. Within\nthis context, we introduce Task Adapters Generation from Instructions (TAGI),\nwhich automatically constructs the task-specific model in a parameter\ngeneration manner based on the given task instructions without retraining for\nunseen tasks. Specifically, we utilize knowledge distillation to enhance the\nconsistency between TAGI developed through Learning with Instruction and\ntask-specific models developed through Training with Instance, by aligning the\nlabels, output logits, and adapter parameters between them. TAGI is endowed\nwith cross-task generalization capabilities through a two-stage training\nprocess that includes hypernetwork pretraining and finetuning. We evaluate TAGI\non the Super-Natural Instructions and P3 datasets. The experimental results\ndemonstrate that TAGI can match or even outperform traditional meta-trained\nmodels and other hypernetwork models, while significantly reducing\ncomputational requirements.", "published": "2024-06-18 08:14:28", "link": "http://arxiv.org/abs/2406.12382v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IPEval: A Bilingual Intellectual Property Agency Consultation Evaluation\n  Benchmark for Large Language Models", "abstract": "The rapid development of Large Language Models (LLMs) in vertical domains,\nincluding intellectual property (IP), lacks a specific evaluation benchmark for\nassessing their understanding, application, and reasoning abilities. To fill\nthis gap, we introduce IPEval, the first evaluation benchmark tailored for IP\nagency and consulting tasks. IPEval comprises 2657 multiple-choice questions\nacross four major dimensions: creation, application, protection, and management\nof IP. These questions span patent rights (inventions, utility models,\ndesigns), trademarks, copyrights, trade secrets, and other related laws.\nEvaluation methods include zero-shot, 5-few-shot, and Chain of Thought (CoT)\nfor seven LLM types, predominantly in English or Chinese. Results show superior\nEnglish performance by models like GPT series and Qwen series, while\nChinese-centric LLMs excel in Chinese tests, albeit specialized IP LLMs lag\nbehind general-purpose ones. Regional and temporal aspects of IP underscore the\nneed for LLMs to grasp legal nuances and evolving laws. IPEval aims to\naccurately gauge LLM capabilities in IP and spur development of specialized\nmodels. Website: \\url{https://ipeval.github.io/}", "published": "2024-06-18 08:18:18", "link": "http://arxiv.org/abs/2406.12386v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EMO-KNOW: A Large Scale Dataset on Emotion and Emotion-cause", "abstract": "Emotion-Cause analysis has attracted the attention of researchers in recent\nyears. However, most existing datasets are limited in size and number of\nemotion categories. They often focus on extracting parts of the document that\ncontain the emotion cause and fail to provide more abstractive, generalizable\nroot cause. To bridge this gap, we introduce a large-scale dataset of emotion\ncauses, derived from 9.8 million cleaned tweets over 15 years. We describe our\ncuration process, which includes a comprehensive pipeline for data gathering,\ncleaning, labeling, and validation, ensuring the dataset's reliability and\nrichness. We extract emotion labels and provide abstractive summarization of\nthe events causing emotions. The final dataset comprises over 700,000 tweets\nwith corresponding emotion-cause pairs spanning 48 emotion classes, validated\nby human evaluators. The novelty of our dataset stems from its broad spectrum\nof emotion classes and the abstractive emotion cause that facilitates the\ndevelopment of an emotion-cause knowledge graph for nuanced reasoning. Our\ndataset will enable the design of emotion-aware systems that account for the\ndiverse emotional responses of different people for the same event.", "published": "2024-06-18 08:26:33", "link": "http://arxiv.org/abs/2406.12389v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Unveiling the Flaws: Exploring Imperfections in Synthetic Data and\n  Mitigation Strategies for Large Language Models", "abstract": "Synthetic data has been proposed as a solution to address the issue of\nhigh-quality data scarcity in the training of large language models (LLMs).\nStudies have shown that synthetic data can effectively improve the performance\nof LLMs on downstream benchmarks. However, despite its potential benefits, our\nanalysis suggests that there may be inherent flaws in synthetic data. The\nuniform format of synthetic data can lead to pattern overfitting and cause\nsignificant shifts in the output distribution, thereby reducing the model's\ninstruction-following capabilities. Our work delves into these specific flaws\nassociated with question-answer (Q-A) pairs, a prevalent type of synthetic\ndata, and presents a method based on unlearning techniques to mitigate these\nflaws. The empirical results demonstrate the effectiveness of our approach,\nwhich can reverse the instruction-following issues caused by pattern\noverfitting without compromising performance on benchmarks at relatively low\ncost. Our work has yielded key insights into the effective use of synthetic\ndata, aiming to promote more robust and efficient LLM training.", "published": "2024-06-18 08:38:59", "link": "http://arxiv.org/abs/2406.12397v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Flee the Flaw: Annotating the Underlying Logic of Fallacious Arguments\n  Through Templates and Slot-filling", "abstract": "Prior research in computational argumentation has mainly focused on scoring\nthe quality of arguments, with less attention on explicating logical errors. In\nthis work, we introduce four sets of explainable templates for common informal\nlogical fallacies designed to explicate a fallacy's implicit logic. Using our\ntemplates, we conduct an annotation study on top of 400 fallacious arguments\ntaken from LOGIC dataset and achieve a high agreement score (Krippendorf's\nalpha of 0.54) and reasonable coverage (0.83). Finally, we conduct an\nexperiment for detecting the structure of fallacies and discover that\nstate-of-the-art language models struggle with detecting fallacy templates\n(0.47 accuracy). To facilitate research on fallacies, we make our dataset and\nguidelines publicly available.", "published": "2024-06-18 08:44:45", "link": "http://arxiv.org/abs/2406.12402v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AI-Assisted Human Evaluation of Machine Translation", "abstract": "Annually, research teams spend large amounts of money to evaluate the quality\nof machine translation systems (WMT, inter alia). This is expensive because it\nrequires a lot of expert human labor. In the recently adopted annotation\nprotocol, Error Span Annotation (ESA), annotators mark erroneous parts of the\ntranslation and then assign a final score. A lot of the annotator time is spent\non scanning the translation for possible errors. In our work, we help the\nannotators by pre-filling the error annotations with recall-oriented automatic\nquality estimation. With this AI assistance, we obtain annotations at the same\nquality level while cutting down the time per span annotation by half\n(71s/error span $\\rightarrow$ 31s/error span). The biggest advantage of the\nESA$^\\mathrm{AI}$ protocol is an accurate priming of annotators (pre-filled\nerror spans) before they assign the final score. This alleviates a potential\nautomation bias, which we confirm to be low. In our experiments, we find that\nthe annotation budget can be further reduced by almost 25% with filtering of\nexamples that the AI deems to be likely to be correct.", "published": "2024-06-18 09:12:11", "link": "http://arxiv.org/abs/2406.12419v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open-Source Web Service with Morphological Dictionary-Supplemented Deep\n  Learning for Morphosyntactic Analysis of Czech", "abstract": "We present an open-source web service for Czech morphosyntactic analysis. The\nsystem combines a deep learning model with rescoring by a high-precision\nmorphological dictionary at inference time. We show that our hybrid method\nsurpasses two competitive baselines: While the deep learning model ensures\ngeneralization for out-of-vocabulary words and better disambiguation, an\nimprovement over an existing morphological analyser MorphoDiTa, at the same\ntime, the deep learning model benefits from inference-time guidance of a\nmanually curated morphological dictionary. We achieve 50% error reduction in\nlemmatization and 58% error reduction in POS tagging over MorphoDiTa, while\nalso offering dependency parsing. The model is trained on one of the currently\nlargest Czech morphosyntactic corpora, the PDT-C 1.0, with the trained models\navailable at https://hdl.handle.net/11234/1-5293. We provide the tool as a web\nservice deployed at https://lindat.mff.cuni.cz/services/udpipe/. The source\ncode is available at GitHub (https://github.com/ufal/udpipe/tree/udpipe-2),\nalong with a Python client for a simple use. The documentation for the models\ncan be found at https://ufal.mff.cuni.cz/udpipe/2/models#czech_pdtc1.0_model.", "published": "2024-06-18 09:14:58", "link": "http://arxiv.org/abs/2406.12422v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fighting Randomness with Randomness: Mitigating Optimisation Instability\n  of Fine-Tuning using Delayed Ensemble and Noisy Interpolation", "abstract": "While fine-tuning of pre-trained language models generally helps to overcome\nthe lack of labelled training samples, it also displays model performance\ninstability. This instability mainly originates from randomness in\ninitialisation or data shuffling. To address this, researchers either modify\nthe training process or augment the available samples, which typically results\nin increased computational costs. We propose a new mitigation strategy, called\nDelayed Ensemble with Noisy Interpolation (DENI), that leverages the strengths\nof ensembling, noise regularisation and model interpolation, while retaining\ncomputational efficiency. We compare DENI with 9 representative mitigation\nstrategies across 3 models, 4 tuning strategies and 7 text classification\ndatasets. We show that: 1) DENI outperforms the best performing mitigation\nstrategy (Ensemble), while using only a fraction of its cost; 2) the mitigation\nstrategies are beneficial for parameter-efficient fine-tuning (PEFT) methods,\noutperforming full fine-tuning in specific cases; and 3) combining DENI with\ndata augmentation often leads to even more effective instability mitigation.", "published": "2024-06-18 10:20:36", "link": "http://arxiv.org/abs/2406.12471v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LightPAL: Lightweight Passage Retrieval for Open Domain Multi-Document\n  Summarization", "abstract": "Open-Domain Multi-Document Summarization (ODMDS) is the task of generating\nsummaries from large document collections in response to user queries. This\ntask is crucial for efficiently addressing diverse information needs from\nusers. Traditional retrieve-then-summarize approaches fall short for open-ended\nqueries in ODMDS tasks. These queries often require broader context than\ninitially retrieved passages provide, making it challenging to retrieve all\nrelevant information in a single search. While iterative retrieval methods has\nbeen explored for multi-hop question answering (MQA), it's impractical for\nODMDS due to high latency from repeated LLM inference. Accordingly, we propose\nLightPAL, a lightweight passage retrieval method for ODMDS. LightPAL leverages\nan LLM to pre-construct a graph representing passage relationships, then\nemploys random walk during retrieval, avoiding iterative LLM inference.\nExperiments demonstrate that LightPAL outperforms naive sparse and pre-trained\ndense retrievers in both retrieval and summarization metrics, while achieving\nhigher efficiency compared to iterative MQA approaches.", "published": "2024-06-18 10:57:27", "link": "http://arxiv.org/abs/2406.12494v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Code-Optimise: Self-Generated Preference Data for Correctness and\n  Efficiency", "abstract": "Code Language Models have been trained to generate accurate solutions,\ntypically with no regard for runtime. On the other hand, previous works that\nexplored execution optimisation have observed corresponding drops in functional\ncorrectness. To that end, we introduce Code-Optimise, a framework that\nincorporates both correctness (passed, failed) and runtime (quick, slow) as\nlearning signals via self-generated preference data. Our framework is both\nlightweight and robust as it dynamically selects solutions to reduce\noverfitting while avoiding a reliance on larger models for learning signals.\nCode-Optimise achieves significant improvements in pass@k while decreasing the\ncompetitive baseline runtimes by an additional 6% for in-domain data and up to\n3% for out-of-domain data. As a by-product, the average length of the generated\nsolutions is reduced by up to 48% on MBPP and 23% on HumanEval, resulting in\nfaster and cheaper inference. The generated data and codebase is open-sourced\nat https://github.com/huawei-noah/HEBO/tree/Code_Optimise.", "published": "2024-06-18 11:05:37", "link": "http://arxiv.org/abs/2406.12502v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FuseGen: PLM Fusion for Data-generation based Zero-shot Learning", "abstract": "Data generation-based zero-shot learning, although effective in training\nSmall Task-specific Models (STMs) via synthetic datasets generated by\nPre-trained Language Models (PLMs), is often limited by the low quality of such\nsynthetic datasets. Previous solutions have primarily focused on single PLM\nsettings, where synthetic datasets are typically restricted to specific\nsub-spaces and often deviate from real-world distributions, leading to severe\ndistribution bias. To mitigate such bias, we propose FuseGen, a novel data\ngeneration-based zero-shot learning framework that introduces a new criteria\nfor subset selection from synthetic datasets via utilizing multiple PLMs and\ntrained STMs. The chosen subset provides in-context feedback to each PLM,\nenhancing dataset quality through iterative data generation. Trained STMs are\nthen used for sample re-weighting as well, further improving data quality.\nExtensive experiments across diverse tasks demonstrate that FuseGen\nsubstantially outperforms existing methods, highly effective in boosting STM\nperformance in a PLM-agnostic way. Code is provided in\nhttps://github.com/LindaLydia/FuseGen.", "published": "2024-06-18 11:55:05", "link": "http://arxiv.org/abs/2406.12527v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unified Active Retrieval for Retrieval Augmented Generation", "abstract": "In Retrieval-Augmented Generation (RAG), retrieval is not always helpful and\napplying it to every instruction is sub-optimal. Therefore, determining whether\nto retrieve is crucial for RAG, which is usually referred to as Active\nRetrieval. However, existing active retrieval methods face two challenges: 1.\nThey usually rely on a single criterion, which struggles with handling various\ntypes of instructions. 2. They depend on specialized and highly differentiated\nprocedures, and thus combining them makes the RAG system more complicated and\nleads to higher response latency. To address these challenges, we propose\nUnified Active Retrieval (UAR). UAR contains four orthogonal criteria and casts\nthem into plug-and-play classification tasks, which achieves multifaceted\nretrieval timing judgements with negligible extra inference cost. We further\nintroduce the Unified Active Retrieval Criteria (UAR-Criteria), designed to\nprocess diverse active retrieval scenarios through a standardized procedure.\nExperiments on four representative types of user instructions show that UAR\nsignificantly outperforms existing work on the retrieval timing judgement and\nthe performance of downstream tasks, which shows the effectiveness of UAR and\nits helpfulness to downstream tasks.", "published": "2024-06-18 12:09:02", "link": "http://arxiv.org/abs/2406.12534v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Liar, Liar, Logical Mire: A Benchmark for Suppositional Reasoning in\n  Large Language Models", "abstract": "Knights and knaves problems represent a classic genre of logical puzzles\nwhere characters either tell the truth or lie. The objective is to logically\ndeduce each character's identity based on their statements. The challenge\narises from the truth-telling or lying behavior, which influences the logical\nimplications of each statement. Solving these puzzles requires not only direct\ndeductions from individual statements, but the ability to assess the\ntruthfulness of statements by reasoning through various hypothetical scenarios.\nAs such, knights and knaves puzzles serve as compelling examples of\nsuppositional reasoning. In this paper, we introduce $\\textit{TruthQuest}$, a\nbenchmark for suppositional reasoning based on the principles of knights and\nknaves puzzles. Our benchmark presents problems of varying complexity,\nconsidering both the number of characters and the types of logical statements\ninvolved. Evaluations on $\\textit{TruthQuest}$ show that large language models\nlike Llama 3 and Mixtral-8x7B exhibit significant difficulties solving these\ntasks. A detailed error analysis of the models' output reveals that\nlower-performing models exhibit a diverse range of reasoning errors, frequently\nfailing to grasp the concept of truth and lies. In comparison, more proficient\nmodels primarily struggle with accurately inferring the logical implications of\npotentially false statements.", "published": "2024-06-18 12:24:22", "link": "http://arxiv.org/abs/2406.12546v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "P-Tailor: Customizing Personality Traits for Language Models via Mixture\n  of Specialized LoRA Experts", "abstract": "Personalized large language models (LLMs) have attracted great attention in\nmany applications, such as intelligent education and emotional support. Most\nwork focuses on controlling the character settings based on the profile (e.g.,\nage, skill, experience, and so on). Conversely, the psychological theory-based\npersonality traits with implicit expression and behavior are not well modeled,\nlimiting their potential application in more specialized fields such as the\npsychological counseling agents. In this paper, we propose a mixture of experts\n(MoE)-based personalized LLMs, named P-tailor, to model the Big Five\nPersonality Traits. Particularly, we learn specialized LoRA experts to\nrepresent various traits, such as openness, conscientiousness, extraversion,\nagreeableness and neuroticism. Then, we integrate P-Tailor with a personality\nspecialization loss, promoting experts to specialize in distinct personality\ntraits, thereby enhancing the efficiency of model parameter utilization. Due to\nthe lack of datasets, we also curate a high-quality personality crafting\ndataset (PCD) to learn and develop the ability to exhibit different personality\ntraits across various topics. We conduct extensive experiments to verify the\ngreat performance and effectiveness of P-Tailor in manipulation of the\nfine-grained personality traits of LLMs.", "published": "2024-06-18 12:25:13", "link": "http://arxiv.org/abs/2406.12548v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RichRAG: Crafting Rich Responses for Multi-faceted Queries in\n  Retrieval-Augmented Generation", "abstract": "Retrieval-augmented generation (RAG) effectively addresses issues of static\nknowledge and hallucination in large language models. Existing studies mostly\nfocus on question scenarios with clear user intents and concise answers.\nHowever, it is prevalent that users issue broad, open-ended queries with\ndiverse sub-intents, for which they desire rich and long-form answers covering\nmultiple relevant aspects. To tackle this important yet underexplored problem,\nwe propose a novel RAG framework, namely RichRAG. It includes a sub-aspect\nexplorer to identify potential sub-aspects of input questions, a multi-faceted\nretriever to build a candidate pool of diverse external documents related to\nthese sub-aspects, and a generative list-wise ranker, which is a key module to\nprovide the top-k most valuable documents for the final generator. These ranked\ndocuments sufficiently cover various query aspects and are aware of the\ngenerator's preferences, hence incentivizing it to produce rich and\ncomprehensive responses for users. The training of our ranker involves a\nsupervised fine-tuning stage to ensure the basic coverage of documents, and a\nreinforcement learning stage to align downstream LLM's preferences to the\nranking of documents. Experimental results on two publicly available datasets\nprove that our framework effectively and efficiently provides comprehensive and\nsatisfying responses to users.", "published": "2024-06-18 12:52:51", "link": "http://arxiv.org/abs/2406.12566v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Applying Ensemble Methods to Model-Agnostic Machine-Generated Text\n  Detection", "abstract": "In this paper, we study the problem of detecting machine-generated text when\nthe large language model (LLM) it is possibly derived from is unknown. We do so\nby apply ensembling methods to the outputs from DetectGPT classifiers (Mitchell\net al. 2023), a zero-shot model for machine-generated text detection which is\nhighly accurate when the generative (or base) language model is the same as the\ndiscriminative (or scoring) language model. We find that simple summary\nstatistics of DetectGPT sub-model outputs yield an AUROC of 0.73 (relative to\n0.61) while retaining its zero-shot nature, and that supervised learning\nmethods sharply boost the accuracy to an AUROC of 0.94 but require a training\ndataset. This suggests the possibility of further generalisation to create a\nhighly-accurate, model-agnostic machine-generated text detector.", "published": "2024-06-18 12:58:01", "link": "http://arxiv.org/abs/2406.12570v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Not Everything is All You Need: Toward Low-Redundant Optimization for\n  Large Language Model Alignment", "abstract": "Large language models (LLMs) are still struggling in aligning with human\npreference in complex tasks and scenarios. They are prone to overfit into the\nunexpected patterns or superficial styles in the training data. We conduct an\nempirical study that only selects the top-10\\% most updated parameters in LLMs\nfor alignment training, and see improvements in the convergence process and\nfinal performance. It indicates the existence of redundant neurons in LLMs for\nalignment training. To reduce its influence, we propose a low-redundant\nalignment method named \\textbf{ALLO}, focusing on optimizing the most related\nneurons with the most useful supervised signals. Concretely, we first identify\nthe neurons that are related to the human preference data by a gradient-based\nstrategy, then identify the alignment-related key tokens by reward models for\ncomputing loss. Besides, we also decompose the alignment process into the\nforgetting and learning stages, where we first forget the tokens with unaligned\nknowledge and then learn aligned knowledge, by updating different ratios of\nneurons, respectively. Experimental results on 10 datasets have shown the\neffectiveness of ALLO. Our code and data are available at\n\\url{https://github.com/RUCAIBox/ALLO}.", "published": "2024-06-18 13:34:40", "link": "http://arxiv.org/abs/2406.12606v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Insights to Actions: The Impact of Interpretability and Analysis\n  Research on NLP", "abstract": "Interpretability and analysis (IA) research is a growing subfield within NLP\nwith the goal of developing a deeper understanding of the behavior or inner\nworkings of NLP systems and methods. Despite growing interest in the subfield,\na criticism of this work is that it lacks actionable insights and therefore has\nlittle impact on NLP. In this paper, we seek to quantify the impact of IA\nresearch on the broader field of NLP. We approach this with a mixed-methods\nanalysis of: (1) a citation graph of 185K+ papers built from all papers\npublished at ACL and EMNLP conferences from 2018 to 2023, and their references\nand citations, and (2) a survey of 138 members of the NLP community. Our\nquantitative results show that IA work is well-cited outside of IA, and central\nin the NLP citation graph. Through qualitative analysis of survey responses and\nmanual annotation of 556 papers, we find that NLP researchers build on findings\nfrom IA work and perceive it as important for progress in NLP, multiple\nsubfields, and rely on its findings and terminology for their own work. Many\nnovel methods are proposed based on IA findings and highly influenced by them,\nbut highly influential non-IA work cites IA findings without being driven by\nthem. We end by summarizing what is missing in IA work today and provide a call\nto action, to pave the way for a more impactful future of IA research.", "published": "2024-06-18 13:45:07", "link": "http://arxiv.org/abs/2406.12618v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Makes Two Language Models Think Alike?", "abstract": "Do architectural differences significantly affect the way models represent\nand process language? We propose a new approach, based on metric-learning\nencoding models (MLEMs), as a first step to answer this question. The approach\nprovides a feature-based comparison of how any two layers of any two models\nrepresent linguistic information. We apply the method to BERT, GPT-2 and Mamba.\nUnlike previous methods, MLEMs offer a transparent comparison, by identifying\nthe specific linguistic features responsible for similarities and differences.\nMore generally, the method uses formal, symbolic descriptions of a domain, and\nuse these to compare neural representations. As such, the approach can\nstraightforwardly be extended to other domains, such as speech and vision, and\nto other neural systems, including human brains.", "published": "2024-06-18 13:45:50", "link": "http://arxiv.org/abs/2406.12620v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Growing Trees on Sounds: Assessing Strategies for End-to-End Dependency\n  Parsing of Speech", "abstract": "Direct dependency parsing of the speech signal -- as opposed to parsing\nspeech transcriptions -- has recently been proposed as a task (Pupier et al.\n2022), as a way of incorporating prosodic information in the parsing system and\nbypassing the limitations of a pipeline approach that would consist of using\nfirst an Automatic Speech Recognition (ASR) system and then a syntactic parser.\nIn this article, we report on a set of experiments aiming at assessing the\nperformance of two parsing paradigms (graph-based parsing and sequence labeling\nbased parsing) on speech parsing. We perform this evaluation on a large\ntreebank of spoken French, featuring realistic spontaneous conversations. Our\nfindings show that (i) the graph based approach obtain better results across\nthe board (ii) parsing directly from speech outperforms a pipeline approach,\ndespite having 30% fewer parameters.", "published": "2024-06-18 13:46:10", "link": "http://arxiv.org/abs/2406.12621v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DetectBench: Can Large Language Model Detect and Piece Together Implicit\n  Evidence?", "abstract": "Detecting evidence within the context is a key step in the process of\nreasoning task. Evaluating and enhancing the capabilities of LLMs in evidence\ndetection will strengthen context-based reasoning performance. This paper\nproposes a benchmark called DetectBench for verifying the ability to detect and\npiece together implicit evidence within a long context. DetectBench contains\n3,928 multiple-choice questions, with an average of 994 tokens per question.\nEach question contains an average of 4.55 pieces of implicit evidence, and\nsolving the problem typically requires 7.62 logical jumps to find the correct\nanswer. To enhance the performance of LLMs in evidence detection, this paper\nproposes Detective Reasoning Prompt and Finetune. Experiments demonstrate that\nthe existing LLMs' abilities to detect evidence in long contexts are far\ninferior to humans. However, the Detective Reasoning Prompt effectively\nenhances the capability of powerful LLMs in evidence detection, while the\nFinetuning method shows significant effects in enhancing the performance of\nweaker LLMs. Moreover, when the abilities of LLMs in evidence detection are\nimproved, their final reasoning performance is also enhanced accordingly.", "published": "2024-06-18 14:08:01", "link": "http://arxiv.org/abs/2406.12641v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Estimating Knowledge in Large Language Models Without Generating a\n  Single Token", "abstract": "To evaluate knowledge in large language models (LLMs), current methods query\nthe model and then evaluate its generated responses. In this work, we ask\nwhether evaluation can be done before the model has generated any text.\nConcretely, is it possible to estimate how knowledgeable a model is about a\ncertain entity, only from its internal computation? We study this question with\ntwo tasks: given a subject entity, the goal is to predict (a) the ability of\nthe model to answer common questions about the entity, and (b) the factuality\nof open-ended responses generated by the model about the entity. Experiments\nwith a variety of LLMs show that KEEN, a simple probe trained over internal\nsubject representations, succeeds at both tasks - correlating with both the QA\naccuracy of the model per-subject and FActScore, a recent factuality metric in\nopen-ended generation. Moreover, KEEN naturally aligns with the model's hedging\nbehavior and faithfully reflects changes in the model's knowledge after\nfine-tuning. Lastly, we show a more interpretable yet equally performant\nvariant of KEEN, which highlights a small set of tokens indicative of clusters\nand gaps in the model's knowledge. Being simple and lightweight, KEEN can be\nleveraged to guide decisions such as when it is appropriate to apply further\ntraining or augment queries with retrieval.", "published": "2024-06-18 14:45:50", "link": "http://arxiv.org/abs/2406.12673v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vernacular? I Barely Know Her: Challenges with Style Control and\n  Stereotyping", "abstract": "Large Language Models (LLMs) are increasingly being used in educational and\nlearning applications. Research has demonstrated that controlling for style, to\nfit the needs of the learner, fosters increased understanding, promotes\ninclusion, and helps with knowledge distillation. To understand the\ncapabilities and limitations of contemporary LLMs in style control, we\nevaluated five state-of-the-art models: GPT-3.5, GPT-4, GPT-4o, Llama-3, and\nMistral-instruct- 7B across two style control tasks. We observed significant\ninconsistencies in the first task, with model performances averaging between\n5th and 8th grade reading levels for tasks intended for first-graders, and\nstandard deviations up to 27.6. For our second task, we observed a\nstatistically significant improvement in performance from 0.02 to 0.26.\nHowever, we find that even without stereotypes in reference texts, LLMs often\ngenerated culturally insensitive content during their tasks. We provide a\nthorough analysis and discussion of the results.", "published": "2024-06-18 14:51:30", "link": "http://arxiv.org/abs/2406.12679v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Psychological Depth in Language Models", "abstract": "Evaluations of creative stories generated by large language models (LLMs)\noften focus on objective properties of the text, such as its style, coherence,\nand diversity. While these metrics are indispensable, they do not speak to a\nstory's subjective, psychological impact from a reader's perspective. We\nintroduce the Psychological Depth Scale (PDS), a novel framework rooted in\nliterary theory that measures an LLM's ability to produce authentic and\nnarratively complex stories that provoke emotion, empathy, and engagement. We\nempirically validate our framework by showing that humans can consistently\nevaluate stories based on PDS (0.72 Krippendorff's alpha). We also explore\ntechniques for automating the PDS to easily scale future analyses. GPT-4o,\ncombined with a novel Mixture-of-Personas (MoP) prompting strategy, achieves an\naverage Spearman correlation of 0.51 with human judgment while Llama-3-70B with\nconstrained decoding scores as high as 0.68 for empathy. Finally, we compared\nthe depth of stories authored by both humans and LLMs. Surprisingly, GPT-4\nstories either surpassed or were statistically indistinguishable from\nhighly-rated human-written stories sourced from Reddit. By shifting the focus\nfrom text to reader, the Psychological Depth Scale is a validated, automated,\nand systematic means of measuring the capacity of LLMs to connect with humans\nthrough the stories they tell.", "published": "2024-06-18 14:51:54", "link": "http://arxiv.org/abs/2406.12680v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using LLMs to Aid Annotation and Collection of Clinically-Enriched Data\n  in Bipolar Disorder and Schizophrenia", "abstract": "NLP in mental health has been primarily social media focused. Real world\npractitioners also have high case loads and often domain specific variables, of\nwhich modern LLMs lack context. We take a dataset made by recruiting 644\nparticipants, including individuals diagnosed with Bipolar Disorder (BD),\nSchizophrenia (SZ), and Healthy Controls (HC). Participants undertook tasks\nderived from a standardized mental health instrument, and the resulting data\nwere transcribed and annotated by experts across five clinical variables. This\npaper demonstrates the application of contemporary language models in\nsequence-to-sequence tasks to enhance mental health research. Specifically, we\nillustrate how these models can facilitate the deployment of mental health\ninstruments, data collection, and data annotation with high accuracy and\nscalability. We show that small models are capable of annotation for\ndomain-specific clinical variables, data collection for mental-health\ninstruments, and perform better then commercial large models.", "published": "2024-06-18 15:00:24", "link": "http://arxiv.org/abs/2406.12687v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "[WIP] Jailbreak Paradox: The Achilles' Heel of LLMs", "abstract": "We introduce two paradoxes concerning jailbreak of foundation models: First,\nit is impossible to construct a perfect jailbreak classifier, and second, a\nweaker model cannot consistently detect whether a stronger (in a\npareto-dominant sense) model is jailbroken or not. We provide formal proofs for\nthese paradoxes and a short case study on Llama and GPT4-o to demonstrate this.\nWe discuss broader theoretical and practical repercussions of these results.", "published": "2024-06-18 15:14:35", "link": "http://arxiv.org/abs/2406.12702v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AgentReview: Exploring Peer Review Dynamics with LLM Agents", "abstract": "Peer review is fundamental to the integrity and advancement of scientific\npublication. Traditional methods of peer review analyses often rely on\nexploration and statistics of existing peer review data, which do not\nadequately address the multivariate nature of the process, account for the\nlatent variables, and are further constrained by privacy concerns due to the\nsensitive nature of the data. We introduce AgentReview, the first large\nlanguage model (LLM) based peer review simulation framework, which effectively\ndisentangles the impacts of multiple latent factors and addresses the privacy\nissue. Our study reveals significant insights, including a notable 37.1%\nvariation in paper decisions due to reviewers' biases, supported by\nsociological theories such as the social influence theory, altruism fatigue,\nand authority bias. We believe that this study could offer valuable insights to\nimprove the design of peer review mechanisms. Our code is available at\nhttps://github.com/Ahren09/AgentReview.", "published": "2024-06-18 15:22:12", "link": "http://arxiv.org/abs/2406.12708v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+\n  Languages", "abstract": "LLMs have become a go-to solution not just for text generation, but also for\nnatural language understanding (NLU) tasks. Acquiring extensive knowledge\nthrough language modeling on web-scale corpora, they excel on English NLU, yet\nstruggle to extend their NLU capabilities to underrepresented languages. In\ncontrast, machine translation models (MT) produce excellent multilingual\nrepresentations, resulting in strong translation performance even for\nlow-resource languages. MT encoders, however, lack the knowledge necessary for\ncomprehensive NLU that LLMs obtain through language modeling training on\nimmense corpora. In this work, we get the best both worlds by integrating MT\nencoders directly into LLM backbones via sample-efficient self-distillation.\nThe resulting MT-LLMs preserve the inherent multilingual representational\nalignment from the MT encoder, allowing lower-resource languages to tap into\nthe rich knowledge embedded in English-centric LLMs. Merging the MT encoder and\nLLM in a single model, we mitigate the propagation of translation errors and\ninference overhead of MT decoding inherent to discrete translation-based\ncross-lingual transfer (e.g., translate-test). Evaluation spanning three\nprominent NLU tasks and 127 predominantly low-resource languages renders\nMT-LLMs highly effective in cross-lingual transfer. MT-LLMs substantially and\nconsistently outperform translate-test based on the same MT model, showing that\nwe truly unlock multilingual language understanding for LLMs.", "published": "2024-06-18 16:00:20", "link": "http://arxiv.org/abs/2406.12739v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diversify, Rationalize, and Combine: Ensembling Multiple QA Strategies\n  for Zero-shot Knowledge-based VQA", "abstract": "Knowledge-based Visual Question-answering (K-VQA) often requires the use of\nbackground knowledge beyond the image. However, we discover that a single\nknowledge generation strategy is often insufficient for all K-VQA questions. To\nthis end, we propose Diversification, Evidence Truncation, and Combination for\nKnowledge-based Elucidation (DietCoke), which utilizes a bundle of\ncomplementary question-answering tactics and aggregates their answers using\ntextual rationales. DietCoke comprises of three stages: diversification,\nrationalization, and ensemble. The diversification stage generates three\ndistinctive decision contexts, each leading to its own answer candidate. The\nrationalization stage generates two rationales, the automatic rationale and the\nmechanistic rationale, for each answer candidate using decorrelated techniques.\nFinally, in the ensemble stage, an LLM informed by the rationales selects one\nanswer from the three candidates. Experiments show that DietCoke significantly\noutperforms state-of-the-art LLM-based baselines by 2.8% on OK-VOA and 4.7% on\nA-OKVOA and that the strategies in the ensembles are highly complementary. Code\nis available at: https://github.com/limiaoyu/DietCoke", "published": "2024-06-18 16:06:38", "link": "http://arxiv.org/abs/2406.12746v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hopping Too Late: Exploring the Limitations of Large Language Models on\n  Multi-Hop Queries", "abstract": "Large language models (LLMs) can solve complex multi-step problems, but\nlittle is known about how these computations are implemented internally.\nMotivated by this, we study how LLMs answer multi-hop queries such as \"The\nspouse of the performer of Imagine is\". These queries require two information\nextraction steps: a latent one for resolving the first hop (\"the performer of\nImagine\") into the bridge entity (John Lennon), and another for resolving the\nsecond hop (\"the spouse of John Lennon\") into the target entity (Yoko Ono).\nUnderstanding how the latent step is computed internally is key to\nunderstanding the overall computation. By carefully analyzing the internal\ncomputations of transformer-based LLMs, we discover that the bridge entity is\nresolved in the early layers of the model. Then, only after this resolution,\nthe two-hop query is solved in the later layers. Because the second hop\ncommences in later layers, there could be cases where these layers no longer\nencode the necessary knowledge for correctly predicting the answer. Motivated\nby this, we propose a novel \"back-patching\" analysis method whereby a hidden\nrepresentation from a later layer is patched back to an earlier layer. We find\nthat in up to 66% of previously incorrect cases there exists a back-patch that\nresults in the correct generation of the answer, showing that the later layers\nindeed sometimes lack the needed functionality. Overall, our methods and\nfindings open further opportunities for understanding and improving latent\nreasoning in transformer-based LLMs.", "published": "2024-06-18 16:44:13", "link": "http://arxiv.org/abs/2406.12775v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Composited-Nested-Learning with Data Augmentation for Nested Named\n  Entity Recognition", "abstract": "Nested Named Entity Recognition (NNER) focuses on addressing overlapped\nentity recognition. Compared to Flat Named Entity Recognition (FNER), annotated\nresources are scarce in the corpus for NNER. Data augmentation is an effective\napproach to address the insufficient annotated corpus. However, there is a\nsignificant lack of exploration in data augmentation methods for NNER. Due to\nthe presence of nested entities in NNER, existing data augmentation methods\ncannot be directly applied to NNER tasks. Therefore, in this work, we focus on\ndata augmentation for NNER and resort to more expressive structures,\nComposited-Nested-Label Classification (CNLC) in which constituents are\ncombined by nested-word and nested-label, to model nested entities. The dataset\nis augmented using the Composited-Nested-Learning (CNL). In addition, we\npropose the Confidence Filtering Mechanism (CFM) for a more efficient selection\nof generated data. Experimental results demonstrate that this approach results\nin improvements in ACE2004 and ACE2005 and alleviates the impact of sample\nimbalance.", "published": "2024-06-18 16:46:18", "link": "http://arxiv.org/abs/2406.12779v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UBENCH: Benchmarking Uncertainty in Large Language Models with Multiple\n  Choice Questions", "abstract": "The rapid development of large language models (LLMs) has shown promising\npractical results. However, their low interpretability often leads to errors in\nunforeseen circumstances, limiting their utility. Many works have focused on\ncreating comprehensive evaluation systems, but previous benchmarks have\nprimarily assessed problem-solving abilities while neglecting the response's\nuncertainty, which may result in unreliability. Recent methods for measuring\nLLM reliability are resource-intensive and unable to test black-box models. To\naddress this, we propose UBENCH, a comprehensive benchmark for evaluating LLM\nreliability. UBENCH includes 3,978 multiple-choice questions covering\nknowledge, language, understanding, and reasoning abilities. Experimental\nresults show that UBENCH has achieved state-of-the-art performance, while its\nsingle-sampling method significantly saves computational resources compared to\nbaseline methods that require multiple samplings. Additionally, based on\nUBENCH, we evaluate the reliability of 15 popular LLMs, finding GLM4 to be the\nmost outstanding, closely followed by GPT-4. We also explore the impact of\nChain-of-Thought prompts, role-playing prompts, option order, and temperature\non LLM reliability, analyzing the varying effects on different LLMs.", "published": "2024-06-18 16:50:38", "link": "http://arxiv.org/abs/2406.12784v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All\n  Tools", "abstract": "We introduce ChatGLM, an evolving family of large language models that we\nhave been developing over time. This report primarily focuses on the GLM-4\nlanguage series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent\nour most capable models that are trained with all the insights and lessons\ngained from the preceding three generations of ChatGLM. To date, the GLM-4\nmodels are pre-trained on ten trillions of tokens mostly in Chinese and\nEnglish, along with a small set of corpus from 24 languages, and aligned\nprimarily for Chinese and English usage. The high-quality alignment is achieved\nvia a multi-stage post-training process, which involves supervised fine-tuning\nand learning from human feedback. Evaluations show that GLM-4 1) closely rivals\nor outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH,\nBBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following\nas measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long\ncontext tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by\nAlignBench. The GLM-4 All Tools model is further aligned to understand user\nintent and autonomously decide when and which tool(s) touse -- including web\nbrowser, Python interpreter, text-to-image model, and user-defined functions --\nto effectively complete complex tasks. In practical applications, it matches\nand even surpasses GPT-4 All Tools in tasks like accessing online information\nvia web browsing and solving math problems using Python interpreter. Over the\ncourse, we have open-sourced a series of models, including ChatGLM-6B (three\ngenerations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting\nover 10 million downloads on Hugging face in the year 2023 alone. The open\nmodels can be accessed through https://github.com/THUDM and\nhttps://huggingface.co/THUDM.", "published": "2024-06-18 16:58:21", "link": "http://arxiv.org/abs/2406.12793v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Always Solve Easy Problems if They Can Solve\n  Harder Ones?", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities, but\nstill suffer from inconsistency issues (e.g. LLMs can react differently to\ndisturbances like rephrasing or inconsequential order change). In addition to\nthese inconsistencies, we also observe that LLMs, while capable of solving hard\nproblems, can paradoxically fail at easier ones. To evaluate this hard-to-easy\ninconsistency, we develop the ConsisEval benchmark, where each entry comprises\na pair of questions with a strict order of difficulty. Furthermore, we\nintroduce the concept of consistency score to quantitatively measure this\ninconsistency and analyze the potential for improvement in consistency by\nrelative consistency score. Based on comprehensive experiments across a variety\nof existing models, we find: (1) GPT-4 achieves the highest consistency score\nof 92.2\\% but is still inconsistent to specific questions due to distraction by\nredundant information, misinterpretation of questions, etc.; (2) models with\nstronger capabilities typically exhibit higher consistency, but exceptions also\nexist; (3) hard data enhances consistency for both fine-tuning and in-context\nlearning. Our data and code will be publicly available on GitHub.", "published": "2024-06-18 17:25:47", "link": "http://arxiv.org/abs/2406.12809v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Are the Odds? Language Models Are Capable of Probabilistic\n  Reasoning", "abstract": "Language models (LM) are capable of remarkably complex linguistic tasks;\nhowever, numerical reasoning is an area in which they frequently struggle. An\nimportant but rarely evaluated form of reasoning is understanding probability\ndistributions. In this paper, we focus on evaluating the probabilistic\nreasoning capabilities of LMs using idealized and real-world statistical\ndistributions. We perform a systematic evaluation of state-of-the-art LMs on\nthree tasks: estimating percentiles, drawing samples, and calculating\nprobabilities. We evaluate three ways to provide context to LMs 1) anchoring\nexamples from within a distribution or family of distributions, 2) real-world\ncontext, 3) summary statistics on which to base a Normal approximation. Models\ncan make inferences about distributions, and can be further aided by the\nincorporation of real-world context, example shots and simplified assumptions,\neven if these assumptions are incorrect or misspecified. To conduct this work,\nwe developed a comprehensive benchmark distribution dataset with associated\nquestion-answer pairs that we have released publicly.", "published": "2024-06-18 17:51:24", "link": "http://arxiv.org/abs/2406.12830v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discovering Elementary Discourse Units in Textual Data Using Canonical\n  Correlation Analysis", "abstract": "Canonical Correlation Analysis (CCA) has been exploited immensely for\nlearning latent representations in various fields. This study takes a step\nfurther by demonstrating the potential of CCA in identifying Elementary\nDiscourse Units(EDUs) that captures the latent information within the textual\ndata. The probabilistic interpretation of CCA discussed in this study utilizes\nthe two-view nature of textual data, i.e. the consecutive sentences in a\ndocument or turns in a dyadic conversation, and has a strong theoretical\nfoundation. Furthermore, this study proposes a model for Elementary Discourse\nUnit(EDU) segmentation that discovers EDUs in textual data without any\nsupervision. To validate the model, the EDUs are utilized as textual unit for\ncontent selection in textual similarity task. Empirical results on Semantic\nTextual Similarity(STSB) and Mohler datasets confirm that, despite represented\nas a unigram, the EDUs deliver competitive results and can even beat various\nsophisticated supervised techniques. The model is simple, linear, adaptable and\nlanguage independent making it an ideal baseline particularly when labeled\ntraining data is scarce or nonexistent.", "published": "2024-06-18 18:37:24", "link": "http://arxiv.org/abs/2406.12997v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models", "abstract": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation.", "published": "2024-06-18 20:01:51", "link": "http://arxiv.org/abs/2406.13035v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation", "abstract": "Despite their impressive capabilities, large language models (LLMs) often\nface challenges such as temporal misalignment and generating hallucinatory\ncontent. Enhancing LLMs with retrieval mechanisms to fetch relevant information\nfrom external sources offers a promising solution. Inspired by the proverb\n\"Think twice before you act,\" we propose a dual-angle evaluated\nretrieval-augmented generation framework \\textit{Think-then-Act}. Unlike\nprevious approaches that indiscriminately rewrite queries or perform retrieval\nregardless of necessity, or generate temporary responses before deciding on\nadditional retrieval, which increases model generation costs, our framework\nemploys a two-phase process: (i) assessing the input query for clarity and\ncompleteness to determine if rewriting is necessary; and (ii) evaluating the\nmodel's capability to answer the query and deciding if additional retrieval is\nneeded. Experimental results on five datasets show that the\n\\textit{Think-then-Act} framework significantly improves performance. Our\nframework demonstrates notable improvements in accuracy and efficiency compared\nto existing baselines and performs well in both English and non-English\ncontexts. Ablation studies validate the optimal model confidence threshold,\nhighlighting the resource optimization benefits of our approach.", "published": "2024-06-18 20:51:34", "link": "http://arxiv.org/abs/2406.13050v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Synopses of Movie Narratives: A Dataset for Vision-Language\n  Story Understanding", "abstract": "Story video-text alignment, a core task in computational story understanding,\naims to align video clips with corresponding sentences in their descriptions.\nHowever, progress on the task has been held back by the scarcity of manually\nannotated video-text correspondence and the heavy concentration on English\nnarrations of Hollywood movies. To address these issues, in this paper, we\nconstruct a large-scale multilingual video story dataset named Multilingual\nSynopses of Movie Narratives (M-SYMON), containing 13,166 movie summary videos\nfrom 7 languages, as well as manual annotation of fine-grained video-text\ncorrespondences for 101.5 hours of video. Training on the human annotated data\nfrom SyMoN outperforms the SOTA methods by 15.7 and 16.2 percentage points on\nClip Accuracy and Sentence IoU scores, respectively, demonstrating the\neffectiveness of the annotations. As benchmarks for future research, we create\n6 baseline approaches with different multilingual training strategies, compare\ntheir performance in both intra-lingual and cross-lingual setups, exemplifying\nthe challenges of multilingual video-text alignment. The dataset is released\nat: https://github.com/insundaycathy/M-SyMoN", "published": "2024-06-18 22:44:50", "link": "http://arxiv.org/abs/2406.13092v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GPT Czech Poet: Generation of Czech Poetic Strophes with Language Models", "abstract": "High-quality automated poetry generation systems are currently only available\nfor a small subset of languages. We introduce a new model for generating poetry\nin Czech language, based on fine-tuning a pre-trained Large Language Model. We\ndemonstrate that guiding the generation process by explicitly specifying\nstrophe parameters within the poem text strongly improves the effectiveness of\nthe model. We also find that appropriate tokenization is crucial, showing that\ntokenization methods based on syllables or individual characters instead of\nsubwords prove superior in generating poetic strophes. We further enhance the\nresults by introducing \\textit{Forced~generation}, adding explicit\nspecifications of meter and verse parameters at inference time based on the\nalready generated text. We evaluate a range of setups, showing that our\nproposed approach achieves high accuracies in rhyming and metric aspects of\nformal quality of the generated poems.", "published": "2024-06-18 06:19:45", "link": "http://arxiv.org/abs/2407.12790v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs Are Prone to Fallacies in Causal Inference", "abstract": "Recent work shows that causal facts can be effectively extracted from LLMs\nthrough prompting, facilitating the creation of causal graphs for causal\ninference tasks. However, it is unclear if this success is limited to\nexplicitly-mentioned causal facts in the pretraining data which the model can\nmemorize. Thus, this work investigates: Can LLMs infer causal relations from\nother relational data in text? To disentangle the role of memorized causal\nfacts vs inferred causal relations, we finetune LLMs on synthetic data\ncontaining temporal, spatial and counterfactual relations, and measure whether\nthe LLM can then infer causal relations. We find that: (a) LLMs are susceptible\nto inferring causal relations from the order of two entity mentions in text\n(e.g. X mentioned before Y implies X causes Y); (b) if the order is randomized,\nLLMs still suffer from the post hoc fallacy, i.e. X occurs before Y (temporal\nrelation) implies X causes Y. We also find that while LLMs can correctly deduce\nthe absence of causal relations from temporal and spatial relations, they have\ndifficulty inferring causal relations from counterfactuals, questioning their\nunderstanding of causality.", "published": "2024-06-18 00:14:07", "link": "http://arxiv.org/abs/2406.12158v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the Impact of a Transformer's Latent Space Geometry on\n  Downstream Task Performance", "abstract": "It is generally thought that transformer-based large language models benefit\nfrom pre-training by learning generic linguistic knowledge that can be focused\non a specific task during fine-tuning. However, we propose that much of the\nbenefit from pre-training may be captured by geometric characteristics of the\nlatent space representations, divorced from any specific linguistic knowledge.\nIn this work we explore the relationship between GLUE benchmarking task\nperformance and a variety of measures applied to the latent space resulting\nfrom BERT-type contextual language models. We find that there is a strong\nlinear relationship between a measure of quantized cell density and average\nGLUE performance and that these measures may be predictive of otherwise\nsurprising GLUE performance for several non-standard BERT-type models from the\nliterature. These results may be suggestive of a strategy for decreasing\npre-training requirements, wherein model initialization can be informed by the\ngeometric characteristics of the model's latent space.", "published": "2024-06-18 00:17:30", "link": "http://arxiv.org/abs/2406.12159v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Aqulia-Med LLM: Pioneering Full-Process Open-Source Medical Language\n  Models", "abstract": "Recently, both closed-source LLMs and open-source communities have made\nsignificant strides, outperforming humans in various general domains. However,\ntheir performance in specific professional fields such as medicine, especially\nwithin the open-source community, remains suboptimal due to the complexity of\nmedical knowledge. We propose Aquila-Med, a bilingual medical LLM based on\nAquila, addressing these challenges through continue pre-training, supervised\nfine-tuning (SFT), and reinforcement learning from human feedback (RLHF). We\nconstruct a large-scale Chinese and English medical dataset for continue\npre-training and a high-quality SFT dataset, covering extensive medical\nspecialties. Additionally, we develop a high-quality Direct Preference\nOptimization (DPO) dataset for further alignment. Aquila-Med achieves notable\nresults across single-turn, multi-turn dialogues, and medical multiple-choice\nquestions, demonstrating the effectiveness of our approach. We open-source the\ndatasets and the entire training process, contributing valuable resources to\nthe research community. Our models and datasets will released at\nhttps://huggingface.co/BAAI/AquilaMed-RL.", "published": "2024-06-18 01:30:07", "link": "http://arxiv.org/abs/2406.12182v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Debate as Optimization: Adaptive Conformal Prediction and Diverse\n  Retrieval for Event Extraction", "abstract": "We propose a multi-agent debate as optimization (DAO) system for event\nextraction, where the primary objective is to iteratively refine the large\nlanguage models (LLMs) outputs through debating without parameter tuning. In\nDAO, we introduce two novel modules: the Diverse-RAG (DRAG) module and the\nAdaptive Conformal Prediction (AdaCP) module. DRAG systematically retrieves\nsupporting information that best fits the debate discussion, while AdaCP\nenhances the accuracy and reliability of event extraction by effectively\nrejecting less promising answers. Experimental results demonstrate a\nsignificant reduction in the performance gap between supervised approaches and\ntuning-free LLM-based methods by 18.1% and 17.8% on ACE05 and 17.9% and 15.2%\non CASIE for event detection and argument extraction respectively.", "published": "2024-06-18 01:53:49", "link": "http://arxiv.org/abs/2406.12197v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is persona enough for personality? Using ChatGPT to reconstruct an\n  agent's latent personality from simple descriptions", "abstract": "Personality, a fundamental aspect of human cognition, contains a range of\ntraits that influence behaviors, thoughts, and emotions. This paper explores\nthe capabilities of large language models (LLMs) in reconstructing these\ncomplex cognitive attributes based only on simple descriptions containing\nsocio-demographic and personality type information. Utilizing the HEXACO\npersonality framework, our study examines the consistency of LLMs in recovering\nand predicting underlying (latent) personality dimensions from simple\ndescriptions. Our experiments reveal a significant degree of consistency in\npersonality reconstruction, although some inconsistencies and biases, such as a\ntendency to default to positive traits in the absence of explicit information,\nare also observed. Additionally, socio-demographic factors like age and number\nof children were found to influence the reconstructed personality dimensions.\nThese findings have implications for building sophisticated agent-based\nsimulacra using LLMs and highlight the need for further research on robust\npersonality generation in LLMs.", "published": "2024-06-18 02:32:57", "link": "http://arxiv.org/abs/2406.12216v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ToxiCloakCN: Evaluating Robustness of Offensive Language Detection in\n  Chinese with Cloaking Perturbations", "abstract": "Detecting hate speech and offensive language is essential for maintaining a\nsafe and respectful digital environment. This study examines the limitations of\nstate-of-the-art large language models (LLMs) in identifying offensive content\nwithin systematically perturbed data, with a focus on Chinese, a language\nparticularly susceptible to such perturbations. We introduce\n\\textsf{ToxiCloakCN}, an enhanced dataset derived from ToxiCN, augmented with\nhomophonic substitutions and emoji transformations, to test the robustness of\nLLMs against these cloaking perturbations. Our findings reveal that existing\nmodels significantly underperform in detecting offensive content when these\nperturbations are applied. We provide an in-depth analysis of how different\ntypes of offensive content are affected by these perturbations and explore the\nalignment between human and model explanations of offensiveness. Our work\nhighlights the urgent need for more advanced techniques in offensive language\ndetection to combat the evolving tactics used to evade detection mechanisms.", "published": "2024-06-18 02:44:56", "link": "http://arxiv.org/abs/2406.12223v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "MCSD: An Efficient Language Model with Diverse Fusion", "abstract": "Transformers excel in Natural Language Processing (NLP) due to their prowess\nin capturing long-term dependencies but suffer from exponential resource\nconsumption with increasing sequence lengths. To address these challenges, we\npropose MCSD model, an efficient language model with linear scaling and fast\ninference speed. MCSD model leverages diverse feature fusion, primarily through\nthe multi-channel slope and decay (MCSD) block, to robustly represent features.\nThis block comprises slope and decay sections that extract features across\ndiverse temporal receptive fields, facilitating capture of both local and\nglobal information. In addition, MCSD block conducts element-wise fusion of\ndiverse features to further enhance the delicate feature extraction capability.\nFor inference, we formulate the inference process into a recurrent\nrepresentation, slashing space complexity to $O(1)$ and time complexity to\n$O(N)$ respectively. Our experiments show that MCSD attains higher throughput\nand lower GPU memory consumption compared to Transformers, while maintaining\ncomparable performance to larger-scale language learning models on benchmark\ntests. These attributes position MCSD as a promising base for edge deployment\nand embodied intelligence.", "published": "2024-06-18 03:08:01", "link": "http://arxiv.org/abs/2406.12230v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "\"You Gotta be a Doctor, Lin\": An Investigation of Name-Based Bias of\n  Large Language Models in Employment Recommendations", "abstract": "Social science research has shown that candidates with names indicative of\ncertain races or genders often face discrimination in employment practices.\nSimilarly, Large Language Models (LLMs) have demonstrated racial and gender\nbiases in various applications. In this study, we utilize GPT-3.5-Turbo and\nLlama 3-70B-Instruct to simulate hiring decisions and salary recommendations\nfor candidates with 320 first names that strongly signal their race and gender,\nacross over 750,000 prompts. Our empirical results indicate a preference among\nthese models for hiring candidates with White female-sounding names over other\ndemographic groups across 40 occupations. Additionally, even among candidates\nwith identical qualifications, salary recommendations vary by as much as 5%\nbetween different subgroups. A comparison with real-world labor data reveals\ninconsistent alignment with U.S. labor market characteristics, underscoring the\nnecessity of risk investigation of LLM-powered systems.", "published": "2024-06-18 03:11:43", "link": "http://arxiv.org/abs/2406.12232v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Finding Task-specific Subnetworks in Multi-task Spoken Language\n  Understanding Model", "abstract": "Recently, multi-task spoken language understanding (SLU) models have emerged,\ndesigned to address various speech processing tasks. However, these models\noften rely on a large number of parameters. Also, they often encounter\ndifficulties in adapting to new data for a specific task without experiencing\ncatastrophic forgetting of previously trained tasks. In this study, we propose\nfinding task-specific subnetworks within a multi-task SLU model via neural\nnetwork pruning. In addition to model compression, we expect that the\nforgetting of previously trained tasks can be mitigated by updating only a\ntask-specific subnetwork. We conduct experiments on top of the state-of-the-art\nmulti-task SLU model ``UniverSLU'', trained for several tasks such as emotion\nrecognition (ER), intent classification (IC), and automatic speech recognition\n(ASR). We show that pruned models were successful in adapting to additional ASR\nor IC data with minimal performance degradation on previously trained tasks.", "published": "2024-06-18 06:39:41", "link": "http://arxiv.org/abs/2406.12317v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Retrieval Meets Reasoning: Dynamic In-Context Editing for Long-Text\n  Understanding", "abstract": "Current Large Language Models (LLMs) face inherent limitations due to their\npre-defined context lengths, which impede their capacity for multi-hop\nreasoning within extensive textual contexts. While existing techniques like\nRetrieval-Augmented Generation (RAG) have attempted to bridge this gap by\nsourcing external information, they fall short when direct answers are not\nreadily available. We introduce a novel approach that re-imagines information\nretrieval through dynamic in-context editing, inspired by recent breakthroughs\nin knowledge editing. By treating lengthy contexts as malleable external\nknowledge, our method interactively gathers and integrates relevant\ninformation, thereby enabling LLMs to perform sophisticated reasoning steps.\nExperimental results demonstrate that our method effectively empowers\ncontext-limited LLMs, such as Llama2, to engage in multi-hop reasoning with\nimproved performance, which outperforms state-of-the-art context window\nextrapolation methods and even compares favorably to more advanced commercial\nlong-context models. Our interactive method not only enhances reasoning\ncapabilities but also mitigates the associated training and computational\ncosts, making it a pragmatic solution for enhancing LLMs' reasoning within\nexpansive contexts.", "published": "2024-06-18 06:54:28", "link": "http://arxiv.org/abs/2406.12331v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Attention Score is not All You Need for Token Importance Indicator in KV\n  Cache Reduction: Value Also Matters", "abstract": "Scaling the context size of large language models (LLMs) enables them to\nperform various new tasks, e.g., book summarization. However, the memory cost\nof the Key and Value (KV) cache in attention significantly limits the practical\napplications of LLMs. Recent works have explored token pruning for KV cache\nreduction in LLMs, relying solely on attention scores as a token importance\nindicator. However, our investigation into value vector norms revealed a\nnotably non-uniform pattern questioning their reliance only on attention\nscores. Inspired by this, we propose a new method: Value-Aware Token Pruning\n(VATP) which uses both attention scores and the $ \\ell_{1} $ norm of value\nvectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat\nand Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms\nattention-score-only baselines in over 12 tasks, confirming the effectiveness\nof incorporating value vector norms into token importance evaluation of LLMs.", "published": "2024-06-18 07:01:11", "link": "http://arxiv.org/abs/2406.12335v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Comparative Study of Continuous Sign Language Recognition Techniques", "abstract": "Continuous Sign Language Recognition (CSLR) focuses on the interpretation of\na sequence of sign language gestures performed continually without pauses. In\nthis study, we conduct an empirical evaluation of recent deep learning CSLR\ntechniques and assess their performance across various datasets and sign\nlanguages. The models selected for analysis implement a range of approaches for\nextracting meaningful features and employ distinct training strategies. To\ndetermine their efficacy in modeling different sign languages, these models\nwere evaluated using multiple datasets, specifically RWTH-PHOENIX-Weather-2014,\nArabSign, and GrSL, each representing a unique sign language. The performance\nof the models was further tested with unseen signers and sentences. The\nconducted experiments establish new benchmarks on the selected datasets and\nprovide valuable insights into the robustness and generalization of the\nevaluated techniques under challenging scenarios.", "published": "2024-06-18 07:51:44", "link": "http://arxiv.org/abs/2406.12369v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "PDSS: A Privacy-Preserving Framework for Step-by-Step Distillation of\n  Large Language Models", "abstract": "In the context of real-world applications, leveraging large language models\n(LLMs) for domain-specific tasks often faces two major challenges:\ndomain-specific knowledge privacy and constrained resources. To address these\nissues, we propose PDSS, a privacy-preserving framework for step-by-step\ndistillation of LLMs. PDSS works on a server-client architecture, wherein\nclient transmits perturbed prompts to the server's LLM for rationale\ngeneration. The generated rationales are then decoded by the client and used to\nenrich the training of task-specific small language model(SLM) within a\nmulti-task learning paradigm. PDSS introduces two privacy protection\nstrategies: the Exponential Mechanism Strategy and the Encoder-Decoder\nStrategy, balancing prompt privacy and rationale usability. Experiments\ndemonstrate the effectiveness of PDSS in various text generation tasks,\nenabling the training of task-specific SLM with enhanced performance while\nprioritizing data privacy protection.", "published": "2024-06-18 08:48:14", "link": "http://arxiv.org/abs/2406.12403v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Under-Alignment: Atomic Preference Enhanced Factuality Tuning for\n  Large Language Models", "abstract": "Large language models (LLMs) have achieved remarkable success but still tend\nto generate factually erroneous responses, a phenomenon known as hallucination.\nA recent trend is to use preference learning to fine-tune models to align with\nfactuality. However, existing work primarily evaluates fine-tuned models on\nin-domain (ID) datasets and the factuality on out-of-domain (OOD) datasets\nremains underexplored. In this paper, we conduct a comprehensive evaluation of\nthe factuality of different models tuned by various preference learning\nalgorithms and demonstrate that their performance on OOD datasets either\nincreases minimally or decreases. Subsequently, we reveal that the main cause\nof model's failure to uphold factuality under a distribution shift is\n\\textbf{under-alignment}, rather than \\textbf{over-alignment}, by analyzing the\ntoken distribution shift of the models before and after tuning. Finally, we\npropose \\textbf{APEFT} (\\textbf{A}tomic \\textbf{P}reference \\textbf{E}nhanced\n\\textbf{F}actuality \\textbf{T}uning), a framework that enhances model's\nawareness of factuality at the granularity of individual facts. Extensive\nexperiments demonstrate that APEFT improves model performance by an average of\n$\\boldsymbol{3.45\\%}$ on both ID and OOD datasets, which is highly effective.", "published": "2024-06-18 09:07:30", "link": "http://arxiv.org/abs/2406.12416v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MMUTF: Multimodal Multimedia Event Argument Extraction with Unified\n  Template Filling", "abstract": "With the advancement of multimedia technologies, news documents and\nuser-generated content are often represented as multiple modalities, making\nMultimedia Event Extraction (MEE) an increasingly important challenge. However,\nrecent MEE methods employ weak alignment strategies and data augmentation with\nsimple classification models, which ignore the capabilities of natural\nlanguage-formulated event templates for the challenging Event Argument\nExtraction (EAE) task. In this work, we focus on EAE and address this issue by\nintroducing a unified template filling model that connects the textual and\nvisual modalities via textual prompts. This approach enables the exploitation\nof cross-ontology transfer and the incorporation of event-specific semantics.\nExperiments on the M2E2 benchmark demonstrate the effectiveness of our\napproach. Our system surpasses the current SOTA on textual EAE by +7% F1, and\nperforms generally better than the second-best systems for multimedia EAE.", "published": "2024-06-18 09:14:17", "link": "http://arxiv.org/abs/2406.12420v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Abstraction-of-Thought Makes Language Models Better Reasoners", "abstract": "Abstract reasoning, the ability to reason from the abstract essence of a\nproblem, serves as a key to generalization in human reasoning. However,\neliciting language models to perform reasoning with abstraction remains\nunexplored. This paper seeks to bridge this gap by introducing a novel\nstructured reasoning format called Abstraction-of-Thought (AoT). The uniqueness\nof AoT lies in its explicit requirement for varying levels of abstraction\nwithin the reasoning process. This approach could elicit language models to\nfirst contemplate on the abstract level before incorporating concrete details,\nwhich is overlooked by the prevailing step-by-step Chain-of-Thought (CoT)\nmethod. To align models with the AoT format, we present AoT Collection, a\ngeneric finetuning dataset consisting of 348k high-quality samples with AoT\nreasoning processes, collected via an automated and scalable pipeline. We\nfinetune a wide range of language models with AoT Collection and conduct\nextensive evaluations on 23 unseen tasks from the challenging benchmark\nBig-Bench Hard. Experimental results indicate that models aligned to AoT\nreasoning format substantially outperform those aligned to CoT in many\nreasoning tasks.", "published": "2024-06-18 09:46:44", "link": "http://arxiv.org/abs/2406.12442v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adaptive Token Biaser: Knowledge Editing via Biasing Key Entities", "abstract": "The parametric knowledge memorized by large language models (LLMs) becomes\noutdated quickly. In-context editing (ICE) is currently the most effective\nmethod for updating the knowledge of LLMs. Recent advancements involve\nenhancing ICE by modifying the decoding strategy, obviating the need for\naltering internal model structures or adjusting external prompts. However, this\nenhancement operates across the entire sequence generation, encompassing a\nplethora of non-critical tokens. In this work, we introduce $\\textbf{A}$daptive\n$\\textbf{T}$oken $\\textbf{Bias}$er ($\\textbf{ATBias}$), a new decoding\ntechnique designed to enhance ICE. It focuses on the tokens that are mostly\nrelated to knowledge during decoding, biasing their logits by matching key\nentities related to new and parametric knowledge. Experimental results show\nthat ATBias significantly enhances ICE performance, achieving up to a 32.3%\nimprovement over state-of-the-art ICE methods while incurring only half the\nlatency. ATBias not only improves the knowledge editing capabilities of ICE but\ncan also be widely applied to LLMs with negligible cost.", "published": "2024-06-18 10:18:06", "link": "http://arxiv.org/abs/2406.12468v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Intra and Inter-language Consistency in Embeddings with ICA", "abstract": "Word embeddings represent words as multidimensional real vectors,\nfacilitating data analysis and processing, but are often challenging to\ninterpret. Independent Component Analysis (ICA) creates clearer semantic axes\nby identifying independent key features. Previous research has shown ICA's\npotential to reveal universal semantic axes across languages. However, it\nlacked verification of the consistency of independent components within and\nacross languages. We investigated the consistency of semantic axes in two ways:\nboth within a single language and across multiple languages. We first probed\ninto intra-language consistency, focusing on the reproducibility of axes by\nperforming ICA multiple times and clustering the outcomes. Then, we\nstatistically examined inter-language consistency by verifying those axes'\ncorrespondences using statistical tests. We newly applied statistical methods\nto establish a robust framework that ensures the reliability and universality\nof semantic axes.", "published": "2024-06-18 10:24:50", "link": "http://arxiv.org/abs/2406.12474v1", "categories": ["cs.CL", "stat.ME"], "primary_category": "cs.CL"}
{"title": "The Power of LLM-Generated Synthetic Data for Stance Detection in Online\n  Political Discussions", "abstract": "Stance detection holds great potential to improve online political\ndiscussions through its deployment in discussion platforms for purposes such as\ncontent moderation, topic summarization or to facilitate more balanced\ndiscussions. Typically, transformer-based models are employed directly for\nstance detection, requiring vast amounts of data. However, the wide variety of\ndebate topics in online political discussions makes data collection\nparticularly challenging. LLMs have revived stance detection, but their online\ndeployment in online political discussions faces challenges like inconsistent\noutputs, biases, and vulnerability to adversarial attacks. We show how\nLLM-generated synthetic data can improve stance detection for online political\ndiscussions by using reliable traditional stance detection models for online\ndeployment, while leveraging the text generation capabilities of LLMs for\nsynthetic data generation in a secure offline environment. To achieve this, (i)\nwe generate synthetic data for specific debate questions by prompting a\nMistral-7B model and show that fine-tuning with the generated synthetic data\ncan substantially improve the performance of stance detection, while remaining\ninterpretable and aligned with real world data. (ii) Using the synthetic data\nas a reference, we can improve performance even further by identifying the most\ninformative samples in an unlabelled dataset, i.e., those samples which the\nstance detection model is most uncertain about and can benefit from the most.\nBy fine-tuning with both synthetic data and the most informative samples, we\nsurpass the performance of the baseline model that is fine-tuned on all true\nlabels, while labelling considerably less data.", "published": "2024-06-18 10:36:21", "link": "http://arxiv.org/abs/2406.12480v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MultiSocial: Multilingual Benchmark of Machine-Generated Text Detection\n  of Social-Media Texts", "abstract": "Recent LLMs are able to generate high-quality multilingual texts,\nindistinguishable for humans from authentic human-written ones. Research in\nmachine-generated text detection is however mostly focused on the English\nlanguage and longer texts, such as news articles, scientific papers or student\nessays. Social-media texts are usually much shorter and often feature informal\nlanguage, grammatical errors, or distinct linguistic items (e.g., emoticons,\nhashtags). There is a gap in studying the ability of existing methods in\ndetection of such texts, reflected also in the lack of existing multilingual\nbenchmark datasets. To fill this gap we propose the first multilingual (22\nlanguages) and multi-platform (5 social media platforms) dataset for\nbenchmarking machine-generated text detection in the social-media domain,\ncalled MultiSocial. It contains 472,097 texts, of which about 58k are\nhuman-written and approximately the same amount is generated by each of 7\nmultilingual LLMs. We use this benchmark to compare existing detection methods\nin zero-shot as well as fine-tuned form. Our results indicate that the\nfine-tuned detectors have no problem to be trained on social-media texts and\nthat the platform selection for training matters.", "published": "2024-06-18 12:26:09", "link": "http://arxiv.org/abs/2406.12549v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Low-Resource Machine Translation through the Lens of Personalized\n  Federated Learning", "abstract": "We present a new approach called MeritOpt based on the Personalized Federated\nLearning algorithm MeritFed that can be applied to Natural Language Tasks with\nheterogeneous data. We evaluate it on the Low-Resource Machine Translation\ntask, using the datasets of South East Asian and Finno-Ugric languages. In\naddition to its effectiveness, MeritOpt is also highly interpretable, as it can\nbe applied to track the impact of each language used for training. Our analysis\nreveals that target dataset size affects weight distribution across auxiliary\nlanguages, that unrelated languages do not interfere with the training, and\nauxiliary optimizer parameters have minimal impact. Our approach is easy to\napply with a few lines of code, and we provide scripts for reproducing the\nexperiments at https://github.com/VityaVitalich/MeritOpt.", "published": "2024-06-18 12:50:00", "link": "http://arxiv.org/abs/2406.12564v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Breaking the Ceiling of the LLM Community by Treating Token Generation\n  as a Classification for Ensembling", "abstract": "Ensembling multiple models has always been an effective approach to push the\nlimits of existing performance and is widely used in classification tasks by\nsimply averaging the classification probability vectors from multiple\nclassifiers to achieve better accuracy. However, in the thriving open-source\nLarge Language Model (LLM) community, ensembling methods are rare and typically\nlimited to ensembling the full-text outputs of LLMs, such as selecting the best\noutput using a ranker, which leads to underutilization of token-level\nprobability information. In this paper, we treat the Generation of each token\nby LLMs as a Classification (GaC) for ensembling. This approach fully exploits\nthe probability information at each generation step and better prevents LLMs\nfrom producing early incorrect tokens that lead to snowballing errors. In\nexperiments, we ensemble state-of-the-art LLMs on several benchmarks, including\nexams, mathematics and reasoning, and observe that our method breaks the\nexisting community performance ceiling. Furthermore, we observed that most of\nthe tokens in the answer are simple and do not affect the correctness of the\nfinal answer. Therefore, we also experimented with ensembling only key tokens,\nand the results showed better performance with lower latency across benchmarks.", "published": "2024-06-18 13:17:26", "link": "http://arxiv.org/abs/2406.12585v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bridging Local Details and Global Context in Text-Attributed Graphs", "abstract": "Representation learning on text-attributed graphs (TAGs) is vital for\nreal-world applications, as they combine semantic textual and contextual\nstructural information. Research in this field generally consist of two main\nperspectives: local-level encoding and global-level aggregating, respectively\nrefer to textual node information unification (e.g., using Language Models) and\nstructure-augmented modeling (e.g., using Graph Neural Networks). Most existing\nworks focus on combining different information levels but overlook the\ninterconnections, i.e., the contextual textual information among nodes, which\nprovides semantic insights to bridge local and global levels. In this paper, we\npropose GraphBridge, a multi-granularity integration framework that bridges\nlocal and global perspectives by leveraging contextual textual information,\nenhancing fine-grained understanding of TAGs. Besides, to tackle scalability\nand efficiency challenges, we introduce a graphaware token reduction module.\nExtensive experiments across various models and datasets show that our method\nachieves state-of-theart performance, while our graph-aware token reduction\nmodule significantly enhances efficiency and solves scalability issues.", "published": "2024-06-18 13:35:25", "link": "http://arxiv.org/abs/2406.12608v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EUvsDisinfo: A Dataset for Multilingual Detection of Pro-Kremlin\n  Disinformation in News Articles", "abstract": "This work introduces EUvsDisinfo, a multilingual dataset of disinformation\narticles originating from pro-Kremlin outlets, along with trustworthy articles\nfrom credible / less biased sources. It is sourced directly from the debunk\narticles written by experts leading the EUvsDisinfo project. Our dataset is the\nlargest to-date resource in terms of the overall number of articles and\ndistinct languages. It also provides the largest topical and temporal coverage.\nUsing this dataset, we investigate the dissemination of pro-Kremlin\ndisinformation across different languages, uncovering language-specific\npatterns targeting certain disinformation topics. We further analyse the\nevolution of topic distribution over an eight-year period, noting a significant\nsurge in disinformation content before the full-scale invasion of Ukraine in\n2022. Lastly, we demonstrate the dataset's applicability in training models to\neffectively distinguish between disinformation and trustworthy content in\nmultilingual settings.", "published": "2024-06-18 13:43:22", "link": "http://arxiv.org/abs/2406.12614v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in\n  LLMs-as-Judges", "abstract": "Offering a promising solution to the scalability challenges associated with\nhuman evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an\napproach to evaluating large language models (LLMs). However, there are still\nmany open questions about the strengths and weaknesses of this paradigm, and\nwhat potential biases it may hold. In this paper, we present a comprehensive\nstudy of the performance of various LLMs acting as judges, focusing on a clean\nscenario in which inter-human agreement is high. Investigating thirteen judge\nmodels of different model sizes and families, judging answers of nine different\n'examtaker models' - both base and instruction-tuned - we find that only the\nbest (and largest) models achieve reasonable alignment with humans. However,\nthey are still quite far behind inter-human agreement and their assigned scores\nmay still differ with up to 5 points from human-assigned scores. In terms of\ntheir ranking of the nine exam-taker models, instead, also smaller models and\neven the lexical metric contains may provide a reasonable signal. Through error\nanalysis and other studies, we identify vulnerabilities in judge models, such\nas their sensitivity to prompt complexity and length, and a tendency toward\nleniency. The fact that even the best judges differ from humans in this\ncomparatively simple setup suggest that caution may be wise when using judges\nin more complex setups. Lastly, our research rediscovers the importance of\nusing alignment metrics beyond simple percent alignment, showing that judges\nwith high percent agreement can still assign vastly different scores.", "published": "2024-06-18 13:49:54", "link": "http://arxiv.org/abs/2406.12624v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ask-before-Plan: Proactive Language Agents for Real-World Planning", "abstract": "The evolution of large language models (LLMs) has enhanced the planning\ncapabilities of language agents in diverse real-world scenarios. Despite these\nadvancements, the potential of LLM-powered agents to comprehend ambiguous user\ninstructions for reasoning and decision-making is still under exploration. In\nthis work, we introduce a new task, Proactive Agent Planning, which requires\nlanguage agents to predict clarification needs based on user-agent conversation\nand agent-environment interaction, invoke external tools to collect valid\ninformation, and generate a plan to fulfill the user's demands. To study this\npractical problem, we establish a new benchmark dataset, Ask-before-Plan. To\ntackle the deficiency of LLMs in proactive planning, we propose a novel\nmulti-agent framework, Clarification-Execution-Planning (\\texttt{CEP}), which\nconsists of three agents specialized in clarification, execution, and planning.\nWe introduce the trajectory tuning scheme for the clarification agent and\nstatic execution agent, as well as the memory recollection mechanism for the\ndynamic execution agent. Extensive evaluations and comprehensive analyses\nconducted on the Ask-before-Plan dataset validate the effectiveness of our\nproposed framework.", "published": "2024-06-18 14:07:28", "link": "http://arxiv.org/abs/2406.12639v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for\n  Large Language Models Aligned with Human Cognitive Principles", "abstract": "Assessing the effectiveness of large language models (LLMs) in performing\ndifferent tasks is crucial for understanding their strengths and weaknesses.\nThis paper presents Hierarchical Prompting Taxonomy (HPT), grounded on human\ncognitive principles and designed to assess LLMs by examining the cognitive\ndemands of various tasks. The HPT utilizes the Hierarchical Prompting Framework\n(HPF), which structures five unique prompting strategies in a hierarchical\norder based on their cognitive requirement on LLMs when compared to human\nmental capabilities. It assesses the complexity of tasks with the Hierarchical\nPrompting Index (HPI), which demonstrates the cognitive competencies of LLMs\nacross diverse datasets and offers insights into the cognitive demands that\ndatasets place on different LLMs. This approach enables a comprehensive\nevaluation of an LLMs problem solving abilities and the intricacy of a dataset,\noffering a standardized metric for task complexity. Extensive experiments with\nmultiple datasets and LLMs show that HPF enhances LLM performance by 2% to 63%\ncompared to baseline performance, with GSM8k being the most cognitively complex\ntask among reasoning and coding tasks with an average HPI of 3.20 confirming\nthe effectiveness of HPT. To support future research and reproducibility in\nthis domain, the implementations of HPT and HPF are available here.", "published": "2024-06-18 14:12:27", "link": "http://arxiv.org/abs/2406.12644v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Evidence Attribution in Generated Fact Checking Explanations", "abstract": "Automated fact-checking systems often struggle with trustworthiness, as their\ngenerated explanations can include hallucinations. In this work, we explore\nevidence attribution for fact-checking explanation generation. We introduce a\nnovel evaluation protocol -- citation masking and recovery -- to assess\nattribution quality in generated explanations. We implement our protocol using\nboth human annotators and automatic annotators, and find that LLM annotation\ncorrelates with human annotation, suggesting that attribution assessment can be\nautomated. Finally, our experiments reveal that: (1) the best-performing LLMs\nstill generate explanations with inaccurate attributions; and (2) human-curated\nevidence is essential for generating better explanations. Code and data are\navailable here: https://github.com/ruixing76/Transparent-FCExp.", "published": "2024-06-18 14:13:13", "link": "http://arxiv.org/abs/2406.12645v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CollabStory: Multi-LLM Collaborative Story Generation and Authorship\n  Analysis", "abstract": "The rise of unifying frameworks that enable seamless interoperability of\nLarge Language Models (LLMs) has made LLM-LLM collaboration for open-ended\ntasks a possibility. Despite this, there have not been efforts to explore such\ncollaborative writing. We take the next step beyond human-LLM collaboration to\nexplore this multi-LLM scenario by generating the first exclusively\nLLM-generated collaborative stories dataset called CollabStory. We focus on\nsingle-author to multi-author (up to 5 LLMs) scenarios, where multiple LLMs\nco-author stories. We generate over 32k stories using open-source\ninstruction-tuned LLMs. Further, we take inspiration from the PAN tasks that\nhave set the standard for human-human multi-author writing tasks and analysis.\nWe extend their authorship-related tasks for multi-LLM settings and present\nbaselines for LLM-LLM collaboration. We find that current baselines are not\nable to handle this emerging scenario. Thus, CollabStory is a resource that\ncould help propel an understanding as well as the development of new techniques\nto discern the use of multiple LLMs. This is crucial to study in the context of\nwriting tasks since LLM-LLM collaboration could potentially overwhelm ongoing\nchallenges related to plagiarism detection, credit assignment, maintaining\nacademic integrity in educational settings, and addressing copyright\ninfringement concerns. We make our dataset and code available at\nhttps://github.com/saranya-venkatraman/CollabStory.", "published": "2024-06-18 14:35:12", "link": "http://arxiv.org/abs/2406.12665v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Robustness of Language Models for Tabular Question Answering", "abstract": "Large Language Models (LLMs), already shown to ace various text comprehension\ntasks have also remarkably been shown to tackle table comprehension tasks\nwithout specific training. While previous research has explored LLM\ncapabilities with tabular dataset tasks, our study assesses the influence of\n\\textit{in-context learning}, \\textit{model scale}, \\textit{instruction\ntuning}, and \\textit{domain biases} on Tabular Question Answering (TQA). We\nevaluate the robustness of LLMs on Wikipedia-based \\textbf{WTQ}, financial\nreport-based \\textbf{TAT-QA}, and scientific claims-based \\textbf{SCITAB}, TQA\ndatasets, focusing on their ability to interpret tabular data under various\naugmentations and perturbations robustly. Our findings indicate that\ninstructions significantly enhance performance, with recent models exhibiting\ngreater robustness over earlier versions. However, data contamination and\npractical reliability issues persist, especially with \\textbf{WTQ}. We\nhighlight the need for improved methodologies, including structure-aware\nself-attention mechanisms and better handling of domain-specific tabular data,\nto develop more reliable LLMs for table comprehension.", "published": "2024-06-18 15:41:15", "link": "http://arxiv.org/abs/2406.12719v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Code Like a Linguist?: A Case Study in Low\n  Resource Sound Law Induction", "abstract": "Historical linguists have long written a kind of incompletely formalized\n''program'' that converts reconstructed words in an ancestor language into\nwords in one of its attested descendants that consist of a series of ordered\nstring rewrite functions (called sound laws). They do this by observing pairs\nof words in the reconstructed language (protoforms) and the descendent language\n(reflexes) and constructing a program that transforms protoforms into reflexes.\nHowever, writing these programs is error-prone and time-consuming. Prior work\nhas successfully scaffolded this process computationally, but fewer researchers\nhave tackled Sound Law Induction (SLI), which we approach in this paper by\ncasting it as Programming by Examples. We propose a language-agnostic solution\nthat utilizes the programming ability of Large Language Models (LLMs) by\ngenerating Python sound law programs from sound change examples. We evaluate\nthe effectiveness of our approach for various LLMs, propose effective methods\nto generate additional language-agnostic synthetic data to fine-tune LLMs for\nSLI, and compare our method with existing automated SLI methods showing that\nwhile LLMs lag behind them they can complement some of their weaknesses.", "published": "2024-06-18 15:46:04", "link": "http://arxiv.org/abs/2406.12725v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Model as a Universal Clinical Multi-task Decoder", "abstract": "The development of effective machine learning methodologies for enhancing the\nefficiency and accuracy of clinical systems is crucial. Despite significant\nresearch efforts, managing a plethora of diversified clinical tasks and\nadapting to emerging new tasks remain significant challenges. This paper\npresents a novel paradigm that employs a pre-trained large language model as a\nuniversal clinical multi-task decoder. This approach leverages the flexibility\nand diversity of language expressions to handle task topic variations and\nassociated arguments. The introduction of a new task simply requires the\naddition of a new instruction template. We validate this framework across\nhundreds of tasks, demonstrating its robustness in facilitating multi-task\npredictions, performing on par with traditional multi-task learning and\nsingle-task learning approaches. Moreover, it shows exceptional adaptability to\nnew tasks, with impressive zero-shot performance in some instances and superior\ndata efficiency in few-shot scenarios. This novel approach offers a unified\nsolution to manage a wide array of new and emerging tasks in clinical\napplications.", "published": "2024-06-18 15:58:36", "link": "http://arxiv.org/abs/2406.12738v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for\n  Superintelligent AI", "abstract": "The evolution of Artificial Intelligence (AI) has been significantly\naccelerated by advancements in Large Language Models (LLMs) and Large\nMultimodal Models (LMMs), gradually showcasing potential cognitive reasoning\nabilities in problem-solving and scientific discovery (i.e., AI4Science) once\nexclusive to human intellect. To comprehensively evaluate current models'\nperformance in cognitive reasoning abilities, we introduce OlympicArena, which\nincludes 11,163 bilingual problems across both text-only and interleaved\ntext-image modalities. These challenges encompass a wide range of disciplines\nspanning seven fields and 62 international Olympic competitions, rigorously\nexamined for data leakage. We argue that the challenges in Olympic competition\nproblems are ideal for evaluating AI's cognitive reasoning due to their\ncomplexity and interdisciplinary nature, which are essential for tackling\ncomplex scientific challenges and facilitating discoveries. Beyond evaluating\nperformance across various disciplines using answer-only criteria, we conduct\ndetailed experiments and analyses from multiple perspectives. We delve into the\nmodels' cognitive reasoning abilities, their performance across different\nmodalities, and their outcomes in process-level evaluations, which are vital\nfor tasks requiring complex reasoning with lengthy solutions. Our extensive\nevaluations reveal that even advanced models like GPT-4o only achieve a 39.97%\noverall accuracy, illustrating current AI limitations in complex reasoning and\nmultimodal integration. Through the OlympicArena, we aim to advance AI towards\nsuperintelligence, equipping it to address more complex challenges in science\nand beyond. We also provide a comprehensive set of resources to support AI\nresearch, including a benchmark dataset, an open-source annotation platform, a\ndetailed evaluation tool, and a leaderboard with automatic submission features.", "published": "2024-06-18 16:20:53", "link": "http://arxiv.org/abs/2406.12753v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chumor 1.0: A Truly Funny and Challenging Chinese Humor Understanding\n  Dataset from Ruo Zhi Ba", "abstract": "Existing humor datasets and evaluations predominantly focus on English,\nlacking resources for culturally nuanced humor in non-English languages like\nChinese. To address this gap, we construct Chumor, a dataset sourced from Ruo\nZhi Ba (RZB), a Chinese Reddit-like platform dedicated to sharing\nintellectually challenging and culturally specific jokes. We annotate\nexplanations for each joke and evaluate human explanations against two\nstate-of-the-art LLMs, GPT-4o and ERNIE Bot, through A/B testing by native\nChinese speakers. Our evaluation shows that Chumor is challenging even for SOTA\nLLMs, and the human explanations for Chumor jokes are significantly better than\nexplanations generated by the LLMs.", "published": "2024-06-18 16:22:05", "link": "http://arxiv.org/abs/2406.12754v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Informatics & dairy industry coalition: AI trends and present challenges", "abstract": "Artificial Intelligence (AI) can potentially transform the industry,\nenhancing the production process and minimizing manual, repetitive tasks.\nAccordingly, the synergy between high-performance computing and powerful\nmathematical models enables the application of sophisticated data analysis\nprocedures like Machine Learning. However, challenges exist regarding\neffective, efficient, and flexible processing to generate valuable knowledge.\nConsequently, this work comprehensively describes industrial challenges where\nAI can be exploited, focusing on the dairy industry. The conclusions presented\ncan help researchers apply novel approaches for cattle monitoring and farmers\nby proposing advanced technological solutions to their needs.", "published": "2024-06-18 16:39:21", "link": "http://arxiv.org/abs/2406.12770v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Generating Educational Materials with Different Levels of Readability\n  using LLMs", "abstract": "This study introduces the leveled-text generation task, aiming to rewrite\neducational materials to specific readability levels while preserving meaning.\nWe assess the capability of GPT-3.5, LLaMA-2 70B, and Mixtral 8x7B, to generate\ncontent at various readability levels through zero-shot and few-shot prompting.\nEvaluating 100 processed educational materials reveals that few-shot prompting\nsignificantly improves performance in readability manipulation and information\npreservation. LLaMA-2 70B performs better in achieving the desired difficulty\nrange, while GPT-3.5 maintains original meaning. However, manual inspection\nhighlights concerns such as misinformation introduction and inconsistent edit\ndistribution. These findings emphasize the need for further research to ensure\nthe quality of generated educational content.", "published": "2024-06-18 16:55:10", "link": "http://arxiv.org/abs/2406.12787v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Is It Good Data for Multilingual Instruction Tuning or Just Bad\n  Multilingual Evaluation for Large Language Models?", "abstract": "Multilingual large language models are designed, claimed, and expected to\ncater to speakers of varied languages. We hypothesise that the current\npractices of fine-tuning and evaluating these models may not perfectly align\nwith this objective owing to a heavy reliance on translation, which cannot\ncover language-specific knowledge but can introduce translation defects. It\nremains unknown whether the nature of the instruction data has an impact on the\nmodel output; conversely, it is questionable whether translated test sets can\ncapture such nuances. Due to the often coupled practices of using translated\ndata in both stages, such imperfections could have been overlooked. This work\ninvestigates these issues using controlled native or translated data during the\ninstruction tuning and evaluation stages. We show that native or generation\nbenchmarks reveal a notable difference between native and translated\ninstruction data especially when model performance is high, whereas other types\nof test sets cannot. The comparison between round-trip and single-pass\ntranslations reflects the importance of knowledge from language-native\nresources. Finally, we demonstrate that regularization is beneficial to\nbridging this gap on structured but not generative tasks.", "published": "2024-06-18 17:43:47", "link": "http://arxiv.org/abs/2406.12822v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From RAGs to rich parameters: Probing how language models utilize\n  external knowledge over parametric information for factual queries", "abstract": "Retrieval Augmented Generation (RAG) enriches the ability of language models\nto reason using external context to augment responses for a given user prompt.\nThis approach has risen in popularity due to practical applications in various\napplications of language models in search, question/answering, and chat-bots.\nHowever, the exact nature of how this approach works isn't clearly understood.\nIn this paper, we mechanistically examine the RAG pipeline to highlight that\nlanguage models take shortcut and have a strong bias towards utilizing only the\ncontext information to answer the question, while relying minimally on their\nparametric memory. We probe this mechanistic behavior in language models with:\n(i) Causal Mediation Analysis to show that the parametric memory is minimally\nutilized when answering a question and (ii) Attention Contributions and\nKnockouts to show that the last token residual stream do not get enriched from\nthe subject token in the question, but gets enriched from other informative\ntokens in the context. We find this pronounced shortcut behaviour true across\nboth LLaMa and Phi family of models.", "published": "2024-06-18 17:46:08", "link": "http://arxiv.org/abs/2406.12824v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interpretable Preferences via Multi-Objective Reward Modeling and\n  Mixture-of-Experts", "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as the primary\nmethod for aligning large language models (LLMs) with human preferences. The\nRLHF process typically starts by training a reward model (RM) using human\npreference data. Conventional RMs are trained on pairwise responses to the same\nuser request, with relative ratings indicating which response humans prefer.\nThe trained RM serves as a proxy for human preferences. However, due to the\nblack-box nature of RMs, their outputs lack interpretability, as humans cannot\nintuitively understand why an RM thinks a response is good or not. As RMs act\nas human preference proxies, we believe they should be human-interpretable to\nensure that their internal decision processes are consistent with human\npreferences and to prevent reward hacking in LLM alignment. To build RMs with\ninterpretable preferences, we propose a two-stage approach: i) train an\nAbsolute-Rating Multi-Objective Reward Model (ArmoRM) with multi-dimensional\nabsolute-rating data, each dimension corresponding to a human-interpretable\nobjective (e.g., honesty, verbosity, safety); ii) employ a Mixture-of-Experts\n(MoE) strategy with a gating network that automatically selects the most\nsuitable reward objectives based on the context. We efficiently trained an\nArmoRM with Llama-3 8B and a gating network consisting of a shallow MLP on top\nof the ArmoRM. Our trained model, ArmoRM-Llama3-8B, obtains state-of-the-art\nperformance on RewardBench, a benchmark evaluating RMs for language modeling.\nNotably, the performance of our model surpasses the LLM-as-a-judge method with\nGPT-4 judges by a margin, and approaches the performance of the much larger\nNemotron-4 340B reward model.", "published": "2024-06-18 17:58:28", "link": "http://arxiv.org/abs/2406.12845v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Detecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM\n  Framework for Detecting Factual Errors", "abstract": "Accurate text summarization is one of the most common and important tasks\nperformed by Large Language Models, where the costs of human review for an\nentire document may be high, but the costs of errors in summarization may be\neven greater. We propose Detecting Errors through Ensembling Prompts (DEEP) -\nan end-to-end large language model framework for detecting factual errors in\ntext summarization. Our framework uses a diverse set of LLM prompts to identify\nfactual inconsistencies, treating their outputs as binary features, which are\nthen fed into ensembling models. We then calibrate the ensembled models to\nproduce empirically accurate probabilities that a text is factually consistent\nor free of hallucination. We demonstrate that prior models for detecting\nfactual errors in summaries perform significantly worse without optimizing the\nthresholds on subsets of the evaluated dataset. Our framework achieves\nstate-of-the-art (SOTA) balanced accuracy on the AggreFact-XSUM FTSOTA,\nTofuEval Summary-Level, and HaluEval Summarization benchmarks in detecting\nfactual errors within transformer-generated text summaries. It does so without\nany fine-tuning of the language model or reliance on thresholding techniques\nnot available in practical settings.", "published": "2024-06-18 18:59:37", "link": "http://arxiv.org/abs/2406.13009v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating $n$-Gram Novelty of Language Models Using Rusty-DAWG", "abstract": "How novel are texts generated by language models (LMs) relative to their\ntraining corpora? In this work, we investigate the extent to which modern LMs\ngenerate $n$-grams from their training data, evaluating both (i) the\nprobability LMs assign to complete training $n$-grams and (ii) $n$-novelty, the\nproportion of $n$-grams generated by an LM that did not appear in the training\ndata (for arbitrarily large $n$). To enable arbitrary-length $n$-gram search\nover a corpus in constant time w.r.t. corpus size, we develop Rusty-DAWG, a\nnovel search tool inspired by indexing of genomic data. We compare the novelty\nof LM-generated text to human-written text and explore factors that affect\ngeneration novelty, focusing on the Pythia models. We find that, for $n > 4$,\nLM-generated text is less novel than human-written text, though it is more\nnovel for smaller $n$. Larger LMs and more constrained decoding strategies both\ndecrease novelty. Finally, we show that LMs complete $n$-grams with lower loss\nif they are more frequent in the training data. Overall, our results reveal\nfactors influencing the novelty of LM-generated text, and we release Rusty-DAWG\nto facilitate further pretraining data research.", "published": "2024-06-18 21:31:19", "link": "http://arxiv.org/abs/2406.13069v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TourLLM: Enhancing LLMs with Tourism Knowledge", "abstract": "Recently, large language models (LLMs) have demonstrated their effectiveness\nin various natural language processing (NLP) tasks. However, the lack of\ntourism knowledge limits the performance of LLMs in tourist attraction\npresentations and travel planning. To address this challenge, we constructed a\nsupervised fine-tuning dataset for the culture and tourism domain, named\nCultour. This dataset consists of three parts: tourism knowledge base QA data,\ntravelogues data, and tourism diversity QA data. Additionally, we propose\nTourLLM, a Qwen-based model supervised fine-tuned with Cultour, to improve the\nquality of the information provided about attractions and travel planning. To\nevaluate the performance of TourLLM, we employed both automatic and human\nevaluation, and we proposed a human evaluation criterion named CRA\n(Consistency, Readability, Availability). The experimental results demonstrate\nthe effectiveness of the responses generated by the TourLLM. Our proposed\nCultour is accessible at https://github.com/mrweiqk/Cultour.", "published": "2024-06-18 09:15:46", "link": "http://arxiv.org/abs/2407.12791v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DART-Math: Difficulty-Aware Rejection Tuning for Mathematical\n  Problem-Solving", "abstract": "Solving mathematical problems requires advanced reasoning abilities and\npresents notable challenges for large language models. Previous works usually\nsynthesize data from proprietary models to augment existing datasets, followed\nby instruction tuning to achieve top-tier results. However, our analysis of\nthese datasets reveals severe biases towards easy queries, with frequent\nfailures to generate any correct response for the most challenging queries.\nHypothesizing that difficult queries are crucial to learn complex reasoning, we\npropose Difficulty-Aware Rejection Tuning (DART), a method that allocates\ndifficult queries more trials during the synthesis phase, enabling more\nextensive training on difficult samples. Utilizing DART, we have created new\ndatasets for mathematical problem-solving that focus more on difficult queries\nand are substantially smaller than previous ones. Remarkably, our synthesis\nprocess solely relies on a 7B-sized open-weight model, without reliance on the\ncommonly used proprietary GPT-4. We fine-tune various base models on our\ndatasets ranging from 7B to 70B in size, resulting in a series of strong models\ncalled DART-MATH. In comprehensive in-domain and out-of-domain evaluation on 6\nmathematical benchmarks, DART-MATH outperforms vanilla rejection tuning\nsignificantly, being superior or comparable to previous arts, despite using\nmuch smaller datasets and no proprietary models. Furthermore, our results\nposition our synthetic datasets as the most effective and cost-efficient\npublicly available resources for advancing mathematical problem-solving.", "published": "2024-06-18 07:14:02", "link": "http://arxiv.org/abs/2407.13690v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BPO: Staying Close to the Behavior LLM Creates Better Online LLM\n  Alignment", "abstract": "Direct alignment from preferences (DAP) has emerged as a promising paradigm\nfor aligning large language models (LLMs) to human desiderata from\npre-collected, offline preference datasets. While recent studies indicate that\nexisting offline DAP methods can directly benefit from online training samples,\nwe highlight the need to develop specific online DAP algorithms to fully\nharness the power of online training. Specifically, we identify that the\nlearned LLM should adhere to the proximity of the behavior LLM, which collects\nthe training samples. To this end, we propose online Preference Optimization in\nproximity to the Behavior LLM (BPO), emphasizing the importance of constructing\na proper trust region for LLM alignment.\n  We conduct extensive experiments to validate the effectiveness and\napplicability of our approach by integrating it with various DAP methods,\nresulting in significant performance improvements across a wide range of tasks\nwhen training with the same amount of preference data. Even when only\nintroducing one additional data collection phase, our online BPO improves its\noffline DAP baseline from 72.0% to 80.2% on TL;DR and from 82.2% to 89.1% on\nAnthropic Helpfulness in terms of win rate against human reference text.", "published": "2024-06-18 00:41:40", "link": "http://arxiv.org/abs/2406.12168v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Knowledge Fusion By Evolving Weights of Language Models", "abstract": "Fine-tuning pre-trained language models, particularly large language models,\ndemands extensive computing resources and can result in varying performance\noutcomes across different domains and datasets. This paper examines the\napproach of integrating multiple models from diverse training scenarios into a\nunified model. This unified model excels across various data domains and\nexhibits the ability to generalize well on out-of-domain data. We propose a\nknowledge fusion method named Evolver, inspired by evolutionary algorithms,\nwhich does not need further training or additional training data. Specifically,\nour method involves aggregating the weights of different language models into a\npopulation and subsequently generating offspring models through mutation and\ncrossover operations. These offspring models are then evaluated against their\nparents, allowing for the preservation of those models that show enhanced\nperformance on development datasets. Importantly, our model evolving strategy\ncan be seamlessly integrated with existing model merging frameworks, offering a\nversatile tool for model enhancement. Experimental results on mainstream\nlanguage models (i.e., encoder-only, decoder-only, encoder-decoder) reveal that\nEvolver outperforms previous state-of-the-art models by large margins. The code\nis publicly available at {https://github.com/duguodong7/model-evolution}.", "published": "2024-06-18 02:12:34", "link": "http://arxiv.org/abs/2406.12208v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Interface Design for Self-Supervised Speech Models", "abstract": "Self-supervised speech (SSL) models have recently become widely adopted for\nmany downstream speech processing tasks. The general usage pattern is to employ\nSSL models as feature extractors, and then train a downstream prediction head\nto solve a specific task. However, different layers of SSL models have been\nshown to capture different types of information, and the methods of combining\nthem are not well studied. To this end, we extend the general framework for SSL\nmodel utilization by proposing the interface that connects the upstream and\ndownstream. Under this view, the dominant technique of combining features via a\nlayerwise weighted sum can be regarded as a specific interface. We propose\nseveral alternative interface designs and demonstrate that the weighted sum\ninterface is suboptimal for many tasks. In particular, we show that a\nconvolutional interface whose depth scales logarithmically with the depth of\nthe upstream model consistently outperforms many other interface designs.", "published": "2024-06-18 02:13:48", "link": "http://arxiv.org/abs/2406.12209v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AI-Oracle Machines for Intelligent Computing", "abstract": "We introduce the concept of AI-oracle machines for intelligent computing and\noutline several applications to demonstrate their potential. Following this, we\nadvocate for the development of a comprehensive platform to streamline the\nimplementation of AI-oracle machines.", "published": "2024-06-18 02:25:33", "link": "http://arxiv.org/abs/2406.12213v4", "categories": ["cs.CL", "cs.AI", "cs.FL", "F.1.1; F.4.1; I.2.0"], "primary_category": "cs.CL"}
{"title": "SyncVSR: Data-Efficient Visual Speech Recognition with End-to-End\n  Crossmodal Audio Token Synchronization", "abstract": "Visual Speech Recognition (VSR) stands at the intersection of computer vision\nand speech recognition, aiming to interpret spoken content from visual cues. A\nprominent challenge in VSR is the presence of homophenes-visually similar lip\ngestures that represent different phonemes. Prior approaches have sought to\ndistinguish fine-grained visemes by aligning visual and auditory semantics, but\noften fell short of full synchronization. To address this, we present SyncVSR,\nan end-to-end learning framework that leverages quantized audio for frame-level\ncrossmodal supervision. By integrating a projection layer that synchronizes\nvisual representation with acoustic data, our encoder learns to generate\ndiscrete audio tokens from a video sequence in a non-autoregressive manner.\nSyncVSR shows versatility across tasks, languages, and modalities at the cost\nof a forward pass. Our empirical evaluations show that it not only achieves\nstate-of-the-art results but also reduces data usage by up to ninefold.", "published": "2024-06-18 03:14:22", "link": "http://arxiv.org/abs/2406.12233v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "TroL: Traversal of Layers for Large Language and Vision Models", "abstract": "Large language and vision models (LLVMs) have been driven by the\ngeneralization power of large language models (LLMs) and the advent of visual\ninstruction tuning. Along with scaling them up directly, these models enable\nLLVMs to showcase powerful vision language (VL) performances by covering\ndiverse tasks via natural language instructions. However, existing open-source\nLLVMs that perform comparably to closed-source LLVMs such as GPT-4V are often\nconsidered too large (e.g., 26B, 34B, and 110B parameters), having a larger\nnumber of layers. These large models demand costly, high-end resources for both\ntraining and inference. To address this issue, we present a new efficient LLVM\nfamily with 1.8B, 3.8B, and 7B LLM model sizes, Traversal of Layers (TroL),\nwhich enables the reuse of layers in a token-wise manner. This layer traversing\ntechnique simulates the effect of looking back and retracing the answering\nstream while increasing the number of forward propagation layers without\nphysically adding more layers. We demonstrate that TroL employs a simple layer\ntraversing approach yet efficiently outperforms the open-source LLVMs with\nlarger model sizes and rivals the performances of the closed-source LLVMs with\nsubstantial sizes.", "published": "2024-06-18 03:42:00", "link": "http://arxiv.org/abs/2406.12246v3", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Mitigate Negative Transfer with Similarity Heuristic Lifelong Prompt\n  Tuning", "abstract": "Lifelong prompt tuning has significantly advanced parameter-efficient\nlifelong learning with its efficiency and minimal storage demands on various\ntasks. Our empirical studies, however, highlights certain transferability\nconstraints in the current methodologies: a universal algorithm that guarantees\nconsistent positive transfer across all tasks is currently unattainable,\nespecially when dealing dissimilar tasks that may engender negative transfer.\nIdentifying the misalignment between algorithm selection and task specificity\nas the primary cause of negative transfer, we present the Similarity Heuristic\nLifelong Prompt Tuning (SHLPT) framework. This innovative strategy partitions\ntasks into two distinct subsets by harnessing a learnable similarity metric,\nthereby facilitating fruitful transfer from tasks regardless of their\nsimilarity or dissimilarity. Additionally, SHLPT incorporates a parameter pool\nto combat catastrophic forgetting effectively. Our experiments shows that SHLPT\noutperforms state-of-the-art techniques in lifelong learning benchmarks and\ndemonstrates robustness against negative transfer in diverse task sequences.", "published": "2024-06-18 03:57:49", "link": "http://arxiv.org/abs/2406.12251v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning", "abstract": "Chain-of-Thought (CoT) holds a significant place in augmenting the reasoning\nperformance for large language models (LLMs). While some studies focus on\nimproving CoT accuracy through methods like retrieval enhancement, yet a\nrigorous explanation for why CoT achieves such success remains unclear. In this\npaper, we analyze CoT methods under two different settings by asking the\nfollowing questions: (1) For zero-shot CoT, why does prompting the model with\n\"let's think step by step\" significantly impact its outputs? (2) For few-shot\nCoT, why does providing examples before questioning the model could\nsubstantially improve its reasoning ability? To answer these questions, we\nconduct a top-down explainable analysis from the Hopfieldian view and propose a\nRead-and-Control approach for controlling the accuracy of CoT. Through\nextensive experiments on seven datasets for three different tasks, we\ndemonstrate that our framework can decipher the inner workings of CoT, provide\nreasoning error localization, and control to come up with the correct reasoning\npath.", "published": "2024-06-18 04:07:13", "link": "http://arxiv.org/abs/2406.12255v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Time-Series Anomaly Detection Using Learnable Data\n  Augmentation", "abstract": "Continuous efforts are being made to advance anomaly detection in various\nmanufacturing processes to increase the productivity and safety of industrial\nsites. Deep learning replaced rule-based methods and recently emerged as a\npromising method for anomaly detection in diverse industries. However, in the\nreal world, the scarcity of abnormal data and difficulties in obtaining labeled\ndata create limitations in the training of detection models. In this study, we\naddressed these shortcomings by proposing a learnable data augmentation-based\ntime-series anomaly detection (LATAD) technique that is trained in a\nself-supervised manner. LATAD extracts discriminative features from time-series\ndata through contrastive learning. At the same time, learnable data\naugmentation produces challenging negative samples to enhance learning\nefficiency. We measured anomaly scores of the proposed technique based on\nlatent feature similarities. As per the results, LATAD exhibited comparable or\nimproved performance to the state-of-the-art anomaly detection assessments on\nseveral benchmark datasets and provided a gradient-based diagnosis technique to\nhelp identify root causes.", "published": "2024-06-18 04:25:56", "link": "http://arxiv.org/abs/2406.12260v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CodeNav: Beyond tool-use to using real-world codebases with LLM agents", "abstract": "We present CodeNav, an LLM agent that navigates and leverages previously\nunseen code repositories to solve user queries. In contrast to tool-use LLM\nagents that require ``registration'' of all relevant tools via manual\ndescriptions within the LLM context, CodeNav automatically indexes and searches\nover code blocks in the target codebase, finds relevant code snippets, imports\nthem, and uses them to iteratively generate a solution with execution feedback.\nTo highlight the core-capabilities of CodeNav, we first showcase three case\nstudies where we use CodeNav for solving complex user queries using three\ndiverse codebases. Next, on three benchmarks, we quantitatively compare the\neffectiveness of code-use (which only has access to the target codebase) to\ntool-use (which has privileged access to all tool names and descriptions).\nFinally, we study the effect of varying kinds of tool and library descriptions\non code-use performance, as well as investigate the advantage of the agent\nseeing source code as opposed to natural descriptions of code. All code will be\nmade open source under a permissive license.", "published": "2024-06-18 05:10:38", "link": "http://arxiv.org/abs/2406.12276v1", "categories": ["cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.AI"}
{"title": "Automatic benchmarking of large multimodal models via iterative\n  experiment programming", "abstract": "Assessing the capabilities of large multimodal models (LMMs) often requires\nthe creation of ad-hoc evaluations. Currently, building new benchmarks requires\ntremendous amounts of manual work for each specific analysis. This makes the\nevaluation process tedious and costly. In this paper, we present APEx,\nAutomatic Programming of Experiments, the first framework for automatic\nbenchmarking of LMMs. Given a research question expressed in natural language,\nAPEx leverages a large language model (LLM) and a library of pre-specified\ntools to generate a set of experiments for the model at hand, and progressively\ncompile a scientific report. The report drives the testing procedure: based on\nthe current status of the investigation, APEx chooses which experiments to\nperform and whether the results are sufficient to draw conclusions. Finally,\nthe LLM refines the report, presenting the results to the user in natural\nlanguage. Thanks to its modularity, our framework is flexible and extensible as\nnew tools become available. Empirically, APEx reproduces the findings of\nexisting studies while allowing for arbitrary analyses and hypothesis testing.", "published": "2024-06-18 06:43:46", "link": "http://arxiv.org/abs/2406.12321v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Towards Understanding Domain Adapted Sentence Embeddings for Document\n  Retrieval", "abstract": "A plethora of sentence embedding models makes it challenging to choose one,\nespecially for technical domains rich with specialized vocabulary. In this\nwork, we domain adapt embeddings using telecom, health and science datasets for\nquestion answering. We evaluate embeddings obtained from publicly available\nmodels and their domain-adapted variants, on both point retrieval accuracies,\nas well as their (95\\%) confidence intervals. We establish a systematic method\nto obtain thresholds for similarity scores for different embeddings. As\nexpected, we observe that fine-tuning improves mean bootstrapped accuracies. We\nalso observe that it results in tighter confidence intervals, which further\nimprove when pre-training is preceded by fine-tuning. We introduce metrics\nwhich measure the distributional overlaps of top-$K$, correct and random\ndocument similarities with the question. Further, we show that these metrics\nare correlated with retrieval accuracy and similarity thresholds. Recent\nliterature shows conflicting effects of isotropy on retrieval accuracies. Our\nexperiments establish that the isotropy of embeddings (as measured by two\nindependent state-of-the-art isotropy metric definitions) is poorly correlated\nwith retrieval performance. We show that embeddings for domain-specific\nsentences have little overlap with those for domain-agnostic ones, and\nfine-tuning moves them further apart. Based on our results, we provide\nrecommendations for use of our methodology and metrics by researchers and\npractitioners.", "published": "2024-06-18 07:03:34", "link": "http://arxiv.org/abs/2406.12336v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "WebCanvas: Benchmarking Web Agents in Online Environments", "abstract": "For web agents to be practically useful, they must adapt to the continuously\nevolving web environment characterized by frequent updates to user interfaces\nand content. However, most existing benchmarks only capture the static aspects\nof the web. To bridge this gap, we introduce WebCanvas, an innovative online\nevaluation framework for web agents that effectively addresses the dynamic\nnature of web interactions. WebCanvas contains three main components to\nfacilitate realistic assessments: (1) A novel evaluation metric which reliably\ncapture critical intermediate actions or states necessary for task completions\nwhile disregarding noise caused by insignificant events or changed\nweb-elements. (2) A benchmark dataset called Mind2Web-Live, a refined version\nof original Mind2Web static dataset containing 542 tasks with 2439 intermediate\nevaluation states; (3) Lightweight and generalizable annotation tools and\ntesting pipelines that enables the community to collect and maintain the\nhigh-quality, up-to-date dataset. Building on WebCanvas, we open-source an\nagent framework with extensible modules for reasoning, providing a foundation\nfor the community to conduct online inference and evaluations. Our\nbest-performing agent achieves a task success rate of 23.1% and a task\ncompletion rate of 48.8% on the Mind2Web-Live test set. Additionally, we\nanalyze the performance discrepancies across various websites, domains, and\nexperimental environments. We encourage the community to contribute further\ninsights on online agent evaluation, thereby advancing this field of research.", "published": "2024-06-18 07:58:33", "link": "http://arxiv.org/abs/2406.12373v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "QOG:Question and Options Generation based on Language Model", "abstract": "Question-Options Generation (QOG) is a task that involves generating a set of\nquestion-options pairs given context. This task has various applications,\nincluding fine-tuning large models, information retrieval, and automated\nmultiple-choice question generation for education. In this paper, we develop\nQOG models using three different methods based on fine-tuning\nsequence-to-sequence language models (LMs). Experiments demonstrate that the\nend-to-end QOG model is computationally efficient and stable during both\ntraining and inference, outperforming other methods. Furthermore, our analysis\nindicates that our QOG models are competitive on the QOG task compared to the\nlarge language model Llama 3-8B.", "published": "2024-06-18 08:09:58", "link": "http://arxiv.org/abs/2406.12381v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Performant ASR Models for Medical Entities in Accented Speech", "abstract": "Recent strides in automatic speech recognition (ASR) have accelerated their\napplication in the medical domain where their performance on accented medical\nnamed entities (NE) such as drug names, diagnoses, and lab results, is largely\nunknown. We rigorously evaluate multiple ASR models on a clinical English\ndataset of 93 African accents. Our analysis reveals that despite some models\nachieving low overall word error rates (WER), errors in clinical entities are\nhigher, potentially posing substantial risks to patient safety. To empirically\ndemonstrate this, we extract clinical entities from transcripts, develop a\nnovel algorithm to align ASR predictions with these entities, and compute\nmedical NE Recall, medical WER, and character error rate. Our results show that\nfine-tuning on accented clinical speech improves medical WER by a wide margin\n(25-34 % relative), improving their practical applicability in healthcare\nenvironments.", "published": "2024-06-18 08:19:48", "link": "http://arxiv.org/abs/2406.12387v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "QueerBench: Quantifying Discrimination in Language Models Toward Queer\n  Identities", "abstract": "With the increasing role of Natural Language Processing (NLP) in various\napplications, challenges concerning bias and stereotype perpetuation are\naccentuated, which often leads to hate speech and harm. Despite existing\nstudies on sexism and misogyny, issues like homophobia and transphobia remain\nunderexplored and often adopt binary perspectives, putting the safety of\nLGBTQIA+ individuals at high risk in online spaces. In this paper, we assess\nthe potential harm caused by sentence completions generated by English large\nlanguage models (LLMs) concerning LGBTQIA+ individuals. This is achieved using\nQueerBench, our new assessment framework, which employs a template-based\napproach and a Masked Language Modeling (MLM) task. The analysis indicates that\nlarge language models tend to exhibit discriminatory behaviour more frequently\ntowards individuals within the LGBTQIA+ community, reaching a difference gap of\n7.2% in the QueerBench score of harmfulness.", "published": "2024-06-18 08:40:29", "link": "http://arxiv.org/abs/2406.12399v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large\n  Language Models as Decision Makers", "abstract": "In this paper, we conduct a study to utilize LLMs as a solution for decision\nmaking that requires complex data analysis. We define Decision QA as the task\nof answering the best decision, $d_{best}$, for a decision-making question $Q$,\nbusiness rules $R$ and a database $D$. Since there is no benchmark that can\nexamine Decision QA, we propose Decision QA benchmark, DQA. It has two\nscenarios, Locating and Building, constructed from two video games (Europa\nUniversalis IV and Victoria 3) that have almost the same goal as Decision QA.\nTo address Decision QA effectively, we also propose a new RAG technique called\nthe iterative plan-then-retrieval augmented generation (PlanRAG). Our\nPlanRAG-based LM generates the plan for decision making as the first step, and\nthe retriever generates the queries for data analysis as the second step. The\nproposed method outperforms the state-of-the-art iterative RAG method by 15.8%\nin the Locating scenario and by 7.4% in the Building scenario, respectively. We\nrelease our code and benchmark at https://github.com/myeon9h/PlanRAG.", "published": "2024-06-18 09:25:35", "link": "http://arxiv.org/abs/2406.12430v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large\n  Language Models", "abstract": "We introduce Mathador-LM, a new benchmark for evaluating the mathematical\nreasoning on large language models (LLMs), combining ruleset interpretation,\nplanning, and problem-solving. This benchmark is inspired by the Mathador game,\nwhere the objective is to reach a target number using basic arithmetic\noperations on a given set of base numbers, following a simple set of rules. We\nshow that, across leading LLMs, we obtain stable average performance while\ngenerating benchmark instances \\emph{dynamically}, following a target\ndifficulty level. Thus, our benchmark alleviates concerns about test-set\nleakage into training data, an issue that often undermines popular benchmarks.\nAdditionally, we conduct a comprehensive evaluation of both open and\nclosed-source state-of-the-art LLMs on Mathador-LM. Our findings reveal that\ncontemporary models struggle with Mathador-LM, scoring significantly lower than\naverage 3rd graders. This stands in stark contrast to their strong performance\non popular mathematical reasoning benchmarks. The implementation of Mathador-LM\nbenchmark is available at\n\\href{https://github.com/IST-DASLab/Mathador-LM}{github.com/IST-DASLab/Mathador-LM}.", "published": "2024-06-18 13:02:12", "link": "http://arxiv.org/abs/2406.12572v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval", "abstract": "Differentiable Search Index (DSI) utilizes Pre-trained Language Models (PLMs)\nfor efficient document retrieval without relying on external indexes. However,\nDSI needs full re-training to handle updates in dynamic corpora, causing\nsignificant computational inefficiencies. We introduce PromptDSI, a\nprompt-based rehearsal-free approach for instance-wise incremental learning\ndocument retrieval. PromptDSI attaches prompts to the frozen PLM's encoder of\nDSI, leveraging its powerful representation to efficiently index new corpora\nwhile maintaining a balance between stability and plasticity. We eliminate the\ninitial forward pass of prompt-based continual learning methods that doubles\ntraining and inference time. Moreover, we propose a topic-aware prompt pool\nthat employs neural topic embeddings as fixed keys. This strategy ensures\ndiverse and effective prompt usage, addressing the challenge of parameter\nunderutilization caused by the collapse of the query-key matching mechanism.\nOur empirical evaluations demonstrate that BERT-based PromptDSI matches IncDSI\nin managing forgetting while improving new corpora performance by more than 4%\nHits@10 on NQ320k and upto 3% MRR@10 on MS MARCO 300k.", "published": "2024-06-18 13:25:18", "link": "http://arxiv.org/abs/2406.12593v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Rapid Language Adaptation for Multilingual E2E Speech Recognition Using\n  Encoder Prompting", "abstract": "End-to-end multilingual speech recognition models handle multiple languages\nthrough a single model, often incorporating language identification to\nautomatically detect the language of incoming speech. Since the common scenario\nis where the language is already known, these models can perform as\nlanguage-specific by using language information as prompts, which is\nparticularly beneficial for attention-based encoder-decoder architectures.\nHowever, the Connectionist Temporal Classification (CTC) approach, which\nenhances recognition via joint decoding and multi-task training, does not\nnormally incorporate language prompts due to its conditionally independent\noutput tokens. To overcome this, we introduce an encoder prompting technique\nwithin the self-conditioned CTC framework, enabling language-specific\nadaptation of the CTC model in a zero-shot manner. Our method has shown to\nsignificantly reduce errors by 28% on average and by 41% on low-resource\nlanguages.", "published": "2024-06-18 13:38:58", "link": "http://arxiv.org/abs/2406.12611v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SeTAR: Out-of-Distribution Detection with Selective Low-Rank\n  Approximation", "abstract": "Out-of-distribution (OOD) detection is crucial for the safe deployment of\nneural networks. Existing CLIP-based approaches perform OOD detection by\ndevising novel scoring functions or sophisticated fine-tuning methods. In this\nwork, we propose SeTAR, a novel, training-free OOD detection method that\nleverages selective low-rank approximation of weight matrices in\nvision-language and vision-only models. SeTAR enhances OOD detection via\npost-hoc modification of the model's weight matrices using a simple greedy\nsearch algorithm. Based on SeTAR, we further propose SeTAR+FT, a fine-tuning\nextension optimizing model performance for OOD detection tasks. Extensive\nevaluations on ImageNet1K and Pascal-VOC benchmarks show SeTAR's superior\nperformance, reducing the relatively false positive rate by up to 18.95% and\n36.80% compared to zero-shot and fine-tuning baselines. Ablation studies\nfurther validate SeTAR's effectiveness, robustness, and generalizability across\ndifferent model backbones. Our work offers a scalable, efficient solution for\nOOD detection, setting a new state-of-the-art in this area.", "published": "2024-06-18 13:55:13", "link": "http://arxiv.org/abs/2406.12629v4", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Transforming Surgical Interventions with Embodied Intelligence for\n  Ultrasound Robotics", "abstract": "Ultrasonography has revolutionized non-invasive diagnostic methodologies,\nsignificantly enhancing patient outcomes across various medical domains.\nDespite its advancements, integrating ultrasound technology with robotic\nsystems for automated scans presents challenges, including limited command\nunderstanding and dynamic execution capabilities. To address these challenges,\nthis paper introduces a novel Ultrasound Embodied Intelligence system that\nsynergistically combines ultrasound robots with large language models (LLMs)\nand domain-specific knowledge augmentation, enhancing ultrasound robots'\nintelligence and operational efficiency. Our approach employs a dual strategy:\nfirstly, integrating LLMs with ultrasound robots to interpret doctors' verbal\ninstructions into precise motion planning through a comprehensive understanding\nof ultrasound domain knowledge, including APIs and operational manuals;\nsecondly, incorporating a dynamic execution mechanism, allowing for real-time\nadjustments to scanning plans based on patient movements or procedural errors.\nWe demonstrate the effectiveness of our system through extensive experiments,\nincluding ablation studies and comparisons across various models, showcasing\nsignificant improvements in executing medical procedures from verbal commands.\nOur findings suggest that the proposed system improves the efficiency and\nquality of ultrasound scans and paves the way for further advancements in\nautonomous medical scanning technologies, with the potential to transform\nnon-invasive diagnostics and streamline medical workflows.", "published": "2024-06-18 14:22:16", "link": "http://arxiv.org/abs/2406.12651v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.RO"}
{"title": "MAGIC: Generating Self-Correction Guideline for In-Context Text-to-SQL", "abstract": "Self-correction in text-to-SQL is the process of prompting large language\nmodel (LLM) to revise its previously incorrectly generated SQL, and commonly\nrelies on manually crafted self-correction guidelines by human experts that are\nnot only labor-intensive to produce but also limited by the human ability in\nidentifying all potential error patterns in LLM responses. We introduce MAGIC,\na novel multi-agent method that automates the creation of the self-correction\nguideline. MAGIC uses three specialized agents: a manager, a correction, and a\nfeedback agent. These agents collaborate on the failures of an LLM-based method\non the training set to iteratively generate and refine a self-correction\nguideline tailored to LLM mistakes, mirroring human processes but without human\ninvolvement. Our extensive experiments show that MAGIC's guideline outperforms\nexpert human's created ones. We empirically find out that the guideline\nproduced by MAGIC enhances the interpretability of the corrections made,\nproviding insights in analyzing the reason behind the failures and successes of\nLLMs in self-correction. All agent interactions are publicly available at\nhttps://huggingface.co/datasets/microsoft/MAGIC.", "published": "2024-06-18 15:06:06", "link": "http://arxiv.org/abs/2406.12692v3", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Talk With Human-like Agents: Empathetic Dialogue Through Perceptible\n  Acoustic Reception and Reaction", "abstract": "Large Language Model (LLM)-enhanced agents become increasingly prevalent in\nHuman-AI communication, offering vast potential from entertainment to\nprofessional domains. However, current multi-modal dialogue systems overlook\nthe acoustic information present in speech, which is crucial for understanding\nhuman communication nuances. This oversight can lead to misinterpretations of\nspeakers' intentions, resulting in inconsistent or even contradictory responses\nwithin dialogues. To bridge this gap, in this paper, we propose\nPerceptiveAgent, an empathetic multi-modal dialogue system designed to discern\ndeeper or more subtle meanings beyond the literal interpretations of words\nthrough the integration of speech modality perception. Employing LLMs as a\ncognitive core, PerceptiveAgent perceives acoustic information from input\nspeech and generates empathetic responses based on speaking styles described in\nnatural language. Experimental results indicate that PerceptiveAgent excels in\ncontextual understanding by accurately discerning the speakers' true intentions\nin scenarios where the linguistic meaning is either contrary to or inconsistent\nwith the speaker's true feelings, producing more nuanced and expressive spoken\ndialogues. Code is publicly available at:\n\\url{https://github.com/Haoqiu-Yan/PerceptiveAgent}.", "published": "2024-06-18 15:19:51", "link": "http://arxiv.org/abs/2406.12707v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Mitigating Object Hallucinations in Large Vision-Language Models with\n  Assembly of Global and Local Attention", "abstract": "Despite great success across various multimodal tasks, Large Vision-Language\nModels (LVLMs) often encounter object hallucinations with generated textual\nresponses being inconsistent with the actual objects in images. We examine\ndifferent LVLMs and pinpoint that one root cause of object hallucinations lies\nwith deficient attention on discriminative image features. Specifically, LVLMs\noften predominantly attend to prompt-irrelevant global features instead of\nprompt-relevant local features, undermining their visual grounding capacity and\nleading to object hallucinations. We propose Assembly of Global and Local\nAttention (AGLA), a training-free and plug-and-play approach that mitigates\nhallucinations by assembling global features for response generation and local\nfeatures for visual discrimination simultaneously. Specifically, we introduce\nan image-prompt matching scheme that captures prompt-relevant local features\nfrom images, leading to an augmented view of the input image where\nprompt-relevant content is highlighted while irrelevant distractions are\nsuppressed. Hallucinations can thus be mitigated with a calibrated logit\ndistribution that is from generative global features of the original image and\ndiscriminative local features of the augmented image. Extensive experiments\nshow the superiority of AGLA in LVLM hallucination mitigation, demonstrating\nits wide applicability across both discriminative and generative tasks. Our\ncode is available at https://github.com/Lackel/AGLA.", "published": "2024-06-18 15:38:41", "link": "http://arxiv.org/abs/2406.12718v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Benchmarking Multi-Image Understanding in Vision and Language Models:\n  Perception, Knowledge, Reasoning, and Multi-Hop Reasoning", "abstract": "The advancement of large language models (LLMs) has significantly broadened\nthe scope of applications in natural language processing, with multi-modal LLMs\nextending these capabilities to integrate and interpret visual data. However,\nexisting benchmarks for visual language models (VLMs) predominantly focus on\nsingle-image inputs, neglecting the crucial aspect of multi-image\nunderstanding. In this paper, we introduce a Multi-Image Relational Benchmark\nMIRB, designed to evaluate VLMs' ability to compare, analyze, and reason across\nmultiple images. Our benchmark encompasses four categories: perception, visual\nworld knowledge, reasoning, and multi-hop reasoning. Through a comprehensive\nevaluation of a wide range of open-source and closed-source models, we\ndemonstrate that while open-source VLMs were shown to approach the performance\nof GPT-4V in single-image tasks, a significant performance gap remains in\nmulti-image reasoning tasks. Our findings also reveal that even the\nstate-of-the-art GPT-4V model struggles with our benchmark, underscoring the\nneed for further research and development in this area. We believe our\ncontribution of MIRB could serve as a testbed for developing the\nnext-generation multi-modal models.", "published": "2024-06-18 16:02:18", "link": "http://arxiv.org/abs/2406.12742v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Dissecting Adversarial Robustness of Multimodal LM Agents", "abstract": "As language models (LMs) are used to build autonomous agents in real\nenvironments, ensuring their adversarial robustness becomes a critical\nchallenge. Unlike chatbots, agents are compound systems with multiple\ncomponents taking actions, which existing LMs safety evaluations do not\nadequately address. To bridge this gap, we manually create 200 targeted\nadversarial tasks and evaluation scripts in a realistic threat model on top of\nVisualWebArena, a real environment for web agents. To systematically examine\nthe robustness of agents, we propose the Agent Robustness Evaluation (ARE)\nframework. ARE views the agent as a graph showing the flow of intermediate\noutputs between components and decomposes robustness as the flow of adversarial\ninformation on the graph. We find that we can successfully break latest agents\nthat use black-box frontier LMs, including those that perform reflection and\ntree search. With imperceptible perturbations to a single image (less than 5%\nof total web page pixels), an attacker can hijack these agents to execute\ntargeted adversarial goals with success rates up to 67%. We also use ARE to\nrigorously evaluate how the robustness changes as new components are added. We\nfind that inference-time compute that typically improves benign performance can\nopen up new vulnerabilities and harm robustness. An attacker can compromise the\nevaluator used by the reflexion agent and the value function of the tree search\nagent, which increases the attack success relatively by 15% and 20%. Our data\nand code for attacks, defenses, and evaluation are at\nhttps://github.com/ChenWu98/agent-attack", "published": "2024-06-18 17:32:48", "link": "http://arxiv.org/abs/2406.12814v3", "categories": ["cs.LG", "cs.CL", "cs.CR", "cs.CV"], "primary_category": "cs.LG"}
{"title": "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional\n  Adaptation", "abstract": "Low-rank adaptation (LoRA) has become the default approach to fine-tune large\nlanguage models (LLMs) due to its significant reduction in trainable\nparameters. However, trainable parameter demand for LoRA increases with\nincreasing model embedding dimensions, leading to high compute costs.\nAdditionally, its backward updates require storing high-dimensional\nintermediate activations and optimizer states, demanding high peak GPU memory.\nIn this paper, we introduce large model fine-tuning via spectrally decomposed\nlow-dimensional adaptation (LaMDA), a novel approach to fine-tuning large\nlanguage models, which leverages low-dimensional adaptation to achieve\nsignificant reductions in trainable parameters and peak GPU memory footprint.\nLaMDA freezes a first projection matrix (PMA) in the adaptation path while\nintroducing a low-dimensional trainable square matrix, resulting in substantial\nreductions in trainable parameters and peak GPU memory usage. LaMDA gradually\nfreezes a second projection matrix (PMB) during the early fine-tuning stages,\nreducing the compute cost associated with weight updates to enhance parameter\nefficiency further. We also present an enhancement, LaMDA++, incorporating a\n``lite-weight\" adaptive rank allocation for the LoRA path via normalized\nspectrum analysis of pre-trained model weights. We evaluate LaMDA/LaMDA++\nacross various tasks, including natural language understanding with the GLUE\nbenchmark, text summarization, natural language generation, and complex\nreasoning on different LLMs. Results show that LaMDA matches or surpasses the\nperformance of existing alternatives while requiring up to 17.7x fewer\nparameter updates and up to 1.32x lower peak GPU memory usage during\nfine-tuning. Code will be publicly available.", "published": "2024-06-18 17:52:59", "link": "http://arxiv.org/abs/2406.12832v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Instruction Data Generation and Unsupervised Adaptation for Speech\n  Language Models", "abstract": "In this paper, we propose three methods for generating synthetic samples to\ntrain and evaluate multimodal large language models capable of processing both\ntext and speech inputs. Addressing the scarcity of samples containing both\nmodalities, synthetic data generation emerges as a crucial strategy to enhance\nthe performance of such systems and facilitate the modeling of cross-modal\nrelationships between the speech and text domains. Our process employs large\nlanguage models to generate textual components and text-to-speech systems to\ngenerate speech components. The proposed methods offer a practical and\neffective means to expand the training dataset for these models. Experimental\nresults show progress in achieving an integrated understanding of text and\nspeech. We also highlight the potential of using unlabeled speech data to\ngenerate synthetic samples comparable in quality to those with available\ntranscriptions, enabling the expansion of these models to more languages.", "published": "2024-06-18 08:27:00", "link": "http://arxiv.org/abs/2406.12946v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "SHIELD: Evaluation and Defense Strategies for Copyright Compliance in\n  LLM Text Generation", "abstract": "Large Language Models (LLMs) have transformed machine learning but raised\nsignificant legal concerns due to their potential to produce text that\ninfringes on copyrights, resulting in several high-profile lawsuits. The legal\nlandscape is struggling to keep pace with these rapid advancements, with\nongoing debates about whether generated text might plagiarize copyrighted\nmaterials. Current LLMs may infringe on copyrights or overly restrict\nnon-copyrighted texts, leading to these challenges: (i) the need for a\ncomprehensive evaluation benchmark to assess copyright compliance from multiple\naspects; (ii) evaluating robustness against safeguard bypassing attacks; and\n(iii) developing effective defense targeted against the generation of\ncopyrighted text. To tackle these challenges, we introduce a curated dataset to\nevaluate methods, test attack strategies, and propose lightweight, real-time\ndefense to prevent the generation of copyrighted text, ensuring the safe and\nlawful use of LLMs. Our experiments demonstrate that current LLMs frequently\noutput copyrighted text, and that jailbreaking attacks can significantly\nincrease the volume of copyrighted output. Our proposed defense mechanism\nsignificantly reduces the volume of copyrighted text generated by LLMs by\neffectively refusing malicious requests. Code is publicly available at\nhttps://github.com/xz-liu/SHIELD", "published": "2024-06-18 18:00:03", "link": "http://arxiv.org/abs/2406.12975v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Coding Speech through Vocal Tract Kinematics", "abstract": "Vocal tract articulation is a natural, grounded control space of speech\nproduction. The spatiotemporal coordination of articulators combined with the\nvocal source shapes intelligible speech sounds to enable effective spoken\ncommunication. Based on this physiological grounding of speech, we propose a\nnew framework of neural encoding-decoding of speech -- Speech Articulatory\nCoding (SPARC). SPARC comprises an articulatory analysis model that infers\narticulatory features from speech audio, and an articulatory synthesis model\nthat synthesizes speech audio from articulatory features. The articulatory\nfeatures are kinematic traces of vocal tract articulators and source features,\nwhich are intuitively interpretable and controllable, being the actual physical\ninterface of speech production. An additional speaker identity encoder is\njointly trained with the articulatory synthesizer to inform the voice texture\nof individual speakers. By training on large-scale speech data, we achieve a\nfully intelligible, high-quality articulatory synthesizer that generalizes to\nunseen speakers. Furthermore, the speaker embedding is effectively disentangled\nfrom articulations, which enables accent-perserving zero-shot voice conversion.\nTo the best of our knowledge, this is the first demonstration of universal,\nhigh-performance articulatory inference and synthesis, suggesting the proposed\nframework as a powerful coding system of speech.", "published": "2024-06-18 18:38:17", "link": "http://arxiv.org/abs/2406.12998v4", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring and Benchmarking the Planning Capabilities of Large Language\n  Models", "abstract": "Classical and natural language planning tasks remain a difficult domain for\nmodern large language models (LLMs). In this work, we lay the foundations for\nimproving planning capabilities of LLMs. First, we construct a comprehensive\nbenchmark suite encompassing both classical planning benchmarks and natural\nlanguage scenarios. This suite includes algorithms to methodically generate\ninstances of tasks with varying levels of difficulty, allowing for rigorous and\nsystematic evaluation of LLM performance. Next, we investigate the use of\nmany-shot in-context learning to enhance LLM planning, exploring the\nrelationship between increased context length and improved planning\nperformance. In addition, we demonstrate the positive impact of fine-tuning\nLLMs on optimal planning paths. We also probe the efficacy of chain-of-thought\nreasoning methods to improve LLM planning performance. Moreover, we probe the\nperformance of the proposed methods in out-of-distribution scenarios, assessing\nthe ability to generalize to novel and unseen planning challenges. Finally, we\ninvestigate model's failure modes and reveal insights that hold true across\ndifferent benchmarks.", "published": "2024-06-18 22:57:06", "link": "http://arxiv.org/abs/2406.13094v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Accelerating Complex Disease Treatment through Network Medicine and\n  GenAI: A Case Study on Drug Repurposing for Breast Cancer", "abstract": "The objective of this research is to introduce a network specialized in\npredicting drugs that can be repurposed by investigating real-world evidence\nsources, such as clinical trials and biomedical literature. Specifically, it\naims to generate drug combination therapies for complex diseases (e.g., cancer,\nAlzheimer's). We present a multilayered network medicine approach, empowered by\na highly configured ChatGPT prompt engineering system, which is constructed on\nthe fly to extract drug mentions in clinical trials. Additionally, we introduce\na novel algorithm that connects real-world evidence with disease-specific\nsignaling pathways (e.g., KEGG database). This sheds light on the\nrepurposability of drugs if they are found to bind with one or more protein\nconstituents of a signaling pathway. To demonstrate, we instantiated the\nframework for breast cancer and found that, out of 46 breast cancer signaling\npathways, the framework identified 38 pathways that were covered by at least\ntwo drugs. This evidence signals the potential for combining those drugs.\nSpecifically, the most covered signaling pathway, ID hsa:2064, was covered by\n108 drugs, some of which can be combined. Conversely, the signaling pathway ID\nhsa:1499 was covered by only two drugs, indicating a significant gap for\nfurther research. Our network medicine framework, empowered by GenAI, shows\npromise in identifying drug combinations with a high degree of specificity,\nknowing the exact signaling pathways and proteins that serve as targets. It is\nnoteworthy that ChatGPT successfully accelerated the process of identifying\ndrug mentions in clinical trials, though further investigations are required to\ndetermine the relationships among the drug mentions.", "published": "2024-06-18 23:40:00", "link": "http://arxiv.org/abs/2406.13106v3", "categories": ["cs.AI", "cs.CL", "cs.IR", "I.2; I.2.6"], "primary_category": "cs.AI"}
{"title": "Improving Text-To-Audio Models with Synthetic Captions", "abstract": "It is an open challenge to obtain high quality training data, especially\ncaptions, for text-to-audio models. Although prior methods have leveraged\n\\textit{text-only language models} to augment and improve captions, such\nmethods have limitations related to scale and coherence between audio and\ncaptions. In this work, we propose an audio captioning pipeline that uses an\n\\textit{audio language model} to synthesize accurate and diverse captions for\naudio at scale. We leverage this pipeline to produce a dataset of synthetic\ncaptions for AudioSet, named \\texttt{AF-AudioSet}, and then evaluate the\nbenefit of pre-training text-to-audio models on these synthetic captions.\nThrough systematic evaluations on AudioCaps and MusicCaps, we find leveraging\nour pipeline and synthetic captions leads to significant improvements on audio\ngeneration quality, achieving a new \\textit{state-of-the-art}.", "published": "2024-06-18 00:02:15", "link": "http://arxiv.org/abs/2406.15487v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Causal Discovery Inspired Unsupervised Domain Adaptation for\n  Emotion-Cause Pair Extraction", "abstract": "This paper tackles the task of emotion-cause pair extraction in the\nunsupervised domain adaptation setting. The problem is challenging as the\ndistributions of the events causing emotions in target domains are dramatically\ndifferent than those in source domains, despite the distributions of emotional\nexpressions between domains are overlapped. Inspired by causal discovery, we\npropose a novel deep latent model in the variational autoencoder (VAE)\nframework, which not only captures the underlying latent structures of data but\nalso utilizes the easily transferable knowledge of emotions as the bridge to\nlink the distributions of events in different domains. To facilitate knowledge\ntransfer across domains, we also propose a novel variational posterior\nregularization technique to disentangle the latent representations of emotions\nfrom those of events in order to mitigate the damage caused by the spurious\ncorrelations related to the events in source domains. Through extensive\nexperiments, we demonstrate that our model outperforms the strongest baseline\nby approximately 11.05\\% on a Chinese benchmark and 2.45\\% on a English\nbenchmark in terms of weighted-average F1 score. We have released our source\ncode and the generated dataset publicly at:\nhttps://github.com/tk1363704/CAREL-VAE.", "published": "2024-06-18 13:01:30", "link": "http://arxiv.org/abs/2406.15490v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.4"], "primary_category": "cs.CL"}
{"title": "Saliency Attention and Semantic Similarity-Driven Adversarial\n  Perturbation", "abstract": "In this paper, we introduce an enhanced textual adversarial attack method,\nknown as Saliency Attention and Semantic Similarity driven adversarial\nPerturbation (SASSP). The proposed scheme is designed to improve the\neffectiveness of contextual perturbations by integrating saliency, attention,\nand semantic similarity. Traditional adversarial attack methods often struggle\nto maintain semantic consistency and coherence while effectively deceiving\ntarget models. Our proposed approach addresses these challenges by\nincorporating a three-pronged strategy for word selection and perturbation.\nFirst, we utilize a saliency-based word selection to prioritize words for\nmodification based on their importance to the model's prediction. Second,\nattention mechanisms are employed to focus perturbations on contextually\nsignificant words, enhancing the attack's efficacy. Finally, an advanced\nsemantic similarity-checking method is employed that includes embedding-based\nsimilarity and paraphrase detection. By leveraging models like Sentence-BERT\nfor embedding similarity and fine-tuned paraphrase detection models from the\nSentence Transformers library, the scheme ensures that the perturbed text\nremains contextually appropriate and semantically consistent with the original.\nEmpirical evaluations demonstrate that SASSP generates adversarial examples\nthat not only maintain high semantic fidelity but also effectively deceive\nstate-of-the-art natural language processing models. Moreover, in comparison to\nthe original scheme of contextual perturbation CLARE, SASSP has yielded a\nhigher attack success rate and lower word perturbation rate.", "published": "2024-06-18 14:07:27", "link": "http://arxiv.org/abs/2406.19413v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Nash CoT: Multi-Path Inference with Preference Equilibrium", "abstract": "Chain of thought (CoT) is a reasoning framework that can enhance the\nperformance of Large Language Models (LLMs) on complex inference tasks. In\nparticular, among various studies related to CoT, multi-path inference stands\nout as a simple yet effective improvement. However, there is no optimal setting\nfor the number of inference paths. Therefore, we have to increase the number of\ninference paths to obtain better results, which in turn increases the inference\ncost. To address this limitation, we can utilize question-related role\ntemplates to guide LLMs into relevant roles, thereby increasing the possibility\nof correct inferences for each path and further reducing dependence on the\nnumber of inference paths while improving reasoning accuracy. However, placing\nLLMs into specific roles may reduce their reasoning diversity and performance\non a few tasks where role dependence is low. To alleviate the excessive\nimmersion of the LLM into a specific role, we propose Nash CoT by constructing\na game system on each path that balances the generation from role-specific\nLLMs' and the general LLMs' generation, thereby ensuring both effective role\nadoption and diversity in LLM generation further maintaining the performance of\nmulti-path inference while reducing the requirement of the number of inference\npaths. We evaluate Nash CoT across various inference tasks, including Arabic\nReasoning, Commonsense Question Answering, and Symbolic Inference, achieving\nresults that are comparable to or better than those of multi-path CoT with the\nequal number of inference paths.", "published": "2024-06-18 07:46:13", "link": "http://arxiv.org/abs/2407.07099v3", "categories": ["cs.CL", "cs.AI", "cs.GT", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Free to play: UN Trade and Development's experience with developing its\n  own open-source Retrieval Augmented Generation Large Language Model\n  application", "abstract": "Generative artificial intelligence (AI), and in particular Large Language\nModels (LLMs), have exploded in popularity and attention since the release to\nthe public of ChatGPT's Generative Pre-trained Transformer (GPT)-3.5 model in\nNovember of 2022. Due to the power of these general purpose models and their\nability to communicate in natural language, they can be useful in a range of\ndomains, including the work of official statistics and international\norganizations. However, with such a novel and seemingly complex technology, it\ncan feel as if generative AI is something that happens to an organization,\nsomething that can be talked about but not understood, that can be commented on\nbut not contributed to. Additionally, the costs of adoption and operation of\nproprietary solutions can be both uncertain and high, a barrier for often\ncost-constrained international organizations. In the face of these challenges,\nUnited Nations Trade and Development (UNCTAD), through its Global Crisis\nResponse Group (GCRG), has explored and developed its own open-source Retrieval\nAugmented Generation (RAG) LLM application. RAG makes LLMs aware of and more\nuseful for the organization's domain and work. Developing in-house solutions\ncomes with pros and cons, with pros including cost, flexibility, and fostering\ninstitutional knowledge. Cons include time and skill investments and gaps and\napplication polish and power. The three libraries developed to produce the app,\nnlp_pipeline for document processing and statistical analysis, local_rag_llm\nfor running a local RAG LLM, and streamlit_rag for the user interface, are\npublicly available on PyPI and GitHub with Dockerfiles. A fourth library,\nlocal_llm_finetune, is also available for fine-tuning existing LLMs which can\nthen be used in the application.", "published": "2024-06-18 14:23:54", "link": "http://arxiv.org/abs/2407.16896v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG", "I.2"], "primary_category": "cs.CY"}
{"title": "PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency\n  Spoken Dialogue Systems", "abstract": "Multimodal language models that process both text and speech have a potential\nfor applications in spoken dialogue systems. However, current models face two\nmajor challenges in response generation latency: (1) generating a spoken\nresponse requires the prior generation of a written response, and (2) speech\nsequences are significantly longer than text sequences. This study addresses\nthese issues by extending the input and output sequences of the language model\nto support the parallel generation of text and speech. Our experiments on\nspoken question answering tasks demonstrate that our approach improves latency\nwhile maintaining the quality of response content. Additionally, we show that\nlatency can be further reduced by generating speech in multiple sequences. Demo\nsamples are available at https://rinnakk.github.io/research/publications/PSLM.", "published": "2024-06-18 09:23:54", "link": "http://arxiv.org/abs/2406.12428v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular\n  Property Prediction", "abstract": "Molecular property prediction (MPP) is a fundamental and crucial task in drug\ndiscovery. However, prior methods are limited by the requirement for a large\nnumber of labeled molecules and their restricted ability to generalize for\nunseen and new tasks, both of which are essential for real-world applications.\nTo address these challenges, we present MolecularGPT for few-shot MPP. From a\nperspective on instruction tuning, we fine-tune large language models (LLMs)\nbased on curated molecular instructions spanning over 1000 property prediction\ntasks. This enables building a versatile and specialized LLM that can be\nadapted to novel MPP tasks without any fine-tuning through zero- and few-shot\nin-context learning (ICL). MolecularGPT exhibits competitive in-context\nreasoning capabilities across 10 downstream evaluation datasets, setting new\nbenchmarks for few-shot molecular prediction tasks. More importantly, with just\ntwo-shot examples, MolecularGPT can outperform standard supervised graph neural\nnetwork methods on 4 out of 7 datasets. It also excels state-of-the-art LLM\nbaselines by up to 15.7% increase on classification accuracy and decrease of\n17.9 on regression metrics (e.g., RMSE) under zero-shot. This study\ndemonstrates the potential of LLMs as effective few-shot molecular property\npredictors. The code is available at https://github.com/NYUSHCS/MolecularGPT.", "published": "2024-06-18 12:54:47", "link": "http://arxiv.org/abs/2406.12950v2", "categories": ["q-bio.QM", "cs.AI", "cs.CE", "cs.CL", "cs.LG"], "primary_category": "q-bio.QM"}
{"title": "Text-aware Speech Separation for Multi-talker Keyword Spotting", "abstract": "For noisy environments, ensuring the robustness of keyword spotting (KWS)\nsystems is essential. While much research has focused on noisy KWS, less\nattention has been paid to multi-talker mixed speech scenarios. Unlike the\nusual cocktail party problem where multi-talker speech is separated using\nspeaker clues, the key challenge here is to extract the target speech for KWS\nbased on text clues. To address it, this paper proposes a novel Text-aware\nPermutation Determinization Training method for multi-talker KWS with a\nclue-based Speech Separation front-end (TPDT-SS). Our research highlights the\ncritical role of SS front-ends and shows that incorporating keyword-specific\nclues into these models can greatly enhance the effectiveness. TPDT-SS shows\nremarkable success in addressing permutation problems in mixed keyword speech,\nthereby greatly boosting the performance of the backend. Additionally,\nfine-tuning our system on unseen mixed speech results in further performance\nimprovement.", "published": "2024-06-18 09:48:59", "link": "http://arxiv.org/abs/2406.12447v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Unsupervised Online Continual Learning for Automatic Speech Recognition", "abstract": "Adapting Automatic Speech Recognition (ASR) models to new domains leads to\nCatastrophic Forgetting (CF) of previously learned information. This paper\naddresses CF in the challenging context of Online Continual Learning (OCL),\nwith tasks presented as a continuous data stream with unknown boundaries. We\nextend OCL for ASR into the unsupervised realm, by leveraging self-training\n(ST) to facilitate unsupervised adaptation, enabling models to adapt\ncontinually without label dependency and without forgetting previous knowledge.\nThrough comparative analysis of various OCL and ST methods across two domain\nadaptation experiments, we show that UOCL suffers from significantly less\nforgetting compared to supervised OCL, allowing UOCL methods to approach the\nperformance levels of supervised OCL. Our proposed UOCL extensions further\nboosts UOCL's efficacy. Our findings represent a significant step towards\ncontinually adaptable ASR systems, capable of leveraging unlabeled data across\ndiverse domains.", "published": "2024-06-18 11:12:10", "link": "http://arxiv.org/abs/2406.12503v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Challenging margin-based speaker embedding extractors by using the\n  variational information bottleneck", "abstract": "Speaker embedding extractors are typically trained using a classification\nloss over the training speakers. During the last few years, the standard\nsoftmax/cross-entropy loss has been replaced by the margin-based losses,\nyielding significant improvements in speaker recognition accuracy. Motivated by\nthe fact that the margin merely reduces the logit of the target speaker during\ntraining, we consider a probabilistic framework that has a similar effect. The\nvariational information bottleneck provides a principled mechanism for making\ndeterministic nodes stochastic, resulting in an implicit reduction of the\nposterior of the target speaker. We experiment with a wide range of speaker\nrecognition benchmarks and scoring methods and report competitive results to\nthose obtained with the state-of-the-art Additive Angular Margin loss.", "published": "2024-06-18 13:47:08", "link": "http://arxiv.org/abs/2406.12622v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Transcribe, Align and Segment: Creating speech datasets for low-resource\n  languages", "abstract": "In this work, we showcase a cost-effective method for generating training\ndata for speech processing tasks. First, we transcribe unlabeled speech using a\nstate-of-the-art Automatic Speech Recognition (ASR) model. Next, we align\ngenerated transcripts with the audio and apply segmentation on short\nutterances. Our focus is on ASR for low-resource languages, such as Ukrainian,\nusing podcasts as a source of unlabeled speech.\n  We release a new dataset UK-PODS that features modern conversational\nUkrainian language. It contains over 50 hours of text audio-pairs as well as\nuk-pods-conformer, a 121 M parameters ASR model that is trained on MCV-10 and\nUK-PODS and achieves 3x reduction of Word Error Rate (WER) on podcasts\ncomparing to publically available uk-nvidia-citrinet while maintaining\ncomparable WER on MCV-10 test split. Both dataset UK-PODS\nhttps://huggingface.co/datasets/taras-sereda/uk-pods and ASR uk-pods-conformer\nhttps://huggingface.co/taras-sereda/uk-pods-conformer are available on the\nhugging-face hub.", "published": "2024-06-18 14:47:22", "link": "http://arxiv.org/abs/2406.12674v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Universal Score-based Speech Enhancement with High Content Preservation", "abstract": "We propose UNIVERSE++, a universal speech enhancement method based on\nscore-based diffusion and adversarial training. Specifically, we improve the\nexisting UNIVERSE model that decouples clean speech feature extraction and\ndiffusion. Our contributions are three-fold. First, we make several\nmodifications to the network architecture, improving training stability and\nfinal performance. Second, we introduce an adversarial loss to promote learning\nhigh quality speech features. Third, we propose a low-rank adaptation scheme\nwith a phoneme fidelity loss to improve content preservation in the enhanced\nspeech. In the experiments, we train a universal enhancement model on a large\nscale dataset of speech degraded by noise, reverberation, and various\ndistortions. The results on multiple public benchmark datasets demonstrate that\nUNIVERSE++ compares favorably to both discriminative and generative baselines\nfor a wide range of qualitative and intelligibility metrics.", "published": "2024-06-18 01:49:00", "link": "http://arxiv.org/abs/2406.12194v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speak in the Scene: Diffusion-based Acoustic Scene Transfer toward\n  Immersive Speech Generation", "abstract": "This paper introduces a novel task in generative speech processing, Acoustic\nScene Transfer (AST), which aims to transfer acoustic scenes of speech signals\nto diverse environments. AST promises an immersive experience in speech\nperception by adapting the acoustic scene behind speech signals to desired\nenvironments. We propose AST-LDM for the AST task, which generates speech\nsignals accompanied by the target acoustic scene of the reference prompt.\nSpecifically, AST-LDM is a latent diffusion model conditioned by CLAP\nembeddings that describe target acoustic scenes in either audio or text\nmodalities. The contributions of this paper include introducing the AST task\nand implementing its baseline model. For AST-LDM, we emphasize its core\nframework, which is to preserve the input speech and generate audio\nconsistently with both the given speech and the target acoustic environment.\nExperiments, including objective and subjective tests, validate the feasibility\nand efficacy of our approach.", "published": "2024-06-18 15:00:25", "link": "http://arxiv.org/abs/2406.12688v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A Mel Spectrogram Enhancement Paradigm Based on CWT in Speech Synthesis", "abstract": "Acoustic features play an important role in improving the quality of the\nsynthesised speech. Currently, the Mel spectrogram is a widely employed\nacoustic feature in most acoustic models. However, due to the fine-grained loss\ncaused by its Fourier transform process, the clarity of speech synthesised by\nMel spectrogram is compromised in mutant signals. In order to obtain a more\ndetailed Mel spectrogram, we propose a Mel spectrogram enhancement paradigm\nbased on the continuous wavelet transform (CWT). This paradigm introduces an\nadditional task: a more detailed wavelet spectrogram, which like the\npost-processing network takes as input the Mel spectrogram output by the\ndecoder. We choose Tacotron2 and Fastspeech2 for experimental validation in\norder to test autoregressive (AR) and non-autoregressive (NAR) speech systems,\nrespectively. The experimental results demonstrate that the speech synthesised\nusing the model with the Mel spectrogram enhancement paradigm exhibits higher\nMOS, with an improvement of 0.14 and 0.09 compared to the baseline model,\nrespectively. These findings provide some validation for the universality of\nthe enhancement paradigm, as they demonstrate the success of the paradigm in\ndifferent architectures.", "published": "2024-06-18 00:34:44", "link": "http://arxiv.org/abs/2406.12164v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Binaural Selective Attention Model for Target Speaker Extraction", "abstract": "The remarkable ability of humans to selectively focus on a target speaker in\ncocktail party scenarios is facilitated by binaural audio processing. In this\npaper, we present a binaural time-domain Target Speaker Extraction model based\non the Filter-and-Sum Network (FaSNet). Inspired by human selective hearing,\nour proposed model introduces target speaker embedding into separators using a\nmulti-head attention-based selective attention block. We also compared two\nbinaural interaction approaches -- the cosine similarity of time-domain signals\nand inter-channel correlation in learned spectral representations. Our\nexperimental results show that our proposed model outperforms monaural\nconfigurations and state-of-the-art multi-channel target speaker extraction\nmodels, achieving best-in-class performance with 18.52 dB SI-SDR, 19.12 dB SDR,\nand 3.05 PESQ scores under anechoic two-speaker test configurations.", "published": "2024-06-18 03:24:52", "link": "http://arxiv.org/abs/2406.12236v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "JEN-1 DreamStyler: Customized Musical Concept Learning via Pivotal\n  Parameters Tuning", "abstract": "Large models for text-to-music generation have achieved significant progress,\nfacilitating the creation of high-quality and varied musical compositions from\nprovided text prompts. However, input text prompts may not precisely capture\nuser requirements, particularly when the objective is to generate music that\nembodies a specific concept derived from a designated reference collection. In\nthis paper, we propose a novel method for customized text-to-music generation,\nwhich can capture the concept from a two-minute reference music and generate a\nnew piece of music conforming to the concept. We achieve this by fine-tuning a\npretrained text-to-music model using the reference music. However, directly\nfine-tuning all parameters leads to overfitting issues. To address this\nproblem, we propose a Pivotal Parameters Tuning method that enables the model\nto assimilate the new concept while preserving its original generative\ncapabilities. Additionally, we identify a potential concept conflict when\nintroducing multiple concepts into the pretrained model. We present a concept\nenhancement strategy to distinguish multiple concepts, enabling the fine-tuned\nmodel to generate music incorporating either individual or multiple concepts\nsimultaneously. Since we are the first to work on the customized music\ngeneration task, we also introduce a new dataset and evaluation protocol for\nthe new task. Our proposed Jen1-DreamStyler outperforms several baselines in\nboth qualitative and quantitative evaluations. Demos will be available at\nhttps://www.jenmusic.ai/research#DreamStyler.", "published": "2024-06-18 05:54:11", "link": "http://arxiv.org/abs/2406.12292v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MEMS and ECM Sensor Technologies for Cardiorespiratory Sound Monitoring\n  - A Comprehensive Review", "abstract": "This paper presents a comprehensive review of cardiorespiratory auscultation\nsensing devices (i.e., stethoscopes), which is useful for understanding the\ntheoretical aspects and practical design notes. In this paper, we first\nintroduce the acoustic properties of the heart and lungs, as well as a brief\nhistory of stethoscope evolution. Then, we discuss the basic concept of\nelectret condenser microphones (ECMs) and a stethoscope based on them. Then, we\ndiscuss the microelectromechanical systems (MEMSs) technology, particularly\nfocusing on piezoelectric transducer sensors. This paper comprehensively\nreviews sensing technologies for cardiorespiratory auscultation, emphasizing\nMEMS-based wearable designs in the past decade. To our knowledge, this is the\nfirst paper to summarize ECM and MEMS applications for heart and lung sound\nanalysis.", "published": "2024-06-18 09:28:23", "link": "http://arxiv.org/abs/2406.12432v2", "categories": ["eess.SP", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Towards Audio Codec-based Speech Separation", "abstract": "Recent improvements in neural audio codec (NAC) models have generated\ninterest in adopting pre-trained codecs for a variety of speech processing\napplications to take advantage of the efficiencies gained from high\ncompression, but these have yet been applied to the speech separation (SS)\ntask. SS can benefit from high compression because the compute required for\ntraditional SS models makes them impractical for many edge computing use cases.\nHowever, SS is a waveform-masking task where compression tends to introduce\ndistortions that severely impact performance. Here we propose a novel task of\nAudio Codec-based SS, where SS is performed within the embedding space of a\nNAC, and propose a new model, Codecformer, to address this task. At inference,\nCodecformer achieves a 52x reduction in MAC while producing separation\nperformance comparable to a cloud deployment of Sepformer. This method charts a\nnew direction for performing efficient SS in practical scenarios.", "published": "2024-06-18 09:29:24", "link": "http://arxiv.org/abs/2406.12434v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Bridging the Gap: Integrating Pre-trained Speech Enhancement and\n  Recognition Models for Robust Speech Recognition", "abstract": "Noise robustness is critical when applying automatic speech recognition (ASR)\nin real-world scenarios. One solution involves the used of speech enhancement\n(SE) models as the front end of ASR. However, neural network-based (NN-based)\nSE often introduces artifacts into the enhanced signals and harms ASR\nperformance, particularly when SE and ASR are independently trained. Therefore,\nthis study introduces a simple yet effective SE post-processing technique to\naddress the gap between various pre-trained SE and ASR models. A bridge module,\nwhich is a lightweight NN, is proposed to evaluate the signal-level information\nof the speech signal. Subsequently, using the signal-level information, the\nobservation addition technique is applied to effectively reduce the\nshortcomings of SE. The experimental results demonstrate the success of our\nmethod in integrating diverse pre-trained SE and ASR models, considerably\nboosting the ASR robustness. Crucially, no prior knowledge of the ASR or speech\ncontents is required during the training or inference stages. Moreover, the\neffectiveness of this approach extends to different datasets without\nnecessitating the fine-tuning of the bridge module, ensuring efficiency and\nimproved generalization.", "published": "2024-06-18 15:12:15", "link": "http://arxiv.org/abs/2406.12699v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Integrating Representational Gestures into Automatically Generated\n  Embodied Explanations and its Effects on Understanding and Interaction\n  Quality", "abstract": "In human interaction, gestures serve various functions such as marking speech\nrhythm, highlighting key elements, and supplementing information. These\ngestures are also observed in explanatory contexts. However, the impact of\ngestures on explanations provided by virtual agents remains underexplored. A\nuser study was carried out to investigate how different types of gestures\ninfluence perceived interaction quality and listener understanding. This study\naddresses the effect of gestures in explanation by developing an embodied\nvirtual explainer integrating both beat gestures and iconic gestures to enhance\nits automatically generated verbal explanations. Our model combines beat\ngestures generated by a learned speech-driven synthesis module with manually\ncaptured iconic gestures, supporting the agent's verbal expressions about the\nboard game Quarto! as an explanation scenario. Findings indicate that neither\nthe use of iconic gestures alone nor their combination with beat gestures\noutperforms the baseline or beat-only conditions in terms of understanding.\nNonetheless, compared to prior research, the embodied agent significantly\nenhances understanding.", "published": "2024-06-18 12:23:00", "link": "http://arxiv.org/abs/2406.12544v2", "categories": ["cs.HC", "cs.CV", "cs.GR", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
