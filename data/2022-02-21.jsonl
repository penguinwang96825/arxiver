{"title": "StyleBERT: Chinese pretraining by font style information", "abstract": "With the success of down streaming task using English pre-trained language\nmodel, the pre-trained Chinese language model is also necessary to get a better\nperformance of Chinese NLP task. Unlike the English language, Chinese has its\nspecial characters such as glyph information. So in this article, we propose\nthe Chinese pre-trained language model StyleBERT which incorporate the\nfollowing embedding information to enhance the savvy of language model, such as\nword, pinyin, five stroke and chaizi. The experiments show that the model\nachieves well performances on a wide range of Chinese NLP tasks.", "published": "2022-02-21 02:45:12", "link": "http://arxiv.org/abs/2202.09955v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "USCORE: An Effective Approach to Fully Unsupervised Evaluation Metrics\n  for Machine Translation", "abstract": "The vast majority of evaluation metrics for machine translation are\nsupervised, i.e., (i) are trained on human scores, (ii) assume the existence of\nreference translations, or (iii) leverage parallel data. This hinders their\napplicability to cases where such supervision signals are not available. In\nthis work, we develop fully unsupervised evaluation metrics. To do so, we\nleverage similarities and synergies between evaluation metric induction,\nparallel corpus mining, and MT systems. In particular, we use an unsupervised\nevaluation metric to mine pseudo-parallel data, which we use to remap deficient\nunderlying vector spaces (in an iterative manner) and to induce an unsupervised\nMT system, which then provides pseudo-references as an additional component in\nthe metric. Finally, we also induce unsupervised multilingual sentence\nembeddings from pseudo-parallel data. We show that our fully unsupervised\nmetrics are effective, i.e., they beat supervised competitors on 4 out of our 5\nevaluation datasets. We make our code publicly available.", "published": "2022-02-21 09:22:29", "link": "http://arxiv.org/abs/2202.10062v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT WEAVER: Using WEight AVERaging to enable lifelong learning for\n  transformer-based models in biomedical semantic search engines", "abstract": "Recent developments in transfer learning have boosted the advancements in\nnatural language processing tasks. The performance is, however, dependent on\nhigh-quality, manually annotated training data. Especially in the biomedical\ndomain, it has been shown that one training corpus is not enough to learn\ngeneric models that are able to efficiently predict on new data. Therefore, in\norder to be used in real world applications state-of-the-art models need the\nability of lifelong learning to improve performance as soon as new data are\navailable - without the need of re-training the whole model from scratch. We\npresent WEAVER, a simple, yet efficient post-processing method that infuses old\nknowledge into the new model, thereby reducing catastrophic forgetting. We show\nthat applying WEAVER in a sequential manner results in similar word embedding\ndistributions as doing a combined training on all data at once, while being\ncomputationally more efficient. Because there is no need of data sharing, the\npresented method is also easily applicable to federated learning settings and\ncan for example be beneficial for the mining of electronic health records from\ndifferent clinics.", "published": "2022-02-21 10:34:41", "link": "http://arxiv.org/abs/2202.10101v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Adaptation in Neural Machine Translation using a Qualia-Enriched\n  FrameNet", "abstract": "In this paper we present Scylla, a methodology for domain adaptation of\nNeural Machine Translation (NMT) systems that make use of a multilingual\nFrameNet enriched with qualia relations as an external knowledge base. Domain\nadaptation techniques used in NMT usually require fine-tuning and in-domain\ntraining data, which may pose difficulties for those working with\nlesser-resourced languages and may also lead to performance decay of the NMT\nsystem for out-of-domain sentences. Scylla does not require fine-tuning of the\nNMT model, avoiding the risk of model over-fitting and consequent decrease in\nperformance for out-of-domain translations. Two versions of Scylla are\npresented: one using the source sentence as input, and another one using the\ntarget sentence. We evaluate Scylla in comparison to a state-of-the-art\ncommercial NMT system in an experiment in which 50 sentences from the Sports\ndomain are translated from Brazilian Portuguese to English. The two versions of\nScylla significantly outperform the baseline commercial system in HTER.", "published": "2022-02-21 15:05:23", "link": "http://arxiv.org/abs/2202.10287v1", "categories": ["cs.CL", "E.1"], "primary_category": "cs.CL"}
{"title": "Embarrassingly Simple Performance Prediction for Abductive Natural\n  Language Inference", "abstract": "The task of abductive natural language inference (\\alpha{}nli), to decide\nwhich hypothesis is the more likely explanation for a set of observations, is a\nparticularly difficult type of NLI. Instead of just determining a causal\nrelationship, it requires common sense to also evaluate how reasonable an\nexplanation is. All recent competitive systems build on top of contextualized\nrepresentations and make use of transformer architectures for learning an NLI\nmodel. When somebody is faced with a particular NLI task, they need to select\nthe best model that is available. This is a time-consuming and resource-intense\nendeavour. To solve this practical problem, we propose a simple method for\npredicting the performance without actually fine-tuning the model. We do this\nby testing how well the pre-trained models perform on the \\alpha{}nli task when\njust comparing sentence embeddings with cosine similarity to what the\nperformance that is achieved when training a classifier on top of these\nembeddings. We show that the accuracy of the cosine similarity approach\ncorrelates strongly with the accuracy of the classification approach with a\nPearson correlation coefficient of 0.65. Since the similarity computation is\norders of magnitude faster to compute on a given dataset (less than a minute\nvs. hours), our method can lead to significant time savings in the process of\nmodel selection.", "published": "2022-02-21 18:10:24", "link": "http://arxiv.org/abs/2202.10408v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Items from Psychometric Tests as Training Data for Personality Profiling\n  Models of Twitter Users", "abstract": "Machine-learned models for author profiling in social media often rely on\ndata acquired via self-reporting-based psychometric tests (questionnaires)\nfilled out by social media users. This is an expensive but accurate data\ncollection strategy. Another, less costly alternative, which leads to\npotentially more noisy and biased data, is to rely on labels inferred from\npublicly available information in the profiles of the users, for instance\nself-reported diagnoses or test results. In this paper, we explore a third\nstrategy, namely to directly use a corpus of items from validated psychometric\ntests as training data. Items from psychometric tests often consist of\nsentences from an I-perspective (e.g., \"I make friends easily.\"). Such corpora\nof test items constitute 'small data', but their availability for many concepts\nis a rich resource. We investigate this approach for personality profiling, and\nevaluate BERT classifiers fine-tuned on such psychometric test items for the\nbig five personality traits (openness, conscientiousness, extraversion,\nagreeableness, neuroticism) and analyze various augmentation strategies\nregarding their potential to address the challenges coming with such a small\ncorpus. Our evaluation on a publicly available Twitter corpus shows a\ncomparable performance to in-domain training for 4/5 personality traits with\nT5-based data augmentation.", "published": "2022-02-21 18:24:59", "link": "http://arxiv.org/abs/2202.10415v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Counter Hate Speech in Social Media: A Survey", "abstract": "With the high prevalence of offensive language against minorities in social\nmedia, counter-hate speeches (CHS) generation is considered an automatic way of\ntackling this challenge. The CHS is supposed to appear as a third voice to\neducate people and keep the social [red lines bold] without limiting the\nprinciples of freedom of speech. In this paper, we review the most important\nresearch in the past and present with a main focus on methodologies, collected\ndatasets and statistical analysis CHS's impact on social media. The CHS\ngeneration is based on the optimistic assumption that any attempt to intervene\nthe hate speech in social media can play a positive role in this context.\nBeyond that, previous works ignored the investigation of the sequence of\ncomments before and after the CHS. However, the positive impact is not\nguaranteed, as shown in some previous works. To the best of our knowledge, no\nattempt has been made to survey the related work to compare the past research\nin terms of CHS's impact on social media. We take the first step in this\ndirection by providing a comprehensive review on related works and categorizing\nthem based on different factors including impact, methodology, data source,\netc.", "published": "2022-02-21 06:16:46", "link": "http://arxiv.org/abs/2203.03584v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Audio Visual Scene-Aware Dialog Generation with Transformer-based Video\n  Representations", "abstract": "There have been many attempts to build multimodal dialog systems that can\nrespond to a question about given audio-visual information, and the\nrepresentative task for such systems is the Audio Visual Scene-Aware Dialog\n(AVSD). Most conventional AVSD models adopt the Convolutional Neural Network\n(CNN)-based video feature extractor to understand visual information. While a\nCNN tends to obtain both temporally and spatially local information, global\ninformation is also crucial for boosting video understanding because AVSD\nrequires long-term temporal visual dependency and whole visual information. In\nthis study, we apply the Transformer-based video feature that can capture both\ntemporally and spatially global representations more efficiently than the\nCNN-based feature. Our AVSD model with its Transformer-based feature attains\nhigher objective performance scores for answer generation. In addition, our\nmodel achieves a subjective score close to that of human answers in DSTC10. We\nobserved that the Transformer-based visual feature is beneficial for the AVSD\ntask because our model tends to correctly answer the questions that need a\ntemporally and spatially broad range of visual information.", "published": "2022-02-21 04:09:32", "link": "http://arxiv.org/abs/2202.09979v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Domain-level Pairwise Semantic Interaction for Aspect-Based Sentiment\n  Classification", "abstract": "Aspect-based sentiment classification (ABSC) is a very challenging subtask of\nsentiment analysis (SA) and suffers badly from the class-imbalance. Existing\nmethods only process sentences independently, without considering the\ndomain-level relationship between sentences, and fail to provide effective\nsolutions to the problem of class-imbalance. From an intuitive point of view,\nsentences in the same domain often have high-level semantic connections. The\ninteraction of their high-level semantic features can force the model to\nproduce better semantic representations, and find the similarities and nuances\nbetween sentences better. Driven by this idea, we propose a plug-and-play\nPairwise Semantic Interaction (PSI) module, which takes pairwise sentences as\ninput, and obtains interactive information by learning the semantic vectors of\nthe two sentences. Subsequently, different gates are generated to effectively\nhighlight the key semantic features of each sentence. Finally, the adversarial\ninteraction between the vectors is used to make the semantic representation of\ntwo sentences more distinguishable. Experimental results on four ABSC datasets\nshow that, in most cases, PSI is superior to many competitive state-of-the-art\nbaselines and can significantly alleviate the problem of class-imbalance.", "published": "2022-02-21 07:59:17", "link": "http://arxiv.org/abs/2202.10032v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A new data augmentation method for intent classification enhancement and\n  its application on spoken conversation datasets", "abstract": "Intent classifiers are vital to the successful operation of virtual agent\nsystems. This is especially so in voice activated systems where the data can be\nnoisy with many ambiguous directions for user intents. Before operation begins,\nthese classifiers are generally lacking in real-world training data. Active\nlearning is a common approach used to help label large amounts of collected\nuser input. However, this approach requires many hours of manual labeling work.\nWe present the Nearest Neighbors Scores Improvement (NNSI) algorithm for\nautomatic data selection and labeling. The NNSI reduces the need for manual\nlabeling by automatically selecting highly-ambiguous samples and labeling them\nwith high accuracy. This is done by integrating the classifier's output from a\nsemantically similar group of text samples. The labeled samples can then be\nadded to the training set to improve the accuracy of the classifier. We\ndemonstrated the use of NNSI on two large-scale, real-life voice conversation\nsystems. Evaluation of our results showed that our method was able to select\nand label useful samples with high accuracy. Adding these new samples to the\ntraining data significantly improved the classifiers and reduced error rates by\nup to 10%.", "published": "2022-02-21 11:36:19", "link": "http://arxiv.org/abs/2202.10137v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Adaptive Discounting of Implicit Language Models in RNN-Transducers", "abstract": "RNN-Transducer (RNN-T) models have become synonymous with streaming\nend-to-end ASR systems. While they perform competitively on a number of\nevaluation categories, rare words pose a serious challenge to RNN-T models. One\nmain reason for the degradation in performance on rare words is that the\nlanguage model (LM) internal to RNN-Ts can become overconfident and lead to\nhallucinated predictions that are acoustically inconsistent with the underlying\nspeech. To address this issue, we propose a lightweight adaptive LM discounting\ntechnique AdaptLMD, that can be used with any RNN-T architecture without\nrequiring any external resources or additional parameters. AdaptLMD uses a\ntwo-pronged approach: 1) Randomly mask the prediction network output to\nencourage the RNN-T to not be overly reliant on it's outputs. 2) Dynamically\nchoose when to discount the implicit LM (ILM) based on rarity of recently\npredicted tokens and divergence between ILM and implicit acoustic model (IAM)\nscores. Comparing AdaptLMD to a competitive RNN-T baseline, we obtain up to 4%\nand 14% relative reductions in overall WER and rare word PER, respectively, on\na conversational, code-mixed Hindi-English ASR task.", "published": "2022-02-21 08:44:56", "link": "http://arxiv.org/abs/2203.02317v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CampNet: Context-Aware Mask Prediction for End-to-End Text-Based Speech\n  Editing", "abstract": "The text-based speech editor allows the editing of speech through intuitive\ncutting, copying, and pasting operations to speed up the process of editing\nspeech. However, the major drawback of current systems is that edited speech\noften sounds unnatural due to cut-copy-paste operation. In addition, it is not\nobvious how to synthesize records according to a new word not appearing in the\ntranscript. This paper proposes a novel end-to-end text-based speech editing\nmethod called context-aware mask prediction network (CampNet). The model can\nsimulate the text-based speech editing process by randomly masking part of\nspeech and then predicting the masked region by sensing the speech context. It\ncan solve unnatural prosody in the edited region and synthesize the speech\ncorresponding to the unseen words in the transcript. Secondly, for the possible\noperation of text-based speech editing, we design three text-based operations\nbased on CampNet: deletion, insertion, and replacement. These operations can\ncover various situations of speech editing. Thirdly, to synthesize the speech\ncorresponding to long text in insertion and replacement operations, a\nword-level autoregressive generation method is proposed. Fourthly, we propose a\nspeaker adaptation method using only one sentence for CampNet and explore the\nability of few-shot learning based on CampNet, which provides a new idea for\nspeech forgery tasks. The subjective and objective experiments on VCTK and\nLibriTTS datasets show that the speech editing results based on CampNet are\nbetter than TTS technology, manual editing, and VoCo method. We also conduct\ndetailed ablation experiments to explore the effect of the CampNet structure on\nits performance. Finally, the experiment shows that speaker adaptation with\nonly one sentence can further improve the naturalness of speech. Examples of\ngenerated speech can be found at https://hairuo55.github.io/CampNet.", "published": "2022-02-21 02:05:14", "link": "http://arxiv.org/abs/2202.09950v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Geodesic Quantum Walks", "abstract": "We propose a new family of discrete-spacetime quantum walks capable to\npropagate on any arbitrary triangulations. Moreover we also extend and\ngeneralize the duality principle introduced by one of the authors, linking\ncontinuous local deformations of a given triangulation and the inhomogeneity of\nthe local unitaries that guide the quantum walker. We proved that in the formal\ncontinuous limit, in both space and time, this new family of quantum walks\nconverges to the (1+2)D massless Dirac equation on curved manifolds. We believe\nthat this result has relevance in both modelling/simulating quantum transport\non discrete curved structures, such as fullerene molecules or dynamical causal\ntriangulation, and in addressing fast and efficient optimization problems in\nthe context of the curved space optimization methods.", "published": "2022-02-21 13:52:19", "link": "http://arxiv.org/abs/2202.10235v2", "categories": ["quant-ph", "cs.CG", "cs.CL", "gr-qc"], "primary_category": "quant-ph"}
{"title": "Seeing the advantage: visually grounding word embeddings to better\n  capture human semantic knowledge", "abstract": "Distributional semantic models capture word-level meaning that is useful in\nmany natural language processing tasks and have even been shown to capture\ncognitive aspects of word meaning. The majority of these models are purely text\nbased, even though the human sensory experience is much richer. In this paper\nwe create visually grounded word embeddings by combining English text and\nimages and compare them to popular text-based methods, to see if visual\ninformation allows our model to better capture cognitive aspects of word\nmeaning. Our analysis shows that visually grounded embedding similarities are\nmore predictive of the human reaction times in a large priming experiment than\nthe purely text-based embeddings. The visually grounded embeddings also\ncorrelate well with human word similarity ratings. Importantly, in both\nexperiments we show that the grounded embeddings account for a unique portion\nof explained variance, even when we include text-based embeddings trained on\nhuge corpora. This shows that visual grounding allows our model to capture\ninformation that cannot be extracted using text as the only source of\ninformation.", "published": "2022-02-21 15:13:48", "link": "http://arxiv.org/abs/2202.10292v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interpreting Language Models with Contrastive Explanations", "abstract": "Model interpretability methods are often used to explain NLP model decisions\non tasks such as text classification, where the output space is relatively\nsmall. However, when applied to language generation, where the output space\noften consists of tens of thousands of tokens, these methods are unable to\nprovide informative explanations. Language models must consider various\nfeatures to predict a token, such as its part of speech, number, tense, or\nsemantics. Existing explanation methods conflate evidence for all these\nfeatures into a single explanation, which is less interpretable for human\nunderstanding.\n  To disentangle the different decisions in language modeling, we focus on\nexplaining language models contrastively: we look for salient input tokens that\nexplain why the model predicted one token instead of another. We demonstrate\nthat contrastive explanations are quantifiably better than non-contrastive\nexplanations in verifying major grammatical phenomena, and that they\nsignificantly improve contrastive model simulatability for human observers. We\nalso identify groups of contrastive decisions where the model uses similar\nevidence, and we are able to characterize what input tokens models use during\nvarious language generation decisions.", "published": "2022-02-21 18:32:24", "link": "http://arxiv.org/abs/2202.10419v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transformer Quality in Linear Time", "abstract": "We revisit the design choices in Transformers, and propose methods to address\ntheir weaknesses in handling long sequences. First, we propose a simple layer\nnamed gated attention unit, which allows the use of a weaker single-head\nattention with minimal quality loss. We then propose a linear approximation\nmethod complementary to this new layer, which is accelerator-friendly and\nhighly competitive in quality. The resulting model, named FLASH, matches the\nperplexity of improved Transformers over both short (512) and long (8K) context\nlengths, achieving training speedups of up to 4.9$\\times$ on Wiki-40B and\n12.1$\\times$ on PG-19 for auto-regressive language modeling, and 4.8$\\times$ on\nC4 for masked language modeling.", "published": "2022-02-21 18:59:38", "link": "http://arxiv.org/abs/2202.10447v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "CaMEL: Mean Teacher Learning for Image Captioning", "abstract": "Describing images in natural language is a fundamental step towards the\nautomatic modeling of connections between the visual and textual modalities. In\nthis paper we present CaMEL, a novel Transformer-based architecture for image\ncaptioning. Our proposed approach leverages the interaction of two\ninterconnected language models that learn from each other during the training\nphase. The interplay between the two language models follows a mean teacher\nlearning paradigm with knowledge distillation. Experimentally, we assess the\neffectiveness of the proposed solution on the COCO dataset and in conjunction\nwith different visual feature extractors. When comparing with existing\nproposals, we demonstrate that our model provides state-of-the-art caption\nquality with a significantly reduced number of parameters. According to the\nCIDEr metric, we obtain a new state of the art on COCO when training without\nusing external data. The source code and trained models are publicly available\nat: https://github.com/aimagelab/camel.", "published": "2022-02-21 19:04:46", "link": "http://arxiv.org/abs/2202.10492v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "L-SpEx: Localized Target Speaker Extraction", "abstract": "Speaker extraction aims to extract the target speaker's voice from a\nmulti-talker speech mixture given an auxiliary reference utterance. Recent\nstudies show that speaker extraction benefits from the location or direction of\nthe target speaker. However, these studies assume that the target speaker's\nlocation is known in advance or detected by an extra visual cue, e.g., face\nimage or video. In this paper, we propose an end-to-end localized target\nspeaker extraction on pure speech cues, that is called L-SpEx. Specifically, we\ndesign a speaker localizer driven by the target speaker's embedding to extract\nthe spatial features, including direction-of-arrival (DOA) of the target\nspeaker and beamforming output. Then, the spatial cues and target speaker's\nembedding are both used to form a top-down auditory attention to the target\nspeaker. Experiments on the multi-channel reverberant dataset called\nMC-Libri2Mix show that our L-SpEx approach significantly outperforms the\nbaseline system.", "published": "2022-02-21 05:17:09", "link": "http://arxiv.org/abs/2202.09995v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The PCG-AIID System for L3DAS22 Challenge: MIMO and MISO convolutional\n  recurrent Network for Multi Channel Speech Enhancement and Speech Recognition", "abstract": "This paper described the PCG-AIID system for L3DAS22 challenge in Task 1: 3D\nspeech enhancement in office reverberant environment. We proposed a two-stage\nframework to address multi-channel speech denoising and dereverberation. In the\nfirst stage, a multiple input and multiple output (MIMO) network is applied to\nremove background noise while maintaining the spatial characteristics of\nmulti-channel signals. In the second stage, a multiple input and single output\n(MISO) network is applied to enhance the speech from desired direction and\npost-filtering. As a result, our system ranked 3rd place in ICASSP2022 L3DAS22\nchallenge and significantly outperforms the baseline system, while achieving\n3.2% WER and 0.972 STOI on the blind test-set.", "published": "2022-02-21 07:06:39", "link": "http://arxiv.org/abs/2202.10017v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AVQVC: One-shot Voice Conversion by Vector Quantization with applying\n  contrastive learning", "abstract": "Voice Conversion(VC) refers to changing the timbre of a speech while\nretaining the discourse content. Recently, many works have focused on\ndisentangle-based learning techniques to separate the timbre and the linguistic\ncontent information from a speech signal. Once successful, voice conversion\nwill be feasible and straightforward. This paper proposed a novel one-shot\nvoice conversion framework based on vector quantization voice conversion (VQVC)\nand AutoVC, called AVQVC. A new training method is applied to VQVC to separate\ncontent and timbre information from speech more effectively. The result shows\nthat this approach has better performance than VQVC in separating content and\ntimbre to improve the sound quality of generated speech.", "published": "2022-02-21 07:23:54", "link": "http://arxiv.org/abs/2202.10020v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spanish and English Phoneme Recognition by Training on Simulated\n  Classroom Audio Recordings of Collaborative Learning Environments", "abstract": "Audio recordings of collaborative learning environments contain a constant\npresence of cross-talk and background noise. Dynamic speech recognition between\nSpanish and English is required in these environments. To eliminate the\nstandard requirement of large-scale ground truth, the thesis develops a\nsimulated dataset by transforming audio transcriptions into phonemes and using\n3D speaker geometry and data augmentation to generate an acoustic simulation of\nSpanish and English speech. The thesis develops a low-complexity neural network\nfor recognizing Spanish and English phonemes (available at\ngithub.com/muelitas/keywordRec). When trained on 41 English phonemes, 0.099 PER\nis achieved on Speech Commands. When trained on 36 Spanish phonemes and tested\non real recordings of collaborative learning environments, a 0.7208 LER is\nachieved. Slightly better than Google's Speech-to-text 0.7272 LER, which used\nanywhere from 15 to 1,635 times more parameters and trained on 300 to 27,500\nhours of real data as opposed to 13 hours of simulated audios.", "published": "2022-02-21 21:25:41", "link": "http://arxiv.org/abs/2202.10536v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "S3T: Self-Supervised Pre-training with Swin Transformer for Music\n  Classification", "abstract": "In this paper, we propose S3T, a self-supervised pre-training method with\nSwin Transformer for music classification, aiming to learn meaningful music\nrepresentations from massive easily accessible unlabeled music data. S3T\nintroduces a momentum-based paradigm, MoCo, with Swin Transformer as its\nfeature extractor to music time-frequency domain. For better music\nrepresentations learning, S3T contributes a music data augmentation pipeline\nand two specially designed pre-processors. To our knowledge, S3T is the first\nmethod combining the Swin Transformer with a self-supervised learning method\nfor music classification. We evaluate S3T on music genre classification and\nmusic tagging tasks with linear classifiers trained on learned representations.\nExperimental results show that S3T outperforms the previous self-supervised\nmethod (CLMR) by 12.5 percents top-1 accuracy and 4.8 percents PR-AUC on two\ntasks respectively, and also surpasses the task-specific state-of-the-art\nsupervised methods. Besides, S3T shows advances in label efficiency using only\n10% labeled data exceeding CLMR on both tasks with 100% labeled data.", "published": "2022-02-21 11:36:59", "link": "http://arxiv.org/abs/2202.10139v1", "categories": ["eess.AS", "cs.IR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "L3DAS22 Challenge: Learning 3D Audio Sources in a Real Office\n  Environment", "abstract": "The L3DAS22 Challenge is aimed at encouraging the development of machine\nlearning strategies for 3D speech enhancement and 3D sound localization and\ndetection in office-like environments. This challenge improves and extends the\ntasks of the L3DAS21 edition. We generated a new dataset, which maintains the\nsame general characteristics of L3DAS21 datasets, but with an extended number\nof data points and adding constrains that improve the baseline model's\nefficiency and overcome the major difficulties encountered by the participants\nof the previous challenge. We updated the baseline model of Task 1, using the\narchitecture that ranked first in the previous challenge edition. We wrote a\nnew supporting API, improving its clarity and ease-of-use. In the end, we\npresent and discuss the results submitted by all participants. L3DAS22\nChallenge website: www.l3das.com/icassp2022.", "published": "2022-02-21 17:05:39", "link": "http://arxiv.org/abs/2202.10372v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "r-G2P: Evaluating and Enhancing Robustness of Grapheme to Phoneme\n  Conversion by Controlled noise introducing and Contextual information\n  incorporation", "abstract": "Grapheme-to-phoneme (G2P) conversion is the process of converting the written\nform of words to their pronunciations. It has an important role for\ntext-to-speech (TTS) synthesis and automatic speech recognition (ASR) systems.\nIn this paper, we aim to evaluate and enhance the robustness of G2P models. We\nshow that neural G2P models are extremely sensitive to orthographical\nvariations in graphemes like spelling mistakes. To solve this problem, we\npropose three controlled noise introducing methods to synthesize noisy training\ndata. Moreover, we incorporate the contextual information with the baseline and\npropose a robust training strategy to stabilize the training process. The\nexperimental results demonstrate that our proposed robust G2P model (r-G2P)\noutperforms the baseline significantly (-2.73\\% WER on Dict-based benchmarks\nand -9.09\\% WER on Real-world sources).", "published": "2022-02-21 13:29:30", "link": "http://arxiv.org/abs/2202.11194v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker Adaptation Using Spectro-Temporal Deep Features for Dysarthric\n  and Elderly Speech Recognition", "abstract": "Despite the rapid progress of automatic speech recognition (ASR) technologies\ntargeting normal speech in recent decades, accurate recognition of dysarthric\nand elderly speech remains highly challenging tasks to date. Sources of\nheterogeneity commonly found in normal speech including accent or gender, when\nfurther compounded with the variability over age and speech pathology severity\nlevel, create large diversity among speakers. To this end, speaker adaptation\ntechniques play a key role in personalization of ASR systems for such users.\nMotivated by the spectro-temporal level differences between dysarthric, elderly\nand normal speech that systematically manifest in articulatory imprecision,\ndecreased volume and clarity, slower speaking rates and increased dysfluencies,\nnovel spectrotemporal subspace basis deep embedding features derived using SVD\nspeech spectrum decomposition are proposed in this paper to facilitate\nauxiliary feature based speaker adaptation of state-of-the-art hybrid DNN/TDNN\nand end-to-end Conformer speech recognition systems. Experiments were conducted\non four tasks: the English UASpeech and TORGO dysarthric speech corpora; the\nEnglish DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech datasets.\nThe proposed spectro-temporal deep feature adapted systems outperformed\nbaseline i-Vector and xVector adaptation by up to 2.63% absolute (8.63%\nrelative) reduction in word error rate (WER). Consistent performance\nimprovements were retained after model based speaker adaptation using learning\nhidden unit contributions (LHUC) was further applied. The best speaker adapted\nsystem using the proposed spectral basis embedding features produced the lowest\npublished WER of 25.05% on the UASpeech test set of 16 dysarthric speakers.", "published": "2022-02-21 15:11:36", "link": "http://arxiv.org/abs/2202.10290v3", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "q-bio.QM"], "primary_category": "eess.AS"}
