{"title": "Fantastic Questions and Where to Find Them: FairytaleQA -- An Authentic\n  Dataset for Narrative Comprehension", "abstract": "Question answering (QA) is a fundamental means to facilitate assessment and\ntraining of narrative comprehension skills for both machines and young\nchildren, yet there is scarcity of high-quality QA datasets carefully designed\nto serve this purpose. In particular, existing datasets rarely distinguish\nfine-grained reading skills, such as the understanding of varying narrative\nelements. Drawing on the reading education research, we introduce FairytaleQA,\na dataset focusing on narrative comprehension of kindergarten to eighth-grade\nstudents. Generated by educational experts based on an evidence-based\ntheoretical framework, FairytaleQA consists of 10,580 explicit and implicit\nquestions derived from 278 children-friendly stories, covering seven types of\nnarrative elements or relations. Our dataset is valuable in two folds: First,\nwe ran existing QA models on our dataset and confirmed that this annotation\nhelps assess models' fine-grained learning skills. Second, the dataset supports\nquestion generation (QG) task in the education domain. Through benchmarking\nwith QG models, we show that the QG model trained on FairytaleQA is capable of\nasking high-quality and more diverse questions.", "published": "2022-03-26 00:20:05", "link": "http://arxiv.org/abs/2203.13947v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Densely Connected Criss-Cross Attention Network for Document-level\n  Relation Extraction", "abstract": "Document-level relation extraction (RE) aims to identify relations between\ntwo entities in a given document. Compared with its sentence-level counterpart,\ndocument-level RE requires complex reasoning. Previous research normally\ncompleted reasoning through information propagation on the mention-level or\nentity-level document-graph, but rarely considered reasoning at the\nentity-pair-level.In this paper, we propose a novel model, called Densely\nConnected Criss-Cross Attention Network (Dense-CCNet), for document-level RE,\nwhich can complete logical reasoning at the entity-pair-level. Specifically,\nthe Dense-CCNet performs entity-pair-level logical reasoning through the\nCriss-Cross Attention (CCA), which can collect contextual information in\nhorizontal and vertical directions on the entity-pair matrix to enhance the\ncorresponding entity-pair representation. In addition, we densely connect\nmultiple layers of the CCA to simultaneously capture the features of single-hop\nand multi-hop logical reasoning.We evaluate our Dense-CCNet model on three\npublic document-level RE datasets, DocRED, CDR, and GDA. Experimental results\ndemonstrate that our model achieves state-of-the-art performance on these three\ndatasets.", "published": "2022-03-26 01:01:34", "link": "http://arxiv.org/abs/2203.13953v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Metaphors in Pre-Trained Language Models: Probing and Generalization\n  Across Datasets and Languages", "abstract": "Human languages are full of metaphorical expressions. Metaphors help people\nunderstand the world by connecting new concepts and domains to more familiar\nones. Large pre-trained language models (PLMs) are therefore assumed to encode\nmetaphorical knowledge useful for NLP systems. In this paper, we investigate\nthis hypothesis for PLMs, by probing metaphoricity information in their\nencodings, and by measuring the cross-lingual and cross-dataset generalization\nof this information. We present studies in multiple metaphor detection datasets\nand in four languages (i.e., English, Spanish, Russian, and Farsi). Our\nextensive experiments suggest that contextual representations in PLMs do encode\nmetaphorical knowledge, and mostly in their middle layers. The knowledge is\ntransferable between languages and datasets, especially when the annotation is\nconsistent across training and testing sets. Our findings give helpful insights\nfor both cognitive and NLP scientists.", "published": "2022-03-26 19:05:24", "link": "http://arxiv.org/abs/2203.14139v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Medical Dataset Classification for Kurdish Short Text over Social Media", "abstract": "The Facebook application is used as a resource for collecting the comments of\nthis dataset, The dataset consists of 6756 comments to create a Medical Kurdish\nDataset (MKD). The samples are comments of users, which are gathered from\ndifferent posts of pages (Medical, News, Economy, Education, and Sport). Six\nsteps as a preprocessing technique are performed on the raw dataset to clean\nand remove noise in the comments by replacing characters. The comments (short\ntext) are labeled for positive class (medical comment) and negative class\n(non-medical comment) as text classification. The percentage ratio of the\nnegative class is 55% while the positive class is 45%.", "published": "2022-03-26 22:11:25", "link": "http://arxiv.org/abs/2204.09660v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Autoregressive Linguistic Steganography Based on BERT and Consistency\n  Coding", "abstract": "Linguistic steganography (LS) conceals the presence of communication by\nembedding secret information into a text. How to generate a high-quality text\ncarrying secret information is a key problem. With the widespread application\nof deep learning in natural language processing, recent algorithms use a\nlanguage model (LM) to generate the steganographic text, which provides a\nhigher payload compared with many previous arts. However, the security still\nneeds to be enhanced. To tackle with this problem, we propose a novel\nautoregressive LS algorithm based on BERT and consistency coding, which\nachieves a better trade-off between embedding payload and system security. In\nthe proposed work, based on the introduction of the masked LM, given a text, we\nuse consistency coding to make up for the shortcomings of block coding used in\nthe previous work so that we can encode arbitrary-size candidate token set and\ntake advantages of the probability distribution for information hiding. The\nmasked positions to be embedded are filled with tokens determined by an\nautoregressive manner to enhance the connection between contexts and therefore\nmaintain the quality of the text. Experimental results have shown that,\ncompared with related works, the proposed work improves the fluency of the\nsteganographic text while guaranteeing security, and also increases the\nembedding payload to a certain extent.", "published": "2022-03-26 02:36:55", "link": "http://arxiv.org/abs/2203.13972v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Lite Unified Modeling for Discriminative Reading Comprehension", "abstract": "As a broad and major category in machine reading comprehension (MRC), the\ngeneralized goal of discriminative MRC is answer prediction from the given\nmaterials. However, the focuses of various discriminative MRC tasks may be\ndiverse enough: multi-choice MRC requires model to highlight and integrate all\npotential critical evidence globally; while extractive MRC focuses on higher\nlocal boundary preciseness for answer extraction. Among previous works, there\nlacks a unified design with pertinence for the overall discriminative MRC\ntasks. To fill in above gap, we propose a lightweight POS-Enhanced Iterative\nCo-Attention Network (POI-Net) as the first attempt of unified modeling with\npertinence, to handle diverse discriminative MRC tasks synchronously. Nearly\nwithout introducing more parameters, our lite unified design brings model\nsignificant improvement with both encoder and decoder components. The\nevaluation results on four discriminative MRC benchmarks consistently indicate\nthe general effectiveness and applicability of our model, and the code is\navailable at https://github.com/Yilin1111/poi-net.", "published": "2022-03-26 15:47:19", "link": "http://arxiv.org/abs/2203.14103v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Demonstrating CAT: Synthesizing Data-Aware Conversational Agents for\n  Transactional Databases", "abstract": "Databases for OLTP are often the backbone for applications such as hotel room\nor cinema ticket booking applications. However, developing a conversational\nagent (i.e., a chatbot-like interface) to allow end-users to interact with an\napplication using natural language requires both immense amounts of training\ndata and NLP expertise. This motivates CAT, which can be used to easily create\nconversational agents for transactional databases. The main idea is that, for a\ngiven OLTP database, CAT uses weak supervision to synthesize the required\ntraining data to train a state-of-the-art conversational agent, allowing users\nto interact with the OLTP database. Furthermore, CAT provides an out-of-the-box\nintegration of the resulting agent with the database. As a major difference to\nexisting conversational agents, agents synthesized by CAT are data-aware. This\nmeans that the agent decides which information should be requested from the\nuser based on the current data distributions in the database, which typically\nresults in markedly more efficient dialogues compared with non-data-aware\nagents. We publish the code for CAT as open source.", "published": "2022-03-26 19:46:43", "link": "http://arxiv.org/abs/2203.14144v1", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Joint Transformer/RNN Architecture for Gesture Typing in Indic Languages", "abstract": "Gesture typing is a method of typing words on a touch-based keyboard by\ncreating a continuous trace passing through the relevant keys. This work is\naimed at developing a keyboard that supports gesture typing in Indic languages.\nWe begin by noting that when dealing with Indic languages, one needs to cater\nto two different sets of users: (i) users who prefer to type in the native\nIndic script (Devanagari, Bengali, etc.) and (ii) users who prefer to type in\nthe English script but want the output transliterated into the native script.\nIn both cases, we need a model that takes a trace as input and maps it to the\nintended word. To enable the development of these models, we create and release\ntwo datasets. First, we create a dataset containing keyboard traces for 193,658\nwords from 7 Indic languages. Second, we curate 104,412 English-Indic\ntransliteration pairs from Wikidata across these languages. Using these\ndatasets we build a model that performs path decoding, transliteration, and\ntransliteration correction. Unlike prior approaches, our proposed model does\nnot make co-character independence assumptions during decoding. The overall\naccuracy of our model across the 7 languages varies from 70-95%.", "published": "2022-03-26 11:14:23", "link": "http://arxiv.org/abs/2203.14049v1", "categories": ["cs.LG", "cs.CL", "cs.HC"], "primary_category": "cs.LG"}
{"title": "MQDD: Pre-training of Multimodal Question Duplicity Detection for\n  Software Engineering Domain", "abstract": "This work proposes a new pipeline for leveraging data collected on the Stack\nOverflow website for pre-training a multimodal model for searching duplicates\non question answering websites. Our multimodal model is trained on question\ndescriptions and source codes in multiple programming languages. We design two\nnew learning objectives to improve duplicate detection capabilities. The result\nof this work is a mature, fine-tuned Multimodal Question Duplicity Detection\n(MQDD) model, ready to be integrated into a Stack Overflow search system, where\nit can help users find answers for already answered questions. Alongside the\nMQDD model, we release two datasets related to the software engineering domain.\nThe first Stack Overflow Dataset (SOD) represents a massive corpus of paired\nquestions and answers. The second Stack Overflow Duplicity Dataset (SODD)\ncontains data for training duplicate detection models.", "published": "2022-03-26 15:01:26", "link": "http://arxiv.org/abs/2203.14093v2", "categories": ["cs.CL", "cs.LG", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "A Roadmap for Big Model", "abstract": "With the rapid development of deep learning, training Big Models (BMs) for\nmultiple downstream tasks becomes a popular paradigm. Researchers have achieved\nvarious outcomes in the construction of BMs and the BM application in many\nfields. At present, there is a lack of research work that sorts out the overall\nprogress of BMs and guides the follow-up research. In this paper, we cover not\nonly the BM technologies themselves but also the prerequisites for BM training\nand applications with BMs, dividing the BM review into four parts: Resource,\nModels, Key Technologies and Application. We introduce 16 specific BM-related\ntopics in those four parts, they are Data, Knowledge, Computing System,\nParallel Training System, Language Model, Vision Model, Multi-modal Model,\nTheory&Interpretability, Commonsense Reasoning, Reliability&Security,\nGovernance, Evaluation, Machine Translation, Text Generation, Dialogue and\nProtein Research. In each topic, we summarize clearly the current studies and\npropose some future research directions. At the end of this paper, we conclude\nthe further development of BMs in a more general view.", "published": "2022-03-26 15:38:00", "link": "http://arxiv.org/abs/2203.14101v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A comparative analysis of Graph Neural Networks and commonly used\n  machine learning algorithms on fake news detection", "abstract": "Fake news on social media is increasingly regarded as one of the most\nconcerning issues. Low cost, simple accessibility via social platforms, and a\nplethora of low-budget online news sources are some of the factors that\ncontribute to the spread of false news. Most of the existing fake news\ndetection algorithms are solely focused on the news content only but engaged\nusers prior posts or social activities provide a wealth of information about\ntheir views on news and have significant ability to improve fake news\nidentification. Graph Neural Networks are a form of deep learning approach that\nconducts prediction on graph-described data. Social media platforms are\nfollowed graph structure in their representation, Graph Neural Network are\nspecial types of neural networks that could be usually applied to graphs,\nmaking it much easier to execute edge, node, and graph-level prediction.\nTherefore, in this paper, we present a comparative analysis among some commonly\nused machine learning algorithms and Graph Neural Networks for detecting the\nspread of false news on social media platforms. In this study, we take the UPFD\ndataset and implement several existing machine learning algorithms on text data\nonly. Besides this, we create different GNN layers for fusing graph-structured\nnews propagation data and the text data as the node feature in our GNN models.\nGNNs provide the best solutions to the dilemma of identifying false news in our\nresearch.", "published": "2022-03-26 18:40:03", "link": "http://arxiv.org/abs/2203.14132v1", "categories": ["cs.LG", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "A Neural Vocoder Based Packet Loss Concealment Algorithm", "abstract": "The packet loss problem seriously affects the quality of service in Voice\nover IP (VoIP) sceneries. In this paper, we investigated online receiver-based\npacket loss concealment which is much more portable and applicable. For\nensuring the speech naturalness, rather than directly processing time-domain\nwaveforms or separately reconstructing amplitudes and phases in frequency\ndomain, a flow-based neural vocoder is adopted to generate the substitution\nwaveform of lost packet from Mel-spectrogram which is generated from history\ncontents by a well-designed neural predictor. Furthermore, a waveform\nsimilarity-based smoothing post-process is created to mitigate the\ndiscontinuity of speech and avoid the artifacts. The experimental results show\nthe outstanding performance of the proposed method.", "published": "2022-03-26 07:15:54", "link": "http://arxiv.org/abs/2203.14010v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Remix-cycle-consistent Learning on Adversarially Learned Separator for\n  Accurate and Stable Unsupervised Speech Separation", "abstract": "A new learning algorithm for speech separation networks is designed to\nexplicitly reduce residual noise and artifacts in the separated signal in an\nunsupervised manner. Generative adversarial networks are known to be effective\nin constructing separation networks when the ground truth for the observed\nsignal is inaccessible. Still, weak objectives aimed at\ndistribution-to-distribution mapping make the learning unstable and limit their\nperformance. This study introduces the remix-cycle-consistency loss as a more\nappropriate objective function and uses it to fine-tune adversarially learned\nsource separation models. The remix-cycle-consistency loss is defined as the\ndifference between the mixed speech observed at microphones and the\npseudo-mixed speech obtained by alternating the process of separating the mixed\nsound and remixing its outputs with another combination. The minimization of\nthis loss leads to an explicit reduction in the distortions in the output of\nthe separation network. Experimental comparisons with multichannel speech\nseparation demonstrated that the proposed method achieved high separation\naccuracy and learning stability comparable to supervised learning.", "published": "2022-03-26 13:49:17", "link": "http://arxiv.org/abs/2203.14080v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SpeechSplit 2.0: Unsupervised speech disentanglement for voice\n  conversion Without tuning autoencoder Bottlenecks", "abstract": "SpeechSplit can perform aspect-specific voice conversion by disentangling\nspeech into content, rhythm, pitch, and timbre using multiple autoencoders in\nan unsupervised manner. However, SpeechSplit requires careful tuning of the\nautoencoder bottlenecks, which can be time-consuming and less robust. This\npaper proposes SpeechSplit 2.0, which constrains the information flow of the\nspeech component to be disentangled on the autoencoder input using efficient\nsignal processing methods instead of bottleneck tuning. Evaluation results show\nthat SpeechSplit 2.0 achieves comparable performance to SpeechSplit in speech\ndisentanglement and superior robustness to the bottleneck size variations. Our\ncode is available at https://github.com/biggytruck/SpeechSplit2.", "published": "2022-03-26 21:01:26", "link": "http://arxiv.org/abs/2203.14156v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Speech Representation Anonymization Framework via Selective Noise\n  Perturbation", "abstract": "Privacy and security are major concerns when communicating speech signals to\ncloud services such as automatic speech recognition (ASR) and speech emotion\nrecognition (SER). Existing solutions for speech anonymization mainly focus on\nvoice conversion or voice modification to convert a raw utterance into another\none with similar content but different, or no, identity-related information.\nHowever, an alternative approach to share speech data under the form of\nprivacy-preserving representation has been largely under-explored. In this\npaper, we propose a speech anonymization framework that achieves privacy via\nnoise perturbation to a selected subset of the high-utility representations\nextracted using a pre-trained speech encoder. The subset is chosen with a\nTransformer-based privacy-risk saliency estimator. We validate our framework on\nfour tasks, namely, Automatic Speaker Verification (ASV), ASR, SER and Intent\nClassification (IC) for privacy and utility assessment. Experimental results\nshow that our approach is able to achieve a competitive, or even better,\nutility compared to the speech anonymization baselines from the\nVoicePrivacy2022 Challenges, providing the same level of privacy. Moreover, the\neasily-controlled amount of perturbation allows our framework to have a\nflexible range of privacy-utility trade-offs without re-training any component.", "published": "2022-03-26 23:28:55", "link": "http://arxiv.org/abs/2203.14171v2", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
