{"title": "Confidence penalty, annealing Gaussian noise and zoneout for biLSTM-CRF\n  networks for named entity recognition", "abstract": "Named entity recognition (NER) is used to identify relevant entities in text.\nA bidirectional LSTM (long short term memory) encoder with a neural conditional\nrandom fields (CRF) decoder (biLSTM-CRF) is the state of the art methodology.\nIn this work, we have done an analysis of several methods that intend to\noptimize the performance of networks based on this architecture, which in some\ncases encourage overfitting avoidance. These methods target exploration of\nparameter space, regularization of LSTMs and penalization of confident output\ndistributions. Results show that the optimization methods improve the\nperformance of the biLSTM-CRF NER baseline system, setting a new state of the\nart performance for the CoNLL-2003 Spanish set with an F1 of 87.18.", "published": "2018-08-13 00:16:55", "link": "http://arxiv.org/abs/1808.04029v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Regularizing Neural Machine Translation by Target-bidirectional\n  Agreement", "abstract": "Although Neural Machine Translation (NMT) has achieved remarkable progress in\nthe past several years, most NMT systems still suffer from a fundamental\nshortcoming as in other sequence generation tasks: errors made early in\ngeneration process are fed as inputs to the model and can be quickly amplified,\nharming subsequent sequence generation. To address this issue, we propose a\nnovel model regularization method for NMT training, which aims to improve the\nagreement between translations generated by left-to-right (L2R) and\nright-to-left (R2L) NMT decoders. This goal is achieved by introducing two\nKullback-Leibler divergence regularization terms into the NMT training\nobjective to reduce the mismatch between output probabilities of L2R and R2L\nmodels. In addition, we also employ a joint training strategy to allow L2R and\nR2L models to improve each other in an interactive update process. Experimental\nresults show that our proposed method significantly outperforms\nstate-of-the-art baselines on Chinese-English and English-German translation\ntasks.", "published": "2018-08-13 05:03:42", "link": "http://arxiv.org/abs/1808.04064v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Style Transfer from Sentences with Arbitrary Unknown Styles", "abstract": "Language style transfer is the problem of migrating the content of a source\nsentence to a target style. In many of its applications, parallel training data\nare not available and source sentences to be transferred may have arbitrary and\nunknown styles. First, each sentence is encoded into its content and style\nlatent representations. Then, by recombining the content with the target style,\nwe decode a sentence aligned in the target domain. To adequately constrain the\nencoding and decoding functions, we couple them with two loss functions. The\nfirst is a style discrepancy loss, enforcing that the style representation\naccurately encodes the style information guided by the discrepancy between the\nsentence style and the target style. The second is a cycle consistency loss,\nwhich ensures that the transferred sentence should preserve the content of the\noriginal sentence disentangled from its style. We validate the effectiveness of\nour model in three tasks: sentiment modification of restaurant reviews, dialog\nresponse revision with a romantic style, and sentence rewriting with a\nShakespearean style.", "published": "2018-08-13 06:08:45", "link": "http://arxiv.org/abs/1808.04071v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Live Video Comment Generation Based on Surrounding Frames and Live\n  Comments", "abstract": "In this paper, we propose the task of live comment generation. Live comments\nare a new form of comments on videos, which can be regarded as a mixture of\ncomments and chats. A high-quality live comment should be not only relevant to\nthe video, but also interactive with other users. In this work, we first\nconstruct a new dataset for live comment generation. Then, we propose a novel\nend-to-end model to generate the human-like live comments by referring to the\nvideo and the other users' comments. Finally, we evaluate our model on the\nconstructed dataset. Experimental results show that our method can\nsignificantly outperform the baselines.", "published": "2018-08-13 07:52:49", "link": "http://arxiv.org/abs/1808.04091v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Semantics with Gated Graph Neural Networks for Knowledge Base\n  Question Answering", "abstract": "The most approaches to Knowledge Base Question Answering are based on\nsemantic parsing. In this paper, we address the problem of learning vector\nrepresentations for complex semantic parses that consist of multiple entities\nand relations. Previous work largely focused on selecting the correct semantic\nrelations for a question and disregarded the structure of the semantic parse:\nthe connections between entities and the directions of the relations. We\npropose to use Gated Graph Neural Networks to encode the graph structure of the\nsemantic parse. We show on two data sets that the graph networks outperform all\nbaseline models that do not explicitly model the structure. The error analysis\nconfirms that our approach can successfully process complex semantic parses.", "published": "2018-08-13 09:50:43", "link": "http://arxiv.org/abs/1808.04126v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning for Sequence Tagging: An Empirical Study", "abstract": "We study three general multi-task learning (MTL) approaches on 11 sequence\ntagging tasks. Our extensive empirical results show that in about 50% of the\ncases, jointly learning all 11 tasks improves upon either independent or\npairwise learning of the tasks. We also show that pairwise MTL can inform us\nwhat tasks can benefit others or what tasks can be benefited if they are\nlearned jointly. In particular, we identify tasks that can always benefit\nothers as well as tasks that can always be harmed by others. Interestingly, one\nof our MTL approaches yields embeddings of the tasks that reveal the natural\nclustering of semantic and syntactic tasks. Our inquiries have opened the doors\nto further utilization of MTL in NLP.", "published": "2018-08-13 11:15:23", "link": "http://arxiv.org/abs/1808.04151v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Reference-Based Evaluation of Pronoun Translation Misses the\n  Point", "abstract": "We compare the performance of the APT and AutoPRF metrics for pronoun\ntranslation against a manually annotated dataset comprising human judgements as\nto the correctness of translations of the PROTEST test suite. Although there is\nsome correlation with the human judgements, a range of issues limit the\nperformance of the automated metrics. Instead, we recommend the use of\nsemi-automatic metrics and test suites in place of fully automatic metrics.", "published": "2018-08-13 12:04:44", "link": "http://arxiv.org/abs/1808.04164v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rapid Adaptation of Neural Machine Translation to New Languages", "abstract": "This paper examines the problem of adapting neural machine translation\nsystems to new, low-resourced languages (LRLs) as effectively and rapidly as\npossible. We propose methods based on starting with massively multilingual\n\"seed models\", which can be trained ahead-of-time, and then continuing training\non data related to the LRL. We contrast a number of strategies, leading to a\nnovel, simple, yet effective method of \"similar-language regularization\", where\nwe jointly train on both a LRL of interest and a similar high-resourced\nlanguage to prevent over-fitting to small LRL data. Experiments demonstrate\nthat massively multilingual models, even without any explicit adaptation, are\nsurprisingly effective, achieving BLEU scores of up to 15.5 with no data from\nthe LRL, and that the proposed similar-language regularization method improves\nover other adaptation methods by 1.7 BLEU points average over 4 LRL settings.\nCode to reproduce experiments at https://github.com/neubig/rapid-adaptation", "published": "2018-08-13 13:06:24", "link": "http://arxiv.org/abs/1808.04189v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Semi-Markov Conditional Random Fields for Robust Character-Based\n  Part-of-Speech Tagging", "abstract": "Character-level models of tokens have been shown to be effective at dealing\nwith within-token noise and out-of-vocabulary words. But these models still\nrely on correct token boundaries. In this paper, we propose a novel end-to-end\ncharacter-level model and demonstrate its effectiveness in multilingual\nsettings and when token boundaries are noisy. Our model is a semi-Markov\nconditional random field with neural networks for character and segment\nrepresentation. It requires no tokenizer. The model matches state-of-the-art\nbaselines for various languages and significantly outperforms them on a noisy\nEnglish version of a part-of-speech tagging benchmark dataset. Our code and the\nnoisy dataset are publicly available at http://cistern.cis.lmu.de/semiCRF.", "published": "2018-08-13 13:44:22", "link": "http://arxiv.org/abs/1808.04208v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing morphological complexity of Spanish, Otomi and Nahuatl", "abstract": "We use two small parallel corpora for comparing the morphological complexity\nof Spanish, Otomi and Nahuatl. These are languages that belong to different\nlinguistic families, the latter are low-resourced. We take into account two\nquantitative criteria, on one hand the distribution of types over tokens in a\ncorpus, on the other, perplexity and entropy as indicators of word structure\npredictability. We show that a language can be complex in terms of how many\ndifferent morphological word forms can produce, however, it may be less complex\nin terms of predictability of its internal structure of words.", "published": "2018-08-13 16:16:11", "link": "http://arxiv.org/abs/1808.04314v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disentangled Representation Learning for Non-Parallel Text Style\n  Transfer", "abstract": "This paper tackles the problem of disentangling the latent variables of style\nand content in language models. We propose a simple yet effective approach,\nwhich incorporates auxiliary multi-task and adversarial objectives, for label\nprediction and bag-of-words prediction, respectively. We show, both\nqualitatively and quantitatively, that the style and content are indeed\ndisentangled in the latent space. This disentangled latent representation\nlearning method is applied to style transfer on non-parallel corpora. We\nachieve substantially better results in terms of transfer accuracy, content\npreservation and language fluency, in comparison to previous state-of-the-art\napproaches.", "published": "2018-08-13 17:26:49", "link": "http://arxiv.org/abs/1808.04339v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Capsule Network-based Embedding Model for Knowledge Graph Completion\n  and Search Personalization", "abstract": "In this paper, we introduce an embedding model, named CapsE, exploring a\ncapsule network to model relationship triples (subject, relation, object). Our\nCapsE represents each triple as a 3-column matrix where each column vector\nrepresents the embedding of an element in the triple. This 3-column matrix is\nthen fed to a convolution layer where multiple filters are operated to generate\ndifferent feature maps. These feature maps are reconstructed into corresponding\ncapsules which are then routed to another capsule to produce a continuous\nvector. The length of this vector is used to measure the plausibility score of\nthe triple. Our proposed CapsE obtains better performance than previous\nstate-of-the-art embedding models for knowledge graph completion on two\nbenchmark datasets WN18RR and FB15k-237, and outperforms strong search\npersonalization baselines on SEARCH17.", "published": "2018-08-13 09:35:44", "link": "http://arxiv.org/abs/1808.04122v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Learning Explanations from Language Data", "abstract": "PatternAttribution is a recent method, introduced in the vision domain, that\nexplains classifications of deep neural networks. We demonstrate that it also\ngenerates meaningful interpretations in the language domain.", "published": "2018-08-13 09:51:46", "link": "http://arxiv.org/abs/1808.04127v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "REGMAPR - Text Matching Made Easy", "abstract": "Text matching is a fundamental problem in natural language processing. Neural\nmodels using bidirectional LSTMs for sentence encoding and inter-sentence\nattention mechanisms perform remarkably well on several benchmark datasets. We\npropose REGMAPR - a simple and general architecture for text matching that does\nnot use inter-sentence attention. Starting from a Siamese architecture, we\naugment the embeddings of the words with two features based on exact and para-\nphrase match between words in the two sentences. We train the model using three\ntypes of regularization on datasets for textual entailment, paraphrase\ndetection and semantic related- ness. REGMAPR performs comparably or better\nthan more complex neural models or models using a large number of handcrafted\nfeatures. REGMAPR achieves state-of-the-art results for paraphrase detection on\nthe SICK dataset and for textual entailment on the SNLI dataset among models\nthat do not use inter-sentence attention.", "published": "2018-08-13 17:38:54", "link": "http://arxiv.org/abs/1808.04343v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "D-PAGE: Diverse Paraphrase Generation", "abstract": "In this paper, we investigate the diversity aspect of paraphrase generation.\nPrior deep learning models employ either decoding methods or add random input\nnoise for varying outputs. We propose a simple method Diverse Paraphrase\nGeneration (D-PAGE), which extends neural machine translation (NMT) models to\nsupport the generation of diverse paraphrases with implicit rewriting patterns.\nOur experimental results on two real-world benchmark datasets demonstrate that\nour model generates at least one order of magnitude more diverse outputs than\nthe baselines in terms of a new evaluation metric Jeffrey's Divergence. We have\nalso conducted extensive experiments to understand various properties of our\nmodel with a focus on diversity.", "published": "2018-08-13 10:18:54", "link": "http://arxiv.org/abs/1808.04364v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What is wrong with style transfer for texts?", "abstract": "A number of recent machine learning papers work with an automated style\ntransfer for texts and, counter to intuition, demonstrate that there is no\nconsensus formulation of this NLP task. Different researchers propose different\nalgorithms, datasets and target metrics to address it. This short opinion paper\naims to discuss possible formalization of this NLP task in anticipation of a\nfurther growing interest to it.", "published": "2018-08-13 11:50:03", "link": "http://arxiv.org/abs/1808.04365v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Audio to Scene Image Synthesis using Generative Adversarial\n  Network", "abstract": "Humans can imagine a scene from a sound. We want machines to do so by using\nconditional generative adversarial networks (GANs). By applying the techniques\nincluding spectral norm, projection discriminator and auxiliary classifier,\ncompared with naive conditional GAN, the model can generate images with better\nquality in terms of both subjective and objective evaluations. Almost\nthree-fourth of people agree that our model have the ability to generate images\nrelated to sounds. By inputting different volumes of the same sound, our model\noutput different scales of changes based on the volumes, showing that our model\ntruly knows the relationship between sounds and images to some extent.", "published": "2018-08-13 08:46:42", "link": "http://arxiv.org/abs/1808.04108v1", "categories": ["cs.CL", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Angular-Based Word Meta-Embedding Learning", "abstract": "Ensembling word embeddings to improve distributed word representations has\nshown good success for natural language processing tasks in recent years. These\napproaches either carry out straightforward mathematical operations over a set\nof vectors or use unsupervised learning to find a lower-dimensional\nrepresentation. This work compares meta-embeddings trained for different\nlosses, namely loss functions that account for angular distance between the\nreconstructed embedding and the target and those that account normalized\ndistances based on the vector length. We argue that meta-embeddings are better\nto treat the ensemble set equally in unsupervised learning as the respective\nquality of each embedding is unknown for upstream tasks prior to\nmeta-embedding. We show that normalization methods that account for this such\nas cosine and KL-divergence objectives outperform meta-embedding trained on\nstandard $\\ell_1$ and $\\ell_2$ loss on \\textit{defacto} word similarity and\nrelatedness datasets and find it outperforms existing meta-learning strategies.", "published": "2018-08-13 17:20:20", "link": "http://arxiv.org/abs/1808.04334v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Murmur Detection Using Parallel Recurrent & Convolutional Neural\n  Networks", "abstract": "In this article, we propose a novel technique for classification of the\nMurmurs in heart sound. We introduce a novel deep neural network architecture\nusing parallel combination of the Recurrent Neural Network (RNN) based\nBidirectional Long Short-Term Memory (BiLSTM) & Convolutional Neural Network\n(CNN) to learn visual and time-dependent characteristics of Murmur in PCG\nwaveform. Set of acoustic features are presented to our proposed deep neural\nnetwork to discriminate between Normal and Murmur class. The proposed method\nwas evaluated on a large dataset using 5-fold cross-validation, resulting in a\nsensitivity and specificity of 96 +- 0.6 % , 100 +- 0 % respectively and F1\nScore of 98 +- 0.3 %.", "published": "2018-08-13 19:21:41", "link": "http://arxiv.org/abs/1808.04411v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "ACVAE-VC: Non-parallel many-to-many voice conversion with auxiliary\n  classifier variational autoencoder", "abstract": "This paper proposes a non-parallel many-to-many voice conversion (VC) method\nusing a variant of the conditional variational autoencoder (VAE) called an\nauxiliary classifier VAE (ACVAE). The proposed method has three key features.\nFirst, it adopts fully convolutional architectures to construct the encoder and\ndecoder networks so that the networks can learn conversion rules that capture\ntime dependencies in the acoustic feature sequences of source and target\nspeech. Second, it uses an information-theoretic regularization for the model\ntraining to ensure that the information in the attribute class label will not\nbe lost in the conversion process. With regular CVAEs, the encoder and decoder\nare free to ignore the attribute class label input. This can be problematic\nsince in such a situation, the attribute class label will have little effect on\ncontrolling the voice characteristics of input speech at test time. Such\nsituations can be avoided by introducing an auxiliary classifier and training\nthe encoder and decoder so that the attribute classes of the decoder outputs\nare correctly predicted by the classifier. Third, it avoids producing\nbuzzy-sounding speech at test time by simply transplanting the spectral details\nof the input speech into its converted version. Subjective evaluation\nexperiments revealed that this simple method worked reasonably well in a\nnon-parallel many-to-many speaker identity conversion task.", "published": "2018-08-13 23:31:01", "link": "http://arxiv.org/abs/1808.05092v3", "categories": ["stat.ML", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "stat.ML"}
