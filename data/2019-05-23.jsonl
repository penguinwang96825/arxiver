{"title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy\n  Lifting, the Rest Can Be Pruned", "abstract": "Multi-head self-attention is a key component of the Transformer, a\nstate-of-the-art architecture for neural machine translation. In this work we\nevaluate the contribution made by individual attention heads in the encoder to\nthe overall performance of the model and analyze the roles played by them. We\nfind that the most important and confident heads play consistent and often\nlinguistically-interpretable roles. When pruning heads using a method based on\nstochastic gates and a differentiable relaxation of the L0 penalty, we observe\nthat specialized heads are last to be pruned. Our novel pruning method removes\nthe vast majority of heads without seriously affecting performance. For\nexample, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads\nresults in a drop of only 0.15 BLEU.", "published": "2019-05-23 01:13:24", "link": "http://arxiv.org/abs/1905.09418v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GWU NLP Lab at SemEval-2019 Task 3: EmoContext: Effective Contextual\n  Information in Models for Emotion Detection in Sentence-level in a Multigenre\n  Corpus", "abstract": "In this paper we present an emotion classifier model submitted to the\nSemEval-2019 Task 3: EmoContext. The task objective is to classify emotion\n(i.e. happy, sad, angry) in a 3-turn conversational data set. We formulate the\ntask as a classification problem and introduce a Gated Recurrent Neural Network\n(GRU) model with attention layer, which is bootstrapped with contextual\ninformation and trained with a multigenre corpus. We utilize different word\nembeddings to empirically select the most suited one to represent our features.\nWe train the model with a multigenre emotion corpus to leverage using all\navailable training sets to bootstrap the results. We achieved overall %56.05\nf1-score and placed 144.", "published": "2019-05-23 02:52:10", "link": "http://arxiv.org/abs/1905.09439v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MCScript2.0: A Machine Comprehension Corpus Focused on Script Events and\n  Participants", "abstract": "We introduce MCScript2.0, a machine comprehension corpus for the end-to-end\nevaluation of script knowledge. MCScript2.0 contains approx. 20,000 questions\non approx. 3,500 texts, crowdsourced based on a new collection process that\nresults in challenging questions. Half of the questions cannot be answered from\nthe reading texts, but require the use of commonsense and, in particular,\nscript knowledge. We give a thorough analysis of our corpus and show that while\nthe task is not challenging to humans, existing machine comprehension models\nfail to perform well on the data, even if they make use of a commonsense\nknowledge base. The dataset is available at\nhttp://www.sfb1102.uni-saarland.de/?page_id=2582", "published": "2019-05-23 08:33:56", "link": "http://arxiv.org/abs/1905.09531v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fair is Better than Sensational:Man is to Doctor as Woman is to Doctor", "abstract": "Analogies such as \"man is to king as woman is to X\" are often used to\nillustrate the amazing power of word embeddings. Concurrently, they have also\nbeen used to expose how strongly human biases are encoded in vector spaces\nbuilt on natural language, like \"man is to computer programmer as woman is to\nhomemaker\". Recent work has shown that analogies are in fact not such a\ndiagnostic for bias, and other methods have been proven to be more apt to the\ntask. However, beside the intrinsic problems with the analogy task as a bias\ndetection tool, in this paper we show that a series of issues related to how\nanalogies have been implemented and used might have yielded a distorted picture\nof bias in word embeddings. Human biases are present in word embeddings and\nneed to be addressed. Analogies, though, are probably not the right tool to do\nso. Also, the way they have been most often used has exacerbated some possibly\nnon-existing biases and perhaps hid others. Because they are still widely\npopular, and some of them have become classics within and outside the NLP\ncommunity, we deem it important to provide a series of clarifications that\nshould put well-known, and potentially new cases into the right perspective.", "published": "2019-05-23 18:43:59", "link": "http://arxiv.org/abs/1905.09866v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LMF Reloaded", "abstract": "Lexical Markup Framework (LMF) or ISO 24613 [1] is a de jure standard that\nprovides a framework for modelling and encoding lexical information in\nretrodigitised print dictionaries and NLP lexical databases. An in-depth review\nis currently underway within the standardisation subcommittee ,\nISO-TC37/SC4/WG4, to find a more modular, flexible and durable follow up to the\noriginal LMF standard published in 2008. In this paper we will present some of\nthe major improvements which have so far been implemented in the new version of\nLMF.", "published": "2019-05-23 14:48:39", "link": "http://arxiv.org/abs/1906.02136v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Embedding Bi-Vector Models for Symmetric Relation", "abstract": "Knowledge graph embedding (KGE) models have been proposed to improve the\nperformance of knowledge graph reasoning. However, there is a general\nphenomenon in most of KGEs, as the training progresses, the symmetric relations\ntend to zero vector, if the symmetric triples ratio is high enough in the\ndataset. This phenomenon causes subsequent tasks, e.g. link prediction etc., of\nsymmetric relations to fail. The root cause of the problem is that KGEs do not\nutilize the semantic information of symmetric relations. We propose KGE\nbi-vector models, which represent the symmetric relations as vector pair,\nsignificantly increasing the processing capability of the symmetry relations.\nWe generate the benchmark datasets based on FB15k and WN18 by completing the\nsymmetric relation triples to verify models. The experiment results of our\nmodels clearly affirm the effectiveness and superiority of our models against\nbaseline.", "published": "2019-05-23 09:44:50", "link": "http://arxiv.org/abs/1905.09557v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "An Investigation of Transfer Learning-Based Sentiment Analysis in\n  Japanese", "abstract": "Text classification approaches have usually required task-specific model\narchitectures and huge labeled datasets. Recently, thanks to the rise of\ntext-based transfer learning techniques, it is possible to pre-train a language\nmodel in an unsupervised manner and leverage them to perform effective on\ndownstream tasks. In this work we focus on Japanese and show the potential use\nof transfer learning techniques in text classification. Specifically, we\nperform binary and multi-class sentiment classification on the Rakuten product\nreview and Yahoo movie review datasets. We show that transfer learning-based\napproaches perform better than task-specific models trained on 3 times as much\ndata. Furthermore, these approaches perform just as well for language modeling\npre-trained on only 1/30 of the data. We release our pre-trained models and\ncode as open source.", "published": "2019-05-23 13:24:15", "link": "http://arxiv.org/abs/1905.09642v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Misspelling Oblivious Word Embeddings", "abstract": "In this paper we present a method to learn word embeddings that are resilient\nto misspellings. Existing word embeddings have limited applicability to\nmalformed texts, which contain a non-negligible amount of out-of-vocabulary\nwords. We propose a method combining FastText with subwords and a supervised\ntask of learning misspelling patterns. In our method, misspellings of each word\nare embedded close to their correct variants. We train these embeddings on a\nnew dataset we are releasing publicly. Finally, we experimentally show the\nadvantages of this approach on both intrinsic and extrinsic NLP tasks using\npublic test sets.", "published": "2019-05-23 16:28:08", "link": "http://arxiv.org/abs/1905.09755v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Theme-aware generation model for chinese lyrics", "abstract": "With rapid development of neural networks, deep-learning has been extended to\nvarious natural language generation fields, such as machine translation,\ndialogue generation and even literature creation. In this paper, we propose a\ntheme-aware language generation model for Chinese music lyrics, which improves\nthe theme-connectivity and coherence of generated paragraphs greatly. A\nmulti-channel sequence-to-sequence (seq2seq) model encodes themes and previous\nsentences as global and local contextual information. Moreover, attention\nmechanism is incorporated for sequence decoding, enabling to fuse context into\npredicted next texts. To prepare appropriate train corpus, LDA (Latent\nDirichlet Allocation) is applied for theme extraction. Generated lyrics is\ngrammatically correct and semantically coherent with selected themes, which\noffers a valuable modelling method in other fields including multi-turn\nchatbots, long paragraph generation and etc.", "published": "2019-05-23 08:50:15", "link": "http://arxiv.org/abs/1906.02134v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep learning based mood tagging for Chinese song lyrics", "abstract": "Nowadays, listening music has been and will always be an indispensable part\nof our daily life. In recent years, sentiment analysis of music has been widely\nused in the information retrieval systems, personalized recommendation systems\nand so on. Due to the development of deep learning, this paper commits to find\nan effective approach for mood tagging of Chinese song lyrics. To achieve this\ngoal, both machine-learning and deep-learning models have been studied and\ncompared. Eventually, a CNN-based model with pre-trained word embedding has\nbeen demonstrated to effectively extract the distribution of emotional features\nof Chinese lyrics, with at least 15 percentage points higher than traditional\nmachine-learning methods (i.e. TF-IDF+SVM and LIWC+SVM), and 7 percentage\npoints higher than other deep-learning models (i.e. RNN, LSTM). In this paper,\nmore than 160,000 lyrics corpus has been leveraged for pre-training word\nembedding for mood tagging boost.", "published": "2019-05-23 09:12:59", "link": "http://arxiv.org/abs/1906.02135v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-hop Reading Comprehension via Deep Reinforcement Learning based\n  Document Traversal", "abstract": "Reading Comprehension has received significant attention in recent years as\nhigh quality Question Answering (QA) datasets have become available. Despite\nstate-of-the-art methods achieving strong overall accuracy, Multi-Hop (MH)\nreasoning remains particularly challenging. To address MH-QA specifically, we\npropose a Deep Reinforcement Learning based method capable of learning\nsequential reasoning across large collections of documents so as to pass a\nquery-aware, fixed-size context subset to existing models for answer\nextraction. Our method is comprised of two stages: a linker, which decomposes\nthe provided support documents into a graph of sentences, and an extractor,\nwhich learns where to look based on the current question and already-visited\nsentences. The result of the linker is a novel graph structure at the sentence\nlevel that preserves logical flow while still allowing rapid movement between\ndocuments. Importantly, we demonstrate that the sparsity of the resultant graph\nis invariant to context size. This translates to fewer decisions required from\nthe Deep-RL trained extractor, allowing the system to scale effectively to\nlarge collections of documents.\n  The importance of sequential decision making in the document traversal step\nis demonstrated by comparison to standard IE methods, and we additionally\nintroduce a BM25-based IR baseline that retrieves documents relevant to the\nquery only. We examine the integration of our method with existing models on\nthe recently proposed QAngaroo benchmark and achieve consistent increases in\naccuracy across the board, as well as a 2-3x reduction in training time.", "published": "2019-05-23 02:32:34", "link": "http://arxiv.org/abs/1905.09438v1", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "An Efficient Approach for Super and Nested Term Indexing and Retrieval", "abstract": "This paper describes a new approach, called Terminological Bucket Indexing\n(TBI), for efficient indexing and retrieval of both nested and super terms\nusing a single method. We propose a hybrid data structure for facilitating\nfaster indexing building. An evaluation of our approach with respect to widely\nused existing approaches on several publicly available dataset is provided.\nCompared to Trie based approaches, TBI provides comparable performance on\nnested term retrieval and far superior performance on super term retrieval.\nCompared to traditional hash table, TBI needs 80\\% less time for indexing.", "published": "2019-05-23 16:33:30", "link": "http://arxiv.org/abs/1905.09761v1", "categories": ["cs.DS", "cs.CL", "cs.IR"], "primary_category": "cs.DS"}
{"title": "Copy this Sentence", "abstract": "Attention is an operation that selects some largest element from some set,\nwhere the notion of largest is defined elsewhere. Applying this operation to\nsequence to sequence mapping results in significant improvements to the task at\nhand. In this paper we provide the mathematical definition of attention and\nexamine its application to sequence to sequence models. We highlight the exact\ncorrespondences between machine learning implementations of attention and our\nmathematical definition. We provide clear evidence of effectiveness of\nattention mechanisms evaluating models with varying degrees of attention on a\nvery simple task: copying a sentence. We find that models that make greater use\nof attention perform much better on sequence to sequence mapping tasks,\nconverge faster and are more stable.", "published": "2019-05-23 18:25:35", "link": "http://arxiv.org/abs/1905.09856v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Why Didn't You Listen to Me? Comparing User Control of Human-in-the-Loop\n  Topic Models", "abstract": "To address the lack of comparative evaluation of Human-in-the-Loop Topic\nModeling (HLTM) systems, we implement and evaluate three contrasting HLTM\nmodeling approaches using simulation experiments. These approaches extend\npreviously proposed frameworks, including constraints and informed prior-based\nmethods. Users should have a sense of control in HLTM systems, so we propose a\ncontrol metric to measure whether refinement operations' results match users'\nexpectations. Informed prior-based methods provide better control than\nconstraints, but constraints yield higher quality topics.", "published": "2019-05-23 18:40:57", "link": "http://arxiv.org/abs/1905.09864v2", "categories": ["cs.CL", "cs.HC", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Training language GANs from Scratch", "abstract": "Generative Adversarial Networks (GANs) enjoy great success at image\ngeneration, but have proven difficult to train in the domain of natural\nlanguage. Challenges with gradient estimation, optimization instability, and\nmode collapse have lead practitioners to resort to maximum likelihood\npre-training, followed by small amounts of adversarial fine-tuning. The\nbenefits of GAN fine-tuning for language generation are unclear, as the\nresulting models produce comparable or worse samples than traditional language\nmodels. We show it is in fact possible to train a language GAN from scratch --\nwithout maximum likelihood pre-training. We combine existing techniques such as\nlarge batch sizes, dense rewards and discriminator regularization to stabilize\nand improve language GANs. The resulting model, ScratchGAN, performs comparably\nto maximum likelihood training on EMNLP2017 News and WikiText-103 corpora\naccording to quality and diversity metrics.", "published": "2019-05-23 21:01:24", "link": "http://arxiv.org/abs/1905.09922v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Exploring Diseases and Syndromes in Neurology Case Reports from 1955 to\n  2017 with Text Mining", "abstract": "Background: A large number of neurology case reports have been published, but\nit is a challenging task for human medical experts to explore all of these\npublications. Text mining offers a computational approach to investigate\nneurology literature and capture meaningful patterns. The overarching goal of\nthis study is to provide a new perspective on case reports of neurological\ndisease and syndrome analysis over the last six decades using text mining.\n  Methods: We extracted diseases and syndromes (DsSs) from more than 65,000\nneurology case reports from 66 journals in PubMed over the last six decades\nfrom 1955 to 2017. Text mining was applied to reports on the detected DsSs to\ninvestigate high-frequency DsSs, categorize them, and explore the linear trends\nover the 63-year time frame.\n  Results: The text mining methods explored high-frequency neurologic DsSs and\ntheir trends and the relationships between them from 1955 to 2017. We detected\nmore than 18,000 unique DsSs and found 10 categories of neurologic DsSs. While\nthe trend analysis showed the increasing trends in the case reports for top-10\nhigh-frequency DsSs, the categories had mixed trends.\n  Conclusion: Our study provided new insights into the application of text\nmining methods to investigate DsSs in a large number of medical case reports\nthat occur over several decades. The proposed approach can be used to provide a\nmacro level analysis of medical literature by discovering interesting patterns\nand tracking them over several years to help physicians explore these case\nreports more efficiently.", "published": "2019-05-23 17:38:06", "link": "http://arxiv.org/abs/1906.03183v1", "categories": ["q-bio.QM", "cs.CL", "cs.IR", "stat.AP"], "primary_category": "q-bio.QM"}
{"title": "A Perceptual Weighting Filter Loss for DNN Training in Speech\n  Enhancement", "abstract": "Single-channel speech enhancement with deep neural networks (DNNs) has shown\npromising performance and is thus intensively being studied. In this paper,\ninstead of applying the mean squared error (MSE) as the loss function during\nDNN training for speech enhancement, we design a perceptual weighting filter\nloss motivated by the weighting filter as it is employed in\nanalysis-by-synthesis speech coding, e.g., in code-excited linear prediction\n(CELP). The experimental results show that the proposed simple loss function\nimproves the speech enhancement performance compared to a reference DNN with\nMSE loss in terms of perceptual quality and noise attenuation. The proposed\nloss function can be advantageously applied to an existing DNN-based speech\nenhancement system, without modification of the DNN topology for speech\nenhancement. The source code for the proposed approach is made available.", "published": "2019-05-23 16:24:19", "link": "http://arxiv.org/abs/1905.09754v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
