{"title": "Zero-Label Prompt Selection", "abstract": "Natural language prompts have been shown to facilitate cross-task\ngeneralization for large language models. However, with no or limited labeled\nexamples, the cross-task performance is highly sensitive to the choice of\nprompts, while selecting a high-performing prompt is challenging given the\nscarcity of labels. To address the issue, we propose a Zero-Label Prompt\nSelection (ZPS) method that selects prompts without any labeled data or\ngradient update. Specifically, given the candidate human-written prompts for a\ntask, ZPS labels a set of unlabeled data with a prompt ensemble and uses the\npseudo-labels for prompt selection. Experiments show that ZPS improves over\nprior methods by a sizeable margin in zero-label performance. We also extend\nZPS to a few-shot setting and show its advantages over strong baselines such as\nprompt tuning and model tuning.", "published": "2022-11-09 04:13:31", "link": "http://arxiv.org/abs/2211.04668v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Extractive Summarization with Heterogeneous Graph\n  Embeddings for Chinese Document", "abstract": "In the scenario of unsupervised extractive summarization, learning\nhigh-quality sentence representations is essential to select salient sentences\nfrom the input document. Previous studies focus more on employing statistical\napproaches or pre-trained language models (PLMs) to extract sentence\nembeddings, while ignoring the rich information inherent in the heterogeneous\ntypes of interaction between words and sentences. In this paper, we are the\nfirst to propose an unsupervised extractive summarizaiton method with\nheterogeneous graph embeddings (HGEs) for Chinese document. A heterogeneous\ntext graph is constructed to capture different granularities of interactions by\nincorporating graph structural information. Moreover, our proposed graph is\ngeneral and flexible where additional nodes such as keywords can be easily\nintegrated. Experimental results demonstrate that our method consistently\noutperforms the strong baseline in three summarization datasets.", "published": "2022-11-09 06:07:31", "link": "http://arxiv.org/abs/2211.04698v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FF2: A Feature Fusion Two-Stream Framework for Punctuation Restoration", "abstract": "To accomplish punctuation restoration, most existing methods focus on\nintroducing extra information (e.g., part-of-speech) or addressing the class\nimbalance problem. Recently, large-scale transformer-based pre-trained language\nmodels (PLMS) have been utilized widely and obtained remarkable success.\nHowever, the PLMS are trained on the large dataset with marks, which may not\nfit well with the small dataset without marks, causing the convergence to be\nnot ideal. In this study, we propose a Feature Fusion two-stream framework\n(FF2) to bridge the gap. Specifically, one stream leverages a pre-trained\nlanguage model to capture the semantic feature, while another auxiliary module\ncaptures the feature at hand. We also modify the computation of multi-head\nattention to encourage communication among heads. Then, two features with\ndifferent perspectives are aggregated to fuse information and enhance context\nawareness. Without additional data, the experimental results on the popular\nbenchmark IWSLT demonstrate that FF2 achieves new SOTA performance, which\nverifies that our approach is effective.", "published": "2022-11-09 06:18:17", "link": "http://arxiv.org/abs/2211.04699v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Novel Chapter Abstractive Summarization using Spinal Tree Aware\n  Sub-Sentential Content Selection", "abstract": "Summarizing novel chapters is a difficult task due to the input length and\nthe fact that sentences that appear in the desired summaries draw content from\nmultiple places throughout the chapter. We present a pipelined\nextractive-abstractive approach where the extractive step filters the content\nthat is passed to the abstractive component. Extremely lengthy input also\nresults in a highly skewed dataset towards negative instances for extractive\nsummarization; we thus adopt a margin ranking loss for extraction to encourage\nseparation between positive and negative examples. Our extraction component\noperates at the constituent level; our approach to this problem enriches the\ntext with spinal tree information which provides syntactic context (in the form\nof constituents) to the extraction model. We show an improvement of 3.71\nRouge-1 points over best results reported in prior work on an existing novel\nchapter dataset.", "published": "2022-11-09 14:12:09", "link": "http://arxiv.org/abs/2211.04903v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DoSA : A System to Accelerate Annotations on Business Documents with\n  Human-in-the-Loop", "abstract": "Business documents come in a variety of structures, formats and information\nneeds which makes information extraction a challenging task. Due to these\nvariations, having a document generic model which can work well across all\ntypes of documents and for all the use cases seems far-fetched. For\ndocument-specific models, we would need customized document-specific labels. We\nintroduce DoSA (Document Specific Automated Annotations), which helps\nannotators in generating initial annotations automatically using our novel\nbootstrap approach by leveraging document generic datasets and models. These\ninitial annotations can further be reviewed by a human for correctness. An\ninitial document-specific model can be trained and its inference can be used as\nfeedback for generating more automated annotations. These automated annotations\ncan be reviewed by human-in-the-loop for the correctness and a new improved\nmodel can be trained using the current model as pre-trained model before going\nfor the next iteration. In this paper, our scope is limited to Form like\ndocuments due to limited availability of generic annotated datasets, but this\nidea can be extended to a variety of other documents as more datasets are\nbuilt. An open-source ready-to-use implementation is made available on GitHub\nhttps://github.com/neeleshkshukla/DoSA.", "published": "2022-11-09 15:04:07", "link": "http://arxiv.org/abs/2211.04934v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating and Improving Context Attention Distribution on Multi-Turn\n  Response Generation using Self-Contained Distractions", "abstract": "Despite the rapid progress of open-domain generation-based conversational\nagents, most deployed systems treat dialogue contexts as single-turns, while\nsystems dealing with multi-turn contexts are less studied. There is a lack of a\nreliable metric for evaluating multi-turn modelling, as well as an effective\nsolution for improving it. In this paper, we focus on an essential component of\nmulti-turn generation-based conversational agents: context attention\ndistribution, i.e. how systems distribute their attention on dialogue's\ncontext. For evaluation of this component, We introduce a novel\nattention-mechanism-based metric: DAS ratio. To improve performance on this\ncomponent, we propose an optimization strategy that employs self-contained\ndistractions. Our experiments on the Ubuntu chatlogs dataset show that models\nwith comparable perplexity can be distinguished by their ability on context\nattention distribution. Our proposed optimization strategy improves both\nnon-hierarchical and hierarchical models on the proposed metric by about 10%\nfrom baselines.", "published": "2022-11-09 15:12:20", "link": "http://arxiv.org/abs/2211.04943v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discord Questions: A Computational Approach To Diversity Analysis in\n  News Coverage", "abstract": "There are many potential benefits to news readers accessing diverse sources.\nModern news aggregators do the hard work of organizing the news, offering\nreaders a plethora of source options, but choosing which source to read remains\nchallenging. We propose a new framework to assist readers in identifying source\ndifferences and gaining an understanding of news coverage diversity. The\nframework is based on the generation of Discord Questions: questions with a\ndiverse answer pool, explicitly illustrating source differences. To assemble a\nprototype of the framework, we focus on two components: (1) discord question\ngeneration, the task of generating questions answered differently by sources,\nfor which we propose an automatic scoring method, and create a model that\nimproves performance from current question generation (QG) methods by 5%, (2)\nanswer consolidation, the task of grouping answers to a question that are\nsemantically similar, for which we collect data and repurpose a method that\nachieves 81% balanced accuracy on our realistic test set. We illustrate the\nframework's feasibility through a prototype interface. Even though model\nperformance at discord QG still lags human performance by more than 15%,\ngenerated questions are judged to be more interesting than factoid questions\nand can reveal differences in the level of detail, sentiment, and reasoning of\nsources in news coverage.", "published": "2022-11-09 16:37:55", "link": "http://arxiv.org/abs/2211.05007v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Languages Unintelligible to Multilingual Models through Local\n  Structure Probes", "abstract": "Providing better language tools for low-resource and endangered languages is\nimperative for equitable growth. Recent progress with massively multilingual\npretrained models has proven surprisingly effective at performing zero-shot\ntransfer to a wide variety of languages. However, this transfer is not\nuniversal, with many languages not currently understood by multilingual\napproaches. It is estimated that only 72 languages possess a \"small set of\nlabeled datasets\" on which we could test a model's performance, the vast\nmajority of languages not having the resources available to simply evaluate\nperformances on. In this work, we attempt to clarify which languages do and do\nnot currently benefit from such transfer. To that end, we develop a general\napproach that requires only unlabelled text to detect which languages are not\nwell understood by a cross-lingual model. Our approach is derived from the\nhypothesis that if a model's understanding is insensitive to perturbations to\ntext in a language, it is likely to have a limited understanding of that\nlanguage. We construct a cross-lingual sentence similarity task to evaluate our\napproach empirically on 350, primarily low-resource, languages.", "published": "2022-11-09 16:45:16", "link": "http://arxiv.org/abs/2211.05015v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Local Structure Matters Most in Most Languages", "abstract": "Many recent perturbation studies have found unintuitive results on what does\nand does not matter when performing Natural Language Understanding (NLU) tasks\nin English. Coding properties, such as the order of words, can often be removed\nthrough shuffling without impacting downstream performances. Such insight may\nbe used to direct future research into English NLP models. As many improvements\nin multilingual settings consist of wholesale adaptation of English approaches,\nit is important to verify whether those studies replicate or not in\nmultilingual settings. In this work, we replicate a study on the importance of\nlocal structure, and the relative unimportance of global structure, in a\nmultilingual setting. We find that the phenomenon observed on the English\nlanguage broadly translates to over 120 languages, with a few caveats.", "published": "2022-11-09 16:58:44", "link": "http://arxiv.org/abs/2211.05025v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MACSum: Controllable Summarization with Mixed Attributes", "abstract": "Controllable summarization allows users to generate customized summaries with\nspecified attributes. However, due to the lack of designated annotations of\ncontrolled summaries, existing works have to craft pseudo datasets by adapting\ngeneric summarization benchmarks. Furthermore, most research focuses on\ncontrolling single attributes individually (e.g., a short summary or a highly\nabstractive summary) rather than controlling a mix of attributes together\n(e.g., a short and highly abstractive summary). In this paper, we propose\nMACSum, the first human-annotated summarization dataset for controlling mixed\nattributes. It contains source texts from two domains, news articles and\ndialogues, with human-annotated summaries controlled by five designed\nattributes (Length, Extractiveness, Specificity, Topic, and Speaker). We\npropose two simple and effective parameter-efficient approaches for the new\ntask of mixed controllable summarization based on hard prompt tuning and soft\nprefix tuning. Results and analysis demonstrate that hard prompt models yield\nthe best performance on all metrics and human evaluations. However,\nmixed-attribute control is still challenging for summarization tasks. Our\ndataset and code are available at https://github.com/psunlpgroup/MACSum.", "published": "2022-11-09 17:17:37", "link": "http://arxiv.org/abs/2211.05041v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model", "abstract": "Large language models (LLMs) have been shown to be able to perform new tasks\nbased on a few demonstrations or natural language instructions. While these\ncapabilities have led to widespread adoption, most LLMs are developed by\nresource-rich organizations and are frequently kept from the public. As a step\ntowards democratizing this powerful technology, we present BLOOM, a\n176B-parameter open-access language model designed and built thanks to a\ncollaboration of hundreds of researchers. BLOOM is a decoder-only Transformer\nlanguage model that was trained on the ROOTS corpus, a dataset comprising\nhundreds of sources in 46 natural and 13 programming languages (59 in total).\nWe find that BLOOM achieves competitive performance on a wide variety of\nbenchmarks, with stronger results after undergoing multitask prompted\nfinetuning. To facilitate future research and applications using LLMs, we\npublicly release our models and code under the Responsible AI License.", "published": "2022-11-09 18:48:09", "link": "http://arxiv.org/abs/2211.05100v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Zero-shot Event Extraction with Context-Definition Alignment", "abstract": "Event extraction (EE) is the task of identifying interested event mentions\nfrom text. Conventional efforts mainly focus on the supervised setting.\nHowever, these supervised models cannot generalize to event types out of the\npre-defined ontology. To fill this gap, many efforts have been devoted to the\nzero-shot EE problem. This paper follows the trend of modeling event-type\nsemantics but moves one step further. We argue that using the static embedding\nof the event type name might not be enough because a single word could be\nambiguous, and we need a sentence to define the type semantics accurately. To\nmodel the definition semantics, we use two separate transformer models to\nproject the contextualized event mentions and corresponding definitions into\nthe same embedding space and then minimize their embedding distance via\ncontrastive learning. On top of that, we also propose a warming phase to help\nthe model learn the minor difference between similar definitions. We name our\napproach Zero-shot Event extraction with Definition (ZED). Experiments on the\nMAVEN dataset show that our model significantly outperforms all previous\nzero-shot EE methods with fast inference speed due to the disjoint design.\nFurther experiments also show that ZED can be easily applied to the few-shot\nsetting when the annotation is available and consistently outperforms baseline\nsupervised methods.", "published": "2022-11-09 19:06:22", "link": "http://arxiv.org/abs/2211.05156v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Reasoning-Aware Explainable VQA", "abstract": "The domain of joint vision-language understanding, especially in the context\nof reasoning in Visual Question Answering (VQA) models, has garnered\nsignificant attention in the recent past. While most of the existing VQA models\nfocus on improving the accuracy of VQA, the way models arrive at an answer is\noftentimes a black box. As a step towards making the VQA task more explainable\nand interpretable, our method is built upon the SOTA VQA framework by\naugmenting it with an end-to-end explanation generation module. In this paper,\nwe investigate two network architectures, including Long Short-Term Memory\n(LSTM) and Transformer decoder, as the explanation generator. Our method\ngenerates human-readable textual explanations while maintaining SOTA VQA\naccuracy on the GQA-REX (77.49%) and VQA-E (71.48%) datasets. Approximately\n65.16% of the generated explanations are approved by humans as valid. Roughly\n60.5% of the generated explanations are valid and lead to the correct answers.", "published": "2022-11-09 20:47:45", "link": "http://arxiv.org/abs/2211.05190v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DeepE: a deep neural network for knowledge graph embedding", "abstract": "Recently, neural network based methods have shown their power in learning\nmore expressive features on the task of knowledge graph embedding (KGE).\nHowever, the performance of deep methods often falls behind the shallow ones on\nsimple graphs. One possible reason is that deep models are difficult to train,\nwhile shallow models might suffice for accurately representing the structure of\nthe simple KGs.\n  In this paper, we propose a neural network based model, named DeepE, to\naddress the problem, which stacks multiple building blocks to predict the tail\nentity based on the head entity and the relation. Each building block is an\naddition of a linear and a non-linear function. The stacked building blocks are\nequivalent to a group of learning functions with different non-linear depth.\nHence, DeepE allows deep functions to learn deep features, and shallow\nfunctions to learn shallow features. Through extensive experiments, we find\nDeepE outperforms other state-of-the-art baseline methods. A major advantage of\nDeepE is the robustness. DeepE achieves a Mean Rank (MR) score that is 6%, 30%,\n65% lower than the best baseline methods on FB15k-237, WN18RR and YAGO3-10. Our\ndesign makes it possible to train much deeper networks on KGE, e.g. 40 layers\non FB15k-237, and without scarifying precision on simple relations.", "published": "2022-11-09 00:43:36", "link": "http://arxiv.org/abs/2211.04620v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Method to Judge the Style of Classical Poetry Based on Pre-trained\n  Model", "abstract": "One of the important topics in the research field of Chinese classical poetry\nis to analyze the poetic style. By examining the relevant works of previous\ndynasties, researchers judge a poetic style mostly by their subjective\nfeelings, and refer to the previous evaluations that have become a certain\nconclusion. Although this judgment method is often effective, there may be some\nerrors. This paper builds the most perfect data set of Chinese classical poetry\nat present, trains a BART-poem pre -trained model on this data set, and puts\nforward a generally applicable poetry style judgment method based on this\nBART-poem model, innovatively introduces in-depth learning into the field of\ncomputational stylistics, and provides a new research method for the study of\nclassical poetry. This paper attempts to use this method to solve the problem\nof poetry style identification in the Tang and Song Dynasties, and takes the\npoetry schools that are considered to have a relatively clear and consistent\npoetic style, such as the Hongzheng Qizi and Jiajing Qizi, Jiangxi poetic\nschool and Tongguang poetic school, as the research object, and takes the poems\nof their representative poets for testing. Experiments show that the judgment\nresults of the tested poetry work made by the model are basically consistent\nwith the conclusions given by critics of previous dynasties, verify some\navant-garde judgments of Mr. Qian Zhongshu, and better solve the task of poetry\nstyle recognition in the Tang and Song dynasties.", "published": "2022-11-09 03:11:15", "link": "http://arxiv.org/abs/2211.04657v1", "categories": ["cs.CL", "cs.AI", "J.5; I.2.7"], "primary_category": "cs.CL"}
{"title": "Few-Shot Character Understanding in Movies as an Assessment to\n  Meta-Learning of Theory-of-Mind", "abstract": "When reading a story, humans can quickly understand new fictional characters\nwith a few observations, mainly by drawing analogies to fictional and real\npeople they already know. This reflects the few-shot and meta-learning essence\nof humans' inference of characters' mental states, i.e., theory-of-mind (ToM),\nwhich is largely ignored in existing research. We fill this gap with a novel\nNLP dataset, ToM-in-AMC, the first assessment of machines' meta-learning of ToM\nin a realistic narrative understanding scenario. Our dataset consists of ~1,000\nparsed movie scripts, each corresponding to a few-shot character understanding\ntask that requires models to mimic humans' ability of fast digesting characters\nwith a few starting scenes in a new movie.\n  We propose a novel ToM prompting approach designed to explicitly assess the\ninfluence of multiple ToM dimensions. It surpasses existing baseline models,\nunderscoring the significance of modeling multiple ToM dimensions for our task.\nOur extensive human study verifies that humans are capable of solving our\nproblem by inferring characters' mental states based on their previously seen\nmovies. In comparison, our systems based on either state-of-the-art large\nlanguage models (GPT-4) or meta-learning algorithms lags >20% behind,\nhighlighting a notable limitation in existing approaches' ToM capabilities.", "published": "2022-11-09 05:06:12", "link": "http://arxiv.org/abs/2211.04684v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Nested Named Entity Recognition from Medical Texts: An Adaptive Shared\n  Network Architecture with Attentive CRF", "abstract": "Recognizing useful named entities plays a vital role in medical information\nprocessing, which helps drive the development of medical area research. Deep\nlearning methods have achieved good results in medical named entity recognition\n(NER). However, we find that existing methods face great challenges when\ndealing with the nested named entities. In this work, we propose a novel\nmethod, referred to as ASAC, to solve the dilemma caused by the nested\nphenomenon, in which the core idea is to model the dependency between different\ncategories of entity recognition. The proposed method contains two key modules:\nthe adaptive shared (AS) part and the attentive conditional random field (ACRF)\nmodule. The former part automatically assigns adaptive weights across each task\nto achieve optimal recognition accuracy in the multi-layer network. The latter\nmodule employs the attention operation to model the dependency between\ndifferent entities. In this way, our model could learn better entity\nrepresentations by capturing the implicit distinctions and relationships\nbetween different categories of entities. Extensive experiments on public\ndatasets verify the effectiveness of our method. Besides, we also perform\nablation analyses to deeply understand our methods.", "published": "2022-11-09 09:23:56", "link": "http://arxiv.org/abs/2211.04759v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mask More and Mask Later: Efficient Pre-training of Masked Language\n  Models by Disentangling the [MASK] Token", "abstract": "The pre-training of masked language models (MLMs) consumes massive\ncomputation to achieve good results on downstream NLP tasks, resulting in a\nlarge carbon footprint. In the vanilla MLM, the virtual tokens, [MASK]s, act as\nplaceholders and gather the contextualized information from unmasked tokens to\nrestore the corrupted information. It raises the question of whether we can\nappend [MASK]s at a later layer, to reduce the sequence length for earlier\nlayers and make the pre-training more efficient. We show: (1) [MASK]s can\nindeed be appended at a later layer, being disentangled from the word\nembedding; (2) The gathering of contextualized information from unmasked tokens\ncan be conducted with a few layers. By further increasing the masking rate from\n15% to 50%, we can pre-train RoBERTa-base and RoBERTa-large from scratch with\nonly 78% and 68% of the original computational budget without any degradation\non the GLUE benchmark. When pre-training with the original budget, our method\noutperforms RoBERTa for 6 out of 8 GLUE tasks, on average by 0.4%.", "published": "2022-11-09 14:03:22", "link": "http://arxiv.org/abs/2211.04898v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "miCSE: Mutual Information Contrastive Learning for Low-shot Sentence\n  Embeddings", "abstract": "This paper presents miCSE, a mutual information-based contrastive learning\nframework that significantly advances the state-of-the-art in few-shot sentence\nembedding. The proposed approach imposes alignment between the attention\npattern of different views during contrastive learning. Learning sentence\nembeddings with miCSE entails enforcing the structural consistency across\naugmented views for every sentence, making contrastive self-supervised learning\nmore sample efficient. As a result, the proposed approach shows strong\nperformance in the few-shot learning domain. While it achieves superior results\ncompared to state-of-the-art methods on multiple benchmarks in few-shot\nlearning, it is comparable in the full-shot scenario. This study opens up\navenues for efficient self-supervised learning methods that are more robust\nthan current contrastive methods for sentence embedding.", "published": "2022-11-09 14:57:37", "link": "http://arxiv.org/abs/2211.04928v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Understanding Cross-modal Interactions in V&L Models that Generate Scene\n  Descriptions", "abstract": "Image captioning models tend to describe images in an object-centric way,\nemphasising visible objects. But image descriptions can also abstract away from\nobjects and describe the type of scene depicted. In this paper, we explore the\npotential of a state-of-the-art Vision and Language model, VinVL, to caption\nimages at the scene level using (1) a novel dataset which pairs images with\nboth object-centric and scene descriptions. Through (2) an in-depth analysis of\nthe effect of the fine-tuning, we show (3) that a small amount of curated data\nsuffices to generate scene descriptions without losing the capability to\nidentify object-level concepts in the scene; the model acquires a more holistic\nview of the image compared to when object-centric descriptions are generated.\nWe discuss the parallels between these results and insights from computational\nand cognitive science research on scene perception.", "published": "2022-11-09 15:33:51", "link": "http://arxiv.org/abs/2211.04971v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Creative Writing with an AI-Powered Writing Assistant: Perspectives from\n  Professional Writers", "abstract": "Recent developments in natural language generation (NLG) using neural\nlanguage models have brought us closer than ever to the goal of building\nAI-powered creative writing tools. However, most prior work on human-AI\ncollaboration in the creative writing domain has evaluated new systems with\namateur writers, typically in contrived user studies of limited scope. In this\nwork, we commissioned 13 professional, published writers from a diverse set of\ncreative writing backgrounds to craft stories using Wordcraft, a text editor\nwith built-in AI-powered writing assistance tools. Using interviews and\nparticipant journals, we discuss the potential of NLG to have significant\nimpact in the creative writing domain--especially with respect to\nbrainstorming, generation of story details, world-building, and research\nassistance. Experienced writers, more so than amateurs, typically have\nwell-developed systems and methodologies for writing, as well as distinctive\nvoices and target audiences. Our work highlights the challenges in building for\nthese writers; NLG technologies struggle to preserve style and authorial voice,\nand they lack deep understanding of story contents. In order for AI-powered\nwriting assistants to realize their full potential, it is essential that they\ntake into account the diverse goals and expertise of human writers.", "published": "2022-11-09 17:00:56", "link": "http://arxiv.org/abs/2211.05030v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Improving Performance of Automatic Keyword Extraction (AKE) Methods\n  Using PoS-Tagging and Enhanced Semantic-Awareness", "abstract": "Automatic keyword extraction (AKE) has gained more importance with the\nincreasing amount of digital textual data that modern computing systems\nprocess. It has various applications in information retrieval (IR) and natural\nlanguage processing (NLP), including text summarisation, topic analysis and\ndocument indexing. This paper proposes a simple but effective\npost-processing-based universal approach to improve the performance of any AKE\nmethods, via an enhanced level of semantic-awareness supported by PoS-tagging.\nTo demonstrate the performance of the proposed approach, we considered word\ntypes retrieved from a PoS-tagging step and two representative sources of\nsemantic information - specialised terms defined in one or more\ncontext-dependent thesauri, and named entities in Wikipedia. The above three\nsteps can be simply added to the end of any AKE methods as part of a\npost-processor, which simply re-evaluate all candidate keywords following some\ncontext-specific and semantic-aware criteria. For five state-of-the-art (SOTA)\nAKE methods, our experimental results with 17 selected datasets showed that the\nproposed approach improved their performances both consistently (up to 100% in\nterms of improved cases) and significantly (between 10.2% and 53.8%, with an\naverage of 25.8%, in terms of F1-score and across all five methods), especially\nwhen all the three enhancement steps are used. Our results have profound\nimplications considering the ease to apply our proposed approach to any AKE\nmethods and to further extend it.", "published": "2022-11-09 17:04:13", "link": "http://arxiv.org/abs/2211.05031v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Combining Contrastive Learning and Knowledge Graph Embeddings to develop\n  medical word embeddings for the Italian language", "abstract": "Word embeddings play a significant role in today's Natural Language\nProcessing tasks and applications. While pre-trained models may be directly\nemployed and integrated into existing pipelines, they are often fine-tuned to\nbetter fit with specific languages or domains. In this paper, we attempt to\nimprove available embeddings in the uncovered niche of the Italian medical\ndomain through the combination of Contrastive Learning (CL) and Knowledge Graph\nEmbedding (KGE). The main objective is to improve the accuracy of semantic\nsimilarity between medical terms, which is also used as an evaluation task.\nSince the Italian language lacks medical texts and controlled vocabularies, we\nhave developed a specific solution by combining preexisting CL methods\n(multi-similarity loss, contextualization, dynamic sampling) and the\nintegration of KGEs, creating a new variant of the loss. Although without\nhaving outperformed the state-of-the-art, represented by multilingual models,\nthe obtained results are encouraging, providing a significant leap in\nperformance compared to the starting model, while using a significantly lower\namount of data.", "published": "2022-11-09 17:12:28", "link": "http://arxiv.org/abs/2211.05035v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What is Wrong with Language Models that Can Not Tell a Story?", "abstract": "This paper argues that a deeper understanding of narrative and the successful\ngeneration of longer subjectively interesting texts is a vital bottleneck that\nhinders the progress in modern Natural Language Processing (NLP) and may even\nbe in the whole field of Artificial Intelligence. We demonstrate that there are\nno adequate datasets, evaluation methods, and even operational concepts that\ncould be used to start working on narrative processing.", "published": "2022-11-09 17:24:33", "link": "http://arxiv.org/abs/2211.05044v2", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; J.5"], "primary_category": "cs.CL"}
{"title": "Efficiently Scaling Transformer Inference", "abstract": "We study the problem of efficient generative inference for Transformer\nmodels, in one of its most challenging settings: large deep models, with tight\nlatency targets and long sequence lengths. Better understanding of the\nengineering tradeoffs for inference for large Transformer-based models is\nimportant as use cases of these models are growing rapidly throughout\napplication areas. We develop a simple analytical model for inference\nefficiency to select the best multi-dimensional partitioning techniques\noptimized for TPU v4 slices based on the application requirements. We combine\nthese with a suite of low-level optimizations to achieve a new Pareto frontier\non the latency and model FLOPS utilization (MFU) tradeoffs on 500B+ parameter\nmodels that outperforms the FasterTransformer suite of benchmarks. We further\nshow that with appropriate partitioning, the lower memory requirements of\nmultiquery attention (i.e. multiple query heads share single key/value head)\nenables scaling up to 32x larger context lengths. Finally, we achieve a\nlow-batch-size latency of 29ms per token during generation (using int8 weight\nquantization) and a 76% MFU during large-batch-size processing of input tokens,\nwhile supporting a long 2048-token context length on the PaLM 540B parameter\nmodel.", "published": "2022-11-09 18:50:38", "link": "http://arxiv.org/abs/2211.05102v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Grammatical Error Correction: A Survey of the State of the Art", "abstract": "Grammatical Error Correction (GEC) is the task of automatically detecting and\ncorrecting errors in text. The task not only includes the correction of\ngrammatical errors, such as missing prepositions and mismatched subject-verb\nagreement, but also orthographic and semantic errors, such as misspellings and\nword choice errors respectively. The field has seen significant progress in the\nlast decade, motivated in part by a series of five shared tasks, which drove\nthe development of rule-based methods, statistical classifiers, statistical\nmachine translation, and finally neural machine translation systems which\nrepresent the current dominant state of the art. In this survey paper, we\ncondense the field into a single article and first outline some of the\nlinguistic challenges of the task, introduce the most popular datasets that are\navailable to researchers (for both English and other languages), and summarise\nthe various methods and techniques that have been developed with a particular\nfocus on artificial error generation. We next describe the many different\napproaches to evaluation as well as concerns surrounding metric reliability,\nespecially in relation to subjective human judgements, before concluding with\nan overview of recent progress and suggestions for future work and remaining\nchallenges. We hope that this survey will serve as comprehensive resource for\nresearchers who are new to the field or who want to be kept apprised of recent\ndevelopments.", "published": "2022-11-09 19:34:38", "link": "http://arxiv.org/abs/2211.05166v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Collateral facilitation in humans and language models", "abstract": "Are the predictions of humans and language models affected by similar things?\nResearch suggests that while comprehending language, humans make predictions\nabout upcoming words, with more predictable words being processed more easily.\nHowever, evidence also shows that humans display a similar processing advantage\nfor highly anomalous words when these words are semantically related to the\npreceding context or to the most probable continuation. Using stimuli from 3\npsycholinguistic experiments, we find that this is also almost always also the\ncase for 8 contemporary transformer language models (BERT, ALBERT, RoBERTa,\nXLM-R, GPT-2, GPT-Neo, GPT-J, and XGLM). We then discuss the implications of\nthis phenomenon for our understanding of both human language comprehension and\nthe predictions made by language models.", "published": "2022-11-09 21:08:08", "link": "http://arxiv.org/abs/2211.05198v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HilMeMe: A Human-in-the-Loop Machine Translation Evaluation Metric\n  Looking into Multi-Word Expressions", "abstract": "With the fast development of Machine Translation (MT) systems, especially the\nnew boost from Neural MT (NMT) models, the MT output quality has reached a new\nlevel of accuracy. However, many researchers criticised that the current\npopular evaluation metrics such as BLEU can not correctly distinguish the\nstate-of-the-art NMT systems regarding quality differences. In this short\npaper, we describe the design and implementation of a linguistically motivated\nhuman-in-the-loop evaluation metric looking into idiomatic and terminological\nMulti-word Expressions (MWEs). MWEs have played a bottleneck in many Natural\nLanguage Processing (NLP) tasks including MT. MWEs can be used as one of the\nmain factors to distinguish different MT systems by looking into their\ncapabilities in recognising and translating MWEs in an accurate and meaning\nequivalent manner.", "published": "2022-11-09 21:15:40", "link": "http://arxiv.org/abs/2211.05201v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis of Persian Language: Review of Algorithms, Approaches\n  and Datasets", "abstract": "Sentiment analysis aims to extract people's emotions and opinion from their\ncomments on the web. It widely used in businesses to detect sentiment in social\ndata, gauge brand reputation, and understand customers. Most of articles in\nthis area have concentrated on the English language whereas there are limited\nresources for Persian language. In this review paper, recent published articles\nbetween 2018 and 2022 in sentiment analysis in Persian Language have been\ncollected and their methods, approach and dataset will be explained and\nanalyzed. Almost all the methods used to solve sentiment analysis are machine\nlearning and deep learning. The purpose of this paper is to examine 40\ndifferent approach sentiment analysis in the Persian Language, analysis\ndatasets along with the accuracy of the algorithms applied to them and also\nreview strengths and weaknesses of each. Among all the methods, transformers\nsuch as BERT and RNN Neural Networks such as LSTM and Bi-LSTM have achieved\nhigher accuracy in the sentiment analysis. In addition to the methods and\napproaches, the datasets reviewed are listed between 2018 and 2022 and\ninformation about each dataset and its details are provided.", "published": "2022-11-09 13:08:46", "link": "http://arxiv.org/abs/2212.06041v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Noisy Student Training on Non-target Domain Data for Automatic\n  Speech Recognition", "abstract": "Noisy Student Training (NST) has recently demonstrated extremely strong\nperformance in Automatic Speech Recognition(ASR). In this paper, we propose a\ndata selection strategy named LM Filter to improve the performance of NST on\nnon-target domain data in ASR tasks. Hypotheses with and without a Language\nModel are generated and the CER differences between them are utilized as a\nfilter threshold. Results reveal that significant improvements of 10.4%\ncompared with no data filtering baselines. We can achieve 3.31% CER in\nAISHELL-1 test set, which is best result from our knowledge without any other\nsupervised data. We also perform evaluations on the supervised 1000 hour\nAISHELL-2 dataset and competitive results of 4.73% CER can be achieved.", "published": "2022-11-09 07:23:15", "link": "http://arxiv.org/abs/2211.04717v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Distribution-based Emotion Recognition in Conversation", "abstract": "Automatic emotion recognition in conversation (ERC) is crucial for\nemotion-aware conversational artificial intelligence. This paper proposes a\ndistribution-based framework that formulates ERC as a sequence-to-sequence\nproblem for emotion distribution estimation. The inherent ambiguity of emotions\nand the subjectivity of human perception lead to disagreements in emotion\nlabels, which is handled naturally in our framework from the perspective of\nuncertainty estimation in emotion distributions. A Bayesian training loss is\nintroduced to improve the uncertainty estimation by conditioning each emotional\nstate on an utterance-specific Dirichlet prior distribution. Experimental\nresults on the IEMOCAP dataset show that ERC outperformed the\nsingle-utterance-based system, and the proposed distribution-based ERC methods\nhave not only better classification accuracy, but also show improved\nuncertainty estimation.", "published": "2022-11-09 12:16:28", "link": "http://arxiv.org/abs/2211.04834v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Efficient Speech Translation with Pre-trained Models", "abstract": "When building state-of-the-art speech translation models, the need for large\ncomputational resources is a significant obstacle due to the large training\ndata size and complex models. The availability of pre-trained models is a\npromising opportunity to build strong speech translation systems efficiently.\nIn a first step, we investigate efficient strategies to build cascaded and\nend-to-end speech translation systems based on pre-trained models. Using this\nstrategy, we can train and apply the models on a single GPU. While the\nend-to-end models show superior translation performance to cascaded ones, the\napplication of this technology has a limitation on the need for additional\nend-to-end training data. In a second step, we proposed an additional\nsimilarity loss to encourage the model to generate similar hidden\nrepresentations for speech and transcript. Using this technique, we can\nincrease the data efficiency and improve the translation quality by 6 BLEU\npoints in scenarios with limited end-to-end training data.", "published": "2022-11-09 15:07:06", "link": "http://arxiv.org/abs/2211.04939v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Accountable and Explainable Methods for Complex Reasoning over Text", "abstract": "A major concern of Machine Learning (ML) models is their opacity. They are\ndeployed in an increasing number of applications where they often operate as\nblack boxes that do not provide explanations for their predictions. Among\nothers, the potential harms associated with the lack of understanding of the\nmodels' rationales include privacy violations, adversarial manipulations, and\nunfair discrimination. As a result, the accountability and transparency of ML\nmodels have been posed as critical desiderata by works in policy and law,\nphilosophy, and computer science.\n  In computer science, the decision-making process of ML models has been\nstudied by developing accountability and transparency methods. Accountability\nmethods, such as adversarial attacks and diagnostic datasets, expose\nvulnerabilities of ML models that could lead to malicious manipulations or\nsystematic faults in their predictions. Transparency methods explain the\nrationales behind models' predictions gaining the trust of relevant\nstakeholders and potentially uncovering mistakes and unfairness in models'\ndecisions. To this end, transparency methods have to meet accountability\nrequirements as well, e.g., being robust and faithful to the underlying\nrationales of a model.\n  This thesis presents my research that expands our collective knowledge in the\nareas of accountability and transparency of ML models developed for complex\nreasoning tasks over text.", "published": "2022-11-09 15:14:52", "link": "http://arxiv.org/abs/2211.04946v1", "categories": ["cs.LG", "cs.CL", "cs.CY", "68T50", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Cross-lingual Transfer Learning for Check-worthy Claim Identification\n  over Twitter", "abstract": "Misinformation spread over social media has become an undeniable infodemic.\nHowever, not all spreading claims are made equal. If propagated, some claims\ncan be destructive, not only on the individual level, but to organizations and\neven countries. Detecting claims that should be prioritized for fact-checking\nis considered the first step to fight against spread of fake news. With\ntraining data limited to a handful of languages, developing supervised models\nto tackle the problem over lower-resource languages is currently infeasible.\nTherefore, our work aims to investigate whether we can use existing datasets to\ntrain models for predicting worthiness of verification of claims in tweets in\nother languages. We present a systematic comparative study of six approaches\nfor cross-lingual check-worthiness estimation across pairs of five diverse\nlanguages with the help of Multilingual BERT (mBERT) model. We run our\nexperiments using a state-of-the-art multilingual Twitter dataset. Our results\nshow that for some language pairs, zero-shot cross-lingual transfer is possible\nand can perform as good as monolingual models that are trained on the target\nlanguage. We also show that in some languages, this approach outperforms (or at\nleast is comparable to) state-of-the-art models.", "published": "2022-11-09 18:18:53", "link": "http://arxiv.org/abs/2211.05087v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Accidental Learners: Spoken Language Identification in Multilingual\n  Self-Supervised Models", "abstract": "In this paper, we extend previous self-supervised approaches for language\nidentification by experimenting with Conformer based architecture in a\nmultilingual pre-training paradigm. We find that pre-trained speech models\noptimally encode language discriminatory information in lower layers. Further,\nwe demonstrate that the embeddings obtained from these layers are significantly\nrobust to classify unseen languages and different acoustic environments without\nadditional training. After fine-tuning a pre-trained Conformer model on the\nVoxLingua107 dataset, we achieve results similar to current state-of-the-art\nsystems for language identification. More, our model accomplishes this with 5x\nless parameters. We open-source the model through the NVIDIA NeMo toolkit.", "published": "2022-11-09 18:53:59", "link": "http://arxiv.org/abs/2211.05103v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Large Language Models with Controllable Working Memory", "abstract": "Large language models (LLMs) have led to a series of breakthroughs in natural\nlanguage processing (NLP), owing to their excellent understanding and\ngeneration abilities. Remarkably, what further sets these models apart is the\nmassive amounts of world knowledge they internalize during pretraining. While\nmany downstream applications provide the model with an informational context to\naid its performance on the underlying task, how the model's world knowledge\ninteracts with the factual information presented in the context remains under\nexplored. As a desirable behavior, an LLM should give precedence to the context\nwhenever it contains task-relevant information that conflicts with the model's\nmemorized knowledge. This enables model predictions to be grounded in the\ncontext, which can then be used to update or correct specific model predictions\nwithout frequent retraining. By contrast, when the context is irrelevant to the\ntask, the model should ignore it and fall back on its internal knowledge. In\nthis paper, we undertake a first joint study of the aforementioned two\nproperties, namely controllability and robustness, in the context of LLMs. We\ndemonstrate that state-of-the-art T5 and PaLM (both pretrained and finetuned)\ncould exhibit poor controllability and robustness, which do not scale with\nincreasing model size. As a solution, we propose a novel method - Knowledge\nAware FineTuning (KAFT) - to strengthen both controllability and robustness by\nincorporating counterfactual and irrelevant contexts to standard supervised\ndatasets. Our comprehensive evaluation showcases the utility of KAFT across\nmodel architectures and sizes.", "published": "2022-11-09 18:58:29", "link": "http://arxiv.org/abs/2211.05110v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adaptive Multi-Corpora Language Model Training for Speech Recognition", "abstract": "Neural network language model (NNLM) plays an essential role in automatic\nspeech recognition (ASR) systems, especially in adaptation tasks when text-only\ndata is available. In practice, an NNLM is typically trained on a combination\nof data sampled from multiple corpora. Thus, the data sampling strategy is\nimportant to the adaptation performance. Most existing works focus on designing\nstatic sampling strategies. However, each corpus may show varying impacts at\ndifferent NNLM training stages. In this paper, we introduce a novel adaptive\nmulti-corpora training algorithm that dynamically learns and adjusts the\nsampling probability of each corpus along the training process. The algorithm\nis robust to corpora sizes and domain relevance. Compared with static sampling\nstrategy baselines, the proposed approach yields remarkable improvement by\nachieving up to relative 7% and 9% word error rate (WER) reductions on\nin-domain and out-of-domain adaptation tasks, respectively.", "published": "2022-11-09 06:54:50", "link": "http://arxiv.org/abs/2211.05121v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Uni-Parser: Unified Semantic Parser for Question Answering on Knowledge\n  Base and Database", "abstract": "Parsing natural language questions into executable logical forms is a useful\nand interpretable way to perform question answering on structured data such as\nknowledge bases (KB) or databases (DB). However, existing approaches on\nsemantic parsing cannot adapt to both modalities, as they suffer from the\nexponential growth of the logical form candidates and can hardly generalize to\nunseen data. In this work, we propose Uni-Parser, a unified semantic parser for\nquestion answering (QA) on both KB and DB. We introduce the primitive (relation\nand entity in KB, and table name, column name and cell value in DB) as an\nessential element in our framework. The number of primitives grows linearly\nwith the number of retrieved relations in KB and DB, preventing us from dealing\nwith exponential logic form candidates. We leverage the generator to predict\nfinal logical forms by altering and composing topranked primitives with\ndifferent operations (e.g. select, where, count). With sufficiently pruned\nsearch space by a contrastive primitive ranker, the generator is empowered to\ncapture the composition of primitives enhancing its generalization ability. We\nachieve competitive results on multiple KB and DB QA benchmarks more\nefficiently, especially in the compositional and zero-shot settings.", "published": "2022-11-09 19:33:27", "link": "http://arxiv.org/abs/2211.05165v1", "categories": ["cs.CL", "cs.AI", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Speech separation with large-scale self-supervised learning", "abstract": "Self-supervised learning (SSL) methods such as WavLM have shown promising\nspeech separation (SS) results in small-scale simulation-based experiments. In\nthis work, we extend the exploration of the SSL-based SS by massively scaling\nup both the pre-training data (more than 300K hours) and fine-tuning data (10K\nhours). We also investigate various techniques to efficiently integrate the\npre-trained model with the SS network under a limited computation budget,\nincluding a low frame rate SSL model training setup and a fine-tuning scheme\nusing only the part of the pre-trained model. Compared with a supervised\nbaseline and the WavLM-based SS model using feature embeddings obtained with\nthe previously released 94K hours trained WavLM, our proposed model obtains\n15.9% and 11.2% of relative word error rate (WER) reductions, respectively, for\na simulated far-field speech mixture test set. For conversation transcription\non real meeting recordings using continuous speech separation, the proposed\nmodel achieves 6.8% and 10.6% of relative WER reductions over the purely\nsupervised baseline on AMI and ICSI evaluation sets, respectively, while\nreducing the computational cost by 38%.", "published": "2022-11-09 20:00:21", "link": "http://arxiv.org/abs/2211.05172v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Expressive-VC: Highly Expressive Voice Conversion with Attention Fusion\n  of Bottleneck and Perturbation Features", "abstract": "Voice conversion for highly expressive speech is challenging. Current\napproaches struggle with the balancing between speaker similarity,\nintelligibility and expressiveness. To address this problem, we propose\nExpressive-VC, a novel end-to-end voice conversion framework that leverages\nadvantages from both neural bottleneck feature (BNF) approach and information\nperturbation approach. Specifically, we use a BNF encoder and a Perturbed-Wav\nencoder to form a content extractor to learn linguistic and para-linguistic\nfeatures respectively, where BNFs come from a robust pre-trained ASR model and\nthe perturbed wave becomes speaker-irrelevant after signal perturbation. We\nfurther fuse the linguistic and para-linguistic features through an attention\nmechanism, where speaker-dependent prosody features are adopted as the\nattention query, which result from a prosody encoder with target speaker\nembedding and normalized pitch and energy of source speech as input. Finally\nthe decoder consumes the integrated features and the speaker-dependent prosody\nfeature to generate the converted speech. Experiments demonstrate that\nExpressive-VC is superior to several state-of-the-art systems, achieving both\nhigh expressiveness captured from the source speech and high speaker similarity\nwith the target speaker; meanwhile intelligibility is well maintained.", "published": "2022-11-09 07:03:59", "link": "http://arxiv.org/abs/2211.04710v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Absolute decision corrupts absolutely: conservative online speaker\n  diarisation", "abstract": "Our focus lies in developing an online speaker diarisation framework which\ndemonstrates robust performance across diverse domains. In online speaker\ndiarisation, outputs generated in real-time are irreversible, and a few\nmisjudgements in the early phase of an input session can lead to catastrophic\nresults. We hypothesise that cautiously increasing the number of estimated\nspeakers is of paramount importance among many other factors. Thus, our\nproposed framework includes decreasing the number of speakers by one when the\nsystem judges that an increase in the past was faulty. We also adopt dual\nbuffers, checkpoints and centroids, where checkpoints are combined with\nsilhouette coefficients to estimate the number of speakers and centroids\nrepresent speakers. Again, we believe that more than one centroid can be\ngenerated from one speaker. Thus we design a clustering-based label matching\ntechnique to assign labels in real-time. The resulting system is lightweight\nyet surprisingly effective. The system demonstrates state-of-the-art\nperformance on DIHARD 2 and 3 datasets, where it is also competitive in AMI and\nVoxConverse test sets.", "published": "2022-11-09 09:52:40", "link": "http://arxiv.org/abs/2211.04768v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Global, and Local Optimization Beamforming for Broadband Sources", "abstract": "This paper presents an alternative energy function for Global Optimization\n(GO) beamforming, tailored to acoustic broadband sources. Given, that\nproperties such as the source location, multipole rotation, or flow conditions\nare parameterized over the frequency, a CSM-fitting can be performed for all\nfrequencies at once. A numerical analysis shows that the nonlinear energy\nfunction for the standard GO problem is equivalent to the source's Point Spread\nFunction (PSF) and contains local minima at the grating- and side lobes'\nlocations. The energy function is improved with the proposed broadband energy,\nas it averages the PSF. Further, it simplifies the process of identifying\nsources and reconstructing their spectra from the results. The paper shows that\nthe method is superior on synthetic monopoles compared to standard GO and\nCLEAN-SC. For real-world data the results of the proposed method and CLEAN-SC\nare similar, and outperform standard GO. The main difference is that source\nassumption violations cause noisy maps for CLEAN-SC and cause wrong spectral\nestimations of the proposed method. By using reasonable initial values, the GO\nproblem reduces to a Local Optimization problem with similar results. Further,\nthe proposed method is able to identify synthetic multipoles with different\npole amplitudes and unknown pole rotations.", "published": "2022-11-09 14:45:56", "link": "http://arxiv.org/abs/2211.04921v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Diffeomorphic Flow-based Variational Framework for Multi-speaker\n  Emotion Conversion", "abstract": "This paper introduces a new framework for non-parallel emotion conversion in\nspeech. Our framework is based on two key contributions. First, we propose a\nstochastic version of the popular CycleGAN model. Our modified loss function\nintroduces a Kullback Leibler (KL) divergence term that aligns the source and\ntarget data distributions learned by the generators, thus overcoming the\nlimitations of sample wise generation. By using a variational approximation to\nthis stochastic loss function, we show that our KL divergence term can be\nimplemented via a paired density discriminator. We term this new architecture a\nvariational CycleGAN (VCGAN). Second, we model the prosodic features of target\nemotion as a smooth and learnable deformation of the source prosodic features.\nThis approach provides implicit regularization that offers key advantages in\nterms of better range alignment to unseen and out of distribution speakers. We\nconduct rigorous experiments and comparative studies to demonstrate that our\nproposed framework is fairly robust with high performance against several\nstate-of-the-art baselines.", "published": "2022-11-09 18:03:29", "link": "http://arxiv.org/abs/2211.05071v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LiCo-Net: Linearized Convolution Network for Hardware-efficient Keyword\n  Spotting", "abstract": "This paper proposes a hardware-efficient architecture, Linearized Convolution\nNetwork (LiCo-Net) for keyword spotting. It is optimized specifically for\nlow-power processor units like microcontrollers. ML operators exhibit\nheterogeneous efficiency profiles on power-efficient hardware. Given the exact\ntheoretical computation cost, int8 operators are more computation-effective\nthan float operators, and linear layers are often more efficient than other\nlayers. The proposed LiCo-Net is a dual-phase system that uses the efficient\nint8 linear operators at the inference phase and applies streaming convolutions\nat the training phase to maintain a high model capacity. The experimental\nresults show that LiCo-Net outperforms single-value decomposition filter (SVDF)\non hardware efficiency with on-par detection performance. Compared to SVDF,\nLiCo-Net reduces cycles by 40% on HiFi4 DSP.", "published": "2022-11-09 01:52:17", "link": "http://arxiv.org/abs/2211.04635v1", "categories": ["cs.LG", "cs.AI", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Efficient Large-scale Audio Tagging via Transformer-to-CNN Knowledge\n  Distillation", "abstract": "Audio Spectrogram Transformer models rule the field of Audio Tagging,\noutrunning previously dominating Convolutional Neural Networks (CNNs). Their\nsuperiority is based on the ability to scale up and exploit large-scale\ndatasets such as AudioSet. However, Transformers are demanding in terms of\nmodel size and computational requirements compared to CNNs. We propose a\ntraining procedure for efficient CNNs based on offline Knowledge Distillation\n(KD) from high-performing yet complex transformers. The proposed training\nschema and the efficient CNN design based on MobileNetV3 results in models\noutperforming previous solutions in terms of parameter and computational\nefficiency and prediction performance. We provide models of different\ncomplexity levels, scaling from low-complexity models up to a new\nstate-of-the-art performance of .483 mAP on AudioSet. Source Code available at:\nhttps://github.com/fschmid56/EfficientAT", "published": "2022-11-09 09:58:22", "link": "http://arxiv.org/abs/2211.04772v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Comparative Study of Data Augmentation Techniques for Deep Learning\n  Based Emotion Recognition", "abstract": "Automated emotion recognition in speech is a long-standing problem. While\nearly work on emotion recognition relied on hand-crafted features and simple\nclassifiers, the field has now embraced end-to-end feature learning and\nclassification using deep neural networks. In parallel to these models,\nresearchers have proposed several data augmentation techniques to increase the\nsize and variability of existing labeled datasets. Despite many seminal\ncontributions in the field, we still have a poor understanding of the interplay\nbetween the network architecture and the choice of data augmentation. Moreover,\nonly a handful of studies demonstrate the generalizability of a particular\nmodel across multiple datasets, which is a prerequisite for robust real-world\nperformance. In this paper, we conduct a comprehensive evaluation of popular\ndeep learning approaches for emotion recognition. To eliminate bias, we fix the\nmodel architectures and optimization hyperparameters using the VESUS dataset\nand then use repeated 5-fold cross validation to evaluate the performance on\nthe IEMOCAP and CREMA-D datasets. Our results demonstrate that long-range\ndependencies in the speech signal are critical for emotion recognition and that\nspeed/rate augmentation offers the most robust performance gain across models.", "published": "2022-11-09 17:27:03", "link": "http://arxiv.org/abs/2211.05047v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multimodal Dyadic Impression Recognition via Listener Adaptive\n  Cross-Domain Fusion", "abstract": "As a sub-branch of affective computing, impression recognition, e.g.,\nperception of speaker characteristics such as warmth or competence, is\npotentially a critical part of both human-human conversations and spoken\ndialogue systems. Most research has studied impressions only from the behaviors\nexpressed by the speaker or the response from the listener, yet ignored their\nlatent connection. In this paper, we perform impression recognition using a\nproposed listener adaptive cross-domain architecture, which consists of a\nlistener adaptation function to model the causality between speaker and\nlistener behaviors and a cross-domain fusion function to strengthen their\nconnection. The experimental evaluation on the dyadic IMPRESSION dataset\nverified the efficacy of our method, producing concordance correlation\ncoefficients of 78.8% and 77.5% in the competence and warmth dimensions,\noutperforming previous studies. The proposed method is expected to be\ngeneralized to similar dyadic interaction scenarios.", "published": "2022-11-09 19:23:00", "link": "http://arxiv.org/abs/2211.05163v4", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
